---
title: "Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits"
summary: "This paper proposes a novel multi-draft speculative sampling method for faster LLM decoding. It introduces a two-step optimal token selection architecture (importance sampling and single-draft specula....."
categories: ["AI Generated"]
tags: ["ðŸ”– 2024-10-23", "ðŸ¤— 2024-10-25"]
showSummary: true
date: 2024-10-23
draft: false
---

### TL;DR


{{< lead >}}

This paper proposes a novel multi-draft speculative sampling method for faster LLM decoding. It introduces a two-step optimal token selection architecture (importance sampling and single-draft speculative sampling), offering theoretical analysis and demonstrating significant improvements in block efficiency and token rates, particularly when draft sequences have non-identical distributions. This method improves decoding speed and efficiency for LLMs.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.18234" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}

#### Why does this paper matter?
This paper introduces multi-draft speculative sampling, improving large language model (LLM) decoding efficiency.  It provides a canonical two-step architecture for optimal token selection and theoretical analysis demonstrating improvements over existing methods.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Multi-draft speculative sampling improves LLM decoding efficiency by using multiple draft models to generate candidate tokens. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} An optimal token selection scheme can be decomposed into a two-step process: importance sampling followed by single-draft speculative sampling. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Theoretical analysis provides insights into optimal acceptance probabilities and a new token selection scheme based on weighted importance sampling. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](figures/figures_4_0.png "ðŸ”¼ Optimal Approach for Multi-Draft Speculative Sampling")





{{< table-caption caption="ðŸ”½ Table 1: Block efficiency achieved in the Dolly task for different number of draft models." >}}
| Scheme | K = 2 | K = 3 | K = 4 | K = 5 | K = 6 |
| --- | --- | --- | --- | --- | --- |
| IS | 2.13 åœŸ 0.05 | 2.22 å£« 0.05 | 2.26 åœŸ 0.05 | 2.27 å£« 0.05 | 2.28 å£« 0.06 |
| SpecInfer | 1.76 å£« 0.04 | 1.86 å£« 0.05 | 1.95 åœŸ 0.05 | 2.00 å£« 0.04 | 2.04 å£« 0.05 |
| SpecTr | 1.77 åœŸ 0.04 | 1.89 åœŸ 0.05 | 1.96 åœŸ 0.05 | 2.03 å£« 0.06 | 2.08 åœŸ 0.04 |
{{< /table-caption >}}


------



<details>
<summary>More on figures
</summary>


![](figures/figures_24_0.png "ðŸ”¼ Optimal Approach for Multi-Draft Speculative Sampling")

![](figures/figures_35_0.png "ðŸ”¼ Figure 2: Numerical evaluation of Pr(accept) for the optimal scheme (Theorem 3) as well as two baseline schemes â€“ SpecTr (Sun et al., 2024b) and SpecInfer (Miao et al., 2024). For sake of illustration we select alphabet Î© = {1,2,3} and p = [1/3,1/3, 1/3]. The left plot sets q = [1/3, q2, 2/3-q2] while the right plot sets q = [1/6, q2, 5/6 - q2] where q2 is varied on the x-axis.")


</details>

------







------

<details>
<summary>More on tables
</summary>


{{< table-caption caption="ðŸ”½ Table 2: Effect of LP Truncation and Alphabet Truncation" >}}
|  |  | Block Efficiency | Token Rate (% improvement to SD) |
| --- | --- | --- | --- |
| Alphabet Truncation ( 2âŒ€ ) | 10 | 1.98 å£« 0.03 | -0.57 å£« 3.38% |
| Alphabet Truncation ( 2âŒ€ ) | 20 | 2.00 å£« 0.04 | 1.00 åœŸ 3.08% |
| Alphabet Truncation ( 2âŒ€ ) | 40 | 2.05 å£« 0.04 | 6.63 åœŸ 3.18% |
| Alphabet Truncation ( 2âŒ€ ) | 50 | 2.03 å£« 0.05 | 3.22 åœŸ 3.39% |
| LP-Truncation Threshold (s) | 5 | 2.05 å£« 0.04 | 6.63 å£« 3.18% |
| LP-Truncation Threshold (s) | 10 | 2.04 åœŸ 0.05 | 1.52 åœŸ 3.47% |
| LP-Truncation Threshold (s) | 15 | 2.04 å£« 0.04 | 1.74 åœŸ 2.36% |
{{< /table-caption >}}

{{< table-caption caption="ðŸ”½ Comparison of average acceptance probability across different tasks for K = 2, 4, 8 drafts." >}}
| Scheme | XSum | XSum | XSum | Dolly | Dolly | Dolly |
| --- | --- | --- | --- | --- | --- | --- |
|  | K=2 | K=4 | K=8 | K=2 | K=4 | K=8 |
| Optimal | 0.5009 | 0.5226 | 0.5419 | 0.6384 | 0.6731 | 0.6962 |
| IS | 0.4933 | 0.5145 | 0.5333 | 0.6348 | 0.6691 | 0.6919 |
| SpecTr | 0.4889 | 0.5083 | 0.5263 | 0.6246 | 0.6560 | 0.6800 |
| SpecInfer | 0.4875 | 0.5058 | 0.5227 | 0.6202 | 0.6489 | 0.6722 |
{{< /table-caption >}}

{{< table-caption caption="ðŸ”½ Table 4: Block Efficiency achieved in the Dolly Task with top-k sampling" >}}
| Sampling | Scheme | K = 2 drafts | K = 2 drafts | K = 3 drafts | K = 3 drafts |
| --- | --- | --- | --- | --- | --- |
| Sampling | Scheme | Block Efficiency | Loss | Block Efficiency | Loss |
| top-k (k = 10) | IS | 2.48 åœŸ 0.01 |  | 2.59 å£« 0.02 |  |
| top-k (k = 10) | SpecTr | 2.43 åœŸ 0.01 | 98% | 2.55 å£« 0.01 | 98% |
| top-k (k = 10) | SpecInfer | 2.38 å£« 0.02 | 96% | 2.49 å£« 0.02 | 96% |
| top-k (k = 5) | IS | 2.52 å£« 0.02 |  | 2.63 å£« 0.03 |  |
| top-k (k = 5) | SpecTr | 2.48 åœŸ 0.02 | 98% | 2.56 å£« 0.03 | 97% |
| top-k (k = 5) | SpecInfer | 2.47 å£« 0.01 | 98% | 2.55 å£« 0.04 | 97% |
{{< /table-caption >}}

{{< table-caption caption="ðŸ”½ Comparison of average acceptance probability across different tasks for K = 2, 4, 8 drafts." >}}
| Draft Temp. | 1.2 | 1.4 | 1.6 | 2.0 | 2.4 |
| --- | --- | --- | --- | --- | --- |
| Decoder | Decoder | Decoder | Decoder | Decoder | Decoder |
| IS | 0.186 å£« 0.004 | 0.188 åœŸ 0.002 | 0.191 åœŸ 0.003 | 0.186 åœŸ 0.004 | 0.187 å£« 0.003 |
| Signle-draft SD | 0.190 å£« 0.006 | 0.185 å£« 0.005 | 0.190 å£« 0.004 | 0.186 å£« 0.003 | 0.186 å£« 0.004 |
| SpecInfer | 0.184 åœŸ 0.004 | 0.190 åœŸ 0.002 | 0.187 åœŸ 0.001 | 0.186 å£« 0.003 | 0.186 å£« 0.004 |
| SpecTr | 0.188 åœŸ 0.002 | 0.182 åœŸ 0.006 | 0.188 å£« 0.001 | 0.185 åœŸ 0.006 | 0.188 åœŸ 0.001 |
{{< /table-caption >}}

{{< table-caption caption="ðŸ”½ Comparison of average acceptance probability across different tasks for K = 2, 4, 8 drafts." >}}
| Draft Temp. | 1.2 | 1.4 | 1.6 | 2.0 | 2.4 |
| --- | --- | --- | --- | --- | --- |
| Decoder | Decoder | Decoder | Decoder | Decoder | Decoder |
| IS | 0.037 å£« 0.002 | 0.038 åœŸ 0.004 | 0.034 åœŸ 0.002 | 0.039 å£« 0.003 | 0.039 åœŸ 0.002 |
| Signle-draft SD | 0.036 åœŸ 0.000 | 0.037 åœŸ 0.003 | 0.038 åœŸ 0.004 | 0.037 å£« 0.003 | 0.038 åœŸ 0.002 |
| SpecInfer | 0.035 åœŸ 0.003 | 0.039 åœŸ 0.004 | 0.035 å£« 0.003 | 0.034 å£« 0.009 | 0.036 åœŸ 0.003 |
| SpecTr | 0.039 åœŸ 0.001 | 0.037 åœŸ 0.001 | 0.039 åœŸ 0.001 | 0.036 å£« 0.002 | 0.035 å£« 0.001 |
{{< /table-caption >}}

{{< table-caption caption="ðŸ”½ Table 7: ROUGE-L scores on the XSum task across various decoders and sampling temperatures." >}}
|  | Temperature | Temperature | Temperature | Temperature | Temperature |
| --- | --- | --- | --- | --- | --- |
| Draft 1 | 1.2 | 1.2 | 1.2 | 1.2 | 1.2 |
| Draft 2 | 1.2 | 1.6 | 2.0 | 2.4 | N/A |
| Decoder |  |  |  |  |  |
| IS | 0.187 å£« 0.004 | 0.189 åœŸ 0.007 | 0.189 å£« 0.001 | 0.191 å£« 0.002 | - |
| SpecInfer | 0.184 å£« 0.004 | 0.190 åœŸ 0.003 | 0.185 åœŸ 0.006 | 0.189 åœŸ 0.006 |  |
| Single-draft SD | - |  |  | - | 0.190 åœŸ 0.006 |
{{< /table-caption >}}

{{< table-caption caption="ðŸ”½ Table 8: BLEU scores on the WMT dataset across various decoders and sampling temperatures." >}}
|  | Temperature | Temperature | Temperature | Temperature | Temperature |
| --- | --- | --- | --- | --- | --- |
| Draft 1 | 1.2 | 1.2 | 1.2 | 1.2 | 1.2 |
| Draft 2 | 1.2 | 1.6 | 2.0 | 2.4 | N/A |
| Decoder |  |  |  |  |  |
| IS | 0.036 åœŸ 0.003 | 0.035 åœŸ 0.002 | 0.036 åœŸ 0.002 | 0.035 å£« 0.002 | - |
| SpecInfer | 0.035 å£« 0.003 | 0.038 åœŸ 0.005 | 0.041 åœŸ 0.002 | 0.040 åœŸ 0.002 |  |
| Single-draft SD | - | - | - | - | 0.036 å£« 0.000 |
{{< /table-caption >}}


</details>

------

