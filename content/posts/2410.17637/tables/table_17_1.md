| Setting | Models | Evaluation Metric | Number | Source |
| --- | --- | --- | --- | --- |
| Multi-Images Benchmark | MMMU (Yue et al., 2024) | Multiple Choice | 1,050 | MMMU |
| Multi-Images Benchmark | BLINK (Fu et al., 2024) | Multiple Choice | 3,807 | BLINK |
| Multi-Images Benchmark | NLVR2 (Suhr et al., 2018) | Multiple Choice | 6,967 | NLVR2 |
| Multi-Images Benchmark | Mantis-Eval (Jiang et al., 2024) | Multiple Choice | 217 | Mantis-Eval |
| Multi-Images Benchmark | MVBench (Li et al., 2024c) | Multiple Choice | 4,000 | MVBench |
| Single-Image Benchmark | MMStar (Chen et al., 2024a) | Multiple Choice | 1,500 | MMStar |
| Single-Image Benchmark | Sci-QA (Lu et al., 2022) | Multiple Choice | 4,241 | ScienceQA |
| Single-Image Benchmark | MMVet (Yu et al., 2023) | Subjective Questions | 218 | MM-Vet |
| Single-Image Benchmark | POPE (Li et al., 2023c) | Yes/No | 9,000 | POPE |
| Single-Image Benchmark | MMB (Liu et al., 2023) | Multiple Choice | 1,164 | MMBench |
| Single-Image Benchmark | Math (Lu et al., 2023) | Multiple Choice | 6,141 | Math Vista |
| Single-Image Benchmark | AI2D (Kembhavi et al., 2016) | Multiple Choice | 3,090 | AI2D |
