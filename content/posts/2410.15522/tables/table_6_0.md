|  |  | TRANSLATION-EASY | TRANSLATION-EASY | TRANSLATION-EASY | TRANSLATION-EASY | TRANSLATION-HARD | TRANSLATION-HARD | TRANSLATION-HARD | TRANSLATION-HARD |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Reward Model | Avg | de→en | en→de | zh→en | en→zh | de→en | en→de | zh→en | en→zh |
| GPT-4o | 82.5 | 87.0 | 95.0 | 91.0 | 98.0 | 71.0 | 61.0 | 77.0 | 80.0 |
| GPT-4 Turbo | 82.2 | 87.0 | 95.0 | 94.0 | 97.0 | 62.5 | 66.0 | 72.0 | 84.0 |
| Eurus RM 7B | 80.0 | 85.0 | 91.0 | 92.0 | 96.0 | 59.0 | 61.0 | 74.0 | 82.0 |
| URM LlaMa 3.1 8B | 79.8 | 89.0 | 92.0 | 90.0 | 94.0 | 67.0 | 60.0 | 72.0 | 74.0 |
| Llama 3.1 70B | 79.1 | 81.0 | 93.0 | 92.0 | 97.0 | 56.0 | 61.0 | 67.5 | 85.0 |
| BTRM Qwen 2 7B | 79.0 | 81.0 | 89.0 | 92.0 | 97.0 | 67.0 | 58.0 | 72.0 | 76.0 |
| Llama 3 70B | 77.1 | 80.5 | 88.0 | 92.0 | 96.0 | 56.0 | 63.0 | 58.0 | 83.0 |
| Gemma 2 9B | 76.9 | 80.5 | 93.0 | 84.0 | 97.0 | 57.5 | 66.0 | 52.0 | 85.0 |
| Tulu 2.5 13B RM | 75.8 | 80.0 | 82.0 | 88.0 | 96.0 | 60.0 | 55.0 | 68.0 | 77.0 |
| Aya 23 35B | 74.8 | 75.0 | 89.0 | 84.0 | 95.0 | 55.0 | 66.0 | 54.0 | 80.0 |
