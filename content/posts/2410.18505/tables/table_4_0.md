| Parameter | Value |
| --- | --- |
| attention_dropout | 0.0 |
| bos_token_id | 151849 |
| eos_token_id | 151850 |
| hidden_act | silu |
| hidden_size | 896 |
| intermediate_size | 2432 |
| max_position_embeddings | 4096 |
| num_attention_heads | 14 |
| num_hidden_layers | 24 |
| num_key_value_heads | 2 |
| pad_token_id | 151643 |
| rms_norm_eps | 1e-06 |
| rope_theta | 10000 |
| tie_ word_embeddings | True |
| torch_dtype | bfloat16 |
| vocab_size | 151851 |
