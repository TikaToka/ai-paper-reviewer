| Model | TruthfulQA (MC) | TruthfulQA (MC) | TruthfulQA (MC) | TriviaQA | PopQA | TruthfulQA (Generation) | TruthfulQA (Generation) | TruthfulQA (Generation) | TruthfulQA (Generation) | NQ-Open |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Model | MC1 ↑ | MC2↑ | MC3↑ | EM↑ | EM ↑ | %Truth ↑ | %Info ↑ | %TnI↑ | %Reject ↓ | EM↑ |
| Mistral-7B-Instruct-v0.3 | 50.31 | 65.62 | 38.29 | 59.99 | 26.65 | 80.54 | 97.06 | 77.60 | 26.07 | 31.49 |
| + DoLA (low) (Chuang et al., 2023) | 50.18 | 65.64 | 38.17 | 60.06 | 26.68 | 80.29 | 97.31 | 77.60 | 25.70 | 31.53 |
| + DoLA (high) (Chuang et al., 2023) | 50.18 | 65.61 | 38.18 | 60.03 | 26.68 | 80.54 | 97.06 | 77.60 | 25.70 | 31.53 |
| + AD (Chen et al., 2024) | 43.82 | 64.44 | 35.67 | 59.92 | 26.66 | 80.29 | 97.18 | 77.48 | 25.70 | 30.55 |
| + DeCoRe static (Ours) | 53.49 | 67.13 | 39.48 | 60.09 | 27.02 | 77.85 | 97.43 | 75.40 | 20.81 | 31.38 |
| + DeCoRe entropy (Ours) | 54.84 | 69.08 | 41.82 | 59.64 | 27.11 | 76.99 | 97.80 | 74.79 | 15.91 | 31.45 |
| Qwen2-7B-Instruct | 29.99 | 48.08 | 24.22 | 42.77 | 17.55 | 80.78 | 67.93 | 48.71 | 37.33 | 25.91 |
| + DoLA (low) (Chuang et al., 2023) | 30.11 | 49.11 | 25.09 | 40.57 | 15.85 | 84.58 | 65.36 | 50.06 | 41.74 | 23.84 |
| + DoLA (high) (Chuang et al., 2023) | 20.44 | 47.09 | 22.76 | 37.82 | 13.84 | 83.97 | 61.57 | 45.53 | 45.17 | 21.36 |
| + AD (Chen et al., 2024) | 30.85 | 49.71 | 25.33 | 42.13 | 18.19 | 78.09 | 79.68 | 57.83 | 26.31 | 24.41 |
| + DeCoRe static (Ours) | 31.09 | 48.23 | 25.20 | 42.50 | 17.71 | 79.31 | 69.28 | 48.59 | 37.33 | 26.06 |
| + DeCoRe entropy (Ours) | 34.52 | 51.79 | 27.30 | 41.30 | 17.15 | 76.87 | 76.74 | 53.61 | 26.81 | 25.05 |
