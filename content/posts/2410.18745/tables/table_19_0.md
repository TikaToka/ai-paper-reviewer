| Model | Ltrain | HF PATH | Peak Failure Depth | Acc |
| --- | --- | --- | --- | --- |
| GPT-4-128K |  | - | 0-33.3% | 100.0 |
| Trained on open-source data |  |  |  |  |
| TinyLlama-1.3b-1T(ours) | 2k |  | 0-33.3% | 56.6 |
| TinyLlama-1.1b-1T | 2k | TimyLicon/Tinyliama/LIB-interneciatex-4806-IT | 0-33.3% | 38.0 |
| TinyLlama-1.1b-3T | 2k | TheyJlamaYIng liam.I.IB-uternesdinep:14211421 | 0-33.3% | 69.8 |
| Pythia-1.4b | 2k | EleutherAI/pythia-1.4b | 0-33.3% | 22.5 |
| OpenLlama-3B | 2k | openlm-research/open_llama_3b | 0-33.3% | 85.0 |
| Llama2-7B | 4k | meta-llama/Llama-2-7b | 0-33.3% | 98.0 |
| Llama3-8B | 8k | meta-llama/Llama-3-7b | 0-33.3% | 99.8 |
| Together-base | 32k | togethercomputer/Llama-2-7B-32K | 0-33.3% | 63.0 |
| LWM-base | 32k | LargeWorldModel/LWM-Text-32K | 0-33.3% | 31.8 |
| Mistral-base | 32k | alpindale/Mistral-7B-v0.2-hf | 0-33.3% | 52.8 |
| Llama3.1-8B | 128k | meta-Ilama/Meta-Llama-3.1-8B | 0-33.3% | 66.0 |
| Yarn-base | 128k | NousResearch/Yam-Llama-2-7b-128k | 0-33.3% | 32.4 |
| Yi-6b-200k | 200k | 01-ai/Yi-6B-200K | 0-33.3% | 20.8 |
| Gradient-Llama3-8B | 262k | graiientaiLlama-3-70B-Instruct-Graien-256k | 0-33.3% | 46.0 |
