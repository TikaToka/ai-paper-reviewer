{"references": [{"fullname_first_author": "Vaswani et al.", "paper_title": "Attention is all you need", "publication_date": "2017-XX-XX", "reason": "This paper introduced the Transformer architecture, a fundamental building block of the current state-of-the-art large language models and video generation models, which are extensively used in the target paper."}, {"fullname_first_author": "Brown et al.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-05-XX", "reason": "This paper demonstrated the effectiveness of large language models and their capability to perform few-shot learning, providing a foundation for the semi-autoregressive video generation model proposed in the target paper."}, {"fullname_first_author": "Yu et al.", "paper_title": "Scaling autoregressive multi-modal models: Pre-training and instruction tuning", "publication_date": "2023-09-XX", "reason": "This paper introduced the MAGVITv2 video tokenizer used in the target paper, which is crucial for the proposed approach's performance and directly influenced the methodology."}, {"fullname_first_author": "Hong et al.", "paper_title": "CogVideo: Large-scale pretraining for text-to-video generation via transformers", "publication_date": "2023-XX-XX", "reason": "This paper is a direct competitor to the target paper, proposing an autoregressive video generation model. The target paper benchmarks against CogVideo, directly comparing their approach."}, {"fullname_first_author": "Unterthiner et al.", "paper_title": "Towards accurate generative models of video: A new metric & challenges", "publication_date": "2018-12-XX", "reason": "This paper introduced the Fr\u00e9chet Video Distance (FVD), the primary evaluation metric used in the target paper, for assessing the quality of generated videos. The choice of metric is a key aspect of the target paper's experimental evaluation."}]}