[{"figure_path": "https://arxiv.org/html/2403.12959/x1.png", "caption": "Figure 1: WHAC synergizes human-camera (camera-frame SMPL-X estimation), camera-world (visual odometry), and human-world (our proposed MotionVelocimeter) modeling for constructing world-grounded human and camera trajectories.", "description": "\uadf8\ub9bc 1\uc740 \uc81c\uc548\ub41c WHAC \ud504\ub808\uc784\uc6cc\ud06c\uac00 \uc138\uacc4 \uc88c\ud45c\uacc4\uc5d0\uc11c \uc0ac\ub78c\uacfc \uce74\uba54\ub77c\uc758 \uada4\uc801\uc744 \uc0dd\uc131\ud558\uae30 \uc704\ud574 \uc138 \uac00\uc9c0 \uc694\uc18c(\uce74\uba54\ub77c \ud504\ub808\uc784 SMPL-X \ucd94\uc815, \uc2dc\uac01\uc801 \uc624\ub3c4\uba54\ud2b8\ub9ac, \uadf8\ub9ac\uace0 \uc81c\uc548\ub41c MotionVelocimeter)\ub97c \uc5b4\ub5bb\uac8c \ud1b5\ud569\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uce74\uba54\ub77c \ud504\ub808\uc784 SMPL-X \ucd94\uc815\uc740 \uc0ac\ub78c\uc758 \uc790\uc138\uc640 \ubaa8\uc591\uc744 \uce74\uba54\ub77c \uc88c\ud45c\uacc4\uc5d0\uc11c \ucd94\uc815\ud569\ub2c8\ub2e4. \uc2dc\uac01\uc801 \uc624\ub3c4\uba54\ud2b8\ub9ac\ub294 \uce74\uba54\ub77c\uc758 \uc6c0\uc9c1\uc784\uc744 \ub098\ud0c0\ub0b4\ub294 \uada4\uc801\uc744 \uc81c\uacf5\ud558\uace0, MotionVelocimeter\ub294 \uc0ac\ub78c\uc758 \uc6c0\uc9c1\uc784\uc5d0\uc11c \uc5bb\uc740 \uacf5\uac04\uc801 \ub2e8\uc11c\ub97c \ud65c\uc6a9\ud558\uc5ec \uc138\uacc4 \uc88c\ud45c\uacc4\uc5d0\uc11c \uc0ac\ub78c\uc758 \uc6c0\uc9c1\uc784\uc744 \ucd94\uc815\ud569\ub2c8\ub2e4. \uc774 \uc138 \uac00\uc9c0 \uc815\ubcf4\ub97c \uacb0\ud569\ud558\uc5ec WHAC\ub294 \uc0ac\ub78c\uc758 \uc6c0\uc9c1\uc784\uacfc \uce74\uba54\ub77c\uc758 \uc6c0\uc9c1\uc784 \ubaa8\ub450\ub97c \uc138\uacc4 \uc88c\ud45c\uacc4\uc5d0\uc11c \uc815\ud655\ud55c \uc2a4\ucf00\uc77c\ub85c \ubcf5\uc6d0\ud569\ub2c8\ub2e4.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2403.12959/x2.png", "caption": "Figure 2: Overview of WHAC. SMPL-X estimator extracts camera-frame SMPL-X with dummy depth, which is recovered in Sec.\u00a03.2. The scaleless camera trajectory estimated by VO is then used to canonicalize the human trajectory to estimate its velocity and thus scale in Sec.\u00a03.3. A camera trajectory is then derived for alignment and scale recovery, which subsequently updates the human trajectory in Sec.\u00a03.4.", "description": "\uadf8\ub9bc 2\ub294 \uc81c\uc548\ub41c WHAC \ud504\ub808\uc784\uc6cc\ud06c\uc758 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uba3c\uc800, \uce74\uba54\ub77c \ud504\ub808\uc784 \uae30\ubc18\uc758 SMPL-X \ucd94\uc815\uae30\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub354\ubbf8 \uae4a\uc774 \uac12\uc744 \uac00\uc9c4 SMPL-X \ub9e4\uac1c\ubcc0\uc218\ub97c \ucd94\ucd9c\ud569\ub2c8\ub2e4. \uc774\ud6c4 3.2\uc808\uc5d0\uc11c \uc124\uba85\ud558\ub294 \ubc29\ubc95\uc73c\ub85c \uc2e4\uc81c \uae4a\uc774 \uac12\uc744 \ubcf5\uc6d0\ud569\ub2c8\ub2e4. \uadf8 \ub2e4\uc74c, \uc2dc\uac01\uc801 \uc624\ub3c4\uba54\ud2b8\ub9ac(VO)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucd94\uc815\ub41c \uc2a4\ucf00\uc77c\uc774 \uc5c6\ub294 \uce74\uba54\ub77c \uada4\uc801\uc744 \uc774\uc6a9\ud574 3.3\uc808\uc5d0\uc11c \uc124\uba85\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \uc0ac\ub78c\uc758 \uada4\uc801\uc744 \uc815\uaddc\ud654\ud558\uc5ec \uc18d\ub3c4\uc640 \uc2a4\ucf00\uc77c\uc744 \ucd94\uc815\ud569\ub2c8\ub2e4.  \ub9c8\uc9c0\ub9c9\uc73c\ub85c, \uc815\ub82c \ubc0f \uc2a4\ucf00\uc77c \ubcf5\uad6c\ub97c \uc704\ud574 \uc0c8\ub85c\uc6b4 \uce74\uba54\ub77c \uada4\uc801\uc744 \ub3c4\ucd9c\ud558\uace0, \uc774\ub97c \uc774\uc6a9\ud574 3.4\uc808\uc5d0\uc11c \uc124\uba85\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \uc0ac\ub78c\uc758 \uada4\uc801\uc744 \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4.  \uc989, \uc138\uacc4 \uc88c\ud45c\uacc4\uc5d0\uc11c \uc815\ud655\ud55c \uc2a4\ucf00\uc77c\uc744 \uac00\uc9c4 \uc0ac\ub78c\uacfc \uce74\uba54\ub77c \uada4\uc801\uc744 \ucd94\uc815\ud558\ub294 \uacfc\uc815\uc744 \ub2e8\uacc4\ubcc4\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2403.12959/x3.png", "caption": "Figure 3: a) Human trajectories H\ud835\udc3bHitalic_H derived from camera trajectories C\ud835\udc36Citalic_C of different scales can be vastly different in both shape and direction, despite that the same camera-frame human root depth dtsubscript\ud835\udc51\ud835\udc61d_{t}italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and translations thcsubscriptsuperscript\ud835\udc61\ud835\udc50\u210et^{c}_{h}italic_t start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT are used. b) Different pairs of focal length f\ud835\udc53fitalic_f and tzsubscript\ud835\udc61\ud835\udc67t_{z}italic_t start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT can correspond to the same image.", "description": "\uadf8\ub9bc 3\uc740 \uce74\uba54\ub77c \uada4\uc801\uc758 \uc2a4\ucf00\uc77c\uc774 \ub2e4\ub97c \ub54c \ub3d9\uc77c\ud55c \uce74\uba54\ub77c \ud504\ub808\uc784\uc758 \uc0ac\ub78c \uae4a\uc774(d<sub>t</sub>)\uc640 \ubcc0\ud658(t<sup>c</sup><sub>h</sub>)\uc744 \uc0ac\uc6a9\ud558\ub354\ub77c\ub3c4 \uc0ac\ub78c \uada4\uc801(H)\uc774 \ubaa8\uc591\uacfc \ubc29\ud5a5\uc774 \ud06c\uac8c \ub2ec\ub77c\uc9c8 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  (a)\ub294 \uc2a4\ucf00\uc77c \ucc28\uc774\uc5d0 \ub530\ub978 \uc0ac\ub78c \uada4\uc801\uc758 \ubcc0\ud654\ub97c, (b)\ub294 \ucd08\uc810 \uac70\ub9ac(f)\uc640 \uae4a\uc774(t<sub>z</sub>)\uc758 \uc11c\ub85c \ub2e4\ub978 \uc870\ud569\uc774 \ub3d9\uc77c\ud55c \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 \uce74\uba54\ub77c \uada4\uc801\uc758 \uc2a4\ucf00\uc77c \ucd94\uc815\uc774 \uc5bc\ub9c8\ub098 \uc911\uc694\ud55c\uc9c0\ub97c \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2403.12959/x4.png", "caption": "Figure 4: Visualization of WHAC-A-Mole sample sequences, animated with a) AMASS, b-c) DLP-MoCap, and d-e) DD100. In each sample, the first row depicts the overview (note the camera trajectory shown in bright rays), and the second and the third rows show the camera view and overlaid SMPL-X annotations.", "description": "\uadf8\ub9bc 4\ub294 WHAC-A-Mole \ub370\uc774\ud130\uc14b\uc758 \uc0d8\ud50c \uc2dc\ud000\uc2a4\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  AMASS(a), DLP-MoCap(b-c), DD100(d-e) \ub370\uc774\ud130\uc14b\uc758 \ubaa8\uc158 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc560\ub2c8\uba54\uc774\uc158\uc744 \uc81c\uc791\ud588\uc2b5\ub2c8\ub2e4. \uac01 \uc0d8\ud50c\uc758 \uccab \ubc88\uc9f8 \uc904\uc740 \uc804\uccb4\uc801\uc778 \uc624\ubc84\ubdf0\ub97c \ubcf4\uc5ec\uc8fc\uba70, \uce74\uba54\ub77c\uc758 \uada4\uc801\uc740 \ubc1d\uc740 \uc120\uc73c\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ub450 \ubc88\uc9f8 \uc904\uacfc \uc138 \ubc88\uc9f8 \uc904\uc740 \uce74\uba54\ub77c\uc758 \uc2dc\uc810\uacfc SMPL-X \uc5b4\ub178\ud14c\uc774\uc158\uc774 \uc624\ubc84\ub808\uc774\ub41c \uc774\ubbf8\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ub3d9\uc791\uacfc \uce74\uba54\ub77c \uc6c0\uc9c1\uc784\uc744 \ubcf4\uc5ec\uc8fc\uc5b4,  WHAC-A-Mole \ub370\uc774\ud130\uc14b\uc758 \ub2e4\uc591\uc131\uacfc \ud604\uc2e4\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "4 WHAC-A-Mole Dataset"}, {"figure_path": "https://arxiv.org/html/2403.12959/x5.png", "caption": "Figure 5: Visualization on in-the-wild hard cases. WHAC leverages human-camera-scene collaboration to resolve cases where motion prior alone would fail: a) Skateboarding and b) Treadmill. c) WHAC can also handle fast cases.", "description": "\uadf8\ub9bc 5\ub294 \uc2e4\uc81c \ud658\uacbd\uc5d0\uc11c \uc5b4\ub824\uc6b4 \uacbd\uc6b0\uc5d0 \ub300\ud55c \uc2dc\uac01\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  WHAC\ub294 \ubaa8\uc158 \uc815\ubcf4\ub9cc\uc73c\ub85c\ub294 \ud574\uacb0\ud558\uae30 \uc5b4\ub824\uc6b4 \uacbd\uc6b0 (\uc608: a) \uc2a4\ucf00\uc774\ud2b8\ubcf4\ub529, b) \ub7ec\ub2dd\uba38\uc2e0)\uc5d0 \ub300\ud574 \uc0ac\ub78c-\uce74\uba54\ub77c-\uc7a5\uba74 \uac04\uc758 \uc0c1\ud638 \uc791\uc6a9\uc744 \ud65c\uc6a9\ud558\uc5ec \ud574\uacb0\ud569\ub2c8\ub2e4. c)\ub294 WHAC\uc774 \ube60\ub978 \uc6c0\uc9c1\uc784\ub3c4 \ucc98\ub9ac\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.7 \uc2dc\uac01\ud654"}, {"figure_path": "https://arxiv.org/html/2403.12959/x6.png", "caption": "Figure 6: Visualization of world space results on the EMDB dataset. a1) and b1) depict camera trajectories, while a2) and b2) illustrate human trajectories. Notably, in sequence b, the human is descending stairs, and WHAC effectively captures the global trajectory, indicating a downward direction besides recovering the absolute trajectory scale in the world space. The grid size in the plots is 2m.", "description": "\uadf8\ub9bc 6\uc740 EMDB \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc138\uacc4 \uc88c\ud45c\uacc4 \uae30\ubc18\uc758 \uacb0\uacfc\ub97c \uc2dc\uac01\ud654\ud55c \uac83\uc785\ub2c8\ub2e4. a1)\uacfc b1)\uc740 \uce74\uba54\ub77c \uada4\uc801\uc744, a2)\uc640 b2)\ub294 \uc0ac\ub78c\uc758 \uada4\uc801\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 b \uc2dc\ud000\uc2a4\uc5d0\uc11c \uc0ac\ub78c\uc774 \uacc4\ub2e8\uc744 \ub0b4\ub824\uc624\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc8fc\ub294\ub370, WHAC\ub294 \uc0ac\ub78c\uc758 \ud558\uac15 \ubc29\ud5a5\uc744 \uc815\ud655\ud558\uac8c \ub098\ud0c0\ub0b4\ub294 \uc804\uc5ed \uada4\uc801\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud3ec\ucc29\ud558\uace0 \uc138\uacc4 \uc88c\ud45c\uacc4\uc5d0\uc11c \uc808\ub300\uc801\uc778 \uada4\uc801 \ud06c\uae30\ub97c \ubcf5\uc6d0\ud569\ub2c8\ub2e4. \ud50c\ub86f\uc758 \uaca9\uc790 \ud06c\uae30\ub294 2m\uc785\ub2c8\ub2e4.", "section": "5.7 \uc2dc\uac01\ud654"}, {"figure_path": "https://arxiv.org/html/2403.12959/x7.png", "caption": "Figure 7: Visualization of camera space results on WHAC-A-Mole dataset. Each sample comprises two rows: the first row displays the original input frames from the sequence, while the second row overlays the SMPL-X results. This visualization showcases WHAC\u2019s performance on challenging scenes, including sequences with severe occlusions, intricate human interactions, and dynamic dancing poses.", "description": "\uadf8\ub9bc 7\uc740 WHAC-A-Mole \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uce74\uba54\ub77c \uacf5\uac04 \uacb0\uacfc\ub97c \uc2dc\uac01\ud654\ud55c \uac83\uc785\ub2c8\ub2e4. \uac01 \uc0d8\ud50c\uc740 \ub450 \ud589\uc73c\ub85c \uad6c\uc131\ub418\ub294\ub370, \uccab \ubc88\uc9f8 \ud589\uc740 \uc2dc\ud000\uc2a4\uc758 \uc6d0\ubcf8 \uc785\ub825 \ud504\ub808\uc784\uc744, \ub450 \ubc88\uc9f8 \ud589\uc740 SMPL-X \uacb0\uacfc\ub97c \uacb9\uccd0\uc11c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uc2dc\uac01\ud654\ub294 \uc2ec\uac01\ud55c \ud3d0\uc0c9, \ubcf5\uc7a1\ud55c \uc778\uac04 \uc0c1\ud638 \uc791\uc6a9, \uc5ed\ub3d9\uc801\uc778 \ucda4 \ub3d9\uc791 \ub4f1\uc758 \uc5b4\ub824\uc6b4 \uc7a5\uba74\uc5d0\uc11c WHAC\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc2dc\ub098\ub9ac\uc624(\uc2ec\uac01\ud55c \ud3d0\uc0c9, \ubcf5\uc7a1\ud55c \uc0c1\ud638\uc791\uc6a9, \uc5ed\ub3d9\uc801\uc778 \ucda4 \ub3d9\uc791 \ud3ec\ud568)\uc5d0\uc11c WHAC\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc608\uc2dc\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "5.7 \uc2dc\uac01\ud654"}, {"figure_path": "https://arxiv.org/html/2403.12959/x8.png", "caption": "Figure 8: Illustration of MotionVelocimeter module. The inputs are canonicalized 3D joints regressed from SMPL-X meshes, and the outputs are root velocities in the canonical space.", "description": "\uadf8\ub9bc 8\uc740 \ub17c\ubb38\uc758 3.3\uc808 'Methodology'\uc5d0 \uc18d\ud55c 'MotionVelocimeter' \ubaa8\ub4c8\uc758 \uad6c\uc870\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. SMPL-X \uba54\uc2dc\uc5d0\uc11c \ud68c\uadc0\ub41c \uc815\uaddc\ud654\ub41c 3D \uad00\uc808\uc744 \uc785\ub825\ubc1b\uc544 \uc815\uaddc\ud654\ub41c \uacf5\uac04\uc5d0\uc11c \ub8e8\ud2b8 \uc18d\ub3c4\ub97c \ucd9c\ub825\ud569\ub2c8\ub2e4.  \uc774 \ubaa8\ub4c8\uc740 \uce74\uba54\ub77c \uc6c0\uc9c1\uc784\uc758 \uc2a4\ucf00\uc77c\uc744 \ubcf5\uad6c\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4. 2D \ud0a4\ud3ec\uc778\ud2b8\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 WHAM\uacfc \ub2ec\ub9ac 3D \uad00\uc808 \uc815\ubcf4\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc138\uacc4 \uc88c\ud45c\uacc4\uc5d0\uc11c\uc758 \uc18d\ub3c4 \ucd94\uc815\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uace0 \ubcf5\uc7a1\ud55c \uc6c0\uc9c1\uc784 \ud328\ud134\uc744 \ub354\uc6b1 \ud3ec\uad04\uc801\uc73c\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "3 Methodology"}]