---
title: "Hallucinations Can Improve Large Language Models in Drug Discovery"
summary: "약물 발견에서 LLM의 환각(hallucination)이 성능 향상에 기여한다는 놀라운 연구 결과!"
categories: ["AI Generated", "🤗 Daily Papers"]
tags: ["Natural Language Processing", "Large Language Models", "🏢 Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI)",]
showSummary: true
date: 2025-01-23
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2501.13824 {{< /keyword >}}
{{< keyword icon="writer" >}} Shuzhou Yuan et el. {{< /keyword >}}
 
{{< keyword >}} 🤗 2025-01-24 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2501.13824" target="_self" >}}
↗ arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2501.13824" target="_self" >}}
↗ Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/hallucinations-can-improve-large-language" target="_self" >}}
↗ Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

대규모 언어 모델(LLM)은 과학 연구, 특히 약물 발견 분야에서 점점 더 중요한 도구가 되고 있습니다. 하지만 LLM은 종종 사실과 다른 정보를 생성하는 '환각(hallucination)' 문제를 겪습니다.  일반적으로는 환각을 줄이는 연구가 주를 이루지만, 이 논문은 약물 발견과 같은 창의성이 중요한 분야에서는 환각이 오히려 도움이 될 수 있다는 가설을 세웠습니다.

본 연구는 다양한 LLM을 사용하여 분자를 자연어로 설명하고, 이 설명을 프롬프트에 추가하여 약물 발견 관련 다섯 가지 분류 작업을 수행했습니다. 그 결과, 환각이 포함된 프롬프트를 사용했을 때 LLM의 성능이 향상되었다는 것을 확인했습니다.  특히 Llama-3.1-8B 모델은 환각이 포함된 프롬프트에서 기준선 대비 18.35%의 ROC-AUC 향상을 보였습니다.  GPT-40이 생성한 환각이 모델 간 일관성 있는 성능 향상을 가져왔으며, 중국어 환각이 가장 큰 성능 향상을 보였습니다.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} LLM의 환각(hallucination)이 약물 발견 과제에서 예상외로 성능 향상을 가져온다는 것을 발견했습니다. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} 모델 크기가 커질수록 환각의 영향이 커지고, 특히 GPT-40가 생성한 환각이 가장 일관된 성능 향상을 가져왔습니다. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} 다양한 언어로 생성된 환각을 비교 분석한 결과, 중국어 환각이 가장 큰 성능 향상을 보였습니다. 이는 LLM이 사전 훈련되지 않은 언어를 활용할 수 있는 가능성을 보여줍니다 {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
이 논문은 **약물 발견 분야에서 대규모 언어 모델(LLM)의 성능을 향상시키는 데 있어 환각(hallucination)의 잠재력**을 보여줌으로써 **LLM 기반 약물 발견 연구에 새로운 관점**을 제시합니다.  **환각이 창의성을 증진시킬 수 있다는 새로운 가능성**을 제시하며, **LLM을 활용한 과학적 발견의 새로운 길**을 열 수 있습니다.  이 연구는 다양한 LLM과 약물 발견 과제에 대한 실험을 통해 이러한 가능성을 입증하고, 모델 크기와 생성 온도, 사용 언어와 같은 요인들의 영향을 분석합니다.  이는 **LLM 기반 약물 발견 연구의 새로운 방향**을 제시하고 **미래 연구를 위한 중요한 토대**가 될 것입니다.

------
#### Visual Insights



![](https://arxiv.org/html/2501.13824/extracted/6151573/overall_HHM_score.png)

> 🔼 그림 1은 HHEM-2.1-Open 모델을 사용하여 평가된 사실 일관성 점수를 보여줍니다. 이 모델은 MolT5가 생성한 참조 설명에 대한 환각의 정도를 측정합니다. 점수는 본 연구에서 사용된 5개의 데이터셋에 대한 평균값이며(5.1절 참조), ChemLLM을 제외한 모든 LLM은 10% 미만의 점수를 기록하여 분자의 자연어 설명 생성 시 높은 수준의 환각이 있음을 나타냅니다.
> <details>
> <summary>read the caption</summary>
> Figure 1: Factual consistency scores evaluated using the HHEM-2.1-Open Model, which measures the degree of hallucination relative to the reference descriptions generated by MolT5. The scores represent the average across five datasets used in this work (see §5.1). With the exception of ChemLLM, all other LLMs achieve scores below 10%, indicating a high level of hallucination when generating natural language descriptions of molecules.
> </details>





{{< table-caption >}}
| Model | **[Description]** | **HIV** | **BBBP** | **Clintox** | **SIDER** | **Tox21** | **Avg** | **ΔSMILES** | **ΔMolT5** |
|---|---|---|---|---|---|---|---|---|---| 
| **Llama-3-8B** | SMILES | 67.78 | 53.08 | 63.04 | 61.79 | 60.34 | **61.21** | - | 6.53 |
|  | MolT5 | 47.65 | 59.65 | 43.20 | 61.14 | 61.73 | 54.68 | -6.53 | - |
|  | Ministral | 55.09 | 61.13 | 63.84 | 57.56 | 55.41 | 58.61 | -2.60 | 3.93 |
| **Llama-3.1-8B** | SMILES | 38.10 | 37.56 | 35.30 | 52.70 | 44.89 | 41.71 | - | -4.57 |
|  | MolT5 | 43.04 | 45.30 | 39.28 | 52.06 | 51.72 | 46.28 | 4.57 | - |
|  | GPT-3.5 | 64.66 | 47.20 | **73.19** | 55.57 | 59.70 | 60.07 | **18.35** | **13.79** |
| **Ministral-8B** | SMILES | 59.35 | 48.87 | 60.19 | 57.27 | 57.42 | 56.62 | - | 3.53 |
|  | MolT5 | 44.54 | 44.10 | 53.95 | 63.83 | 59.02 | 53.09 | -3.53 | - |
|  | GPT-4o | 51.66 | **64.94** | 58.73 | 60.54 | 61.07 | 59.39 | 2.77 | 6.30 |
| **Falcon3-Mamba-7B** | SMILES | 40.64 | 48.33 | 31.72 | 52.53 | 47.45 | 44.13 | - | 0.21 |
|  | MolT5 | 49.02 | 47.92 | 18.58 | 55.84 | 48.27 | 43.93 | -0.21 | - |
|  | GPT-4o | 51.59 | 55.73 | 53.55 | 56.03 | 51.54 | 53.69 | 9.55 | 9.76 |
| **ChemLLM-7B** | SMILES | 55.54 | 38.69 | 24.02 | 47.85 | **65.59** | 46.34 | - | -1.93 |
|  | MolT5 | 50.33 | 41.01 | 35.43 | 53.06 | 61.50 | 48.27 | 1.93 | - |
|  | GPT-4o | 52.68 | 55.73 | 40.74 | 44.59 | 62.63 | 51.28 | 4.94 | 3.01 |
| **GPT-3.5** | SMILES | 61.32 | 28.94 | 63.90 | 53.77 | 61.92 | 53.97 | - | -2.56 |
|  | MolT5 | 49.34 | 52.64 | 64.30 | 56.00 | 60.39 | 56.53 | 2.56 | - |
|  | GPT-4o | 49.67 | 34.61 | 60.83 | 62.41 | 65.36 | 54.57 | 0.60 | -1.96 |
| **GPT-4o** | SMILES | 47.19 | 46.79 | 40.00 | 52.30 | 63.25 | 49.91 | - | 2.10 |
|  | MolT5 | 41.73 | 40.57 | 38.09 | **65.22** | 53.42 | 47.80 | -2.10 | - |
|  | GPT-4o | 53.30 | 52.73 | 57.12 | 47.82 | 65.34 | 55.26 | 5.35 | 7.46 |{{< /table-caption >}}

> 🔼 표 1은 7가지 지시 조정된 거대 언어 모델(LLM)에 대한 주요 결과(ROC-AUC 점수)를 보여줍니다. 각 LLM에 대해 성능을 가장 크게 향상시킨 환각만 포함되어 있습니다. [설명] 열은 기준 SMILES, MolT5 또는 LLM에서 생성된 환각 중 어떤 설명을 사용했는지 나타냅니다. ΔSMILES 및 ΔMolT5는 동일한 LLM을 사용하여 기준 SMILES 및 MolT5와 비교한 평균 ROC-AUC 점수 차이를 나타냅니다. 모든 LLM 중 가장 좋은 결과는 굵게 표시되어 있습니다.
> <details>
> <summary>read the caption</summary>
> Table 1: The main results (ROC-AUC &) for the LLMs. Only the hallucinations that result in the most improvement for each LLM are included in the table. [D⁢e⁢s⁢c⁢r⁢i⁢p⁢t⁢i⁢o⁢n]delimited-[]𝐷𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑜𝑛[Description][ italic_D italic_e italic_s italic_c italic_r italic_i italic_p italic_t italic_i italic_o italic_n ] represents the description from the baseline SMILES, MolT5, or hallucinations generated by the LLMs. ΔΔ\Deltaroman_ΔSMILES and ΔΔ\Deltaroman_ΔMolT5 denote the difference in average ROC-AUC scores compared to the baselines SMILES and MolT5, respectively, using the same LLM. We highlight the best results across all the LLMs in bold.
> </details>





### In-depth insights


#### Hallucination Benefits
이 연구는 **대규모 언어 모델(LLM)의 환각(hallucination)이 약물 발견 분야에서 예상치 못한 이점을 제공할 수 있다는 흥미로운 주장**을 제시합니다.  LLM이 생성하는 환각은 사실과 다를 수 있지만, **창의성과 혁신을 촉진**하는 데 기여할 수 있다는 것입니다.  즉, 완벽한 정확성보다는 **새로운 아이디어나 접근 방식을 제시**하는 데 그 가치가 있다는 것입니다.  연구는 여러 개의 LLM과 약물 분류 작업에 대한 실험을 통해 이러한 가설을 검증하고,  **환각 정보를 포함한 프롬프트가 환각이 없는 프롬프트보다 더 나은 성능**을 보임을 확인했습니다.  특히, GPT-40이 생성한 환각이 여러 모델에서 가장 일관된 성능 향상을 가져왔다는 점은 주목할 만합니다.  하지만, **모델 크기와 생성 온도, 그리고 언어의 영향**도 고려되어야 하며, 이러한 요인들이 환각의 효과에 영향을 미친다는 것을 보여주는 추가적인 분석이 필요합니다.  **환각의 이점을 극대화**하려면, LLM이 생성하는 환각의 특성을 이해하고 이를 효과적으로 활용하는 방법에 대한 추가 연구가 필요합니다.

#### LLM Drug Discovery
LLM을 활용한 신약 개발 분야는 **막대한 잠재력**을 지니고 있습니다. 방대한 화합물 데이터베이스를 분석하고, 분자 구조와 특성 간의 관계를 파악하며, 새로운 약물 후보 물질을 예측하는 데 LLM이 기여할 수 있습니다. 하지만, **환각(hallucination)** 문제는 LLM의 정확성과 신뢰도에 대한 우려를 제기합니다. 본 논문은 이러한 환각 현상이 오히려 **창의적인 신약 후보 물질 발견**에 도움이 될 수 있다는 흥미로운 가설을 제시하며, 이를 실험적으로 검증합니다.  **다양한 LLM과 약물 분류 작업**을 통해 환각이 포함된 입력이 성능 향상에 기여함을 보였습니다.  특히, GPT-4가 생성한 환각이 여러 모델에서 가장 일관된 성능 향상을 가져왔다는 점은 주목할 만합니다.  **모델 크기와 언어**도 성능에 영향을 미치는 요소로 제시되었으며, 추가적인 실험적 분석과 사례 연구를 통해 이러한 현상의 원인을 탐구하고 있습니다.  하지만, **환각 현상의 긍정적 효과에 대한 추가 연구**가 필요하며, 실제 신약 개발 과정에 적용하기 위해서는  LLM의 신뢰성과 정확성을 더욱 높이는 연구가 지속되어야 할 것입니다.

#### Prompt Engineering
프롬프트 엔지니어링은 **대규모 언어 모델(LLM)**의 성능을 극대화하기 위한 중요한 전략입니다.  본 논문에서는 약물 발견 분야에서 LLM의 활용에 초점을 맞추고, **잘못된 정보(hallucination)을 포함하는 프롬프트가 모델의 창의성과 성능 향상에 기여할 수 있다는 흥미로운 가설**을 제시합니다.  이는 기존의 오류 최소화 중심 접근 방식과는 대조적입니다.  **프롬프트에 hallucination을 포함시키는 방법**은 LLM이 분자 구조를 자연어로 기술하고, 이를 프롬프트의 일부로 활용하는 방식으로 제시됩니다.  이는 **모델이 분자 구조의 세부 정보뿐 아니라, 잠재적인 응용 분야나 특징에 대한 고차원적인 정보**까지 처리하도록 유도하는 데 효과적일 수 있습니다.  본 연구는 **다양한 LLM과 약물 분류 작업**을 통해 이 가설을 검증하고, 프롬프트 엔지니어링의 중요성을 강조하는 실험 결과를 제시합니다.  **모델 크기, 생성 온도, 언어** 등 여러 요인이 성능에 미치는 영향을 분석하고, **주의 메커니즘 분석**을 통한 심층적 이해를 제공합니다.

#### Model Size Effects
본 논문에서 모델 크기 효과에 대한 분석은 **모델의 크기가 클수록 환각이 성능 향상에 미치는 영향이 커진다**는 것을 보여줍니다.  **특히 80억 매개변수 모델에서 효과가 정점에 달하고, 그 이상의 크기에서는 성능 향상폭이 감소**하는 경향이 나타납니다. 이는 모델 크기가 클수록, 더 많은 데이터와 복잡한 패턴을 학습하여 환각 정보를 효과적으로 활용할 수 있기 때문일 것입니다.  하지만, **단순히 모델 크기만 키운다고 해서 성능 향상이 무한정 이루어지는 것은 아니며**,  특정 크기를 넘어서면 추가적인 성능 개선에는 한계가 있음을 시사합니다.  따라서, **최적의 모델 크기는 과제 및 데이터 특성에 따라 달라질 수 있으며**, 이에 대한 추가적인 연구가 필요합니다. 이러한 결과는 **LLM 기반 약물 발견 분야에서 모델 크기의 최적화 전략을 수립하는 데 중요한 시사점**을 제공합니다.

#### Future Research
본 논문은 환각(hallucination)이 약물 발견 분야에서 거대 언어 모델(LLM)의 성능을 향상시킬 수 있다는 흥미로운 가설을 제시하고 검증합니다.  **미래 연구는 여러 가지 측면에서 진행될 수 있습니다.** 첫째, 다양한 LLM 아키텍처와 크기의 모델에 대한 환각의 영향을 더욱 광범위하게 조사하여 **모델 아키텍처와 환각 간의 상관관계**를 규명해야 합니다. 둘째, **다양한 약물 발견 작업** (예: 약물-표적 상호작용 예측, 독성 예측 등)에 대한 환각의 일반화 가능성을 심층적으로 조사할 필요가 있습니다. 셋째, **환각의 품질**을 정량화하고, 환각이 모델 성능에 미치는 영향을 더 잘 이해하기 위해 새로운 평가 지표를 개발하는 것이 중요합니다.  마지막으로, **환각을 제어하고 조절하는 방법**을 연구하여 모델의 신뢰성과 정확성을 유지하면서 환각의 이점을 최대한 활용하는 방법을 찾아야 합니다. 이러한 연구를 통해 환각이라는 특징을 약물 발견 분야에 효과적으로 적용하는 방법을 파악하고, LLM 기반 약물 발견 기술을 한층 더 발전시킬 수 있을 것입니다.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2501.13824/extracted/6151573/method.png)

> 🔼 그림 2는 HIV 데이터셋의 분자 예시를 사용하여 평가 방법을 보여줍니다.  LLM이 SMILES 문자열을 기반으로 분자의 텍스트 설명을 생성하는 과정(1단계)과, 생성된 텍스트(환각 포함)를 프롬프트에 추가하여 LLM이 분자의 특정 속성을 예측하도록 하는 과정(2단계)을 나타냅니다.  LLM의 성능을 평가하기 위해 답변은 '예' 또는 '아니오'로 제한됩니다.  입력과 관련 없는 명백한 환각 부분은 색상으로 강조 표시되어 있습니다. 이 그림은 LLM이 SMILES 문자열만으로 분자 특성을 예측하는 것보다 환각이 포함된 텍스트를 함께 사용하여 예측 정확도가 향상되는지 확인하는 실험의 과정을 시각적으로 설명합니다.
> <details>
> <summary>read the caption</summary>
> Figure 2: Illustration of the evaluation method with an example molecule from the HIV dataset: 1. We use LLMs to generate a textual description of the molecule based on its SMILES string. 2. The generated text, which contains hallucinations, is added to the prompt, and the LLM is tasked with predicting the specific property of the molecule. The answer is constrained to “Yes” or “No” to evaluate the LLM’s performance. We highlight obvious hallucinations that are unrelated to the input using colors.
> </details>



![](https://arxiv.org/html/2501.13824/extracted/6151573/overall_hallu_improvement.png)

> 🔼 이 그림은 7가지 다른 대규모 언어 모델(LLM)에서 다양한 모델이 생성한 환각(hallucination)을 사용했을 때, 기준 모델(baseline)과 비교하여 성능이 향상된 정도를 보여줍니다.  x축은 환각을 생성한 모델을 나타내고, y축은 평균 성능 향상률(%)을 나타냅니다.  각 막대는 특정 환각 생성 모델을 사용했을 때 7개의 LLM에서 평균적으로 성능이 얼마나 향상되었는지 보여줍니다.  이를 통해 어떤 모델이 생성한 환각이 다른 LLM의 성능 향상에 가장 효과적인지 비교 분석할 수 있습니다.
> <details>
> <summary>read the caption</summary>
> Figure 3: Overall average improvement across seven LLMs using hallucinations generated by different models compared to the baselines. The x-axis indicates the source model for the generated hallucinations.
> </details>



![](https://arxiv.org/html/2501.13824/extracted/6151573/model_size.png)

> 🔼 그림 4는 서로 다른 크기의 Llama-3 모델에 대한 평균 성능 향상을 보여줍니다.  x축은 모델 크기(1B, 3B, 8B, 70B)를 나타내고, y축은 SMILES 기준선과 MolT5 기준선에 대한 평균 성능 향상 백분율을 나타냅니다.  각 모델 크기에 대해 여러 환각 텍스트를 사용하여 평균 성능 향상을 계산했습니다. 이 그래프는 모델 크기가 증가함에 따라 환각이 모델 성능에 미치는 영향이 커짐을 보여줍니다. 특히, 8B 모델은 SMILES 기준선과 비교하여 상당한 성능 향상을 보이는 반면, 70B 모델은 8B 모델과 비교하여 성능 향상이 크게 증가하지는 않습니다. 이는 모델 크기가 특정 크기 이상이 되면 환각 효과가 일정 수준에 도달할 수 있음을 시사합니다.
> <details>
> <summary>read the caption</summary>
> Figure 4: Average improvement for Llama-3 with different model size.
> </details>



![](https://arxiv.org/html/2501.13824/extracted/6151573/temperature.png)

> 🔼 그림 5는 다섯 개의 데이터 세트에 걸쳐 생성된 텍스트의 평균 성능과 환각 점수를 다양한 온도 설정에서 보여줍니다.  다양한 온도 설정에서 생성된 텍스트의 정확성(factual consistency)을 평가하기 위해 HHM-2.1-Open 모델을 사용했습니다.  x축은 온도 값을, y축은 평균 ROC-AUC 점수와 환각 점수를 나타냅니다.  이 그림은 온도가 생성된 텍스트의 정확성과 모델 성능에 미치는 영향을 보여주는 실험 결과를 시각적으로 표현합니다.
> <details>
> <summary>read the caption</summary>
> Figure 5: Average performance and hallucination scores of the generated text at different temperature settings across five datasets.
> </details>



![](https://arxiv.org/html/2501.13824/extracted/6151573/language.png)

> 🔼 그림 6은 Llama-3.1-8B 모델을 사용하여 모든 데이터셋에서 6가지 언어의 환각에 대한 성능을 보여줍니다.  각 언어(영어, 중국어, 독일어, 프랑스어, 일본어, 스페인어)로 생성된 환각을 사용하여 모델의 성능을 평가하고, ROC-AUC 점수를 사용하여 측정했습니다. 이 그래프는 다양한 언어로 생성된 환각이 Llama-3.1-8B 모델의 약물 발견 작업 성능에 미치는 영향을 비교 분석한 것입니다. 각 데이터셋에서 가장 좋은 성능을 보인 언어를 확인하고,  모델이 특정 언어의 환각에 더 민감하게 반응하는지 여부를 파악하는 데 도움이 됩니다.
> <details>
> <summary>read the caption</summary>
> Figure 6: Performance of hallucinations in six languages across all datasets using Llama-3.1-8B.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Model | Full-name | Link |
|---|---|---|
| Llama-3-8B | meta-llama/Meta-Llama-3-8B-Instruct | https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct |
| Llama-3.1-8B | meta-llama/Llama-3.1-8B-Instruct | https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct |
| Ministral-8B | mistralai/Ministral-8B-Instruct-2410 | https://huggingface.co/mistralai/Ministral-8B-Instruct-2410 |
| Falcon3-Mamba-7B | tiiuae/Falcon3-Mamba-7B-Instruct | https://huggingface.co/tiiuae/Falcon3-Mamba-7B-Instruct |
| ChemLLM-7B | AI4Chem/ChemLLM-7B-Chat-1_5-DPO | https://huggingface.co/AI4Chem/ChemLLM-7B-Chat-1_5-DPO |
| GPT-3.5 | gpt-3.5-turbo | - |
| GPT-4o | gpt-4o-2024-08-06 | - |{{< /table-caption >}}
> 🔼 표 2는 논문에서 평가에 사용된 언어 모델들의 세부 정보를 보여줍니다. 각 모델의 전체 이름과 해당 모델의 HuggingFace 페이지 링크가 포함되어 있습니다. 이 표는 사용된 모델의 버전과 접근 방법을 명확히 하여 재현성을 높이는 데 도움이 됩니다.
> <details>
> <summary>read the caption</summary>
> Table 2: Details of the evaluated models, including their full names and links to their respective HuggingFace pages.
> </details>

{{< table-caption >}}
| Dataset | Number | Positive Label |
|---|---|---|
| HIV | 4113 | 175 |
| BBBP | 205 | 172 |
| Clintox | 148 | 11 |
| SIDER | 143 | 74 |
| Tox21 | 783 | 108 |{{< /table-caption >}}
> 🔼 이 표는 논문에서 사용된 데이터셋의 요약 정보를 보여줍니다. 각 데이터셋의 샘플 수와 양성 레이블의 개수를 포함합니다.  HIV, BBBP, Clintox, SIDER, Tox21 등 다섯 가지 데이터셋에 대한 정보가 요약되어 있습니다. 이 정보는 각 데이터셋의 크기와 양성/음성 샘플의 비율을 이해하는 데 도움이 됩니다.
> <details>
> <summary>read the caption</summary>
> Table 3: Summary of the datasets used, including the number of samples and the count of positive labels in each dataset.
> </details>

{{< table-caption >}}
| Model | HIV | BBBP | Clintox | SIDER | Tox21 | Avg |
|---|---|---|---|---|---|---|
| Llama-3.1-8B | 7.12 | 7.76 | 8.67 | 9.45 | 6.86 | 7.97 |
| Llama-3-8B | 7.15 | 8.04 | 7.24 | 7.57 | 7.11 | 7.42 |
| Ministral-8B | 14.45 | 13.47 | 14.23 | 13.17 | 12.56 | 13.58 |
| Falcon3-Mamba-7B | 8.74 | 9.05 | 8.92 | 11.13 | 9.26 | 9.42 |
| ChemLLM-7B | 20.48 | 17.75 | 23.36 | 22.20 | 20.66 | 20.89 |
| GPT-3.5 | 8.62 | 8.35 | 6.85 | 7.20 | 6.90 | 7.58 |
| GPT-4o | 8.51 | 7.67 | 8.51 | 7.28 | 7.44 | 7.88 |{{< /table-caption >}}
> 🔼 표 4는 다섯 개의 데이터셋에 걸쳐 다양한 대규모 언어 모델(LLM)에 대한 HHM-2.1 점수(%)를 보여줍니다. 모든 모델은 온도, 최대 새 토큰 등 동일한 하이퍼파라미터를 사용합니다.  이 표는 각 LLM이 분자를 자연어로 설명하는 능력의 정확성을 측정한 결과를 보여줍니다.  점수가 높을수록 모델이 생성한 설명이 참조 설명과 일치하는 정도가 높다는 것을 의미합니다. 이는 LLM이 화학적 용어와 구조를 얼마나 잘 이해하고 있는지 평가하는 데 도움이 됩니다.
> <details>
> <summary>read the caption</summary>
> Table 4: The HHM-2.1 score % for LLMs across five datasets, all the models are using the same hyperparameters, e.g.: the temperature, max new tokens.
> </details>

{{< table-caption >}}
 | **[Description]** | **HIV** | **BBBP** | **Clintox** | **SIDER** | **Tox21** | **Avg** | **Δsmiles** | **ΔMolT5** |
|---|---|---|---|---|---|---|---|---|
| **Llama-3-8B** |  |  |  |  |  |  |  |  |
| `SMILES` | 67.78 | 53.08 | 63.04 | 61.79 | 60.34 | 61.21 | 0.00 | 6.53 |
| `MolT5` | 47.65 | 59.65 | 43.20 | 61.14 | 61.73 | 54.68 | -6.53 | 0.00 |
| **Llama-3** |  |  |  |  |  |  |  |  |
| `Llama-3` | 53.13 | 58.21 | 57.13 | 57.07 | 59.75 | 57.06 | -4.15 | 2.38 |
| **Llama-3.1** |  |  |  |  |  |  |  |  |
| `Llama-3.1` | 57.60 | 51.02 | 39.75 | 60.63 | 59.70 | 53.74 | -7.46 | -0.93 |
| **Ministral** |  |  |  |  |  |  |  |  |
| `Ministral` | 55.09 | 61.13 | 63.84 | 57.56 | 55.41 | 58.61 | -2.60 | 3.93 |
| **Falcon3** |  |  |  |  |  |  |  |  |
| `Falcon3` | 54.02 | 51.73 | 43.00 | 47.06 | 48.91 | 48.94 | -12.26 | -5.73 |
| **ChemLLM** |  |  |  |  |  |  |  |  |
| `ChemLLM` | 46.83 | 60.22 | 52.36 | 58.50 | 50.83 | 53.75 | -7.46 | -0.93 |
| **GPT-3.5** |  |  |  |  |  |  |  |  |
| `GPT-3.5` | 58.45 | 46.52 | 57.13 | 56.15 | 66.00 | 56.85 | -4.36 | 2.18 |
| **GPT-4o** |  |  |  |  |  |  |  |  |
| `GPT-4o` | 45.97 | 53.72 | 54.81 | 45.86 | 60.90 | 52.25 | -8.96 | -2.42 |
| **Llama-3.1-8B** |  |  |  |  |  |  |  |  |
| `SMILES` | 38.10 | 37.56 | 35.30 | 52.70 | 44.89 | 41.71 | 0.00 | -4.57 |
| `MolT5` | 43.04 | 45.30 | 39.28 | 52.06 | 51.72 | 46.28 | 4.57 | 0.00 |
| `Llama-3` | 56.80 | 47.71 | 68.08 | 54.21 | 60.75 | 57.51 | 15.80 | 11.23 |
| `Llama-3.1` | 50.92 | 42.11 | 49.97 | 38.11 | 51.74 | 46.57 | 4.86 | 0.29 |
| `Ministral` | 59.73 | 50.18 | 70.01 | 51.65 | 55.59 | 57.43 | 15.72 | 11.15 |
| `Falcon3` | 57.92 | 51.11 | 53.22 | 42.38 | 47.57 | 50.44 | 8.73 | 4.16 |
| `ChemLLM` | 50.83 | 52.18 | 47.45 | 53.17 | 49.84 | 50.69 | 8.98 | 4.41 |
| `GPT-3.5` | 64.66 | 47.20 | 73.19 | 55.57 | 59.70 | 60.07 | 18.35 | 13.79 |
| `GPT-4o` | 56.64 | 53.37 | 55.28 | 52.60 | 61.65 | 55.91 | 14.20 | 9.63 |
| **Ministral-8B** |  |  |  |  |  |  |  |  |
| `SMILES` | 59.35 | 48.87 | 60.19 | 57.27 | 57.42 | 56.62 | 0.00 | 3.53 |
| `MolT5` | 44.54 | 44.10 | 53.95 | 63.83 | 59.02 | 53.09 | -3.53 | 0.00 |
| `Llama-3` | 52.91 | 31.27 | 65.49 | 50.45 | 55.81 | 51.19 | -5.43 | -1.90 |
| `Llama-3.1` | 56.06 | 46.62 | 36.23 | 55.95 | 58.33 | 50.64 | -5.98 | -2.45 |
| `Ministral` | 60.04 | 46.42 | 55.34 | 52.82 | 53.58 | 53.64 | -2.98 | 0.56 |
| `Falcon3` | 55.37 | 50.16 | 53.62 | 47.14 | 58.30 | 52.92 | -3.70 | -0.17 |
| `ChemLLM` | 46.35 | 48.66 | 62.11 | 58.97 | 55.36 | 54.29 | -2.33 | 1.20 |
| `GPT-3.5` | 53.23 | 36.89 | 46.85 | 51.63 | 59.86 | 49.69 | -6.93 | -3.40 |
| `GPT-4o` | 51.66 | 64.94 | 58.73 | 60.54 | 61.07 | 59.39 | 2.77 | 6.30 |
| **Falcon3-Mamba-7B** |  |  |  |  |  |  |  |  |
| `SMILES` | 40.64 | 48.33 | 31.72 | 52.53 | 47.45 | 44.13 | 0.00 | 0.21 |
| `MolT5` | 49.02 | 47.92 | 18.58 | 55.84 | 48.27 | 43.93 | -0.21 | 0.00 |
| `Llama-3` | 47.84 | 53.40 | 50.76 | 51.31 | 48.44 | 50.35 | 6.22 | 6.42 |
| `Llama-3.1` | 48.36 | 60.15 | 34.84 | 57.60 | 54.26 | 51.04 | 6.91 | 7.12 |
| `Ministral` | 47.32 | 47.82 | 27.94 | 51.90 | 47.54 | 44.50 | 0.37 | 0.58 |
| `Falcon3` | 44.59 | 57.05 | 39.42 | 50.10 | 46.64 | 47.56 | 3.43 | 3.63 |
| `ChemLLM` | 53.46 | 61.82 | 45.12 | 53.56 | 43.43 | 51.48 | 7.35 | 7.55 |
| `GPT-3.5` | 51.63 | 48.47 | 45.26 | 50.98 | 46.73 | 48.61 | 4.48 | 4.69 |
| `GPT-4o` | 51.59 | 55.73 | 53.55 | 56.03 | 51.54 | 53.69 | 9.55 | 9.76 |
| **ChemLLM-7B** |  |  |  |  |  |  |  |  |
| `SMILES` | 55.54 | 38.69 | 24.02 | 47.85 | 65.59 | 46.34 | 0.00 | -1.93 |
| `MolT5` | 50.33 | 41.01 | 35.43 | 53.06 | 61.50 | 48.27 | 1.93 | 0.00 |
| `Llama-3` | 56.29 | 45.68 | 47.51 | 44.95 | 51.59 | 49.20 | 2.87 | 0.94 |
| `Llama-3.1` | 57.17 | 43.02 | 28.33 | 43.83 | 56.96 | 45.86 | -0.47 | -2.40 |
| `Ministral` | 61.35 | 37.70 | 39.95 | 37.76 | 51.64 | 45.68 | -0.66 | -2.59 |
| `Falcon3` | 57.52 | 39.76 | 29.46 | 44.71 | 50.18 | 44.33 | -2.01 | -3.94 |
| `ChemLLM` | 45.37 | 41.83 | 42.80 | 53.15 | 46.60 | 45.95 | -0.39 | -2.32 |
| `GPT-3.5` | 57.48 | 38.16 | 51.76 | 41.72 | 59.37 | 49.70 | 3.36 | 1.43 |
| `GPT-4o` | 52.68 | 55.73 | 40.74 | 44.59 | 62.63 | 51.28 | 4.94 | 3.01 |
| **GPT-3.5** |  |  |  |  |  |  |  |  |
| `SMILES` | 61.32 | 28.94 | 63.90 | 53.77 | 61.92 | 53.97 | 0.00 | -2.56 |
| `MolT5` | 49.34 | 52.64 | 64.30 | 56.00 | 60.39 | 56.53 | 2.56 | 0.00 |
| `Llama-3` | 52.55 | 30.00 | 38.27 | 52.22 | 55.71 | 45.75 | -8.22 | -10.78 |
| `Llama-3.1` | 55.90 | 39.31 | 41.83 | 57.16 | 59.60 | 50.76 | -3.21 | -5.77 |
| `Ministral` | 55.84 | 29.63 | 39.89 | 56.61 | 56.31 | 47.66 | -6.31 | -8.88 |
| `Falcon3` | 52.11 | 27.96 | 54.29 | 48.69 | 59.10 | 48.43 | -5.54 | -8.10 |
| `ChemLLM` | 50.27 | 45.97 | 63.03 | 53.77 | 54.57 | 53.52 | -0.45 | -3.01 |
| `GPT-3.5` | 58.47 | 33.53 | 44.81 | 62.24 | 56.46 | 51.10 | -2.87 | -5.43 |
| `GPT-4o` | 49.67 | 34.61 | 60.83 | 62.41 | 65.36 | 54.57 | 0.60 | -1.96 |
| **GPT-4o** |  |  |  |  |  |  |  |  |
| `SMILES` | 47.19 | 46.79 | 40.00 | 52.30 | 63.25 | 49.91 | 0.00 | 2.10 |
| `MolT5` | 41.73 | 40.57 | 38.09 | 65.22 | 53.42 | 47.80 | -2.10 | 0.00 |
| `Llama-3` | 48.95 | 36.23 | 52.97 | 42.73 | 62.00 | 48.58 | -1.33 | 0.77 |
| `Llama-3.1` | 53.02 | 45.57 | 54.10 | 48.71 | 64.07 | 53.09 | 3.19 | 5.29 |
| `Ministral` | 55.34 | 40.13 | 49.46 | 42.44 | 64.17 | 50.31 | 0.40 | 2.50 |
| `Falcon3` | 54.01 | 35.37 | 50.00 | 45.83 | 63.34 | 49.71 | -0.20 | 1.90 |
| `ChemLLM` | 53.00 | 54.92 | 55.59 | 45.36 | 61.15 | 54.00 | 4.10 | 6.20 |
| `GPT-3.5` | 47.51 | 40.80 | 46.03 | 47.27 | 63.82 | 49.09 | -0.82 | 1.28 |
| `GPT-4o` | 53.30 | 52.73 | 57.12 | 47.82 | 65.34 | 55.26 | 5.35 | 7.46 |{{< /table-caption >}}
> 🔼 표 5는 모든 LLMs에 대한 전체 결과(ROC-AUC %)를 보여줍니다. 여기에는 각 모델과 기준 모델에서 생성된 환각이 포함됩니다. [설명]은 환각 또는 기준선의 출처를 나타냅니다.
> <details>
> <summary>read the caption</summary>
> Table 5: Full results (ROC-AUC %) for all LLMs, including hallucinations generated by each model and baselines. [𝑫⁢𝒆⁢𝒔⁢𝒄⁢𝒓⁢𝒊⁢𝒑⁢𝒕⁢𝒊⁢𝒐⁢𝒏]delimited-[]𝑫𝒆𝒔𝒄𝒓𝒊𝒑𝒕𝒊𝒐𝒏\boldsymbol{[Description]}bold_[ bold_italic_D bold_italic_e bold_italic_s bold_italic_c bold_italic_r bold_italic_i bold_italic_p bold_italic_t bold_italic_i bold_italic_o bold_italic_n bold_] indicates the source of the hallucination or baseline used.
> </details>

{{< table-caption >}}
| **[Description]** | **HIV** | **BBBP** | **Clintox** | **SIDER** | **Tox21** | **Avg** | **Δ SMILES** | **Δ MolT5** |
|---|---|---|---|---|---|---|---|---|
| **Llama-3.2-1B** |  |  |  |  |  |  |  |  |
| `SMILES` | 44.27 | 32.31 | 19.31 | 60.79 | 46.88 | 40.71 | 0.00 | -13.77 |
| `MolT5` | 45.26 | 38.30 | 79.50 | 57.95 | 51.42 | 54.49 | 13.77 | 0.00 |
| **Llama-3** |  |  |  |  |  |  |  |  |
| `Llama-3` | 46.79 | 47.32 | 60.19 | 56.23 | 44.25 | 50.96 | 10.24 | -3.53 |
| **Llama-3.1** |  |  |  |  |  |  |  |  |
| `Llama-3.1` | 44.79 | 50.93 | 51.56 | 59.09 | 53.73 | 52.02 | 11.31 | -2.47 |
| **Ministral** |  |  |  |  |  |  |  |  |
| `Ministral` | 46.94 | 42.00 | 54.48 | 51.94 | 52.71 | 49.61 | 8.90 | -4.87 |
| **Falcon3** |  |  |  |  |  |  |  |  |
| `Falcon3` | 52.08 | 58.58 | 56.01 | 50.55 | 47.14 | 52.87 | 12.16 | -1.62 |
| **ChemLLM** |  |  |  |  |  |  |  |  |
| `ChemLLM` | 46.23 | 44.77 | 60.25 | 56.35 | 44.19 | 50.36 | 9.65 | -4.13 |
| **GPT-3.5** |  |  |  |  |  |  |  |  |
| `GPT-3.5` | 54.33 | 45.24 | 58.00 | 56.01 | 58.94 | 54.50 | 13.79 | 0.02 |
| **GPT-4o** |  |  |  |  |  |  |  |  |
| `GPT-4o` | 50.29 | 60.02 | 64.43 | 52.88 | 52.10 | 55.94 | 15.23 | 1.46 |
| **Llama-3.2-3B** |  |  |  |  |  |  |  |  |
| `SMILES` | 43.57 | 40.40 | 40.15 | 56.13 | 48.62 | 45.77 | 0.00 | -1.67 |
| `MolT5` | 48.17 | 45.77 | 51.23 | 49.47 | 42.55 | 47.44 | 1.67 | 0.00 |
| `Llama-3` | 48.52 | 32.42 | 53.55 | 52.60 | 54.09 | 48.23 | 2.46 | 0.80 |
| `Llama-3.1` | 46.99 | 40.93 | 33.78 | 53.09 | 50.48 | 45.05 | -0.72 | -2.39 |
| `Ministral` | 49.65 | 47.18 | 50.76 | 47.79 | 50.82 | 49.24 | 3.47 | 1.80 |
| `Falcon3` | 59.43 | 44.96 | 39.81 | 52.51 | 42.38 | 47.82 | 2.05 | 0.38 |
| `ChemLLM` | 44.99 | 44.75 | 46.05 | 45.73 | 47.30 | 45.76 | -0.01 | -1.68 |
| `GPT-3.5` | 53.66 | 32.61 | 58.93 | 55.70 | 47.01 | 49.58 | 3.81 | 2.14 |
| `GPT-4o` | 50.12 | 42.20 | 38.42 | 59.38 | 54.33 | 48.89 | 3.12 | 1.45 |
| **Llama-3.1-8B** |  |  |  |  |  |  |  |  |
| `SMILES` | 38.10 | 37.56 | 35.30 | 52.70 | 44.89 | 41.71 | 0.00 | -4.57 |
| `MolT5` | 43.04 | 45.30 | 39.28 | 52.06 | 51.72 | 46.28 | 4.57 | 0.00 |
| `Llama-3` | 56.80 | 47.71 | 68.08 | 54.21 | 60.75 | 57.51 | 15.80 | 11.23 |
| `Llama-3.1` | 50.92 | 42.11 | 49.97 | 38.11 | 51.74 | 46.57 | 4.86 | 0.29 |
| `Ministral` | 59.73 | 50.18 | 70.01 | 51.65 | 55.59 | 57.43 | 15.72 | 11.15 |
| `Falcon3` | 57.92 | 51.11 | 53.22 | 42.38 | 47.57 | 50.44 | 8.73 | 4.16 |
| `ChemLLM` | 50.83 | 52.18 | 47.45 | 53.17 | 49.84 | 50.69 | 8.98 | 4.41 |
| `GPT-3.5` | 64.66 | 47.20 | 73.19 | 55.57 | 59.70 | 60.07 | 18.35 | 13.79 |
| `GPT-4o` | 56.64 | 53.37 | 55.28 | 52.60 | 61.65 | 55.91 | 14.20 | 9.63 |
| **Llama-3.1-70B** |  |  |  |  |  |  |  |  |
| `SMILES` | 49.21 | 39.40 | 52.12 | 45.98 | 47.73 | 46.89 | 0.00 | -3.11 |
| `MolT5` | 42.02 | 42.86 | 61.94 | 54.66 | 48.48 | 49.99 | 3.11 | 0.00 |
| `Llama-3` | 48.34 | 47.54 | 68.08 | 45.73 | 50.95 | 52.13 | 5.24 | 2.14 |
| `Llama-3.1` | 53.67 | 43.36 | 37.79 | 56.27 | 48.69 | 47.95 | 1.07 | -2.04 |
| `Ministral` | 59.85 | 60.62 | 68.31 | 51.22 | 48.28 | 57.66 | 10.77 | 7.67 |
| `Falcon3` | 54.12 | 40.02 | 60.75 | 43.24 | 53.02 | 50.23 | 3.34 | 0.24 |
| `ChemLLM` | 46.11 | 59.21 | 57.43 | 50.18 | 48.85 | 52.35 | 5.47 | 2.36 |
| `GPT-3.5` | 52.63 | 47.72 | 54.91 | 52.76 | 44.31 | 50.47 | 3.58 | 0.47 |
| `GPT-4o` | 53.69 | 62.98 | 50.23 | 56.67 | 53.07 | 55.33 | 8.44 | 5.34 |{{< /table-caption >}}
> 🔼 표 6은 모델 크기 실험의 결과(ROC-AUC %)를 보여줍니다.  각 모델은 SMILES 문자열만을 사용한 기준(baseline)과 MolT5에 의해 생성된 참조 설명을 사용한 기준, 그리고 다양한 LLM에 의해 생성된 환각(hallucination)을 포함한 세 가지 설정으로 평가되었습니다.  [Description] 열은 환각의 출처, 즉 특정 모델에 의해 생성되었는지 또는 기준 설명에서 가져왔는지를 나타냅니다.  즉,  표는 모델 크기가 다를 때(1B, 3B, 8B, 70B)  약물 발견 작업에서 다양한 LLM의 성능에 미치는 환각의 영향을 보여줍니다.
> <details>
> <summary>read the caption</summary>
> Table 6: Full results (ROC-AUC %) of model size experiments. [𝑫⁢𝒆⁢𝒔⁢𝒄⁢𝒓⁢𝒊⁢𝒑⁢𝒕⁢𝒊⁢𝒐⁢𝒏]delimited-[]𝑫𝒆𝒔𝒄𝒓𝒊𝒑𝒕𝒊𝒐𝒏\boldsymbol{[Description]}bold_[ bold_italic_D bold_italic_e bold_italic_s bold_italic_c bold_italic_r bold_italic_i bold_italic_p bold_italic_t bold_italic_i bold_italic_o bold_italic_n bold_] indicates the hallucination source, whether generated by a specific model or from baseline descriptions.
> </details>

{{< table-caption >}}
| Model | HIV | BBBP | Clintox | SIDER | Tox21 | Avg |
|---|---|---|---|---|---|---|
| Llama-3.2-1B | 10.26 | 13.06 | 10.54 | 10.43 | 9.74 | 10.81 |
| Llama-3.2-3B | 10.16 | 10.06 | 8.41 | 9.59 | 10.61 | 9.77 |
| Llama-3.1-8B | 7.15 | 8.04 | 7.24 | 7.57 | 7.11 | 7.42 |
| Llama-3.1-70B | 8.37 | 6.82 | 7.31 | 8.60 | 6.62 | 7.54 |{{< /table-caption >}}
> 🔼 표 7은 다양한 크기의 Llama 모델에 대한 HHEM-2.1 점수를 보여줍니다. HHEM-2.1 점수는 모델이 생성한 분자 설명의 사실 정확도를 측정하는 지표입니다. 이 표는 모델 크기가 클수록 HHEM-2.1 점수가 어떻게 변하는지 보여주어 모델 크기와 환각 점수 간의 관계를 분석하는 데 도움이 됩니다.
> <details>
> <summary>read the caption</summary>
> Table 7: HHM-2.1 scores for Llama models of different sizes.
> </details>

{{< table-caption >}}
| Temperature | HIV | BBBP | Clintox | SIDER | Tox21 | Avg |
|---|---|---|---|---|---|---|
| 0.1 | 61.82 | 64.85 | 49.77 | 54.13 | 54.93 | 57.10 |
| 0.3 | 61.38 | 50.67 | 52.36 | 54.95 | 54.31 | 54.73 |
| 0.5 | 60.03 | 48.11 | 69.48 | 50.10 | 57.38 | 57.02 |
| 0.7 | 62.85 | 49.84 | 51.36 | 57.27 | 58.95 | 56.05 |
| 0.9 | 61.34 | 45.12 | 56.20 | 48.69 | 59.66 | 54.20 |{{< /table-caption >}}
> 🔼 표 8은 Llama-3.1-8B 모델을 사용하여 서로 다른 생성 온도에서 생성된 환각을 사용한 결과를 보여줍니다. ROC-AUC(%) 점수는 다양한 생성 온도(0.1, 0.3, 0.5, 0.7, 0.9)에서 다섯 가지 약물 발견 데이터셋(HIV, BBBP, Clintox, SIDER, Tox21)에 대해 제시됩니다. 각 온도에 대한 평균 ROC-AUC 점수도 계산되어 표에 포함됩니다. 이 표는 생성 온도가 Llama-3.1-8B 모델의 성능에 미치는 영향을 분석하는 데 사용됩니다.
> <details>
> <summary>read the caption</summary>
> Table 8: Full results (ROC-AUC %) of Llama-3.1-8B using hallucination generated by different generation temperatures.
> </details>

{{< table-caption >}}
| Temperature | HIV | BBBP | Clintox | SIDER | Tox21 | Avg |
|---|---|---|---|---|---|---|
| 0.1 | 8.02 | 8.25 | 7.52 | 7.81 | 8.07 | 7.93 |
| 0.3 | 8.57 | 8.75 | 6.87 | 7.39 | 8.05 | 7.92 |
| 0.5 | 8.35 | 8.27 | 7.83 | 9.38 | 5.96 | 7.96 |
| 0.7 | 8.10 | 6.88 | 7.37 | 7.57 | 8.71 | 7.73 |
| 0.9 | 6.61 | 6.85 | 6.42 | 7.96 | 6.27 | 6.82 |{{< /table-caption >}}
> 🔼 표 9는 Llama-3.1-8B 모델이 다양한 온도 설정에서 생성한 환각에 대한 HHM-2.1 점수를 보여줍니다.  HHM-2.1 점수는 모델이 생성한 텍스트의 사실 정확도를 측정하는 지표입니다. 이 표는 다양한 온도에서 생성된 텍스트의 사실 정확도에 미치는 온도의 영향을 분석하는 데 사용됩니다.  온도가 높을수록 환각 점수가 높아지고, 사실 정확도는 낮아지는 경향을 보입니다.
> <details>
> <summary>read the caption</summary>
> Table 9: The HHM-2.1 score for Llama3.1-8B generated hallucinations in different temperatures.
> </details>

{{< table-caption >}}
| Language | HIV | BBBP | Clintox | SIDER | Tox21 | Avg |
|---|---|---|---|---|---|---|
| English | 63.50 | 59.87 | 47.31 | 58.22 | 50.49 | 55.88 |
| Chinese | 57.23 | 57.12 | 65.03 | 52.21 | 57.47 | 57.81 |
| German | 57.31 | 47.57 | 51.56 | 60.22 | 52.42 | 53.82 |
| French | 60.10 | 42.28 | 68.41 | 54.15 | 56.10 | 56.21 |
| Japanese | 55.51 | 51.39 | 47.11 | 49.00 | 54.91 | 51.59 |
| Spanish | 55.85 | 44.91 | 58.06 | 50.43 | 61.79 | 54.21 |{{< /table-caption >}}
> 🔼 표 10은 Llama-3.1-8B 모델을 사용하여 다양한 언어로 생성된 환각을 사용했을 때의 결과(ROC-AUC %)를 보여줍니다.  각 언어별로 생성된 환각이 모델 성능에 미치는 영향을 평가한 실험 결과입니다.  표에는 다섯 가지 약물 발견 데이터셋(HIV, BBBP, Clintox, SIDER, Tox21)에 대한 ROC-AUC 점수와 함께 각 언어별 평균 점수가 포함되어 있습니다.
> <details>
> <summary>read the caption</summary>
> Table 10: Full results (ROC-AUC %) of Llama3.1-8B using hallucination generated in different languages.
> </details>

{{< table-caption >}}
| Name | Description |
|---|---| 
| SMILES | N#Cc1ccccc1O |
| Reference description | The molecule is a nitrile that is hydrogen cyanide in which the hydrogen has been replaced by a hydroxy group. It has a role as an EC 1.2.1.3 [aldehyde dehydrogenase (NAD(+))] inhibitor. It is a nitrile and a member of phenols. It derives from a hydride of a pyridine. |
| English | The given molecular formula is N#Cc1ccccc1O. This formula represents a molecule of benzenecarbonitrile, also known as benzonitrile. The molecule consists of a benzene ring, which is a planar, ring-shaped structure composed of six carbon atoms, each bonded to a hydrogen atom. In this case, one of the carbon atoms in the benzene ring is bonded to a nitrile group (CN), which consists of a carbon atom triple-bonded to a nitrogen atom. Additionally, the molecule has an oxygen atom bonded to one of the carbon atoms in the benzene ring, making it a benzonitrile derivative with an alkoxy group (-O-). |
| Chinese | To describe the molecule in Chinese, we first need to identify the chemical structure. The given chemical formula is N#Cc1ccccc1O. Breaking it down: - ’N’ represents Nitrogen. - ’#’ represents a triple bond. - ’C’ represents Carbon. - ’c1’ represents a ring (benzene ring). - ’ccc’ represents three connected carbons. - ’1’ is the number of the ring. - ’O’ represents Oxygen. So, the molecule is a Benzotriazole molecule. In Chinese, it is written as: 苯三唑 |
| German | Die Molekülformel in deutscher Sprache ist: N-(Phenyl)-acetohydrazid Die Molekülstruktur ist ein Hydrazid, das aus einem Acetohydrazid-Molekül und einem Phenyl-Rest besteht. Der Name ”N-(Phenyl)-acetohydrazid” beschreibt die Struktur des Moleküls, wobei das Acetohydrazid-Molekül an das Phenyl-Rest gebunden ist. Eine alternative Bezeichnung für dieses Molekül ist 1-Hydroxy-2-phenyl-2-hydrazinethanon, aber ”N-(Phenyl)-acetohydrazid” ist eine allgemein akzeptierte und einfache Bezeichnung. Es ist wichtig zu beachten, dass die Molekülformel ”N#Cc1ccccc1O” eine SMILES-Notation ist, eine kurze Bezeichnung für Moleküle, die in der Chemie verwendet wird. Diese Notation kann jedoch nicht direkt als Name eines Moleküls verwendet werden. |
| French | La molécule décrite par la formule chimique ” N#Cc1ccccc1O ” est un isocyanure, plus précisément un isocyanure aromatique. Dans la nomenclature chimique, la formule peut être décomposée comme suit : - ” N ” représente un atome d’azote. - Le ” # ” indique un pont triple entre l’azote et le carbone suivant. - ” C ” représente des atomes de carbone. - Les ” c ” entre les atomes de carbone indiquent des liaisons entre eux. - Le chiffre ” 1 ” entre les parenthèses indique un cycle (un anneau) composé de 6 atomes de carbone. - ” O ” représente un atome d’oxygène. La molécule décrite est donc un isocyanure aromatique cétogénique, qui peut être nommé ” isocyanure benzène ” ou ” isocyanure cétogénique benzénique ”. Cependant, il est important de noter que la formule chimique donnée peut également correspondre à d |
| Japanese | The molecule you provided is 3-Quinuclidinone. In Japanese, the name of this molecule is 3-キヌキジノン (3-Kinukijinon). Here’s a breakdown of the name: - 3-: indicates the position of the carbonyl group - キヌキジ - (Kinukiji-): refers to the quinuclidine ring - ノン (Nōn): indicates the presence of a carbonyl group (ketone) So, the correct Japanese name for the molecule N#Cc1ccccc1O is 3-キヌキジノン (3-Kinukijinon). |
| Spanish | La molécule descrita es un anillo aromático de benzeno (C6H6) con un grupo aminocloruro sustituido (NHCl) y un grupo hidroxilo (OH) sustituido en el anillo. Sin embargo, considerando la notación SMILES proporcionada (N#Cc1ccccc1O), es más probable que la molécula sea una anilina (C6H5NH2) con un grupo hidroxilo (OH) sustituido en el anillo. La estructura química sería: C1=CC=C(C=C1)N=O.|{{< /table-caption >}}
> 🔼 표 11은 Llama-3.1-8B 모델이 다양한 언어로 생성한 분자 및 설명의 예시를 보여줍니다.  다양한 언어 (영어, 중국어, 독일어, 프랑스어, 일본어, 스페인어)로 된 동일한 분자에 대한 설명을 비교하여, 각 언어 모델의 성능과 특징을 보여주는 표입니다.  각 언어에 대한 설명의 정확성과 자세함은 다를 수 있습니다.
> <details>
> <summary>read the caption</summary>
> Table 11: Example of molecule and descriptions in different languages generated by Llama-3.1-8B.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}