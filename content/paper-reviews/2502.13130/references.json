{"references": [{"fullname_first_author": "Anthony Brohan", "paper_title": "RT-2: Robotics Transformer for real-world control at scale", "publication_date": "2022-12-06", "reason": "This paper introduces RT-2, a significant advancement in robotics transformer models, which is directly relevant to Magma's focus on robot manipulation and action grounding."}, {"fullname_first_author": "Jianwei Yang", "paper_title": "OpenVLA: An open-source vision-language-action model", "publication_date": "2024-00-00", "reason": "OpenVLA is a direct precursor to Magma, providing an open-source foundation that Magma significantly extends and improves upon in terms of multimodal understanding and action-taking capabilities."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual Instruction Tuning", "publication_date": "2023-00-00", "reason": "Visual Instruction Tuning is a core technique used in Magma's training, enabling it to effectively leverage large amounts of heterogeneous vision-language data for multimodal understanding and action prediction."}, {"fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "publication_date": "2024-00-00", "reason": "Phi-3 is a foundational language model that Magma builds upon, providing the core language understanding capability necessary for interpreting complex tasks and generating meaningful responses."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "CLIP, introduced in this paper, is a crucial component of Magma's architecture, providing the vision encoder that enables the model to effectively interpret visual data and integrate it with language understanding."}]}