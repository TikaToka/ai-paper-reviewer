[{"content": "| Data Type | Set-of-Mark | Trace-of-Mark |\n|---|---|---|\n| UI Screenshots | \u2713 | \u2717 |\n| Robotics Images | \u2713 | \u2713 |\n| Instructional Videos | \u2713 | \u2713 |", "caption": "Table 1: SoM and ToM applied to various data types. ToM is not applied to UI data as they are a sequence of discrete screenshots.", "description": "\ubcf8 \ud45c\ub294 \ub17c\ubb38\uc758 3.2\uc808(\ubc29\ubc95\ub860)\uc5d0\uc11c \uc81c\uc2dc\ub41c Set-of-Mark(SoM)\uc640 Trace-of-Mark(ToM) \ub77c\ubca8\ub9c1 \uae30\ubc95\uc774 \ub2e4\uc591\ud55c \ub370\uc774\ud130 \uc720\ud615\uc5d0 \uc5b4\ub5bb\uac8c \uc801\uc6a9\ub418\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  SoM\uc740 \uc774\ubbf8\uc9c0 \ub610\ub294 \ube44\ub514\uc624 \ub0b4\uc5d0\uc11c \ud589\ub3d9 \uac00\ub2a5\ud55c \uac1d\uccb4\ub97c \ud45c\uc2dc\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\uba70, UI(\uc0ac\uc6a9\uc790 \uc778\ud130\ud398\uc774\uc2a4) \uc2a4\ud06c\ub9b0\uc0f7, \ub85c\ubd07 \uc870\uc791 \uc774\ubbf8\uc9c0, \uadf8\ub9ac\uace0 \uc0ac\ub78c\uc758 \ud589\ub3d9 \ube44\ub514\uc624 \ub4f1 \ub2e4\uc591\ud55c \ub370\uc774\ud130 \uc720\ud615\uc5d0 \uc801\uc6a9\ub429\ub2c8\ub2e4.  ToM\uc740 \ube44\ub514\uc624 \ub0b4\uc5d0\uc11c \uac1d\uccb4\uc758 \uc6c0\uc9c1\uc784\uc744 \ucd94\uc801\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\uba70, \uc8fc\ub85c \ub85c\ubd07 \uc870\uc791 \ube44\ub514\uc624\uc640 \uc0ac\ub78c\uc758 \ud589\ub3d9 \ube44\ub514\uc624\uc5d0 \uc801\uc6a9\ub429\ub2c8\ub2e4. UI \uc2a4\ud06c\ub9b0\uc0f7\uc758 \uacbd\uc6b0, \uc5f0\uc18d\uc801\uc778 \uc2a4\ud06c\ub9b0\uc0f7 \uc2dc\ud000\uc2a4\uac00 \uc544\ub2c8\uae30 \ub54c\ubb38\uc5d0 ToM\uc774 \uc801\uc6a9\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.", "section": "3. Multimodal Agentic Modeling"}, {"content": "| Model | Size | VQAv2 | TextVQA | POPE | _SS_-Mobile | _SS_-Desktop | SS-Web | VWB-Ele-G | VWB-Act-G | SE-Google Robot | SE-Bridge |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| GPT-4V [99] | n/a | 77.2 | **78.0** | n/a | 22.6/24.5 | 20.2/11.8 | 9.2/8.8 | **67.5** | **75.7** | - | - |\n| GPT-4V-OmniParser [83] | n/a | n/a | n/a | n/a | **92.7**/49.4 | 64.9/26.3 | **77.3**/39.7 | - | - | - | - |\n| LLaVA-1.5 [71] | 7.4B | 78.5 | 58.2 | 85.9 | - | - | - | 12.1 | 13.6 | - | - |\n| LLaVA-Next [75] | 7.4B | **81.8** | 64.9 | **86.5** | - | - | - | 15.0 | 8.7 | - | - |\n| Qwen-VL [3] | 9.6B | 78.8 | 63.8 | n/a | 7.5/4.8 | 5.7/5.0 | 3.5/2.4 | 14.0 | 10.7 | - | - |\n| Qwen-VL-Chat [3] | 9.6B | 78.2 | 61.5 | n/a | - | - | - | - | - | - | - |\n| Fuyu [4] | 8B | 74.2 | n/a | n/a | 41.0/1.3 | 33.0/3.6 | 33.9/4.4 | 19.4 | 15.5 | - | - |\n| SeeClick [19] | 9.6B | - | - | - | **78.0**/**52.0** | 72.2/**30.0** | 55.7/32.5 | 9.9 | 1.9 | - | - |\n| Octo [113] | 93M | - | - | - | - | - | - | - | - | 6.0 | **15.9** |\n| RT-1-X [23] | 35M | - | - | - | - | - | - | - | - | **34.2** | 1.1 |\n| OpenVLA [54] | 8B | - | - | - | - | - | - | - | - | 31.7 | 14.5 |\n| Magma-8B (Ours) | 8.6B | **80.0** | **66.5** | **87.4** | 60.4/**58.5** | **75.3**/52.9 | 69.1/**52.0** | **96.3** | **71.8** | **52.3** | **35.4** |", "caption": "Table 2: Zero-shot evaluation on agentic intelligence. We report the results for pretrained Magma without any domain-specific finetuning. Magma is the only model that can conduct the full task spectrum. \u201cSS\u201d denotes the ScreenSpot benchmark proposed in SeeClick\u00a0[19]; \u201cVWB\u201d denotes VisualWebBench\u00a0[79]; \u201cSE\u201d denotes the SimplerEnv simulator\u00a0[65]. \u2018n/a\u2019 means not available and \u2018-\u2019 means not supported. For all related evaluations, we use OmniParser to provide the detection results only, without local semantics.", "description": "\ud45c 2\ub294 \uc601\uc5b4\ub85c \ub41c \uc81c\ubaa9 \uadf8\ub300\ub85c \ub2e4\uc591\ud55c \ubaa8\ub2ec\ub9ac\ud2f0\ub97c \uc0ac\uc6a9\ud558\ub294 \uc778\uacf5\uc9c0\ub2a5 \uc5d0\uc774\uc804\ud2b8\uc758 \uc81c\ub85c\uc0f7 \uc131\ub2a5 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Magma \ubaa8\ub378\uc740 \ub3c4\uba54\uc778 \ud2b9\ud654\ub41c \ucd94\uac00 \ud559\uc2b5 \uc5c6\uc774 \uc0ac\uc804 \ud6c8\ub828\ub41c \uc0c1\ud0dc\ub85c \ud3c9\uac00\ub418\uc5c8\uc73c\uba70, UI \ud0d0\uc0c9, \ub85c\ubd07 \uc870\uc791, \ub2e4\uc591\ud55c \uba40\ud2f0\ubaa8\ub2ec \uc774\ud574 \uc791\uc5c5 \ub4f1 \uad11\ubc94\uc704\ud55c \uc791\uc5c5\uc744 \uc218\ud589\ud560 \uc218 \uc788\ub294 \uc720\uc77c\ud55c \ubaa8\ub378\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ScreenSpot, VisualWebBench, SimplerEnv \uc138 \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c\uc758 \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uba70, 'SS', 'VWB', 'SE'\ub294 \uac01\uac01 \ud574\ub2f9 \ubca4\uce58\ub9c8\ud06c\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. 'n/a'\ub294 \ud574\ub2f9 \ub370\uc774\ud130\uac00 \uc5c6\uc74c\uc744, '-'\ub294 \uc9c0\uc6d0\ub418\uc9c0 \uc54a\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  OmniParser\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2dc\uac01\uc801 \uacb0\uacfc\ub9cc \uc81c\uacf5\ud588\uc74c\uc744 \uba85\uc2dc\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8 (Experiment)"}, {"content": "| Model | SoM+ToM | SS-Overal | VWB-Ele-G | VWB-Act-G | SE-Bridge | SE-Google |\n|---|---|---|---|---|---|---|\n| `Magma`-8B (UI) | \u2717 | 57.7 | 68.5 | 58.3 | - | - |\n| `Magma`-8B (OXE) | \u2717 | - | - | - | 22.2 | 35.7 |\n| `Magma`-8B (ACT) | \u2717 | 56.2 | 89.1 | 21.4 | 17.5 | 31.5 |\n| `Magma`-8B (Full) | \u2717 | 57.4 | 90.1 | 25.2 | 17.7 | 37.5 |\n| `Magma`-8B (Full) | \u2713 | **61.4** | **96.3** | **71.8** | **35.4** | **52.3** |", "caption": "Table 3: Ablation study on the effect of data mixtures and pretraining techniques. w/o SoM+Tom means using original action supervisions\u00a0(2D coordinates for UI and 7DoF for robots.)", "description": "\ubcf8 \ud45c\ub294 \ub370\uc774\ud130 \ud63c\ud569\uacfc \uc0ac\uc804 \ud559\uc2b5 \uae30\ubc95\uc758 \ud6a8\uacfc\uc5d0 \ub300\ud55c \ucd94\uac00 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  SoM(Set-of-Mark)\uacfc ToM(Trace-of-Mark)\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0(w/o SoM+ToM)\ub97c \uae30\uc900\uc73c\ub85c, UI \uc791\uc5c5(2D \uc88c\ud45c)\uacfc \ub85c\ubd07 \uc870\uc791(7DoF)\uc5d0 \ub300\ud55c \uc6d0\ub798\uc758 \uc791\uc5c5 \uac10\ub3c5 \ubc29\uc2dd\uacfc \ube44\uad50\ud558\uc5ec SoM\uacfc ToM\uc744 \uc0ac\uc6a9\ud55c \uc0ac\uc804 \ud559\uc2b5\uc758 \uc601\ud5a5\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uc870\ud569\uc73c\ub85c \uc0ac\uc804 \ud559\uc2b5\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uac01 \uae30\ubc95\uc774 \ubaa8\ub378 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. Experiment"}, {"content": "| Method | Backbone | Input Source |  | Cross-Website |  |  | Cross-Task |  |  | Cross-Domain |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  |  | DoM Tree | Image | Ele. Acc | Op. F1 | Step SR | Ele. Acc | Op. F1 | Step SR | Ele. Acc | Op. F1 | Step SR |\n| GPT-4-MindAct [27] | GPT-4 [98] | \u2713 |  | 35.8 | 51.1 | 30.1 | 41.6 | 60.6 | 36.2 | 37.1 | 46.5 | 26.4 |\n| GPT-4V-OmniParser [83] | GPT-4V [99] | \u2713 | \u2713 | 41.0 | 84.8 | 36.5 | 42.4 | 87.6 | 39.4 | 45.5 | 85.7 | 42.0 |\n| SeeAct [141] | GPT-4V [99] |  | \u2713 |  |  | 13.9 | - | - | 20.3 | - | - | 23.7 |\n|  | Gemini-Pro [36] | \u2713 | \u2713 | 21.5 | 67.7 | 19.6 | 21.5 | 67.7 | 19.6 | 20.7 | 64.3 | 18.0 |\n|  | GPT-4V [99] | \u2713 | \u2713 | 38.0 | 67.8 | 32.4 | 46.4 | 73.4 | 40.2 | 42.4 | 69.3 | 36.8 |\n| Fuyu-8B\u2021 | Fuyu-8B [4] |  | \u2713 | 4.8 | 81.3 | 4.0 | 8.3 | 83.9 | 6.6 | 3.6 | 83.0 | 3.0 |\n| Fuyu-8B-GUI [17] | Fuyu-8B [4] |  | \u2713 | 13.9 | 80.7 | 12.2 | 19.1 | 86.1 | 15.6 | 14.2 | 83.1 | 11.7 |\n| MiniCPM-V\u2021 | MiniCPM-V [128] |  | \u2713 | 8.2 | 78.2 | 6.0 | 11.0 | 85.6 | 8.5 | 6.5 | 81.4 | 5.2 |\n| MiniCPM-V-GUI [17] | MiniCPM-V [128] |  | \u2713 | 20.3 | 81.7 | 17.3 | 23.8 | 86.8 | 20.8 | 17.9 | 74.5 | 17.6 |\n| Qwen-VL\u266e | Qwen-VL [3] |  | \u2713 | 13.2 | 83.5 | 9.2 | 15.9 | 86.7 | 13.3 | 14.1 | 84.3 | 12.0 |\n| SeeClick [19] | Qwen-VL [3] |  | \u2713 | 21.4 | 80.6 | 16.4 | 28.3 | 87.0 | 25.5 | 23.2 | 84.8 | 20.8 |\n| CogAgent\u2020 [43] | CogVLM [118] |  | \u2713 | 27.3 | - | 23.4 | 30.2 | - | 26.9 | 33.1 | - | 28.5 |\n| Qwen2-UIX [78] | Qwen2 [124] |  | \u2713 | 39.2 | - | 31.0 | 43.4 | - | 38.2 | 40.4 | - | 34.9 |\n| Magma-8B (Ours) | LLaMA3 [92] |  | \u2713 | 57.2 | 76.9 | 45.4 | 54.8 | 79.7 | 43.4 | 55.7 | 80.6 | 47.3 |", "caption": "Table 4: Efficient finetuning on Mind2Web for web UI navigation. \u201cEle. Acc\u201d denotes element selection accuracy. \u201cOp. F1\u201d denotes the token-wise F1 score between predicted ground-truth operation. \u201cStep SR\u201d denotes the step-wise success rate. \u2021 Numbers reported in Chen et\u00a0al. [17]. \u266e Numbers reported in Cheng et\u00a0al. [19]. \u2020 Numbers reported in Liu et\u00a0al. [78].", "description": "\ud45c 4\ub294 Mind2Web \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc6f9 UI \ud0d0\uc0c9 \uc791\uc5c5\uc5d0 \ub300\ud55c \ud6a8\uc728\uc801\uc778 \ubbf8\uc138 \uc870\uc815 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc138 \uac00\uc9c0 \uc8fc\uc694 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \"Ele. Acc\"\ub294 \uc694\uc18c \uc120\ud0dd \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0b4\uba70, \uc608\uce21\ub41c \uc815\ub2f5 \uc791\uc5c5\uacfc \uc2e4\uc81c \uc791\uc5c5 \uac04\uc758 \ud1a0\ud070 \uc77c\uce58 F1 \uc810\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \"Step SR\"\uc740 \ub2e8\uacc4\ubcc4 \uc131\uacf5\ub960\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uc5ec\ub7ec \ubaa8\ub378\uc758 \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uba70, Chen et al. [17], Cheng et al. [19], Liu et al. [78] \uc5d0\uc11c \ubcf4\uace0\ub41c \uacb0\uacfc\uc640 \ube44\uad50\ud558\uc5ec Magma \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.1.2 \ud6a8\uc728\uc801\uc778 \ubbf8\uc138 \uc870\uc815"}, {"content": "| Method | Backbone | DoM Tree | Image | General | Install | GoogleApps | Single | WebShopping | Overall |\n|---|---|---|---|---|---|---|---|---|---| \n| GPT-4V-SeeAct<sup>\u2020</sup> [141] | GPT-4V [99] |  | \u2713 | 34.1 | 39.4 | 40.0 | 46.2 | 38.2 | 39.6 |\n| GPT-4V-ReAct<sup>\u2020</sup> [127] | GPT-4V [99] |  | \u2713 | 36.2 | 42.5 | 46.6 | 49.1 | 39.2 | 42.7 |\n| GPT-4V-OmniParser [83] | GPT-4V [99] | \u2713 | \u2713 | 48.3 | 57.8 | 51.6 | 77.4 | 52.9 | 57.7 |\n| Fuyu-8B<sup>\u2021</sup> | Fuyu-8B [4] |  | \u2713 | - | 45.9 | 40.0 | 47.2 | 40.8 | - |\n| Fuyu-8B-GUI [17] | Fuyu-8B [4] |  | \u2713 | - | 50.9 | 41.6 | 45.7 | 43.8 | - |\n| MiniCPM-V<sup>\u2021</sup> | MiniCPM-V [128] |  | \u2713 | - | 50.2 | 45.1 | 56.2 | 44.0 | - |\n| MiniCPM-V-GUI [17] | MiniCPM-V [128] |  | \u2713 | - | 62.3 | 46.5 | 67.3 | 57.5 | - |\n| Qwen-VL<sup>\u266e</sup> | Qwen-VL [3] |  | \u2713 | 49.5 | 59.9 | 46.9 | 64.7 | 50.7 | 54.3 |\n| SeeClick [19] | Qwen-VL [3] |  | \u2713 | 54.0 | 66.4 | 54.9 | 63.5 | 57.6 | 59.3 |\n| Magma-8B (Ours) | LLaMA3 [92] |  | \u2713 | **61.5** | **73.2** | **62.7** | **77.5** | **61.7** | **67.3** |", "caption": "Table 5: Efficient finetuning on AITW for mobile UI navigation. We compared models either using DoM tree or image screenshot. We finetune our Magma jointly and then report the results on individual tasks. \u2020 Numbers reported in Zhang et\u00a0al. [138]. \u2021 Numbers reported in Chen et\u00a0al. [17]. \u266e Numbers reported in Cheng et\u00a0al. [19].", "description": "\ud45c 5\ub294 AITW(Android In The Wild) \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ubc14\uc77c UI \ud0d0\uc0c9 \uc791\uc5c5\uc5d0 \ub300\ud55c \ud6a8\uc728\uc801\uc778 \ubbf8\uc138 \uc870\uc815 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  DOM \ud2b8\ub9ac \ub610\ub294 \uc774\ubbf8\uc9c0 \uc2a4\ud06c\ub9b0\uc0f7\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5ec\ub7ec \ubaa8\ub378\uc744 \ube44\uad50\ud588\uc2b5\ub2c8\ub2e4.  Magma \ubaa8\ub378\uc740 \uc5ec\ub7ec \uc791\uc5c5\uc5d0 \ub300\ud574 \ud1b5\ud569\uc801\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ub418\uc5c8\uc73c\uba70 \uac1c\ubcc4 \uc791\uc5c5\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \ubcf4\uace0\ub429\ub2c8\ub2e4.  Zhang et al. [138], Chen et al. [17], Cheng et al. [19]\uc758 \uc5f0\uad6c\uc5d0\uc11c \ubcf4\uace0\ub41c \uc218\uce58\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ub418\uc5b4 Magma\uc758 \uc131\ub2a5\uc744 \ub354\uc6b1 \uba85\ud655\ud558\uac8c \ube44\uad50\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.", "section": "5.1.2 \ud6a8\uc728\uc801\uc778 \ubbf8\uc138 \uc870\uc815"}, {"content": "| Model | VSR | BLINK-val | SpatialEval - Spatial Map | SpatialEval - Maze Nav. | SpatialEval - Spatial Grid |\n|---|---|---|---|---|---| \n| GPT-4o | 74.8 | 60.0 | - | - | - |\n| Gemini | - | 61.4 | - | - | - |\n| LLaVA-1.5-7B | 57.1* | 37.1 | 28.4 | 28.8 | 41.6 |\n| LLaVA-1.6-7B [75] | 52.2* | - | 28.0 | 34.8 | 32.2 |\n| Qwen-VL-9.6B [3] | - | 40.3 | 28.7 | 31.8 | 25.7 |\n| Magma-8B (Act<sup>w/o</sup>) | 62.8 | 30.1 | 36.9 | **44.8** | 37.5 |\n| Magma-8B (Full<sup>w/o</sup>) | 58.1 | 38.3 | 27.5 | 33.5 | 47.3 |\n| Magma-8B (Full) | **65.1** | **41.0** | **43.4** | 36.5 | **64.5** |", "caption": "Table 6: Spatial reasoning evaluations. We use * to denote results that are obtained by us evaluating the provided model weights. Superscript \u2018w/o\u2019 means models pretrained without SoM/ToM.", "description": "\ud45c 6\uc740 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uacf5\uac04 \ucd94\ub860 \ub2a5\ub825\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc740 VSR, BLINK-val, SpatialEval2 \uc138 \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud574 \ud3c9\uac00\ub418\uc5c8\uc73c\uba70, Zero-shot \uc124\uc815\uc5d0\uc11c \uc218\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5(\uc815\ud655\ub3c4 \ub610\ub294 \uc131\uacf5\ub960)\uc774 \ubca4\uce58\ub9c8\ud06c\ubcc4\ub85c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  * \ud45c\uc2dc\ub294 \uc5f0\uad6c\ud300\uc774 \uc81c\uacf5\ud55c \ubaa8\ub378 \uac00\uc911\uce58\ub97c \uc9c1\uc811 \ud3c9\uac00\ud558\uc5ec \uc5bb\uc740 \uacb0\uacfc\uc784\uc744 \ub098\ud0c0\ub0b4\uba70, 'w/o'\ub294 SoM\uacfc ToM\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uc989, SoM(Set-of-Mark)\uacfc ToM(Trace-of-Mark) \uae30\ubc95\uc758 \ud6a8\uacfc\ub97c \ud655\uc778\ud558\uae30 \uc704\ud55c \ube44\uad50 \ubd84\uc11d \uacb0\uacfc\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.2 \uacf5\uac04 \ucd94\ub860 \ud3c9\uac00"}, {"content": "| Model | VQAv2 | GQA | MME | POPE | TextVQA | ChartQA | DocVQA |\n|---|---|---|---|---|---|---|---| \n| LLaVA-1.5-7B [61] | 76.6 | 62.6 | 1510.8 | 85.9 | 46.1 | 18.2 | 28.1 |\n| LLaVA-Next-7B [75] | 80.1 | **64.2** | 1519.3 | **86.4** | 64.9 | 54.8 | 74.4 |\n| Magma-8B (SFT) | 79.5 | 61.5 | 1510.1 | 86.2 | 67.7 | 73.0 | 80.4 |\n| Magma-8B (Act<sup class=\"ltx_sup\">w/o</sup>) | 81.3 | 63.5 | 1559.5 | 86.1 | 69.8 | 71.0 | 84.1 |\n| Magma-8B (Full<sup class=\"ltx_sup\">w/o</sup>) | 81.3 | 62.9 | 1576.0 | 86.3 | 69.6 | 71.7 | 83.8 |\n| Magma-8B (Full) | **81.4** | 64.0 | **1588.7** | 86.3 | **70.2** | **76.2** | **84.8** |", "caption": "Table 7: Finetuned performance on multimodal image understanding tasks. Pretraining on full set with SoM and ToM\u00a0(last row) attains the overall best performance compared with our own baselines and counterparts of the same model class.", "description": "\ud45c 7\uc740 \ub2e4\uc591\ud55c \uba40\ud2f0\ubaa8\ub2ec \uc774\ubbf8\uc9c0 \uc774\ud574 \uc791\uc5c5\uc5d0 \ub300\ud55c \ubbf8\uc138 \uc870\uc815\ub41c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  SoM(Set-of-Mark)\uacfc ToM(Trace-of-Mark)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc804\uccb4 \ub370\uc774\ud130\uc14b\uc73c\ub85c \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378(\ub9c8\uc9c0\ub9c9 \ud589)\uc774 \ub3d9\uc77c\ud55c \ubaa8\ub378 \ud074\ub798\uc2a4\uc758 \uc790\uccb4 \uae30\uc900 \ubc0f \ub300\uc751 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec \uc804\ubc18\uc801\uc73c\ub85c \ucd5c\uc0c1\uc758 \uc131\ub2a5\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.  \uc989, \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc9c8\ubb38 \uc751\ub2f5 \ubc0f \uc774\ubbf8\uc9c0 \uc774\ud574 \uacfc\uc81c\uc5d0\uc11c Magma \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.  \uc0ac\uc804 \ud6c8\ub828 \uacfc\uc815\uc5d0\uc11c SoM\uacfc ToM \uae30\ubc95\uc744 \uc0ac\uc6a9\ud55c \ubaa8\ub378\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8"}, {"content": "| Method | Backbone | IntentQA | Next-QA | VideoMME (w/o subs) |  |  | MVBench |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Gemini-1.5 [36] | - | - | - | 81.7 | 74.3 | 67.4 | - | - | - | 37.7 |\n| GPT-4V [2] | GPT-4 | - | - | 70.5 | 55.8 | 53.5 | - | - | - | 43.7 |\n| LLaVA-OV [60] | Qwen2-7B | - | 79.4 | 68.1 | 54.9 | 47.8 | 46.0 | 74.5 | 48.0 | 56.7 |\n| Long-Llava 9B [119] | Long-Llava 9B | - | - | 52.4 | 42.2 | 36.4 | - | - | - | 49.1 |\n| LongVA [136] | Qwen2-7B | - | 69.3 | 61.1 | 50.4 | 46.2 | 49.0 | 53.0 | 42.5 | 51.3 |\n| ShareGPT4Video [15] | LLaMA3-8B | - | - | 48.3 | 36.3 | 35.0 | 40.0 | 49.5 | 41.5 | 51.2 |\n| Video-Llama2 [20] | Llama2-7B | - | - | 55.9 | 45.4 | 42.1 | - | - | - | 34.1 |\n| Video-Chat2 [63] | Mistral 7B | - | 43.3 | 48.3 | 37.0 | 33.2 | 47.5 | 75.0 | 50.5 | 60.4 |\n| Video-Llava [69] | Vicuna-7B | - | 51.4 | 45.3 | 38.0 | 36.2 | 50.0 | 38.5 | 30.5 | 43.0 |\n| IG-VLM [55] | Vicuna-7B | 60.3 | - | - | - | - | - | - | - | - |\n| SF-LLaVA [121] | Vicuna-7B | 60.1 | - | - | - | - | - | - | - | - |\n| Magma-8B (Ours) | LLaMA3-8B | 88.6 | 80.9 | 72.9 | 55.8 | 44.3 | 65.0 | 79.0 | 55.5 | 59.4 |", "caption": "Table 8: Zero-shot Video QA benchmarks. We compare our Magma model to other state-of-the-art approaches with comparable numbers of parameters. Our Magma model performs competitively and even outperforms some state-of-the-art approaches such as Video-Llama2 and ShareGPT4Video on most benchmarks, despite using much fewer video instruction tuning data.", "description": "\ud45c 8\uc740 \ub2e4\uc591\ud55c \ucd5c\ucca8\ub2e8 \ube44\ub514\uc624 \uc9c8\uc758\uc751\ub2f5(VQA) \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc601\uc810(Zero-shot) \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4. \ub9e4\uac1c\ubcc0\uc218 \uc218\uac00 \ube44\uc2b7\ud55c \ub2e4\ub978 \ucd5c\ucca8\ub2e8 \uc811\uadfc \ubc29\uc2dd\ub4e4\uacfc Magma \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec Magma \ubaa8\ub378\uc774 Video-Llama2 \ubc0f ShareGPT4Video\uc640 \uac19\uc740 \uc77c\ubd80 \ucd5c\ucca8\ub2e8 \uc811\uadfc \ubc29\uc2dd\ubcf4\ub2e4 \ub300\ubd80\ubd84\uc758 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uace0 \uc2ec\uc9c0\uc5b4 \ub2a5\uac00\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788 Magma \ubaa8\ub378\uc740 \ube44\ub514\uc624 \uc9c0\uc2dc\uc5b4 \ubbf8\uc138 \uc870\uc815 \ub370\uc774\ud130\ub97c \ud6e8\uc52c \uc801\uac8c \uc0ac\uc6a9\ud588\uc74c\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.  \uac01 \ubca4\uce58\ub9c8\ud06c\ub294 \ub2e4\uc591\ud55c \uc885\ub958\uc758 \ube44\ub514\uc624 \uc9c8\uc758\uc751\ub2f5 \uacfc\uc81c\ub97c \ud3ec\ud568\ud558\uba70, Magma \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ub2e4\uac01\uc801\uc73c\ub85c \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8"}, {"content": "| Setting | Pretraining | Finetuning |  |  | \n|---|---|---|---|---| \n|  |  | UI | Image/Video | Real Robot | \n| batch size | 1024 | 32 |  |  | \n| base learning rate | 1e-5 | 1e-5 | 1e-5 | 1e-5 | \n| learning rate scheduler | Constant | Cosine | Cosine | Constant | \n| training epochs | 3 | 3 | 1 | 20 | \n| optimizer | adamw | adamw | adamw | adamw | \n| Image Resolution | 512 | 768 | 768 | 256 | \n| Number of Crops | 4 or 1 | 4 | 4 or 1 | 1 | ", "caption": "Table 9: Experimental settings pretraining and finetuning of Magma models. We maximally use either 32 Nvidia H100s or 64 AMD MI300 GPUs for all training jobs.", "description": "\ud45c 9\ub294 Magma \ubaa8\ub378\uc758 \uc0ac\uc804 \ud6c8\ub828 \ubc0f \ubbf8\uc138 \uc870\uc815\uc5d0 \uc0ac\uc6a9\ub41c \uc2e4\ud5d8 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0ac\uc804 \ud6c8\ub828 \ub2e8\uacc4\uc5d0\uc11c\ub294 \ubc30\uce58 \ud06c\uae30, \uae30\ubcf8 \ud559\uc2b5\ub960, \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904\ub7ec, \ucd5c\uc801\ud654\uae30, \uc5d0\ud3ed \uc218, \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4, \uc790\ub974\uae30 \ud69f\uc218 \ub4f1\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uc124\uc815\ud569\ub2c8\ub2e4. \ubbf8\uc138 \uc870\uc815 \ub2e8\uacc4\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc774 \uac01 \uc791\uc5c5\uc5d0 \ub9de\uac8c \uc870\uc815\ub429\ub2c8\ub2e4.  \ubaa8\ub4e0 \ud6c8\ub828 \uc791\uc5c5\uc5d0\ub294 \ucd5c\ub300 32\uac1c\uc758 Nvidia H100 GPU \ub610\ub294 64\uac1c\uc758 AMD MI300 GPU\uac00 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \ud45c\ub294 \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b(SeeClick-Web, SeeClick-Mobile, Vision2UI)\uc5d0 \ub300\ud55c \uc124\uc815\uacfc UI, \uc774\ubbf8\uc9c0/\ube44\ub514\uc624, \uc2e4\uc81c \ub85c\ubd07 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc124\uc815\uc744 \uad6c\ubd84\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.2. Method"}, {"content": "| Source | Task | Size |\n|---|---|---|\n| SeeClick-Web | text_2_point | 271K |\n|  | text_2_bbox | 54K |\n|  | point_2_text | 54K |\n|  | bbox_2_text | 54K |\n| SeeClick-Mobile | text_2_point | 274K |\n|  | text_2_bbox | 56K |\n|  | UI summarization | 48K |\n|  | widget captioning | 42K |\n| Visison2UI | input_2_point | 980K |\n|  | input_2_bbox | 982K |\n|  | text_2_point | 794K |\n|  | text_2_bbox | 774K |\n|  | point_2_text | 199K |\n|  | bbox_2_text | 193K |\n| Magma-PT-UI\u00a0(Ours) | Mixed | 2.8M |", "caption": "Table 10: Statistics of UI related pretraining data.", "description": "\ud45c 10\uc740 \ub17c\ubb38\uc758 \ub370\uc774\ud130\uc14b \uad6c\uc131 \ubd80\ubd84\uc5d0\uc11c UI \uad00\ub828 \uc804\ucc98\ub9ac \ub370\uc774\ud130\uc758 \ud1b5\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  UI \ub370\uc774\ud130\uc14b\uc740 \ud06c\uac8c SeeClick-Web\uacfc SeeClick-Mobile, Vision2UI \uc138 \uac00\uc9c0\ub85c \ub098\ub258\uba70 \uac01 \ub370\uc774\ud130\uc14b \ubcc4\ub85c text_2_point, text_2_bbox, point_2_text, bbox_2_text \uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uc791\uc5c5 \uc720\ud615\uc758 \ub370\uc774\ud130 \uc218\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uc791\uc5c5 \uc720\ud615\uc740 UI \uc694\uc18c\uc758 \ud14d\uc2a4\ud2b8\uc640 \uc88c\ud45c \uc815\ubcf4 \uac04\uc758 \ub9e4\ud551 \uad00\uacc4\ub97c \ub098\ud0c0\ub0b4\ub294 \ub370 \uc0ac\uc6a9\ub418\ub294 \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 \ubaa8\ub378\uc774 UI \uc0c1\ud638\uc791\uc6a9\uc744 \uc774\ud574\ud558\uace0 \uc0dd\uc131\ud558\ub294 \ub370 \uc0ac\uc6a9\ub41c \ud559\uc2b5 \ub370\uc774\ud130\uc758 \uc591\uacfc \ub2e4\uc591\uc131\uc744 \uc815\ud655\ud558\uac8c \ud30c\uc545\ud558\ub294 \ub370 \uc911\uc694\ud569\ub2c8\ub2e4.", "section": "4.2 SoM \ubc0f ToM \uc0dd\uc131"}, {"content": "| Dataset | Size | Domain |\n|---|---|---|\n| ShareGPT [106] | 40K | Text |\n| ShareGPT4V [13] | 39K | General |\n| LLaVA-Instruct [71] | 158K | General |\n| LAION-GPT4V [58] | 11K | General |\n| VQAv2 [39] | 83K | General VQA |\n| GQA [45] | 72K | General VQA |\n| OKVQA [105] | 9K | Knowledge VQA |\n| OCRVQA [93] | 80K | OCR VQA |\n| ChartQA [87] | 7K | Chart VQA |\n| DVQA [46] | 16K | Chart VQA |\n| DocVQA [89] | 10K | Document VQA |\n| AI2D [51] | 2K | Infographic VQA |\n| SynthDog-EN [53] | 20K | Document Understanding |\n| A-OKVQA | 66K | Knowledge VQA |\n| RefCOCO [133] | 48K | Grounding Desc. |\n| VG [57] | 86K | Referring Exp. |\n| InfographicsVQA [90] | 24k | Infographic VQA |\n| ChartQA (Aug) [87] | 20k | Chart VQA |\n| FigureQA [47] | 20k | Chart/Figure VQA |\n| TQA [52] | 1.5k | Textbook VQA |\n| ScienceQA [82] | 5k | Textbook VQA |\n| Magma-SFT-Image (Ours) | 820k | Mixed |", "caption": "Table 11: A detailed breakdown of our 820k Magma image instruction tuning data used in our multimodal image understanding experiments shown in Table\u00a05 in our main submission.", "description": "\ubcf8 \ub17c\ubb38\uc758 \ud45c 11\uc740 \ubcf8 \ub17c\ubb38 \uc81c\ucd9c \uc2dc \ud45c 5\uc5d0\uc11c \uc5b8\uae09\ub41c \ub2e4\uc911 \ubaa8\ub4dc \uc774\ubbf8\uc9c0 \uc774\ud574 \uc2e4\ud5d8\uc5d0 \uc0ac\uc6a9\ub41c 82\ub9cc \uac1c\uc758 Magma \uc774\ubbf8\uc9c0 \uc9c0\uc2dc \ud29c\ub2dd \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ubd84\uc11d\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 Magma \ubaa8\ub378\uc758 \ub2e4\uc911 \ubaa8\ub4dc \uc774\ubbf8\uc9c0 \uc774\ud574 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc758 \uad6c\uc131\uacfc \uaddc\ubaa8\uc5d0 \ub300\ud55c \uc138\ubd80 \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\uc5ec, \uc2e4\ud5d8 \uacb0\uacfc\uc758 \uc2e0\ub8b0\uc131\uacfc \uc7ac\ud604\uc131\uc744 \ub192\uc774\ub294 \ub370 \uae30\uc5ec\ud569\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\uc640 \ud2b9\uc9d5 (\uc77c\ubc18 VQA, OCR VQA, \ucc28\ud2b8 VQA, \ubb38\uc11c VQA, \ubc30\uacbd \uc124\uba85 \ub4f1)\uc774 \uba85\uc2dc\ub418\uc5b4 \uc788\uc5b4 \ub370\uc774\ud130 \uc14b\uc758 \ub2e4\uc591\uc131\uacfc \uade0\ud615\uc744 \ud30c\uc545\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "4.3 Pretraining"}]