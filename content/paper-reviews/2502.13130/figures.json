[{"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/intro_fig.png", "caption": "Figure 1: We introduce Magma, the first foundation model that is capable of interpreting and grounding multimodal inputs within its environment. Given a described goal, Magma is able to formulate plans and execute actions to achieve it. By effectively transferring knowledge from freely available visual and language data, Magma bridges verbal and spatial intelligence to navigate complex tasks.", "description": "\uadf8\ub9bc 1\uc740 \ub2e4\uc591\ud55c \ubaa8\ub4dc\uc758 \uc785\ub825\uc744 \ud574\uc11d\ud558\uace0 \ud658\uacbd \ub0b4\uc5d0\uc11c \uc9c0\uc2dd\uc744 \ud65c\uc6a9\ud560 \uc218 \uc788\ub294 \ucd5c\ucd08\uc758 \uae30\ucd08 \ubaa8\ub378\uc778 Magma\ub97c \uc18c\uac1c\ud569\ub2c8\ub2e4. Magma\ub294 \uc0ac\uc6a9\uc790\uac00 \uc124\uba85\ud55c \ubaa9\ud45c\uac00 \uc8fc\uc5b4\uc9c0\uba74 \uacc4\ud68d\uc744 \uc138\uc6b0\uace0 \ud589\ub3d9\uc744 \uc2e4\ud589\ud558\uc5ec \ubaa9\ud45c\ub97c \ub2ec\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc790\uc720\ub86d\uac8c \uc0ac\uc6a9 \uac00\ub2a5\ud55c \uc2dc\uac01 \ubc0f \uc5b8\uc5b4 \ub370\uc774\ud130\uc5d0\uc11c \uc9c0\uc2dd\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \uc804\uc774\ud568\uc73c\ub85c\uc368 Magma\ub294 \uc5b8\uc5b4\uc801 \ubc0f \uacf5\uac04\uc801 \uc9c0\ub2a5\uc744 \uc5f0\uacb0\ud558\uc5ec \ubcf5\uc7a1\ud55c \uc791\uc5c5\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/som_fig.png", "caption": "Figure 2: A multimodal AI agent should be capable of mutimodal understanding and action-prediction towards a given goal.", "description": "\uadf8\ub9bc 2\ub294 \ub2e4\uc591\ud55c \ubaa8\ub4dc(\uc2dc\uac01, \uc5b8\uc5b4 \ub4f1)\uc758 \uc785\ub825\uc744 \uc774\ud574\ud558\uace0(\ub2e4\uc911 \ubaa8\ub4dc \uc774\ud574) \uc8fc\uc5b4\uc9c4 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud55c \ud589\ub3d9\uc744 \uc608\uce21\ud558\ub294(\ub2e4\uc911 \ubaa8\ub4dc \ud589\ub3d9 \uc608\uce21) \ub2a5\ub825\uc744 \uac16\ucd98 \ub2e4\uc911 \ubaa8\ub4dc AI \uc5d0\uc774\uc804\ud2b8\uc758 \uac1c\ub150\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc5d0\uc774\uc804\ud2b8\ub294 \uc0ac\uc6a9\uc790\uc758 \ubaa9\ud45c\ub97c \uc785\ub825\ubc1b\uc544 \ub2e4\uc591\ud55c \ubaa8\ub4dc\uc758 \uc815\ubcf4\ub97c \ucc98\ub9ac\ud558\uace0, \ucd5c\uc885\uc801\uc73c\ub85c \uc801\uc808\ud55c \ud589\ub3d9\uc744 \ucd9c\ub825\ud569\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc758 \ud575\uc2ec \uac1c\ub150\uc744 \uac04\ub7b5\ud558\uac8c \uc2dc\uac01\uc801\uc73c\ub85c \ud45c\ud604\ud55c \uac83\uc785\ub2c8\ub2e4.", "section": "3. Multimodal Agentic Modeling"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/tom_fig.png", "caption": "Figure 3: Set-of-Mark supervisions for action grounding on UI screenshot\u00a0(left), robot manipulation\u00a0(middle) and human video\u00a0(right). All coordinates are normalized by image size\u00a0(height, width) and then quantized into 256 bins. Images better viewed by zooming in.", "description": "\uadf8\ub9bc 3\uc740 Set-of-Mark(SoM)\ub97c \uc774\uc6a9\ud558\uc5ec \ud589\ub3d9 \uc811\uc9c0\ub97c \uc704\ud55c \uc9c0\ub3c4 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\ubd80\ud130 UI \uc2a4\ud06c\ub9b0\uc0f7, \ub85c\ubd07 \uc870\uc791, \uc0ac\ub78c\uc758 \ube44\ub514\uc624\uc5d0 \ub300\ud55c SoM \uc9c0\ub3c4 \ud559\uc2b5\uc758 \uc608\uc2dc\uac00 \ub098\uc640\uc788\uc2b5\ub2c8\ub2e4. \uac01 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uc218\ud589 \uac00\ub2a5\ud55c \ub3d9\uc791\uacfc \uad00\ub828\ub41c \uc2dc\uac01\uc801 \uac1c\uccb4(\uc608: UI\uc5d0\uc11c \ud074\ub9ad \uac00\ub2a5\ud55c \ubc84\ud2bc, \ub85c\ubd07 \uc554\uc758 \uc704\uce58, \uc0ac\ub78c\uc758 \uc190 \ub3d9\uc791)\uc5d0 SoM \ub808\uc774\ube14\uc744 \uc9c0\uc815\ud588\uc2b5\ub2c8\ub2e4. \uc774\ubbf8\uc9c0 \ud06c\uae30(\ub192\uc774, \ub108\ube44)\ub85c \uc815\uaddc\ud654\ud55c \ud6c4 256\uac1c\uc758 \uad6c\uac04\uc73c\ub85c \uc591\uc790\ud654\ud558\uc5ec \uc88c\ud45c\ub97c \ud45c\uc2dc\ud569\ub2c8\ub2e4. \ub354 \uc790\uc138\ud55c \ub0b4\uc6a9\uc744 \ubcf4\uc2dc\ub824\uba74 \uc774\ubbf8\uc9c0\ub97c \ud655\ub300\ud558\uc5ec \ubcf4\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4.", "section": "3.2. Method"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/tom_fig5.png", "caption": "Figure 4: Trace-of-Mark supervisions for robot manipulation\u00a0(left) and human action\u00a0(right). Same coordinate normalization and quantization is used as SoM. Images show the future traces to predict.", "description": "\uadf8\ub9bc 4\ub294 \ub85c\ubd07 \uc870\uc791(\uc67c\ucabd)\uacfc \uc778\uac04 \ud589\ub3d9(\uc624\ub978\ucabd)\uc5d0 \ub300\ud55c Trace-of-Mark(ToM) \uc9c0\ub3c4 \uac10\ub3c5\uc758 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. ToM\uc740 \uc774\ubbf8\uc9c0 \ub0b4\uc5d0\uc11c \uc218\ud589 \uac00\ub2a5\ud55c \ub3d9\uc791\uc744 \ub098\ud0c0\ub0b4\ub294 Set-of-Mark(SoM)\uacfc \ub3d9\uc77c\ud55c \uc88c\ud45c \uc815\uaddc\ud654 \ubc0f \uc591\uc790\ud654 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c\ub294 \ubaa8\ub378\uc774 \uc608\uce21\ud574\uc57c \ud558\ub294 \ubbf8\ub798\uc758 \uada4\uc801(Trace)\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc774\ubbf8\uc9c0\ub4e4\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc989, SoM\uc774 \ud604\uc7ac \uc2dc\uc810\uc5d0\uc11c\uc758 \uc870\uc791 \uac00\ub2a5\ud55c \uac1d\uccb4\ub97c \ud45c\uc2dc\ud558\ub294 \ubc18\uba74, ToM\uc740 \ubbf8\ub798\uc758 \ub3d9\uc791 \uada4\uc801\uc744 \uc608\uce21\ud558\uc5ec \ubaa8\ub378\uc774 \uc7a5\uae30\uc801\uc778 \uacc4\ud68d\uc744 \uc138\uc6b0\uace0 \uc2e4\ud589\ud560 \uc218 \uc788\ub3c4\ub85d \ub3d5\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4. \ub85c\ubd07 \ud314\uc774\ub098 \uc0ac\ub78c \uc190\uc758 \uc6c0\uc9c1\uc784\uc744 \ucd94\uc801\ud558\uc5ec \ubbf8\ub798\uc758 \uada4\uc801\uc744 \uc608\uce21\ud568\uc73c\ub85c\uc368, \ubaa8\ub378\uc740 \uc2dc\uac04\uc801 \ub9e5\ub77d\uc744 \uc774\ud574\ud558\uace0 \ub354\uc6b1 \uc815\ud655\ud55c \ub3d9\uc791 \uacc4\ud68d\uc744 \uc218\ub9bd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.2. Method"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_pt_v3.png", "caption": "Figure 5: An illustration of Alg.\u00a02 to handle videos with camera motions for SoM/ToM generation.", "description": "\uadf8\ub9bc 5\ub294 \uce74\uba54\ub77c \uc6c0\uc9c1\uc784\uc774 \uc788\ub294 \ube44\ub514\uc624\uc5d0\uc11c SoM(Set-of-Mark) \ubc0f ToM(Trace-of-Mark)\uc744 \uc0dd\uc131\ud558\uae30 \uc704\ud55c \uc54c\uace0\ub9ac\uc998 2\uc758 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc54c\uace0\ub9ac\uc998\uc740 \uba3c\uc800 \ube44\ub514\uc624 \ud504\ub808\uc784\uc5d0 \ub300\ud574 CoTracker\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc810 \ucd94\uc801\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c \uc804\uc5ed \uc6c0\uc9c1\uc784\uc774 \uc788\ub294\uc9c0 \ud655\uc778\ud558\uace0, \uc788\ub2e4\uba74 \ud638\ubaa8\uadf8\ub798\ud53c \ubcc0\ud658\uc744 \uc801\uc6a9\ud558\uc5ec \uce74\uba54\ub77c \uc6c0\uc9c1\uc784\uc758 \uc601\ud5a5\uc744 \uc904\uc785\ub2c8\ub2e4. \uc774\ud6c4 \ucd94\uc801\ub41c \uc810\ub4e4\uc744 \uc804\uacbd\uacfc \ubc30\uacbd\uc73c\ub85c \ubd84\ub958\ud558\uace0, K-Means \ud074\ub7ec\uc2a4\ud130\ub9c1\uc744 \ud1b5\ud574 \uc804\uacbd \ucd94\uc801\uc744 \ud074\ub7ec\uc2a4\ud130\ub9c1\ud569\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c \uccab \ubc88\uc9f8 \ud504\ub808\uc784\uc5d0 SoM\uc744 \uc801\uc6a9\ud558\uace0, \uc804\uacbd \ubc0f \ubc30\uacbd \ucd94\uc801\uc744 \ubc18\ud658\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378\uc740 \ub3d9\uc801 \ube44\ub514\uc624 \ub370\uc774\ud130\uc5d0\uc11c\ub3c4 \ud6a8\uacfc\uc801\uc73c\ub85c \uc791\uc5c5 \uc9c0\uc810\uc744 \ucc3e\uace0 \ubbf8\ub798\uc758 \ub3d9\uc791\uc744 \uacc4\ud68d\ud560 \uc218 \uc788\uac8c \ub429\ub2c8\ub2e4.", "section": "3.2. Method"}, {"figure_path": "https://arxiv.org/html/2502.13130/x4.png", "caption": "Figure 6: Overview of Pretraining Data Sources. A diverse collection of datasets including instructional videos (orange), robotics manipulation (green), UI navigation (pink), and multimodal understanding (blue). Note that we count the size of each dataset by the number of image samples. For video and robotics data, we extract the images from the short clips and trajectories, respectively.", "description": "\uadf8\ub9bc 6\uc740 Magma \ubaa8\ub378\uc758 \uc0ac\uc804 \ud6c8\ub828\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uc601\uc5ed\uc758 \ub370\uc774\ud130\uc14b\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uad50\uc721\uc6a9 \ube44\ub514\uc624(\uc8fc\ud669\uc0c9), \ub85c\ubd07 \uc870\uc791(\ub179\uc0c9), UI \ud0d0\uc0c9(\ubd84\ud64d\uc0c9), \ub2e4\uc911 \ubaa8\ub4dc \uc774\ud574(\ud30c\ub780\uc0c9) \ub370\uc774\ud130\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\ub294 \uc774\ubbf8\uc9c0 \uc0d8\ud50c \uc218\ub97c \uae30\uc900\uc73c\ub85c \uacc4\uc0b0\ub429\ub2c8\ub2e4. \ube44\ub514\uc624 \ubc0f \ub85c\ubd07 \ub370\uc774\ud130\uc758 \uacbd\uc6b0 \uc9e7\uc740 \ud074\ub9bd\uacfc \uada4\uc801\uc5d0\uc11c \uc774\ubbf8\uc9c0\ub97c \ucd94\ucd9c\ud569\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ub370\uc774\ud130\ub85c \uad6c\uc131\ub41c Magma \ubaa8\ub378\uc758 \uac15\ub825\ud55c \ub2e4\uc911 \ubaa8\ub4dc \ud559\uc2b5 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Multimodal Agentic Pretraining"}, {"figure_path": "https://arxiv.org/html/2502.13130/x5.png", "caption": "Figure 7: Magma pretraining pipeline. For all training data, texts are tokenized into tokens, while images and videos from different domains are encoded by a shared vision encoder. The resulted discrete and continuous tokens are then fed into a LLM to generate the outputs in verbal, spatial and action types. Our proposed method reconcile the multimodal understanding and action prediction tasks.", "description": "\uadf8\ub9bc 7\uc740 Magma\uc758 \uc0ac\uc804 \ud6c8\ub828 \ud30c\uc774\ud504\ub77c\uc778\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub4e0 \ud6c8\ub828 \ub370\uc774\ud130\uc5d0\uc11c \ud14d\uc2a4\ud2b8\ub294 \ud1a0\ud070\uc73c\ub85c \ubd84\ud560\ub418\uace0, \uc11c\ub85c \ub2e4\ub978 \ub3c4\uba54\uc778\uc758 \uc774\ubbf8\uc9c0\uc640 \ube44\ub514\uc624\ub294 \uacf5\uc720\ub41c \ube44\uc804 \uc778\ucf54\ub354\ub97c \ud1b5\ud574 \uc778\ucf54\ub529\ub429\ub2c8\ub2e4. \uadf8 \uacb0\uacfc \uc0dd\uc131\ub41c \uc774\uc0b0 \ubc0f \uc5f0\uc18d \ud1a0\ud070\uc740 LLM\uc5d0 \uc785\ub825\ub418\uc5b4 \uc5b8\uc5b4, \uacf5\uac04 \ubc0f \ud589\ub3d9 \uc720\ud615\uc758 \ucd9c\ub825\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc81c\uc548\ub41c \ubc29\ubc95\uc740 \ub2e4\uc911 \ubaa8\ub4dc \uc774\ud574\uc640 \ud589\ub3d9 \uc608\uce21 \uc791\uc5c5\uc744 \uc870\uc815\ud569\ub2c8\ub2e4.  \uac04\ub7b5\ud788 \ub9d0\ud574, \ub2e4\uc591\ud55c \ub370\uc774\ud130(\ud14d\uc2a4\ud2b8, \uc774\ubbf8\uc9c0, \ube44\ub514\uc624)\ub97c \uac01\uac01\uc758 \ud615\ud0dc\uc5d0 \ub9de\ucdb0 \ucc98\ub9ac\ud55c \ud6c4, \uc774\ub4e4\uc744 \ud1b5\ud569\ud558\uc5ec LLM\uc5d0 \uc785\ub825\ud558\uace0 \ucd5c\uc885 \uacb0\uacfc\ub97c \ub3c4\ucd9c\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4.", "section": "3. Multimodal Agentic Modeling"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/images/magma_libero.png", "caption": "Figure 8: SimplerEnv performance comparison on Google Robots and Bridge. Magma(OXE) represents our model trained solely on Open-X-Embodiment (OXE)\u00a0[22], while Magma is our pretrained model. Results for each task are averaged across visual matching and variant aggregation scenarios.", "description": "\uadf8\ub9bc 8\uc740 Google Robots \ubc0f Bridge \ud658\uacbd\uc5d0\uc11c SimplerEnv \uc791\uc5c5\uc5d0 \ub300\ud55c Magma \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 OpenVLA \ubaa8\ub378\uacfc \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. Magma(OXE)\ub294 Open-X-Embodiment \ub370\uc774\ud130\uc14b [22]\uc73c\ub85c\ub9cc \ud559\uc2b5\ub41c \ubaa8\ub378\uc774\uace0, Magma\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c \uc0ac\uc804 \ud559\uc2b5\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4. \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \uacb0\uacfc\ub294 \uc2dc\uac01\uc801 \ub9e4\uce6d \ubc0f \ubcc0\ud615 \uc9d1\uacc4 \uc2dc\ub098\ub9ac\uc624\uc5d0 \uac78\uccd0 \ud3c9\uade0\ub41c \uac83\uc785\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ub85c\ubd07 \uc870\uc791 \uc791\uc5c5\uc5d0\uc11c Magma \ubaa8\ub378\uc758 \uc6b0\uc218\ud55c \uc81c\ub85c\uc0f7 \uc77c\ubc18\ud654 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 OpenVLA \ubaa8\ub378\ubcf4\ub2e4 \ud6e8\uc52c \ub354 \ub192\uc740 \uc131\uacf5\ub960\uc744 \ub2ec\uc131\ud558\uc600\uc2b5\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8 (Experiment)"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_spatial_visualizations_v2.png", "caption": "Figure 9: Few-shot finetuning and generalization performance on real robot. On a WidowX robot, we evaluate Magma on 4 tasks including diverse everyday object manipulation.", "description": "\uadf8\ub9bc 9\ub294 \uc2e4\uc81c WidowX \ub85c\ubd07\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \uc77c\uc0c1\uc801\uc778 \ubb3c\uccb4 \uc870\uc791 \uc791\uc5c5\uc744 \ud3ec\ud568\ud55c 4\uac00\uc9c0 \uc791\uc5c5\uc5d0 \ub300\ud574 Magma \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac04\ub7b5\ud558\uac8c \ub9d0\ud558\uba74,  \uc18c\ub7c9\uc758 \ub370\uc774\ud130\ub85c \ubbf8\uc138 \uc870\uc815\uc744 \uc218\ud589\ud55c \ud6c4\uc5d0\ub3c4 Magma \ubaa8\ub378\uc740 \ub2e4\uc591\ud55c \ubb3c\uccb4 \uc870\uc791 \uc791\uc5c5\uc5d0\uc11c \uc6b0\uc218\ud55c \uc77c\ubc18\ud654 \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4.  \uac01 \uc791\uc5c5\uc758 \uc131\uacf5\ub960\uc744 \ube44\uad50\ud558\uc5ec Magma \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uacfc \uac15\ub825\ud568\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2502.13130/x6.png", "caption": "Figure 10: Few-shot finetuning results on the LIBERO simulation benchmark, using 10 trajectories per task for fine-tuning.", "description": "\uadf8\ub9bc 10\uc740 LIBERO \uc2dc\ubbac\ub808\uc774\uc158 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \uba87 \uc0f7 \ubbf8\uc138 \uc870\uc815 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc791\uc5c5\uc5d0 \ub300\ud574 10\uac1c\uc758 \uada4\uc801\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubbf8\uc138 \uc870\uc815\uc744 \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ub85c\ubd07 \uc870\uc791 \uc791\uc5c5\uc5d0 \ub300\ud55c Magma \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uba70, \ud2b9\ud788 \uc81c\ud55c\ub41c \ub370\uc774\ud130\ub85c\ub3c4 \ud6a8\uacfc\uc801\uc73c\ub85c \uc77c\ubc18\ud654\ud560 \uc218 \uc788\uc74c\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.  OpenVLA\uc640 Magma \ubaa8\ub378\uc744 \ube44\uad50\ud558\uc5ec Magma \ubaa8\ub378\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2502.13130/x7.png", "caption": "Figure 11: Spatial evaluation predictions. Spatial reasoning questions are challenging even for GPT-4o but Magma can answer relatively well despite relying on much fewer pretraining data.", "description": "\uadf8\ub9bc 11\uc740 Magma \ubaa8\ub378\uc758 \uacf5\uac04 \ucd94\ub860 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uc8fc\uc5b4\uc9c4 \ubbf8\ub85c\uc5d0\uc11c \uc0c9\uc0c1\uc774 \uc788\ub294 \ube14\ub85d\ub4e4\uc758 \uc758\ubbf8(\ub179\uc0c9: \uc2dc\uc791\uc810, \ube68\uac04\uc0c9: \ub05d\uc810, \ud30c\ub780\uc0c9: \uacbd\ub85c)\ub97c \uc124\uba85\ud558\uace0, \ud30c\ub780\uc0c9 \uacbd\ub85c\ub97c \ub530\ub77c S\uc5d0\uc11c E\uae4c\uc9c0 \uc774\ub3d9\ud560 \ub54c \uc624\ub978\ucabd\uc73c\ub85c \uba87 \ubc88 \ud68c\uc804\ud558\ub294\uc9c0 \ubb3b\ub294 \uc9c8\ubb38\uc5d0 \ub300\ud55c Magma\uc640 GPT-4\uc758 \uc608\uce21 \uacb0\uacfc\ub97c \ube44\uad50\ud569\ub2c8\ub2e4. GPT-4\uc870\ucc28 \uc5b4\ub824\uc6cc\ud558\ub294 \uc774\ub7ec\ud55c \uacf5\uac04 \ucd94\ub860 \ubb38\uc81c\uc5d0 \ub300\ud574 Magma\ub294 \uc0c1\ub300\uc801\uc73c\ub85c \uc801\uc740 \uc591\uc758 \uc0ac\uc804 \ud559\uc2b5 \ub370\uc774\ud130\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \uc815\ub2f5\uc744 \uc798 \ub9de\ucda5\ub2c8\ub2e4. \uc774\ub294 Magma \ubaa8\ub378\uc758 \uac15\ub825\ud55c \uacf5\uac04 \ucd94\ub860 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc785\ub2c8\ub2e4.", "section": "5.2 \uacf5\uac04 \ucd94\ub860 \ud3c9\uac00"}, {"figure_path": "https://arxiv.org/html/2502.13130/x8.png", "caption": "Figure 12: Training samples in our Magma-PT-UI. It covers a wide range of action grounding and UI understanding tasks including: (a) Given the bounding box or point coordinates as the query, assistant should return the natural language description or the content. (b) Given the natural language or the exact content as the query, assistant should return the value of the bounding box coordinates.. (c) Given the natural language as the query, assistant should return the value of the point coordinate. (d) Widget captioning. (e) UI summarization.", "description": "\uadf8\ub9bc 12\ub294 Magma-PT-UI\uc758 \ud559\uc2b5 \ub370\uc774\ud130 \uc0d8\ud50c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uc561\uc158 \uadf8\ub77c\uc6b4\ub529 \ubc0f UI \uc774\ud574 \uc791\uc5c5\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. (a)\ub294 \uacbd\uacc4 \uc0c1\uc790 \ub610\ub294 \uc810 \uc88c\ud45c\ub97c \uc9c8\uc758\ub85c \uc8fc\uba74, \uc5b4\uc2dc\uc2a4\ud134\ud2b8\uac00 \uc790\uc5f0\uc5b4 \uc124\uba85 \ub610\ub294 \ub0b4\uc6a9\uc744 \ubc18\ud658\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (b)\ub294 \uc790\uc5f0\uc5b4 \ub610\ub294 \uc815\ud655\ud55c \ub0b4\uc6a9\uc744 \uc9c8\uc758\ub85c \uc8fc\uba74, \uc5b4\uc2dc\uc2a4\ud134\ud2b8\uac00 \uacbd\uacc4 \uc0c1\uc790 \uc88c\ud45c\uc758 \uac12\uc744 \ubc18\ud658\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (c)\ub294 \uc790\uc5f0\uc5b4\ub97c \uc9c8\uc758\ub85c \uc8fc\uba74 \uc5b4\uc2dc\uc2a4\ud134\ud2b8\uac00 \uc810 \uc88c\ud45c\uc758 \uac12\uc744 \ubc18\ud658\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (d)\ub294 \uc704\uc82f \ucea1\uc158\uc744, (e)\ub294 UI \uc694\uc57d\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubcf8 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \uc9c8\ubb38\uacfc \uadf8\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \uc751\ub2f5\uc744 \ubcf4\uc5ec\uc90c\uc73c\ub85c\uc368 Magma \ubaa8\ub378\uc758 \uba40\ud2f0\ubaa8\ub2ec \uc774\ud574 \ub2a5\ub825\uacfc \uc561\uc158 \uadf8\ub77c\uc6b4\ub529 \ub2a5\ub825\uc744 \ubcf4\ub2e4 \uba85\ud655\ud558\uac8c \uc124\uba85\ud569\ub2c8\ub2e4.", "section": "3.2. Method"}, {"figure_path": "https://arxiv.org/html/2502.13130/x9.png", "caption": "Figure 13: Action distributions in three types of action-oriented pretraining datasets. (a) UI Navigation; (b) Robotic Manipulation; (c) Instructional Videos.", "description": "\uadf8\ub9bc 13\uc740 \uc138 \uac00\uc9c0 \uc791\uc5c5 \uc911\uc2ec \uc0ac\uc804 \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub3d9\uc0ac \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  (a)\ub294 UI \ud0d0\uc0c9, (b)\ub294 \ub85c\ubd07 \uc870\uc791, (c)\ub294 \uad50\uc721\uc6a9 \ube44\ub514\uc624\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uac01 \uadf8\ub798\ud504\ub294 \ud574\ub2f9 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uac00\uc7a5 \ud754\ud558\uac8c \ub098\ud0c0\ub098\ub294 \ub3d9\uc0ac\ub4e4\uc744 \ube48\ub3c4 \uc21c\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \ub9c9\ub300 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. UI \ud0d0\uc0c9 \ub370\uc774\ud130\uc14b\uc5d0\uc11c\ub294 '\uc120\ud0dd', '\uc774\ub3d9', '\uac80\uc0c9' \ub4f1\uc758 \ub3d9\uc0ac\uac00 \ub9ce\uc774 \ub098\ud0c0\ub098\uba70, \ub85c\ubd07 \uc870\uc791 \ub370\uc774\ud130\uc14b\uc5d0\uc11c\ub294 '\ub193\uae30', '\uc62e\uae30\uae30', '\ubc00\uae30' \ub4f1\uc758 \ubb3c\ub9ac\uc801 \uc870\uc791 \ub3d9\uc0ac\uac00 \ub9ce\uc774 \ub098\ud0c0\ub0a9\ub2c8\ub2e4. \uad50\uc721\uc6a9 \ube44\ub514\uc624 \ub370\uc774\ud130\uc14b\uc5d0\uc11c\ub294 \ub354\uc6b1 \ub2e4\uc591\ud55c \ub3d9\uc0ac\uac00 \ub098\ud0c0\ub098\ub294\ub370, \uc774\ub294 \ube44\ub514\uc624\uc758 \uc791\uc5c5\uc774 \ub354 \ubcf5\uc7a1\ud558\uace0 \ub2e4\uc591\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 Magma \ubaa8\ub378\uc758 \uc0ac\uc804 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5\uc744 \ubcf4\uc5ec\uc8fc\uba70, \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc791\uc5c5 \uc720\ud615\uacfc \ud2b9\uc131\uc744 \ubc18\uc601\ud558\ub294 \ub3d9\uc0ac \ubd84\ud3ec\ub97c \ud1b5\ud574 Magma \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \uc791\uc5c5\uc744 \uc218\ud589\ud560 \uc218 \uc788\ub294 \ub2a5\ub825\uc744 \uac16\ucd94\ub3c4\ub85d \ub3d5\ub294 \uac83\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.2 \ubc29\ubc95\ub860"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/openvla_hotdog.png", "caption": "Figure 14: Real robot setup. Magma is deployed on a WidowX 250 robot arm to perform a sequence of kitchen manipulation tasks including object pick-place and soft manipulation.", "description": "\uadf8\ub9bc 14\ub294 \uc2e4\uc81c \ub85c\ubd07 \uc2e4\ud5d8 \ud658\uacbd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. WidowX 250 \ub85c\ubd07 \ud314\uc5d0 Magma \ubaa8\ub378\uc744 \uc801\uc6a9\ud558\uc5ec \uc8fc\ubc29 \uad00\ub828 \uc870\uc791 \uc791\uc5c5(\ubb3c\uac74 \uc9d1\uc5b4 \uc62e\uae30\uae30, \ubd80\ub4dc\ub7ec\uc6b4 \uc870\uc791 \ub4f1)\uc744 \uc218\ud589\ud558\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\ub294 \ub85c\ubd07 \ud314\uc774 \uc5ec\ub7ec \uac00\uc9c0 \uc8fc\ubc29 \ub3c4\uad6c\ub4e4\uc744 \uc870\uc791\ud558\ub294 \uc5f0\uc18d\uc801\uc778 \ub3d9\uc791\ub4e4\uc774 \ub098\uc5f4\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 Magma \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \ubb3c\uccb4 \uc870\uc791 \uc791\uc5c5\ub4e4\uc744 \uc131\uacf5\uc801\uc73c\ub85c \uc218\ud589\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.", "section": "3. Multimodal Agentic Modeling"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/openvla_mushroom.png", "caption": "Figure 15: Examples for mobile UI navigation sample. We prompt the model with two tasks: \u201cWhat\u2019s the weather like in Tokyo\u201d and \u201cInstall app \u2018Instagram\u2019\u201d. The model take actions sequentially given the new observation and history action information.", "description": "\uadf8\ub9bc 15\ub294 \ubaa8\ubc14\uc77c UI \ud0d0\uc0c9 \uc0d8\ud50c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\uc5d0\uac8c \"\ub3c4\ucfc4 \ub0a0\uc528\ub294 \uc5b4\ub5bb\uc2b5\ub2c8\uae4c?\"\uc640 \"Instagram \uc571 \uc124\uce58\"\ub77c\ub294 \ub450 \uac00\uc9c0 \uc791\uc5c5\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4. \ubaa8\ub378\uc740 \uc0c8\ub85c\uc6b4 \uad00\ucc30 \uacb0\uacfc\uc640 \uc774\uc804 \uc791\uc5c5 \uc815\ubcf4\ub97c \ubc14\ud0d5\uc73c\ub85c \uc21c\ucc28\uc801\uc73c\ub85c \uc791\uc5c5\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.  \uac01 \uc774\ubbf8\uc9c0\ub294 \uc0ac\uc6a9\uc790 \uc778\ud130\ud398\uc774\uc2a4\uc758 \uc2a4\ud06c\ub9b0\uc0f7\uacfc \uc218\ud589\ub41c \ub2e8\uacc4\ub97c \ubcf4\uc5ec\uc8fc\uace0, \ubaa8\ub378\uc774 \uc5b4\ub5a4 \uc694\uc18c\ub97c \uc120\ud0dd\ud558\uac70\ub098, \uc5b4\ub5a4 \ud14d\uc2a4\ud2b8\ub97c \uc785\ub825\ud574\uc57c \ud558\ub294\uc9c0 \uc2dc\uac01\uc801\uc73c\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378\uc774 \uc2dc\uac01\uc801 \uc815\ubcf4\uc640 \uacfc\uac70 \uc0c1\ud638\uc791\uc6a9 \uae30\ub85d\uc744 \ud65c\uc6a9\ud558\uc5ec \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub294 \uacfc\uc815\uc744 \uc774\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.1. Zero-Shot Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_hotdog.png", "caption": "a Robot policy rollout for task \u201cPut the sausage to hotdog\u201d for OpenVLA model. (Failure)", "description": "\uc774 \uadf8\ub9bc\uc740 OpenVLA \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \"\uc18c\uc2dc\uc9c0\ub97c \ud56b\ub3c4\uadf8\uc5d0 \ub123\uae30\" \uc791\uc5c5\uc744 \uc218\ud589\ud588\uc744 \ub54c \uc2e4\ud328\ud55c \ub85c\ubd07\uc758 \ub3d9\uc791\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ub85c\ubd07 \uc554\uc774 \uc18c\uc2dc\uc9c0\ub97c \uc9d1\uc73c\ub824\uace0 \uc2dc\ub3c4\ud558\uc9c0\ub9cc \uc131\uacf5\ud558\uc9c0 \ubabb\ud558\uace0 \uc5ec\ub7ec \ubc88 \uc2e4\ud328\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc77c\ub828\uc758 \uc774\ubbf8\uc9c0\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. OpenVLA \ubaa8\ub378\uc774 \uc774 \uc791\uc5c5\uc5d0 \ud544\uc694\ud55c \uc815\ubc00\ud55c \uacf5\uac04\uc801 \uc774\ud574\uc640 \uacc4\ud68d \ub2a5\ub825\uc774 \ubd80\uc871\ud558\uc5ec \uc2e4\ud328\ud588\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "5.1. \uc2e4\ud5d8: \uc791\uc6a9 \ub2a5\ub825 \ud3c9\uac00"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_mushroom.png", "caption": "b Robot policy rollout for task \u201cPick up the mushroom to the pot\u201d for OpenVLA model. (Failure)", "description": "\uc774 \uadf8\ub9bc\uc740 OpenVLA \ubaa8\ub378\uc774 \"\ubc84\uc12f\uc744 \ub0c4\ube44\uc5d0 \ub123\uc73c\uc138\uc694\"\ub77c\ub294 \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub294 \ub3d9\uc548 \ub85c\ubd07\uc758 \ub3d9\uc791\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ub85c\ubd07 \uc554\uc774 \ubc84\uc12f\uc744 \uc9d1\uc73c\ub824\uace0 \uc2dc\ub3c4\ud558\ub294 \uc5ec\ub7ec \ub2e8\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc77c\ub828\uc758 \uc774\ubbf8\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \ub85c\ubd07\uc740 \ubc84\uc12f\uc744 \uc131\uacf5\uc801\uc73c\ub85c \uc9d1\uac70\ub098 \ub0c4\ube44\uc5d0 \ub123\uc9c0 \ubabb\ud558\uc5ec \uc791\uc5c5\uc5d0 \uc2e4\ud328\ud569\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 OpenVLA \ubaa8\ub378\uc758 \uacf5\uac04\uc801 \ucd94\ub860 \ubc0f \uacc4\ud68d \ub2a5\ub825\uc758 \ud55c\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.1. \uc2e4\ud5d8: \uc791\uc6a9 \ub2a5\ub825 \ud3c9\uac00"}]