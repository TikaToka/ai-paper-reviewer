{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper introduces RLHF, a crucial technique used in reinforcement learning for aligning language models with human preferences, which is foundational to the discussed RFT method."}, {"fullname_first_author": "Kehua Feng", "paper_title": "SciKnowEval: Evaluating multi-level scientific knowledge of large language models", "publication_date": "2024-01-01", "reason": "This paper introduces SciKnowEval, the benchmark dataset used for evaluating the performance of the proposed OpenRFT model, making it a key component of the experimental setup."}, {"fullname_first_author": "Trung Quoc Luong", "paper_title": "Reasoning with reinforced fine-tuning", "publication_date": "2024-01-01", "reason": "This paper introduces ReFT, a closely related method to RFT that shares similarities and inspires the proposed OpenRFT model, providing valuable context and comparison."}, {"fullname_first_author": "Yu Zhao", "paper_title": "Marco-01: Towards open reasoning models for open-ended solutions", "publication_date": "2024-01-01", "reason": "This paper introduces Marco-01, a model that is also attempting to replicate and build upon the OpenAI's model, making it relevant for comparison and contextualization within the field."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-06", "reason": "This paper introduces the PPO algorithm, which is the core reinforcement learning algorithm used in OpenRFT, playing a key role in the model's training and optimization."}]}