{"references": [{"fullname_first_author": "Achiam, J.", "paper_title": "Constrained policy optimization", "publication_date": "2017-00-00", "reason": "This paper introduces constrained policy optimization, a fundamental method used in safe reinforcement learning, which is directly relevant to the paper's approach to safe LLM alignment."}, {"fullname_first_author": "Sootla, A.", "paper_title": "Saut\u00e9 RL: Almost surely safe reinforcement learning using state augmentation", "publication_date": "2022-00-00", "reason": "This paper presents the state augmentation technique that is extended and applied to LLMs in the paper's approach, providing theoretical guarantees for almost sure safety."}, {"fullname_first_author": "Hern\u00e1ndez-Lerma, O.", "paper_title": "Discrete-time Markov control processes with discounted unbounded costs: optimality criteria", "publication_date": "1992-00-00", "reason": "This foundational paper in Markov decision processes provides theoretical underpinnings for analyzing optimality and convergence in constrained MDPs, which are crucial aspects of the proposed method."}, {"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This highly influential paper introduces RLHF, a widely used technique for aligning LLMs.  It is referenced to contrast the proposed method which avoids retraining."}, {"fullname_first_author": "Stiennon, N.", "paper_title": "Learning to summarize with human feedback", "publication_date": "2020-00-00", "reason": "This paper is a key reference showing the use of RLHF for LLM alignment, providing context for the paper's focus on inference-time alignment as a more efficient alternative."}]}