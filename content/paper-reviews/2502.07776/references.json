{"references": [{"fullname_first_author": "Vaswani et al.", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduces the Transformer architecture, which is fundamental to many modern LLMs and the prompt caching techniques discussed in the paper."}, {"fullname_first_author": "Zheng et al.", "paper_title": "SGLang: Efficient execution of structured language model programs", "publication_date": "2024-12-01", "reason": "This paper is a key reference as it proposes prompt caching, a core optimization technique analyzed in this paper."}, {"fullname_first_author": "Gim et al.", "paper_title": "Prompt cache: Modular attention reuse for low-latency inference", "publication_date": "2024-12-01", "reason": "This paper also discusses prompt caching, providing another important perspective on this technique and its implications."}, {"fullname_first_author": "Kocher et al.", "paper_title": "Spectre attacks: Exploiting speculative execution", "publication_date": "2019-05-01", "reason": "This seminal work details Spectre attacks, a class of side-channel attacks that the current paper's timing attack builds upon conceptually."}, {"fullname_first_author": "Bernstein", "paper_title": "Cache-timing attacks on AES", "publication_date": "2005-04-14", "reason": "This foundational work is essential as it is among the first to thoroughly explore and document cache-timing attacks which inform the core methodology for the privacy attacks presented here."}]}