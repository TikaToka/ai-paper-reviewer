{"references": [{"fullname_first_author": "Zhangir Azerbayev", "paper_title": "Llemma: An open language model for mathematics", "publication_date": "2024-00-00", "reason": "This paper introduces a novel open-source language model specifically designed for mathematical reasoning, which is highly relevant to the LIMO model's focus on enhancing mathematical reasoning capabilities in LLMs."}, {"fullname_first_author": "Bradley Brown", "paper_title": "Large language monkeys: Scaling inference compute with repeated sampling", "publication_date": "2024-00-00", "reason": "This paper explores scaling inference computation in large language models, a key aspect of LIMO's approach to efficiently eliciting complex reasoning abilities through extended reasoning chains."}, {"fullname_first_author": "Tianzhe Chu", "paper_title": "SFT memorizes, RL generalizes: A comparative study of foundation model post-training", "publication_date": "2025-00-00", "reason": "This paper directly addresses the common belief that supervised fine-tuning primarily leads to memorization, which LIMO challenges by demonstrating that complex reasoning abilities can be effectively elicited with surprisingly few examples."}, {"fullname_first_author": "Daya Guo", "paper_title": "DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning", "publication_date": "2025-00-00", "reason": "DeepSeek-R1 is a strong baseline model compared against LIMO, and this paper's focus on reinforcement learning for enhancing reasoning capabilities provides a contrasting approach to LIMO's data-efficient method."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the MATH dataset", "publication_date": "2021-03-03", "reason": "The MATH dataset, introduced in this paper, is a benchmark used in the LIMO evaluation, making this paper crucial for understanding the context and performance metrics of the LIMO model."}]}