[{"figure_path": "https://arxiv.org/html/2412.07769/extracted/6059664/Figures/llavamed_spider.png", "caption": "Figure 1: Model Performance Comparison on BiMed-MBench: These comparisons are made across different categories, including CT, MRI, CXR, Histology, Gross, and their Arabic counterparts (CT_ar, MRI_ar, CXR_ar, Histology_ar, Gross_ar). The models compared are LLaVA-pp, LLaVA-Med, BiMediX2, Dragonfly-Med, MiniGPT-Med, and BiomedGPT. Each axis represents the performance score in a specific category, allowing for a visual comparison of how each model performs in bilingual medical contexts.", "description": "\uc774 \uadf8\ub9bc\uc740 BiMed-MBench\uc5d0\uc11c \uc5ec\ub7ec \uc758\ub8cc LMM\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ub808\uc774\ub354 \ucc28\ud2b8\uc785\ub2c8\ub2e4. CT, MRI, CXR, \uc870\uc9c1\ud559, \uadf8\ub85c\uc2a4 \uc774\ubbf8\uc9c0\uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uc758\ub8cc \uc601\uc0c1 \ubc94\uc8fc\uc640 \uac01\uac01\uc758 \uc544\ub78d\uc5b4 \ubc94\uc8fc\uc5d0\uc11c LLaVA-pp, LLaVA-Med, BiMediX2, Dragonfly-Med, MiniGPT-Med, BiomedGPT \ub4f1\uc758 \ubaa8\ub378 \uc131\ub2a5\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. \uac01 \ucd95\uc740 \ud2b9\uc815 \ubc94\uc8fc\uc5d0\uc11c\uc758 \uc131\ub2a5 \uc810\uc218\ub97c \ub098\ud0c0\ub0b4\uba70, \uac01 \ubaa8\ub378\uc774 \uc774\uc911 \uc5b8\uc5b4 \uc758\ub8cc \ud658\uacbd\uc5d0\uc11c \uc5bc\ub9c8\ub098 \uc798 \uc218\ud589\ub418\ub294\uc9c0 \uc2dc\uac01\uc801\uc73c\ub85c \ube44\uad50\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.", "section": "Technical Report"}, {"figure_path": "https://arxiv.org/html/2412.07769/x1.png", "caption": "Figure 2: BiMediX2: Overall Architecture Our model is designed for medical image analysis and bilingual multi-turn conversations. Medical images are processed through a Vision Encoder and aligned with a Projector, while the text inputs are tokenized using the default tokenizer. The resulting tokens are then passed into the language model (Meta Llama 3.1) to generate responses in the prompted language. We only train the language model using LoRA adapters, while the projector is finetuned for medical image-text alignment. A robust data generation framework translates an English data corpus into Arabic using GPT-4o, with verification by a medical expert to ensure accurate and contextually appropriate translations. This approach supports effective training and benchmarking in a bilingual context.", "description": "BiMediX2\ub294 \uc758\ub8cc \uc774\ubbf8\uc9c0 \ubd84\uc11d\uacfc \uc774\uc911 \uc5b8\uc5b4 \ub2e4\uc911 \ub300\ud654\ub97c \uc704\ud574 \uc124\uacc4\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4. \uc758\ub8cc \uc774\ubbf8\uc9c0\ub294 Vision Encoder\ub97c \ud1b5\ud574 \ucc98\ub9ac\ub418\uace0 Projector\uc640 \uc815\ub82c\ub418\ub294 \ubc18\uba74, \ud14d\uc2a4\ud2b8 \uc785\ub825\uc740 \uae30\ubcf8 \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud1a0\ud070\ud654\ub429\ub2c8\ub2e4. \uacb0\uacfc \ud1a0\ud070\uc740 \uc5b8\uc5b4 \ubaa8\ub378(Meta Llama 3.1)\ub85c \uc804\ub2ec\ub418\uc5b4 \ud504\ub86c\ud504\ud2b8\ub41c \uc5b8\uc5b4\ub85c \uc751\ub2f5\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc5b8\uc5b4 \ubaa8\ub378\uc740 LoRA \uc5b4\ub311\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub418\uace0 \ud504\ub85c\uc81d\ud130\ub294 \uc758\ub8cc \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8 \uc815\ub82c\uc744 \uc704\ud574 \ubbf8\uc138 \uc870\uc815\ub429\ub2c8\ub2e4. \uac15\ub825\ud55c \ub370\uc774\ud130 \uc0dd\uc131 \ud504\ub808\uc784\uc6cc\ud06c\ub294 \uc601\uc5b4 \ub370\uc774\ud130 \ucf54\ud37c\uc2a4\ub97c GPT-40\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc544\ub78d\uc5b4\ub85c \ubc88\uc5ed\ud558\uace0 \uc758\ub8cc \uc804\ubb38\uac00\uc758 \uac80\uc99d\uc744 \ud1b5\ud574 \uc815\ud655\ud558\uace0 \ubb38\ub9e5\uc801\uc73c\ub85c \uc801\uc808\ud55c \ubc88\uc5ed\uc744 \ubcf4\uc7a5\ud569\ub2c8\ub2e4. \uc774 \uc811\uadfc \ubc29\uc2dd\uc740 \uc774\uc911 \uc5b8\uc5b4 \ucee8\ud14d\uc2a4\ud2b8\uc5d0\uc11c \ud6a8\uacfc\uc801\uc778 \ud6c8\ub828 \ubc0f \ubca4\uce58\ub9c8\ud0b9\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4.", "section": "BIMEDIX2"}, {"figure_path": "https://arxiv.org/html/2412.07769/x2.png", "caption": "Figure 3: State of the art comparison of models in Clinical LLM Benchmarks", "description": "\uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc784\uc0c1 LLM \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc5ec\ub7ec \uc758\ub8cc LLM \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. BiMediX2 70B\ub294 PubMedQA, MedQA, MedMCQA, USMLE, Medical MMLU\ub97c \ud3ec\ud568\ud55c \ub300\ubd80\ubd84\uc758 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ucd5c\uace0 \uc810\uc218\ub97c \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub294 BiMediX2\uac00 \uc758\ub8cc \ubd84\uc57c\uc5d0\uc11c \ub2e4\ub978 \ubaa8\ub378\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "3 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2412.07769/x3.png", "caption": "Figure 4: Performance comparison on UPHILL OpenQA (Kaur et\u00a0al. (2023)), assessing the model\u2019s ability to address false medical claims at different presupposition levels.", "description": "\uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc758\ub8cc LLM\uc758 UPHILL OpenQA \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. UPHILL OpenQA\ub294 \ub2e4\uc591\ud55c \ub2e8\uacc4\uc758 \uc804\uc81c\ub97c \ud3ec\ud568\ud558\ub294 \uac74\uac15 \uad00\ub828 \uc9c8\ubb38\uc744 \ucc98\ub9ac\ud560 \ub54c LLM\uc758 \uc0ac\uc2e4\uc801 \uc815\ud655\uc131\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. BiMediX2 70B\ub294 60.6%\uc758 \ucd5c\uace0 \uc804\uccb4 \uc0ac\uc2e4\uc801 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud588\uc73c\uba70, BiMediX2 8B(56.1%)\uac00 \uadf8 \ub4a4\ub97c \uc774\uc5c8\uc2b5\ub2c8\ub2e4. GPT-4(51.5%), Meditron 70B(49.6%), Med42(53.5%)\uc640 \uac19\uc740 \ub2e4\ub978 \ubaa8\ub378\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub6f0\uc5b4\ub0ac\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc758\ub8cc \uad00\ub828 \ud5c8\uc704 \uc8fc\uc7a5\uc744 \uad6c\ubcc4\ud558\uace0 \uc218\uc815\ud558\ub294 BiMediX2\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 RESULTS"}, {"figure_path": "https://arxiv.org/html/2412.07769/x9.png", "caption": "Figure 5: Qualitative Examples of our BiMediX2 for Medical Image Understanding in a Conversational Context.", "description": "\uc774 \uadf8\ub9bc\uc740 \ub300\ud654\ud615 \uc0c1\ud669\uc5d0\uc11c BiMediX2\uc758 \uc758\ub8cc \uc774\ubbf8\uc9c0 \uc774\ud574 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc815\uc131\uc801 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uc0c1\ub2e8 \ubd80\ubd84\uc740 \uc694\ucd94\uc758 \uc2dc\uc0c1 CT \uc2a4\uce94\uacfc \uad00\ub828\ub41c \ub300\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\uc740 \uc2a4\uce94 \uc720\ud615\uc744 \uc2dd\ubcc4\ud558\uace0 \ucc99\ucd94\uc758 \uc544\ub7ab\ubd80\ubd84\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd98 \uc2e0\uccb4\uc758 \uc218\uc9c1 \ub2e8\uba74\uc774\ub77c\uace0 \uc124\uba85\ud569\ub2c8\ub2e4. \uc774\uc0c1 \uc5ec\ubd80\ub97c \ubb3b\uc790 \ubaa8\ub378\uc740 L4 \ucc99\ucd94\uc758 \uace8\uc808\uc744 \uc815\ud655\ud558\uac8c \uc2dd\ubcc4\ud558\uace0 \uc678\uc0c1\uc774\ub098 \uc2a4\ud2b8\ub808\uc2a4\uc640 \uac19\uc740 \uc7a0\uc7ac\uc801\uc778 \uc6d0\uc778\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \ud558\ub2e8 \ubd80\ubd84\uc740 \uc67c\ucabd \ub09c\uc18c\uc758 \uceec\ub7ec \ub3c4\ud50c\ub7ec \ucd08\uc74c\ud30c \uc2a4\uce94\uc744 \ubcf4\uc5ec\uc8fc\uace0, \uc67c\ucabd \ub09c\uc18c \ub0ad\uc885\uc774\ub77c\ub294 \uc7a0\uc7ac\uc801 \uc774\uc0c1\uc744 \uc2dd\ubcc4\ud569\ub2c8\ub2e4. \ubaa8\ub378\uc740 \uc601\uc0c1 \uae30\uc220\uc744 \uc124\uba85\ud558\uace0, \uc7a5\uae30\ub97c \uba85\uba85\ud558\uace0, \uac80\ucd9c\ub41c \uc774\uc0c1\uc744 \ub17c\uc758\ud558\uba74\uc11c \ucd94\uac00 \ud3c9\uac00\uc758 \ud544\uc694\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc608\uc2dc\ub294 BiMediX2\uac00 \ubcf5\uc7a1\ud55c \uc758\ub8cc \uc601\uc0c1\uc744 \ud574\uc11d\ud558\uace0 \uc784\uc0c1 \uc758\uc0ac \uacb0\uc815\uc744 \uc9c0\uc6d0\ud558\ub294 \uc720\ub4dd\ud55c \ub2f5\ubcc0\uc744 \uc81c\uacf5\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 \uacb0\uacfc"}]