{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-14", "reason": "This paper details the architecture and capabilities of GPT-4, the foundation of BiMediX2's language model, and showcases its general abilities relevant to building powerful medical models."}, {"fullname_first_author": "Chunyuan Li", "paper_title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day", "publication_date": "2023-06-02", "reason": "This work introduces LLaVA-Med, a large language and vision assistant for biomedicine which heavily influenced BiMediX2, especially in dataset curation and benchmark evaluations for VQA and report generation/summarization tasks."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning Transferable Visual Models From Natural Language Supervision", "publication_date": "2021-03-01", "reason": "This foundational work introduces CLIP, the vision encoder utilized in BiMediX2, enabling effective visual representation learning for the model's multimodal capabilities."}, {"fullname_first_author": "Kai Zhang", "paper_title": "A Generalist Vision\u2013Language Foundation Model for Diverse Biomedical Tasks", "publication_date": "2024-01-01", "reason": "Introducing BioMedGPT, it establishes a new standard for multimodal medical models and serves as a key comparison point and inspiration for the development of BiMediX2."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models", "publication_date": "2021-06-10", "reason": "This paper introduces LoRA, the parameter-efficient fine-tuning technique employed by BiMediX2 for adapting its language model while maintaining computational efficiency."}]}