[{"figure_path": "https://arxiv.org/html/2501.04001/x2.png", "caption": "Figure 1: Illustration of capabilities of our proposed Sa2VA. (a). Given a video, Sa2VA is able to segment the referred object and understand the whole scene. (b).Sa2VA supports image conversation, video conversation, image referring segmentation, video referring segmentation, and grounded caption generation with single-shot instruction-tuning. (c).Sa2VA achieves strong results on multiple images, video referring segmentation, and chat benchmarks compared with existing MLLMs, such as GLaMM\u00a0[66] and OMG-LLaVA\u00a0[99].", "description": "\ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 Sa2VA\uc758 \uae30\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4. (a)\ub294 Sa2VA\uac00 \ube44\ub514\uc624\uc5d0\uc11c \uc5b8\uae09\ub41c \uac1d\uccb4\ub97c \ubd84\ud560\ud558\uace0 \uc804\uccb4 \uc7a5\uba74\uc744 \uc774\ud574\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (b)\ub294 Sa2VA\uac00 \uc774\ubbf8\uc9c0 \ub300\ud654, \ube44\ub514\uc624 \ub300\ud654, \uc774\ubbf8\uc9c0 \ucc38\uc870 \ubd84\ud560, \ube44\ub514\uc624 \ucc38\uc870 \ubd84\ud560 \ubc0f \ub2e8\uc77c \uc0f7 \uc9c0\uc2dc\uc5b4 \ubbf8\uc138 \uc870\uc815\uc744 \ud1b5\ud55c \uae30\ubc18 \ucea1\uc158 \uc0dd\uc131\uc744 \uc9c0\uc6d0\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (c)\ub294 GLaMM [66] \ubc0f OMG-LLaVA [99]\uc640 \uac19\uc740 \uae30\uc874 MLLM\uacfc \ube44\uad50\ud558\uc5ec Sa2VA\uac00 \uc5ec\ub7ec \uc774\ubbf8\uc9c0, \ube44\ub514\uc624 \ucc38\uc870 \ubd84\ud560 \ubc0f \ucc44\ud305 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uac15\ub825\ud55c \uacb0\uacfc\ub97c \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.04001/x3.png", "caption": "Figure 2: Our proposed Sa2VA model. The model first encodes the input texts, visual prompts, images, and videos into token embeddings. These tokens are then processed through a large language model (LLM). The output text tokens are used to generate the [SEG] token and associated language outputs. The SAM-2 decoder receives the image and video features from the SAM-2 encoder, along with the [SEG] token, to generate corresponding image and video masks.", "description": "\uadf8\ub9bc 2\ub294 \uc81c\uc548\ub41c Sa2VA \ubaa8\ub378\uc758 \uad6c\uc870\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Sa2VA \ubaa8\ub378\uc740 \ud14d\uc2a4\ud2b8, \ube44\uc8fc\uc5bc \ud504\ub86c\ud504\ud2b8, \uc774\ubbf8\uc9c0, \ube44\ub514\uc624\ub97c \ud1a0\ud070 \uc784\ubca0\ub529\uc73c\ub85c \uc778\ucf54\ub529\ud558\ub294 \uac83\uc73c\ub85c \uc2dc\uc791\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ud1a0\ud070\ub4e4\uc740 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc744 \ud1b5\ud574 \ucc98\ub9ac\ub429\ub2c8\ub2e4. \ucd9c\ub825 \ud14d\uc2a4\ud2b8 \ud1a0\ud070\uc740 [SEG] \ud1a0\ud070\uacfc \uc5f0\uad00\ub41c \uc5b8\uc5b4 \ucd9c\ub825\uc744 \uc0dd\uc131\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. SAM-2 \ub514\ucf54\ub354\ub294 SAM-2 \uc778\ucf54\ub354\ub85c\ubd80\ud130 \uc774\ubbf8\uc9c0 \ubc0f \ube44\ub514\uc624 \uae30\ub2a5\uacfc [SEG] \ud1a0\ud070\uc744 \ud568\uaed8 \ubc1b\uc544 \ud574\ub2f9 \uc774\ubbf8\uc9c0 \ubc0f \ube44\ub514\uc624 \ub9c8\uc2a4\ud06c\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uc989, \ub2e4\uc591\ud55c \uc720\ud615\uc758 \uc785\ub825 \ub370\uc774\ud130(\ud14d\uc2a4\ud2b8, \uc774\ubbf8\uc9c0, \ube44\ub514\uc624)\ub97c \ud1a0\ud070\uc73c\ub85c \ubcc0\ud658\ud558\uace0, LLM\uc744 \ud1b5\ud574 \uc758\ubbf8\ub97c \ubd84\uc11d\ud558\uc5ec, \ucd5c\uc885\uc801\uc73c\ub85c \uc774\ubbf8\uc9c0\uc640 \ube44\ub514\uc624\uc5d0 \ub300\ud55c \uc138\ubc00\ud55c \uc601\uc5ed\uc744 \uc9c0\uc815\ud558\ub294 \ub9c8\uc2a4\ud06c\ub97c \uc0dd\uc131\ud558\ub294 \ubaa8\ub378\uc785\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.04001/x4.png", "caption": "Figure 3: Data annotation pipeline. Our proposed automatic data annotation pipeline consists of three stages: object/part-level, scene-level, and video-level text expression annotation. We use different colors in the final expression to highlight the information derived from each stage. Best view on screen and zoom out.", "description": "\uadf8\ub9bc 3\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \uc790\ub3d9 \ub370\uc774\ud130 \uc8fc\uc11d \ud30c\uc774\ud504\ub77c\uc778\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud30c\uc774\ud504\ub77c\uc778\uc740 \uac1d\uccb4/\ubd80\ubd84 \uc218\uc900, \uc7a5\uba74 \uc218\uc900, \ube44\ub514\uc624 \uc218\uc900\uc758 \uc138 \uac00\uc9c0 \ub2e8\uacc4\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \uac01 \ub2e8\uacc4\uc5d0\uc11c \uc0dd\uc131\ub41c \ud14d\uc2a4\ud2b8 \ud45c\ud604\uc740 \ucd5c\uc885 \ud45c\ud604\uc5d0\uc11c \uc11c\ub85c \ub2e4\ub978 \uc0c9\uc0c1\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uac01 \ub2e8\uacc4\uc5d0\uc11c \ud30c\uc0dd\ub41c \uc815\ubcf4\ub97c \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac1d\uccb4/\ubd80\ubd84 \uc218\uc900 \uc8fc\uc11d\uc5d0\uc11c\ub294 \uac1d\uccb4\uc758 \ud2b9\uc9d5\uc744 \uc790\uc138\ud558\uac8c \uc124\uba85\ud569\ub2c8\ub2e4. \uc7a5\uba74 \uc218\uc900 \uc8fc\uc11d\uc5d0\uc11c\ub294 \uac1d\uccb4\uc640 \uc8fc\ubcc0 \ud658\uacbd\uacfc\uc758 \uad00\uacc4\ub97c \uc124\uba85\ud569\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c \ube44\ub514\uc624 \uc218\uc900 \uc8fc\uc11d\uc5d0\uc11c\ub294 \ube44\ub514\uc624 \uc804\uccb4\uc5d0 \uac78\uccd0 \uac1d\uccb4\uc758 \uc6c0\uc9c1\uc784\uacfc \ub3d9\uc791\uc744 \ud3ec\ucc29\ud558\ub294 \uc124\uba85\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uacb0\uacfc\uc801\uc73c\ub85c, \uc774\ub7ec\ud55c \ub2e4\ub2e8\uacc4 \uc811\uadfc \ubc29\uc2dd\uc740 \ubcf5\uc7a1\ud55c \ube44\ub514\uc624 \uc7a5\uba74 \ub0b4\uc5d0\uc11c \uac1d\uccb4\ub97c \ub354\uc6b1 \uc815\ud655\ud558\uace0 \ud3ec\uad04\uc801\uc73c\ub85c \uc124\uba85\ud558\ub294 \ud48d\ubd80\ud55c \ud14d\uc2a4\ud2b8 \uc8fc\uc11d\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.", "section": "3.3 Ref-SAV Dataset and Benchmark"}, {"figure_path": "https://arxiv.org/html/2501.04001/x5.png", "caption": "Figure 4: The samples of our Ref-SAV benchmark. Our proposed benchmark features multi-granularity, complex occlusion and reappearing, and both short and long-format text expressions.", "description": "\uadf8\ub9bc 4\ub294 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 Ref-SAV \ubca4\uce58\ub9c8\ud06c\uc758 \uc0d8\ud50c\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Ref-SAV\ub294 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \uac1d\uccb4, \ubcf5\uc7a1\ud55c \ud3d0\uc0c9\uacfc \uc7ac\ucd9c\ud604, \uadf8\ub9ac\uace0 \uc9e7\uace0 \uae34 \ud615\uc2dd\uc758 \ud14d\uc2a4\ud2b8 \uc124\uba85\uc744 \ud2b9\uc9d5\uc73c\ub85c \ud558\ub294 \uc0c8\ub85c\uc6b4 \ubca4\uce58\ub9c8\ud06c\uc785\ub2c8\ub2e4. \uc774\ub294 \uae30\uc874\uc758 \ucc38\uc870 \ube44\ub514\uc624 \uac1d\uccb4 \ubd84\ud560 \ub370\uc774\ud130\uc14b\uc758 \ud55c\uacc4\ub97c \uadf9\ubcf5\ud558\uace0 \ub354\uc6b1 \ud604\uc2e4\uc801\uc778 \uc2dc\ub098\ub9ac\uc624\ub97c \ubc18\uc601\ud558\uae30 \uc704\ud574 \uace0\uc548\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc740 Ref-SAV \ubca4\uce58\ub9c8\ud06c\uc758 \ub2e4\uc591\uc131\uacfc \uc5b4\ub824\uc6c0\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec\uac00\uc9c0 \uc608\uc2dc\ub4e4\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "3.3 Ref-SAV \ub370\uc774\ud130\uc14b \ubc0f \ubca4\uce58\ub9c8\ud06c"}, {"figure_path": "https://arxiv.org/html/2501.04001/x6.png", "caption": "Figure 5: Visualization results on image referring segmentation task.", "description": "\uadf8\ub9bc 5\ub294 \uc774\ubbf8\uc9c0 \ucc38\uc870 \ubd84\ud560 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc2dc\uac01\ud654 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc5b8\uc5b4 \uc124\uba85\uc744 \uc0ac\uc6a9\ud558\uc5ec Sa2VA \ubaa8\ub378\uc774 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uc11c\ub85c \ub2e4\ub978 \uac1d\uccb4\ub97c \uc815\ud655\ud558\uac8c \ubd84\ud560\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \uc608\uc2dc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uc774\ubbf8\uc9c0\uc5d0\ub294 \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ub41c \uc5b8\uc5b4 \uc124\uba85\uacfc \ubaa8\ub378\uc774 \uc0dd\uc131\ud55c \ubd84\ud560 \ub9c8\uc2a4\ud06c\uac00 \ud568\uaed8 \ud45c\uc2dc\ub429\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 Sa2VA \ubaa8\ub378\uc758 \uc815\ud655\uc131\uacfc \ub2e4\uc591\ud55c \uac1d\uccb4\uc640 \ubcf5\uc7a1\ud55c \uc2dc\ub098\ub9ac\uc624\ub97c \ucc98\ub9ac\ud558\ub294 \ub2a5\ub825\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.04001/x7.png", "caption": "Figure 6: Visualization results on video referring segmentation.", "description": "\uadf8\ub9bc 6\uc740 \ube44\ub514\uc624 \uac1d\uccb4 \ubd84\ud560\uc5d0 \ub300\ud55c \uc2dc\uac01\ud654 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc774\ubbf8\uc9c0\ub294 \uc0ac\uc6a9\uc790\uac00 \ud2b9\uc815 \uac1d\uccb4\ub97c \uc5b8\uae09\ud558\ub294 \uc9c8\ubb38(\uc608: \u201c\uac80\uc740\uc0c9 \ud328\uce58\uac00 \uc788\ub294 \ud770\uc0c9 \uac15\uc544\uc9c0\ub97c \ubd84\ud560\ud574 \uc8fc\uc138\uc694.\u201d)\uacfc \ud568\uaed8, \ubaa8\ub378\uc774 \uc0dd\uc131\ud55c \ubd84\ud560 \ub9c8\uc2a4\ud06c\ub97c \ud45c\uc2dc\ud569\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc870\uac74(\uc870\uba85 \ubcc0\ud654, \uc6c0\uc9c1\uc784, \ubd80\ubd84\uc801 \uac00\ub9bc \ub4f1) \ud558\uc5d0\uc11c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \ube44\ub514\uc624 \ud074\ub9bd\uc758 \ud504\ub808\uc784\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4.  \uac01 \ud074\ub9bd\uc740 \ubaa8\ub378\uc774 \ud574\ub2f9 \uac1d\uccb4\ub97c \uc815\ud655\ud558\uac8c \ubd84\ud560\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 \ubaa8\ub378\uc774 \ube44\ub514\uc624 \uc18d \ubcf5\uc7a1\ud558\uace0 \ub3d9\uc801\uc778 \uc2dc\uac01\uc801 \uc815\ubcf4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \uc774\ud574\ud558\uace0 \ucc98\ub9ac\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2501.04001/x8.png", "caption": "Figure 7: Visualization results on visual prompt understanding task. We use the masks predicted by our model under the GCG task as visual prompts, and generated region-level descriptions for these masks. The object masks and their captions for the corresponding region are highlighted in the same color.", "description": "\uc774 \uadf8\ub9bc\uc740 Sa2VA \ubaa8\ub378\uc774 GCG(Grounded Caption Generation) \uc791\uc5c5\uc5d0\uc11c \uc0dd\uc131\ud55c \ub9c8\uc2a4\ud06c\ub97c \uc2dc\uac01\uc801 \ud504\ub86c\ud504\ud2b8\ub85c \uc0ac\uc6a9\ud558\uc5ec \uc2dc\uac01\uc801 \ud504\ub86c\ud504\ud2b8 \uc774\ud574 \uc791\uc5c5\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ub9c8\uc2a4\ud06c\ub294 \ud2b9\uc815 \uc601\uc5ed\uc744 \ub098\ud0c0\ub0b4\uba70, \ud574\ub2f9 \uc601\uc5ed\uc5d0 \ub300\ud55c \uc9c0\uc5ed \uc218\uc900 \uc124\uba85\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \ub9c8\uc2a4\ud06c\uc640 \ud574\ub2f9 \uc124\uba85\uc740 \ub3d9\uc77c\ud55c \uc0c9\uc0c1\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc2dc\uac01\uc801 \uc774\ud574\ub3c4\ub97c \ub192\uc600\uc2b5\ub2c8\ub2e4. \uc989, \ubaa8\ub378\uc774 \uc774\ubbf8\uc9c0\uc758 \ud2b9\uc815 \uc601\uc5ed\uc744 \uc815\ud655\ud558\uac8c \uc2dd\ubcc4\ud558\uace0 \uadf8\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uc124\uba85\uc744 \uc0dd\uc131\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.04001/x9.png", "caption": "Figure 8: Visualization results on GCG tasks. Top: our method. Bottom: OMG-LLaVA\u00a0[99]. Note that, our method has stronger and fined-grained grounding ability and text alignment than OMG-LLaVA\u00a0[99], previous strong baseline.", "description": "\uadf8\ub9bc 8\uc740 GCG(Grounded Caption Generation) \uc791\uc5c5\uc5d0 \ub300\ud55c \uc2dc\uac01\ud654 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0c1\ub2e8\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ubc29\ubc95\uc758 \uacb0\uacfc\uc774\uace0, \ud558\ub2e8\uc740 \uc774\uc804 \ucd5c\uace0 \uc131\ub2a5 \uae30\uc900(baseline)\uc774\uc5c8\ub358 OMG-LLaVA [99]\uc758 \uacb0\uacfc\uc785\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \uc81c\uc548\ub41c \ubc29\ubc95\uc774 \uc774\uc804 \ucd5c\uace0 \uc131\ub2a5 \uae30\uc900\ubcf4\ub2e4 \ub354 \uac15\ub825\ud558\uace0 \uc138\ubc00\ud55c \uc811\uc9c0(grounding) \ub2a5\ub825\uacfc \ud14d\uc2a4\ud2b8 \uc815\ub82c\uc744 \uac00\uc9c0\uace0 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01\uac01\uc758 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574, \uc0dd\uc131\ub41c \ucea1\uc158\uc774 \ud574\ub2f9 \uac1d\uccb4 \ub9c8\uc2a4\ud06c\uc640 \uc5bc\ub9c8\ub098 \uc798 \uc815\ub82c\ub418\uc5b4 \uc788\ub294\uc9c0 \uc2dc\uac01\uc801\uc73c\ub85c \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc138\ubc00\ud55c \uc811\uc9c0 \ub2a5\ub825\uc740 \uc0dd\uc131\ub41c \ucea1\uc158\uc774 \uac1d\uccb4\uc758 \ud2b9\uc9d5\uc744 \ubcf4\ub2e4 \uc815\ud655\ud558\uace0 \uc790\uc138\ud558\uac8c \uc124\uba85\ud558\ub294 \uac83\uc744 \uc758\ubbf8\ud558\uace0, \ud14d\uc2a4\ud2b8 \uc815\ub82c\uc740 \uc0dd\uc131\ub41c \ub9c8\uc2a4\ud06c\uc640 \ucea1\uc158\uc774 \uac1d\uccb4\uc758 \uacbd\uacc4\uc5d0 \uc815\ud655\ud558\uac8c \uc77c\uce58\ud558\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "4.1. \uc8fc\uc694 \uacb0\uacfc"}]