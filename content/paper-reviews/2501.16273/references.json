{"references": [{"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduced the Transformer architecture, the foundation upon which many modern language models, including those discussed in the current paper, are built."}, {"fullname_first_author": "Raffel, C.", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-00-00", "reason": "This work introduced the T5 model, a significant encoder-decoder model that is directly compared to decoder-only models in the current study."}, {"fullname_first_author": "Wang, T.", "paper_title": "What language model architecture and pretraining objective works best for zero-shot generalization?", "publication_date": "2022-00-00", "reason": "This paper provides a crucial comparative analysis of encoder-decoder vs. decoder-only architectures, directly informing the methodology and hypotheses of the current study."}, {"fullname_first_author": "Hinton, G.", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-00-00", "reason": "This foundational paper on knowledge distillation is highly relevant because the current research uses knowledge distillation to improve the performance of smaller encoder-decoder models."}, {"fullname_first_author": "Touvron, H.", "paper_title": "LLaMA: Open and efficient foundation language models", "publication_date": "2023-00-00", "reason": "The LLaMA model is used as a teacher model in the knowledge distillation process of the current study, making this paper highly relevant to the methodology and results."}]}