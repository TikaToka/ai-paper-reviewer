[{"content": "| Model | Acc (%) | # Neg | GPT-4o # Test | GPT-4o Detail (%) | GPT-4o Simple (%) | Gemini-1.5-Flash # Test | Gemini-1.5-Flash Detail (%) | Gemini-1.5-Flash Simple (%) | Claude-3.5-Sonnet # Test | Claude-3.5-Sonnet Detail (%) | Claude-3.5-Sonnet Simple (%) |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| LLaVa-OneVision-7B | 25.6 | 2933 | 373 | 36.2 | 18.0 | 428 | 29.0 | 15.7 | 2953 | 4.1 | 2.4 |\n| InternVL2-8B | 38.1 | 2440 | 379 | 49.6 | 41.2 | 375 | 48.8 | 44.4 | 376 | 43.4 | 40.2 |\n| Molmo-7B | 25.6 | 2931 | 452 | 55.1 | 52.0 | 507 | 36.5 | 38.9 | 597 | 37.4 | 40.0 |\n| MiniCPM-V | 16.2 | 3301 | 552 | 28.4 | 20.3 | 741 | 16.6 | 25.4 | 772 | 18.7 | 27.1 |\n| GLM-4V-9B | 20.2 | 3146 | 440 | 38.6 | 28.2 | 568 | 30.1 | 29.9 | 603 | 30.0 | 26.4 |\n| Phi3.5-Vision-4.2B | 19.0 | 3192 | 534 | 36.1 | 33.7 | 579 | 31.3 | 33.7 | 616 | 26.8 | 29.1 |\n| LLaVa-1.5-7B | 13.5 | 3409 | 763 | 23.2 | 14.3 | 678 | 18.0 | 14.7 | 816 | 8.3 | 11.2 |\n| LLaVa-1.6-Mistral-7B | 14.8 | 3357 | 549 | 41.0 | 35.9 | 661 | 5.9 | 5.9 | 617 | 33.5 | 33.2 |\n| Fuyu-8B | 21.8 | 3083 | 582 | 24.1 | 19.8 | 635 | 15.0 | 12.9 | 755 | 14.0 | 11.5 |\n| Qwen2-VL-7B | 22.5 | 3052 | 295 | 66.8 | 72.2 | 470 | 41.9 | 44.9 | 505 | 50.5 | 52.7 |", "caption": "Table 1: Correction Rate Results of three Feedback Providers on MathVerse Dataset. Acc (%): The average accuracy of MathVerse\u2019s testmini set. The results are tested by ourselves. # Neg: The number of negative samples produced by the model. # Test: The total number of test samples evaluated. Detail (%): correction rate of using LMM-generated feedback. Simple (%): correction rate of using simple feedback (0 or 1).", "description": "\ud45c 1\uc740 MathVerse \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574 \uc138 \uac00\uc9c0 \ud53c\ub4dc\ubc31 \uc81c\uacf5\uc790\uc758 \uc815\uc815\ub960 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Acc(%)\ub294 MathVerse\uc758 testmini \uc138\ud2b8\uc758 \ud3c9\uade0 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. #Neg\ub294 \ubaa8\ub378\uc774 \uc0dd\uc131\ud55c \uc74c\uc131 \uc0d8\ud50c\uc758 \uc218, #Test\ub294 \ud3c9\uac00\ub41c \ud14c\uc2a4\ud2b8 \uc0d8\ud50c\uc758 \ucd1d \uc218, Detail(%)\ub294 LMM\uc774 \uc0dd\uc131\ud55c \ud53c\ub4dc\ubc31\uc744 \uc0ac\uc6a9\ud55c \uc815\uc815\ub960, Simple(%)\uc740 \ub2e8\uc21c\ud55c \ud53c\ub4dc\ubc31(0 \ub610\ub294 1)\uc744 \uc0ac\uc6a9\ud55c \uc815\uc815\ub960\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ubcf8 \uc5f0\uad6c\uc5d0\uc11c \uc9c1\uc811 \ud14c\uc2a4\ud2b8\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "3.1 INTERACTIVE BENCHMARKING"}, {"content": "| Model | Acc (%) | # Neg | GPT-4o # Test | GPT-4o Detail (%) | GPT-4o Simple (%) | Gemini-1.5-Flash # Test | Gemini-1.5-Flash Detail (%) | Gemini-1.5-Flash Simple (%) | Claude-3.5-Sonnet # Test | Claude-3.5-Sonnet Detail (%) | Claude-3.5-Sonnet Simple (%) |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| LLaVa-OneVision-7B | 47.1 | 915 | 312 | 31.7 | 15.7 | 333 | 35.4 | 18.6 | 408 | 27.5 | 16.4 |\n| InternVL2-8B | 45.7 | 939 | 343 | 50.1 | 41.4 | 329 | 57.1 | 50.2 | 437 | 50.1 | 41.2 |\n| Molmo-7B | 43.8 | 973 | 362 | 51.7 | 48.9 | 383 | 41.5 | 43.1 | 436 | 29.8 | 27.5 |\n| MiniCPM-V | 38.1 | 1071 | 410 | 27.3 | 23.7 | 503 | 21.5 | 21.7 | 540 | 24.4 | 23.3 |\n| GLM-4V-9B | 46.0 | 935 | 327 | 38.8 | 30.0 | 359 | 38.7 | 31.5 | 441 | 34.9 | 27.9 |\n| Phi3.5-Vision-4.2B | 43.2 | 983 | 366 | 44.3 | 42.3 | 396 | 40.9 | 39.6 | 484 | 39.9 | 38.0 |\n| LLaVa-1.5-7B | 36.5 | 1099 | 506 | 31.9 | 12.3 | 470 | 20.0 | 16.0 | 595 | 13.9 | 13.4 |\n| LLaVa-1.6-Mistral-7B | 38.8 | 1058 | 432 | 46.1 | 36.1 | 429 | 14.7 | 14.7 | 515 | 42.3 | 35.3 |\n| Fuyu-8B | 34.1 | 1140 | 481 | 6.0 | 8.7 | 1140 | 3.7 | 3.5 | 612 | 9.5 | 6.9 |\n| Qwen2-VL-7B | 48.1 | 898 | 268 | 50.4 | 44.8 | 322 | 39.4 | 37.6 | 389 | 42.9 | 37.3 |", "caption": "Table 2: Correction Rate Results of three Feedback Providers on MMMU-Pro Dataset. We test models on a single image setting of MMMU-Pro.", "description": "\ud45c 2\ub294 MMMU-Pro \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574 \uc138 \uac00\uc9c0 \ud53c\ub4dc\ubc31 \uc81c\uacf5\uc790\uc758 \uc815\uc815\ub960 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\uc740 MMMU-Pro\uc758 \ub2e8\uc77c \uc774\ubbf8\uc9c0 \uc124\uc815\uc5d0\uc11c \ud14c\uc2a4\ud2b8\ub429\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \ubaa8\ub378\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4, \uc74c\uc131 \uc0d8\ud50c \uc218, \ud14c\uc2a4\ud2b8 \uc0d8\ud50c \uc218, \uc138 \uac00\uc9c0 \ud53c\ub4dc\ubc31 \uc81c\uacf5\uc790(GPT-40, Gemini-1.5-Flash, Claude-3.5-Sonnet) \uac01\uac01\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ud53c\ub4dc\ubc31\uacfc \uac04\ub2e8\ud55c \ud53c\ub4dc\ubc31\uc744 \uc0ac\uc6a9\ud55c \uc815\uc815\ub960\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.1 Interactive Benchmarking"}, {"content": "| Model | Visual Logic | MMMU-Pro | MathVerse | Math<sup>Text</sup> | Coding<sup>Text</sup> | Average |\n|---|---|---|---|---|---|---|\n| Gemini-2.0 | 21.3 | 50.0 | 70.0 | 50.0 | 50.0 | 32.5 |\n| Claude-3.5 | 37.5 | 60.0 | 80.0 | 70.0 | 70.0 | 48.3 |\n| OpenAI-o1 | 28.8 | 60.0 | 90.0 | 90.0 | 90.0 | 46.7 |\n| GPT-4o | 25.0 | 70.0 | 80.0 | 60.0 | 50.0 | 38.3 |", "caption": "Table 3: Human Evaluation Results across LMMs on InterFeedback-Human. MathTextText{}^{\\text{Text}}start_FLOATSUPERSCRIPT Text end_FLOATSUPERSCRIPT and CodingTextText{}^{\\text{Text}}start_FLOATSUPERSCRIPT Text end_FLOATSUPERSCRIPT represent two text-only task categories. The scores represent the average percentage of correct samples among all samples.", "description": "\ud45c 3\uc740 InterFeedback-Human \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5ec\ub7ec \ub300\uaddc\ubaa8 \ub2e4\uc911 \ubaa8\ub4dc \ubaa8\ub378(LMM)\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MathText\uc640 CodingText\ub294 \ud14d\uc2a4\ud2b8 \uae30\ubc18 \uc791\uc5c5 \uc720\ud615\uc744 \ub098\ud0c0\ub0b4\uba70, \uac01 \uc810\uc218\ub294 \uc804\uccb4 \uc0d8\ud50c \uc911 \uc815\ub2f5 \uc0d8\ud50c\uc758 \ud3c9\uade0 \ube44\uc728\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc989, \uac01 LMM\uc774 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ubb38\uc81c(\uc2dc\uac01\uc801 \ub17c\ub9ac, \uc218\ud559, \ucf54\ub529 \ub4f1)\uc5d0 \ub300\ud574 \uc5bc\ub9c8\ub098 \uc815\ud655\ud558\uac8c \ub2f5\ubcc0\ud588\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4. ", "section": "3.2 Human Benchmarking"}, {"content": "| Model | # Round | Visual Logic | MMMU-Pro | MathVerse | Math<sup>Text</sup> | Coding<sup>Text</sup> | Average |\n|---|---|---|---|---|---|---|---| \n| Gemini-2.0 | 1 | 38.1 | 20.0 | 33.3 | 0.0 | 80.0 | 37.0 |\n|  | 2 | 20.6 | 0.0 | 33.3 | 20.0 | 20.0 | 19.8 |\n|  | 3 | 41.3 | 80.0 | 33.3 | 80.0 | 0.0 | 43.2 |\n| Claude-3.5 | 1 | 38.0 | 0.0 | 50.0 | 33.3 | 66.7 | 37.1 |\n|  | 2 | 32.0 | 25.0 | 50.0 | 33.3 | 66.7 | 30.6 |\n|  | 3 | 30.0 | 75.0 | 0.0 | 66.7 | 0.0 | 32.3 |\n| OpenAI-o1 | 1 | 38.6 | 0.0 | 100.0 | 11.1 | 100.0 | 39.1 |\n|  | 2 | 21.1 | 0.0 | 0.0 | 0.0 | 0.0 | 18.8 |\n|  | 3 | 40.4 | 100.0 | 0.0 | 0.0 | 0.0 | 42.2 |\n| GPT-4o | 1 | 41.7 | 33.3 | 100.0 | 25.0 | 40.0 | 41.9 |\n|  | 2 | 31.7 | 0.0 | 0.0 | 0.0 | 0.0 | 25.7 |\n|  | 3 | 26.7 | 66.7 | 0.0 | 75.0 | 60.0 | 32.4 |", "caption": "Table 4: Correction Rate Results across various LMMs on InterFeedback-Human. MathTextText{}^{\\text{Text}}start_FLOATSUPERSCRIPT Text end_FLOATSUPERSCRIPT and CodingTextText{}^{\\text{Text}}start_FLOATSUPERSCRIPT Text end_FLOATSUPERSCRIPT represent two text-only task categories. # Round denotes the number of interaction rounds. The correction rate is the percentage of corrected samples among all erroneous samples.", "description": "\ud45c 4\ub294 InterFeedback-Human \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \ub300\uaddc\ubaa8 \ub2e4\uc911 \ubaa8\ub4dc \ubaa8\ub378(LMM)\uc758 \uc815\uc815\ub960 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MathText\uc640 CodingText\ub294 \ud14d\uc2a4\ud2b8\ub9cc \uc0ac\uc6a9\ud558\ub294 \ub450 \uac00\uc9c0 \uc791\uc5c5 \uc720\ud615\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. # Round\ub294 \uc0c1\ud638 \uc791\uc6a9 \ub77c\uc6b4\ub4dc \uc218\ub97c \ub098\ud0c0\ub0b4\uba70, \uc815\uc815\ub960\uc740 \uc798\ubabb\ub41c \uc0d8\ud50c \uc911\uc5d0\uc11c \uc815\uc815\ub41c \uc0d8\ud50c\uc758 \ube44\uc728\uc744 \ubc31\ubd84\uc728\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 LMM\uc774 \ud53c\ub4dc\ubc31\uc744 \ud1b5\ud574 \ubb38\uc81c \ud574\uacb0 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub41c \ub2e4\uc591\ud55c \uc0c1\ud638\uc791\uc6a9 \ub77c\uc6b4\ub4dc\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.2 Human Benchmarking"}, {"content": "| Model | Release Time | Source |\n|---|---|---|\n| _Closed-source Models_ |  |  |\n| GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.15027v1#bib.bib30\" title=\"\">2023</a>)</cite> | 2024-08-26 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/hello-gpt-4o/\" title=\"\">https://openai.com/index/hello-gpt-4o/</a> |\n| OpenAI-o1 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.15027v1#bib.bib31\" title=\"\">2024</a>)</cite> | 2024-12-17 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/o1/\" title=\"\">https://openai.com/o1/</a> |\n| Gemini-1.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Gemini, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.15027v1#bib.bib11\" title=\"\">2024</a>)</cite> | 2024-09-24 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://deepmind.google/technologies/gemini/\" title=\"\">https://deepmind.google/technologies/gemini/</a> |\n| Gemini-2.0-Flash | 2025-01-21 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://deepmind.google/technologies/gemini/\" title=\"\">https://deepmind.google/technologies/gemini/</a> |\n| Claude-3.5-Sonnet | 2024-10-22 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.anthropic.com/claude/sonnet\" title=\"\">https://www.anthropic.com/claude/sonnet</a> |\n| _Closed-source Models_ |  |  |\n| LLaVA-One-Vision | 2024-08-05 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://llava-vl.github.io/blog/2024-08-05-llava-onevision/\" title=\"\">https://llava-vl.github.io/blog/2024-08-05-llava-onevision/</a> |\n| InterVL2-8B | 2024-07-04 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://internvl.github.io/blog/2024-07-02-InternVL-2.0/\" title=\"\">https://internvl.github.io/blog/2024-07-02-InternVL-2.0/</a> |\n| Molmo-7B | 2024-09-24 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/allenai/Molmo-7B-D-0924\" title=\"\">https://huggingface.co/allenai/Molmo-7B-D-0924</a> |\n| MiniCPM-V | 2024-08-03 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openbmb/MiniCPM-V\" title=\"\">https://huggingface.co/openbmb/MiniCPM-V</a> |\n| GLM-4V-9B | 2024-11-01 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/THUDM/glm-4v-9b\" title=\"\">https://huggingface.co/THUDM/glm-4v-9b</a> |\n| Pih3.5-Vision-4.2B | 2024-08-20 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/microsoft/Phi-3.5-vision-instruct\" title=\"\">https://huggingface.co/microsoft/Phi-3.5-vision-instruct</a> |\n| LLaVA-1.5-7B | 2023-10-05 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/liuhaotian/llava-v1.5-7b\" title=\"\">https://huggingface.co/liuhaotian/llava-v1.5-7b</a> |\n| LLaVA-1.6-Mistral-7B | 2024-01-30 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf\" title=\"\">https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf</a> |\n| Fuyu-8B | 2023-10-27 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/adept/fuyu-8b\" title=\"\">https://huggingface.co/adept/fuyu-8b</a> |\n| Qwen2-VL-7B | 2024-08-30 | <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Qwen/Qwen2-VL-7B\" title=\"\">https://huggingface.co/Qwen/Qwen2-VL-7B</a> |", "caption": "Table 5: The release time and model source of LMMs used in our InterFeedback-Bench.", "description": "\ud45c 5\ub294 InterFeedback-Bench \uc2e4\ud5d8\uc5d0 \uc0ac\uc6a9\ub41c \ub300\uaddc\ubaa8 \ub2e4\uc911 \ubaa8\ub2ec \ubaa8\ub378(LMM)\uc758 \ucd9c\uc2dc \uc2dc\uc810\uacfc \ubaa8\ub378 \uc18c\uc2a4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc774\ub984, \ucd9c\uc2dc\uc77c, \uadf8\ub9ac\uace0 \ubaa8\ub378 \uc815\ubcf4\ub97c \uc5bb\uc744 \uc218 \uc788\ub294 \ucd9c\ucc98(\uc8fc\ub85c \uc6f9\uc0ac\uc774\ud2b8 \ub9c1\ud06c)\ub97c \uc81c\uacf5\ud558\uc5ec, \uc0ac\uc6a9\ub41c LMM\uc5d0 \ub300\ud55c \uc0c1\uc138\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774\ub294 \ubcf8 \ub17c\ubb38\uc758 \uc7ac\ud604\uc131\uacfc \ud22c\uba85\uc131\uc744 \ub192\uc774\uae30 \uc704\ud55c \uc911\uc694\ud55c \uc815\ubcf4\uc785\ub2c8\ub2e4.", "section": "A \ucd94\uac00 \uc2e4\ud5d8 \uc138\ubd80 \uc815\ubcf4"}]