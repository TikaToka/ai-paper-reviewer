{"references": [{"fullname_first_author": "Haotian Liu", "paper_title": "LLaVA: Labeled Language-and-Vision Assistant", "publication_date": "2023-10-05", "reason": "This paper introduces LLaVA, a foundational model for language and vision tasks, which is used as a benchmark within InterFeedback-Bench."}, {"fullname_first_author": "Peng Wang", "paper_title": "Qwen-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution", "publication_date": "2024-08-30", "reason": "Qwen-VL is another significant multimodal model serving as a benchmark in the InterFeedback-Bench evaluation."}, {"fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI", "publication_date": "2024-06-10", "reason": "MMMU-Pro, a challenging multimodal benchmark dataset, is extensively used in the InterFeedback-Bench, showcasing the importance of this paper."}, {"fullname_first_author": "Renrui Zhang", "paper_title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?", "publication_date": "2024-03-14", "reason": "MathVerse, a visual mathematics benchmark dataset, is used in the InterFeedback-Bench, underscoring its importance in the evaluation."}, {"fullname_first_author": "Matt Deitke", "paper_title": "Molmo and Pixmo: Open Weights and Open Data for State-of-the-Art Vision-Language Models", "publication_date": "2024-09-24", "reason": "The Molmo model, a significant large multimodal model, is included in the InterFeedback-Bench evaluation, highlighting the paper's contribution."}]}