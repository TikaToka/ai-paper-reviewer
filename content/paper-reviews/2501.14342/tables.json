[{"content": "| Dataset | EM | F1 | EM | F1 | EM | F1 | EM | F1 |\n|---|---|---|---|---|---|---|---|---|\n| **Few-shot w/o Retrieval** |  |  |  |  |  |  |  |  |\n| 3-shot Llama-3.1-8B-Inst. | 27.6 | 32.1 | 20.8 | 28.8 | 17.6 | 21.3 | 3.4 | 9.7 |\n| 3-shot GPT-4o | 39.5 | 47.3 | 38.2 | 51.2 | 49.6 | 61.5 | 15.8 | 27.2 |\n| **w/ Retrieval** |  |  |  |  |  |  |  |  |\n| 3-shot Llama-3.1-8B-Inst. | 30.7 | 39.9 | 34.1 | 46.6 | 28.0 | 37.3 | 7.7 | 15.4 |\n| 3-shot GPT-4o | 49.0 | 56.2 | 45.8 | 59.4 | 53.6 | 63.8 | 15.7 | 25.8 |\n| Self-RAG-7B | 12.2 | 24.1 | 16.6 | 29.4 | 5.6 | 16.8 | 4.6 | 13.2 |\n| ITER-RETGEN | 35.5 | 47.4 | 45.1 | 60.4 | 40.0 | 50.7 | 26.1 | 42.0 |\n| DRAG (32k) | 45.9 | 53.7 | 46.9 | 60.3 | 48.8 | 59.2 | 15.4 | 26.0 |\n| IterDRAG (32k) | 44.3 | 54.6 | 38.3 | 49.8 | 46.4 | 56.2 | 12.5 | 23.1 |\n| Search-o1-32B | 58.0 | 71.4 | 45.2 | 57.3 | 56.0 | 67.8 | 16.6 | 28.2 |\n| Fine-tuned Llama-8B w/ E5<sub>large</sub> | 55.1 | 60.7 | 50.3 | 63.5 | 40.8 | 53.7 | 17.4 | 28.1 |\n| **CoRAG-8B (Ours)** |  |  |  |  |  |  |  |  |\n|  \u25b7 L=1, greedy | 56.5 | 62.3 | 50.1 | 63.2 | 37.6 | 51.4 | 18.6 | 29.3 |\n|  \u25b7 L=6, greedy | 70.6 | 75.5 | 54.4 | 67.5 | 48.0 | 63.5 | 27.7 | 38.5 |\n|  \u25b7 L=6, best-of-4 | 71.7 | 76.5 | 55.3 | 68.5 | 51.2 | 63.1 | 28.1 | 39.7 |\n|  \u25b7 L=10, best-of-8 | **72.5** | **77.3** | **56.3** | **69.8** | 54.4 | **68.3** | **30.9** | **42.4** |", "caption": "Table 1: Results on multi-hop QA datasets.\nWe report the performance of CoRAG-8B using various decoding strategies and retrieval chain lengths L\ud835\udc3fLitalic_L.\nThe \u201cFew-shot w/o Retrieval\u201d configuration utilizes only QA pairs without retrieval augmentation.\nBoth DRAG and IterDRAG are based on Gemini 1.5 Flash\u00a0[29],\nwhile Search-o1-32B is based on QwQ\u00a0[35] and the Bing Search API.", "description": "\ud45c 1\uc740 \ub2e4\uc591\ud55c \ub514\ucf54\ub529 \uc804\ub7b5\uacfc \uac80\uc0c9 \uccb4\uc778 \uae38\uc774(L)\ub97c \uc0ac\uc6a9\ud558\uc5ec CoRAG-8B \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\uc911 \ud649 \uc9c8\uc758\uc751\ub2f5(QA) \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \"\ud4e8\uc0f7(Few-shot) w/o Retrieval\" \uc124\uc815\uc740 \uac80\uc0c9 \uc99d\uac15 \uc5c6\uc774 QA \uc30d\ub9cc \uc0ac\uc6a9\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. DRAG\uc640 IterDRAG\ub294 Gemini 1.5 Flash [29] \uae30\ubc18\uc774\uba70, Search-o1-32B\ub294 QwQ [35]\uc640 Bing \uac80\uc0c9 API\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b(2WikiMultihopQA, HotpotQA, Bamboogle, MuSiQue)\uc5d0 \ub300\ud55c EM\uacfc F1 \uc810\uc218\uac00 \uc81c\uc2dc\ub418\uc5b4 CoRAG-8B\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \uae30\uc900\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.", "section": "4.2 \uc8fc\uc694 \uacb0\uacfc"}, {"content": "| System | AIDA | WnWi | WnCw | T-REx | zsRE | NQ | HoPo | TQA | FEVER | Fact |\n|---|---|---|---|---|---|---|---|---|---|---|\n| KILT-RAG | 72.6 | 48.1 | 47.6 | 59.2 | 44.7 | 44.4 | 27.0 | 71.3 | 86.3 |\n| SEAL | - | - | - | 83.6 | 74.6 | 53.7 | 40.5 | 70.9 | 89.5 |\n| Atlas-11B | 90.6 | - | - | 85.1 | 80.8 | 61.3 | 50.6 | 84.0 | **93.5** |\n| RA-DIT 65B | 80.5 | - | - | 72.8 | 78.1 | 43.5 | 36.6 | 72.8 | 86.9 |\n| FiD with RS | - | - | - | 85.2 | 83.7 | 61.2 | 39.1 | 84.6 | 92.2 |\n| Previous Best* | 90.6 | 87.4 | 71.2 | 87.7 | 85.3 | 62.3 | 50.6 | 84.6 | **93.5** |\n| CoRAG-8B (Ours) | **93.9** | **88.2** | **76.7** | **88.0** | **87.2** | **63.1** | **60.6** | **88.3** | 93.1 |", "caption": "Table 2: The downstream results on the hidden test set of the KILT benchmark.\nAll scores are sourced directly from the official leaderboard,\nwith the exception that \u201cRA-DIT 65B\u201d is from the original paper\u00a0[21].\n\u2217*\u2217: \u201cPrevious Best\u201d refers to the highest score for each task on the public KILT leaderboard as of January 10, 2025.", "description": "\ud45c 2\ub294 KILT \ubca4\uce58\ub9c8\ud06c\uc758 \uc228\uaca8\uc9c4 \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0 \ub300\ud55c \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \uc810\uc218\ub294 \uacf5\uc2dd \ub9ac\ub354\ubcf4\ub4dc\uc5d0\uc11c \uc9c1\uc811 \uac00\uc838\uc628 \uac83\uc774\uba70, \"RA-DIT 65B\" \uc810\uc218\ub9cc \uc6d0\ubcf8 \ub17c\ubb38 [21]\uc5d0\uc11c \uac00\uc838\uc628 \uc810\uc218\uc785\ub2c8\ub2e4.  \"\uc774\uc804 \ucd5c\uace0\uc810\"\uc740 2025\ub144 1\uc6d4 10\uc77c \uae30\uc900\uc73c\ub85c \uacf5\uac1c KILT \ub9ac\ub354\ubcf4\ub4dc\uc5d0\uc11c \uac01 \uacfc\uc81c\uc758 \ucd5c\uace0 \uc810\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \uc2dc\uc2a4\ud15c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \uc9c0\ud45c(\uc608: \uc815\ud655\ub3c4, F1 \uc810\uc218)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uc2dc\uc2a4\ud15c \uac04\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \uc6a9\uc774\ud558\uac8c \ud569\ub2c8\ub2e4.", "section": "4.2 \uc8fc\uc694 \uacb0\uacfc"}, {"content": "| Metric | 2WikiQA |  | HotpotQA |  | Bamboogle |  | MuSiQue |  |\n|---|---|---|---|---|---|---|---|---|\n|  | EM | F1 | EM | F1 | EM | F1 | EM | F1 |\n| CoRAG-8B (L=6, greedy) | 70.6 | 75.5 | **54.4** | **67.5** | **48.0** | **63.5** | **27.7** | **38.5** |\n| \u25b7 iterative training | **72.2** | **76.9** | 53.4 | 66.5 | 45.6 | 60.9 | 26.6 | 37.6 |\n| *Weak-to-strong Generalization* |  |  |  |  |  |  |  |  |\n| \u00a0\u00a0w/ Llama-3.2-1B-Inst. | 59.3 | 64.2 | 50.3 | 63.6 | 40.8 | 51.6 | 22.3 | 32.7 |\n| \u00a0\u00a0w/ Llama-3.2-3B-Inst. | 69.9 | 74.0 | 53.9 | 67.3 | 45.6 | 59.8 | 25.2 | 36.0 |\n| *Different Retrievers* |  |  |  |  |  |  |  |  |\n| E5-base w/o chain-of-retrieval | 53.1 | 58.9 | 47.9 | 61.1 | 38.4 | 52.7 | 15.8 | 26.4 |\n| \u25b7 L=6, best-of-4 | 70.8 | 75.4 | 53.0 | 66.2 | 47.2 | 59.8 | 26.3 | 37.6 |\n| BM25 w/o chain-of-retrieval | 49.1 | 55.3 | 46.9 | 60.3 | 36.8 | 48.6 | 14.3 | 24.8 |\n| \u25b7 L=6, best-of-4 | 62.6 | 67.7 | 51.6 | 64.7 | 37.6 | 52.5 | 23.5 | 33.0 |", "caption": "Table 3: Ablation study results.\n\u201cIterative training\u201d employs a trained CoRAG model for another round of rejection sampling.\n\u201cWeak-to-strong Generalization\u201d utilizes weaker LLMs for retrieval chain generation\nwhile using stronger LLMs (Llama-3.1-8B-Inst.) for training.\n\u201cDifferent Retrievers\u201d replaces the text retriever at test time.", "description": "\ud45c 3\uc740 CoRAG \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \ub300\ud55c \ucd94\uac00 \ubd84\uc11d\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \"\ubc18\ubcf5 \ud6c8\ub828(Iterative training)\"\uc740 \ud6c8\ub828\ub41c CoRAG \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucd94\uac00\uc801\uc778 \ub9ac\uc81d\uc158 \uc0d8\ud50c\ub9c1\uc744 \uc218\ud589\ud558\ub294 \ubc29\uc2dd\uc744,  \"\uc57d-\uac15 \uc77c\ubc18\ud654(Weak-to-strong Generalization)\"\ub294 \ucd94\ucd9c \uccb4\uc778 \uc0dd\uc131\uc5d0\ub294 \uc131\ub2a5\uc774 \uc57d\ud55c LLM\uc744 \uc0ac\uc6a9\ud558\uace0 \ud6c8\ub828\uc5d0\ub294 Llama-3.1-8B-Inst. \uc640 \uac19\uc740 \uac15\ub825\ud55c LLM\uc744 \uc0ac\uc6a9\ud558\ub294 \ubc29\uc2dd\uc744,  \"\ub2e4\ub978 \uac80\uc0c9\uae30(Different Retrievers)\"\ub294 \ud14c\uc2a4\ud2b8 \uc2dc\uc810\uc5d0\uc11c \ub2e4\ub978 \ud14d\uc2a4\ud2b8 \uac80\uc0c9\uae30\ub97c \uc0ac\uc6a9\ud558\ub294 \ubc29\uc2dd\uc744 \uac01\uac01 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc138 \uac00\uc9c0 \ubc29\uc2dd\uc744 \ud1b5\ud574 CoRAG \ubaa8\ub378\uc758 \uacac\uace0\uc131\uacfc \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "5 \ubd84\uc11d"}, {"content": "|           | Multi-hop QA | KILT Benchmark |\n|-----------|---------------|-----------------|\n| Initialization | *Llama-3.1-8B-Instruct* | *Llama-3.1-8B-Instruct* |\n| Learning rate | 5 \u00d7 10<sup>-6</sup> | 10<sup>-5</sup> |\n| Batch size | 256 | 1024 |\n| Epoch | 1 | 1 |\n| Warmup steps | 100 | 100 |\n| # Training samples | 125k | 660k |\n| # Retrieved passages | 20 | 20 |\n| Max sequence length | 3072 | 3072 |", "caption": "Table 4: Hyperparameters for training CoRAG.", "description": "\ud45c 4\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 CoRAG \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a4\ub294 \ub370 \uc0ac\uc6a9\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uba40\ud2f0\ud649 \uc9c8\uc758\uc751\ub2f5(Multi-hop QA) \ub370\uc774\ud130\uc14b\uacfc KILT \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud574 \uac01\uac01 \ub2e4\ub978 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ucd08\uae30\ud654 \ubc29\ubc95, \ud559\uc2b5\ub960, \ubc30\uce58 \ud06c\uae30, \uc5d0\ud3ed \uc218, \uc6cc\ubc0d\uc5c5 \ub2e8\uacc4, \ud559\uc2b5 \uc0d8\ud50c \uc218, \uac80\uc0c9\ub41c \uad6c\uc808 \uc218, \ucd5c\ub300 \uc2dc\ud000\uc2a4 \uae38\uc774 \ub4f1\uc758 \uc138\ubd80 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  Multi-hop QA\uc640 KILT \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc124\uc815\uc774 \uac01\uac01 \ubcc4\ub3c4\ub85c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3 Methodology"}, {"content": "| System | Entity Linking |  |  | Slot Filling |  | Open QA |  |  | Fact |\n|---|---|---|---|---|---|---|---|---|---|---| \n|  CoRAG-8B (Ours)  |  |  |  |  |  |  |  |  |  |\n| \u25b7 L=1, greedy | 90.4 | 86.0 | **76.8** | **87.0** | 82.1 | 62.5 | 56.4 | 88.4 | 91.4 |\n| \u25b7 L=6, greedy | **92.7** | **87.4** | 75.8 | 86.6 | **83.8** | **63.2** | 59.1 | 88.6 | 93.8 |\n| \u25b7 L=6, best-of-4 | 92.5 | **87.4** | 75.8 | 86.3 | 83.5 | 62.6 | 59.6 | **88.9** | **93.9** |\n| \u25b7 L=6, tree search | 91.8 | 86.8 | 75.5 | 86.4 | 83.0 | 62.4 | **59.9** | **88.9** | **93.9** |", "caption": "Table 5: Downstream results on the public validation set of the KILT benchmark.", "description": "\ud45c 5\ub294 KILT \ubca4\uce58\ub9c8\ud06c\uc758 \uacf5\uac1c \uac80\uc99d \uc138\ud2b8\uc5d0 \ub300\ud55c \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  KILT \ubca4\uce58\ub9c8\ud06c\ub294 \uc9c0\uc2dd \uc9d1\uc57d\uc801\uc778 \uc5ec\ub7ec \uac00\uc9c0 \uc791\uc5c5(\uc608: \uc5d4\ud2f0\ud2f0 \uc5f0\uacb0, \uc2ac\ub86f \ucc44\uc6b0\uae30, \uac1c\ubc29\ud615 \uc9c8\ubb38 \ub2f5\ubcc0, \ud329\ud2b8 \uac80\uc99d)\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 CoRAG \ubaa8\ub378\uc744 \ud3ec\ud568\ud55c \ub2e4\uc591\ud55c \uc2dc\uc2a4\ud15c\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \uc791\uc5c5\uc5d0 \ub300\ud574 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4(EM \ub610\ub294 F1) \uc810\uc218\uac00 \uc81c\uc2dc\ub418\uc5b4 CoRAG \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\ub978 \uac15\ub825\ud55c \uae30\uc900 \ubaa8\ub378\uacfc \ube44\uad50\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.", "section": "4.2 \uc8fc\uc694 \uacb0\uacfc"}, {"content": "| System | AIDA | WnWi | WnCw | T-REx | zsRE | NQ | HoPo | TQA | FEVER |\n|---|---|---|---|---|---|---|---|---|---| \n| Fine-tuned E5<sub>mistral</sub> | 92.9 | 86.7 | 76.0 | 80.5 | 95.3 | 77.7 | 66.7 | 78.9 | 90.9 |\n| \u25b7 w/ re-ranking | 93.3 | 88.0 | 77.1 | 83.2 | 97.6 | 78.2 | 78.2 | 81.5 | 92.3 |", "caption": "Table 6: Retrieval results (R-Precision) on the public validation set of the KILT benchmark.\nFor re-ranking,\nwe use the top-100100100100 candidates from the fine-tuned retriever as input.", "description": "\ud45c 6\uc740 KILT \ubca4\uce58\ub9c8\ud06c\uc758 \uacf5\uac1c \uac80\uc99d \uc138\ud2b8\uc5d0 \ub300\ud55c \uac80\uc0c9 \uacb0\uacfc(R-\uc815\ubc00\ub3c4)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\uc2dc \uc21c\uc704\ub97c \ub9e4\uae30\uae30 \uc704\ud574 \ubbf8\uc138 \uc870\uc815\ub41c \uac80\uc0c9\uae30\uc5d0\uc11c \uc0c1\uc704 100\uac1c\uc758 \ud6c4\ubcf4\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \ud45c\ub294 \uc5d4\ud2f0\ud2f0 \uc5f0\uacb0, \uc2ac\ub86f \ucc44\uc6b0\uae30, \uc5f4\ub9b0 \uc9c8\ubb38\uc751\ub2f5, \uc0ac\uc2e4 \uac80\uc99d \ub4f1 \ub2e4\uc591\ud55c KILT \uc791\uc5c5\uc5d0 \ub300\ud55c R-\uc815\ubc00\ub3c4 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc791\uc5c5\uc5d0 \ub300\ud574 \uc2dc\uc2a4\ud15c\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uc5b4\ub5a4 \uc2dc\uc2a4\ud15c\uc774 \ud2b9\uc815 \uc791\uc5c5\uc5d0\uc11c \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2 \uc8fc\uc694 \uacb0\uacfc"}, {"content": "| Dataset | Task Description |\n|---|---| \n| HotpotQA / 2WikiMultihopQA | answer multi-hop questions |\n| NQ | answer natural questions from Google search |\n| AidaYago 2 / WnWi / WnCw / Blink | link the mention surrounded by [START_ENT] and [END_ENT] to the title of the correct Wikipedia page |\n| FEVER | verify if the claim is supported or refuted |\n| T-REx / Zero-Shot RE | given head entity and relation separated by [SEP], find the correct tail entity, return the title of its Wikipedia page |\n| Trivia QA | answer trivia questions |\n| MuSiQue / Bamboogle | answer multi-hop questions |", "caption": "Table 7: Task descriptions for each dataset.", "description": "\ud45c 7\uc740 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uac01 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc791\uc5c5 \uc124\uba85\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b(HotpotQA, NQ, AidaYago, FEVER, T-REx, TriviaQA, MuSiQue, Bamboogle)\uc5d0 \ub300\ud574 \uc5b4\ub5a4 \uc720\ud615\uc758 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294\uc9c0\ub97c \uac04\ub7b5\ud558\uac8c \uc124\uba85\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 HotpotQA\uc640 2WikiMultihopQA\ub294 \ub2e4\uc911 \ub2e8\uacc4 \ucd94\ub860\uc774 \ud544\uc694\ud55c \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub2f5\ubcc0\uc744 \uc694\uad6c\ud558\ub294 \ubc18\uba74, NQ\ub294 \uad6c\uae00 \uac80\uc0c9\uc5d0\uc11c \ub098\uc628 \uc790\uc5f0\uc5b4 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub2f5\ubcc0\uc744 \uc694\uad6c\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub418\uc5b4, \uc2e4\ud5d8 \uacb0\uacfc \ud574\uc11d\uc5d0 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "4 Experiments"}, {"content": "| Query | RAG without Chain-of-Retrieval | CoRAG (Ours) |\n|---|---|---|\n| What wrestling team is Mathew Thomas Rehwoldt a part of? | Final Answer: <span class=\"ltx_text ltx_font_bold\" style=\"color:#FF0000;\">WWE<span class=\"ltx_text ltx_font_medium\"> \u2717</span></span> | Sub-query 1: What is Mathew Thomas Rehwoldt\u2019s profession?\nSub-answer 1: No relevant information found.\nSub-query 2: What is Mathew Thomas Rehwoldt\u2019s name in the wrestling industry?\nSub-answer 2: Aiden English\nSub-query 3: What wrestling team is Aiden English a part of?\nSub-answer 3: The Vaudevillains\nFinal Answer: <span class=\"ltx_text ltx_font_bold\" style=\"color:#0000FF;\">The Vaudevillains<span class=\"ltx_text ltx_font_medium\"> \u2713</span></span> |\n| How many months apart are Johan Mj\u00e4llby and Neil Lennon in age? | Final Answer: <span class=\"ltx_text ltx_font_bold\" style=\"color:#FF0000;\">two months<span class=\"ltx_text ltx_font_medium\"> \u2717</span></span> | Sub-query 1: What is Johan Mj\u00e4llby\u2019s birthdate?\nSub-answer 1: 9 February 1971\nSub-query 2: What is Neil Lennon\u2019s birthdate?\nSub-answer 2: 25 June 1971\nSub-query 3: What is the difference in months between 9 February 1971 and 25 June 1971?\nSub-answer 3: 4 months\nFinal Answer: <span class=\"ltx_text ltx_font_bold\" style=\"color:#0000FF;\">4<span class=\"ltx_text ltx_font_medium\"> \u2713</span></span> |\n| Thomas Parker Sanborn was a model for the protagonist in the only novel by a poet originally from what country? | Final Answer: <span class=\"ltx_text ltx_font_bold\" style=\"color:#FF0000;\">Italian<span class=\"ltx_text ltx_font_medium\"> \u2717</span></span> | Sub-query 1: What country is the poet originally from?\nSub-answer 1: Hungary, Romania, Switzerland, China, Ukraine, France, Turkey.\nSub-query 2: What is the name of the poet who wrote the only novel?\nSub-answer 2: Emily Bront\u00eb\nSub-query 3: What country is the poet who wrote the only novel originally from?\nSub-answer 3: United States\nSub-query 4: What is the name of the poet who wrote the only novel that Thomas Parker Sanborn was a model for?\nSub-answer 4: George Santayana\nSub-query 5: What poet wrote the only novel that Thomas Parker Sanborn was a model for?\nSub-answer 5: George Santayana\nSub-query 6: What country is the poet George Santayana originally from?\nSub-answer 6: Spain.\nFinal Answer: <span class=\"ltx_text ltx_font_bold\" style=\"color:#0000FF;\">Spain<span class=\"ltx_text ltx_font_medium\"> \u2713</span></span> |\n| Which university is further North, University of New Hampshire or Stony Brook University? | Final Answer: <span class=\"ltx_text ltx_font_bold\" style=\"color:#FF0000;\">University of New Hampshire<span class=\"ltx_text ltx_font_medium\"> \u2717</span></span> | Sub-query 1: What is the location of the University of New Hampshire?\nSub-answer 1: Durham, New Hampshire.\nSub-query 2: What is the location of Stony Brook University?\nSub-answer 2: Stony Brook, NY.\nSub-query 3: What is the latitude of Durham, New Hampshire?\nSub-answer 3: No relevant information found.\nSub-query 4: What is the latitude of Stony Brook, NY?\nSub-answer 4: No relevant information found.\nFinal Answer: <span class=\"ltx_text ltx_font_bold\" style=\"color:#FF0000;\">University of New Hampshire<span class=\"ltx_text ltx_font_medium\"> \u2717</span></span> |", "caption": "Table 8: Examples from the validation set of the HotpotQA dataset.\nFor conciseness,\nall retrieved documents at each step are omitted.\nCorrect answers are highlighted in blue, while incorrect answers are highlighted in red.", "description": "\ud45c 8\uc740 HotpotQA \uac80\uc99d \uc138\ud2b8\uc758 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac04\uacb0\uc131\uc744 \uc704\ud574 \uac01 \ub2e8\uacc4\uc5d0\uc11c \uac80\uc0c9\ub41c \ubb38\uc11c\ub294 \uc0dd\ub7b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \uc815\ub2f5\uc740 \ud30c\ub780\uc0c9\uc73c\ub85c, \uc624\ub2f5\uc740 \ube68\uac04\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 CoRAG \ubaa8\ub378\uc774 \ub2e4\ub2e8\uacc4 \ucd94\ub860 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\ub4e4\uc744 \uc81c\uc2dc\ud558\uba70, \ub2e8\uc77c \uac80\uc0c9 \ub2e8\uacc4\ub9cc \uc0ac\uc6a9\ud558\ub294 \uae30\uc874 \ubc29\ubc95\uacfc \ube44\uad50\ud558\uc5ec CoRAG\uc758 \uc7a5\uc810\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc608\uc2dc\ub294 \uc9c8\ubb38, \uae30\uc874 RAG \ubaa8\ub378\uc758 \ub2f5\ubcc0, CoRAG \ubaa8\ub378\uc758 \ub2f5\ubcc0(\uc911\uac04 \ub2e8\uacc4 \uc9c8\ubb38 \ubc0f \ub2f5\ubcc0 \ud3ec\ud568)\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  CoRAG \ubaa8\ub378\uc740 \uc911\uac04 \ub2e8\uacc4 \uc9c8\ubb38\uc744 \ud1b5\ud574 \ucd94\uac00 \uc815\ubcf4\ub97c \uc5bb\uace0, \uc624\ub2f5\uc744 \uc218\uc815\ud558\uba70, \ucd5c\uc885\uc801\uc73c\ub85c \uc815\ub2f5\uc5d0 \ub3c4\ub2ec\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2 \uc8fc\uc694 \uacb0\uacfc"}]