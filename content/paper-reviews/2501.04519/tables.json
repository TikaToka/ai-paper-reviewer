[{"content": "| Task | rStar-Math (Qwen-7B) | rStar-Math (Qwen-1.5B) | rStar-Math (Phi3-mini) | OpenAI o1-preview | OpenAI o1-mini | QWQ 32B-preview | GPT-4o | DeepSeek-V3 |\n|---|---|---|---|---|---|---|---|---|\n| MATH | 90.0 | 88.6 | 86.4 | 85.5 | 90.0 | 90.6 | 76.6 | 90.2 |\n| AIME 2024 | 53.3 | 46.7 | 43.3 | 44.6 | 56.7 | 50.0 | 9.3 | 39.2 |\n| Olympiad Bench | 65.6 | 64.6 | 60.3 | - | 65.3 | 61.2 | 43.3 | 55.4 |\n| College Math | 60.5 | 59.3 | 59.1 | - | 57.8 | 55.8 | 48.5 | 58.9 |\n| Omni-Math | 50.5 | 48.5 | 46.0 | 52.5 | 60.5 | 49.6 | 30.5 | 35.9 |", "caption": "Table 1: \\sysname enables frontier math reasoning in SLMs via deep thinking over 64 trajectories.", "description": "\ud45c 1\uc740 rStar-Math \ubaa8\ub378\uc774 64\uac1c\uc758 \uc2dc\ubbac\ub808\uc774\uc158 \uacbd\ub85c\ub97c \ud1b5\ud574 \uc2ec\uce35\uc801 \uc0ac\uace0 \uacfc\uc815\uc744 \uac70\uccd0 \uc791\uc740 \uc5b8\uc5b4 \ubaa8\ub378(SLM)\uc758 \uc218\ud559\uc801 \ucd94\ub860 \ub2a5\ub825\uc744 \ucd5c\ucca8\ub2e8 \uc218\uc900\uc73c\ub85c \ub04c\uc5b4\uc62c\ub838\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc218\ud559 \ubb38\uc81c \ud574\uacb0 \uacfc\uc81c(MATH, AI\u039c\u0395 2024, Olympiad Bench, College Math, Omni-Math)\uc5d0\uc11c rStar-Math\ub97c \uc801\uc6a9\ud588\uc744 \ub54c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uc815\ud655\ub3c4(pass@1)\ub97c \ube44\uad50\ud558\uc5ec, SLM\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uad6c\uccb4\uc801\uc73c\ub85c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "1 Introduction"}, {"content": "| # | models in MCTS | GSM-level | MATH-level | Olympiad-level | All |\n|---|---|---|---|---|---| \n| Round 1 | DeepSeek-Coder-V2-Instruct | 96.61% | 67.36% | 20.99% | 60.17% |\n| Round 2 | policy SLM-r1 | 97.88% | 67.40% | 56.04% | 66.60% |\n| Round 3 | policy SLM-r2, PPM-r2 | 98.15% | 88.69% | 62.16% | 77.86% |\n| Round 4 | policy SLM-r3, PPM-r3 | 98.15% | 94.53% | 80.58% | 90.25% |", "caption": "Table 2: Percentage of the 747k math problems correctly solved in each round. Only problems have correct solutions are included in the training set. The first round uses DeepSeek-Coder-Instruct as the policy LLM, while later rounds use our fine-tuned 7B policy SLM.", "description": "\ud45c 2\ub294 4\ud68c\uc758 \uc790\uae30 \uc9c4\ud654 \uacfc\uc815\uc744 \uac70\uce5c \ud6c4 \uac01 \ub77c\uc6b4\ub4dc\uc5d0\uc11c \uc815\ub2f5\uc774 \uc788\ub294 747,000\uac1c\uc758 \uc218\ud559 \ubb38\uc81c \uc911 \uc815\ud655\ud558\uac8c \ud480\ub9b0 \ubb38\uc81c\uc758 \ube44\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uccab \ubc88\uc9f8 \ub77c\uc6b4\ub4dc\uc5d0\uc11c\ub294 DeepSeek-Coder-Instruct \ubaa8\ub378\uc774 \uc815\ucc45 LLM\uc73c\ub85c \uc0ac\uc6a9\ub418\uc5c8\uace0, \uc774\ud6c4 \ub77c\uc6b4\ub4dc\uc5d0\uc11c\ub294 \ubbf8\uc138 \uc870\uc815\ub41c 7B \uc815\ucc45 SLM\uc774 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubaa8\ub378\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc790\uae30 \uc9c4\ud654 \uacfc\uc815\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc911\uc694\ud55c \uc9c0\ud45c\uc785\ub2c8\ub2e4.  \uac01 \ub77c\uc6b4\ub4dc\ubcc4\ub85c GSM \ub808\ubca8, MATH \ub808\ubca8, \uc62c\ub9bc\ud53c\uc544\ub4dc \ub808\ubca8 \ubb38\uc81c\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \uac1c\ubcc4\uc801\uc73c\ub85c \uc81c\uc2dc\ud558\uc5ec \uc138\ubd80\uc801\uc778 \uc131\ub2a5 \ubd84\uc11d\uc5d0 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3.4 \uc790\uae30 \uc9c4\ud654\ub41c \uc2ec\uce35 \uc0ac\uace0"}, {"content": "| Round# | MATH | AIME 2024 | AMC 2023 | Olympiad Bench | College Math | GSM8K | GaokaoEn 2023 |\n|---|---|---|---|---|---|---|---| \n| DeepSeek-Coder-V2-Instruct (bootstrap model) | 75.3 | 13.3 | 57.5 | 37.6 | 46.2 | 94.9 | 64.7 |\n| Base (Qwen2.5-Math-7B) | 58.8 | 0.0 | 22.5 | 21.8 | 41.6 | 91.6 | 51.7 |\n| policy SLM-r1 | 69.6 | 3.3 | 30.0 | 34.7 | 44.5 | 88.4 | 57.4 |\n| policy SLM-r2 | 73.6 | 10.0 | 35.0 | 39.0 | 45.7 | 89.1 | 59.7 |\n| policy SLM-r3 | 75.8 | 16.7 | 45.0 | 44.1 | 49.6 | 89.3 | 62.8 |\n| policy SLM-r4 | 78.4 | 26.7 | 47.5 | 47.1 | 52.5 | 89.7 | 65.7 |", "caption": "Table 3: Pass@1 accuracy of the resulting policy SLM in each round, showing continuous improvement until surpassing the bootstrap model.", "description": "\ud45c 3\uc740 rStar-Math \ubaa8\ub378\uc758 \uc790\uae30 \uc9c4\ud654 \uacfc\uc815\uc5d0\uc11c \uac01 \ub77c\uc6b4\ub4dc\ubcc4\ub85c \ub3c4\ucd9c\ub41c \uc815\ucc45 SLM\uc758 Pass@1 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub77c\uc6b4\ub4dc\ub9c8\ub2e4 \uc815\ucc45 SLM\uacfc PPM\uc774 \ud5a5\uc0c1\ub418\uba74\uc11c \uc131\ub2a5\uc774 \uc9c0\uc18d\uc801\uc73c\ub85c \uac1c\uc120\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc73c\uba70, \ucd5c\uc885\uc801\uc73c\ub85c\ub294 \ucd08\uae30 \ubd80\ud2b8\uc2a4\ud2b8\ub7a9 \ubaa8\ub378\uc744 \ub2a5\uac00\ud558\ub294 \uc131\ub2a5\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.  \uc774\ub294 rStar-Math\uc758 \uc790\uae30 \uc9c4\ud654 \uacfc\uc815\uc774 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc99d\uac70\uc785\ub2c8\ub2e4.", "section": "3.4 \uc790\uae30 \uc9c4\ud654\ub41c \uc2ec\uce35 \uc0ac\uace0"}, {"content": "| Round# | MATH | AIME 2024 | AMC 2023 | Olympiad Bench | College Math | GSM8K | GaokaoEn 2023 |\n|---|---|---|---|---|---|---|---| \n| PPM-r1 | 75.2 | 10.0 | 57.5 | 35.7 | 45.4 | 90.9 | 60.3 |\n| PPM-r2 | 84.1 | 26.7 | 75.0 | 52.7 | 54.2 | 93.3 | 73.0 |\n| PPM-r3 | 85.2 | 33.3 | 77.5 | 59.5 | 55.6 | 93.9 | 76.6 |\n| PPM-r4 | 87.0 | 43.3 | 77.5 | 61.5 | 56.8 | 94.2 | 77.8 |", "caption": "Table 4: The quality of PPM consistently improves across rounds. The policy model has been fixed with policy SLM-r1 for a fair comparison.", "description": "\ud45c 4\ub294 PPM\uc758 \uc131\ub2a5\uc774 \ubc18\ubcf5 \ud559\uc2b5\uc744 \ud1b5\ud574 \uc9c0\uc18d\uc801\uc73c\ub85c \ud5a5\uc0c1\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574 \uc815\ucc45 \ubaa8\ub378\uc740 SLM-r1\ub85c \uace0\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \uac01 \ub77c\uc6b4\ub4dc\uc5d0\uc11c\uc758 MATH, AIME 2024, AMC 2023, Olympiad Bench, College Math, GSM8K, GaokaoEn 2023 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c PPM\uc758 \uc815\ud655\ub3c4(Pass@1)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 PPM\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \ubc18\ubcf5 \ud559\uc2b5\uc5d0 \ub530\ub77c \uc5b4\ub5bb\uac8c \uac1c\uc120\ub418\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.4 \uc790\uae30 \uc9c4\ud654 \uc2ec\uce35 \uc0ac\uace0"}, {"content": "| Model | Method | MATH | AIME 2024 | AMC 2023 | Olympiad Bench | College Math | GSM8K | Gaokao En 2023 |\n|---|---|---|---|---|---|---|---|---|\n| *Frontier LLMs* |  |  |  |  |  |  |  |  |\n| GPT-4o | System 1 | 76.6 | 9.3 | 47.5 | 43.3 | 48.5 | 92.9 | 67.5 |\n| Claude3.5-Sonnet | System 1 | 78.3 | 16.0 | - | - | - | 96.4 | - |\n| GPT-o1-preview | - | 85.5 | 44.6 | 90.0 | - | - | - | - |\n| GPT-o1-mini | - | **90.0** | **56.7** | **95.0** | **65.3** | 57.8 | 94.8 | 78.4 |\n| *Open-Sourced Reasoning LLMs* |  |  |  |  |  |  |  |  |\n| DeepSeek-Coder-V2-Instruct | System 1 | 75.3 | 13.3 | 57.5 | 37.6 | 46.2 | 94.9 | 64.7 |\n| Mathstral-7B-v0.1 | System 1 | 57.8 | 0.0 | 37.5 | 21.5 | 33.7 | 84.9 | 46.0 |\n| NuminaMath-72B-CoT | System 1 | 64.0 | 3.3 | 70.0 | 32.6 | 39.7 | 90.8 | 58.4 |\n| LLaMA3.1-8B-Instruct | System 1 | 51.4 | 6.7 | 25.0 | 15.4 | 33.8 | 76.6 | 38.4 |\n| LLaMA3.1-70B-Instruct | System 1 | 65.4 | 23.3 | 50.0 | 27.7 | 42.5 | 94.1 | 54.0 |\n| Qwen2.5-Math-72B-Instruct | System 1 | 85.6 | 30.0 | 70.0 | 49.0 | 49.5 | 95.9 | 71.9 |\n| Qwen2.5-Math-72B-Instruct+72B ORM | System 2 | 85.8 | 36.7 | 72.5 | 54.5 | 50.6 | 96.4 | 76.9 |\n| *General Base Model: Phi3-mini-Instruct (3.8B)* |  |  |  |  |  |  |  |  |\n| Phi3-mini-Instruct (base model) | System 1 | 41.4 | 3.33 | 7.5 | 12.3 | 33.1 | 85.7 | 37.1 |\n| \\sysname **(3.8B SLM+7B PPM)** | System 2 | **85.4** | **40.0** | **77.5** | **59.3** | **58.0** | **94.5** | **77.1** |\n| \\sysname<sup>*64*</sup> **(3.8B SLM+7B PPM)** | System 2 | **86.4** | **43.3** | **80.0** | **60.3** | **59.1** | **94.7** | **77.7** |\n| *Math-Specialized Base Model: Qwen2.5-Math-1.5B* |  |  |  |  |  |  |  |  |\n| Qwen2.5-Math-1.5B (base model) | System 1 | 51.2 | 0.0 | 22.5 | 16.7 | 38.4 | 74.6 | 46.5 |\n| Qwen2.5-Math-1.5B-Instruct | System 1 | 60.0 | 10.0 | 60.0 | 38.1 | 47.7 | 84.8 | 65.5 |\n| Qwen2.5-Math-1.5B-Instruct+72B ORM | System 2 | 83.4 | 20.0 | 72.5 | 47.3 | 50.2 | 94.1 | 73.0 |\n| \\sysname **(1.5B SLM+7B PPM)** | System 2 | **87.8** | **46.7** | **80.0** | **63.5** | **59.0** | **94.3** | **77.7** |\n| \\sysname<sup>*64*</sup> **(1.5B SLM+7B PPM)** | System 2 | **88.6** | **46.7** | **85.0** | **64.6** | **59.3** | **94.8** | **79.5** |\n| *Math-Specialized Base Model: Qwen2.5-Math-7B* |  |  |  |  |  |  |  |  |\n| Qwen2.5-Math-7B (base model) | System 1 | 53.4 | 3.3 | 25.0 | 17.3 | 39.4 | 80.4 | 47.3 |\n| Qwen2.5-Math-7B-Instruct | System 1 | 73.2 | 13.3 | 62.5 | 38.2 | 45.9 | 89.9 | 62.1 |\n| Qwen2.5-Math-7B-Instruct+72B ORM | System 2 | 83.4 | 23.3 | 62.5 | 47.6 | 47.9 | **95.1** | 71.9 |\n| \\sysname **(7B SLM+7B PPM)** | System 2 | **88.2** | **43.3** | **80.0** | **63.1** | **58.4** | 94.6 | **78.2** |\n| \\sysname<sup>*64*</sup> **(7B SLM+7B PPM)** | System 2 | **88.6** | **46.7** | **85.0** | **63.4** | **59.3** | 94.8 | **79.2** |\n| Competition and College Level |  | MATH | AIME 2024 | AMC 2023 | Olympiad | College Math | GSM8K | Gaokao |\n|---|---|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  | OOD |  |", "caption": "Table 5: The results of \\sysname and other frontier LLMs on the most challenging math benchmarks. \\sysname64 shows the Pass@1 accuracy achieved when sampling 64 trajectories.", "description": "\ud45c 5\ub294 rStar-Math\ub97c \ud3ec\ud568\ud55c \ucd5c\ucca8\ub2e8 \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc774 \uac00\uc7a5 \uc5b4\ub824\uc6b4 \uc218\ud559 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub2ec\uc131\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  rStar-Math\ub294 64\uac1c\uc758 \uc2dc\ubbac\ub808\uc774\uc158 \uacb0\uacfc\ub97c \uc0d8\ud50c\ub9c1\ud558\uc5ec \uc5bb\uc740 \uc815\ud655\ub3c4(Pass@1)\ub97c \n  rStar-Math64\ub85c \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c(MATH, AIME 2024, AMC 2023, Olympiad Bench, College Math, GSM8K, GaokaoEn 2023)\uc5d0 \ub300\ud55c \uac01 \ubaa8\ub378\uc758 Pass@1 \uc815\ud655\ub3c4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 rStar-Math\uc758 \uc131\ub2a5\uc744 \ub2e4\ub978 \ucd5c\ucca8\ub2e8 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec rStar-Math\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "4 \ud3c9\uac00"}, {"content": "| Round# | MATH | AIME 2024 | AMC 2023 | Olympiad Bench | College Math | GSM8K | GaokaoEn 2023 |\n|---|---|---|---|---|---|---|---| \n| GPT-4o | 76.6 | 9.3 | 47.5 | 43.3 | 48.5 | 92.9 | 67.5 |\n| Base 7B model | 58.8 | 0.0 | 22.5 | 21.8 | 41.6 | 91.6 | 51.7 |\n| Round 1 | 75.2 | 10.0 | 57.5 | 35.7 | 45.4 | 90.9 | 60.3 |\n| Round 2 | 86.6 | 43.3 | 75.0 | 59.4 | 55.6 | 94.0 | 76.4 |\n| Round 3 | 87.0 | 46.7 | 80.0 | 61.6 | 56.5 | 94.2 | 77.1 |\n| Round 4 | 89.4 | 50.0 | 87.5 | 65.3 | 59.0 | 95.0 | 80.5 |", "caption": "Table 6: The continuously improved math reasoning capabilities through \\sysname self-evolved deep thinking. Starting from round 2, the 7B base model powered by \\sysname surpasses GPT-4o.", "description": "\ud45c 6\uc740 rStar-Math\uc758 \uc790\uae30 \uc9c4\ud654\uc801 \uc2ec\uce35 \uc0ac\uace0 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 4\ub77c\uc6b4\ub4dc\uc758 \uc790\uae30 \uc9c4\ud654\ub97c \uac70\uce58\uba74\uc11c, rStar-Math\ub294 \uc218\ud559 \ucd94\ub860 \ub2a5\ub825\uc774 \uc9c0\uc18d\uc801\uc73c\ub85c \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788 2\ub77c\uc6b4\ub4dc\ubd80\ud130\ub294 7B \uae30\ubcf8 \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 GPT-4o\ub97c \ub2a5\uac00\ud558\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 MATH, AIME 2024, AMC 2023, Olympiad Bench, College Math, GSM8K, GaokaoEn 2023 \ub4f1 \ub2e4\uc591\ud55c \uc218\ud559 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uac01 \ub77c\uc6b4\ub4dc\ubcc4 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec rStar-Math\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.4 \uc790\uae30 \uc9c4\ud654\uc801 \uc2ec\uce35 \uc0ac\uace0"}, {"content": "| Dataset | MATH | AIME | AMC | Olympiad Bench | College Math | GSM8K | GaokaoEn 2023 |\n|---|---|---|---|---|---|---|---| \n| GPT-4o | - | 76.6 | 9.3 | 47.5 | 43.3 | 48.5 | **92.9** | **67.5** |\n| GPT4-distillation (Open-sourced) | MetaMath | 55.2 | 3.33 | 32.5 | 19.1 | 39.2 | 85.1 | 43.6 |\n| NuminaMath-CoT | 69.6 | 10.0 | **50.0** | 37.2 | 43.4 | 89.8 | 59.5 |\n| Self-generation by policy SLM-r3 | Random sample | 72.4 | 10.0 | 45.0 | 41.0 | 48.0 | 87.5 | 57.1 |\n|  | Rejection sampling | 73.4 | 13.3 | 47.5 | 44.7 | 50.8 | 89.3 | 61.7 |\n| Step-by-step verified (ours) | **78.4** | **26.7** | 47.5 | **47.1** | **52.5** | 89.7 | 65.7 |", "caption": "Table 7: Ablation study on the effectiveness of our step-by-step verified reasoning trajectories as the SFT dataset. We report the SFT accuracy of Qwen2.5-Math-7B fine-tuned with different datasets.", "description": "\ud45c 7\uc740 \ub2e8\uacc4\ubcc4\ub85c \uac80\uc99d\ub41c \ucd94\ub860 \uacbd\ub85c\uc758 \ud6a8\uacfc\ub97c \ud3c9\uac00\ud558\uae30 \uc704\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294  Qwen2.5-Math-7B \ubaa8\ub378\uc744 \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ud558\uc5ec  \uc218\ud559 \ucd94\ub860 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 MATH, AIME, AMC, Olympiad Bench, College Math, GSM8K, GaokaoEn \ub4f1 \ub2e4\uc591\ud55c \uc218\ud559 \ubb38\uc81c \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4(Pass@1)\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc740 \uc11c\ub85c \ub2e4\ub978 \ubc29\uc2dd\uc73c\ub85c \uc0dd\uc131\ub418\uc5c8\uc73c\uba70,  \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ud558\ub294 \ub2e8\uacc4\ubcc4 \uac80\uc99d\ub41c \ucd94\ub860 \uacbd\ub85c\ub97c \uc0ac\uc6a9\ud55c \ub370\uc774\ud130\uc14b\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc8fc\uace0\uc790 \ud569\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574,  \ub2e8\uacc4\ubcc4 \uac80\uc99d\ub41c \ucd94\ub860 \uacbd\ub85c \ub370\uc774\ud130\uc14b\uc774 Qwen2.5-Math-7B \ubaa8\ub378\uc758 \uc218\ud559 \ucd94\ub860 \ub2a5\ub825 \ud5a5\uc0c1\uc5d0 \uc5bc\ub9c8\ub098 \ud6a8\uacfc\uc801\uc778\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.2 Step-by-Step Verified Reasoning Trajectory"}, {"content": "| RM | Inference | MATH | AIME | AMC | Olympiad Bench | College Math | GSM8K | GaokaoEn |\n|---|---|---|---|---|---|---|---|---|\n| o1-mini | - | 90.0 | 56.7 | 95.0 | 65.3 | 55.6 | 94.8 | 78.6 |\n| ORM | Best-of-N | 82.6 | 26.7 | 65.0 | 55.1 | 55.5 | 92.3 | 72.5 |\n| PQM | MCTS | 88.2 | 46.7 | 85.0 | 62.9 | 57.6 | 94.6 | 79.5 |\n| PPM | MCTS | 89.4 | 50.0 | 87.5 | 65.3 | 59.0 | 95.0 | 80.5 |", "caption": "Table 8: Ablation study on the reward model. Process reward models (PQM and PPM) outperform ORM, with PPM pushing the frontier of math reasoning capabilities.", "description": "\ud45c 8\uc740 \ubcf4\uc0c1 \ubaa8\ub378\uc5d0 \ub300\ud55c \ucd94\uac00 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e8\uc21c\ud788 \uacb0\uacfc(Outcome)\ub9cc\uc744 \ubcf4\uc0c1\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 ORM(Outcome Reward Model)\uacfc \uc911\uac04 \uacfc\uc815(Process)\uc758 \uc810\uc218\ub97c \ubcf4\uc0c1\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 PQM(Process Reward Model), PPM(Process Preference Model) \uc138 \uac00\uc9c0 \ubc29\ubc95\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. \uadf8 \uacb0\uacfc, \uc911\uac04 \uacfc\uc815\uc758 \uc810\uc218\ub97c \ud65c\uc6a9\ud558\ub294 PQM\uacfc PPM\uc774 ORM\ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc6b0\uc218\ud558\uba70, \ud2b9\ud788 PPM\uc774 \uc218\ud559 \ucd94\ub860 \ub2a5\ub825\uc758 \ucd5c\ucca8\ub2e8\uc744 \uc774\ub04c\uace0 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 Evaluation"}, {"content": "| MATH | AIME 2024 | AMC 2023 | Olympiad Bench | College Math | GSM8K | GaokaoEn 2023 |\n|---|---|---|---|---|---|---|\n| 5453 | 15693 | 14544 | 7889 | 4503 | 3299 | 6375 |", "caption": "Table 9: Inference costs of \\sysname. We show the average number of generated tokens required to generate a trajectory for a given question.", "description": "\ud45c 9\ub294 rStar-Math \ubaa8\ub378\uc774 \uc8fc\uc5b4\uc9c4 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub2f5\uc744 \uc0dd\uc131\ud558\uae30 \uc704\ud574 \uc0dd\uc131\ud574\uc57c \ud558\ub294 \ud1a0\ud070\uc758 \ud3c9\uade0 \uac1c\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc9c8\ubb38 \uc720\ud615(MATH, AIME 2024, AMC 2023, Olympiad Bench, College Math, GSM8K, GaokaoEn 2023)\uc5d0 \ub300\ud574 \uc0dd\uc131\ub41c \ud1a0\ud070\uc758 \ud3c9\uade0 \uc218\ub97c \uc81c\uc2dc\ud558\uc5ec, rStar-Math \ubaa8\ub378\uc758 \ucd94\ub860 \ube44\uc6a9\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uac01 \ubb38\uc81c \uc720\ud615\uc5d0 \ub530\ub978 \ubaa8\ub378\uc758 \uacc4\uc0b0 \ubcf5\uc7a1\ub3c4\uc640 \uc790\uc6d0 \uc18c\ubaa8\ub7c9\uc744 \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4 Evaluation"}, {"content": "| Model | MATH | AIME 2024 | AMC 2023 | Olympiad Bench | College Math | GSM8K | GaokaoEn 2023 |\n|---|---|---|---|---|---|---|---| \n| *General Base Model: Phi3-mini-Instruct (3.8B)* |  |  |  |  |  |  |  |\n| Phi3-mini-Instruct | 41.4 | 3.33 | 7.5 | 12.3 | 33.1 | 85.7 | 37.1 |\n| **Our policy model** | **68.0** | **10.0** | **37.5** | **36.6** | **48.7** | **87.9** | **53.2** |\n| *Math-Specialized Base Model: Qwen2.5-Math-1.5B* |  |  |  |  |  |  |  |\n| Qwen2.5-Math-1.5B | 51.2 | 0.0 | 22.5 | 16.7 | 38.4 | 74.6 | 46.5 |\n| Qwen2.5-Math-1.5B-Instruct | 60.0 | 10.0 | **60.0** | 38.1 | 47.7 | **84.8** | 65.5 |\n| **Our policy model** | **74.8** | **13.3** | 47.5 | **42.5** | **50.1** | 83.1 | **58.7** |\n| *Math-Specialized Base Model: Qwen2-Math-7B* |  |  |  |  |  |  |  |\n| Qwen2-Math-7B | 53.4 | 3.3 | 25.0 | 17.3 | 39.4 | 80.4 | 47.3 |\n| Qwen2-Math-7B-Instruct | 73.2 | 13.3 | **62.5** | 38.2 | 45.9 | **89.9** | 62.1 |\n| **Our policy model** | **73.8** | **16.7** | 45.0 | **43.9** | **52.0** | 88.3 | **65.2** |\n| *Math-Specialized Base Model: Qwen2.5-Math-7B* |  |  |  |  |  |  |  |\n| Qwen2.5-Math-7B | 58.8 | 0.0 | 22.5 | 21.8 | 41.6 | 91.6 | 51.7 |\n| Qwen2.5-Math-7B-Instruct | **82.6** | 6.0 | **62.5** | 41.6 | 46.8 | **95.2** | **66.8** |\n| **Our policy model** | 78.4 | **26.7** | 47.5 | **47.1** | **52.5** | 89.7 | 65.7 |", "caption": "Table 10: Pass@1 (greedy) accuracy of our fine-tuned policy models for Phi3-mini, Qwen2.5-Math-1.5B, Qwen2-Math-7B and Qwen2.5-Math-7B.", "description": "\ud45c 10\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ubc29\ubc95\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ub41c \uc815\ucc45 \ubaa8\ub378\ub4e4\uc758 Pass@1 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Pass@1 \uc815\ud655\ub3c4\ub294 \ubaa8\ub378\uc774 \uc815\ub2f5\uc744 \ucc98\uc74c \uc81c\uc2dc\ud588\uc744 \ub54c\uc758 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uc138 \uac00\uc9c0 \ud06c\uae30\uc758 \uc5b8\uc5b4 \ubaa8\ub378 (Phi3-mini, Qwen2.5-Math-1.5B, Qwen2-Math-7B, Qwen2.5-Math-7B)\uc5d0 \ub300\ud55c Pass@1 \uc815\ud655\ub3c4\uac00 \ub2e4\uc591\ud55c \uc218\ud559\uc801 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud574 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uae30\ubcf8 \uc131\ub2a5\uacfc \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ubc29\ubc95\uc744 \uc801\uc6a9\ud55c \ud6c4\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec, \uc81c\uc548\ud558\ub294 \ubc29\ubc95\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 Evaluation"}]