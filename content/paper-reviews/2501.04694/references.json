{"references": [{"fullname_first_author": "OpenAI", "paper_title": "GPT-4 Technical Report", "publication_date": "2023", "reason": "GPT-4 is a leading large language model and its technical report provides valuable insights into its architecture and capabilities, which are relevant to the advancements in code generation presented in this paper."}, {"fullname_first_author": "Jason Wei", "paper_title": "Finetuned Language Models Are Zero-Shot Learners", "publication_date": "2021", "reason": "This paper introduced instruction tuning as a key technique for improving the performance and alignment of LLMs, which is directly relevant to the instruction tuning method used in this paper for code generation."}, {"fullname_first_author": "Qihao Zhu", "paper_title": "DeepSeek-coder-v2: Breaking the barrier of closed-source models in code intelligence", "publication_date": "2024", "reason": "This paper introduced a powerful closed-source model and established a new state-of-the-art in code generation, providing a strong benchmark for comparison with the proposed EpiCoder model."}, {"fullname_first_author": "Anton Lozhkov", "paper_title": "The Stack v2", "publication_date": "2024", "reason": "The Stack v2 is a large-scale code dataset used for pre-training code LLMs, therefore it is a crucial dataset for the training and development of EpiCoder."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021", "reason": "This paper introduced several benchmark datasets for evaluating the performance of code generation LLMs; these benchmarks are used to evaluate EpiCoder, making it a crucial reference."}]}