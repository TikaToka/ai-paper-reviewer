[{"content": "| Dataset Name | Image Source | Sample | Annotated by | Avg. Words | Masks |\n|---|---|---|---|---|---| \n| BLIP-LCS | LAION [53], CC [4], SBU [45] | 558K | BLIP [30] | 54 | \u2717 |\n| DenseFusion1M [32] | LAION [53] | 1,059K | Vision Specialist Models | 191 | \u2717 |\n| LLaVA-Recap118K [38] | COCO [35] | 118K | LLaVA-NEXT [38] | 186 | \u2717 |\n| LLaVA-Details-23K [37] | COCO [35] | 23K | GPT4 | 105 | \u2717 |\n| ShareGPT4V [5] | LAION [53], CC [4], SBU [45], COCO [35] etc. | 100K | GPT4-Vision | 162 | \u2717 |\n| ShareGPT4V-PT [5] | LAION [53], CC [4], SBU [45], COCO [35] etc. | 1,246K | Share-Captioner [5] | 144 | \u2717 |\n| PixelLM-MUSE [51] | LVIS [17] | 246K | GPT4-Vision | - | 3.7\u2021 |\n| Osprey [69] | COCO [35] | 724K | GPT4-Vision | - | - |\n| GLaMM-GCG [50] | RefCOCOg [40],PSG [65],Flick30K [47] | 214K | Vision Specialist Models | 128 | 3.6 |\n| COCO-caption [6] | COCO [35] | 118K | Human | 11 | \u2717 |\n| DCI [61] | SA-1B [24] | 8K | Human | 144 | \u2717 |\n| DOCCI [44] | DOCCI [44] | 9.6K | Human | 136 | \u2717 |\n| IIW [15] | WebLI [15] | 8.5K | Human | 217 | \u2717 |\n| COCONut-PanCap (ours) | COCO [35] | 118K | Human | 203 | 13.2 |", "caption": "Table 1: \nDataset (training set) Comparison. \nOur proposed COCONut-PanCap dataset stands out for its detailed (2nd highest in Average Words), high-quality (human interactive annotated) captions and high-density segmentation masks (1st in Average Masks).\n\u2021 denotes the mask number for referring segmentation which only counts the targets in QA format. Note that \u201cSamples\u201d means the number of collected annotations, where there may exist one image with multiple different annotation, i.e., in region-level datasets like Osprey.", "description": "\ud45c 1\uc740 \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0 \ucea1\uc158 \ub370\uc774\ud130\uc14b\uc758 \ube44\uad50 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc81c\uc548\ub41c COCONut-PanCap \ub370\uc774\ud130\uc14b\uc740 \ud3c9\uade0 \ub2e8\uc5b4 \uc218 \uae30\uc900\uc73c\ub85c \ub450 \ubc88\uc9f8\ub85c \uc790\uc138\ud55c \ucea1\uc158\uc744 \uc81c\uacf5\ud558\uba70, \uc0ac\ub78c\uc774 \uc9c1\uc811 \uc0c1\ud638\uc791\uc6a9\ud558\uba70 \uc8fc\uc11d\uc744 \ub2ec\uc558\uae30 \ub54c\ubb38\uc5d0 \ub192\uc740 \ud488\uc9c8\uc744 \uc790\ub791\ud569\ub2c8\ub2e4. \ub610\ud55c, \ud3c9\uade0 \ubd84\ud560 \ub9c8\uc2a4\ud06c \uc218\uc5d0\uc11c 1\uc704\ub97c \ucc28\uc9c0\ud558\uc5ec \uace0\ubc00\ub3c4 \ubd84\ud560 \ub9c8\uc2a4\ud06c\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \ucc38\uace0\ub85c, \ucc38\uc870 \ubd84\ud560(Referring Segmentation)\uc758 \ub9c8\uc2a4\ud06c \uc218\ub294 \uc9c8\ubb38 \ub2f5\ubcc0 \ud615\uc2dd\uc5d0\uc11c \ub300\uc0c1\ub9cc \uacc4\uc0b0\ud569\ub2c8\ub2e4. '\uc0d8\ud50c'\uc740 \uc218\uc9d1\ub41c \uc8fc\uc11d\uc758 \uc218\ub97c \ub098\ud0c0\ub0b4\uba70, Osprey\uc640 \uac19\uc740 \uc601\uc5ed \uc218\uc900 \ub370\uc774\ud130\uc14b\uc5d0\uc11c\ub294 \ud558\ub098\uc758 \uc774\ubbf8\uc9c0\uc5d0 \uc5ec\ub7ec \uac1c\uc758 \uc8fc\uc11d\uc774 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3 COCONut-PanCap \ub370\uc774\ud130\uc14b"}, {"content": "| Dataset Name | Samples | Avg. Words | Caption | T2I | Grd. Seg. |\n|---|---|---|---|---|---| \n| COCO-30K [6] | 30,000 | 11 | \u2713 | \u2713 | \u2717 |\n| DOCCI-test [44] | 5,000 | 136 | \u2713 | \u2713 | \u2717 |\n| IIW-test [15] | 445 | 217 | \u2713 | \u2713 | \u2717 |\n| GenEval [16] | 553 | 8 | \u2717 | \u2713 | \u2717 |\n| T2I-CompBench val [20] | 2400 | 9 | \u2717 | \u2713 | \u2717 |\n| GLaMM-GCG val-test [50] | 2,000 | 128 | \u2713 | \u2717 | \u2713 |\n| COCONut-PanCap val (ours) | 25,000 | 233 | \u2713 | \u2713 | \u2713 |", "caption": "Table 2: \nDataset (evaluation set) Comparison.\nOur COCONut-PanCap validation set provides detailed captions and supports multiple multi-modal tasks, including image captioning, text-to-image generation (T2I), and grounded segmentation (Grd. Seg.).", "description": "\ud45c 2\ub294 COCONut-PanCap \uac80\uc99d \uc138\ud2b8\uc5d0 \ub300\ud55c \ube44\uad50 \ubd84\uc11d \ud45c\uc785\ub2c8\ub2e4.  \uae30\uc874 \ub370\uc774\ud130\uc14b\uacfc \ube44\uad50\ud558\uc5ec COCONut-PanCap \uac80\uc99d \uc138\ud2b8\uac00 \uc790\uc138\ud55c \ucea1\uc158\uc744 \uc81c\uacf5\ud558\uace0 \uc774\ubbf8\uc9c0 \ucea1\uc158 \uc0dd\uc131, \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uc0dd\uc131, \uadf8\ub9ac\uace0 \uae30\ubc18 \ubd84\ud560 \ub4f1 \ub2e4\uc591\ud55c \ub2e4\uc911 \ubaa8\ub4dc \uc791\uc5c5\uc744 \uc9c0\uc6d0\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989,  \ub2e4\uc591\ud55c \uba40\ud2f0\ubaa8\ub2ec \uc791\uc5c5\uc5d0 \ud65c\uc6a9\ub420 \uc218 \uc788\ub294 \ud48d\ubd80\ud55c \ub370\uc774\ud130\uc14b\uc784\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "3. COCONut-PanCap \ub370\uc774\ud130\uc14b"}, {"content": "| Training recipe | Method | Pretrain Dataset | Instruction-tuning dataset | Mask pooled | CAPTURE | CIDEr | BLEU@4 | METEOR | ROUGE-L |\n|---|---|---|---|---|---|---|---|---|---| \n| finetune | LLaVA-NeXT* | LAION-CC-SBU | LLaVA 665K | \u2717 | 55.4 | 10.8 | 4.2 | 13.2 | 23.1 |\n| finetune | LLaVA-NeXT | LAION-CC-SBU | LLaVA 665K-COCONut-PanCap | \u2717 | 58.7 | 11.2 | 4.8 | 16.2 | 24.6 |\n| finetune | LLaVA-NeXT-pool | LAION-CC-SBU | LLaVA 665K-COCONut-PanCap | \u2713 | 61.4 | 13.1 | 5.3 | 17.1 | 26.8 |\n| finetune | LLaVA-NeXT-I | LAION-CC-SBU | LLaVA 665K-InternVL2-Cap | \u2717 | 53.9 | 9.4 | 4.4 | 11.5 | 21.4 |\n| finetune | LLaVA-NeXT-Q | LAION-CC-SBU | LLaVA 665K-Qwen2VL-Cap | \u2717 | 55.4 | 8.9 | 4.6 | 12.9 | 22.5 |\n| finetune | LLaVA-NeXT-G | LAION-CC-SBU | LLaVA 665K-GPT4V-Cap | \u2717 | 56.2 | 9.6 | 4.7 | 13.3 | 22.8 |", "caption": "Table 3: Caption Benchmark Results Evaluated on Our COCONut-PanCap Val Set. Note that the amount of data in the instruction dataset remains the same; only the sources of the detailed captions vary, with a total of 23K images that have detailed captions.", "description": "\ud45c 3\uc740 COCONut-PanCap \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c \ud3c9\uac00\ub41c \uc790\ub9c9 \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\uc5d0 \ub530\ub974\uba74, \uc9c0\uc2dc \ub370\uc774\ud130 \uc138\ud2b8\uc758 \ub370\uc774\ud130 \uc591\uc740 \ub3d9\uc77c\ud558\uac8c \uc720\uc9c0\ub418\uace0 \uc790\uc138\ud55c \ucea1\uc158\uc758 \ucd9c\ucc98\ub9cc \ub2e4\ub985\ub2c8\ub2e4. \ucd1d 23,000\uac1c\uc758 \uc774\ubbf8\uc9c0\uac00 \uc790\uc138\ud55c \ucea1\uc158\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4.  LLaVA-NeXT \uae30\ubcf8 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec, COCONut-PanCap \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788,  Panoptic Segmentation \ub9c8\uc2a4\ud06c\ub97c \ucd94\uac00\ud55c LLaVA-NeXT-pool \ubaa8\ub378\uc740 \ub2e4\ub978 \ubaa8\ub378\ub4e4\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ud6e8\uc52c \ub6f0\uc5b4\ub0a8\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \ud569\uc131 \ucea1\uc158\uc73c\ub85c \ud6c8\ub828\ub41c \ubaa8\ub378\ub4e4\uc740 \uc778\uac04\uc774 \uc791\uc131\ud55c \ucea1\uc158\uc73c\ub85c \ud6c8\ub828\ub41c \ubaa8\ub378\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub0ae\uc740 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Method | Pretrain dataset | Instruction dataset | Mask pooled | CAPTURE | CIDEr | BLEU@4 | METEOR | PQ | PQ<sup>thing</sup> | PQ<sup>stuff</sup> |\n|---|---|---|---|---|---|---|---|---|---|---|\n| LISA+ * | LAION-CC-SBU | GranDf | \u2717 | 46.2 | 6.6 | 3.8 | 9.8 | 0.43 | 0.41 | 0.45 |\n| LISA+ | LAION-CC-SBU | COCONut-PanCap (ours) | \u2717 | 57.9 | 8.1 | 4.9 | 13.8 | 0.50 | 0.49 | 0.44 |\n| GLaMM GCG * | LAION-CC-SBU+GranD | GranDf | \u2717 | 43.2 | 6.5 | 3.6 | 10.6 | 0.27 | 0.35 | 0.21 |\n| GLaMM GCG | LAION-CC-SBU+GranD | COCONut-PanCap (ours) | \u2717 | 56.8 | 7.8 | 5.2 | 14.3 | 0.55 | 0.54 | 0.46 |\n| PanCaper (ours) | LAION-CC-SBU | COCONut-PanCap (ours) | \u2717 | 62.6 | 12.0 | 5.8 | 15.4 | 0.56 | 0.55 | 0.66 |\n| PanCaper-Pro (ours) | LAION-CC-SBU | COCONut-PanCap (ours) | \u2713 | 64.3 | 12.5 | 6.4 | 17.9 | 0.61 | 0.58 | 0.68 |", "caption": "Table 4: Joint Panoptic Segmentation and Grounded Captioning (PGC) on COCONut-PanCap Val Set. * denotes reproduced results.", "description": "\ud45c 4\ub294 COCONut-PanCap \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c \uc218\ud589\ub41c \uacf5\ub3d9 \uc804\uacbd \ubd84\ud560 \ubc0f \uadfc\uac70 \uae30\ubc18 \ucea1\uc158 \uc0dd\uc131(PGC) \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec,  PanCaper \ubaa8\ub378\uc774 \ub2e4\ub978 PGC \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574  CIDEr, CAPTURE, BLEU@4, METEOR \ubc0f PQ \uc810\uc218\uc5d0\uc11c \ud6e8\uc52c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  * \ud45c\uc2dc\ub294 \uc7ac\ud604\ub41c \uacb0\uacfc\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  PanCaper-Pro \ubaa8\ub378\uc740 \ub9c8\uc2a4\ud06c \ud480\ub9c1 \uae30\ub2a5\uc744 \ucd94\uac00\ud558\uc5ec \ub354\uc6b1 \ud5a5\uc0c1\ub41c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 COCONut-PanCap \ub370\uc774\ud130\uc14b\uc774 PGC \uc791\uc5c5\uc5d0 \ub9e4\uc6b0 \ud6a8\uacfc\uc801\uc784\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4. PGC Baseline: PanCaper"}, {"content": "| Training dataset | Evaluation dataset | FID \u2193 | FDdinov2 \u2193 | CLIPScore \u2191 |\n|---|---|---|---|---|\n| SD3 PT dataset [12] | DOCCI test set [44] | 30.2 | 345 | 74.9 |\n| COCO-caption [6] |  | 27.6 | 321 | 76.8 |\n| DOCCI [44] |  | 22.1 | 300 | 77.8 |\n| COCONut-PanCap (ours) |  | 21.4 | 290 | 77.9 |\n| SD3 PT dataset [12] |  | 31.8 | 300 | 73.8 |\n| COCO-caption [6] | COCONut-PanCap | 28.0 | 294 | 74.0 |\n| DOCCI [44] | val set (ours) | 24.3 | 267 | 75.1 |\n| COCONut-PanCap (ours) |  | 23.1 | 260 | 77.3 |", "caption": "Table 5: Benchmark Results on Text Conditioned Image Generation. Stable-Diffusion-3 (SD3) medium is finetuned with COCO-Caption (short), DOCCI and our COCONut-Panoptic and evaluated on DOCCI test set\u00a0[44] and our COCONut-PanCap val set. \u2018SD3 PT dataset\u2019 denotes the pretraining dataset of SD3, and thus the rows correspond to zero-shot evaluation of SD3.", "description": "\ud45c 5\ub294 \ud14d\uc2a4\ud2b8 \uc870\uac74\ubd80 \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc5d0 \ub300\ud55c \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Stable Diffusion 3(SD3) \ubbf8\ub514\uc5c4 \ubaa8\ub378\uc744 COCO-Caption(\uc9e7\uc740 \ubc84\uc804), DOCCI, \uadf8\ub9ac\uace0 \uc5f0\uad6c\ud300\uc774 \uc81c\uc791\ud55c COCONut-PanCap \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubbf8\uc138 \uc870\uc815\ud588\uc73c\uba70, DOCCI \ud14c\uc2a4\ud2b8 \uc138\ud2b8[44]\uc640 \uc5f0\uad6c\ud300\uc758 COCONut-PanCap \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. 'SD3 PT \ub370\uc774\ud130\uc14b'\uc740 SD3\uc758 \uc0ac\uc804 \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc744 \ub098\ud0c0\ub0b4\uba70, \ud574\ub2f9 \ud589\uc740 SD3\uc758 \uc81c\ub85c\uc0f7 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "|       | w/o FT | COCO-caption [6] | DOCCI [44] | COCONut-PanCap |\n| :---- | :----: | :----: | :----: | :----: |\n| color attribution | 0.37 | 0.34 | 0.38 | 0.40 |\n| colors | 0.73 | 0.70 | 0.74 | 0.75 |\n| position | 0.33 | 0.30 | 0.36 | 0.36 |\n| counting | 0.65 | 0.64 | 0.65 | 0.70 |\n| single object | 0.96 | 0.94 | 0.95 | 0.96 |\n| two objects | 0.80 | 0.78 | 0.81 | 0.89 |\n| overall score | 0.64 | 0.62 | 0.65 | 0.68 |", "caption": "Table 6: \nEffects of Fine-tuning the SD3-medium (T2I model) with Different Datasets on GenEval\u00a0[16]. w/o FT denotes the model is not finetuned with any datasets (i.e., zero-shot testing).", "description": "\ud45c 6\uc740 GenEval [16] \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec SD3-medium (\ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubaa8\ub378)\uc744 \uc5ec\ub7ec \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ud588\uc744 \ub54c\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'w/o FT'\ub294 \uc5b4\ub5a4 \ub370\uc774\ud130\uc14b\uc73c\ub85c\ub3c4 \ubbf8\uc138 \uc870\uc815\ud558\uc9c0 \uc54a\uc740 \ubaa8\ub378(\uc989, \uc81c\ub85c\uc0f7 \ud14c\uc2a4\ud2b8)\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud45c\ub294 \uc0c9\uc0c1 \uc9c0\uc815, \uc0c9\uc0c1, \uc704\uce58, \uacc4\uc0b0, \ub2e8\uc77c \uac1c\uccb4 \ubc0f \ub450 \uac1c\uccb4\uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc18d\uc131\uc744 \ud3c9\uac00\ud558\ub294 GenEval \uc9c0\ud45c\uc5d0 \ub530\ub978 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ud558\ub294 \uac83\uc774 GenEval \uc791\uc5c5\uc5d0\uc11c \ubaa8\ub378 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \uc5b4\ub5bb\uac8c \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Method | LLM | Instruction-tuning Dataset | MM-Vet | Seed-IMG | MMBench-en | TextVQA | POPE | MME |\n|---|---|---|---|---|---|---|---|---|\n| LLaVA-NeXT * | Llama3-8B | orginal LLaVA 665K [38] | 43.5 | 70.1 | 71.4 | 68.9 | 85.4 | 1523 |\n| LLaVA-NeXT-20K | Llama3-8B | LLaVA 665K-COCONut-PanCap-20K | 44.1 | 72.5 | 73.6 | 69.8 | 86.1 | 1552 |\n| LLaVA-NeXT-50K | Llama3-8B | LLaVA 665K-COCONut-PanCap-50K | 44.6 | 73.1 | 74.2 | 70.0 | 87.1 | 1600 |\n| LLaVA-NeXT-Full | Llama3-8B | LLaVA 665K-COCONut-PanCap-118K | 45.5 | 74.3 | 75.1 | 70.7 | 87.9 | 1612 |\n| LLaVA-1.5 | Vicuna-7B | LLaVA 665K-ShareGPT4V-100K | 37.8 | 67.4 | 70.5 | 64.6 | 84.7 | 1519 |\n| LLaVA-1.5 | Vicuna-7B | LLaVA 665K-COCONut-PanCap-20K | 38.5 | 67.7 | 70.9 | 64.5 | 84.9 | 1521 |", "caption": "Table 7: Benchmark Results and Ablation Study on VQA. By adding extra detailed caption data for instruction tuning, the models show increased improvement. * denotes reproduced results. Using only 20K human labeled data can still achieve comparable performance to 100K synthetic data.", "description": "\ud45c 7\uc740 VQA(Visual Question Answering) \uc791\uc5c5\uc5d0 \ub300\ud55c \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\uc640 ablation study\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubcf8 \uc5f0\uad6c\ub294 instruction tuning\uc5d0 \uc0ac\uc6a9\ub418\ub294 \ub370\uc774\ud130\uc14b\uc5d0 \uc0c1\uc138\ud55c \ucea1\uc158 \ub370\uc774\ud130\ub97c \ucd94\uac00\ud568\uc73c\ub85c\uc368 \ubaa8\ub378 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c VQA \ubca4\uce58\ub9c8\ud06c(MM-Vet, Seed-IMG, MMBench-en, TextVQA, POPE, MME)\uc5d0\uc11c \uc5ec\ub7ec \ubaa8\ub378 \ubcc0\ud615(LLaVA-NeXT, LLaVA-NeXT-20K, LLaVA-NeXT-50K, LLaVA-NeXT-Full, LLaVA-1.5)\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ud2b9\ud788, 2\ub9cc \uac1c\uc758 \uc0ac\ub78c\uc774 \uc9c1\uc811 \uc791\uc131\ud55c \ub370\uc774\ud130\ub9cc\uc73c\ub85c\ub3c4 10\ub9cc \uac1c\uc758 \ud569\uc131 \ub370\uc774\ud130\uc640 \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. * \ud45c\uc2dc\ub294 \uc7ac\ud604\ub41c \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "5. Experimental Results"}, {"content": "| Method | refCOCO val | refCOCO testA | refCOCO testB | refCOCO+ val | refCOCO+ testA | refCOCO+ testB | refCOCOg val | refCOCOg test |\n|---|---|---|---|---|---|---|---|---|\n| GLaMM* [50] | 77.5 | 79.2 | 74.9 | 71.3 | 74.7 | 61.5 | 71.3 | 71.9 |\n| PixelLM [51] | 73.0 | 76.5 | 68.2 | 66.3 | 71.7 | 58.3 | 69.3 | 70.5 |\n| LISA-7B [28] | 74.1 | 76.5 | 71.1 | 62.4 | 67.4 | 56.5 | 66.4 | 68.5 |\n| PanCaper<sup>+</sup> | 74.5 | 76.7 | 69.9 | 69.9 | 73.4 | 59.5 | 69.8 | 70.6 |\n| PanCaper<sup>+</sup> + COCONut-PanCap | 76.2 | 77.1 | 72.3 | 70.5 | 73.9 | 60.1 | 72.1 | 71.6 |", "caption": "Table 8: Benchmark Results on Referring Segmentation. * denotes reproduced results. It is noted that GLaMM uses extra data from the GranD dataset for pretraining. + denotes our PanCaper model is adapted for referring segmentation task.", "description": "\ud45c 8\uc740 Referring Segmentation \uc791\uc5c5\uc5d0 \ub300\ud55c \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \ubc29\ubc95(LISA+, PixelLM, GLaMM, PanCaper)\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uc81c\uc2dc\ud558\uba70, \uac01 \ubc29\ubc95\uc758 \uc0ac\uc804 \ud559\uc2b5 \ub370\uc774\ud130\uc14b,  \uc0ac\uc6a9\ub41c Instruction dataset,  \uadf8\ub9ac\uace0 Referring Segmentation \uc791\uc5c5\uc5d0 \ub300\ud55c \uc131\uacfc \uc9c0\ud45c(refCOCO, refCOCO+, refCOCOg\uc5d0 \ub300\ud55c val, testA, testB)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  GLaMM\uc740 GranD \ub370\uc774\ud130\uc14b\uc744 \ucd94\uac00\uc801\uc73c\ub85c \uc0ac\uc804 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud588\uc73c\uba70, PanCaper+\ub294 PanCaper \ubaa8\ub378\uc744 Referring Segmentation \uc791\uc5c5\uc5d0 \ub9de\ucdb0 \uc218\uc815\ud55c \ubc84\uc804\uc784\uc744 \uc8fc\uc11d\uc5d0\uc11c \uc124\uba85\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \ub530\ub77c\uc11c \uc774 \ud45c\ub294 \uc81c\uc548\ub41c PanCaper \ubaa8\ub378\uc758 Referring Segmentation \uc791\uc5c5 \uc131\ub2a5\uc744 \uae30\uc874 \ubc29\ubc95\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \ud3c9\uac00\ud558\ub294 \ub370 \uc911\uc810\uc744 \ub461\ub2c8\ub2e4.", "section": "4. PGC Baseline: PanCaper"}]