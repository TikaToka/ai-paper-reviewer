[{"figure_path": "https://arxiv.org/html/2502.02589/x2.png", "caption": "Figure 1: \nCOCONut-PanCap Dataset.\nTop: The proposed COCONut-PanCap dataset features detailed captions grounded with dense panoptic segmentation masks.\nBottom: COCONut-PanCap supports various fine-grained understanding and generation tasks, including detailed captioning, panoptic segmentation grounded caption, and text-to-image generation.\nThe dataset also facilitates several downstream tasks, such as visual question-answering (VQA) and referring segmentation.", "description": "\uadf8\ub9bc 1\uc740 COCONut-PanCap \ub370\uc774\ud130\uc14b\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0c1\ub2e8\uc5d0\ub294 \ubc00\uc9d1\ub41c \uc804\uccb4 \uc601\uc5ed \ubd84\ud560 \ub9c8\uc2a4\ud06c\ub97c \uae30\ubc18\uc73c\ub85c \uc138\ubd80\uc801\uc778 \ucea1\uc158\uc744 \uc81c\uacf5\ud558\ub294 COCONut-PanCap \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud558\ub2e8\uc5d0\ub294 \uc138\ubd80\uc801\uc778 \ucea1\uc158 \uc0dd\uc131, \uc804\uccb4 \uc601\uc5ed \ubd84\ud560 \uae30\ubc18 \ucea1\uc158, \uc774\ubbf8\uc9c0 \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \ub4f1 \ub2e4\uc591\ud55c \uc138\ubd80\uc801\uc778 \uc774\ud574 \ubc0f \uc0dd\uc131 \uc791\uc5c5\uc744 \uc9c0\uc6d0\ud558\ub294 COCONut-PanCap\uc758 \uae30\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c, \uc2dc\uac01\uc801 \uc9c8\uc758\uc751\ub2f5(VQA) \ubc0f \ucc38\uc870 \ubd84\ud560\uacfc \uac19\uc740 \uc5ec\ub7ec \ud558\uc704 \uc791\uc5c5\ub3c4 \uc9c0\uc6d0\ud569\ub2c8\ub2e4.  \uc804\uccb4\uc801\uc73c\ub85c, \uc774 \uadf8\ub9bc\uc740 COCONut-PanCap \ub370\uc774\ud130\uc14b\uc774 \uc774\ubbf8\uc9c0 \uc774\ud574 \ubc0f \uc0dd\uc131 \uc791\uc5c5\uc5d0 \uc0ac\uc6a9\ub418\ub294 \ub2e4\uc591\ud55c \uae30\ub2a5\uc744 \uc81c\uacf5\ud568\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "COCONut-PanCap \ub370\uc774\ud130\uc14b"}, {"figure_path": "https://arxiv.org/html/2502.02589/x3.png", "caption": "Figure 2: Annotation Pipeline. Given an input image, human-annotated panoptic segmentation masks are overlaid using set-of-marks\u00a0[66] visualization techniques to prompt the vision-language model (VLM). After generating an initial draft, human effort is investigated for editing and verification. Finally, the annotated metadata will be formatted to construct the datasets for various tasks at instruction tuning or finetuning stage.", "description": "\uadf8\ub9bc 2\ub294 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c \uc8fc\uc11d \ucd94\uac00 \ud30c\uc774\ud504\ub77c\uc778\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uba3c\uc800, \uc0ac\ub78c\uc774 \uc9c1\uc811 \uc8fc\uc11d\uc744 \ub2e8 \uc804\uacbd \ubd84\ud560 \ub9c8\uc2a4\ud06c\uac00 \uc8fc\uc5b4\uc9c4 \uc785\ub825 \uc774\ubbf8\uc9c0\uc5d0 \uc624\ubc84\ub808\uc774\ub429\ub2c8\ub2e4. \uc774\ub54c, set-of-marks [66] \uc2dc\uac01\ud654 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \ube44\uc804 \uc5b8\uc5b4 \ubaa8\ub378(VLM)\uc744 \ud504\ub86c\ud504\ud2b8\ud569\ub2c8\ub2e4. VLM\uc774 \ucd08\uae30 \ucd08\uc548\uc744 \uc0dd\uc131\ud55c \ud6c4\uc5d0\ub294 \uc0ac\ub78c\uc774 \uc9c1\uc811 \ud3b8\uc9d1\ud558\uace0 \uac80\uc99d\ud558\ub294 \uacfc\uc815\uc744 \uac70\uce69\ub2c8\ub2e4. \ucd5c\uc885\uc801\uc73c\ub85c, \uc8fc\uc11d\uc774 \ub2ec\ub9b0 \uba54\ud0c0\ub370\uc774\ud130\ub294 \uc9c0\uc2dc \uc870\uc815 \ub610\ub294 \ubbf8\uc138 \uc870\uc815 \ub2e8\uacc4\uc5d0\uc11c \ub2e4\uc591\ud55c \uc791\uc5c5\uc744 \uc704\ud55c \ub370\uc774\ud130\uc14b\uc744 \uad6c\uc131\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \uc774 \uacfc\uc815\uc744 \ud1b5\ud574 \ubcf4\ub2e4 \uc815\ud655\ud558\uace0 \uc0c1\uc138\ud55c \uc774\ubbf8\uc9c0 \ucea1\uc158\uc744 \uc5bb\uc744 \uc218 \uc788\uc73c\uba70, \ub2e4\uc591\ud55c \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc5d0 \ud65c\uc6a9\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3 COCONut-PanCap Dataset"}, {"figure_path": "https://arxiv.org/html/2502.02589/extracted/6178518/figures/nouns_freq.png", "caption": "Figure 3: Designed Prompt Template.\nBy giving the concatenated set-of-marks images, the right side (round-1) shows the initial response and the corresponding human edits. Once finalized by humans, these edits will be merged into a single detailed caption grounded with panoptic segmentation masks, as shown in the left side (round-2).", "description": "\uadf8\ub9bc 3\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \uc138\ubd84\ud654\ub41c \ucea1\uc158 \uc0dd\uc131\uc744 \uc704\ud55c \uc8fc\uc11d \ub77c\ubca8\ub9c1 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc624\ub978\ucabd(Round 1)\uc740 \uacb0\ud569\ub41c set-of-marks \uc774\ubbf8\uc9c0\ub97c \uc785\ub825\uc73c\ub85c \uc8fc\uc5b4\uc9c4 \ucd08\uae30 \uc751\ub2f5\uacfc \uc0ac\ub78c\uc774 \uc218\uc815\ud55c \ub0b4\uc6a9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0ac\ub78c\uc774 \uc218\uc815\ud55c \ub0b4\uc6a9\uc774 \ubc18\uc601\ub41c \ucd5c\uc885 \uacb0\uacfc\ub294 \uc67c\ucabd(Round 2)\uc5d0 \ud45c\uc2dc\ub418\ub294\ub370, \uc5ec\uae30\uc5d0\ub294 \ud328\ub178\ub77c\ub9c8 \ubd84\ud560 \ub9c8\uc2a4\ud06c\ub97c \uae30\ubc18\uc73c\ub85c \uc0dd\uc131\ub41c \uc138\ubd84\ud654\ub41c \ucea1\uc158\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. COCONut-PanCap \ub370\uc774\ud130\uc14b"}, {"figure_path": "https://arxiv.org/html/2502.02589/extracted/6178518/figures/user_study.png", "caption": "Figure 4: Frequency of Extracted Nouns from the COCONut-PanCap Dataset. The top 10 most frequent nouns are: people, table, room, street, dining, man, person, cars, chairs, and field.", "description": "\uadf8\ub9bc 4\ub294 COCONut-PanCap \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ucd94\ucd9c\ub41c \uba85\uc0ac\ub4e4\uc758 \ube48\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac00\uc7a5 \ube48\ubc88\ud558\uac8c \ub098\ud0c0\ub098\ub294 \uc0c1\uc704 10\uac1c \uba85\uc0ac\ub294 \uc0ac\ub78c, \ud14c\uc774\ube14, \ubc29, \uac70\ub9ac, \uc2dd\uc0ac, \ub0a8\uc790, \uc0ac\ub78c(person), \uc790\ub3d9\ucc28, \uc758\uc790, \uadf8\ub9ac\uace0 \ub4e4\ud310\uc785\ub2c8\ub2e4. \uc774\ub294 \ub370\uc774\ud130\uc14b\uc5d0 \ud3ec\ud568\ub41c \uc774\ubbf8\uc9c0\ub4e4\uc774 \ub2e4\uc591\ud55c \uc2e4\ub0b4\uc678 \ud658\uacbd\uacfc \uc0ac\ub78c, \uc0ac\ubb3c \ub4f1\uc744 \ubb18\uc0ac\ud558\uace0 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  \uac01 \uba85\uc0ac\uc758 \ube48\ub3c4\uc218\ub294 \uc218\uc9c1 \ub9c9\ub300 \uadf8\ub798\ud504\ub85c \ud45c\ud604\ub418\uc5b4 \uc788\uc73c\uba70, \ube48\ub3c4\uac00 \ub192\uc744\uc218\ub85d \ub9c9\ub300\uc758 \uae38\uc774\uac00 \uae38\uc5b4\uc9d1\ub2c8\ub2e4. \uc774 \uadf8\ub798\ud504\ub97c \ud1b5\ud574 COCONut-PanCap \ub370\uc774\ud130\uc14b\uc758 \uc8fc\uc694 \ud2b9\uc9d5\uacfc \uc774\ubbf8\uc9c0 \ucf58\ud150\uce20\uc758 \ubd84\ud3ec\ub97c \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.3 \ub370\uc774\ud130\uc14b \ubd84\uc11d"}, {"figure_path": "https://arxiv.org/html/2502.02589/extracted/6178518/figures/fid_capture.png", "caption": "Figure 5: Caption Quality via User Study. The study involved human evaluators assessing a random sample of 1,000 captions, with a strong preference shown for captions from our dataset.", "description": "\ubcf8 \ub17c\ubb38\uc758 \uadf8\ub9bc 5\ub294 \uc0ac\uc6a9\uc790 \uc5f0\uad6c\ub97c \ud1b5\ud574 \ucea1\uc158\uc758 \uc9c8\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 1,000\uac1c\uc758 \ucea1\uc158 \uc0d8\ud50c\uc5d0 \ub300\ud574 \uc778\uac04 \ud3c9\uac00\uc790\ub4e4\uc774 \ubb34\uc791\uc704\ub85c \ud3c9\uac00\ub97c \uc218\ud589\ud558\uc600\uc73c\uba70, \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c \ub370\uc774\ud130\uc14b(COCONut-PanCap)\uc5d0\uc11c \uc0dd\uc131\ub41c \ucea1\uc158\uc5d0 \ub300\ud55c \uc120\ud638\ub3c4\uac00 \ud6e8\uc52c \ub192\uc558\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. COCONut-PanCap \ub370\uc774\ud130\uc14b \ubd84\uc11d"}, {"figure_path": "https://arxiv.org/html/2502.02589/x4.png", "caption": "Figure 6: \nVarying Synthetic and Human-Annotation Ratios.\nCAPTURE is used to evaluate the performance of LLaVA-NeXT on detailed captioning, while FID assesses the performance of SD3-medium on text-conditioned image generation.", "description": "\uadf8\ub9bc 6\uc740 \ud569\uc131 \ub370\uc774\ud130\uc640 \uc0ac\ub78c\uc774 \uc791\uc131\ud55c \uc8fc\uc11d\uc758 \ube44\uc728\uc744 \ub2ec\ub9ac\ud558\uc5ec LLaVA-NeXT\uc758 \uc0c1\uc138 \uc790\ub9c9 \uc0dd\uc131 \uc131\ub2a5\uacfc SD3-medium\uc758 \ud14d\uc2a4\ud2b8 \uc870\uac74 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. CAPTURE \uc9c0\ud45c\ub294 LLaVA-NeXT\uc758 \uc0c1\uc138 \uc790\ub9c9 \uc0dd\uc131 \uc131\ub2a5\uc744, FID \uc9c0\ud45c\ub294 SD3-medium\uc758 \ud14d\uc2a4\ud2b8 \uc870\uac74 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \uadf8\ub798\ud504\ub294 \ud569\uc131 \ub370\uc774\ud130 \ube44\uc728\uc774 \uac10\uc18c\ud558\uace0 \uc0ac\ub78c\uc774 \uc791\uc131\ud55c \uc8fc\uc11d \ube44\uc728\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ub450 \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.02589/x5.png", "caption": "(a) LLaVA-NeXt-AnyRes", "description": "\uadf8\ub9bc 7(a)\ub294 \uc6d0 \ub17c\ubb38\uc758 LLaVA-NeXt-AnyRes \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\uc640 \ud45c\ud604\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uc5f0\uc0b0 \ube44\uc6a9\uc744 \uad00\ub9ac\ud558\uae30 \uc704\ud574 \uadf8\ub9ac\ub4dc \uae30\ubc18 \uad6c\uc131\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ud328\uce58 \uc218\uc900\uacfc \uc774\ubbf8\uc9c0 \uc218\uc900 \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud558\uc5ec \uacb0\ud569\ud55c \ud6c4 LLM\uc5d0 \uc804\ub2ec\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.  \uc774 \ubc29\uc2dd\uc740 \uc131\ub2a5 \ud6a8\uc728\uc131\uacfc \uc6b4\uc601 \ube44\uc6a9 \uac04\uc758 \uade0\ud615\uc744 \ub9de\ucd94\ub294 \ub370 \ud6a8\uacfc\uc801\uc774\uc9c0\ub9cc, \uac1c\uccb4\uc758 \uc77c\ubd80\uac00 \ub2e4\ub978 \ud328\uce58\ub85c \ub098\ub258\ub294 \uacbd\uc6b0(\uc608: \uac1c\uc758 \uba38\ub9ac \uc77c\ubd80\uac00 \uc5ec\ub7ec \ud328\uce58\ub85c \ub098\ub268) \ubd88\uc644\uc804\ud55c \ud2b9\uc9d5 \ucd94\ucd9c\uc774 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "A.1. Detailed Captioning"}, {"figure_path": "https://arxiv.org/html/2502.02589/x6.png", "caption": "(b) our LLaVA-NeXt-pool", "description": "\uadf8\ub9bc 7(b)\ub294 \ub17c\ubb38\uc758 A.1\uc808 \"Detailed Captioning\" \uc5d0\uc11c \uc81c\uc548\ub41c LLaVA-NeXt-pool\uc758 \uad6c\uc870\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\uc874 LLaVA-NeXt(\uadf8\ub9bc 7(a))\ub294 \uc774\ubbf8\uc9c0\ub97c \ud328\uce58 \ub2e8\uc704\ub85c \ub098\ub204\uc5b4 \ucc98\ub9ac\ud558\ub294 \ubc18\uba74, LLaVA-NeXt-pool\uc740 COCONut-PanCap \ub370\uc774\ud130\uc14b\uc758 \ud328\ub178\ud504\ud2f1 \ubd84\ud560 \ub9c8\uc2a4\ud06c\ub97c \ud65c\uc6a9\ud558\uc5ec \uac1d\uccb4\uc758 \uc138\ubd80 \uc815\ubcf4\ub97c \ubcf4\uc874\ud558\uba74\uc11c \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uac1c\ubcc4 \uac1d\uccb4\uc5d0 \ub300\ud55c \ud2b9\uc9d5 \ucd94\ucd9c\uc774 \ub354\uc6b1 \uc644\ubcbd\ud574\uc9c0\uace0, \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\uc5d0\uc11c\ub3c4 \uc131\ub2a5 \uc800\ud558 \uc5c6\uc774 \uc138\ubd80 \uc815\ubcf4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uac1c\uc120\ub41c \ud2b9\uc9d5 \ucd94\ucd9c \ubc29\uc2dd\uc740 \uac1c\ubcc4 \uac1d\uccb4\uc758 \ud2b9\uc9d5\uc744 \ub354\uc6b1 \uc815\ud655\ud558\uac8c \ud3ec\ucc29\ud558\uc5ec, \ud2b9\ud788 \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\uc5d0\uc11c\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud569\ub2c8\ub2e4.", "section": "A.1 Detailed Captioning"}, {"figure_path": "https://arxiv.org/html/2502.02589/x7.png", "caption": "Figure 7: Comparison of LLaVA-NeXt and our proposed LLaVA-NeXt-pool.", "description": "\uadf8\ub9bc 7\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \uc0c8\ub85c\uc6b4 LLaVA-NeXt-pool \ubaa8\ub378\uacfc \uae30\uc874 LLaVA-NeXt \ubaa8\ub378\uc758 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\uc874 LLaVA-NeXt\ub294 \uc774\ubbf8\uc9c0\ub97c \ud328\uce58 \ub2e8\uc704\ub85c \ub098\ub204\uc5b4 \ucc98\ub9ac\ud558\ub294 \ubc18\uba74, \uc81c\uc548\ub41c LLaVA-NeXt-pool\uc740 \ud328\uce58 \ub2e8\uc704 \ucc98\ub9ac\uc758 \ub2e8\uc810\uc744 \uadf9\ubcf5\ud558\uae30 \uc704\ud574 panoptic segmentation \ub9c8\uc2a4\ud06c\ub97c \ud65c\uc6a9\ud558\uc5ec \uac1d\uccb4\uc758 \uc138\ubd80 \uc815\ubcf4\ub97c \ub354 \uc798 \uc720\uc9c0\ud558\uba74\uc11c \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\uc758 \uc138\ubd80 \uc815\ubcf4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ucc98\ub9ac\ud558\uace0 \uc131\ub2a5\uacfc \ud6a8\uc728\uc131\uc744 \ub3d9\uc2dc\uc5d0 \ub192\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ub450 \ubaa8\ub378\uc758 \uad6c\uc870\uc801 \ucc28\uc774\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\uace0, \uc81c\uc548\ub41c \ubaa8\ub378\uc758 \uc7a5\uc810\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "A.1. Detailed Captioning"}, {"figure_path": "https://arxiv.org/html/2502.02589/x8.png", "caption": "Figure 8: Architecture of PanCaper. We utilize a pretrained vision encoder from kMaX-DeepLab\u00a0[67] as our vision backbone, which effectively extracts dense features essential for panoptic segmentation.", "description": "\uadf8\ub9bc 8\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 PanCaper \ubaa8\ub378\uc758 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  PanCaper\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c kMaX-DeepLab [67]\uc758 \ube44\uc804 \uc778\ucf54\ub354\ub97c \ubc31\ubcf8\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \ud328\ub178\ud53d \ubd84\ud560\uc5d0 \ud544\uc218\uc801\uc778 \uace0\ubc00\ub3c4 \ud2b9\uc9d5\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ucd94\ucd9c\ud569\ub2c8\ub2e4.  \ubaa8\ub378\uc740 \ube44\uc804 \ubc31\ubcf8, \ub9c8\uc2a4\ud06c \ub514\ucf54\ub354, \ub2e4\uc911 \ubaa8\ub4dc LLM\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 \uad6c\uc131 \uc694\uc18c\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4.  \ube44\uc804 \ubc31\ubcf8\uc740 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uace0\ubc00\ub3c4 \uc2dc\uac01\uc801 \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud558\uace0, \ub9c8\uc2a4\ud06c \ub514\ucf54\ub354\ub294 \uc774\ub7ec\ud55c \ud2b9\uc9d5\uacfc LLM\uc758 \ucd9c\ub825\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud328\ub178\ud53d \ubd84\ud560 \ub9c8\uc2a4\ud06c\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ub2e4\uc911 \ubaa8\ub4dc LLM\uc740 \uc774\ubbf8\uc9c0\uc640 \ub9c8\uc2a4\ud06c \uc815\ubcf4\ub97c \ucc98\ub9ac\ud558\uc5ec \uac1d\uccb4\uc5d0 \ub300\ud55c \uc138\ubd84\ud654\ub41c \uc124\uba85\uc744 \uc0dd\uc131\ud558\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uad6c\uc131 \uc694\uc18c\ub4e4\uc758 \uc0c1\ud638\uc791\uc6a9\uc744 \ud1b5\ud574 PanCaper\ub294 \uc774\ubbf8\uc9c0\uc758 \ud328\ub178\ud53d \ubd84\ud560\uacfc \uc815\ud655\ud55c \ucea1\uc158 \uc0dd\uc131\uc744 \ub3d9\uc2dc\uc5d0 \uc218\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. PGC Baseline: PanCaper"}, {"figure_path": "https://arxiv.org/html/2502.02589/x9.png", "caption": "Figure 9: \nVisualization of the Panoptic Grounded Caption.\nOur annotated captions ground the panoptic segmentation masks.", "description": "\uadf8\ub9bc 9\ub294 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c COCONut-PanCap \ub370\uc774\ud130\uc14b\uc758 \ud328\ub178\ud53d \uae30\ubc18 \uc811\uc9c0\ud654\ub41c \ucea1\uc158 \uc2dc\uac01\ud654 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\ub294 \ub2e4\uc591\ud55c \uc7a5\uba74\uc758 \uc774\ubbf8\uc9c0\uc640 \uac01 \uc601\uc5ed\uc5d0 \ub300\ud55c \uc0c1\uc138\ud55c \uc124\uba85\uc774 \ub2f4\uae34 \ucea1\uc158\uc774 \ud568\uaed8 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uc774\ubbf8\uc9c0\ub294 \ud328\ub178\ud53d \ubd84\ud560 \ub9c8\uc2a4\ud06c\ub97c \uc774\uc6a9\ud558\uc5ec \uac1d\uccb4\ub97c \uc815\ud655\ud558\uac8c \uad6c\ubd84\ud558\uace0 \uc788\uc73c\uba70, \ucea1\uc158\uc740 \ud574\ub2f9 \ub9c8\uc2a4\ud06c\uc5d0 \uae30\ubc18\ud558\uc5ec \uc0dd\uc131\ub418\uc5b4 \uac1d\uccb4\uc640 \uc2dc\uac01\uc801 \uc694\uc18c \uac04\uc758 \uc815\ud655\ud55c \ub300\uc751 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 COCONut-PanCap \ub370\uc774\ud130\uc14b\uc774 \uc774\ubbf8\uc9c0\uc758 \uc2dc\uac01\uc801 \uc774\ud574\uc640 \ucea1\uc158 \uc0dd\uc131 \ubaa8\ub450\uc5d0 \ud6a8\uacfc\uc801\uc73c\ub85c \ud65c\uc6a9\ub420 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3 COCONut-PanCap \ub370\uc774\ud130\uc14b"}, {"figure_path": "https://arxiv.org/html/2502.02589/x10.png", "caption": "Figure 10: \nVisualization of the Panoptic Grounded Caption.\nOur annotated captions ground the panoptic segmentation masks.", "description": "\uadf8\ub9bc 10\uc740 COCONut-PanCap \ub370\uc774\ud130\uc14b\uc758 \ud328\ub178\ub77c\ub9c8 \uae30\ubc18 \uc811\uc9c0 \ucea1\uc158\uc758 \uc2dc\uac01\ud654 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc5d0\uc11c\ub294 \uac01 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 \uc0ac\ub78c\uc774 \uc8fc\uc11d\uc744 \ub2e8 \ud328\ub178\ub77c\ub9c8 \ubd84\ud560 \ub9c8\uc2a4\ud06c\uc5d0 \uae30\ubc18\ud55c \uc790\uc138\ud55c \ucea1\uc158\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ubbf8\uc9c0\uc758 \uac01 \uc601\uc5ed\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uc124\uba85\uacfc \ud568\uaed8 \uc774\ubbf8\uc9c0 \ub0b4\uc758 \uac1c\uccb4 \ubc0f \uc601\uc5ed\uc5d0 \ub300\ud55c \uc815\ud655\ud55c \ucc38\uc870\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uc2dc\uac01\uc801 \uc774\ud574\uc640 \uc790\uc5f0\uc5b4 \ucc98\ub9ac \ubaa8\ub378\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud569\ub2c8\ub2e4.", "section": "3 COCONut-PanCap \ub370\uc774\ud130\uc14b"}, {"figure_path": "https://arxiv.org/html/2502.02589/x11.png", "caption": "Figure 11: \nTier Examples for the User Study.\nOur COCONut-PanCap annotations are tied with GPT-4V annotations for some simple cases.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc758 \uc0ac\uc6a9\uc790 \uc5f0\uad6c\uc5d0\uc11c GPT-4V \uc5b4\ub178\ud14c\uc774\uc158\uacfc COCONut-PanCap \uc5b4\ub178\ud14c\uc774\uc158\uc774 \uc77c\uce58\ud558\ub294 \uac04\ub2e8\ud55c \uc0ac\ub840\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uadf8\ub9bc\uc740 \uc138 \uac00\uc9c0 \uc774\ubbf8\uc9c0\uc640 \uac01 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud55c \ub450 \uac00\uc9c0 \ucea1\uc158(COCONut-PanCap\uacfc GPT-4V\uac00 \uc0dd\uc131\ud55c \ucea1\uc158)\uc744 \uc81c\uc2dc\ud558\uba70, \uac01 \ucea1\uc158\uc758 \ucc28\uc774\uc810\uacfc \uc720\uc0ac\uc810\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uc5b4\ub178\ud14c\uc774\uc158 \uc77c\uce58 \uc5ec\ubd80\ub97c \ud310\ub2e8\ud558\ub294 \uc0ac\uc6a9\uc790 \uc5f0\uad6c\uc758 \uc77c\ubd80\ubd84\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uac01 \uc774\ubbf8\uc9c0\ub294 \uc81c\ube0c\ub77c, \uae30\ub9b0, \uc624\ud1a0\ubc14\uc774\ub97c \ubcf4\uc5ec\uc8fc\uba70, COCONut-PanCap \ucea1\uc158\uc740 \uc8fc\ub85c \uc774\ubbf8\uc9c0 \ub0b4 \uac1d\uccb4\uc640 \uc18d\uc131\uc744 \uac04\uacb0\ud558\uac8c \uae30\uc220\ud558\ub294 \ubc18\uba74, GPT-4V \ucea1\uc158\uc740 \ub354\uc6b1 \ud48d\ubd80\ud558\uace0 \uc0c1\uc138\ud55c \uc124\uba85\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ube44\uad50\ub97c \ud1b5\ud574 \uc0ac\uc6a9\uc790 \uc5f0\uad6c\uc5d0\uc11c \uc5b4\ub178\ud14c\uc774\uc158 \uc77c\uce58 \uae30\uc900\uc744 \uc124\uc815\ud558\ub294 \uacfc\uc815\uacfc \uadf8 \uacb0\uacfc\ub97c \uc774\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. COCONut-PanCap Dataset"}]