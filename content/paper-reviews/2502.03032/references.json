{"references": [{"fullname_first_author": "Bricken, T.", "paper_title": "Towards monosemanticity: Decomposing language models with dictionary learning.", "publication_date": "2023-XX-XX", "reason": "This paper introduces the concept of Sparse Autoencoders (SAEs) for disentangling directions in LLMs into monosemantic features, which is the core methodology of the present work."}, {"fullname_first_author": "Balagansky, N.", "paper_title": "Mechanistic permutability: Match features across layers.", "publication_date": "2024-XX-XX", "reason": "This paper examines the inter-layer feature links in LLMs, providing a foundation for the current research's cross-layer feature mapping approach."}, {"fullname_first_author": "Dunefsky, J.", "paper_title": "Transcoders find interpretable llm feature circuits.", "publication_date": "2024-XX-XX", "reason": "This paper proposes a data-free approach to find computational circuits in LLMs using transcoders, which is closely related to the current work's feature matching and circuit identification techniques."}, {"fullname_first_author": "Engels, J.", "paper_title": "Not all language model features are linear.", "publication_date": "2024-XX-XX", "reason": "This paper discusses the linear representation hypothesis in LLMs, providing essential background for interpreting the linear directions found by SAEs."}, {"fullname_first_author": "Mikolov, T.", "paper_title": "Linguistic regularities in continuous space word representations.", "publication_date": "2013-XX-XX", "reason": "This seminal paper established the concept that neural networks encode concepts as linear directions within hidden representations, which is a foundational concept for the current research's approach."}]}