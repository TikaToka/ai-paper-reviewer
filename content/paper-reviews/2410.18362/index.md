---
title: "WAFFLE: Multi-Modal Model for Automated Front-End Development"
summary: "WAFFLE: a new fine-tuning method dramatically improves UI design-to-HTML code generation by using structure-aware attention and contrastive learning, outperforming current state-of-the-art models."
categories: ["AI Generated"]
tags: ["ðŸ”– 24-10-24", "ðŸ¤— 24-10-25"]
showSummary: true
date: 2024-10-24
draft: false
---

### TL;DR


{{< lead >}}

This research introduces WAFFLE, a novel fine-tuning approach for multi-modal large language models (MLLMs) to generate HTML code from UI design images.  It tackles two key challenges: representing HTML's hierarchical structure and bridging the visual and text-based formats. WAFFLE uses a structure-aware attention mechanism to help the model understand HTML structure better, and contrastive learning to align the model's understanding of UI images and HTML code.  Experiments show WAFFLE significantly improves HTML match, CW-SSIM, CLIP, and LLEM scores on benchmark datasets, outperforming existing fine-tuning methods.  The new structure-aware attention and contrastive learning approaches are significant contributions, and the generated dataset enhances future research. The method is model-independent, applicable to various MLLMs for UI-to-HTML code generation.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.18362" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}
<br><br>
{{< button href="https://huggingface.co/papers/2410.18362" target="_self" >}}
{{< icon "hf-logo" >}} &nbsp; on Hugging Face
{{< /button >}}

#### Why does it matter?
This paper is crucial for researchers in front-end development and multi-modal learning.  It introduces a novel fine-tuning strategy, WAFFLE, significantly improving UI image-to-HTML code generation.  The structure-aware attention and contrastive learning techniques are valuable contributions, opening avenues for advancing MLLMs in code generation and other related fields.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} WAFFLE significantly improves UI image-to-HTML code generation accuracy. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} Structure-aware attention enhances LLMs' understanding of HTML's hierarchical structure. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Contrastive learning aligns LLMs' understanding of UI images and HTML code. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](figures/figures_1_0.png)

> ðŸ”¼ Figure 1 shows that removing child elements from a parent element in HTML does not affect the visual layout of the parent element or its sibling elements.
> <details>
> <summary>read the caption</summary>
> Figure 1: Removing the children of the element <div id='left-column'> highlighted in yellow does not affect the structure of the visual layout of itself or its sibling element <div id='right-column'>.
> </details>





![](charts/charts_13_0.png)

> ðŸ”¼ The t-SNE plots visualize the relationship between image and text embeddings generated by Standard FT and WAFFLE-attn, revealing how WAFFLE-attn better aligns these modalities.
> <details>
> <summary>read the caption</summary>
> Figure 7: t-SNE plots of the text and image embeddings, computed by Moondream2 fine-tuned with Standard FT and WAFFLE-attn.
> </details>





{{< table-caption >}}
<table id='1' style='font-size:18px'><tr><td>Shanchao Liang</td><td>Nan Jiang</td><td>Shangshu Qian</td><td>Lin Tan</td></tr><tr><td>Purdue University</td><td>Purdue University</td><td>Purdue University</td><td>Purdue University</td></tr><tr><td>liang422@purdue.edu</td><td>jiang719@purdue.edu</td><td>qian151 @purdue.edu</td><td>lintan@purdue.edu</td></tr></table>{{< /table-caption >}}

> ðŸ”¼ Table 2 presents the performance comparison of different fine-tuning methods on the WebSight-Test dataset, using several metrics including HTML Match, CW-SSIM, CLIP, and LLEM.
> <details>
> <summary>read the caption</summary>
> Table 2: Main results on the WebSight-Test dataset.
> </details>



### More visual insights

<details>
<summary>More on figures
</summary>


![](figures/figures_3_0.png)

> ðŸ”¼ The figure illustrates the overall architecture of WAFFLE, showing its training data mutation process, structure-aware attention mechanism, and contrastive learning objective.
> <details>
> <summary>read the caption</summary>
> Figure 3: Overview of WAFFLE, including training data mutation, structure-aware attention, and contrastive learning.
> </details>



![](figures/figures_4_0.png)

> ðŸ”¼ The figure illustrates WAFFLE's structure-aware attention mechanism, showing how tokens attend to parent, sibling, and self elements in the HTML code.
> <details>
> <summary>read the caption</summary>
> Figure 4: Example of structure-aware attention.
> </details>



![](figures/figures_12_0.png)

> ðŸ”¼ The figure shows a comparison of webpage generation results from GPT-40, standard fine-tuning, and WAFFLE on a sample from the WebSight-Test dataset, highlighting WAFFLE's superior performance.
> <details>
> <summary>read the caption</summary>
> Figure 5: Example test instance from WebSight-Test dataset, with the generated images by GPT-40, Standard FT, and WAFFLE.
> </details>



![](figures/figures_12_1.png)

> ðŸ”¼ The figure shows the effect of different portions of structure-aware attention heads on validation LLEM score and training loss.
> <details>
> <summary>read the caption</summary>
> Figure 6: Illustration of the tuning process of the parameter that controls the effect of structure-aware attention. In (b), the green line almost overlaps with the blue line.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table id='0' style='font-size:14px'><tr><td>Artists</td><td>Artists</td></tr><tr><td colspan="2">(a) Rendered webpage from code in (b) (c) Rendered webpage from code in (d)</td></tr><tr><td>#grid { display: grid; grid-template-columns: 1fr 1fr; }</td><td>#grid { display: grid; grid-template-columns: 1fr 2fr; }</td></tr><tr><td><div id="grid"></td><td><div id="grid"></td></tr><tr><td><div style="background-color: red; "> </div></td><td><div style="background-color: red;"> </div></td></tr><tr><td><div style="background-color: blue;"> </div></td><td><div style="background-color: blue; "> </div></td></tr><tr><td><div style="background-color: green; "> </div></td><td><div style="background-color: green; "> </div></td></tr><tr><td><div style="background-color: yellow; "> </div></td><td><div style="background-color: yellow; "> </div></td></tr><tr><td><div style="background-color: orange;"> </div></td><td><div style="background-color: orange; "> </div></td></tr><tr><td><div style="background-color: purple;"> </div> </div></td><td><div style="background-color: purple;"> </div> </div></td></tr><tr><td>(b) Snippet of HTML and CSS code</td><td>(d) Small modification on CSS in (b)</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 2 presents the performance comparison of different techniques on the WebSight-Test dataset, evaluating metrics such as HTML-Match, CW-SSIM, CLIP, and LLEM.


{{< table-caption >}}
<table id='2' style='font-size:14px'><tr><td colspan="6">CSS</td><td rowspan="2">HTML</td><td rowspan="2">Total</td></tr><tr><td>Color</td><td>Size</td><td>Margin</td><td>Font</td><td>Display</td><td>Position</td></tr><tr><td>12</td><td>11</td><td>19</td><td>10</td><td>1</td><td>2</td><td>I 8</td><td>63</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 1 shows the most frequent causes of failures in existing web MLLMs, categorizing errors into seven common types and listing their corresponding frequencies.


{{< table-caption >}}
<table id='0' style='font-size:14px'><tr><td rowspan="2">Backbones</td><td rowspan="2">Techniques</td><td rowspan="2">HTML-Match (%) â†‘</td><td rowspan="2">CW-SSIM â†‘</td><td rowspan="2">CLIP â†‘</td><td colspan="5">Low-Level Element Matching (LLEM) (%) â†‘</td></tr><tr><td>Average</td><td>Block-Match</td><td>Text</td><td>Position</td><td>Color</td></tr><tr><td rowspan="3">Gemini 1.5 Pro GPT-4o mini GPT-4o</td><td>Prompting</td><td>9.40</td><td>0.3385</td><td>88.55</td><td>90.16</td><td>94.31</td><td>98.41</td><td>84.73</td><td>83.18</td></tr><tr><td>Prompting</td><td>10.20</td><td>0.3055</td><td>87.72</td><td>87.54</td><td>92.59</td><td>98.48</td><td>82.65</td><td>76.45</td></tr><tr><td>Prompting</td><td>11.40</td><td>0.3666</td><td>89.03</td><td>92.18</td><td>94.66</td><td>98.43</td><td>87.04</td><td>88.60</td></tr><tr><td rowspan="2">Moondream2</td><td>Standard FT</td><td>21.60</td><td>0.4233</td><td>89.92</td><td>90.59</td><td>91.73</td><td>96.98</td><td>87.56</td><td>86.77</td></tr><tr><td>WAFFLE</td><td>27.60</td><td>0.4486</td><td>89.98</td><td>91.72</td><td>92.26</td><td>97.25</td><td>89.55</td><td>87.81</td></tr><tr><td rowspan="2">VLM-WebSight</td><td>Standard FT</td><td>28.00</td><td>0.5023</td><td>93.30</td><td>92.73</td><td>97.95</td><td>90.72</td><td>91.07</td><td>93.45</td></tr><tr><td>WAFFLE</td><td>37.00</td><td>0.6005</td><td>94.57</td><td>95.16</td><td>93.62</td><td>98.16</td><td>93.29</td><td>95.57</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 2 presents the performance comparison of different techniques on the WebSight-Test dataset, showing HTML-Match, CW-SSIM, CLIP, and LLEM metrics.


{{< table-caption >}}
<table id='2' style='font-size:14px'><tr><td rowspan="2">Backbones</td><td rowspan="2">Techniques</td><td rowspan="2">CW-SSIM â†‘</td><td rowspan="2">CLIP â†‘</td><td colspan="5">Low-Level Element Matching (LLEM) (%) â†‘</td></tr><tr><td>Average</td><td>Block-Match</td><td>Text</td><td>Position</td><td>Color</td></tr><tr><td rowspan="3">Gemini 1.5 Pro* GPT-4o-mini GPT-4o</td><td>Prompting</td><td>0.2652</td><td>87.76</td><td>87.17</td><td>91.82</td><td>97.40</td><td>82.67</td><td>76.81</td></tr><tr><td>Prompting</td><td>0.2304</td><td>86.06</td><td>78.84</td><td>70.64</td><td>92.39</td><td>78.55</td><td>73.78</td></tr><tr><td>Prompting</td><td>0.2776</td><td>89.03</td><td>83.67</td><td>75.98</td><td>94.29</td><td>83.38</td><td>81.01</td></tr><tr><td rowspan="2">Moondream2</td><td>Standard FT</td><td>0.1348</td><td>46.63</td><td>40.71</td><td>29.56</td><td>49.41</td><td>40.73</td><td>43.14</td></tr><tr><td>WAFFLE</td><td>0.2142</td><td>79.62</td><td>67.83</td><td>44.32</td><td>83.59</td><td>71.61</td><td>71.81</td></tr><tr><td rowspan="2">VLM-WebSight</td><td>Standard FT</td><td>0.2518</td><td>82.35</td><td>73.00</td><td>55.77</td><td>84.14</td><td>74.74</td><td>77.36</td></tr><tr><td>WAFFLE</td><td>0.2815</td><td>85.98</td><td>77.81</td><td>61.47</td><td>88.20</td><td>79.30</td><td>82.28</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 3 presents the performance comparison of various fine-tuning strategies on the Design2Code dataset, showing the effectiveness of WAFFLE in improving HTML-Match, CW-SSIM, CLIP, and LLEM metrics.


{{< table-caption >}}
<table id='0' style='font-size:14px'><tr><td rowspan="2">Backbones</td><td rowspan="2">Techniques</td><td colspan="4">WebSight-Test</td><td colspan="3">Design2Code</td></tr><tr><td>HTML-Match (%) â†‘</td><td>CW-SSIM â†‘</td><td>CLIP â†‘</td><td>LLEM (%) â†‘</td><td>CW-SSIM â†‘</td><td>CLIP â†‘</td><td>LLEM (%) â†‘</td></tr><tr><td rowspan="4">Moondream2</td><td>Standard FT</td><td>21.60</td><td>0.4233</td><td>89.92</td><td>90.59</td><td>0.1348</td><td>46.63</td><td>40.71</td></tr><tr><td>WAFFLE-attn</td><td>23.60</td><td>0.4311</td><td>90.47</td><td>91.34</td><td>0.1821</td><td>67.73</td><td>56.49</td></tr><tr><td>WAFFLE-contra</td><td>26.00</td><td>0.4296</td><td>89.55</td><td>91.21</td><td>0.2100</td><td>76.63</td><td>65.82</td></tr><tr><td>WAFFLE</td><td>27.60</td><td>0.4486</td><td>89.98</td><td>91.72</td><td>0.2142</td><td>79.62</td><td>67.83</td></tr><tr><td rowspan="4">VLM-WebSight</td><td>Standard FT</td><td>28.00</td><td>0.5023</td><td>93.30</td><td>92.73</td><td>0.2518</td><td>82.35</td><td>73.00</td></tr><tr><td>WAFFLE-attn</td><td>30.80</td><td>0.5411</td><td>94.29</td><td>94.20</td><td>0.2480</td><td>85.64</td><td>75.34</td></tr><tr><td>WAFFLE-contra</td><td>35.80</td><td>0.5677</td><td>95.08</td><td>95.30</td><td>0.2653</td><td>85.16</td><td>76.48</td></tr><tr><td>WAFFLE</td><td>37.00</td><td>0.6005</td><td>94.57</td><td>95.16</td><td>0.2815</td><td>85.98</td><td>77.81</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 4 presents the performance comparison of WAFFLE and its ablation models on the WebSight-Test and Design2Code datasets, showing the effectiveness of contrastive learning and structure-aware attention.


{{< table-caption >}}
<table id='2' style='font-size:14px'><tr><td>Techniques</td><td>Rank 1 â†‘</td><td>Rank 2 â†‘</td><td>Rank 3 â†‘</td><td>Avg Rankings â†“</td></tr><tr><td>Standard FT</td><td>7117 (24)</td><td>14|13 (27)</td><td>17|18 (35)</td><td>2.9012.42 (2.66)</td></tr><tr><td>WAFFLE-attn</td><td>15|16 (31)</td><td>9117 (26)</td><td>24116 (40)</td><td>2.55 12.37 (2.46)</td></tr><tr><td>WAFFLE-contra</td><td>38120 (58)</td><td>8111 (19)</td><td>10|15 (25)</td><td>1.67 12.38 (2.02)</td></tr><tr><td>WAFFLE</td><td>27132 (59)</td><td>18112 (30)</td><td>10| 9 (19)</td><td>1.88 l1.85 (1.87)</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 5 presents human evaluation results on two datasets, showing WAFFLE's superior performance in generating high-quality HTML code compared to other methods.


{{< table-caption >}}
<br><table id='8' style='font-size:14px'><tr><td>Techniques</td><td>Prior</td><td>Current</td><td>Drop (%)</td></tr><tr><td>WAFFLE-attn</td><td>0.8002</td><td>0.5797</td><td>27.55</td></tr><tr><td>WAFFLE</td><td>0.8291</td><td>0.7932</td><td>4.34</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 6 shows the impact of intermediate errors during HTML code generation on the CW-SSIM score using the VLM-WebSight backbone.


{{< table-caption >}}
<table id='14' style='font-size:14px'><tr><td>Class</td><td>Failure Type</td><td>Specification</td></tr><tr><td rowspan="6">CSS</td><td>Color</td><td>Random Color in Range [#000000 , #FFFFFF]</td></tr><tr><td>Size</td><td>Random Size in [0, 500] pixels</td></tr><tr><td>Margin</td><td>Random Size in [0, 100] pixels</td></tr><tr><td>Font</td><td>Random Size in [0, 40] pixels</td></tr><tr><td>Display</td><td>Random Keyword for text-align, display, flex-direction, and justify-content</td></tr><tr><td>Position</td><td>Random Keyword for border-radius, position, top, and right</td></tr><tr><td>HTML</td><td>Structure</td><td>Duplication of a Random HTML Element, excluding <head>, <header>, <html>, <body></td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 7 specifies the mutation rules used to create the contrastive learning dataset by mutating HTML code and CSS styles for each element based on failure types.


{{< table-caption >}}
<br><table id='3' style='font-size:16px'><tr><td>Techniques</td><td>d(vi, ti) â†“</td><td>sim(vi, ti) â†‘</td></tr><tr><td>Standard FT</td><td>1.3395</td><td>0.1027</td></tr><tr><td>WAFFLE-attn</td><td>0.8447</td><td>0.6244</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 8 presents the distance and similarity between averaged image and text embeddings, comparing standard fine-tuning with WAFFLE-attn using Moondream2 as the backbone.


{{< table-caption >}}
<table id='0' style='font-size:14px'><tr><td>Techniques</td><td>d(vi, c) â†‘</td><td>sim(vi, cg) â†“</td></tr><tr><td>Standard FT</td><td>0.1224</td><td>0.9910</td></tr><tr><td>WAFFLE-attn</td><td>0.7590</td><td>0.6202</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 9 presents the distances and similarities between averaged image embeddings and centroids of their groups of mutants using Moondream2 as the backbone.


</details>


### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}