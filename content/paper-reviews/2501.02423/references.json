{"references": [{"fullname_first_author": "Kaplan, J.", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-27", "reason": "This paper establishes foundational scaling laws for LLMs, providing a basis for understanding the relationship between model size, data size, and performance, which is extended in this paper to include low-precision training."}, {"fullname_first_author": "Hoffmann, J.", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-22", "reason": "This paper introduces the Chinchilla scaling laws, refining the understanding of optimal model-data size relationships, which is used as a baseline in this paper's study of low-precision training."}, {"fullname_first_author": "Kumar, T.", "paper_title": "Scaling laws for precision", "publication_date": "2024-11-26", "reason": "This paper is crucial because it's the most relevant prior work directly addressing scaling laws for precision in LLMs, which this paper extends and improves by considering floating-point quantization."}, {"fullname_first_author": "Dubey, A.", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-21", "reason": "This paper provides the LLaMA models used in the experiments of this paper, making it a foundational dataset for the empirical results."}, {"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-12-05", "reason": "This paper introduces the Transformer architecture, which is the basis for the LLMs used in the experiments of this paper, making it the primary model architecture being studied."}]}