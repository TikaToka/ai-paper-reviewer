[{"content": "| Scale | Model | GSM8K | MATH500 | OlympiadBench | AMC2023 | AIME2024 | Avg. |\n|---|---|---|---|---|---|---|---| \n| Large | GPT-4o | / | 60.3 | 43.3 | / | 9.3 | / |\n|  | o1-preview | / | 85.5 | / | 82.5 | 44.6 | / |\n|  | Llama-3.1-70B-Instruct | 94.1 | 68.0 | 29.4 | 42.5 | 13.3 | 49.5 |\n|  | OpenMath2-Llama3.1-70B | 94.1 | 71.8 | 30.1 | 45.0 | 13.3 | 50.9 |\n|  | QwQ-32B-Preview | 95.5 | 90.6 | 61.2 | 77.5 | 50.0 | 75.0 |\n| Small | Llama-3.1-8b-Instruct | 84.4 | 51.9 | 15.1 | 22.5 | 3.3 | 35.4 |\n|  | OpenMath2-Llama3.1-8B | 90.5 | 67.8 | 28.9 | 37.5 | 6.7 | 46.3 |\n|  | NuminaMath-7B-CoT | 78.9 | 54.6 | 15.9 | 20.0 | 10.0 | 35.9 |\n|  | Qwen-2.5-7B-Instruct | 91.6 | 75.5 | 35.5 | 52.5 | 6.7 | 52.4 |\n|  | Qwen-2.5-Math-7B-Instruct | 95.2 | 83.6 | 41.6 | 62.5 | 16.7 | 59.9 |\n|  | **Satori-Qwen-7B** | **93.2** | **85.6** | **46.6** | **67.5** | **20.0** | **62.6** |\n|  | **Satori-Qwen-7B (Round 2)** | **93.9** | **83.6** | **48.5** | **72.5** | **23.3** | **64.4** |", "caption": "Table 1: Results on Mathematic Benchmarks. Satori-Qwen-7B achieves SOTA performance across five benchmarks, and outperforms Qwen-2.5-Math-7B-Instruct which uses the same base model Qwen-2.5-Math-7B. After round-2 training, Satori-Qwen-7B (Round 2) demonstrates even stronger performance on hard tasks.", "description": "\ud45c 1\uc740 \uc218\ud559 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c Satori-Qwen-7B\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Satori-Qwen-7B\ub294 \ub2e4\uc12f \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ucd5c\ucca8\ub2e8 \uc131\ub2a5\uc744 \ub2ec\uc131\ud588\uc73c\uba70, \ub3d9\uc77c\ud55c \uae30\ubc18 \ubaa8\ub378\uc778 Qwen-2.5-Math-7B\ub97c \uc0ac\uc6a9\ud558\ub294 Qwen-2.5-Math-7B-Instruct\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub6f0\uc5b4\ub0a9\ub2c8\ub2e4. 2\ucc28 \ud6c8\ub828 \ud6c4 Satori-Qwen-7B (2\ucc28)\ub294 \uc5b4\ub824\uc6b4 \uacfc\uc81c\uc5d0\uc11c \ub354\uc6b1 \ud5a5\uc0c1\ub41c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \uc218\ud559\uc801 \ucd94\ub860 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec Satori-Qwen-7B \ubaa8\ub378\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \ub3d9\uc77c\ud55c \uae30\ubc18 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\ub294 \ub2e4\ub978 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec Satori-Qwen-7B\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uba85\ud655\ud558\uac8c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "5. Experiment"}, {"content": "| Scale | Model | FOLIO | BGQA | CRUXEval | StrategyQA | TableBench | STEM | Avg. | \n|---|---|---|---|---|---|---|---|---| \n| Large | Llama-3.1-70B-Instruct | 65.0 | 58.3 | 59.6 | 88.8 | 34.2 | 61.7 | 61.3 | \n|  | OpenMath2-Llama3.1-70B | 68.5 | 68.7 | 35.1 | 95.6 | 46.8 | 15.1 | 55.0 | \n|  | QwQ-32B-Preview | 84.2 | 71.1 | 65.2 | 88.2 | 51.5 | 71.3 | 71.9 | \n| Small | Llama-3.1-8b-Instruct | 63.5 | 50.3 | 38.5 | 92.2 | 32.4 | 43.4 | 53.4 | \n|  | OpenMath2-Llama3.1-8B | 57.1 | 49.0 | 11.1 | 84.4 | 34.2 | 10.9 | 41.1 | \n|  | NuminaMath-7B-CoT | 53.2 | 44.6 | 28.0 | 77.8 | 29.1 | 11.3 | 40.7 | \n|  | Qwen-2.5-7B-Instruct | 72.4 | 53.0 | 58.1 | 91.3 | 43.2 | 57.1 | 62.5 | \n|  | Qwen-2.5-Math-7B-Instruct | 68.9 | 51.3 | 28.0 | 85.3 | 36.2 | 45.2 | 52.5 | \n|  | **Satori-Qwen-7B** | 71.4 | 61.8 | 42.5 | 86.3 | 43.4 | 56.7 | 60.4 | \n|  | **Satori-Qwen-7B (Round 2)** | 72.9 | 58.5 | 41.1 | 90.4 | 44.6 | 57.4 | 60.8 | ", "caption": "Table 2: Results on Out-of-domain Benchmarks. Trained only on math datasets, Satori-Qwen-7B exhibits strong transferability across diverse out-of-domain benchmarks and outperforms Qwen-2.5-Math-7B-Instruct by a large margin. Moreover, despite not being trained in other domains, Satori-Qwen-7B achieves performance comparable to or exceeding other small-scale general instruct models.", "description": "\ud45c 2\ub294 \uc218\ud559 \ub370\uc774\ud130\uc14b\uc73c\ub85c\ub9cc \ud559\uc2b5\ub41c Satori-Qwen-7B \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \ub3c4\uba54\uc778\uc758 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uac15\ub825\ud55c \uc804\uc774 \ud559\uc2b5 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\uace0, Qwen-2.5-Math-7B-Instruct \ubaa8\ub378\ubcf4\ub2e4 \ud6e8\uc52c \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub354\uc6b1\uc774, \ub2e4\ub978 \ub3c4\uba54\uc778\uc5d0 \ub300\ud55c \ud559\uc2b5 \uc5c6\uc774\ub3c4 Satori-Qwen-7B\ub294 \ub2e4\ub978 \uc18c\uaddc\ubaa8 \uc77c\ubc18 \uc9c0\uc2dc \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec \ub3d9\ub4f1\ud558\uac70\ub098 \uadf8 \uc774\uc0c1\uc758 \uc131\ub2a5\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 Satori-Qwen-7B \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \ub2a5\ub825\uacfc \ubc94\uc6a9\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc8fc\uc694 \uc99d\uac70\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "5. Experiment"}, {"content": "| Model | GSM8K | MATH500 | Olym. | AMC2023 | AIME2024 |\n|---|---|---|---|---|---| \n| Qwen-2.5-Math-7B-Instruct | 95.2 | 83.6 | 41.6 | 62.5 | 16.7 |\n| Qwen-7B-CoT (SFT+RL) | 93.1 | 84.4 | 42.7 | 60.0 | 10.0 |\n| Satori-Qwen-7B | 93.2 | 85.6 | 46.6 | 67.5 | 20.0 |", "caption": "Table 3: COAT Training v.s. CoT Training. Qwen-2.5-Math-7B trained with COAT reasoning format (Satori-Qwen-7B) outperforms the same base model but trained with classical CoT reasoning format (Qwen-7B-CoT)", "description": "\ud45c 3\uc740 Chain-of-Action-Thought (COAT) \ucd94\ub860 \ubc29\uc2dd\uacfc \uae30\uc874 Chain-of-Thought (CoT) \ucd94\ub860 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  \ub3d9\uc77c\ud55c \uae30\ubc18 \ubaa8\ub378\uc778 Qwen-2.5-Math-7B\ub97c \uc0ac\uc6a9\ud558\uc5ec, COAT \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ub41c Satori-Qwen-7B \ubaa8\ub378\uc774 \uae30\uc874 CoT \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ub41c Qwen-7B-CoT \ubaa8\ub378\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 COAT \ubc29\uc2dd\uc774 \ub2e8\uc77c LLM\uc758 \ucd94\ub860 \ub2a5\ub825 \ud5a5\uc0c1\uc5d0 \ub354 \ud6a8\uacfc\uc801\uc784\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "6. Analysis"}, {"content": "| Model | MATH500 T\u2192F | MATH500 F\u2192T | OlympiadBench T\u2192F | OlympiadBench F\u2192T | MMLUProSTEM T\u2192F | MMLUProSTEM F\u2192T |\n|---|---|---|---|---|---|---|\n| Satori-Qwen-7B-FT | 79.4% | 20.6% | 65.6% | 34.4% | 59.2% | 40.8% |\n| Satori-Qwen-7B | 39.0% | 61.0% | 42.1% | 57.9% | 46.5% | 53.5% |", "caption": "Table 4: Satori\u2019s Self-correction Capability. T\u2192\u2192\\rightarrow\u2192F: negative self-correction; F\u2192\u2192\\rightarrow\u2192T: positive self-correction.", "description": "\ud45c 4\ub294 Satori \ubaa8\ub378\uc758 \uc790\uac00 \uc218\uc815 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  T\u2192F\ub294 \uc798\ubabb\ub41c \ub2f5\uc744 \uc798\ubabb\ub41c \ub2f5\uc73c\ub85c \uc218\uc815\ud55c \uacbd\uc6b0(\uc790\uac00 \uc218\uc815 \uc2e4\ud328)\ub97c, F\u2192T\ub294 \uc798\ubabb\ub41c \ub2f5\uc744 \uc62c\ubc14\ub978 \ub2f5\uc73c\ub85c \uc218\uc815\ud55c \uacbd\uc6b0(\uc790\uac00 \uc218\uc815 \uc131\uacf5)\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \ubb38\uc81c \uc720\ud615(\ub3c4\uba54\uc778 \ub0b4 MATH500 \ubc0f OlympiadBench, \ub3c4\uba54\uc778 \uc678 MMLUProSTEM)\uc5d0 \ub300\ud55c \uc790\uac00 \uc218\uc815 \uc131\uacf5\ub960\uacfc \uc2e4\ud328\uc728\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 Satori \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ubb38\uc81c\uc5d0\uc11c \uc5bc\ub9c8\ub098 \ud6a8\uacfc\uc801\uc73c\ub85c \uc790\uac00 \uc218\uc815 \ub2a5\ub825\uc744 \ubc1c\ud718\ud558\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.1. \uc218\ud559 \ub3c4\uba54\uc778\uc758 \uc8fc\uc694 \uacb0\uacfc"}, {"content": "| (In-domain) | GSM8K | MATH500 | Olym. | AMC2023 | AIME2024 |\n|---|---|---|---|---|---|---|\n| Qwen-2.5-Math-7B-Instruct | 95.2 | 83.6 | 41.6 | 62.5 | 16.7 |\n| Satori-Qwen-7B-FT (300K) | 92.3 | 78.2 | 40.9 | 65.0 | 16.7 |\n| **Satori-Qwen-7B** | 93.2 | 85.6 | 46.6 | 67.5 | 20.0 |\n| (Out-of-domain) | BGQA | CRUX | STGQA | TableBench | STEM |\n|---|---|---|---|---|---|---|\n| Qwen-2.5-Math-7B-Instruct | 51.3 | 28.0 | 85.3 | 36.3 | 45.2 |\n| Satori-Qwen-7B-FT (300K) | 50.5 | 29.5 | 74.0 | 35.0 | 47.8 |\n| **Satori-Qwen-7B** | 61.8 | 42.5 | 86.3 | 43.4 | 56.7 |", "caption": "Table 5: Large-scale FT V.S. Large-scale RL Satori-Qwen (10K FT data + 300K RL data) outperforms same base model Qwen-2.5-Math-7B trained with 300K FT data (w/o RL) across all math and out-of-domain benchmarks.", "description": "\ud45c 5\ub294 \ub300\uaddc\ubaa8 \ubbf8\uc138 \uc870\uc815(Fine-tuning, FT)\uacfc \uac15\ud654 \ud559\uc2b5(Reinforcement Learning, RL)\uc744 \uc0ac\uc6a9\ud55c Satori-Qwen \ubaa8\ub378\uacfc, \ub3d9\uc77c\ud55c \uae30\ubc18 \ubaa8\ub378\uc778 Qwen-2.5-Math-7B\uc5d0 \ub300\ud574 30\ub9cc \uac74\uc758 FT \ub370\uc774\ud130\ub9cc\uc73c\ub85c \ud559\uc2b5\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  Satori-Qwen \ubaa8\ub378\uc740 1\ub9cc \uac74\uc758 FT \ub370\uc774\ud130\uc640 30\ub9cc \uac74\uc758 RL \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ub418\uc5c8\uc73c\uba70, \uc218\ud559 \uad00\ub828 \uacfc\uc81c\uc640 \ub3c4\uba54\uc778 \uc678\ubd80 \uacfc\uc81c \ubaa8\ub450\uc5d0\uc11c Qwen-2.5-Math-7B \ubaa8\ub378\ubcf4\ub2e4 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \uac15\ud654 \ud559\uc2b5\uc774 \ubaa8\ub378\uc758 \ucd94\ub860 \ub2a5\ub825 \ud5a5\uc0c1\uc5d0 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "5. Experiment"}, {"content": "| Bonus Scale | GSM8K | MATH500 | Olym. | AMC2023 | AIME2024 |\n|---|---|---|---|---|---| \n| 0.0 | 93.6 | 84.4 | 48.9 | 62.5 | 16.7 |\n| 0.5 (default) | 93.2 | 85.6 | 46.6 | 67.5 | 20.0 |", "caption": "Table 6: Ablation Study on Reflection Bonus.", "description": "\ubcf8 \ud45c\ub294 reflection bonus\uc758 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\uae30 \uc704\ud574 ablation study\ub97c \uc218\ud589\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  reflection bonus\ub294 \ubaa8\ub378\uc774 self-reflection \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub3c4\ub85d \ud558\ub294 \ubcf4\uc0c1 \uba54\ucee4\ub2c8\uc998\uc785\ub2c8\ub2e4.  \ud45c\ub294 reflection bonus\uc758 \ud06c\uae30(bonus scale)\ub97c \ub2e4\ub974\uac8c \uc124\uc815\ud588\uc744 \ub54c, \uc5ec\ub7ec \uc218\ud559 \ucd94\ub860 \ubca4\uce58\ub9c8\ud06c(GSM8K, MATH500, Olympiad, AMC2023, AIME2024)\uc5d0\uc11c\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\ubcf8\uac12(default)\uc778 0.5\uc640 0.0\uc758 \ub450 \uac00\uc9c0 \uac12\uc744 \ube44\uad50\ud558\uc5ec reflection bonus\uac00 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "E. \ucd94\uac00 \uacb0\uacfc (Additional Results)"}]