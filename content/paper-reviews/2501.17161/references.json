{"references": [{"fullname_first_author": "Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to the RLHF (Reinforcement Learning from Human Feedback) approach used in the main study for aligning language models to human preferences, a crucial technique in post-training."}, {"fullname_first_author": "Zhai", "paper_title": "Fine-tuning large vision-language models as decision-making agents via reinforcement learning", "publication_date": "2024-01-01", "reason": "This paper introduces the RL4VLM framework, directly adopted in the main study, providing a detailed methodology for applying RL to enhance vision-language models, a key component of the comparative analysis."}, {"fullname_first_author": "Dubey", "paper_title": "The Llama 3 Herd of models", "publication_date": "2024-07-01", "reason": "This paper introduces the Llama-3-Vision-11B model used as the backbone in the study, providing the foundation for the comparative study of SFT and RL post-training methods."}, {"fullname_first_author": "Yang", "paper_title": "V-IRL: Grounding virtual intelligence in real life", "publication_date": "2024-01-01", "reason": "This paper introduces the V-IRL environment, one of the two key evaluation tasks in the main study, specifically focusing on visual navigation and allowing the assessment of spatial reasoning in complex, real-world scenarios."}, {"fullname_first_author": "Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-01", "reason": "This paper introduces the concept of outcome-based reward functions via verifiers, a crucial aspect of the RL approach used in the main study to improve the model's ability to generalize."}]}