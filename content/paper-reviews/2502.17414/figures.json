[{"figure_path": "https://arxiv.org/html/2502.17414/x2.png", "caption": "Figure 1: We present\u00a0X-Dancer, a unified transformer-diffusion framework for zero-shot, music-driven human image animation from a single static image, capable of handling diverse body forms and appearances. Our method enables the synthesis of highly expressive and diverse full-body dance motions that are synchronized with music, including detailed movements at the head and hands, which are then seamlessly translated into vivid and lifelike dance videos. Code and model will be available for research purposes.", "description": "\uadf8\ub9bc 1\uc740 X-Dancer\uc758 \ud575\uc2ec \uae30\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. X-Dancer\ub294 \ub2e8\uc77c \uc815\uc9c0 \uc774\ubbf8\uc9c0\uc5d0\uc11c \ucd9c\ubc1c\ud558\uc5ec \uc74c\uc545\uc5d0 \ub9de\ucdb0 \uc6c0\uc9c1\uc774\ub294 \uc0ac\uc2e4\uc801\uc778 \ud480 \ubc14\ub514 \ub304\uc2a4 \ub3d9\uc791\uc744 \uc0dd\uc131\ud558\ub294 \ud1b5\ud569 \ubcc0\ud658\uae30-\ud655\uc0b0 \ud504\ub808\uc784\uc6cc\ud06c\uc785\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uccb4\ud615\uacfc \uc678\ubaa8\ub97c \ucc98\ub9ac\ud560 \uc218 \uc788\uc73c\uba70, \uba38\ub9ac\uc640 \uc190\uc758 \uc138\ubc00\ud55c \uc6c0\uc9c1\uc784\uae4c\uc9c0 \ud3ec\ud568\ud558\uc5ec \uc74c\uc545\uacfc \uc644\ubcbd\ud558\uac8c \ub3d9\uae30\ud654\ub41c \ub9e4\uc6b0 \ud45c\ud604\ub825 \uc788\uace0 \ub2e4\uc591\ud55c \ub304\uc2a4 \ub3d9\uc791\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc0dd\uc131\ub41c \uc6c0\uc9c1\uc784\uc740 \uc0dd\uc0dd\ud558\uace0 \uc0ac\uc2e4\uc801\uc778 \ub304\uc2a4 \ube44\ub514\uc624\ub85c \ub9e4\ub044\ub7fd\uac8c \ubcc0\ud658\ub429\ub2c8\ub2e4. \uc5f0\uad6c \ubaa9\uc801\uc73c\ub85c \ucf54\ub4dc\uc640 \ubaa8\ub378\uc744 \uacf5\uac1c\ud560 \uc608\uc815\uc785\ub2c8\ub2e4.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2502.17414/x3.png", "caption": "Figure 2: Overview of\u00a0X-Dancer.\nWe propose a cross-conditional transformer model to autoregressively generate 2D human poses synchronized with input music, followed by a diffusion model that produces high-fidelity videos from a single reference image IR.subscript\ud835\udc3c\ud835\udc45I_{R}.italic_I start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT . First, we develop a multi-part compositional tokenization for 2D poses, encoding and quantizing each body part independently with keypoint confidence. These tokens are then merged into a whole-body, confidence-aware pose using a shared decoder. Next, we train a GPT-based transformer to autoregressively predict future pose tokens with causal attention, conditioned on past poses and aligned music embeddings. For global music style and motion context, we incorporate the entire music sequence and sampled prior poses. With a learnable motion decoder, we generate multi-scale spatial pose guidance upsampled from a learned feature map, incorporating the generated motion tokens within a temporal window (16 frames) using AdaIN. By co-training the motion decoder and temporal modules, our diffusion model is capable of synthesizing temporally smooth and high-fidelity video frames, while maintaining consistent appearance with the reference image with a trained reference net.", "description": "\uadf8\ub9bc 2\ub294 X-Dancer \ubaa8\ub378\uc758 \uc804\uccb4 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uba3c\uc800, \uc785\ub825 \uc74c\uc545\uacfc \ub3d9\uae30\ud654\ub41c 2D \uc778\uccb4 \uc790\uc138\ub97c \uc790\ub3d9 \ud68c\uadc0\uc801\uc73c\ub85c \uc0dd\uc131\ud558\uae30 \uc704\ud574 \uad50\ucc28 \uc870\uac74\ubd80 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\uc744 \uc81c\uc548\ud569\ub2c8\ub2e4. \uc774\ud6c4, \ub2e8\uc77c \ucc38\uc870 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uace0\ucda9\uc2e4\ub3c4 \ube44\ub514\uc624\ub97c \uc0dd\uc131\ud558\ub294 \ud655\uc0b0 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, 2D \uc790\uc138\uc5d0 \ub300\ud574 \ub2e4\uc911 \ubd80\ubd84 \uad6c\uc131 \ud1a0\ud070\ud654\ub97c \uac1c\ubc1c\ud558\uc5ec \uac01 \uc2e0\uccb4 \ubd80\ubd84\uc744 \ud0a4\ud3ec\uc778\ud2b8 \uc2e0\ub8b0\ub3c4\uc640 \ud568\uaed8 \ub3c5\ub9bd\uc801\uc73c\ub85c \uc778\ucf54\ub529\ud558\uace0 \uc591\uc790\ud654\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ud1a0\ud070\ub4e4\uc740 \uacf5\uc720 \ub514\ucf54\ub354\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc804\uccb4 \uc2e0\uccb4\uc758 \uc2e0\ub8b0\ub3c4 \uc778\uc2dd \uc790\uc138\ub85c \ubcd1\ud569\ub429\ub2c8\ub2e4. \ub2e4\uc74c\uc73c\ub85c, \uacfc\uac70 \uc790\uc138\uc640 \uc815\ub82c\ub41c \uc74c\uc545 \uc784\ubca0\ub529\uc744 \uc870\uac74\uc73c\ub85c \ud558\uc5ec \ubbf8\ub798 \uc790\uc138 \ud1a0\ud070\uc744 \uc790\ub3d9 \ud68c\uadc0\uc801\uc73c\ub85c \uc608\uce21\ud558\ub294 GPT \uae30\ubc18 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub97c \ud559\uc2b5\uc2dc\ud0b5\ub2c8\ub2e4. \uc804\ubc18\uc801\uc778 \uc74c\uc545 \uc2a4\ud0c0\uc77c\uacfc \ub3d9\uc791 \ub9e5\ub77d\uc744 \uc704\ud574 \uc804\uccb4 \uc74c\uc545 \uc2dc\ud000\uc2a4\uc640 \uc0d8\ud50c\ub9c1\ub41c \uc774\uc804 \uc790\uc138\ub97c \ud1b5\ud569\ud569\ub2c8\ub2e4. \ud559\uc2b5 \uac00\ub2a5\ud55c \ub3d9\uc791 \ub514\ucf54\ub354\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ub41c \ud2b9\uc9d5 \ub9f5\uc5d0\uc11c \uc5c5\uc0d8\ud50c\ub9c1\ub41c \ub2e4\uc911 \uc2a4\ucf00\uc77c \uacf5\uac04 \uc790\uc138 \uc548\ub0b4\ub97c \uc0dd\uc131\ud558\uace0, AdaIN\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2dc\uac04\uc801 \ucc3d(16\ud504\ub808\uc784) \ub0b4\uc5d0\uc11c \uc0dd\uc131\ub41c \ub3d9\uc791 \ud1a0\ud070\uc744 \ud1b5\ud569\ud569\ub2c8\ub2e4. \ub3d9\uc791 \ub514\ucf54\ub354\uc640 \uc2dc\uac04\uc801 \ubaa8\ub4c8\uc744 \uacf5\ub3d9\uc73c\ub85c \ud559\uc2b5\ud568\uc73c\ub85c\uc368, \ud655\uc0b0 \ubaa8\ub378\uc740 \ucc38\uc870 \uc774\ubbf8\uc9c0\uc640 \uc77c\uad00\ub41c \ubaa8\uc591\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uc2dc\uac04\uc801\uc73c\ub85c \ubd80\ub4dc\ub7fd\uace0 \uace0\ucda9\uc2e4\ub3c4\uc758 \ube44\ub514\uc624 \ud504\ub808\uc784\uc744 \ud569\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.17414/x4.png", "caption": "Figure 3: Qualitative Comparisons. Among all the methods, X-Dancer\u00a0achieves the most expressive and high-fidelity human dance video synthesis, maintaining the highest consistency with both the reference human characteristics and the background scene.", "description": "\uadf8\ub9bc 3\uc740 \ub2e4\uc591\ud55c \uc74c\uc545 \uae30\ubc18 \ucda4 \ub3d9\uc791 \uc0dd\uc131 \ubc29\ubc95\ub4e4\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  X-Dancer\ub294 \uae30\uc874 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \ub354\uc6b1 \ud45c\ud604\ub825\uc774 \ud48d\ubd80\ud558\uace0 \uc815\ud655\ub3c4\uac00 \ub192\uc740 \uace0\ud574\uc0c1\ub3c4 \ucda4 \ub3d9\uc791 \uc601\uc0c1 \ud569\uc131\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, \uae30\uc900 \uc774\ubbf8\uc9c0\uc758 \uc0ac\ub78c \ud2b9\uc9d5\uacfc \ubc30\uacbd \uc7a5\uba74\uc758 \uc77c\uad00\uc131\uc744 \uac00\uc7a5 \uc798 \uc720\uc9c0\ud558\uba74\uc11c \uc790\uc5f0\uc2a4\ub7fd\uace0 \uc0ac\uc2e4\uc801\uc778 \ucda4 \ub3d9\uc791\uc744 \uc0dd\uc131\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\ub294 \uae30\uc900 \uc774\ubbf8\uc9c0\uc640 X-Dancer\ub97c \ud3ec\ud568\ud55c \ub2e4\ub978 \ub124 \uac00\uc9c0 \ubc29\ubc95(X-Dancer, Hallo, Bailando + PoseGuider, EDGE + PoseGuider)\uc758 \uacb0\uacfc \uc601\uc0c1\ub4e4\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \uacb0\uacfc"}]