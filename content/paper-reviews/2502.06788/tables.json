[{"content": "| Stage | Dataset | #Num | Total |\n|---|---|---|---| \n| 1 / 2.1 | Datacomp [24] | 44M | 77M |\n|  | LAION [59] | 15M |  |\n|  | SA-1B [34] | 11M |  |\n|  | OpenImages [35] | 7M |  |\n| 2.2 | Infinity-MM-GeneralQA [27] | 15M | 15M |\n| 3 | LLaVA-onevision [38] | 3.5M | 7.3M |\n|  | Infinity-MM-instruct [27] | 3.8M |  |", "caption": "Table 1: Details of training datasets across all stages. Note that we construct DenseFusion++ to re-caption web-scale image-text data.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc138\ubd80 \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ucd1d 4\uac00\uc9c0 \ub2e8\uacc4(Stage)\ub85c \ub098\ub258\uc5b4 \uc788\uc73c\uba70, \uac01 \ub2e8\uacc4\ubcc4\ub85c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \uc885\ub958\uc640 \uac1c\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \uc6f9 \uaddc\ubaa8\uc758 \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\ub97c \uc7ac\ucea1\uc158(re-caption)\ud558\uae30 \uc704\ud574 DenseFusion++\ub77c\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\ub2e4\ub294 \uc810\uc744 \uac15\uc870\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc740 \uc774\ubbf8\uc9c0 \ub370\uc774\ud130\uc14b(Datacomp, LAION, SA-1B, OpenImages \ub4f1)\uacfc \uc9c8\ubb38 \ub2f5\ubcc0(QA) \ubc0f \ub2e4\uc911 \ubaa8\ub4dc \ub300\ud654 \ub370\uc774\ud130\uc14b(LLaVA-onevision, Infinity-MM-instruct \ub4f1)\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4.  \ucd1d \ub370\uc774\ud130 \uac1c\uc218\ub294 \ub2e8\uacc4\ubcc4\ub85c \ub2e4\ub974\uba70, \ub2e8\uacc4\uac00 \uc9c4\ud589\ub420\uc218\ub85d \ub354 \ub9ce\uc740 \ub370\uc774\ud130\uac00 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "3. Methodology"}, {"content": "| Method | #A-Param | #Data | #Vtoken | MMMU | MMB<sup>en</sup> | SEED<sup>I</sup> | MMV | MME | POPE | GQA | SQA<sup>I</sup> | TQA | CQA | AI2D | RWQA | OCRB |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| *Encoder-based Vision-Language Models:* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| InternVL-1.5 | 2.2B | \u2013 / \u2013 | 3328 | 34.6 | 70.9 | 69.8 | 39.3 | 1902 | 88.3 | 61.6 | 84.9 | 70.5 | 74.8 | 69.8 | \u2013 | 654 |\n| QwenVL-Chat | 7B | 7.2B / 50M | 256 | 35.9 | 60.6 | 58.2 | \u2013 | 1848 | \u2013 | 57.5 | 68.2 | 61.5 | 49.8 | 45.9 | 49.3 | 488 |\n| LLaVA-1.5 | 7B | 0.4B+ / 665K | 576 | 35.3 | 64.3 | 64.3 | 30.5 | 1859 | 85.9 | 62.0 | 66.8 | 46.1 | 18.2 | 54.8 | 54.8 | 318 |\n| LLaVA-1.6 | 7B | 0.4B+ / 760K | 2880 | 35.1 | 67.4 | 64.7 | 43.9 | 1842 | 86.4 | 64.2 | 70.2 | 64.9 | 54.8 | 66.6 | 57.8 | 532 |\n| Cambrian | 7B | 10B+ / 7M | 576 | 42.7 | 75.9 | 74.7 | \u2013 | \u2013 | \u2013 | \u2013 | 64.6 | 80.4 | 71.7 | 73.3 | 73.0 | 64.2 | \u2013 |\n| LLaVA-OV | 7B | 10B+ / 3.2M | 7290 | 47.3 | 81.7 | 74.8 | 58.8 | 1998 | \u2013 | \u2013 | 96.6 | \u2013 | 78.8 | 81.6 | 65.5 | \u2013 |\n| *Encoder-free Vision-Language Models:* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Fuyu | 8B | \u2013 / \u2013 | \u2013 | 27.9 | 10.7 | \u2013 | 21.4 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | 64.5 | \u2013 |\n| Chameleon | 7B | 1.4B+ / 1.8M | 1024 | 25.4 | 31.1 | 30.6 | 8.3 | 170 | \u2013 | \u2013 | 47.2 | 4.8 | 2.9 | 46.0 | \u2013 | 7.0 |\n| EVE | 7B | 33M / 1.8M | 2304 | 32.6 | 52.3 | 64.6 | 25.7 | 1628 | 85.0 | 62.6 | 64.9 | 56.8 | 59.1 | 61.0 | \u2013 | 398 |\n| SOLO | 8B | 43.7M / 2M | 1024 | \u2013 | \u2013 | 64.4 | \u2013 | 1260 | \u2013 | \u2013 | 73.3 | \u2013 | \u2013 | 61.4 | \u2013 | \u2013 |\n| Mono-InternVL | 1.8B | 1.3B / 7M | 6400 | 33.7 | 65.5 | 67.4 | 40.1 | 1875 | \u2013 | 59.5 | 93.6 | 72.6 | 73.7 | 68.6 | \u2013 | 767 |\n| Emu3 | 8B | \u2013 / \u2013 | 16K | 31.6 | 58.5 | 68.2 | 37.2 | \u2013 | 85.2 | 60.3 | 89.2 | 64.7 | 68.6 | 70.0 | 57.4 | 687 |\n| **EVEv2.0** | 7B | 92M / 7.3M | 2500 | **39.3** | **66.3** | **71.4** | **45.0** | 1709 | **87.6** | **62.9** | **96.2** | 71.1 | **73.9** | **74.8** | **62.4** | 702 |", "caption": "Table 2: Comparison with existing vision-language models on various vision-language benchmarks, including MMMU\u00a0[94];\nMMBenen{}^{\\text{en}}start_FLOATSUPERSCRIPT en end_FLOATSUPERSCRIPT: MMBench-EN\u00a0[50];\nSEEDII{}^{\\text{I}}start_FLOATSUPERSCRIPT I end_FLOATSUPERSCRIPT: SEEDBench-Img\u00a0[37];\nMMV: MMVet\u00a0[93];\nMME\u00a0[23];\nPOPE\u00a0[44];\nGQA\u00a0[30];\nSQAII{}^{\\text{I}}start_FLOATSUPERSCRIPT I end_FLOATSUPERSCRIPT: ScienceQA-Img\u00a0[53];\nTVQA: TextVQA\u00a0[60];\nCQA: ChartQA\u00a0[55];\nAI2D\u00a0[32];\nRWQA: RealWorldQA\u00a0[84];\nOCRB: OCRBench\u00a0[51].\nNote that #A-Param denotes the number of activated parameters;\n#Data represents the pre-training / fine-tuning data volume;\n#Vtoken indicates the maximum image patch tokens.\nFor MME, we sum up the perception and cognition scores.\nThe best two results are marked in bold and underline.", "description": "\ud45c 2\ub294 MMMU [94], MMBench-EN [50], SEEDBench-Img [37], MMVet [93], MME [23], POPE [44], GQA [30], ScienceQA-Img [53], TextVQA [60], ChartQA [55], AI2D [32], RealWorldQA [84], OCRBench [51]\ub97c \ud3ec\ud568\ud55c \ub2e4\uc591\ud55c \ube44\uc804-\uc5b8\uc5b4 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uae30\uc874 \ube44\uc804-\uc5b8\uc5b4 \ubaa8\ub378\uacfc\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  #A-Param\uc740 \ud65c\uc131\ud654\ub41c \ub9e4\uac1c\ubcc0\uc218\uc758 \uc218, #Data\ub294 \uc0ac\uc804 \ud6c8\ub828/\ubbf8\uc138 \uc870\uc815 \ub370\uc774\ud130 \uc591, #Vtoken\uc740 \ucd5c\ub300 \uc774\ubbf8\uc9c0 \ud328\uce58 \ud1a0\ud070 \uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. MME\uc758 \uacbd\uc6b0 \uc778\uc2dd \ubc0f \uc778\uc9c0 \uc810\uc218\ub97c \ud569\uc0b0\ud588\uc2b5\ub2c8\ub2e4. \ucd5c\uace0 \uc131\ub2a5\uc744 \uae30\ub85d\ud55c \uc0c1\uc704 \ub450 \uacb0\uacfc\ub294 \uad75\uc740 \uae00\uc528\uccb4\uc640 \ubc11\uc904\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Exp. | Model | LLM | Stage 1 Data | Stage 1 T.M. | Stage 2 Training Data | Stage 2 Trainable Module | Stage 3 Data | Stage 3 T.M. |\n|---|---|---|---|---|---|---|---|---|\n| Fig.2 (i) | EVEv1.0 | Vicuna-7B | EVE-cap-16M | PEL | EVE-cap-33M | PEL, LLM | LLaVA-mix-665k | PEL, LLM |\n| Fig.5 | EVEv1.0 | Qwen2.5-7B | EVE-recap-10M | PEL | EVE-recap-8/29M | PEL | LLaVA-mix-665k | PEL, LLM |\n|  | EVEv1.2 | Qwen2.5-7B | EVE-recap-10M | PEL | EVE-recap-8/29M | PEL, VLayers | LLaVA-mix-665k | PEL, LLM |\n|  | EVEv1.5 | Qwen2.5-7B | EVE-recap-10M | PEL | EVE-recap-8/29M | PEL, VLayers | LLaVA-mix-665k | PEL, LLM |\n|  | EVEv2.0 | Qwen2.5-7B | EVE-recap-10M | PEL | EVE-recap-8/29M | PEL, VLayers | LLaVA-mix-665k | PEL, LLM |\n| Fig.6 | EVEv1.0 | Vicuna-7B | 10M varied data | PEL | 8M same data from Stage 1 | PEL, LLM | LLaVA-mix-665k | PEL, LLM |\n| Fig.7 | EVEv1.0 | Vicuna-7B | 10M varied data | PEL | 8M same data from Stage 1 | PEL, LLM | LLaVA-mix-665k | PEL, LLM |\n| Fig.8 | EVEv1.0 | Vicuna-7B | EVE-recap-10M | PEL | EVE-recap-8/29M | PEL, LLM | LLaVA-mix-665k | PEL, LLM |\n| Fig.2 (ii) | EVEv1.0 | Qwen2-7B | EVE-recap-10M | PEL | EVE-recap-29M | PEL | EVE-recap-48M | PEL, LLM |\n|  | EVEv1.2 | Qwen2-7B | EVE-recap-10M | PEL | EVE-recap-29M | PEL, VLayers | EVE-recap-48M | PEL, LLM |\n| Tab.2 | EVEv2.0 | Qwen2.5-7B | EVE-recap-10M | PEL | EVE-recap-77M | PEL, VLayers | EVE-multi-task-15M | PEL, LLM |\n| Fig.9 | EVEv2.0 | Qwen2.5-7B | EVE-recap-10M | PEL | EVE-recap-77M | PEL, VLayers | EVE-multi-task-15M | PEL, LLM |", "caption": "Table 3: Experiment Details in the main body. Note that T.M. denotes a trainable module in each stage. PEL and VLayers represent patch embedding layers and newly added vision layers in the large language model. EVE-recap-8/29M indicates a subset 8M of 29M training data.", "description": "\ud45c 3\uc740 \ub17c\ubb38\uc758 \ubcf8\ub860\uc5d0 \uc788\ub294 \uc2e4\ud5d8 \uc138\ubd80 \uc0ac\ud56d\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  T.M.\uc740 \uac01 \ub2e8\uacc4\uc5d0\uc11c \ud6c8\ub828 \uac00\ub2a5\ud55c \ubaa8\ub4c8\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. PEL\uc740 \ud328\uce58 \uc784\ubca0\ub529 \uacc4\uce35\uc744, VLayers\ub294 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378\uc5d0 \uc0c8\ub85c \ucd94\uac00\ub41c \ube44\uc804 \uacc4\uce35\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. EVE-recap-8/29M\uc740 29M \ud6c8\ub828 \ub370\uc774\ud130\uc758 8M \ubd80\ubd84\uc9d1\ud569\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \uc2e4\ud5d8 \uc124\uc815\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc5b8\uc5b4 \ubaa8\ub378, \ud6c8\ub828 \ub370\uc774\ud130, \ud6c8\ub828 \uac00\ub2a5\ud55c \ubaa8\ub4c8 \ubc0f \uac01 \ub2e8\uacc4\uc758 \uc138\ubd80 \uc0ac\ud56d\uc744 \ubcf4\uc5ec\uc8fc\uc5b4, \uac01 \uc2e4\ud5d8\uc758 \uad6c\uc131 \uc694\uc18c\uc640 \ub370\uc774\ud130 \uc0ac\uc6a9 \ubc29\uc2dd\uc744 \uba85\ud655\ud788 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3. Methodology"}, {"content": "| Stage | Dataset | #Data |\n|---|---|---|\n| 2.2 | Cambrian-FL [71], Infinity-Instruct-FL [27],  | 15M |\n|  | LVIS-Instruct-FL [77], Sharegpt4v-FL [12], |  |\n|  | ALLaVA-laion-FL [11], ALLaVA-vflan-FL [11], |  |\n|  | LLaVA-Pretrain-FL [47], DocReason-FL [29], |  |\n|  | DocDownstream-FL [29], DocStruct4M-FL [29]. |  |\n| 3 | LLaVA-onevision [38], Infinity-MM-Synthesis [27], | 7.3M |\n|  | Infinity-MM-Preference [27], Infinity-Instruct-FL [27], |  |\n|  | DenseFusion [43], Cambrian-FL [71], Docmatix-FL [36], |  |\n|  | LVIS-Instruct-FL [77], BLIP-OCR [17], LLaVA-mix [47]. |  |", "caption": "Table 4: Dataset details in Stage 2.2, and 3 for fine-tuning EVEv2.0. Note that ***-FL denotes the filtered training dataset.", "description": "\ud45c 4\ub294 EVEv2.0 \ubbf8\uc138 \uc870\uc815\uc744 \uc704\ud55c 2.2\ub2e8\uacc4\uc640 3\ub2e8\uacc4\uc758 \ub370\uc774\ud130\uc14b \uc138\ubd80 \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  2.2\ub2e8\uacc4\ub294 \ub2e4\uc591\ud55c \uc9c8\uc758\uc751\ub2f5(QA) \ub370\uc774\ud130\uc14b\uacfc \ub2e4\uc911 \ubaa8\ub4dc \ub300\ud654 \ub370\uc774\ud130\uc14b\uc744 \ud3ec\ud568\ud558\uba70, 3\ub2e8\uacc4\ub294 \uace0\ud488\uc9c8\uc758 \ub2e4\uc911 \ubaa8\ub4dc \uc9c0\uc2dc \ub370\uc774\ud130\uc14b\uc744 \ucd94\uac00\ub85c \ud3ec\ud568\ud569\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc774\ub984\uacfc \ud06c\uae30\uac00 \ud45c\uc5d0 \uba85\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, ***-FL\uc740 \ud544\ud130\ub9c1\ub41c \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 EVEv2.0 \ubaa8\ub378\uc758 \ud6c8\ub828 \uacfc\uc815\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \uad6c\uc131\uacfc \uaddc\ubaa8\ub97c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "3.3. Training Procedure"}, {"content": "| Configuration | Stage 1 | Stage 2.1 | Stage 2.2 | Stage 3 |\n|---|---|---|---|---|\n| Maximum Patch Token | 625 | 625-2500 | 2500 | 2500 |\n| Optimizer | AdamW | AdamW | AdamW | AdamW |\n| Hyperparameters | $\\beta_{1}=0.9,\\beta_{2}=0.999,eps=1e^{-8}$ | $\\beta_{1}=0.9,\\beta_{2}=0.999,eps=1e^{-8}$ | $\\beta_{1}=0.9,\\beta_{2}=0.999,eps=1e^{-8}$ | $\\beta_{1}=0.9,\\beta_{2}=0.999,eps=1e^{-8}$ |\n| Peak learning rate | $2e^{-4}$ | $1e^{-4}$ | $2e^{-5}$ | $1e^{-5}$ |\n| LR schedule | cosine decay with warm-up | cosine decay with warm-up | cosine decay with warm-up | cosine decay with warm-up |\n| Warm-up steps | 0.03 | 0.03 | 0.03 | 0.03 |\n| Weight decay | 0.0 | 0.0 | 0.0 | 0.0 |\n| Global batch size | 1024 | 1024 | 512 | 512 |\n| Numerical precision | bfloat16 | bfloat16 | bfloat16 | bfloat16 |", "caption": "Table 5: Hyper-parameter configurations in Stage 1-3 for training EVEv2.0. Note that we set the training epoch in each stage as 1.", "description": "\ud45c 5\ub294 EVEv2.0 \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0a4\ub294 \ub3d9\uc548 1\ub2e8\uacc4, 2\ub2e8\uacc4, 3\ub2e8\uacc4\ubcc4 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ub2e8\uacc4\uc758 \ud6c8\ub828 \uc5d0\ud3ed\uc740 1\ub85c \uc124\uc815\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ucd5c\ub300 \ud328\uce58 \ud1a0\ud070 \uc218, \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998, \ud559\uc2b5\ub960, \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904, \uc6dc\uc5c5 \uc2a4\ud15d, \uac00\uc911\uce58 \uac10\uc1e0, \uae00\ub85c\ubc8c \ubc30\uce58 \ud06c\uae30, \uadf8\ub9ac\uace0 \uc218\uce58\uc801 \uc815\ubc00\ub3c4 \ub4f1\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac12\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubaa8\ub378 \ud6c8\ub828\uc758 \uc138\ubd80\uc801\uc778 \uc124\uc815\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3.3. Training Procedure"}, {"content": "| User | EVEv2.0 |\n|---|---| \n| ![MAR.jpg](https://arxiv.org/html/2502.06788/extracted/6191103/figures/examples/MAR.jpg) | Cornell University We gratefully acknowledge support member. arXiv:2406.11838 Computer Science > Computer Vision and Pattern Recognition Submitted on 17 Jun 2024 (v1), last revised 1 Nov 2024 (this version, v3) Autoregressive Image Generation without Vector Quantization Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, Kaiming He Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: this https URL Comments: Neurips 2024 (Spotlight); Code: this https URL Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2406.11838 [cs.CV] (or arXiv:2406.11838v3 [cs.CV] for this version) Submission history From: Tianhong Li [view email] [v1] Mon, 17 Jun 2024 17:59:58 UTC (5,459 KB) [v2] Sun, 28 Jul 2024 05:30:08 UTC (5,447 KB) [v3] Fri, 1 Nov 2024 14:45:36 UTC (8,971 KB) |", "caption": "Table 6: EVEv2.0 exhibits excellent OCR recognition capabilities in understanding Webpage.", "description": "\ud45c 6\uc740 EVEv2.0 \ubaa8\ub378\uc774 \uc6f9\ud398\uc774\uc9c0\uc758 \ub0b4\uc6a9\uc744 \uc815\ud655\ud558\uac8c \uc778\uc2dd\ud558\uace0 \uc774\ud574\ud558\ub294 \ub2a5\ub825, \ud2b9\ud788 \uad11\ud559 \ubb38\uc790 \uc778\uc2dd(OCR) \ub2a5\ub825\uc774 \ub6f0\uc5b4\ub0a8\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\ub4e4\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \ub2e8\uc21c\ud788 OCR \uc131\ub2a5\ub9cc\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc774 \uc544\ub2c8\ub77c, \uc6f9\ud398\uc774\uc9c0\uc758 \ub808\uc774\uc544\uc6c3\uacfc \uad6c\uc870\ub97c \uc774\ud574\ud558\uace0,  \ud14d\uc2a4\ud2b8 \uc815\ubcf4\ub97c \uc815\ud655\ud558\uac8c \ucd94\ucd9c\ud558\uc5ec \ubcf4\uc5ec\uc8fc\ub294 EVEv2.0 \uc758 \uc6b0\uc218\uc131\uc744 \ub2e4\uc591\ud55c \uc608\uc2dc\ub97c \ud1b5\ud574 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 \ub2e8\uc21c\ud55c \ubb38\uc790 \uc778\uc2dd\uc744 \ub118\uc5b4, \ubcf5\uc7a1\ud55c \uc6f9\ud398\uc774\uc9c0 \uad6c\uc870\uae4c\uc9c0 \uc774\ud574\ud558\ub294 EVEv2.0 \uc758 \uace0\uae09 \uc2dc\uac01\uc801 \uc774\ud574 \ub2a5\ub825\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| User | Please describe the image in detail. |\n|---|---| \n| **EVEv2.0** | The image is a promotional poster for \u201dNeanderthal,\u201d which appears to be a documentary or film about Neanderthals. The title \u201dNeanderthal\u201d is prominently displayed in large, bold yellow letters across the center of the image. Below the title, there is additional text that reads \u201dYears 7 and 8 Welcome of the Day,\u201d suggesting that this event might be targeted towards students in those grade levels. The background of the poster features a close-up photograph of a man with a Neanderthal-like appearance. He has a long beard, a prominent forehead, and a strong jawline, which are characteristic features often associated with Neanderthals. The man is holding a wooden stick or pole, which adds to the prehistoric theme of the image. The background behind him is blurred, drawing focus to his face and upper body. At the bottom right corner of the poster, there is more text indicating the date of the event: \u201dFriday 23 January 2015.\u201d This provides a specific time frame for when the event or release of the documentary/film would have taken place. Overall, the poster uses visual elements such as the Neanderthal figure and the title to convey information about the subject matter, while also providing essential details like the target audience and the date of the event. The design effectively captures attention and communicates key information to viewers. |\n|  | ![ocr_beijing.jpg](https://arxiv.org/html/2502.06788/extracted/6191103/figures/examples/ocr_beijing.jpg) |", "caption": "Table 7: EVEv2.0 exhibits excellent OCR recognition capabilities in understanding PowerPoint.", "description": "\ud45c 7\uc740 EVEv2.0 \ubaa8\ub378\uc774 PowerPoint \ubb38\uc11c \ub0b4\uc758 \ud14d\uc2a4\ud2b8 \uc815\ubcf4\ub97c \uc815\ud655\ud558\uac8c \uc778\uc2dd\ud558\uace0 \uc774\ud574\ud558\ub294 \ub2a5\ub825, \uc989 \uad11\ud559 \ubb38\uc790 \uc778\uc2dd(OCR) \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\ub4e4\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  PowerPoint \uc2ac\ub77c\uc774\ub4dc \ub0b4\uc758 \ub2e4\uc591\ud55c \ud14d\uc2a4\ud2b8 \uc694\uc18c\ub4e4\uc5d0 \ub300\ud574 EVEv2.0\uc774 \uc5bc\ub9c8\ub098 \uc815\ud655\ud558\uac8c \ubb38\uc790\ub97c \uc778\uc2dd\ud558\uace0, \uc774\ub97c \ubc14\ud0d5\uc73c\ub85c \uc2ac\ub77c\uc774\ub4dc\uc758 \ub0b4\uc6a9\uc744 \uc774\ud574\ud558\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \uc0ac\ub840\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 EVEv2.0\uc758 \uac15\ub825\ud55c OCR \uae30\ub2a5\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.3. Ablation Studies"}, {"content": "| User | Please describe the image in detail. |\n|---|---| \n| **EVEv2.0** | In the image, there is a man standing in front of a bathroom mirror, taking a selfie with his cell phone. He is wearing a purple shirt and leopard print pants. The man appears to be smiling as he captures the photo. In the background, there is a potted plant with white flowers placed on the counter near the sink. The sink is white and has a modern design. There is also a poster hanging on the wall next to the mirror. The poster features a silhouette of a person\u2019s head with several circles on it. Below the silhouette, there is text that reads \u201dCOME SETS,\u201d followed by some additional information about Friday night events at Codame.com. The bathroom appears to be well-lit, likely due to natural light coming from a window or another source not visible in the image. The overall atmosphere seems casual and relaxed, with the man enjoying his time in the bathroom while capturing a moment through his phone camera. |\n|  | ![background](https://arxiv.org/html/2502.06788/figures/examples/background.jpg) |", "caption": "Table 8: EVEv2.0 exhibits excellent visual recognition capabilities in understanding Poster.", "description": "\ud45c 8\uc740 EVEv2.0 \ubaa8\ub378\uc774 \ud3ec\uc2a4\ud130\uc758 \ub0b4\uc6a9\uc744 \uc5bc\ub9c8\ub098 \uc798 \uc774\ud574\ud558\ub294\uc9c0, \uc989 \uc2dc\uac01\uc801 \uc778\uc2dd \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.  \ud3ec\uc2a4\ud130\uc758 \uc774\ubbf8\uc9c0\ub97c \ubcf4\uace0, EVEv2.0\uc774 \ud3ec\uc2a4\ud130\uc758 \uc81c\ubaa9, \ub0b4\uc6a9, \uc774\ubbf8\uc9c0 \uc18d \uc778\ubb3c\uc758 \ud2b9\uc9d5, \ub0a0\uc9dc \ub4f1\uc744 \uc815\ud655\ud558\uac8c \ud30c\uc545\ud558\uace0 \uc124\uba85\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 \ub2e8\uc21c\ud788 \uc774\ubbf8\uc9c0\ub97c \uc778\uc2dd\ud558\ub294 \uac83\uc744 \ub118\uc5b4\uc11c, \uc774\ubbf8\uc9c0 \uc548\uc758 \ubb38\ub9e5\uacfc \uc758\ubbf8\uae4c\uc9c0 \uc774\ud574\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  \ub530\ub77c\uc11c, \uc774 \ud45c\ub294 EVEv2.0\uc758 \uac15\ub825\ud55c \uc2dc\uac01\uc801 \uc774\ud574 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc911\uc694\ud55c \uadfc\uac70\uac00 \ub429\ub2c8\ub2e4.", "section": "4.3 Ablation Studies"}, {"content": "| User | Description |\n|---|---| \n| **EVEv2.0** | ![mac](https://arxiv.org/html/2502.06788/figures/examples/mac.jpg) The image depicts a well-organized workspace featuring a wooden desk with various items on it. At the center of the desk is a closed silver laptop computer, which has an Apple logo on its lid. This laptop is accompanied by a white keyboard placed directly in front of it, suggesting that the user might prefer using an external keyboard for typing. To the right of the laptop, there is a white wireless mouse, indicating that the user also utilizes a mouse for navigation and other tasks. On the left side of the desk, there is a glass containing a beverage, possibly water or another drink, which is positioned near the edge of the desk. In addition to these items, there are two cylindrical objects resembling speakers, one on each side of the desk. These speakers likely provide audio output for the user while working or listening to music. Behind the desk, there is a bookshelf filled with numerous books, suggesting that the user may have a keen interest in reading or research. The presence of multiple books indicates that this workspace could be used for studying, work, or leisure activities involving reading. Overall, the image portrays a functional and organized workspace designed for productivity and comfort, with essential computing equipment and additional accessories such as speakers and a beverage.|", "caption": "Table 9: EVEv2.0 exhibits excellent recognition capabilities in the real-world scene.", "description": "\ud45c 9\ub294 EVEv2.0 \ubaa8\ub378\uc774 \uc2e4\uc81c \uc138\uc0c1\uc758 \ubcf5\uc7a1\ud55c \uc2dc\uac01 \uc815\ubcf4\ub97c \uc5bc\ub9c8\ub098 \uc798 \uc774\ud574\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc0ac\uc9c4 \uc18d\uc758 \uc5ec\ub7ec \ubb3c\uccb4\ub4e4\uacfc \ubc30\uacbd, \uadf8\ub9ac\uace0 \uadf8\ub4e4\uc758 \uad00\uacc4\ub97c \uc815\ud655\ud558\uac8c \uc778\uc2dd\ud558\uace0 \uc124\uba85\ud558\ub294 \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  \ub2e8\uc21c\ud788 \ubb3c\uccb4\ub97c \uc778\uc2dd\ud558\ub294 \uac83\uc744 \ub118\uc5b4\uc11c, \uc0ac\uc9c4 \uc18d\uc758 \uc0c1\ud669\uacfc \ubd84\uc704\uae30\uae4c\uc9c0\ub3c4 \uc774\ud574\ud558\uace0 \uc790\uc5f0\uc2a4\ub7fd\uac8c \ubb18\uc0ac\ud558\ub294 EVEv2.0 \uc758 \ub6f0\uc5b4\ub09c \uc2dc\uac01\uc801 \uc774\ud574 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.", "section": "4.3 Ablation Studies"}, {"content": "| User | EVEv2.0 |\n|---|---|---|\n| <img src=\"https://arxiv.org/html/2502.06788/figures/examples/som.png\" width=\"628\" height=\"573\"> | In the image, there are three objects marked with numbers: 9, 12, and 15.<br>Object 9: A laptop computer is placed on the desk. It appears to be open and ready for use.<br>Object 12: A lamp is situated near the desk, providing light for the workspace. It has a sleek design and complements the overall aesthetic of the room.<br>Object 15: A potted plant is located on the desk, adding a touch of greenery and life to the space. It contributes to creating a more pleasant and inviting atmosphere in the room. |", "caption": "Table 10: EVEv2.0 exhibits excellent recognition capabilities in the real-world scene.", "description": "\ud45c 10\uc740 EVEv2.0 \ubaa8\ub378\uc774 \uc2e4\uc81c \uc138\uc0c1\uc758 \uc0ac\uc9c4\uc744 \uc5bc\ub9c8\ub098 \uc798 \uc774\ud574\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc0ac\uc9c4 \uc18d \uc0ac\ubb3c\ub4e4\uc744 \uc815\ud655\ud558\uac8c \uc778\uc2dd\ud558\uace0, \uac01 \uc0ac\ubb3c\uc758 \ud2b9\uc9d5\uacfc \uc0c1\ud638 \uad00\uacc4\ub97c \uc790\uc138\ud558\uac8c \uc124\uba85\ud558\uc5ec, \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud55c \ud48d\ubd80\ud55c \uc774\ud574\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 EVEv2.0 \ubaa8\ub378\uc758 \ub6f0\uc5b4\ub09c \uc2dc\uac01\uc801 \uc778\uc2dd \ub2a5\ub825\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}]