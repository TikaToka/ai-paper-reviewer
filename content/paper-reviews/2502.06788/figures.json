[{"figure_path": "https://arxiv.org/html/2502.06788/x1.png", "caption": "Figure 1: Overview of (1) diverse vision construction inside existing VLMs and (2) potential architecture variants of Encoder-Free VLMs.", "description": "\uadf8\ub9bc 1\uc740 \uae30\uc874 \ube44\uc804-\uc5b8\uc5b4 \ubaa8\ub378(VLMs)\uc5d0\uc11c \ub2e4\uc591\ud55c \uc2dc\uac01 \uc815\ubcf4 \ucc98\ub9ac \ubc29\uc2dd\uacfc \uc778\ucf54\ub354 \uc5c6\ub294 VLMs\uc758 \uc7a0\uc7ac\uc801 \uc544\ud0a4\ud14d\ucc98 \ubcc0\ud615\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (1)\uc5d0\uc11c\ub294 \uae30\uc874 VLMs\ub4e4\uc774 \uc2dc\uac01\uc801 \uc758\ubbf8\ub97c \ucd94\ucd9c\ud558\uae30 \uc704\ud574 \uc0ac\uc804 \ud6c8\ub828\ub41c \ube44\uc804 \uc778\ucf54\ub354, \uc774\uc0b0 \ud1a0\ud070\ud654\uae30, \ucd5c\uc18c\ud55c\uc758 \uc2dc\uac01 \ub808\uc774\uc5b4\ub97c \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \ub2e4\uc591\ud55c \uc2dc\uac01 \uc815\ubcf4 \ucc98\ub9ac \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.  CLIP, ViT, VQGAN \ub4f1 \uc5ec\ub7ec \ubaa8\ub378\ub4e4\uc774 \ube44\uad50 \ub300\uc0c1\uc73c\ub85c \uc81c\uc2dc\ub418\uba70 \uac01 \ubaa8\ub378\uc758 \uc2dc\uac01 \uc815\ubcf4 \ucc98\ub9ac \ubc29\uc2dd\uc758 \ucc28\uc774\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (2)\uc5d0\uc11c\ub294 \uc778\ucf54\ub354 \uc5c6\ub294 VLMs\uc758 \uc124\uacc4\ub97c \uc704\ud55c \uba87 \uac00\uc9c0 \uc7a0\uc7ac\uc801\uc778 \uc544\ud0a4\ud14d\ucc98 \ubcc0\ud615\ub4e4\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uc6d0\ud615 \uc544\ud0a4\ud14d\ucc98, \uc7ac\ub9e4\uac1c\ubcc0\uc218\ud654 \uc544\ud0a4\ud14d\ucc98, \uc804\ubb38\uac00 \ud63c\ud569(MoE) \uc544\ud0a4\ud14d\ucc98, \ubd84\ud560 \uc815\ubcf5 \uc544\ud0a4\ud14d\ucc98 \ub4f1\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70,  \uac01 \uc544\ud0a4\ud14d\ucc98\uac00 \uc2dc\uac01 \uc815\ubcf4 \ucc98\ub9ac\uc640 \uc5b8\uc5b4 \uc815\ubcf4 \ucc98\ub9ac \uac04\uc758 \uac04\uc12d\uc744 \uc904\uc774\uace0, \ud6a8\uc728\uc801\uc778 \uc2dc\uac01 \uc778\uc2dd\uc744 \uad6c\ucd95\ud558\ub294 \ub370 \uc5b4\ub5bb\uac8c \uc811\uadfc\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \uc778\ucf54\ub354 \uc5c6\ub294 VLMs\uc758 \ud2b9\uc9d5\uacfc \uc124\uacc4 \ubc29\ud5a5\uc744 \uc774\ud574\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.06788/x2.png", "caption": "Figure 2: Preliminary analyses across various VLMs\u2019 scaling efficiency during pre-training or fine-tuning (More details in\u00a0Appendix\u00a0A). Notably, VE / DT / EVE apply varying image downsampling rates (142 / 82 / 322). For fairness, we choose slightly different resolutions that yield relatively balanced token counts of 576 / 1024 / 625 tokens per image. Besides, we quantify weight changes between LLMs and VLMs by averaging absolute value variation within specific layer number or type. We report accuracy on GQA\u00a0[30], SEED\u00a0[26], TextVQA\u00a0[60], and SQA\u00a0[53] to examine VLMs\u2019 capabilities across general in-domain, open-domain, OCR-related, and text-related knowledge tasks.", "description": "\uadf8\ub9bc 2\ub294 \ub2e4\uc591\ud55c VLMs(Vision-Language Models)\uc758 \uc0ac\uc804 \ud6c8\ub828 \ub610\ub294 \ubbf8\uc138 \uc870\uc815 \uc911 \ud655\uc7a5 \ud6a8\uc728\uc131\uc5d0 \ub300\ud55c \uc608\ube44 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ubd80\ub85d A\ub97c \ucc38\uc870\ud558\uc2ed\uc2dc\uc624. \ud2b9\ud788, VE(Vision Encoder), DT(Discrete Tokenizer), EVE(Encoder-Free VLM)\ub294 \uc11c\ub85c \ub2e4\ub978 \uc774\ubbf8\uc9c0 \ub2e4\uc6b4\uc0d8\ud50c\ub9c1 \ube44\uc728(1/4, 1/8, 1/32)\uc744 \uc801\uc6a9\ud569\ub2c8\ub2e4. \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574, \uc0c1\ub300\uc801\uc73c\ub85c \uade0\ud615 \uc7a1\ud78c \ud1a0\ud070 \uc218(576/1024/625)\ub97c \uc720\uc9c0\ud558\ub294 \uc57d\uac04 \ub2e4\ub978 \ud574\uc0c1\ub3c4\ub97c \uc120\ud0dd\ud588\uc2b5\ub2c8\ub2e4.  \ub610\ud55c, \ud2b9\uc815 \uacc4\uce35 \ubc88\ud638 \ub610\ub294 \uc720\ud615 \ub0b4\uc5d0\uc11c \uc808\ub300\uac12 \ubcc0\ud654\uc758 \ud3c9\uade0\uc744 \ud1b5\ud574 LLM(Large Language Model)\uacfc VLM \uac04\uc758 \uac00\uc911\uce58 \ubcc0\ud654\ub97c \uc815\ub7c9\ud654\ud588\uc2b5\ub2c8\ub2e4. GQA [30], SEED [26], TextVQA [60], SQA [53]\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \ubcf4\uace0\ud558\uc5ec \uc77c\ubc18\uc801\uc778 \ub3c4\uba54\uc778 \ub0b4, \uc624\ud508 \ub3c4\uba54\uc778, OCR \uad00\ub828, \ud14d\uc2a4\ud2b8 \uad00\ub828 \uc9c0\uc2dd \uc791\uc5c5 \uc804\ubc18\uc5d0 \uac78\uce5c VLM\uc758 \uae30\ub2a5\uc744 \uac80\ud1a0\ud569\ub2c8\ub2e4.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2502.06788/x3.png", "caption": "Figure 3: Overview of our proposed EVEv2.0 framework. We first adopt a patch embedding layer to encode images losslessly, and then concatenate visual and textual tokens into a unified decoder-only vision-language model. Here, it extends the standard autoregressive transformer by incorporating modality-specific weights for each multi-head self-attention layer, feed-forward layer, and layer normalization.", "description": "\uadf8\ub9bc 3\uc740 \uc81c\uc548\ub41c EVEv2.0 \ud504\ub808\uc784\uc6cc\ud06c\uc758 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uba3c\uc800 \uc774\ubbf8\uc9c0\ub97c \uc190\uc2e4 \uc5c6\uc774 \uc778\ucf54\ub529\ud558\uae30 \uc704\ud574 \ud328\uce58 \uc784\ubca0\ub529 \uacc4\uce35\uc744 \uc0ac\uc6a9\ud55c \ud6c4, \uc2dc\uac01 \ubc0f \ud14d\uc2a4\ud2b8 \ud1a0\ud070\uc744 \ud1b5\ud569\ud558\uc5ec \ub514\ucf54\ub354 \uc804\uc6a9 \ube44\uc804-\uc5b8\uc5b4 \ubaa8\ub378\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. \uc774\ub294 \ud45c\uc900 \uc790\uae30 \ud68c\uadc0 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub97c \ud655\uc7a5\ud558\uc5ec \ub2e4\uc911 \ud5e4\ub4dc \uc790\uae30 \uc8fc\uc758 \uacc4\uce35, \ud53c\ub4dc\ud3ec\uc6cc\ub4dc \uacc4\uce35 \ubc0f \ub808\uc774\uc5b4 \uc815\uaddc\ud654 \uac01\uac01\uc5d0 \ub300\ud574 \ubaa8\ub2ec \ud2b9\uc815 \uac00\uc911\uce58\ub97c \ud1b5\ud569\ud569\ub2c8\ub2e4.  EVEv2.0\uc740 \ubaa8\ub2ec \uac04\uc758 \uac04\uc12d\uc744 \ucd5c\uc18c\ud654\ud558\uace0 \ud6a8\uc728\uc801\uc778 \ud559\uc2b5\uc744 \uac00\ub2a5\ud558\uac8c \ud558\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud328\uce58 \uc784\ubca0\ub529\uc740 \uc774\ubbf8\uc9c0 \uc815\ubcf4\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \ubca1\ud130\ud654\ud558\uace0, \ud1b5\ud569\ub41c \ub514\ucf54\ub354\ub294 \uc2dc\uac01 \ubc0f \uc5b8\uc5b4 \uc815\ubcf4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ucc98\ub9ac\ud569\ub2c8\ub2e4.  \ubaa8\ub2ec \ud2b9\uc815 \uac00\uc911\uce58\ub294 \uac01 \ubaa8\ub2ec\uc758 \ud2b9\uc131\uc744 \uace0\ub824\ud558\uc5ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "3.2. Model Architecture"}, {"figure_path": "https://arxiv.org/html/2502.06788/x4.png", "caption": "Figure 4: Overview of training procedure. PEL/WEL denotes patch/word embedding layer. We begin by training the patch embedding layer to establish initial alignment across modalities. Afterward, we only update vision layers within the LLM to enhance visual perception progressively. Notably, we gradually increase the image resolutions from 800\u00d7\\times\u00d7800 to 1600\u00d7\\times\u00d71600 and keep the original image aspect ratio. Finally, we train the entire model via QA and instruction data to strengthen cross-modality correspondence and complex understanding.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc758 \ud6c8\ub828 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. PEL/WEL\uc740 \ud328\uce58/\ub2e8\uc5b4 \uc784\ubca0\ub529 \ub808\uc774\uc5b4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uba3c\uc800 \ud328\uce58 \uc784\ubca0\ub529 \ub808\uc774\uc5b4\ub97c \ud6c8\ub828\ud558\uc5ec \uc5ec\ub7ec \ubaa8\ub4dc \uac04\uc758 \ucd08\uae30 \uc815\ub82c\uc744 \ud655\ub9bd\ud569\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c LLM \ub0b4\uc758 \ube44\uc804 \ub808\uc774\uc5b4\ub9cc \uc5c5\ub370\uc774\ud2b8\ud558\uc5ec \uc2dc\uac01\uc801 \uc778\uc2dd\uc744 \uc810\uc9c4\uc801\uc73c\ub85c \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4. \ud2b9\ud788 \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4\ub97c 800x800\uc5d0\uc11c 1600x1600\uc73c\ub85c \uc810\uc9c4\uc801\uc73c\ub85c \ub192\uc774\uace0 \uc6d0\ubcf8 \uc774\ubbf8\uc9c0 \uc885\ud6a1\ube44\ub97c \uc720\uc9c0\ud569\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c QA \ubc0f \uc9c0\uc2dc \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc804\uccb4 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uc5ec \ubaa8\ub4dc \uac04\uc758 \ub300\uc751 \uad00\uacc4\uc640 \ubcf5\uc7a1\ud55c \uc774\ud574\ub97c \uac15\ud654\ud569\ub2c8\ub2e4.", "section": "3.3. \ud6c8\ub828 \uc808\ucc28"}, {"figure_path": "https://arxiv.org/html/2502.06788/x7.png", "caption": "Figure 5: Training loss curve and evaluation results in Stage 2. We adopt various EVE variants based on Qwen-2.5\u00a0[70] as the baseline. We first train the patch embedding layer using EVE-recap-10M in Stage 1, and further unfreeze vision layers except LLM layers in Stage 2.", "description": "\uadf8\ub9bc 5\ub294 \ub17c\ubb38\uc758 \uc2e4\ud5d8 \uacfc\uc815 \uc911 2\ub2e8\uacc4(Stage 2)\uc5d0\uc11c \ub2e4\uc591\ud55c EVE \ubcc0\ud615 \ubaa8\ub378\uc758 \ud559\uc2b5 \uc190\uc2e4 \uace1\uc120\uacfc \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\uc900 \ubaa8\ub378(baseline)\ub85c Qwen-2.5 [70] \uae30\ubc18\uc758 EVE \ubcc0\ud615 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uba3c\uc800, 1\ub2e8\uacc4(Stage 1)\uc5d0\uc11c\ub294 EVE-recap-10M \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud328\uce58 \uc784\ubca0\ub529 \ub808\uc774\uc5b4\ub97c \ud559\uc2b5\uc2dc\ucf30\uc2b5\ub2c8\ub2e4. \uadf8 \ud6c4, 2\ub2e8\uacc4\uc5d0\uc11c\ub294 LLM \ub808\uc774\uc5b4\ub97c \uc81c\uc678\ud55c \ube44\uc804 \ub808\uc774\uc5b4\uc758 \uac00\uc911\uce58\ub97c \ud574\uc81c\ud558\uc5ec \ucd94\uac00 \ud559\uc2b5\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \uac01 \ubaa8\ub378\uc758 \uc190\uc2e4 \uace1\uc120\uacfc \uc5ec\ub7ec \ube44\uc804-\uc5b8\uc5b4 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\uc5b4, \ub2e4\uc591\ud55c \ud559\uc2b5 \uc804\ub7b5\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "3.2. Model Architecture"}, {"figure_path": "https://arxiv.org/html/2502.06788/x8.png", "caption": "Figure 6: Evaluation results of different data sources and caption engines. We utilize EVEv1.0 based on Vicuna-7B\u00a0[16] as the baseline.\nHere \u201d*-raw\u201c, \u201d*-cap\u201c, or \u201d*-recap\u201c denote noisy web image captions, the samples annotated by both LLaVA-1.5 (13B) and Emu2 (17B), or modified DenseFusion++ (7B), respectively. Note that \u201dL.O.S.\u201c represents the mixture of LAION\u00a0[59], OpenImages\u00a0[35], and SAM\u00a0[34].", "description": "\uadf8\ub9bc 6\uc740 \uc11c\ub85c \ub2e4\ub978 \ub370\uc774\ud130 \uc18c\uc2a4\uc640 \ucea1\uc158 \uc5d4\uc9c4\uc744 \uc0ac\uc6a9\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube44\uad50 \uae30\uc900\uc73c\ub85c Vicuna-7B\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 EVEv1.0 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \n\n\"*-raw\"\ub294 \uc7a1\uc74c\uc774 \ub9ce\uc740 \uc6f9 \uc774\ubbf8\uc9c0 \ucea1\uc158\uc744, \"*-cap\"\ub294 LLaVA-1.5(13B)\uc640 Emu2(17B) \ubaa8\ub450\uc5d0 \uc758\ud574 \uc8fc\uc11d\uc774 \ub2ec\ub9b0 \uc0d8\ud50c\uc744, \"*-recap\"\ub294 \uc218\uc815\ub41c DenseFusion++(7B)\uc5d0 \uc758\ud574 \uc8fc\uc11d\uc774 \ub2ec\ub9b0 \uc0d8\ud50c\uc744 \uac01\uac01 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \"L.O.S.\"\ub294 LAION, OpenImages, SAM \ub370\uc774\ud130\uc14b\uc758 \ud63c\ud569\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uac01 \uadf8\ub798\ud504\ub294 \uc11c\ub85c \ub2e4\ub978 \ub370\uc774\ud130 \uc18c\uc2a4\uc640 \ucea1\uc158 \uc5d4\uc9c4\uc758 \uc870\ud569\uc5d0 \ub530\ub978 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\uba70, \ub370\uc774\ud130 \ud488\uc9c8\uacfc \uc591\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2502.06788/x9.png", "caption": "Figure 7: Evaluation results of mixed data ratio. We adopt EVEv1.0 with Vicuna-7B\u00a0[16] for validation. Note that x:y:z denote the proportion of synthesized data : language-only data : web data.", "description": "\uadf8\ub9bc 7\uc740 \ub2e4\uc591\ud55c \ube44\uc728\uc758 \ud63c\ud569 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec EVEv1.0 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Vicuna-7B \uc5b8\uc5b4 \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c EVEv1.0 \ubaa8\ub378\uc744 \uac80\uc99d\ud588\uc2b5\ub2c8\ub2e4. x:y:z\ub294 \ud569\uc131 \ub370\uc774\ud130, \uc5b8\uc5b4 \uc804\uc6a9 \ub370\uc774\ud130, \uc6f9 \ub370\uc774\ud130\uc758 \ube44\uc728\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uadf8\ub798\ud504\ub294 \uac01 \ub370\uc774\ud130 \uc720\ud615\uc758 \ube44\uc728 \ubcc0\ud654\uc5d0 \ub530\ub978 \ubaa8\ub378 \uc131\ub2a5\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\uc5b4, \ucd5c\uc801\uc758 \ub370\uc774\ud130 \uade0\ud615\uc744 \ucc3e\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ub370\uc774\ud130 \uc870\ud569\uc5d0 \ub530\ub978 \ubaa8\ub378 \uc131\ub2a5 \ubcc0\ud654\ub97c \ud1b5\ud574, \ucd5c\uc801\uc758 \ub370\uc774\ud130 \uade0\ud615\uc744 \uc720\uc9c0\ud558\ub294 \uac83\uc774 \ubaa8\ub378 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uc911\uc694\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.06788/x10.png", "caption": "Figure 8: Evaluation results of image settings.\nWe use EVEv1.0 with Vicuna-7B\u00a0[16].\n\u201cAnyRatio_maxL\u201d: longest image edge as 800,\n\u201cAnyRatio_LD\u201d: fixed image area as 8002,\n\u201cAnyRatio_HD\u201d: fixed image area as 16002,\n\u201cAnyResolution\u201d: arbitrary resolution.", "description": "\uadf8\ub9bc 8\uc740 \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0 \uc124\uc815\uc5d0 \ub530\ub978 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Vicuna-7B \uc5b8\uc5b4 \ubaa8\ub378\uacfc EVEv1.0 \ube44\uc804 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub124 \uac00\uc9c0 \ub2e4\ub978 \uc774\ubbf8\uc9c0 \ucc98\ub9ac \ubc29\uc2dd\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.  \"AnyRatio_maxL\"\uc740 \uac00\uc7a5 \uae34 \uc774\ubbf8\uc9c0 \ubcc0\uc758 \uae38\uc774\ub97c 800\ud53d\uc140\ub85c \uace0\uc815\ud558\uace0, \"AnyRatio_LD\"\ub294 \uc774\ubbf8\uc9c0 \uba74\uc801\uc744 800\u00b2\ud53d\uc140\ub85c \uace0\uc815\ud558\uba70, \"AnyRatio_HD\"\ub294 \uc774\ubbf8\uc9c0 \uba74\uc801\uc744 1600\u00b2\ud53d\uc140\ub85c \uace0\uc815\ud569\ub2c8\ub2e4.  \ub9c8\uc9c0\ub9c9\uc73c\ub85c \"AnyResolution\"\uc740 \uc784\uc758\uc758 \ud574\uc0c1\ub3c4\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uac01 \uc124\uc815\uc5d0 \ub530\ub978 \uc5ec\ub7ec \ube44\uc804-\uc5b8\uc5b4 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uc774\ubbf8\uc9c0 \ucc98\ub9ac \ubc29\uc2dd\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud30c\uc545\ud569\ub2c8\ub2e4.", "section": "4.3 Ablation Studies"}]