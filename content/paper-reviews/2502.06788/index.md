---
title: "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models"
summary: "EVEv2.0: ì¸ì½”ë” ì—†ëŠ” ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ íšê¸°ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¨ ìƒˆë¡œìš´ ê¸°ì¤€."
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Multimodal Learning", "Vision-Language Models", "ğŸ¢ DLUT",]
showSummary: true
date: 2025-02-10
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2502.06788 {{< /keyword >}}
{{< keyword icon="writer" >}} Haiwen Diao et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2025-02-11 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2502.06788" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2502.06788" target="_self" >}}
â†— Hugging Face
{{< /button >}}




### TL;DR


{{< lead >}}

ê¸°ì¡´ì˜ ì¸ì½”ë” ê¸°ë°˜ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)ì€ ë³µì¡í•œ êµ¬ì¡°ì™€ ë†’ì€ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ ì¸í•´ íš¨ìœ¨ì ì¸ ë°°í¬ê°€ ì–´ë µë‹¤ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.  ìµœê·¼ ì¸ì½”ë”ê°€ ì—†ëŠ” VLMì´ ë“±ì¥í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í•˜ì˜€ìœ¼ë‚˜, **ì¸ì½”ë” ê¸°ë°˜ ëª¨ë¸ê³¼ì˜ ì„±ëŠ¥ ì°¨ì´**ë¥¼ ì¤„ì´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.  



ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **ëª¨ë‹¬ ê°„ ê°„ì„­ ê°ì†Œ ì „ëµ**, **íš¨ìœ¨ì ì¸ í•™ìŠµ ì „ëµ**, ê·¸ë¦¬ê³  **ëª¨ë“ˆ ë¶„ë¦¬ ì•„í‚¤í…ì²˜(Divide-and-Conquer)**ë¥¼ ì ìš©í•œ EVEv2.0 ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤.  ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ì‹¤í—˜ ê²°ê³¼, EVEv2.0 ëª¨ë¸ì€ ê¸°ì¡´ ì¸ì½”ë” ì—†ëŠ” VLMì„ ë›°ì–´ë„˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ì¸ì½”ë” ê¸°ë°˜ ëª¨ë¸ê³¼ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ìƒë‹¹íˆ ì¤„ì˜€ìŠµë‹ˆë‹¤.  **ë°ì´í„° íš¨ìœ¨ì„±**ê³¼ **ë¹„ì „ ì¶”ë¡  ëŠ¥ë ¥**ì—ì„œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ í–¥í›„ ì¸ì½”ë” ì—†ëŠ” VLM ì—°êµ¬ì— ìƒˆë¡œìš´ ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} ì¸ì½”ë” ì—†ëŠ” VLMì—ì„œì˜ ëª¨ë‹¬ ê°„ ê°„ì„­ì„ ì¤„ì´ëŠ” íš¨ê³¼ì ì¸ ì „ëµ ì œì‹œ {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} í–¥ìƒëœ í•™ìŠµ ì „ëµì„ í†µí•œ ì¸ì½”ë” ì—†ëŠ” VLM ìµœì í™” {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} ë°ì´í„° íš¨ìœ¨ì„±ê³¼ ê°•ë ¥í•œ ë¹„ì „ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°–ì¶˜ EVEv2.0 ëª¨ë¸ ê°œë°œ {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
ë³¸ ë…¼ë¬¸ì€ **ì¸ì½”ë”ê°€ ì—†ëŠ” ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM)**ì˜ ì„±ëŠ¥ í–¥ìƒì— ì¤‘ì ì„ ë‘ê³  ìˆìœ¼ë©°, íš¨ìœ¨ì ì¸ í•™ìŠµ ì „ëµê³¼ ëª¨ë¸ êµ¬ì¡° ê°œì„ ì„ í†µí•´ ê¸°ì¡´ì˜ ì¸ì½”ë” ê¸°ë°˜ VLMê³¼ì˜ ì„±ëŠ¥ ê²©ì°¨ë¥¼ ì¤„ì´ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤. ì´ëŠ” **ë°ì´í„° íš¨ìœ¨ì„±**ê³¼ **ë¹„ì „ ì¶”ë¡  ëŠ¥ë ¥**ì„ í–¥ìƒì‹œì¼œ ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì—ì„œì˜ í™œìš© ê°€ëŠ¥ì„±ì„ ë†’ì…ë‹ˆë‹¤. ë˜í•œ, **ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼**ë¥¼ ì œì‹œí•˜ì—¬ í–¥í›„ ì—°êµ¬ ë°©í–¥ì„ ì œì‹œí•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.

------
#### Visual Insights



![](https://arxiv.org/html/2502.06788/x1.png)

> ğŸ”¼ ê·¸ë¦¼ 1ì€ ê¸°ì¡´ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLMs)ì—ì„œ ë‹¤ì–‘í•œ ì‹œê° ì •ë³´ ì²˜ë¦¬ ë°©ì‹ê³¼ ì¸ì½”ë” ì—†ëŠ” VLMsì˜ ì ì¬ì  ì•„í‚¤í…ì²˜ ë³€í˜•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. (1)ì—ì„œëŠ” ê¸°ì¡´ VLMsë“¤ì´ ì‹œê°ì  ì˜ë¯¸ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì‚¬ì „ í›ˆë ¨ëœ ë¹„ì „ ì¸ì½”ë”, ì´ì‚° í† í°í™”ê¸°, ìµœì†Œí•œì˜ ì‹œê° ë ˆì´ì–´ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ë‹¤ì–‘í•œ ì‹œê° ì •ë³´ ì²˜ë¦¬ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.  CLIP, ViT, VQGAN ë“± ì—¬ëŸ¬ ëª¨ë¸ë“¤ì´ ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ì œì‹œë˜ë©° ê° ëª¨ë¸ì˜ ì‹œê° ì •ë³´ ì²˜ë¦¬ ë°©ì‹ì˜ ì°¨ì´ì ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. (2)ì—ì„œëŠ” ì¸ì½”ë” ì—†ëŠ” VLMsì˜ ì„¤ê³„ë¥¼ ìœ„í•œ ëª‡ ê°€ì§€ ì ì¬ì ì¸ ì•„í‚¤í…ì²˜ ë³€í˜•ë“¤ì„ ì œì‹œí•©ë‹ˆë‹¤.  ì›í˜• ì•„í‚¤í…ì²˜, ì¬ë§¤ê°œë³€ìˆ˜í™” ì•„í‚¤í…ì²˜, ì „ë¬¸ê°€ í˜¼í•©(MoE) ì•„í‚¤í…ì²˜, ë¶„í•  ì •ë³µ ì•„í‚¤í…ì²˜ ë“±ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°,  ê° ì•„í‚¤í…ì²˜ê°€ ì‹œê° ì •ë³´ ì²˜ë¦¬ì™€ ì–¸ì–´ ì •ë³´ ì²˜ë¦¬ ê°„ì˜ ê°„ì„­ì„ ì¤„ì´ê³ , íš¨ìœ¨ì ì¸ ì‹œê° ì¸ì‹ì„ êµ¬ì¶•í•˜ëŠ” ë° ì–´ë–»ê²Œ ì ‘ê·¼í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì´ ê·¸ë¦¼ì€ ì¸ì½”ë” ì—†ëŠ” VLMsì˜ íŠ¹ì§•ê³¼ ì„¤ê³„ ë°©í–¥ì„ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 1: Overview of (1) diverse vision construction inside existing VLMs and (2) potential architecture variants of Encoder-Free VLMs.
> </details>





{{< table-caption >}}
| Stage | Dataset | #Num | Total |
|---|---|---|---| 
| 1 / 2.1 | Datacomp [24] | 44M | 77M |
|  | LAION [59] | 15M |  |
|  | SA-1B [34] | 11M |  |
|  | OpenImages [35] | 7M |  |
| 2.2 | Infinity-MM-GeneralQA [27] | 15M | 15M |
| 3 | LLaVA-onevision [38] | 3.5M | 7.3M |
|  | Infinity-MM-instruct [27] | 3.8M |  |{{< /table-caption >}}

> ğŸ”¼ ì´ í‘œëŠ” ë…¼ë¬¸ì˜ í›ˆë ¨ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„¸ë¶€ ì •ë³´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì´ 4ê°€ì§€ ë‹¨ê³„(Stage)ë¡œ ë‚˜ë‰˜ì–´ ìˆìœ¼ë©°, ê° ë‹¨ê³„ë³„ë¡œ ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì˜ ì¢…ë¥˜ì™€ ê°œìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ, ì›¹ ê·œëª¨ì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¬ìº¡ì…˜(re-caption)í•˜ê¸° ìœ„í•´ DenseFusion++ë¼ëŠ” ë°©ë²•ì„ ì‚¬ìš©í–ˆë‹¤ëŠ” ì ì„ ê°•ì¡°í•˜ê³  ìˆìŠµë‹ˆë‹¤.  ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì€ ì´ë¯¸ì§€ ë°ì´í„°ì…‹(Datacomp, LAION, SA-1B, OpenImages ë“±)ê³¼ ì§ˆë¬¸ ë‹µë³€(QA) ë° ë‹¤ì¤‘ ëª¨ë“œ ëŒ€í™” ë°ì´í„°ì…‹(LLaVA-onevision, Infinity-MM-instruct ë“±)ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.  ì´ ë°ì´í„° ê°œìˆ˜ëŠ” ë‹¨ê³„ë³„ë¡œ ë‹¤ë¥´ë©°, ë‹¨ê³„ê°€ ì§„í–‰ë ìˆ˜ë¡ ë” ë§ì€ ë°ì´í„°ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 1: Details of training datasets across all stages. Note that we construct DenseFusion++ to re-caption web-scale image-text data.
> </details>





### In-depth insights


#### Encoder-Free VLMs
ë³¸ ë…¼ë¬¸ì—ì„œ ë‹¤ë£¬ "Encoder-Free VLMs"ëŠ” ê¸°ì¡´ì˜ Vision-Language Model(VLM)ë“¤ì´ ì‚¬ì „ í•™ìŠµëœ ë¹„ì „ ì¸ì½”ë”ì— ì˜ì¡´í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, **ì¸ì½”ë” ì—†ì´ ìˆœìˆ˜í•˜ê²Œ ë””ì½”ë”ë§Œìœ¼ë¡œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬**í•˜ëŠ” ëª¨ë¸ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ ëª¨ë¸ì˜ êµ¬ì¡°ì  ë‹¨ìˆœí™” ë° íš¨ìœ¨ì ì¸ ë°°í¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ í†µí•© ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œì˜ ì ì¬ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  **í•˜ì§€ë§Œ, ë¹„ì „ ì¸ì½”ë” ì—†ì´ ì§ì ‘ì ìœ¼ë¡œ ì‹œê°ì  ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ì–´ë ¤ì›€**ì´ ìˆìœ¼ë©°,  **ë¹„ì „-ì–¸ì–´ ê°„ì˜ ê°„ì„­ì„ ì¤„ì´ê³  íš¨ê³¼ì ì¸ ìµœì í™” ì „ëµ**ì´ í•„ìš”í•˜ë‹¤ëŠ” ì ì´ **ì¤‘ìš”í•œ ê³¼ì œ**ë¡œ ì œê¸°ë©ë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ê³¼ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì „ëµë“¤ì„ ì œì‹œí•˜ê³ ,  **ìƒˆë¡œìš´ Encoder-Free VLMì¸ EVEv2.0**ì„ í†µí•´ ê¸°ì¡´ì˜ ì¸ì½”ë” ê¸°ë°˜ ëª¨ë¸ë“¤ê³¼ì˜ ì„±ëŠ¥ ê²©ì°¨ë¥¼ ì¤„ì´ëŠ” ë° ì„±ê³µí•©ë‹ˆë‹¤.  **ëª¨ë“ˆ ê°„ì˜ ë…ë¦½ì„±ì„ ê°•í™”**í•˜ê³  **ë°ì´í„° í™•ì¥ì„±ì„ ë†’ì´ëŠ” ì„¤ê³„**ë¥¼ í†µí•´ **ë°ì´í„° íš¨ìœ¨ì„±ê³¼ ì‹œê°ì  ì¶”ë¡  ì„±ëŠ¥**ì„ í–¥ìƒì‹œí‚¨ ì ì´ ì£¼ìš” ì„±ê³¼ì…ë‹ˆë‹¤.  ì´ëŠ” **ë‹¨ì¼ ëª¨ë¸ ë‚´ì—ì„œ ì‹œê°ì  ì¸ì‹ ë° ì–¸ì–´ ì´í•´ì˜ í†µí•©**ì´ë¼ëŠ” ë©€í‹°ëª¨ë‹¬ í•™ìŠµì˜ í•µì‹¬ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ì¤‘ìš”í•œ ì§„ì „ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

#### Modular Design
ëª¨ë“ˆí™” ì„¤ê³„ëŠ” ë³µì¡í•œ ì‹œìŠ¤í…œì„ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„í•´í•˜ì—¬ ê°œë°œ, ìœ ì§€ë³´ìˆ˜ ë° í™•ì¥ì„ ìš©ì´í•˜ê²Œ í•˜ëŠ” ì„¤ê³„ ì›ì¹™ì…ë‹ˆë‹¤.  **ê° ëª¨ë“ˆì€ íŠ¹ì • ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ë©° ë‹¤ë¥¸ ëª¨ë“ˆê³¼ì˜ ìƒí˜¸ì‘ìš©ì€ ëª…í™•í•˜ê²Œ ì •ì˜ë©ë‹ˆë‹¤.** ì´ë¥¼ í†µí•´ ì‹œìŠ¤í…œì˜ ë³µì¡ì„±ì„ ì¤„ì´ê³ , ê°œë°œ ì‹œê°„ì„ ë‹¨ì¶•í•˜ë©°, ìœ ì§€ë³´ìˆ˜ ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  **ëª¨ë“ˆ ê°„ì˜ ì¸í„°í˜ì´ìŠ¤ëŠ” ì˜ ì •ì˜ë˜ì–´ì•¼ í•˜ë©°, ëª¨ë“ˆ ë‚´ë¶€ì˜ ë³€ê²½ì´ ë‹¤ë¥¸ ëª¨ë“ˆì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šë„ë¡ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤.**  **ì¬ì‚¬ìš©ì„± ë˜í•œ ëª¨ë“ˆí™” ì„¤ê³„ì˜ ì¤‘ìš”í•œ ì´ì ì…ë‹ˆë‹¤.**  ì˜ ì„¤ê³„ëœ ëª¨ë“ˆì€ ì—¬ëŸ¬ í”„ë¡œì íŠ¸ì—ì„œ ì¬ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ê°œë°œ ì‹œê°„ ë° ë¹„ìš©ì„ í¬ê²Œ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  **í•˜ì§€ë§Œ ëª¨ë“ˆí™” ì„¤ê³„ëŠ” ê³¼ë„í•œ ëª¨ë“ˆ ë¶„í• ë¡œ ì¸í•´ ì‹œìŠ¤í…œ í†µí•©ì— ì–´ë ¤ì›€ì„ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.** ë”°ë¼ì„œ **ëª¨ë“ˆì˜ í¬ê¸°ì™€ ê¸°ëŠ¥ì€ ì‹ ì¤‘í•˜ê²Œ ê²°ì •í•´ì•¼ í•˜ë©°, ëª¨ë“ˆ ê°„ì˜ ìƒí˜¸ì‘ìš©ì´ íš¨ìœ¨ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ë„ë¡ ì„¤ê³„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.**  ê²°ë¡ ì ìœ¼ë¡œ, ëª¨ë“ˆí™” ì„¤ê³„ëŠ” ë³µì¡í•œ ì‹œìŠ¤í…œ ê°œë°œì— ìˆì–´ íš¨ìœ¨ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ ë†’ì´ëŠ” ë° í•„ìˆ˜ì ì¸ ìš”ì†Œì´ì§€ë§Œ,  **ì ì ˆí•œ ëª¨ë“ˆ í¬ê¸° ë° ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„ê°€ ì¤‘ìš”í•˜ë©°, ê³¼ë„í•œ ëª¨ë“ˆ ë¶„í• ì€ í”¼í•´ì•¼ í•©ë‹ˆë‹¤.**

#### Training Strategies
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” íš¨ê³¼ì ì¸ **Vision-Language Model (VLM) í•™ìŠµ ì „ëµ**ì— ëŒ€í•´ ì‹¬ë„ìˆê²Œ ë…¼ì˜í•©ë‹ˆë‹¤.  íŠ¹íˆ, **ì¸ì½”ë”ê°€ ì—†ëŠ” VLM ì•„í‚¤í…ì²˜**ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¨ë©°, ê¸°ì¡´ì˜ ë°©ì‹ê³¼ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì–´ë‚¸ í•µì‹¬ ì „ëµë“¤ì„ ë¶„ì„í•©ë‹ˆë‹¤.  **ë‹¨ê³„ë³„ í•™ìŠµ ë°©ì‹**ì„ í†µí•´ ëª¨ë¸ì˜ ì•ˆì •ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì´ê³ , **ëª¨ë“ˆ ê°„ ê°„ì„­ì„ ìµœì†Œí™”**í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.  **ë°ì´í„° ì¦ê°• ë° ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ í™œìš©** ì „ëµì€ ë°ì´í„° íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ê³ , ë‹¤ì–‘í•œ ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.  **ëª¨ë¸ì˜ êµ¬ì¡°ì  ë¶„í• **ê³¼ **ëª¨ë“ˆë³„ ë§¤ê°œë³€ìˆ˜ ë¶„ë¦¬**ëŠ” í•™ìŠµ ê³¼ì •ì˜ ì•ˆì •ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤.  **ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì˜ í™œìš©**ì€ ì‹œê°ì  ì •ë³´ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ì§€ë§Œ, ë™ì‹œì— **ëª¨ë¸ ê°„ ê°„ì„­ì„ ìµœì†Œí™”**í•˜ëŠ” ì „ëµì´ ë³‘í–‰ë˜ì–´ì•¼ í•¨ì„ ê°•ì¡°í•©ë‹ˆë‹¤.  **ì´ˆê¸° ë‹¨ê³„ì˜ ì‚¬ì „ í•™ìŠµ**ì€ ëª¨ë¸ì˜ ê¸°ì´ˆì ì¸ ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ì„ êµ¬ì¶•í•˜ê³ , í›„ì† ë‹¨ê³„ì˜ í•™ìŠµì„ ìœ„í•œ í† ëŒ€ë¥¼ ë§ˆë ¨í•©ë‹ˆë‹¤.  ì´ëŸ¬í•œ ë‹¤ì–‘í•œ ì „ëµì˜ ì¡°í•©ì„ í†µí•´, ë³¸ ì—°êµ¬ëŠ” **ë°ì´í„° íš¨ìœ¨ì„±**ê³¼ **ì„±ëŠ¥**ì„ ë™ì‹œì— í–¥ìƒì‹œí‚¤ëŠ” í˜ì‹ ì ì¸ VLM í•™ìŠµ ë°©ì‹ì„ ì œì‹œí•©ë‹ˆë‹¤.

#### Data Efficiency
ë³¸ ë…¼ë¬¸ì—ì„œ ë‹¤ë£¬ EVEv2.0 ëª¨ë¸ì€ **ë°ì´í„° íš¨ìœ¨ì„±** ì¸¡ë©´ì—ì„œ ì£¼ëª©í•  ë§Œí•œ íŠ¹ì§•ì„ ë³´ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì¸ì½”ë” ê¸°ë°˜ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ë“¤ê³¼ ë¹„êµí–ˆì„ ë•Œ, í›¨ì”¬ ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ëŠ” **ëª¨ë¸ êµ¬ì¡°ì˜ íš¨ìœ¨ì ì¸ ì„¤ê³„**ì™€ **í›ˆë ¨ ì „ëµì˜ ê°œì„ ** ë•ë¶„ì…ë‹ˆë‹¤. íŠ¹íˆ, **ëª¨ë“ˆ ê°„ì˜ ê°„ì„­ì„ ìµœì†Œí™”í•˜ëŠ” Divide-and-Conquer ì•„í‚¤í…ì²˜**ì™€ **ë‹¨ê³„ì  í›ˆë ¨ ê³¼ì •**ì„ í†µí•´ ë°ì´í„° í™œìš©ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.  **ì €ìë“¤ì€ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ì´ìš©í•œ ì‚¬ì „ í›ˆë ¨** ëŒ€ì‹ , íš¨ìœ¨ì ì¸ í•©ì„± ë°ì´í„° ìƒì„± ì „ëµì„ í™œìš©í•˜ì—¬ ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.  ì´ëŠ” **ë¹„ìš© íš¨ìœ¨ì ì´ë©´ì„œë„ ì„±ëŠ¥ ì €í•˜ ì—†ì´** ëª¨ë¸ì„ í›ˆë ¨í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  **ë°ì´í„° íš¨ìœ¨ì„±ì˜ í–¥ìƒì€** íŠ¹íˆ, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ í™•ë³´ê°€ ì–´ë ¤ìš´ ìƒí™©ì—ì„œ ë§¤ìš° ì¤‘ìš”í•˜ë©°,  EVEv2.0 ëª¨ë¸ì€ ì´ëŸ¬í•œ ë¬¸ì œì— ëŒ€í•œ ì‹¤ìš©ì ì¸ í•´ê²°ì±…ì„ ì œì‹œí•©ë‹ˆë‹¤.  ì´ëŠ” í–¥í›„ **ë¹„ì „-ì–¸ì–´ ëª¨ë¸ ì—°êµ¬ì˜ ìƒˆë¡œìš´ ë°©í–¥**ì„ ì œì‹œí•˜ëŠ” ì¤‘ìš”í•œ ë°œê²¬ì…ë‹ˆë‹¤.  **ëª¨ë¸ì˜ í™•ì¥ì„±**ê³¼ **ì‹¤ì œ ì‘ìš© ê°€ëŠ¥ì„±**ì„ ë†’ì´ëŠ”ë° í¬ê²Œ ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.

#### Future VLMs
ë¯¸ë˜ì˜ ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLMs)ì€ **ëª¨ë‹¬ ê°„ì˜ ê°„ì„­ì„ ì¤„ì´ê³ , íš¨ìœ¨ì ì¸ ì‹œê°ì  ì§€ê° í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, í™•ì¥ì„±ì„ ë†’ì´ëŠ” ë°©í–¥**ìœ¼ë¡œ ë°œì „í•  ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” **ëª¨ë‹¬ íŠ¹ì§•ì„ ë¶„ë¦¬í•˜ê³  ê³„ì¸µì ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” ì„¤ê³„**, **íš¨ìœ¨ì ì¸ í›ˆë ¨ ì „ëµ**, ê·¸ë¦¬ê³  **ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹**ì˜ ì‚¬ìš©ì„ í†µí•´ ë‹¬ì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  **ë°ì´í„° íš¨ìœ¨ì„±ê³¼ ì‹œê°ì  ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒ**ì‹œí‚¤ëŠ” ë° ì¤‘ì ì„ ë‘ì–´, ë³´ë‹¤ ì ì€ ìì›ìœ¼ë¡œë„ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ ê°œë°œì´ ì¤‘ìš”í•´ì§ˆ ê²ƒì…ë‹ˆë‹¤.  ë˜í•œ, ë‹¤ì–‘í•œ ëª¨ë‹¬(ì˜ˆ: ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤)ì„ í†µí•©í•˜ê³  ì‹¤ì œ ì‘ìš© ë¶„ì•¼ì— ì ìš© ê°€ëŠ¥í•˜ë„ë¡ **ë²”ìš©ì„±ì„ ë†’ì´ëŠ” ì—°êµ¬**ê°€ í™œë°œí•´ì§ˆ ê²ƒì…ë‹ˆë‹¤.  **ëª¨ë¸ì˜ êµ¬ì¡°ì  ë‹¨ìˆœí™”**ë¥¼ í†µí•´ ë°°í¬ ë° ì‚¬ìš©ì˜ ìš©ì´ì„±ì„ ë†’ì´ëŠ” ê²ƒ ë˜í•œ ì¤‘ìš”í•œ ë°œì „ ë°©í–¥ì´ ë  ê²ƒì…ë‹ˆë‹¤.  **ëª¨ë¸ì˜ í™•ì¥ì„±ê³¼ ì•ˆì •ì„±ì„ í™•ë³´í•˜ëŠ” ì—°êµ¬**ë„ ì§€ì†ë  ê²ƒì´ë©°, ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì‹¤ì œ ì‘ìš© ë¶„ì•¼ì—ì„œ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ í™œìš©ë  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2502.06788/x2.png)

> ğŸ”¼ ê·¸ë¦¼ 2ëŠ” ë‹¤ì–‘í•œ VLMs(Vision-Language Models)ì˜ ì‚¬ì „ í›ˆë ¨ ë˜ëŠ” ë¯¸ì„¸ ì¡°ì • ì¤‘ í™•ì¥ íš¨ìœ¨ì„±ì— ëŒ€í•œ ì˜ˆë¹„ ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ìì„¸í•œ ë‚´ìš©ì€ ë¶€ë¡ Aë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤. íŠ¹íˆ, VE(Vision Encoder), DT(Discrete Tokenizer), EVE(Encoder-Free VLM)ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§ ë¹„ìœ¨(1/4, 1/8, 1/32)ì„ ì ìš©í•©ë‹ˆë‹¤. ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´, ìƒëŒ€ì ìœ¼ë¡œ ê· í˜• ì¡íŒ í† í° ìˆ˜(576/1024/625)ë¥¼ ìœ ì§€í•˜ëŠ” ì•½ê°„ ë‹¤ë¥¸ í•´ìƒë„ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.  ë˜í•œ, íŠ¹ì • ê³„ì¸µ ë²ˆí˜¸ ë˜ëŠ” ìœ í˜• ë‚´ì—ì„œ ì ˆëŒ€ê°’ ë³€í™”ì˜ í‰ê· ì„ í†µí•´ LLM(Large Language Model)ê³¼ VLM ê°„ì˜ ê°€ì¤‘ì¹˜ ë³€í™”ë¥¼ ì •ëŸ‰í™”í–ˆìŠµë‹ˆë‹¤. GQA [30], SEED [26], TextVQA [60], SQA [53]ì— ëŒ€í•œ ì •í™•ë„ë¥¼ ë³´ê³ í•˜ì—¬ ì¼ë°˜ì ì¸ ë„ë©”ì¸ ë‚´, ì˜¤í”ˆ ë„ë©”ì¸, OCR ê´€ë ¨, í…ìŠ¤íŠ¸ ê´€ë ¨ ì§€ì‹ ì‘ì—… ì „ë°˜ì— ê±¸ì¹œ VLMì˜ ê¸°ëŠ¥ì„ ê²€í† í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 2: Preliminary analyses across various VLMsâ€™ scaling efficiency during pre-training or fine-tuning (More details inÂ AppendixÂ A). Notably, VE / DT / EVE apply varying image downsampling rates (142 / 82 / 322). For fairness, we choose slightly different resolutions that yield relatively balanced token counts of 576 / 1024 / 625 tokens per image. Besides, we quantify weight changes between LLMs and VLMs by averaging absolute value variation within specific layer number or type. We report accuracy on GQAÂ [30], SEEDÂ [26], TextVQAÂ [60], and SQAÂ [53] to examine VLMsâ€™ capabilities across general in-domain, open-domain, OCR-related, and text-related knowledge tasks.
> </details>



![](https://arxiv.org/html/2502.06788/x3.png)

> ğŸ”¼ ê·¸ë¦¼ 3ì€ ì œì•ˆëœ EVEv2.0 í”„ë ˆì„ì›Œí¬ì˜ ê°œìš”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ì†ì‹¤ ì—†ì´ ì¸ì½”ë”©í•˜ê¸° ìœ„í•´ íŒ¨ì¹˜ ì„ë² ë”© ê³„ì¸µì„ ì‚¬ìš©í•œ í›„, ì‹œê° ë° í…ìŠ¤íŠ¸ í† í°ì„ í†µí•©í•˜ì—¬ ë””ì½”ë” ì „ìš© ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤. ì´ëŠ” í‘œì¤€ ìê¸° íšŒê·€ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ í™•ì¥í•˜ì—¬ ë‹¤ì¤‘ í—¤ë“œ ìê¸° ì£¼ì˜ ê³„ì¸µ, í”¼ë“œí¬ì›Œë“œ ê³„ì¸µ ë° ë ˆì´ì–´ ì •ê·œí™” ê°ê°ì— ëŒ€í•´ ëª¨ë‹¬ íŠ¹ì • ê°€ì¤‘ì¹˜ë¥¼ í†µí•©í•©ë‹ˆë‹¤.  EVEv2.0ì€ ëª¨ë‹¬ ê°„ì˜ ê°„ì„­ì„ ìµœì†Œí™”í•˜ê³  íš¨ìœ¨ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.  íŒ¨ì¹˜ ì„ë² ë”©ì€ ì´ë¯¸ì§€ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë²¡í„°í™”í•˜ê³ , í†µí•©ëœ ë””ì½”ë”ëŠ” ì‹œê° ë° ì–¸ì–´ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.  ëª¨ë‹¬ íŠ¹ì • ê°€ì¤‘ì¹˜ëŠ” ê° ëª¨ë‹¬ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 3: Overview of our proposed EVEv2.0 framework. We first adopt a patch embedding layer to encode images losslessly, and then concatenate visual and textual tokens into a unified decoder-only vision-language model. Here, it extends the standard autoregressive transformer by incorporating modality-specific weights for each multi-head self-attention layer, feed-forward layer, and layer normalization.
> </details>



![](https://arxiv.org/html/2502.06788/x4.png)

> ğŸ”¼ ë³¸ ê·¸ë¦¼ì€ ë…¼ë¬¸ì˜ í›ˆë ¨ ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. PEL/WELì€ íŒ¨ì¹˜/ë‹¨ì–´ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë¨¼ì € íŒ¨ì¹˜ ì„ë² ë”© ë ˆì´ì–´ë¥¼ í›ˆë ¨í•˜ì—¬ ì—¬ëŸ¬ ëª¨ë“œ ê°„ì˜ ì´ˆê¸° ì •ë ¬ì„ í™•ë¦½í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ LLM ë‚´ì˜ ë¹„ì „ ë ˆì´ì–´ë§Œ ì—…ë°ì´íŠ¸í•˜ì—¬ ì‹œê°ì  ì¸ì‹ì„ ì ì§„ì ìœ¼ë¡œ í–¥ìƒì‹œí‚µë‹ˆë‹¤. íŠ¹íˆ ì´ë¯¸ì§€ í•´ìƒë„ë¥¼ 800x800ì—ì„œ 1600x1600ìœ¼ë¡œ ì ì§„ì ìœ¼ë¡œ ë†’ì´ê³  ì›ë³¸ ì´ë¯¸ì§€ ì¢…íš¡ë¹„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ QA ë° ì§€ì‹œ ë°ì´í„°ë¥¼ í†µí•´ ì „ì²´ ëª¨ë¸ì„ í›ˆë ¨í•˜ì—¬ ëª¨ë“œ ê°„ì˜ ëŒ€ì‘ ê´€ê³„ì™€ ë³µì¡í•œ ì´í•´ë¥¼ ê°•í™”í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 4: Overview of training procedure. PEL/WEL denotes patch/word embedding layer. We begin by training the patch embedding layer to establish initial alignment across modalities. Afterward, we only update vision layers within the LLM to enhance visual perception progressively. Notably, we gradually increase the image resolutions from 800Ã—\timesÃ—800 to 1600Ã—\timesÃ—1600 and keep the original image aspect ratio. Finally, we train the entire model via QA and instruction data to strengthen cross-modality correspondence and complex understanding.
> </details>



![](https://arxiv.org/html/2502.06788/x7.png)

> ğŸ”¼ ê·¸ë¦¼ 5ëŠ” ë…¼ë¬¸ì˜ ì‹¤í—˜ ê³¼ì • ì¤‘ 2ë‹¨ê³„(Stage 2)ì—ì„œ ë‹¤ì–‘í•œ EVE ë³€í˜• ëª¨ë¸ì˜ í•™ìŠµ ì†ì‹¤ ê³¡ì„ ê³¼ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê¸°ì¤€ ëª¨ë¸(baseline)ë¡œ Qwen-2.5 [70] ê¸°ë°˜ì˜ EVE ë³€í˜• ëª¨ë¸ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë¨¼ì €, 1ë‹¨ê³„(Stage 1)ì—ì„œëŠ” EVE-recap-10M ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ íŒ¨ì¹˜ ì„ë² ë”© ë ˆì´ì–´ë¥¼ í•™ìŠµì‹œì¼°ìŠµë‹ˆë‹¤. ê·¸ í›„, 2ë‹¨ê³„ì—ì„œëŠ” LLM ë ˆì´ì–´ë¥¼ ì œì™¸í•œ ë¹„ì „ ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ í•´ì œí•˜ì—¬ ì¶”ê°€ í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ê·¸ë¦¼ì€ ê° ëª¨ë¸ì˜ ì†ì‹¤ ê³¡ì„ ê³¼ ì—¬ëŸ¬ ë¹„ì „-ì–¸ì–´ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì •í™•ë„ë¥¼ ë³´ì—¬ì£¼ì–´, ë‹¤ì–‘í•œ í•™ìŠµ ì „ëµì´ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 5: Training loss curve and evaluation results in Stage 2. We adopt various EVE variants based on Qwen-2.5Â [70] as the baseline. We first train the patch embedding layer using EVE-recap-10M in Stage 1, and further unfreeze vision layers except LLM layers in Stage 2.
> </details>



![](https://arxiv.org/html/2502.06788/x8.png)

> ğŸ”¼ ê·¸ë¦¼ 6ì€ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì†ŒìŠ¤ì™€ ìº¡ì…˜ ì—”ì§„ì„ ì‚¬ìš©í•œ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ë¹„êµ ê¸°ì¤€ìœ¼ë¡œ Vicuna-7Bë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” EVEv1.0 ëª¨ë¸ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.   '*-raw'ëŠ” ì¡ìŒì´ ë§ì€ ì›¹ ì´ë¯¸ì§€ ìº¡ì…˜ì„, '*-cap'ëŠ” LLaVA-1.5(13B)ì™€ Emu2(17B) ëª¨ë‘ì— ì˜í•´ ì£¼ì„ì´ ë‹¬ë¦° ìƒ˜í”Œì„, '*-recap'ëŠ” ìˆ˜ì •ëœ DenseFusion++(7B)ì— ì˜í•´ ì£¼ì„ì´ ë‹¬ë¦° ìƒ˜í”Œì„ ê°ê° ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 'L.O.S.'ëŠ” LAION, OpenImages, SAM ë°ì´í„°ì…‹ì˜ í˜¼í•©ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  ê° ê·¸ë˜í”„ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì†ŒìŠ¤ì™€ ìº¡ì…˜ ì—”ì§„ì˜ ì¡°í•©ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”ë¥¼ ë³´ì—¬ì£¼ë©°, ë°ì´í„° í’ˆì§ˆê³¼ ì–‘ì´ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 6: Evaluation results of different data sources and caption engines. We utilize EVEv1.0 based on Vicuna-7BÂ [16] as the baseline. Here â€*-rawâ€œ, â€*-capâ€œ, or â€*-recapâ€œ denote noisy web image captions, the samples annotated by both LLaVA-1.5 (13B) and Emu2 (17B), or modified DenseFusion++ (7B), respectively. Note that â€L.O.S.â€œ represents the mixture of LAIONÂ [59], OpenImagesÂ [35], and SAMÂ [34].
> </details>



![](https://arxiv.org/html/2502.06788/x9.png)

> ğŸ”¼ ê·¸ë¦¼ 7ì€ ë‹¤ì–‘í•œ ë¹„ìœ¨ì˜ í˜¼í•© ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ EVEv1.0 ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  Vicuna-7B ì–¸ì–´ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ EVEv1.0 ëª¨ë¸ì„ ê²€ì¦í–ˆìŠµë‹ˆë‹¤. x:y:zëŠ” í•©ì„± ë°ì´í„°, ì–¸ì–´ ì „ìš© ë°ì´í„°, ì›¹ ë°ì´í„°ì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê·¸ë˜í”„ëŠ” ê° ë°ì´í„° ìœ í˜•ì˜ ë¹„ìœ¨ ë³€í™”ì— ë”°ë¥¸ ëª¨ë¸ ì„±ëŠ¥ì˜ ë³€í™”ë¥¼ ë³´ì—¬ì£¼ì–´, ìµœì ì˜ ë°ì´í„° ê· í˜•ì„ ì°¾ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.  ë‹¤ì–‘í•œ ë°ì´í„° ì¡°í•©ì— ë”°ë¥¸ ëª¨ë¸ ì„±ëŠ¥ ë³€í™”ë¥¼ í†µí•´, ìµœì ì˜ ë°ì´í„° ê· í˜•ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì— ì¤‘ìš”í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 7: Evaluation results of mixed data ratio. We adopt EVEv1.0 with Vicuna-7BÂ [16] for validation. Note that x:y:z denote the proportion of synthesized data : language-only data : web data.
> </details>



![](https://arxiv.org/html/2502.06788/x10.png)

> ğŸ”¼ ê·¸ë¦¼ 8ì€ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì„¤ì •ì— ë”°ë¥¸ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. Vicuna-7B ì–¸ì–´ ëª¨ë¸ê³¼ EVEv1.0 ë¹„ì „ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë„¤ ê°€ì§€ ë‹¤ë¥¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ë°©ì‹ì„ ë¹„êµí•©ë‹ˆë‹¤.  'AnyRatio_maxL'ì€ ê°€ì¥ ê¸´ ì´ë¯¸ì§€ ë³€ì˜ ê¸¸ì´ë¥¼ 800í”½ì…€ë¡œ ê³ ì •í•˜ê³ , 'AnyRatio_LD'ëŠ” ì´ë¯¸ì§€ ë©´ì ì„ 800Â²í”½ì…€ë¡œ ê³ ì •í•˜ë©°, 'AnyRatio_HD'ëŠ” ì´ë¯¸ì§€ ë©´ì ì„ 1600Â²í”½ì…€ë¡œ ê³ ì •í•©ë‹ˆë‹¤.  ë§ˆì§€ë§‰ìœ¼ë¡œ 'AnyResolution'ì€ ì„ì˜ì˜ í•´ìƒë„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê° ì„¤ì •ì— ë”°ë¥¸ ì—¬ëŸ¬ ë¹„ì „-ì–¸ì–´ ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•˜ì—¬ ì´ë¯¸ì§€ ì²˜ë¦¬ ë°©ì‹ì´ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 8: Evaluation results of image settings. We use EVEv1.0 with Vicuna-7BÂ [16]. â€œAnyRatio_maxLâ€: longest image edge as 800, â€œAnyRatio_LDâ€: fixed image area as 8002, â€œAnyRatio_HDâ€: fixed image area as 16002, â€œAnyResolutionâ€: arbitrary resolution.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Method | #A-Param | #Data | #Vtoken | MMMU | MMB<sup>en</sup> | SEED<sup>I</sup> | MMV | MME | POPE | GQA | SQA<sup>I</sup> | TQA | CQA | AI2D | RWQA | OCRB |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| *Encoder-based Vision-Language Models:* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| InternVL-1.5 | 2.2B | â€“ / â€“ | 3328 | 34.6 | 70.9 | 69.8 | 39.3 | 1902 | 88.3 | 61.6 | 84.9 | 70.5 | 74.8 | 69.8 | â€“ | 654 |
| QwenVL-Chat | 7B | 7.2B / 50M | 256 | 35.9 | 60.6 | 58.2 | â€“ | 1848 | â€“ | 57.5 | 68.2 | 61.5 | 49.8 | 45.9 | 49.3 | 488 |
| LLaVA-1.5 | 7B | 0.4B+ / 665K | 576 | 35.3 | 64.3 | 64.3 | 30.5 | 1859 | 85.9 | 62.0 | 66.8 | 46.1 | 18.2 | 54.8 | 54.8 | 318 |
| LLaVA-1.6 | 7B | 0.4B+ / 760K | 2880 | 35.1 | 67.4 | 64.7 | 43.9 | 1842 | 86.4 | 64.2 | 70.2 | 64.9 | 54.8 | 66.6 | 57.8 | 532 |
| Cambrian | 7B | 10B+ / 7M | 576 | 42.7 | 75.9 | 74.7 | â€“ | â€“ | â€“ | â€“ | 64.6 | 80.4 | 71.7 | 73.3 | 73.0 | 64.2 | â€“ |
| LLaVA-OV | 7B | 10B+ / 3.2M | 7290 | 47.3 | 81.7 | 74.8 | 58.8 | 1998 | â€“ | â€“ | 96.6 | â€“ | 78.8 | 81.6 | 65.5 | â€“ |
| *Encoder-free Vision-Language Models:* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Fuyu | 8B | â€“ / â€“ | â€“ | 27.9 | 10.7 | â€“ | 21.4 | â€“ | â€“ | â€“ | â€“ | â€“ | â€“ | â€“ | 64.5 | â€“ |
| Chameleon | 7B | 1.4B+ / 1.8M | 1024 | 25.4 | 31.1 | 30.6 | 8.3 | 170 | â€“ | â€“ | 47.2 | 4.8 | 2.9 | 46.0 | â€“ | 7.0 |
| EVE | 7B | 33M / 1.8M | 2304 | 32.6 | 52.3 | 64.6 | 25.7 | 1628 | 85.0 | 62.6 | 64.9 | 56.8 | 59.1 | 61.0 | â€“ | 398 |
| SOLO | 8B | 43.7M / 2M | 1024 | â€“ | â€“ | 64.4 | â€“ | 1260 | â€“ | â€“ | 73.3 | â€“ | â€“ | 61.4 | â€“ | â€“ |
| Mono-InternVL | 1.8B | 1.3B / 7M | 6400 | 33.7 | 65.5 | 67.4 | 40.1 | 1875 | â€“ | 59.5 | 93.6 | 72.6 | 73.7 | 68.6 | â€“ | 767 |
| Emu3 | 8B | â€“ / â€“ | 16K | 31.6 | 58.5 | 68.2 | 37.2 | â€“ | 85.2 | 60.3 | 89.2 | 64.7 | 68.6 | 70.0 | 57.4 | 687 |
| **EVEv2.0** | 7B | 92M / 7.3M | 2500 | **39.3** | **66.3** | **71.4** | **45.0** | 1709 | **87.6** | **62.9** | **96.2** | 71.1 | **73.9** | **74.8** | **62.4** | 702 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 2ëŠ” MMMU [94], MMBench-EN [50], SEEDBench-Img [37], MMVet [93], MME [23], POPE [44], GQA [30], ScienceQA-Img [53], TextVQA [60], ChartQA [55], AI2D [32], RealWorldQA [84], OCRBench [51]ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ë¹„ì „-ì–¸ì–´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ê³¼ì˜ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  #A-Paramì€ í™œì„±í™”ëœ ë§¤ê°œë³€ìˆ˜ì˜ ìˆ˜, #DataëŠ” ì‚¬ì „ í›ˆë ¨/ë¯¸ì„¸ ì¡°ì • ë°ì´í„° ì–‘, #Vtokenì€ ìµœëŒ€ ì´ë¯¸ì§€ íŒ¨ì¹˜ í† í° ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. MMEì˜ ê²½ìš° ì¸ì‹ ë° ì¸ì§€ ì ìˆ˜ë¥¼ í•©ì‚°í–ˆìŠµë‹ˆë‹¤. ìµœê³  ì„±ëŠ¥ì„ ê¸°ë¡í•œ ìƒìœ„ ë‘ ê²°ê³¼ëŠ” êµµì€ ê¸€ì”¨ì²´ì™€ ë°‘ì¤„ë¡œ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 2: Comparison with existing vision-language models on various vision-language benchmarks, including MMMUÂ [94]; MMBenen{}^{\text{en}}start_FLOATSUPERSCRIPT en end_FLOATSUPERSCRIPT: MMBench-ENÂ [50]; SEEDII{}^{\text{I}}start_FLOATSUPERSCRIPT I end_FLOATSUPERSCRIPT: SEEDBench-ImgÂ [37]; MMV: MMVetÂ [93]; MMEÂ [23]; POPEÂ [44]; GQAÂ [30]; SQAII{}^{\text{I}}start_FLOATSUPERSCRIPT I end_FLOATSUPERSCRIPT: ScienceQA-ImgÂ [53]; TVQA: TextVQAÂ [60]; CQA: ChartQAÂ [55]; AI2DÂ [32]; RWQA: RealWorldQAÂ [84]; OCRB: OCRBenchÂ [51]. Note that #A-Param denotes the number of activated parameters; #Data represents the pre-training / fine-tuning data volume; #Vtoken indicates the maximum image patch tokens. For MME, we sum up the perception and cognition scores. The best two results are marked in bold and underline.
> </details>

{{< table-caption >}}
| Exp. | Model | LLM | Stage 1 Data | Stage 1 T.M. | Stage 2 Training Data | Stage 2 Trainable Module | Stage 3 Data | Stage 3 T.M. |
|---|---|---|---|---|---|---|---|---|
| Fig.2 (i) | EVEv1.0 | Vicuna-7B | EVE-cap-16M | PEL | EVE-cap-33M | PEL, LLM | LLaVA-mix-665k | PEL, LLM |
| Fig.5 | EVEv1.0 | Qwen2.5-7B | EVE-recap-10M | PEL | EVE-recap-8/29M | PEL | LLaVA-mix-665k | PEL, LLM |
|  | EVEv1.2 | Qwen2.5-7B | EVE-recap-10M | PEL | EVE-recap-8/29M | PEL, VLayers | LLaVA-mix-665k | PEL, LLM |
|  | EVEv1.5 | Qwen2.5-7B | EVE-recap-10M | PEL | EVE-recap-8/29M | PEL, VLayers | LLaVA-mix-665k | PEL, LLM |
|  | EVEv2.0 | Qwen2.5-7B | EVE-recap-10M | PEL | EVE-recap-8/29M | PEL, VLayers | LLaVA-mix-665k | PEL, LLM |
| Fig.6 | EVEv1.0 | Vicuna-7B | 10M varied data | PEL | 8M same data from Stage 1 | PEL, LLM | LLaVA-mix-665k | PEL, LLM |
| Fig.7 | EVEv1.0 | Vicuna-7B | 10M varied data | PEL | 8M same data from Stage 1 | PEL, LLM | LLaVA-mix-665k | PEL, LLM |
| Fig.8 | EVEv1.0 | Vicuna-7B | EVE-recap-10M | PEL | EVE-recap-8/29M | PEL, LLM | LLaVA-mix-665k | PEL, LLM |
| Fig.2 (ii) | EVEv1.0 | Qwen2-7B | EVE-recap-10M | PEL | EVE-recap-29M | PEL | EVE-recap-48M | PEL, LLM |
|  | EVEv1.2 | Qwen2-7B | EVE-recap-10M | PEL | EVE-recap-29M | PEL, VLayers | EVE-recap-48M | PEL, LLM |
| Tab.2 | EVEv2.0 | Qwen2.5-7B | EVE-recap-10M | PEL | EVE-recap-77M | PEL, VLayers | EVE-multi-task-15M | PEL, LLM |
| Fig.9 | EVEv2.0 | Qwen2.5-7B | EVE-recap-10M | PEL | EVE-recap-77M | PEL, VLayers | EVE-multi-task-15M | PEL, LLM |{{< /table-caption >}}
> ğŸ”¼ í‘œ 3ì€ ë…¼ë¬¸ì˜ ë³¸ë¡ ì— ìˆëŠ” ì‹¤í—˜ ì„¸ë¶€ ì‚¬í•­ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  T.M.ì€ ê° ë‹¨ê³„ì—ì„œ í›ˆë ¨ ê°€ëŠ¥í•œ ëª¨ë“ˆì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. PELì€ íŒ¨ì¹˜ ì„ë² ë”© ê³„ì¸µì„, VLayersëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì— ìƒˆë¡œ ì¶”ê°€ëœ ë¹„ì „ ê³„ì¸µì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. EVE-recap-8/29Mì€ 29M í›ˆë ¨ ë°ì´í„°ì˜ 8M ë¶€ë¶„ì§‘í•©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ í‘œëŠ” ê° ì‹¤í—˜ ì„¤ì •ì—ì„œ ì‚¬ìš©ëœ ì–¸ì–´ ëª¨ë¸, í›ˆë ¨ ë°ì´í„°, í›ˆë ¨ ê°€ëŠ¥í•œ ëª¨ë“ˆ ë° ê° ë‹¨ê³„ì˜ ì„¸ë¶€ ì‚¬í•­ì„ ë³´ì—¬ì£¼ì–´, ê° ì‹¤í—˜ì˜ êµ¬ì„± ìš”ì†Œì™€ ë°ì´í„° ì‚¬ìš© ë°©ì‹ì„ ëª…í™•íˆ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 3: Experiment Details in the main body. Note that T.M. denotes a trainable module in each stage. PEL and VLayers represent patch embedding layers and newly added vision layers in the large language model. EVE-recap-8/29M indicates a subset 8M of 29M training data.
> </details>

{{< table-caption >}}
| Stage | Dataset | #Data |
|---|---|---|
| 2.2 | Cambrian-FL [71], Infinity-Instruct-FL [27],  | 15M |
|  | LVIS-Instruct-FL [77], Sharegpt4v-FL [12], |  |
|  | ALLaVA-laion-FL [11], ALLaVA-vflan-FL [11], |  |
|  | LLaVA-Pretrain-FL [47], DocReason-FL [29], |  |
|  | DocDownstream-FL [29], DocStruct4M-FL [29]. |  |
| 3 | LLaVA-onevision [38], Infinity-MM-Synthesis [27], | 7.3M |
|  | Infinity-MM-Preference [27], Infinity-Instruct-FL [27], |  |
|  | DenseFusion [43], Cambrian-FL [71], Docmatix-FL [36], |  |
|  | LVIS-Instruct-FL [77], BLIP-OCR [17], LLaVA-mix [47]. |  |{{< /table-caption >}}
> ğŸ”¼ í‘œ 4ëŠ” EVEv2.0 ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ 2.2ë‹¨ê³„ì™€ 3ë‹¨ê³„ì˜ ë°ì´í„°ì…‹ ì„¸ë¶€ ì •ë³´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  2.2ë‹¨ê³„ëŠ” ë‹¤ì–‘í•œ ì§ˆì˜ì‘ë‹µ(QA) ë°ì´í„°ì…‹ê³¼ ë‹¤ì¤‘ ëª¨ë“œ ëŒ€í™” ë°ì´í„°ì…‹ì„ í¬í•¨í•˜ë©°, 3ë‹¨ê³„ëŠ” ê³ í’ˆì§ˆì˜ ë‹¤ì¤‘ ëª¨ë“œ ì§€ì‹œ ë°ì´í„°ì…‹ì„ ì¶”ê°€ë¡œ í¬í•¨í•©ë‹ˆë‹¤.  ê° ë°ì´í„°ì…‹ì˜ ì´ë¦„ê³¼ í¬ê¸°ê°€ í‘œì— ëª…ì‹œë˜ì–´ ìˆìœ¼ë©°, ***-FLì€ í•„í„°ë§ëœ í›ˆë ¨ ë°ì´í„°ì…‹ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  ì´ í‘œëŠ” EVEv2.0 ëª¨ë¸ì˜ í›ˆë ¨ ê³¼ì •ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì˜ êµ¬ì„±ê³¼ ê·œëª¨ë¥¼ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 4: Dataset details in Stage 2.2, and 3 for fine-tuning EVEv2.0. Note that ***-FL denotes the filtered training dataset.
> </details>

{{< table-caption >}}
| Configuration | Stage 1 | Stage 2.1 | Stage 2.2 | Stage 3 |
|---|---|---|---|---|
| Maximum Patch Token | 625 | 625-2500 | 2500 | 2500 |
| Optimizer | AdamW | AdamW | AdamW | AdamW |
| Hyperparameters | $\beta_{1}=0.9,\beta_{2}=0.999,eps=1e^{-8}$ | $\beta_{1}=0.9,\beta_{2}=0.999,eps=1e^{-8}$ | $\beta_{1}=0.9,\beta_{2}=0.999,eps=1e^{-8}$ | $\beta_{1}=0.9,\beta_{2}=0.999,eps=1e^{-8}$ |
| Peak learning rate | $2e^{-4}$ | $1e^{-4}$ | $2e^{-5}$ | $1e^{-5}$ |
| LR schedule | cosine decay with warm-up | cosine decay with warm-up | cosine decay with warm-up | cosine decay with warm-up |
| Warm-up steps | 0.03 | 0.03 | 0.03 | 0.03 |
| Weight decay | 0.0 | 0.0 | 0.0 | 0.0 |
| Global batch size | 1024 | 1024 | 512 | 512 |
| Numerical precision | bfloat16 | bfloat16 | bfloat16 | bfloat16 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 5ëŠ” EVEv2.0 ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ë™ì•ˆ 1ë‹¨ê³„, 2ë‹¨ê³„, 3ë‹¨ê³„ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ë‹¨ê³„ì˜ í›ˆë ¨ ì—í­ì€ 1ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  í‘œì—ëŠ” ìµœëŒ€ íŒ¨ì¹˜ í† í° ìˆ˜, ìµœì í™” ì•Œê³ ë¦¬ì¦˜, í•™ìŠµë¥ , í•™ìŠµë¥  ìŠ¤ì¼€ì¤„, ì›œì—… ìŠ¤í…, ê°€ì¤‘ì¹˜ ê°ì‡ , ê¸€ë¡œë²Œ ë°°ì¹˜ í¬ê¸°, ê·¸ë¦¬ê³  ìˆ˜ì¹˜ì  ì •ë°€ë„ ë“±ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ í‘œëŠ” ëª¨ë¸ í›ˆë ¨ì˜ ì„¸ë¶€ì ì¸ ì„¤ì •ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 5: Hyper-parameter configurations in Stage 1-3 for training EVEv2.0. Note that we set the training epoch in each stage as 1.
> </details>

{{< table-caption >}}
| User | EVEv2.0 |
|---|---| 
| ![MAR.jpg](https://arxiv.org/html/2502.06788/extracted/6191103/figures/examples/MAR.jpg) | Cornell University We gratefully acknowledge support member. arXiv:2406.11838 Computer Science > Computer Vision and Pattern Recognition Submitted on 17 Jun 2024 (v1), last revised 1 Nov 2024 (this version, v3) Autoregressive Image Generation without Vector Quantization Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, Kaiming He Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: this https URL Comments: Neurips 2024 (Spotlight); Code: this https URL Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2406.11838 [cs.CV] (or arXiv:2406.11838v3 [cs.CV] for this version) Submission history From: Tianhong Li [view email] [v1] Mon, 17 Jun 2024 17:59:58 UTC (5,459 KB) [v2] Sun, 28 Jul 2024 05:30:08 UTC (5,447 KB) [v3] Fri, 1 Nov 2024 14:45:36 UTC (8,971 KB) |{{< /table-caption >}}
> ğŸ”¼ í‘œ 6ì€ EVEv2.0 ëª¨ë¸ì´ ì›¹í˜ì´ì§€ì˜ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ì¸ì‹í•˜ê³  ì´í•´í•˜ëŠ” ëŠ¥ë ¥, íŠ¹íˆ ê´‘í•™ ë¬¸ì ì¸ì‹(OCR) ëŠ¥ë ¥ì´ ë›°ì–´ë‚¨ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œë“¤ì„ ì œì‹œí•©ë‹ˆë‹¤.  ë‹¨ìˆœíˆ OCR ì„±ëŠ¥ë§Œì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì›¹í˜ì´ì§€ì˜ ë ˆì´ì•„ì›ƒê³¼ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³ ,  í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì—¬ ë³´ì—¬ì£¼ëŠ” EVEv2.0 ì˜ ìš°ìˆ˜ì„±ì„ ë‹¤ì–‘í•œ ì˜ˆì‹œë¥¼ í†µí•´ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì´ëŠ” ë‹¨ìˆœí•œ ë¬¸ì ì¸ì‹ì„ ë„˜ì–´, ë³µì¡í•œ ì›¹í˜ì´ì§€ êµ¬ì¡°ê¹Œì§€ ì´í•´í•˜ëŠ” EVEv2.0 ì˜ ê³ ê¸‰ ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ì„ ê°•ì¡°í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 6: EVEv2.0 exhibits excellent OCR recognition capabilities in understanding Webpage.
> </details>

{{< table-caption >}}
| User | Please describe the image in detail. |
|---|---| 
| **EVEv2.0** | The image is a promotional poster for â€Neanderthal,â€ which appears to be a documentary or film about Neanderthals. The title â€Neanderthalâ€ is prominently displayed in large, bold yellow letters across the center of the image. Below the title, there is additional text that reads â€Years 7 and 8 Welcome of the Day,â€ suggesting that this event might be targeted towards students in those grade levels. The background of the poster features a close-up photograph of a man with a Neanderthal-like appearance. He has a long beard, a prominent forehead, and a strong jawline, which are characteristic features often associated with Neanderthals. The man is holding a wooden stick or pole, which adds to the prehistoric theme of the image. The background behind him is blurred, drawing focus to his face and upper body. At the bottom right corner of the poster, there is more text indicating the date of the event: â€Friday 23 January 2015.â€ This provides a specific time frame for when the event or release of the documentary/film would have taken place. Overall, the poster uses visual elements such as the Neanderthal figure and the title to convey information about the subject matter, while also providing essential details like the target audience and the date of the event. The design effectively captures attention and communicates key information to viewers. |
|  | ![ocr_beijing.jpg](https://arxiv.org/html/2502.06788/extracted/6191103/figures/examples/ocr_beijing.jpg) |{{< /table-caption >}}
> ğŸ”¼ í‘œ 7ì€ EVEv2.0 ëª¨ë¸ì´ PowerPoint ë¬¸ì„œ ë‚´ì˜ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì¸ì‹í•˜ê³  ì´í•´í•˜ëŠ” ëŠ¥ë ¥, ì¦‰ ê´‘í•™ ë¬¸ì ì¸ì‹(OCR) ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œë“¤ì„ ì œì‹œí•©ë‹ˆë‹¤.  PowerPoint ìŠ¬ë¼ì´ë“œ ë‚´ì˜ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ìš”ì†Œë“¤ì— ëŒ€í•´ EVEv2.0ì´ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ë¬¸ìë¥¼ ì¸ì‹í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŠ¬ë¼ì´ë“œì˜ ë‚´ìš©ì„ ì´í•´í•˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” ì—¬ëŸ¬ ì‚¬ë¡€ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  ì´ë¥¼ í†µí•´ EVEv2.0ì˜ ê°•ë ¥í•œ OCR ê¸°ëŠ¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 7: EVEv2.0 exhibits excellent OCR recognition capabilities in understanding PowerPoint.
> </details>

{{< table-caption >}}
| User | Please describe the image in detail. |
|---|---| 
| **EVEv2.0** | In the image, there is a man standing in front of a bathroom mirror, taking a selfie with his cell phone. He is wearing a purple shirt and leopard print pants. The man appears to be smiling as he captures the photo. In the background, there is a potted plant with white flowers placed on the counter near the sink. The sink is white and has a modern design. There is also a poster hanging on the wall next to the mirror. The poster features a silhouette of a personâ€™s head with several circles on it. Below the silhouette, there is text that reads â€COME SETS,â€ followed by some additional information about Friday night events at Codame.com. The bathroom appears to be well-lit, likely due to natural light coming from a window or another source not visible in the image. The overall atmosphere seems casual and relaxed, with the man enjoying his time in the bathroom while capturing a moment through his phone camera. |
|  | ![background](https://arxiv.org/html/2502.06788/figures/examples/background.jpg) |{{< /table-caption >}}
> ğŸ”¼ í‘œ 8ì€ EVEv2.0 ëª¨ë¸ì´ í¬ìŠ¤í„°ì˜ ë‚´ìš©ì„ ì–¼ë§ˆë‚˜ ì˜ ì´í•´í•˜ëŠ”ì§€, ì¦‰ ì‹œê°ì  ì¸ì‹ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.  í¬ìŠ¤í„°ì˜ ì´ë¯¸ì§€ë¥¼ ë³´ê³ , EVEv2.0ì´ í¬ìŠ¤í„°ì˜ ì œëª©, ë‚´ìš©, ì´ë¯¸ì§€ ì† ì¸ë¬¼ì˜ íŠ¹ì§•, ë‚ ì§œ ë“±ì„ ì •í™•í•˜ê²Œ íŒŒì•…í•˜ê³  ì„¤ëª…í•˜ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì´ëŠ” ë‹¨ìˆœíˆ ì´ë¯¸ì§€ë¥¼ ì¸ì‹í•˜ëŠ” ê²ƒì„ ë„˜ì–´ì„œ, ì´ë¯¸ì§€ ì•ˆì˜ ë¬¸ë§¥ê³¼ ì˜ë¯¸ê¹Œì§€ ì´í•´í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  ë”°ë¼ì„œ, ì´ í‘œëŠ” EVEv2.0ì˜ ê°•ë ¥í•œ ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ì¤‘ìš”í•œ ê·¼ê±°ê°€ ë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 8: EVEv2.0 exhibits excellent visual recognition capabilities in understanding Poster.
> </details>

{{< table-caption >}}
| User | Description |
|---|---| 
| **EVEv2.0** | ![mac](https://arxiv.org/html/2502.06788/figures/examples/mac.jpg) The image depicts a well-organized workspace featuring a wooden desk with various items on it. At the center of the desk is a closed silver laptop computer, which has an Apple logo on its lid. This laptop is accompanied by a white keyboard placed directly in front of it, suggesting that the user might prefer using an external keyboard for typing. To the right of the laptop, there is a white wireless mouse, indicating that the user also utilizes a mouse for navigation and other tasks. On the left side of the desk, there is a glass containing a beverage, possibly water or another drink, which is positioned near the edge of the desk. In addition to these items, there are two cylindrical objects resembling speakers, one on each side of the desk. These speakers likely provide audio output for the user while working or listening to music. Behind the desk, there is a bookshelf filled with numerous books, suggesting that the user may have a keen interest in reading or research. The presence of multiple books indicates that this workspace could be used for studying, work, or leisure activities involving reading. Overall, the image portrays a functional and organized workspace designed for productivity and comfort, with essential computing equipment and additional accessories such as speakers and a beverage.|{{< /table-caption >}}
> ğŸ”¼ í‘œ 9ëŠ” EVEv2.0 ëª¨ë¸ì´ ì‹¤ì œ ì„¸ìƒì˜ ë³µì¡í•œ ì‹œê° ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì´í•´í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì‚¬ì§„ ì†ì˜ ì—¬ëŸ¬ ë¬¼ì²´ë“¤ê³¼ ë°°ê²½, ê·¸ë¦¬ê³  ê·¸ë“¤ì˜ ê´€ê³„ë¥¼ ì •í™•í•˜ê²Œ ì¸ì‹í•˜ê³  ì„¤ëª…í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.  ë‹¨ìˆœíˆ ë¬¼ì²´ë¥¼ ì¸ì‹í•˜ëŠ” ê²ƒì„ ë„˜ì–´ì„œ, ì‚¬ì§„ ì†ì˜ ìƒí™©ê³¼ ë¶„ìœ„ê¸°ê¹Œì§€ë„ ì´í•´í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë¬˜ì‚¬í•˜ëŠ” EVEv2.0 ì˜ ë›°ì–´ë‚œ ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 9: EVEv2.0 exhibits excellent recognition capabilities in the real-world scene.
> </details>

{{< table-caption >}}
| User | EVEv2.0 |
|---|---|---|
| <img src="https://arxiv.org/html/2502.06788/figures/examples/som.png" width="628" height="573"> | In the image, there are three objects marked with numbers: 9, 12, and 15.<br>Object 9: A laptop computer is placed on the desk. It appears to be open and ready for use.<br>Object 12: A lamp is situated near the desk, providing light for the workspace. It has a sleek design and complements the overall aesthetic of the room.<br>Object 15: A potted plant is located on the desk, adding a touch of greenery and life to the space. It contributes to creating a more pleasant and inviting atmosphere in the room. |{{< /table-caption >}}
> ğŸ”¼ í‘œ 10ì€ EVEv2.0 ëª¨ë¸ì´ ì‹¤ì œ ì„¸ìƒì˜ ì‚¬ì§„ì„ ì–¼ë§ˆë‚˜ ì˜ ì´í•´í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì‚¬ì§„ ì† ì‚¬ë¬¼ë“¤ì„ ì •í™•í•˜ê²Œ ì¸ì‹í•˜ê³ , ê° ì‚¬ë¬¼ì˜ íŠ¹ì§•ê³¼ ìƒí˜¸ ê´€ê³„ë¥¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•˜ì—¬, ì´ë¯¸ì§€ì— ëŒ€í•œ í’ë¶€í•œ ì´í•´ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì´ë¥¼ í†µí•´ EVEv2.0 ëª¨ë¸ì˜ ë›°ì–´ë‚œ ì‹œê°ì  ì¸ì‹ ëŠ¥ë ¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 10: EVEv2.0 exhibits excellent recognition capabilities in the real-world scene.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}