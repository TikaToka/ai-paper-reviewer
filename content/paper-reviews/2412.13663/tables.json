[{"content": "|       | Model      | IR (DPR) BEIR | IR (DPR) MLDR<sub>OOD</sub> | IR (DPR) MLDR<sub>ID</sub> | IR (ColBERT) BEIR | IR (ColBERT) MLDR<sub>OOD</sub> | NLU | Code GLUE | Code CSN | Code SQA |\n| :---- | :--------- | :---------------: | :-----------------------: | :---------------------: | :----------------: | :--------------------------: | :-: | :-------: | :------: | :------: |\n| Base  | BERT       | 38.9             | 23.9                      | 32.2                    | 49.0              | 28.1                         | 84.7 | 41.2      | 59.5     |        |\n|       | RoBERTa    | 37.7             | 22.9                      | 32.8                    | 48.7              | 28.2                         | 86.4 | 44.3      | 59.6     |        |\n|       | DeBERTaV3  | 20.2             | 5.4                       | 13.4                    | 47.1              | 21.9                         | 88.1 | 17.5      | 18.6     |        |\n|       | NomicBERT  | 41.0             | 26.7                      | 30.3                    | 49.9              | 61.3                         | 84.0 | 41.6      | 61.4     |        |\n|       | GTE-en-MLM | 41.4             | **34.3**                 | 44.4                    | 48.2              | 69.3                         | 85.6 | 44.9      | 71.4     |        |\n|       | ModernBERT | **41.6**         | 27.4                      | **51.3**                | **80.2**           | **88.4**                     |     | **56.4**  | **73.6** |        |\n| Large | BERT       | 38.9             | 23.3                      | 31.7                    | 49.5              | 28.5                         | 85.2 | 41.6      | 60.8     |        |\n|       | RoBERTa    | 41.4             | 22.6                      | 36.1                    | 49.8              | 28.8                         | 88.9 | 47.3      | 68.1     |        |\n|       | DeBERTaV3  | 25.6             | 7.1                       | 19.2                    | 46.7              | 23.0                         | **91.4** | 21.2      | 19.7     |        |\n|       | GTE-en-MLM | 42.5             | **36.4**                 | 48.9                    | 50.7              | 71.3                         | 87.6 | 40.5      | 66.9     |        |\n|       | ModernBERT | **44.0**         | 34.3                      | **52.4**                | **80.4**           | 90.4                         |     | **59.5**  | **83.9** |        |", "caption": "Table 1: Results for all models across an overview of all tasks. CSN refers to CodeSearchNet and SQA to StackQA. MLDRID refers to in-domain (fine-tuned on the training set) evaluation, and MLDROOD to out-of-domain.", "description": "\ud45c 1\uc740 \ub2e4\uc591\ud55c \ud558\ub958 \uc791\uc5c5\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. BEIR, MLDROOD, MLDRID\ub294 \uac80\uc0c9 \uc131\ub2a5\uc744, GLUE\ub294 \uc790\uc5f0\uc5b4 \uc774\ud574 \ub2a5\ub825\uc744, CSN\uc740 \ucf54\ub4dc \uac80\uc0c9 \uc131\ub2a5\uc744, SQA\ub294 Stack Overflow \uc9c8\ubb38 \ub2f5\ubcc0 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \uc9c0\ud45c\uc785\ub2c8\ub2e4. MLDRID\ub294 \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744, MLDROOD\ub294 \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uacfc \ub2e4\ub978 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud3c9\uac00\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "3 Downstream Evaluation"}, {"content": "| MLDR<sub>OOD</sub> |\n", "caption": "Table 2: Memory (max batch size, BS) and Inference (in thousands of tokens per second) efficiency results on an NVIDIA RTX 4090, averaged over 10 runs. Dashes indicate unsupported configurations.", "description": "\ud45c 2\ub294 NVIDIA RTX 4090\uc5d0\uc11c 10\ubc88\uc758 \uc2e4\ud589\uc5d0 \uac78\uccd0 \ud3c9\uade0\ub0b8 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9(\ucd5c\ub300 \ubc30\uce58 \ud06c\uae30, BS) \ubc0f \ucd94\ub860 \uc18d\ub3c4(\ucd08\ub2f9 \uc218\ucc9c \ud1a0\ud070) \ud6a8\uc728\uc131 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc9c0\uc6d0\ub418\uc9c0 \uc54a\ub294 \uad6c\uc131\uc5d0 \ub300\ud574\uc11c\ub294 \ub300\uc2dc(-)\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uba54\ubaa8\ub9ac \ud6a8\uc728\uc131\uacfc \ucc98\ub9ac \uc18d\ub3c4\ub97c \ube44\uad50\ud558\uc5ec ModernBERT\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubc30\uce58 \ud06c\uae30\uac00 \ud074\uc218\ub85d \ub354 \ub9ce\uc740 \ub370\uc774\ud130\ub97c \ub3d9\uc2dc\uc5d0 \ucc98\ub9ac\ud560 \uc218 \uc788\uace0, \ucd08\ub2f9 \ud1a0\ud070 \uc218\uac00 \ud074\uc218\ub85d \ucd94\ub860\uc774 \ub354 \ube60\ub985\ub2c8\ub2e4.", "section": "4 \ud6a8\uc728\uc131"}, {"content": "| MLDR<sub>ID</sub>|\n|---|---|", "caption": "Table 3: ModernBERT training settings. Dropout and below are shared across all phases.", "description": "\ud45c 3\uc740 ModernBERT \ubaa8\ub378\uc758 \ud559\uc2b5 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Dropout \uc774\ud6c4\uc758 \uc124\uc815\ub4e4\uc740 \ubaa8\ub4e0 \ud559\uc2b5 \ub2e8\uacc4\uc5d0\uc11c \uacf5\uc720\ub429\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ud1a0\ud070 \uc218, \ucd5c\ub300 \uc2dc\ud000\uc2a4 \uae38\uc774, \ubc30\uce58 \ud06c\uae30, \uc6cc\ubc0d\uc5c5 \ub2e8\uacc4 \ud1a0\ud070 \uc218, \ub9c8\uc774\ud06c\ub85c \ubc30\uce58 \ud06c\uae30, \ud559\uc2b5\ub960, \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904, \uac00\uc911\uce58 \uac10\uc1e0, \ucd1d \ud559\uc2b5 \uc2dc\uac04, \ubaa8\ub378 \ucd08\uae30\ud654 \ubc29\ubc95 \ub4f1\uc758 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788 \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904\uc740 ModernBERT\uc758 \uc131\ub2a5\uc5d0 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud558\ub294 \uac83\uc73c\ub85c \ubcf4\uc774\uba70, trapezoidal \uc2a4\ucf00\uc904\uacfc 1 sqrt \uac10\uc1e0\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ubaa8\ub378 \ucd08\uae30\ud654\ub294 Megatron \ubc29\uc2dd \ub610\ub294 \uae30\uc874 ModernBERT-base \ubaa8\ub378\uc744 \uc774\uc6a9\ud569\ub2c8\ub2e4.", "section": "2 Methods"}, {"content": "| GLUE |\n|---|---|", "caption": "Table 4: ModernBERT model design", "description": "\ud45c 4\ub294 \ub17c\ubb38\uc758 2.1.1\uc808 \"Modern Transformer\"\uc5d0\uc11c ModernBERT \ubaa8\ub378\uc758 \uc124\uacc4\uc5d0 \ub300\ud55c \uc138\ubd80 \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 ModernBERT \uae30\ubcf8 \ubc0f \ub300\ud615 \ubaa8\ub378 \ubaa8\ub450\uc5d0 \ub300\ud55c \uc544\ud0a4\ud14d\ucc98 \uad6c\uc131 \uc694\uc18c(\uc608: \ub808\uc774\uc5b4 \uc218, \uc740\ub2c9 \ud06c\uae30, \ud65c\uc131\ud654 \ud568\uc218, \uc815\uaddc\ud654 \ubc29\ubc95 \ub4f1)\uc758 \uc0ac\uc591\uc774 \uc790\uc138\ud788 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 ModernBERT \ubaa8\ub378\uc758 \uc8fc\uc694 \ud2b9\uc9d5\uacfc \ub2e4\ub978 \ubaa8\ub378\uacfc\uc758 \ucc28\uc774\uc810\uc744 \uc774\ud574\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "2 Methods"}, {"content": "| CSN |\n|---|---|", "caption": "Table 5: GLUE\u00a0Wang et\u00a0al. (2018) dev set scores. \u03b1 taken from Table 8 of\u00a0Liu et\u00a0al. (2019a), \u03b2 taken from Table S3 of\u00a0Portes et\u00a0al. (2023), \u03b3 from Table 2 of\u00a0Nussbaum et\u00a0al. (2024), \u03b4 from Table 21 of\u00a0Zhang et\u00a0al. (2024), \u03f5 from Table 2 of\u00a0Qiang et\u00a0al. (2024) and \u03b6 from Table 3 of\u00a0He et\u00a0al. (2023)", "description": "\ud45c 5\ub294 GLUE(General Language Understanding Evaluation) \ubca4\uce58\ub9c8\ud06c\uc758 \ud558\uc704 \uc791\uc5c5\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \uc778\ucf54\ub354 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  GLUE\ub294 \uc790\uc5f0\uc5b4 \uc774\ud574(NLU) \uc791\uc5c5\uc758 \ubc94\uc704\ub97c \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\ub294 \ub2e4\uc911 \uc791\uc5c5 \ubca4\uce58\ub9c8\ud06c\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 ModernBERT \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uae30\uc874\uc758 \ub2e4\ub978 \uc778\ucf54\ub354 \ubaa8\ub378(BERT, RoBERTa, DeBERTaV3, MosaicBERT, NomicBERT, GTE-en-MLM)\uacfc \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc740 \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5(\uc608: \ubb38\uc7a5 \ubd84\ub958, \uc790\uc5f0\uc5b4 \ucd94\ub860, \ubb38\uc7a5 \uc9dd \ube44\uad50)\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4 \ub610\ub294 F1 \uc810\uc218\ub85c \uce21\uc815\ub429\ub2c8\ub2e4. \ud45c\uc5d0 \uc81c\uc2dc\ub41c \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc758 \uacb0\uacfc\ub294 Liu et al.(2019a), Portes et al.(2023), Nussbaum et al.(2024), Zhang et al.(2024), Qiang et al.(2024), He et al.(2023)\uc758 \uc5f0\uad6c\uc5d0\uc11c \uac00\uc838\uc628 \uac83\uc785\ub2c8\ub2e4.", "section": "3.1 \ud3c9\uac00 \uc124\uc815"}, {"content": "| SQA |\n|---|---|", "caption": "Table 6: Fine-tuning hyperparameters for ModernBERT on GLUE tasks. LR: Learning Rate, WD: Weight Decay, Ep: Epochs.", "description": "\ud45c 6\uc740 GLUE \uc791\uc5c5\uc5d0 \ub300\ud55c ModernBERT\uc758 \ubbf8\uc138 \uc870\uc815 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LR\uc740 \ud559\uc2b5\ub960, WD\ub294 \uac00\uc911\uce58 \uac10\uc1e0, Ep\ub294 \uc5d0\ud3ec\ud06c\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 ModernBERT \ubaa8\ub378\uc744 GLUE \ubca4\uce58\ub9c8\ud06c\uc758 \ub2e4\uc591\ud55c \uc790\uc5f0\uc5b4 \uc774\ud574 \uc791\uc5c5\uc5d0 \uc801\uc6a9\ud560 \ub54c \uc0ac\uc6a9\ub41c \ud559\uc2b5\ub960, \uac00\uc911\uce58 \uac10\uc1e0 \ubc0f \uc5d0\ud3ec\ud06c \uc218\ub97c \uc790\uc138\ud788 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \ucd5c\uc801\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \ucc3e\uae30 \uc704\ud574 \uc2e4\ud5d8\uc801\uc73c\ub85c \uacb0\uc815\ub41c \uac12\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2.2 Training"}, {"content": "|   |   |   | Short |   |   | Long |   |   |\n|---|---|---|---|---|---|---|---|---|\n|   |   |   | BS | Fixed | Variable | BS | Fixed | Variable |\n|---|---|---|---|---|---|---|---|---|\n| Base | BERT | 110M | 1096 | **180.4** | 90.2 | \u2013 | \u2013 | \u2013 |\n|   | RoBERTa | 125M | 664 | 179.9 | 89.9 | \u2013 | \u2013 | \u2013 |\n|   | DeBERTaV3 | 183M | 236 | 70.2 | 35.1 | \u2013 | \u2013 | \u2013 |\n|   | NomicBERT | 137M | 588 | 117.1 | 58.5 | 36 | 46.1 | 23.1 |\n|   | GTE-en-MLM | 137M | 640 | 123.7 | 61.8 | 38 | 46.8 | 23.4 |\n|   | GTE-en-MLM<sub>xformers</sub> | 137M | 640 | 122.5 | 128.6 | 38 | 47.5 | 67.3 |\n|   | ModernBERT | 149M | **1604** | 148.1 | **147.3** | **98** | **123.7** | **133.8** |\n| Large | BERT | 330M | **792** | **54.4** | 27.2 | \u2013 | \u2013 | \u2013 |\n|   | RoBERTa | 355M | 460 | 42.0 | 21.0 | \u2013 | \u2013 | \u2013 |\n|   | DeBERTaV3 | 434M | 134 | 24.6 | 12.3 | \u2013 | \u2013 | \u2013 |\n|   | GTE-en-MLM | 435M | 472 | 38.7 | 19.3 | 28 | 16.2 | 8.1 |\n|   | GTE-en-MLM<sub>xformers</sub> | 435M | 472 | 38.5 | 40.4 | 28 | 16.5 | 22.8 |\n|   | ModernBERT | 395M | 770 | 52.3 | **52.9** | **48** | **46.8** | **49.8** |", "caption": "Table 7: BEIR\u00a0Thakur et\u00a0al. (2021) nDCG@10 scores for single-vector retrieval models.", "description": "\ud45c 7\uc740 BEIR \ubca4\uce58\ub9c8\ud06c(Thakur et al., 2021)\uc758 \ub2e8\uc77c \ubca1\ud130 \uac80\uc0c9 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  BEIR\uc740 \ub2e4\uc591\ud55c \uc9c8\ubb38\uc751\ub2f5 \ub370\uc774\ud130\uc14b\uc744 \ud3ec\ud568\ud558\ub294 \uc885\ud569\uc801\uc778 \uc815\ubcf4 \uac80\uc0c9 \ubca4\uce58\ub9c8\ud06c\uc785\ub2c8\ub2e4. \ud45c\ub294 \ub2e4\uc591\ud55c \uc778\ucf54\ub354 \ubaa8\ub378(BERT, RoBERTa, DeBERTa-v3, NomicBERT, GTE-en-MLM, ModernBERT)\uc5d0 \ub300\ud55c nDCG@10 \uc810\uc218\ub97c \ubcf4\uc5ec\uc8fc\uba70, \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc5ec\ub7ec \uc815\ubcf4 \uac80\uc0c9 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ube44\uad50\ud569\ub2c8\ub2e4. nDCG@10\uc740 \uc0c1\uc704 10\uac1c \uac80\uc0c9 \uacb0\uacfc\uc758 \uc21c\uc704 \uc815\ud655\ub3c4\ub97c \uce21\uc815\ud558\ub294 \uc9c0\ud45c\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 ModernBERT \ubaa8\ub378\uc758 \ub2e8\uc77c \ubca1\ud130 \uac80\uc0c9 \uc131\ub2a5\uc744 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud558\uc5ec ModernBERT\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud574 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "3.1.2 \ud14d\uc2a4\ud2b8 \uac80\uc0c9"}, {"content": "|                | Pretraining Phase       |                      | Context Extension: Phase One |                        | Context Extension: Phase Two |                        |\n|----------------|------------------------|----------------------|-----------------------------|-------------------------|-----------------------------|------------------------|\n|                | Base                    | Large                 | Base                         | Large                     | Base                         | Large                     |\n| Training Tokens | 1.719 trillion          |                      | 250 billion                 |                        | 50 billion                  |                        |\n| Max Sequence Length | 1,024                  |                      | 8,192                        |                        | 8,192                        |                        |\n| Batch Size      | 4,608                  | 4,928                 | 72                          | 77                        | 72                          | 78                        |\n| Warmup (tokens) | 50 billion              | 10 billion             | -                            | -                         | -                            | -                         |\n| Microbatch Size | 96                     | 56                    | 12                          | 7                         | 12                          | 6                         |\n| Learning Rate   | 8e-4                   | 5e-4, 5e-5             | 3e-4                         | 5e-5                      | 3e-4                         | 5e-5                      |\n| Schedule        | Trapezoidal             |                      | -                            | -                         | 1-sqrt                      |                        |\n| Warmup (tokens) | 3 billion               | 2 billion              | -                            | -                         | -                            | -                         |\n| Decay (tokens)  | -                       | -                      | -                            | -                         | 50 billion                 |                        |\n| Weight Decay    | 1e-5                   | 1e-5, 1e-6             | 1e-5                         | 1e-6                      | 1e-5                         | 1e-6                      |\n| Total Time (hours)| 194.2                  | 425.3                 | 39.9                         | 80.7                      | 11.5                         | 21.7                      |\n| Training Time (hours) | 191.1                  | 420.4                 | 36.3                         | 75.1                      | 7.5                          | 15.3                      |\n| Model Initialization | Megatron                | From Base              | -                            | -                         | -                            | -                         |\n| Dropout (attn out)| 0.1                     |                      |                      |                        |                      |                        |\n| Dropout (all other layers) | 0.0                     |                      |                      |                        |                      |                        |\n| Optimizer       | StableAdamW             |                      |                      |                        |                      |                        |\n| Betas           | (0.90, 0.98)           |                      |                      |                        |                      |                        |\n| Epsilon         | 1e-06                  |                      |                      |                        |                      |                        |\n| Training Hardware | 8x H100                 |                      |                      |                        |                      |                        |\n| Training Strategy | Distributed DataParallel |                      |                      |                        |                      |                        |\n| Software Libraries | PyTorch 2.4.0, Cuda 12.4.0, Composer 0.24.1, Flash Attention 2.6.3, FA3 commit 32792d3 |                      |                      |                        |                      |                        |", "caption": "Table 8: BEIR\u00a0Thakur et\u00a0al. (2021) nDCG@10 scores for multi-vector retrieval models.", "description": "\ud45c 8\uc740 BEIR (Thakur et al., 2021) \ubca4\uce58\ub9c8\ud06c\uc758 \ub2e4\uc911 \ubca1\ud130 \uac80\uc0c9 \uc791\uc5c5\uc5d0\uc11c \ub2e4\uc591\ud55c \uc778\ucf54\ub354 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 nDCG@10 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc740 \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ud3c9\uac00\ub418\uc5c8\uc73c\uba70,  \ub2e4\uc911 \ubca1\ud130 \uac80\uc0c9 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc9c8\uc758\uc640 \ubb38\uc11c \uac04\uc758 \uc720\uc0ac\ub3c4\ub97c \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 ModernBERT \ubaa8\ub378\uc774 \ub2e4\uc911 \ubca1\ud130 \uac80\uc0c9 \uc791\uc5c5\uc5d0\uc11c\ub3c4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc8fc\uc694 \uacb0\uacfc \uc911 \ud558\ub098\uc785\ub2c8\ub2e4.", "section": "3.1.2 \ud14d\uc2a4\ud2b8 \uac80\uc0c9"}, {"content": "| Feature | Base | Large |\n|---|---|---|\n| Vocabulary | 50,368 | 50,368 |\n| Unused Tokens | 83 | 83 |\n| Layers | 22 | 28 |\n| Hidden Size | 768 | 1024 |\n| Transformer Block | Pre-Norm | Pre-Norm |\n| Activation Function | GeLU | GeLU |\n| Linear Bias | False | False |\n| Attention | Multi-head | Multi-head |\n| Attention Heads | 12 | 16 |\n| Global Attention | Every three layers | Every three layers |\n| Local Attention Window | 128 | 128 |\n| Intermediate Size | 1,152 | 2,624 |\n| GLU Expansion | 2,304 | 5,248 |\n| Normalization | LayerNorm | LayerNorm |\n| Norm Epsilon | 1e-5 | 1e-5 |\n| Norm Bias | False | False |\n| RoPE theta | 160,000 | 160,000 |\n| Local Attn RoPE theta | 10,000 | 10,000 |", "caption": "Table 9: Learning rate used for reported results on BEIR\u00a0Thakur et\u00a0al. (2021) for both single and multi vector retrieval", "description": "\ubcf8 \ud45c\ub294 BEIR \ub370\uc774\ud130\uc14b(Thakur et al., 2021)\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e8\uc77c \ubca1\ud130 \uac80\uc0c9\uacfc \ub2e4\uc911 \ubca1\ud130 \uac80\uc0c9 \ubaa8\ub450\uc5d0 \ub300\ud574 \ubcf4\uace0\ub41c \uacb0\uacfc\uc5d0 \uc0ac\uc6a9\ub41c \ud559\uc2b5\ub960\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e8\uc77c \ubca1\ud130 \uac80\uc0c9(DPR)\uacfc \ub2e4\uc911 \ubca1\ud130 \uac80\uc0c9(ColBERT)\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \ubaa8\ub450 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \ubaa8\ub378 \uc720\ud615(\uae30\ubcf8 \ubc0f \ub300\uaddc\ubaa8)\ubcc4\ub85c \ucd5c\uc801\uc758 \ud559\uc2b5\ub960\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.1.2 \ud14d\uc2a4\ud2b8 \uac80\uc0c9"}, {"content": "|       | Model                     | Params | Seq. | CoLA | SST-2 | MRPC | STS-B | QQP  | MNLI | QNLI | RTE  |\n| :---- | :------------------------- | :----- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Base  | BERT<sup>\u03b2</sup>           | 110M   | 512   | 59.0 | 93.1 | 89.5 | 89.4 | 91.4 | 85.4 | 91.6 | 78.2 |\n|       | RoBERTa<sup>\u03b1</sup>         | 125M   | 512   | 63.6 | 94.8 | 90.2 | 91.2 | 91.9 | 87.6 | 92.8 | 78.7 |\n|       | DeBERTav3<sup>\u03f5</sup>       | 183M   | 512   | **69.2** | 95.6 | 89.5 | 91.6 | **92.4** | **90.0** | **94.0** | 83.8 |\n|       | MosaicBERT-128<sup>\u03b2</sup> | 137M   | 128   | 58.2 | 93.5 | 89.0 | 90.3 | 92.0 | 85.6 | 91.4 | 83.0 |\n|       | NomicBERT-2048<sup>\u03b3</sup>   | 137M   | 2048  | 50.0 | 93.0 | 88.0 | 90.0 | 92.0 | 86.0 | 92.0 | 82.0 |\n|       | GTE-en-MLM<sup>\u03b4</sup>     | 137M   | 8192  | 57.0 | 93.4 | 92.1 | 90.2 | 88.8 | 86.7 | 91.9 | 84.8 |\n|       | ModernBERT                 | 149M   | 8192  | **96.0** | **92.2** | **91.8** | 92.1 | 89.1 | 93.9 | **87.4** | 65.1 |\n| Large | BERT<sup>\u03b2</sup>           | 330M   | 512   | 56.2 | 93.3 | 87.8 | 90.6 | 90.9 | 86.3 | 92.8 | 83.8 |\n|       | RoBERTa<sup>\u03b1</sup>         | 355M   | 512   | 68.0 | 96.4 | 90.9 | 92.4 | 92.2 | 90.2 | 94.7 | 86.6 |\n|       | DeBERTav3<sup>\u03b6</sup>       | 434M   | 512   | **75.3** | 96.9 | 92.2 | **93.3** | **91.8** | **96.0** | **92.7** | 71.4 |\n|       | GTE-en-MLM<sup>\u03b4</sup>     | 434M   | 8192  | 60.4 | 95.1 | **93.5** | 89.2 | 89.2 | 93.9 | 88.1 | 71.4 |\n|       | ModernBERT                 | 395M   | 8192  | **97.1** | 91.7 | 92.8 | 92.7 | 90.8 | 95.2 | 92.1 | 71.4 |", "caption": "Table 10: Token statistics for the synthetic datasets used in efficiency evaluations.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 \ud6a8\uc728\uc131 \ud3c9\uac00\uc5d0 \uc0ac\uc6a9\ub41c \ud569\uc131 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ud1a0\ud070 \ud1b5\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'Fixed'\ub294 \uace0\uc815 \uae38\uc774 \ud1a0\ud070 \uc2dc\ud000\uc2a4\ub97c, 'Variable'\uc740 \uac00\ubcc0 \uae38\uc774 \ud1a0\ud070 \uc2dc\ud000\uc2a4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b\uc758 \ucd1d \ud1a0\ud070 \uc218, \ud45c\uc900\ud3b8\ucc28, \ud3c9\uade0 \uae38\uc774, \uac00\uc7a5 \uae34 \uc2dc\ud000\uc2a4 \uae38\uc774, \uac00\uc7a5 \uc9e7\uc740 \uc2dc\ud000\uc2a4 \uae38\uc774, \uc2dc\ud000\uc2a4 \uc218\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc5b4, \ud6a8\uc728\uc131 \ud3c9\uac00 \uc2dc \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc758 \ud2b9\uc9d5\uc744 \uc0c1\uc138\ud558\uac8c \ud30c\uc545\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.  'Short' \uc640 'Long' \uc740 \uac01\uac01 \uc9e7\uc740 \ucee8\ud14d\uc2a4\ud2b8\uc640 \uae34 \ucee8\ud14d\uc2a4\ud2b8\ub97c \ub098\ud0c0\ub0b4\uba70,  \uac01 \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774\uc5d0 \ub530\ub978 \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.", "section": "4 \ud6a8\uc728\uc131"}, {"content": "Task|LR|WD|Ep|LR|WD|Ep\n---|---|---|---:|:---:|:---:|---:\nCoLA|8e-5|1e-6|5|3e-5|8e-6|5\nMNLI|5e-5|5e-6|1|3e-5|1e-5|1\nMRPC|5e-5|5e-6|10|8e-5|5e-6|2\nQNLI|8e-5|5e-6|2|3e-5|5e-6|2\nQQP|5e-5|5e-6|10|5e-5|8e-6|2\nRTE|5e-5|1e-5|3|5e-5|8e-6|3\nSST-2|8e-5|1e-5|2|1e-5|1e-6|3\nSTSB|8e-5|5e-6|10|8e-5|1e-5|10", "caption": "Table 11: Inference runtime for all models. Bold indicates the best for the column within two SDs.", "description": "\ud45c 11\uc740 \ub2e4\uc591\ud55c \ubaa8\ub378\uc5d0 \ub300\ud55c \ucd94\ub860 \uc2dc\uac04\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ud06c\uae30(\ub9e4\uac1c\ubcc0\uc218 \uc218), \ubc30\uce58 \ud06c\uae30, \uadf8\ub9ac\uace0 \uace0\uc815 \uae38\uc774 \ubc0f \uac00\ubcc0 \uae38\uc774 \uc2dc\ud000\uc2a4\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4.  '\uace0\uc815 \uae38\uc774'\ub294 \ubaa8\ub4e0 \uc2dc\ud000\uc2a4\uac00 \uac19\uc740 \uae38\uc774(512 \ub610\ub294 8192 \ud1a0\ud070)\uc784\uc744 \ub098\ud0c0\ub0b4\uace0, '\uac00\ubcc0 \uae38\uc774'\ub294 \uc2dc\ud000\uc2a4 \uae38\uc774\uac00 \ub2e4\uc591\ud568\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud45c\uc5d0\uc11c \uad75\uac8c \ud45c\uc2dc\ub41c \uac12\uc740 \ud45c\uc900 \ud3b8\ucc28 \ub450 \ubc30 \uc774\ub0b4\uc5d0\uc11c \uac00\uc7a5 \ube60\ub978 \ucd94\ub860 \uc2dc\uac04\uc744 \uac00\uc9c4 \ubaa8\ub378\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc758 \ucd94\ub860 \ud6a8\uc728\uc131\uc744 \ube44\uad50\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.", "section": "4 \ud6a8\uc728\uc131"}]