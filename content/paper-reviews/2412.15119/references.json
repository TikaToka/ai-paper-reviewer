{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Improving Language Understanding by Generative Pre-training", "publication_date": "2018-06-11", "reason": "This paper is foundational to the field of large language models (LLMs), introducing techniques and concepts heavily used in the current research of visual autoregressive generation."}, {"fullname_first_author": "Mark Chen", "paper_title": "Generative Pretraining from Pixels", "publication_date": "2020-06-01", "reason": "This work is highly influential in the field of visual autoregressive modeling, demonstrating the potential of directly generating images from pixels using transformer-based approaches."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Taming Transformers for High-Resolution Image Synthesis", "publication_date": "2021-06-15", "reason": "This paper significantly advanced the capabilities of transformer-based image generation, achieving high-resolution synthesis and improving the quality of generated images compared to previous works."}, {"fullname_first_author": "Peize Sun", "paper_title": "Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation", "publication_date": "2024-06-01", "reason": "This work directly compares autoregressive and diffusion-based models for image generation, establishing a strong baseline for the current state-of-the-art and impacting the direction of subsequent research."}, {"fullname_first_author": "Lijun Yu", "paper_title": "Language Model Beats Diffusion\u2014Tokenizer is Key to Visual Generation", "publication_date": "2024-02-28", "reason": "This work highlights the importance of tokenization techniques in visual generation, arguing that careful selection of the tokenizer can be more impactful than differences between autoregressive and diffusion-based approaches."}]}