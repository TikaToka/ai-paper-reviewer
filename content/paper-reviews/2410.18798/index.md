---
title: "Distill Visual Chart Reasoning Ability from LLMs to MLLMs"
summary: "Researchers synthesize a new multimodal dataset, REACHQA, using code as an intermediary to efficiently distill visual chart reasoning abilities from LLMs to MLLMs."
categories: ["AI Generated"]
tags: ["🔖 24-10-24", "🤗 24-10-25"]
showSummary: true
date: 2024-10-24
draft: false
---

### TL;DR


{{< lead >}}

This research tackles the challenge of enhancing multimodal large language models (MLLMs) with advanced visual reasoning abilities, particularly for complex chart question answering (CQA).  The key contribution is a novel data synthesis method called Code-as-Intermediary Translation (CIT). CIT uses code as a bridge between visual chart representations and textual descriptions, allowing LLMs to understand and reason about cross-modal information.  This approach is significantly more efficient and cost-effective than traditional manual data collection and annotation.  Using CIT, the researchers created REACHQA, a new dataset with 3,000 reasoning-intensive charts and 20,000 Q&A pairs.  Experiments demonstrate that fine-tuning models on REACHQA substantially improves their performance on various chart-related benchmarks, even generalizing to broader mathematical reasoning tasks.  This work offers a valuable approach for developing advanced visual reasoning capabilities in LLMs and creating high-quality multimodal datasets efficiently.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.18798" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}
<br><br>
{{< button href="https://huggingface.co/papers/2410.18798" target="_self" >}}
{{< icon "hf-logo" >}} &nbsp; on Hugging Face
{{< /button >}}

#### Why does it matter?
This paper is highly important for researchers working on multimodal learning and large language models.  It introduces a novel and efficient data synthesis method, addresses the challenges of creating high-quality multimodal datasets, and demonstrates improved performance on various benchmarks.  The proposed method opens new avenues for research in cost-effective data augmentation and the development of advanced visual reasoning capabilities in LLMs.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Code-as-Intermediary Translation (CIT) is proposed as a cost-effective data synthesis method for improving visual reasoning in MLLMs. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} REACHQA, a new dataset with 3k reasoning-intensive charts and 20k Q&A pairs, is created to enhance both visual recognition and reasoning abilities. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Fine-tuning models on REACHQA improves performance on chart-related benchmarks and general mathematical reasoning tasks. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](figures/figures_2_0.png)

> 🔼 The figure shows the error distribution of a LLM on a chart question answering benchmark, highlighting the two main error types: recognition and reasoning errors.
> <details>
> <summary>read the caption</summary>
> Figure 1: Error distribution of incorrect answers by MiniCPM-V2.5-Llama3 (Yao et al., 2024) on ChartQA test set (Masry et al., 2022), as judged by GPT-40. We present an example chart from ChartQA along with two error cases: one for recognition and one for reasoning. The 'Other Errors' include question misunderstood errors, knowledge and hallucination errors, or refusal to answer.
> </details>





![](charts/charts_9_1.png)

> 🔼 The radar chart compares the performance of three models (Base Model, ReachQA-20k, and ReachQA-LLaVA-Mix-40k) across ten different benchmark tasks, including six general-purpose multimodal tasks and four specialized chart-related tasks.
> <details>
> <summary>read the caption</summary>
> Figure 4: Performance comparison of models on 6 general tasks and 4 specialized tasks.
> </details>





{{< table-caption >}}
<table id='2' style='font-size:14px'><tr><td rowspan="2">Datasets</td><td colspan="4">Chart Properties</td><td colspan="3">Q&A Properties</td><td colspan="3">Dataset Properties</td></tr><tr><td># Chart Type</td><td># Chart Topic</td><td>Textual Format</td><td>Vis. Comp.</td><td>Temp. Free</td><td>Vis. Refer.</td><td>Rat. Annot.</td><td>Train Set</td><td>Test Set</td><td>Scal.</td></tr><tr><td>PlotQA Methani et al. 2020</td><td>3</td><td>-</td><td>Table</td><td>X</td><td>X</td><td></td><td>X</td><td></td><td></td><td>X</td></tr><tr><td>ChartQA Masry et al. 2022</td><td>3</td><td>15</td><td>Table</td><td>X</td><td></td><td></td><td>X</td><td>V</td><td></td><td>X</td></tr><tr><td>OpenCQA Kanthara et al. 2022</td><td>5</td><td>10</td><td>Caption</td><td>X</td><td>V</td><td>X</td><td>V</td><td>X</td><td></td><td>X</td></tr><tr><td>MathVista Lu et al. 2024</td><td></td><td>-</td><td>-</td><td>X</td><td>V</td><td>X</td><td>X</td><td>X</td><td></td><td>X</td></tr><tr><td>CharXiv Wang et al 2024c</td><td></td><td>-</td><td>-</td><td>V</td><td>メ</td><td>V</td><td>X</td><td>X</td><td></td><td>X</td></tr><tr><td>ChartBench Xu et al 2023</td><td>9 / 42</td><td>-</td><td>Table</td><td>X</td><td>X</td><td>X</td><td>X</td><td>V</td><td></td><td>V</td></tr><tr><td>ChartX Xia et al. 2024</td><td>18</td><td>22</td><td>Code*</td><td>X</td><td>V</td><td>X</td><td>X</td><td>X</td><td></td><td>V</td></tr><tr><td>MMC Liu et al. 2024a</td><td>6</td><td>5</td><td>Caption</td><td>V</td><td></td><td>X</td><td></td><td></td><td></td><td>メ</td></tr><tr><td>ChartLlama Han et al. 2023</td><td>10</td><td>-</td><td>Table</td><td>X</td><td>V</td><td>X</td><td></td><td></td><td>V</td><td>V</td></tr><tr><td>ChartAst Meng et al. 2024</td><td>9</td><td>-</td><td>Table</td><td>X</td><td>X</td><td>X</td><td></td><td></td><td>X</td><td>メ</td></tr><tr><td>ChartInstruct Masry et al. 2024a</td><td>-</td><td>-</td><td>Table</td><td>X</td><td>V</td><td>X</td><td></td><td></td><td>X</td><td>メ</td></tr><tr><td>ChartGemma Masry et al. 2024b</td><td></td><td>-</td><td>-</td><td>X</td><td>V</td><td>V</td><td></td><td></td><td>X</td><td>メ</td></tr><tr><td>REACHQA (ours)</td><td>10 / 32</td><td>8</td><td>Code</td><td>V</td><td>V</td><td>V</td><td></td><td></td><td>V</td><td>V</td></tr></table>{{< /table-caption >}}

> 🔼 Table 1 compares existing chart-related datasets across three properties: chart properties, Q&A properties, and dataset properties, highlighting their differences in visual diversity, complexity, generation methods, and scalability.
> <details>
> <summary>read the caption</summary>
> Table 1: Comparison of existing chart-related datasets across three properties. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat. rationale, Annot.=annotation and Scal.=scalable. Cells marked with 'X' indicate mixed attributes (e.g., partially template-based; scalable Q&A but non-scalable chart data.). “*” indicates that while the chart-plotting codes are public, the Q&A generation still relies on data tables.
> </details>



### More visual insights

<details>
<summary>More on figures
</summary>


![](figures/figures_5_0.png)

> 🔼 The figure illustrates the Code-as-Intermediary Translation (CIT) method used to create the REACHQA dataset, showing the process from seed codes to final data.
> <details>
> <summary>read the caption</summary>
> Figure 2: Overview of the Code-as-Intermediary Translation (CIT) method for synthesizing multimodal instruction data. The process begins with 33 seed codes and generates plot codes across various chart types, topics, and complexity levels through the Self-Instruct and Evol-Instruct stages. The chart set and instruction set are constructed bi-directionally, and the final filtered data yields REACHQA, a dataset for distilling visual chart reasoning abilities from LLMs to MLLMs.
> </details>



![](figures/figures_10_0.png)

> 🔼 The figure shows the error distribution of a language model on a chart question answering benchmark, highlighting the proportions of errors due to misrecognition and reasoning mistakes.
> <details>
> <summary>read the caption</summary>
> Figure 1: Error distribution of incorrect answers by MiniCPM-V2.5-Llama3 (Yao et al., 2024) on ChartQA test set (Masry et al., 2022), as judged by GPT-40. We present an example chart from ChartQA along with two error cases: one for recognition and one for reasoning. The 'Other Errors' include question misunderstood errors, knowledge and hallucination errors, or refusal to answer.
> </details>



![](figures/figures_17_0.png)

> 🔼 Figure 6 shows visualizations of charts from various datasets, highlighting the visual diversity and complexity of REACHQA compared to other datasets.
> <details>
> <summary>read the caption</summary>
> Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b).
> </details>



![](figures/figures_17_1.png)

> 🔼 Figure 6 shows visualizations of charts from various datasets, highlighting the increased visual complexity and diversity of REACHQA compared to other datasets.
> <details>
> <summary>read the caption</summary>
> Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b).
> </details>



![](figures/figures_17_2.png)

> 🔼 Figure 6 shows visualizations of charts from various chart datasets, highlighting the increased visual complexity and diversity in REACHQA compared to other datasets.
> <details>
> <summary>read the caption</summary>
> Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b).
> </details>



![](figures/figures_17_3.png)

> 🔼 Figure 6 visualizes charts from various chart datasets, highlighting the increased complexity and diversity of charts in REACHQA compared to other datasets.
> <details>
> <summary>read the caption</summary>
> Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b).
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table id='5' style='font-size:14px'><tr><td>Input</td><td>Acc.</td><td>Reas. Comp.</td><td>Vis. Refer.</td><td>Cost ($)</td></tr><tr><td>Table</td><td>2.72</td><td>2.51</td><td>1.19</td><td>0.047</td></tr><tr><td>Code</td><td>2.60</td><td>2.56</td><td>2.15</td><td>0.092</td></tr><tr><td>Chart</td><td>1.91</td><td>1.53</td><td>2.36</td><td>0.107</td></tr></table>{{< /table-caption >}}
> 🔼 {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 1 compares existing chart-related datasets across three properties: Q&A properties, chart properties, and dataset properties, highlighting their differences in terms of textual/visual format, complexity, templates, visual complexity, type, topic, scalability, rationale, training/test sets, and free reference annotations.


{{< table-caption >}}
<table id='8' style='font-size:14px'><tr><td>Statistics</td><td>Train Set</td><td>Test Set</td></tr><tr><td>Total charts</td><td>3,249</td><td>500</td></tr><tr><td>- # Chart types</td><td>10 / 32</td><td>10 / 32</td></tr><tr><td>- # Overlay plots</td><td>1,030</td><td>220</td></tr><tr><td>- # Multiple plots</td><td>593</td><td>251</td></tr><tr><td>- Average size (px)</td><td>2480x1571</td><td>2798 x 1601</td></tr><tr><td>Unique questions</td><td>19, 963</td><td>2,000</td></tr><tr><td>- # Reco. per chart</td><td>2.53</td><td>2</td></tr><tr><td>- # Reas. per chart</td><td>3.62</td><td>2</td></tr><tr><td>Avg. Reco. Q. length</td><td>22.1</td><td>21.0</td></tr><tr><td>Avg. Reco. A. length</td><td>38.3</td><td>7.0</td></tr><tr><td>Avg. Reas. Q. length</td><td>38.2</td><td>35.4</td></tr><tr><td>Avg. Reas. A. length</td><td>68.4</td><td>24.9</td></tr></table>{{< /table-caption >}}
> 🔼 {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 1 compares existing chart-related datasets across three properties: chart properties, Q&A properties, and dataset properties, highlighting their differences in terms of visual diversity, complexity, template use, reasoning capabilities, annotation, and scalability.


{{< table-caption >}}
<table id='2' style='font-size:14px'><tr><td rowspan="2">Models</td><td rowspan="2">Avg. (↑)</td><td>ChartQA</td><td colspan="2">ChartBench</td><td>ChartX</td><td colspan="2">REACHQA</td><td colspan="2">CharXiv</td><td colspan="2">Math Vista</td><td>MATH-V</td></tr><tr><td>QA</td><td>Binary</td><td>NQA</td><td>QA</td><td>Reas.</td><td>Reco.</td><td>Reas.</td><td>Desc.</td><td>Math</td><td>General</td><td>QA</td></tr><tr><td colspan="13">Baselines</td></tr><tr><td>Human</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>65.10</td><td>84.60</td><td>80.50</td><td>92.10</td><td colspan="2">60.30</td><td>75.66</td></tr><tr><td>Random (GPT-4o)</td><td>20.82</td><td>30.04</td><td>40.21</td><td>22.73</td><td>19.85</td><td>8.20</td><td>13.30</td><td>10.80</td><td>19.85</td><td colspan="2">17.90</td><td>25.36</td></tr><tr><td colspan="13">Proprietary Multimodal Large Language Models</td></tr><tr><td>GPT-4o mini</td><td>49.34</td><td>77.52</td><td>70.26</td><td>34.93</td><td>35.45</td><td>27.20</td><td>53.50</td><td>34.10</td><td>74.92</td><td colspan="2">56.70</td><td>28.85</td></tr><tr><td>GPT-4o</td><td>59.85</td><td>85.70</td><td>81.03</td><td>52.88</td><td>46.60</td><td>39.70</td><td>66.80</td><td>47.10</td><td>84.45</td><td colspan="2">63.80</td><td>30.39</td></tr><tr><td>Claude 3.5 Sonnet</td><td>64.50</td><td>90.80</td><td>76.72</td><td>48.29</td><td>58.24</td><td>51.70</td><td>74.30</td><td>60.20</td><td>84.30</td><td colspan="2">67.70</td><td>32.76</td></tr><tr><td colspan="13">Chart-augmented Multimodal Large Language Models</td></tr><tr><td>ChartInstruct-7B</td><td>25.93</td><td>66.64</td><td>61.40</td><td>26.95</td><td>26.62</td><td>6.00</td><td>10.50</td><td>8.80</td><td>21.40</td><td>15.37</td><td>31.52</td><td>10.07</td></tr><tr><td>ChartAssistant-13B</td><td>28.25</td><td>79.90</td><td>58.15</td><td>24.62</td><td>23.20</td><td>10.70</td><td>19.60</td><td>11.70</td><td>16.93</td><td>17.78</td><td>39.57</td><td>8.55</td></tr><tr><td>ChartGemma-3B</td><td>33.08</td><td>80.16</td><td>78.90</td><td>34.10</td><td>35.15</td><td>9.20</td><td>27.80</td><td>12.50</td><td>21.30</td><td>19.07</td><td>38.04</td><td>7.70</td></tr><tr><td colspan="13">Open-Source Multimodal Large Language Models</td></tr><tr><td>LLaVA-Next-Llama3-8B</td><td>24.46</td><td>45.80</td><td>42.90</td><td>15.86</td><td>15.45</td><td>6.50</td><td>17.90</td><td>17.20</td><td>31.45</td><td>22.41</td><td>44.13</td><td>9.44</td></tr><tr><td>+ REACHQA (Reco.)</td><td>32.88 (+34.4%)</td><td>66.96</td><td>56.95</td><td>29.52</td><td>27.25</td><td>8.80</td><td>29.00</td><td>22.20</td><td>32.58</td><td>27.40</td><td>49.78</td><td>11.25</td></tr><tr><td>+ REACHQA (Reas.)</td><td>32.39 (+32.4%)</td><td>64.48</td><td>56.80</td><td>25.14</td><td>25.90</td><td>8.40</td><td>26.30</td><td>22.70</td><td>35.67</td><td>28.89</td><td>50.65</td><td>11.38</td></tr><tr><td>+ REACHQA (All)</td><td>32.98 (+34.8%)</td><td>64.56</td><td>57.00</td><td>29.33</td><td>27.08</td><td>11.10</td><td>29.60</td><td>22.50</td><td>32.33</td><td>27.59</td><td>50.43</td><td>11.25</td></tr><tr><td>MiniCPM-V2.5-Llama3</td><td>33.39</td><td>66.92</td><td>48.90</td><td>22.29</td><td>- 23.72</td><td>10.30</td><td>25.30</td><td>22.00</td><td>46.20</td><td>37.22</td><td>53.04</td><td>11.45</td></tr><tr><td>+ REACHQA (Reco.)</td><td>38.62 (+15.7%)</td><td>71.12</td><td>56.65</td><td>33.29</td><td>29.53</td><td>10.60</td><td>34.10</td><td>25.60</td><td>48.75</td><td>41.48</td><td>60.43</td><td>13.22</td></tr><tr><td>+ REACHQA (Reas.)</td><td>38.52 (+15.4%)</td><td>71.72</td><td>56.65</td><td>29.62</td><td>28.23</td><td>11.00</td><td>33.00</td><td>27.50</td><td>48.70</td><td>43.52</td><td>60.22</td><td>13.52</td></tr><tr><td>+ REACHQA (All)</td><td>38.67 (+15.8%)</td><td>71.44</td><td>55.80</td><td>30.43</td><td>29.68</td><td>11.00</td><td>35.10</td><td>28.30</td><td>47.62</td><td>42.22</td><td>60.00</td><td>13.75</td></tr><tr><td>InternVL2-8B</td><td>40.03</td><td>73.80</td><td>52.05</td><td>32.86</td><td>35.10</td><td>16.20</td><td>33.70</td><td>26.30</td><td>46.10</td><td>46.11</td><td>61.74</td><td>16.38</td></tr><tr><td>+ REACHQA (Reco.)</td><td>48.21 (+20.4%)</td><td>82.92</td><td>66.35</td><td>46.14</td><td>46.62</td><td>19.90</td><td>49.50</td><td>32.20</td><td>54.38</td><td>47.96</td><td>67.61</td><td>16.78</td></tr><tr><td>+ REACHQA (Reas.)</td><td>47.87 (+19.6%)</td><td>82.84</td><td>64.05</td><td>46.52</td><td>44.88</td><td>20.10</td><td>49.40</td><td>32.80</td><td>52.40</td><td>49.44</td><td>66.52</td><td>17.66</td></tr><tr><td>+ REACHQA (All)</td><td>48.35 (+20.8%)</td><td>82.44</td><td>65.90</td><td>47.29</td><td>45.38</td><td>21.30</td><td>49.80</td><td>32.70</td><td>54.83</td><td>48.89</td><td>66.30</td><td>17.01</td></tr></table>{{< /table-caption >}}
> 🔼 {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 1 compares existing chart-related datasets across three properties: chart properties, Q&A properties, and dataset properties.


{{< table-caption >}}
<br><table id='9' style='font-size:16px'><tr><td>Base Model</td><td>16.39</td><td>6.50</td><td>17.20</td><td>32.40</td><td>9.44</td></tr><tr><td>+ ChartBench</td><td>17.06</td><td>7.30</td><td>17.00</td><td>33.60</td><td>10.33</td></tr><tr><td></td><td>17.67</td><td>7.10</td><td>20.40</td><td>32.10</td><td>11.08</td></tr><tr><td>ChartGemma</td><td>19.11</td><td>10.00</td><td>19.40</td><td>36.40</td><td>10.62</td></tr><tr><td></td><td>20.74</td><td>11.10</td><td>22.50</td><td>38.10</td><td>11.25</td></tr></table>{{< /table-caption >}}
> 🔼 {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 1 compares existing chart-related datasets across three properties: chart properties, Q&A properties, and dataset properties, highlighting their differences in visual diversity, complexity, question generation methods, and scalability.


{{< table-caption >}}
<table id='6' style='font-size:14px'><tr><td>Major Category</td><td>Minor Category</td></tr><tr><td>~Line Charts Pie Charts n Bar Charts 3D Bar Charts Node Charts Radar Charts Area Charts Box Charts Scatter Charts Specific Charts</td><td>line chart, line chart with data annotation, line chart with error bar pie chart, donut pie chart, sector pie chart, ring chart bar chart, bar chart with data annotation, stacked bar chart, percentage bar chart, horizontal bar chart 3D bar chart, stacked 3D bar chart, percentage 3D bar chart directed node chart, undirected node chart radar chart, radar chart with area filling area chart, stacked area chart vertical box chart, horizontal box chart scatter chart, scatter chart with smooth fitting, 3D scatter chart (bubble chart) heat map, rose chart, funnel chart, waterfall chart, histogram, tree map</td></tr></table>{{< /table-caption >}}
> 🔼 {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 1 compares existing chart-related datasets across three properties: chart properties, Q&A properties, and dataset properties, highlighting their strengths and weaknesses for use in chart question answering tasks.


{{< table-caption >}}
<table id='8' style='font-size:14px'><tr><td>Art and Design</td><td>Futurism and Innovation</td><td>Agriculture and Food Production</td></tr><tr><td>Music and Performance</td><td>Astronomy and Space</td><td>Transportation and Logistics</td></tr><tr><td>Business and Finance</td><td>Social Media and the Web</td><td>Real Estate and Housing Market</td></tr><tr><td>Travel and Exploration</td><td>Society and Community</td><td>Government and Public Policy</td></tr><tr><td>Books and Publishing</td><td>Physics and Chemistry</td><td>Education and Academics</td></tr><tr><td>Literature and Writing</td><td>Energy and Utilities</td><td>Environment and Sustainability</td></tr><tr><td>History and Culture</td><td>Biology and Life Sciences</td><td>Language and Communication</td></tr><tr><td>Architecture and Building</td><td>Retail and E-commerce</td><td>Social Sciences and Humanities</td></tr><tr><td>Fashion and Style</td><td>Religion and Spirituality</td><td>Manufacturing and Production</td></tr><tr><td>Marketing and Advertising</td><td>Food and Beverage Industry</td><td>Artificial Intelligence and Robotics</td></tr><tr><td>Law and Legal Affairs</td><td>Healthcare and Health</td><td>Human Resources and Employee Management</td></tr><tr><td>Film and Cinema</td><td>Sports and Entertainment</td><td>Computer Science and Information Technology</td></tr><tr><td>Mathematics and Statistics</td><td>Science and Engineering</td><td></td></tr></table>{{< /table-caption >}}
> 🔼 {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 1 compares existing chart-related datasets across three properties (Q&A properties, chart properties, and dataset properties) highlighting their differences in terms of textual format, complexity, templates, visual features, scalability, and annotation.


{{< table-caption >}}
<table id='2' style='font-size:14px'><tr><td>Step</td><td>Avg. #tokens of Input</td><td>Avg. #tokens of Output</td><td>Times</td><td>Cost ($)</td></tr><tr><td>Self-Instruct</td><td>1, 500 + 2, 000 = 3, 500</td><td>500 + 500 = 1, 000</td><td>3,000</td><td>~ 56.25</td></tr><tr><td>Evol-Instruct</td><td>700 + 1, 300 = 2, 000</td><td>300 + 700 = 1, 000</td><td>3,000</td><td>~ 45.00</td></tr><tr><td>Self-Repair</td><td>500</td><td>500</td><td>1,500</td><td>~ 9.38</td></tr><tr><td>Reas-QA-Gen.</td><td>1,000 + 1, 500 x 4 = 7, 000</td><td>500 + 300 x 4 = 1, 700</td><td>3,249</td><td>~ 112.09</td></tr><tr><td>Reco-QA-Gen.</td><td>800 + 1, 200 x 4= 5, 600</td><td>300 + 200 x4= 1, 100</td><td>3,249</td><td>~ 81.23</td></tr></table>{{< /table-caption >}}
> 🔼 {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 1 compares existing chart-related datasets across three properties: Q&A properties, chart properties, and dataset properties, highlighting their differences in terms of format, complexity, and scalability.


</details>


### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/21.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/22.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/23.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/24.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/25.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/26.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}