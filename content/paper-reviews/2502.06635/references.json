{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper introduces Llama, a foundational large language model that is both open-source and computationally efficient, serving as a significant basis for Steel-LLM's development."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "The Qwen model architecture and training methodology significantly influenced Steel-LLM's design, serving as a primary architectural reference."}, {"fullname_first_author": "Ge Zhang", "paper_title": "MAP-Neo-7B", "publication_date": "2024-05-17", "reason": "This paper introduced a comprehensive methodology of LLM development, including data, code, model weights, and intermediate checkpoints which was very valuable for Steel-LLM's development."}, {"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention: Fast and memory-efficient exact attention with io-awareness", "publication_date": "2022-05-14", "reason": "This paper's introduction of FlashAttention, a highly efficient self-attention mechanism, directly improved Steel-LLM's training speed and resource utilization."}, {"fullname_first_author": "Joan Puigcerver", "paper_title": "From sparse to soft mixtures of experts", "publication_date": "2024-08-01", "reason": "This paper's introduction of soft MoE, a fully differentiable mixture-of-experts approach, was directly incorporated into Steel-LLM's architecture to enhance model performance."}]}