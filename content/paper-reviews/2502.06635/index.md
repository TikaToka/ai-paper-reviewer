---
title: "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM"
summary: "ì œí•œëœ ìì›ìœ¼ë¡œ ê°œë°œëœ ì˜¤í”ˆì†ŒìŠ¤ ì¤‘êµ­ì–´ ì¤‘ì‹¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ Steel-LLMì´ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©°, ì†Œê·œëª¨ ì—°êµ¬íŒ€ì˜ LLM ê°œë°œì„ ìœ„í•œ ì‹¤ìš©ì  ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤."
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Natural Language Processing", "Large Language Models", "ğŸ¢ Tsinghua University",]
showSummary: true
date: 2025-02-10
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2502.06635 {{< /keyword >}}
{{< keyword icon="writer" >}} Qingshui Gu et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2025-02-11 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2502.06635" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2502.06635" target="_self" >}}
â†— Hugging Face
{{< /button >}}




### TL;DR


{{< lead >}}

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë°œì „ì—ë„ ë¶ˆêµ¬í•˜ê³ , **ê°œë°œì˜ íˆ¬ëª…ì„±, ì ‘ê·¼ì„± ë° ìì› íš¨ìœ¨ì„±ì€ ì—¬ì „íˆ ê³¼ì œ**ì…ë‹ˆë‹¤. ë§ì€ ê¸°ì¡´ LLMë“¤ì€ í›ˆë ¨ ë°ì´í„°, ì½”ë“œ ë° ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ë¥¼ ê³µê°œí•˜ì§€ ì•Šì•„ ì¬í˜„ì„±ì— ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìœ¼ë©°, **ëŒ€ê·œëª¨ ê³„ì‚° ìì›ì´ í•„ìš”**í•˜ì—¬ ì†Œê·œëª¨ ì—°êµ¬íŒ€ì˜ ì ‘ê·¼ì„±ì´ ë‚®ìŠµë‹ˆë‹¤.



ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì œí•œëœ ê³„ì‚° ìì›(8ê°œì˜ GPU)ì„ ì‚¬ìš©í•˜ì—¬ ê°œë°œëœ **ì˜¤í”ˆì†ŒìŠ¤ ì¤‘êµ­ì–´ ì¤‘ì‹¬ LLMì¸ Steel-LLMì„ ì†Œê°œ**í•©ë‹ˆë‹¤. Steel-LLMì€ **íˆ¬ëª…ì„±ì„ ì¤‘ì‹œ**í•˜ì—¬ í›ˆë ¨ íŒŒì´í”„ë¼ì¸, ë°ì´í„°ì…‹, ëª¨ë¸ ì•„í‚¤í…ì²˜ ë° ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ë¥¼ ê³µê°œí•˜ê³ , **ì†Œê·œëª¨ ì—°êµ¬ë¥¼ ìœ„í•œ ì‹¤ìš©ì ì¸ ì§€ì¹¨**ì„ ì œê³µí•©ë‹ˆë‹¤.  **CEVAL ë° CMMLU ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥**ì„ ë³´ì´ë©°, **ì†Œê·œëª¨ ì—°êµ¬íŒ€ì˜ LLM ê°œë°œì„ ìœ„í•œ ì¤‘ìš”í•œ ìì›**ì´ ë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} ì œí•œëœ ê³„ì‚° ìì›ì„ ì‚¬ìš©í•˜ì—¬ ê³ í’ˆì§ˆì˜ ì˜¤í”ˆì†ŒìŠ¤ ì¤‘êµ­ì–´ ì¤‘ì‹¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ê°œë°œí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤Œ {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} ë°ì´í„° ìˆ˜ì§‘, ëª¨ë¸ ì„¤ê³„, í›ˆë ¨ ë°©ë²•ë¡  ë“± ëª¨ë¸ êµ¬ì¶• ê³¼ì •ì— ëŒ€í•œ ìì„¸í•˜ê³  ì‹¤ìš©ì ì¸ ì„¤ëª… ì œê³µ {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} CEVAL ë° CMMLU ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
**ë³¸ ë…¼ë¬¸ì€ ì œí•œëœ ê³„ì‚° ìì›ìœ¼ë¡œ ê³ í’ˆì§ˆì˜ ì˜¤í”ˆì†ŒìŠ¤ ì¤‘êµ­ì–´ ì¤‘ì‹¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ë° ìˆì–´ ê·€ì¤‘í•œ í†µì°°ë ¥ê³¼ ì‹¤ìš©ì ì¸ ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.**  ì´ëŠ” ì†Œê·œëª¨ ì—°êµ¬íŒ€ê³¼ ê°œì¸ ì—°êµ¬ìë“¤ì—ê²Œ íŠ¹íˆ ì¤‘ìš”í•˜ë©°, **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ê°œë°œì˜ ì ‘ê·¼ì„±ì„ ë†’ì´ê³ , ì¤‘êµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì˜ ë°œì „ì— ê¸°ì—¬í•  ê²ƒ**ìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ë˜í•œ, **ë³¸ ì—°êµ¬ëŠ” íˆ¬ëª…ì„±ê³¼ ì¬í˜„ì„±ì„ ê°•ì¡°í•˜ì—¬ ì—°êµ¬ ê³µë™ì²´ì— í¬ê²Œ ê¸°ì—¬**í•  ê²ƒì…ë‹ˆë‹¤.

------
#### Visual Insights



![](https://arxiv.org/html/2502.06635/x1.png)

> ğŸ”¼ ê·¸ë¦¼ 1ì€ Steel-LLM Transformer ë¸”ë¡ì˜ ì•„í‚¤í…ì²˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  Steel-LLMì€ Soft Mixture of Experts (Soft MOE)ì™€ í–¥ìƒëœ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬(FFN)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì œí•œëœ ë¦¬ì†ŒìŠ¤ ë‚´ì—ì„œ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.  ê·¸ë¦¼ì€ í† í°ì´ Self-Attention, Soft MOE, ê·¸ë¦¬ê³  í–¥ìƒëœ FFN ë ˆì´ì–´ë¥¼ ê±°ì¹˜ëŠ” ê³¼ì •ì„ ìì„¸íˆ ë³´ì—¬ì£¼ë©°, ê° ë ˆì´ì–´ì˜ êµ¬ì„± ìš”ì†Œì™€ ë°ì´í„° íë¦„ì„ ì‹œê°ì ìœ¼ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  íŠ¹íˆ Soft MOEëŠ” ì „ë¬¸ê°€(Expert)ë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ê³„ì‚° ë¶€í•˜ë¥¼ ì¤„ì´ëŠ” ì „ëµì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì´ ê·¸ë¦¼ì„ í†µí•´ Steel-LLMì˜ íš¨ìœ¨ì ì¸ ì„¤ê³„ì™€ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ í•µì‹¬ êµ¬ì„± ìš”ì†Œë“¤ì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 1: The architecture of Steel-LLM Transformer block
> </details>





{{< table-caption >}}
| Parameters | Value |
|---|---| 
| Layers | 18 |
| Heads | 32 |
| KV heads | 32 |
| Num_experts | 6 |
| Slots_per_expert | 1 |
| Hidden size | 1,792 |
| Intermediate size | 1,792 |
| Vocab size | 151,936 |{{< /table-caption >}}

> ğŸ”¼ í‘œ 1ì€ Steel-LLM ëª¨ë¸ì˜ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ë ˆì´ì–´ ìˆ˜, í—¤ë“œ ìˆ˜, íˆë“  ì‚¬ì´ì¦ˆ, ì¤‘ê°„ ì‚¬ì´ì¦ˆ, ì–´íœ˜ í¬ê¸°, ì „ë¬¸ê°€ ìˆ˜, ìŠ¬ë¡¯ ìˆ˜ ë“±ì˜ ì£¼ìš” êµ¬ì¡°ì  ë§¤ê°œë³€ìˆ˜ì™€ í•™ìŠµ ì „ëµì— ëŒ€í•œ ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì´ í‘œëŠ” Steel-LLMì˜ ì•„í‚¤í…ì²˜ì™€ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¤‘ìš”í•œ ìš”ì†Œë“¤ì„ í•œëˆˆì— ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 1: Key model parameters.
> </details>





### In-depth insights


#### Resource-Efficient LLMs
ìì› íš¨ìœ¨ì ì¸ LLMsëŠ” **ì œí•œëœ ì»´í“¨íŒ… ìì›**ì„ ì‚¬ìš©í•˜ì—¬ ê³ í’ˆì§ˆì˜ ì–¸ì–´ ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ì´ëŠ” ëŒ€ê·œëª¨ ì¸í”„ë¼ì— ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” ì†Œê·œëª¨ ì—°êµ¬íŒ€ì´ë‚˜ ê°œì¸ ì—°êµ¬ìë“¤ì—ê²Œ íŠ¹íˆ ì¤‘ìš”í•©ë‹ˆë‹¤.  **íˆ¬ëª…ì„±**ê³¼ **ì¬í˜„ì„±**ì„ ê°•ì¡°í•˜ì—¬, ëª¨ë¸ ê°œë°œ ê³¼ì •ì˜ ëª¨ë“  ì„¸ë¶€ ì •ë³´(ë°ì´í„°, ì½”ë“œ, ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ë“±)ë¥¼ ê³µê°œí•¨ìœ¼ë¡œì¨ ë‹¤ë¥¸ ì—°êµ¬ìë“¤ì´ ì—°êµ¬ë¥¼ ì¬í˜„í•˜ê³  ë°œì „ì‹œí‚¬ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.  **ì†Œê·œëª¨ ì—°êµ¬ì— ëŒ€í•œ ì‹¤ì§ˆì ì¸ ì§€ì¹¨**ì„ ì œê³µí•˜ì—¬, ì œí•œëœ ìì›ì„ ê°€ì§„ ì—°êµ¬ìë“¤ì´ ìì‹ ë§Œì˜ LLMsë¥¼ ê°œë°œí•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ ì„¤ê³„, í›ˆë ¨ ë°©ë²•ë¡ , ë°ì´í„° ì¤€ë¹„ ê³¼ì • ë“±ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì„ í¬í•¨í•©ë‹ˆë‹¤.  **ì¤‘ìš”í•œ ê²ƒì€, ê³ í’ˆì§ˆì˜ ì„±ëŠ¥**ì„ ìœ ì§€í•˜ë©´ì„œ ìì› íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•˜ì—¬, ëŒ€ê·œëª¨ ëª¨ë¸ê³¼ ê²½ìŸë ¥ ìˆëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ ì—°êµ¬ì˜ **ë¯¼ì£¼í™”**ë¥¼ ì¦ì§„ì‹œí‚¤ê³ , ë‹¤ì–‘í•œ ì–¸ì–´ì™€ íŠ¹ì • ë„ë©”ì¸ì— íŠ¹í™”ëœ LLMs ê°œë°œì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

#### Chinese-Centric Approach
ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ Steel-LLMì˜ ì¤‘êµ­ì–´ ì¤‘ì‹¬ ì ‘ê·¼ ë°©ì‹ì€ **ë°ì´í„°ì…‹ êµ¬ì„±**ì—ì„œ ê°€ì¥ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤.  **ëŒ€ê·œëª¨ ì¤‘êµ­ì–´ ë°ì´í„°**ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í–ˆìœ¼ë©°, ì†ŒëŸ‰ì˜ ì˜ì–´ ë°ì´í„°ë¥¼ ë³´ì¡°ì ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ê¸°ì¡´ì˜ ë‹¤êµ­ì–´ LLMë“¤ì´ ì˜ì–´ ì¤‘ì‹¬ìœ¼ë¡œ í•™ìŠµë˜ëŠ” ê²½í–¥ì„ ë²—ì–´ë‚˜, **ì¤‘êµ­ì–´ì˜ íŠ¹ìˆ˜ì„±ê³¼ í’ë¶€í•¨ì„ ì œëŒ€ë¡œ ë°˜ì˜**í•˜ë ¤ëŠ” ì˜ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  **ìì› ì œì•½**ì—ë„ ë¶ˆêµ¬í•˜ê³ ,  **ê³ í’ˆì§ˆì˜ ì˜¤í”ˆì†ŒìŠ¤ ì¤‘êµ­ì–´ LLM**ì„ ê°œë°œí•˜ê² ë‹¤ëŠ” ëª©í‘œ ì•„ë˜, íš¨ìœ¨ì ì¸ ëª¨ë¸ ê°œë°œ ë° í›ˆë ¨ ë°©ë²•ë¡ ì„ ì„ íƒí•˜ì—¬ **ì‹¤ìš©ì„±**ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.  **íˆ¬ëª…ì„±**ì„ ì¤‘ì‹œí•˜ì—¬ ë°ì´í„°ì…‹, ì½”ë“œ, ëª¨ë¸ êµ¬ì¡° ë“±ì„ ëª¨ë‘ ê³µê°œí•¨ìœ¼ë¡œì¨, ë‹¤ë¥¸ ì—°êµ¬ìë“¤ì˜ ì¬í˜„ ë° ì¶”ê°€ ì—°êµ¬ë¥¼ ì§€ì›í•˜ê³ , **ì¤‘êµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì˜ ë°œì „**ì— ê¸°ì—¬í•˜ê³ ì í•˜ëŠ” ì˜ì§€ë¥¼ ë³´ì…ë‹ˆë‹¤.  ì´ëŠ” ì¤‘êµ­ì–´ ì¤‘ì‹¬ì˜ ì ‘ê·¼ì´ ë‹¨ìˆœí•œ ì–¸ì–´ ì„ íƒì„ ë„˜ì–´, **ì˜¤í”ˆì†ŒìŠ¤ ìƒíƒœê³„ êµ¬ì¶•**ê³¼ **í•™ë¬¸ì  ê³µìœ **ë¥¼ ìœ„í•œ ì „ëµì ì¸ ì„ íƒì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

#### Soft MOE & FFN Enhancements
ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ ì†Œí”„íŠ¸ MOE(Mixture of Experts)ì™€ FFN(Feed-Forward Network) ê°œì„ ì€ **ê³„ì‚° ìì›ì´ ì œí•œëœ í™˜ê²½ì—ì„œë„ ê³ í’ˆì§ˆì˜ ì–¸ì–´ ëª¨ë¸ì„ êµ¬ì¶•**í•˜ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.  **ê¸°ì¡´ì˜ ìŠ¤íŒŒìŠ¤ MOEì˜ ë‹¨ì ì„ ê·¹ë³µ**í•˜ê¸° ìœ„í•´ **ì „ì²´ì ìœ¼ë¡œ ë¯¸ë¶„ ê°€ëŠ¥í•œ ì†Œí”„íŠ¸ MOE**ë¥¼ ì±„íƒí•˜ì—¬ ì „ë¬¸ê°€ ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í•©ë‹ˆë‹¤.  ì´ëŠ” **ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ì™„ë²½í•˜ê²Œ í•™ìŠµ**í•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.  **FFN ê°œì„ **ì€ SwiGLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ë‘ ë²ˆì§¸ MLP ê³„ì¸µì—ë„ ì ìš©í•˜ì—¬ ë¹„ì„ í˜• í‘œí˜„ ëŠ¥ë ¥ì„ ê°•í™”í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.  **ê³„ì‚° íš¨ìœ¨ì„±ì„ ê³ ë ¤**í•˜ì—¬ Rotary Position Embedding(RoPE)ì„ ì‚¬ìš©í•˜ê³ , ëŒ€ë¶€ë¶„ì˜ ê³„ì¸µì—ì„œ ë°”ì´ì–´ìŠ¤ë¥¼ ì œê±°í•˜ëŠ” ë“±ì˜ ìµœì í™” ê¸°ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ ì œí•œëœ GPU ë©”ëª¨ë¦¬ í™˜ê²½ì—ì„œë„ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.  ê²°ë¡ ì ìœ¼ë¡œ, ì´ëŸ¬í•œ ê°œì„ ë“¤ì€ **ìì› ì œì•½ í•˜ì—ì„œë„ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±**í•˜ëŠ” ë° í¬ê²Œ ê¸°ì—¬í•˜ë©°, **ì†Œê·œëª¨ ì—°êµ¬íŒ€ì—ë„ ì‹¤ì§ˆì ì¸ ê°€ì´ë“œë¼ì¸**ì„ ì œê³µí•©ë‹ˆë‹¤.

#### Open-Source Transparency
ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ Steel-LLMì€ ì˜¤í”ˆì†ŒìŠ¤ì˜ íˆ¬ëª…ì„±ì„ ì¤‘ì‹œí•˜ì—¬ **ëª¨ë¸ ê°œë°œ ê³¼ì • ì „ë°˜ì— ëŒ€í•œ ì™„ì „í•œ ì •ë³´ ê³µê°œ**ë¥¼ ì§€í–¥í•©ë‹ˆë‹¤.  ì´ëŠ” ë‹¨ìˆœíˆ ìµœì¢… ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë§Œ ê³µê°œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **í›ˆë ¨ íŒŒì´í”„ë¼ì¸, ë°ì´í„°ì…‹, ëª¨ë¸ ì•„í‚¤í…ì²˜, ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ê¹Œì§€ ëª¨ë‘ ê³µê°œ**í•¨ìœ¼ë¡œì¨, ì¬í˜„ì„±ì„ ë†’ì´ê³  í•™ê³„ì˜ ë” ë„“ì€ ì°¸ì—¬ë¥¼ ìœ ë„í•©ë‹ˆë‹¤.  **ì†Œê·œëª¨ ì—°êµ¬íŒ€ì´ë‚˜ ê°œì¸ ì—°êµ¬ìë“¤ì´ ì ‘ê·¼í•˜ê¸° ì–´ë ¤ìš´ ëŒ€ê·œëª¨ ì»´í“¨íŒ… ìì›ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ìµœì†Œí™”**í•˜ë©´ì„œ ê³ í’ˆì§ˆì˜ LLMì„ ê°œë°œí•˜ê³  ë°°í¬í•  ìˆ˜ ìˆë„ë¡ ì‹¤ì§ˆì ì¸ ì§€ì¹¨ì„ ì œê³µí•˜ëŠ” ì ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.  **í•œì •ëœ ìì›ìœ¼ë¡œë„ íˆ¬ëª…ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë™ì‹œì— í™•ë³´**í•œ ë³¸ ì—°êµ¬ëŠ” í–¥í›„ ì˜¤í”ˆì†ŒìŠ¤ LLM ìƒíƒœê³„ ë°œì „ì— í¬ê²Œ ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë©°, íŠ¹íˆ ì˜ì–´ê¶Œì´ ì•„ë‹Œ ì–¸ì–´ê¶Œì—ì„œì˜ LLM ê°œë°œì— í° ì˜í–¥ì„ ë¯¸ì¹  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.  **ì‹¤ì œì ì¸ ëª¨ë¸ êµ¬ì¶• ê²½í—˜ ê³µìœ **ë¥¼ í†µí•´ ë‹¤ë¥¸ ì—°êµ¬ìë“¤ì´ ìœ ì‚¬í•œ ë…¸ë ¥ì„ ë”ìš± íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ”ë‹¤ëŠ” ì ì—ì„œ ì¤‘ìš”í•œ ì˜ì˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

#### Benchmark & Ablation Studies
ë³¸ ë…¼ë¬¸ì˜ ë²¤ì¹˜ë§ˆí¬ ë° ì—ì´ë¸”ë ˆì´ì…˜ ì—°êµ¬ëŠ” **Steel-LLMì˜ ì„±ëŠ¥ì„ ë‹¤ì–‘í•œ ì¸¡ë©´ì—ì„œ í‰ê°€**í•˜ê³ , **ëª¨ë¸ ì„¤ê³„ ì„ íƒì˜ ì˜í–¥ì„ ë¶„ì„**í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ì—ˆìŠµë‹ˆë‹¤.  ë‹¤ì–‘í•œ ë°ì´í„° êµ¬ì„±(ì˜ˆ: ì¤‘êµ­ì–´ ë°ì´í„° ë¹„ì¤‘, ì˜ì–´ ë°ì´í„° ì¶”ê°€ ì—¬ë¶€)ê³¼ ë¯¸ì„¸ì¡°ì • ì „ëµì„ ì‹¤í—˜í•˜ì—¬ **Steel-LLMì˜ ê°•ì ê³¼ ì•½ì ì„ ë°íˆê³  ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•˜ëŠ” ìš”ì†Œë¥¼ íŒŒì•…**í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì¤‘êµ­ì–´ ì¤‘ì‹¬ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì§‘ì¤‘ì ì¸ í•™ìŠµì´ ì¤‘êµ­ì–´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ì ì ˆí•œ ì–‘ì˜ ì˜ì–´ ë°ì´í„° ì¶”ê°€ëŠ” ë‹¤êµ­ì–´ ëŠ¥ë ¥ í–¥ìƒì— ë„ì›€ì´ ë˜ì—ˆë‹¤ëŠ” ì ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  **ì—ì´ë¸”ë ˆì´ì…˜ ì—°êµ¬ëŠ” ëª¨ë¸ì˜ ê° êµ¬ì„±ìš”ì†Œ(ì˜ˆ: Soft MOE, Enhanced FFN)**ê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ëª¨ë¸ì˜ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ ê°„ì˜ ê· í˜•ì„ ìµœì í™”í•˜ëŠ” ë° ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶„ì„ ê²°ê³¼ëŠ” í–¥í›„ ìœ ì‚¬í•œ ëª¨ë¸ ê°œë°œì— ìˆì–´ ì¤‘ìš”í•œ ì§€ì¹¨ìœ¼ë¡œ í™œìš©ë  ìˆ˜ ìˆìœ¼ë©°, **ì œí•œëœ ìì›ì„ ê°€ì§„ ì—°êµ¬ìë“¤ì—ê²Œ ì‹¤ìš©ì ì¸ ê°€ì´ë“œë¼ì¸**ì„ ì œì‹œí•©ë‹ˆë‹¤.  **ëª¨ë¸ì˜ íˆ¬ëª…ì„±ê³¼ ì¬í˜„ì„±**ì„ í™•ë³´í•˜ê¸° ìœ„í•œ ë…¸ë ¥ìœ¼ë¡œ, ì‹¤í—˜ ê³¼ì •ê³¼ ê²°ê³¼ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ê³¼ ë°ì´í„° ê³µê°œê°€ ì´ë£¨ì–´ì¡Œë‹¤ëŠ” ì  ë˜í•œ ì£¼ëª©í•  ë§Œí•©ë‹ˆë‹¤.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2502.06635/x2.png)

> ğŸ”¼ ê·¸ë¦¼ 2ëŠ” Steel-LLM ì‚¬ì „ í›ˆë ¨ì— ì‚¬ìš©ëœ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  SkyPile, WanJuan, Wikipedia-cn, Baidu Baike, Baidu QA, Zhihu QA, BELLE, Moss, Firefly, Starcodeì™€ ê°™ì€ ë‹¤ì–‘í•œ ì¶œì²˜ì˜ ë°ì´í„°ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, íŠ¹íˆ ì¤‘êµ­ì–´ ë°ì´í„°ê°€ ì••ë„ì ìœ¼ë¡œ ë§ìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ì–´ ë°ì´í„°ì™€ ì½”ë“œ ë°ì´í„°ë„ ì¼ë¶€ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ, ì „ì²´ ë°ì´í„°ì˜ ìƒë‹¹ ë¶€ë¶„ì€ ì¤‘êµ­ì–´ ì›¹ í˜ì´ì§€, ë°±ê³¼ì‚¬ì „, ëŒ€í™” ë°ì´í„° ë“±ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” Steel-LLMì´ ì¤‘êµ­ì–´ ì¤‘ì‹¬ì˜ ì–¸ì–´ ëª¨ë¸ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 2: Pretraining Data Distribution
> </details>



![](https://arxiv.org/html/2502.06635/x3.png)

> ğŸ”¼ ê·¸ë¦¼ 3ì€ Steel-LLMì˜ ì‚¬ì „ í›ˆë ¨ ì†ì‹¤ ê³¡ì„ ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì²˜ìŒ 200,000 ë‹¨ê³„ëŠ” NVIDIA A100-80G GPUë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆê³ , ë‚˜ë¨¸ì§€ ë‹¨ê³„ëŠ” 8ê°œì˜ H800-80G GPUë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. xì¶•ì€ í† í° ìˆ˜ (ë‹¨ìœ„: 10ì–µ)ì´ê³  yì¶•ì€ ì†ì‹¤ ê°’ì…ë‹ˆë‹¤. ë‘ ê°€ì§€ ë‹¤ë¥¸ GPUë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í–ˆìŒì„ ë³´ì—¬ì£¼ëŠ” ë‘ ê°œì˜ ê³¡ì„ ì´ í‘œì‹œë©ë‹ˆë‹¤. ê³¡ì„ ì€ í›ˆë ¨ ê³¼ì •ì—ì„œ ì†ì‹¤ì´ ê°ì†Œí•˜ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚´ë©°, Steel-LLMì˜ í›ˆë ¨ ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 3: Pre-training loss curve for Steel-LLM
> </details>



![](https://arxiv.org/html/2502.06635/x4.png)

> ğŸ”¼ ê·¸ë¦¼ 4ëŠ” Steel-LLM ëª¨ë¸ì˜ ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì • ê³¼ì •ì—ì„œì˜ ì†ì‹¤ ê³¡ì„ ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  xì¶•ì€ ë¯¸ì„¸ ì¡°ì • ë‹¨ê³„(step) ìˆ˜ë¥¼, yì¶•ì€ ì†ì‹¤ ê°’(loss)ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  ê³¡ì„ ì€ ë¯¸ì„¸ ì¡°ì •ì´ ì§„í–‰ë¨ì— ë”°ë¼ ì†ì‹¤ ê°’ì´ ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©°, ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  ì´ ê·¸ë˜í”„ëŠ” Steel-LLMì˜ í•™ìŠµ ì•ˆì •ì„±ê³¼ íš¨ìœ¨ì„±ì„ í‰ê°€í•˜ëŠ” ë° ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 4: Supervised Fine-tuning loss curve for Steel-LLM
> </details>



![](https://arxiv.org/html/2502.06635/x5.png)

> ğŸ”¼ ê·¸ë¦¼ 5ëŠ” Steel-LLMì— ëŒ€í•œ ì§ì ‘ ì„ í˜¸ë„ ìµœì í™”(DPO) ì†ì‹¤ ê³¡ì„ ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. DPOëŠ” ì‚¬ìš©ì ì„ í˜¸ë„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ì´ ê·¸ë˜í”„ëŠ” í›ˆë ¨ ë‹¨ê³„ì— ë”°ë¥¸ ì†ì‹¤ ê°’ì˜ ë³€í™”ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì†ì‹¤ ê°’ì´ ê°ì†Œí•˜ëŠ” ê²ƒì€ ëª¨ë¸ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ê³¡ì„ ì˜ í˜•íƒœëŠ” DPO í›ˆë ¨ ê³¼ì •ì˜ ìˆ˜ë ´ ì†ë„ì™€ ì•ˆì •ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ Steel-LLMì˜ í›ˆë ¨ ê³¼ì •ì—ì„œ DPO ê¸°ë²•ì´ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ì„ ê°œì„ í•˜ëŠ”ë° ê¸°ì—¬í–ˆìŒì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 5: Direct Preference Optimization loss curve for Steel-LLM
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Component | Exp 1 | Exp 2 | Exp 3 | Exp 4 | Exp 5 | Exp 6 | Exp 7 | Exp 8 |
|---|---|---|---|---|---|---|---|---|
| FlashAttention | âœ“ | âœ“ | Ã— | âœ“ | âœ“ | âœ“ | âœ“ | Ã— |
| SelfAttention(PyTorch) | Ã— | Ã— | âœ“ | Ã— | Ã— | Ã— | Ã— | âœ“ |
| RoPE(CUDA) | âœ“ | Ã— | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | Ã— |
| RoPE(PyTorch) | Ã— | âœ“ | Ã— | Ã— | Ã— | Ã— | Ã— | âœ“ |
| RMSNorm(CUDA) | Ã— | Ã— | Ã— | Ã— | âœ“ | Ã— | âœ“ | Ã— |
| RMSNorm(PyTorch) | âœ“ | âœ“ | âœ“ | âœ“ | Ã— | âœ“ | Ã— | âœ“ |
| Loss Function(Triton) | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | Ã— | âœ“ | Ã— |
| Loss Function(PyTorch) | Ã— | Ã— | Ã— | Ã— | Ã— | âœ“ | Ã— | âœ“ |
| FSDP | âœ“ | âœ“ | âœ“ | Ã— | âœ“ | âœ“ | Ã— | âœ“ |
| FSDP(no share param) | Ã— | Ã— | Ã— | âœ“ | Ã— | Ã— | âœ“ | Ã— |
| Speed(tokens/s/gpu) | 13400 | 12500 | 10600 | 13800 | 14600 | 13000 | 15000 | 10500 |
| GPU Memory(GB) | 65 | 65 | 69 | 69 | 61 | 75 | 66 | 75 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 2ëŠ” Steel-LLM ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ëœ ë‹¤ì–‘í•œ ì„¤ì •ë“¤ì˜ ë¹„êµ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  FlashAttention, ROPE(CUDA/PyTorch), RMSNorm(CUDA/PyTorch), ì†ì‹¤ í•¨ìˆ˜(Triton/PyTorch), FSDP(íŒŒë¼ë¯¸í„° ê³µìœ  ìœ ë¬´) ë“±ì˜ ë‹¤ì–‘í•œ í•™ìŠµ ìµœì í™” ê¸°ë²•ì„ ì‚¬ìš©í•œ ì‹¤í—˜ ê²°ê³¼ë¥¼ í† í°/ì´ˆ/GPU ì†ë„ì™€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ë©´ì—ì„œ ë¹„êµ ë¶„ì„í•˜ì—¬, ê° ê¸°ë²•ë“¤ì˜ íš¨ê³¼ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì œì‹œí•©ë‹ˆë‹¤.  ì´ë¥¼ í†µí•´ Steel-LLM í•™ìŠµ íš¨ìœ¨ì„ ìµœëŒ€ 50%ê¹Œì§€ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•œ ìµœì ì˜ ì„¤ì •ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 2: Comparison of different training configurations.
> </details>

{{< table-caption >}}
| Experiment | CEVAL Acc. | CMMLU Acc. | MMLU Acc. |
|---|---|---|---|
| Full Infinity-Instruct + Wanjuan MCQ | 32.35 | 26.32 | 25.50 |
| 700K Chinese Infinity-Instruct + Wanjuan MCQ | 38.57 | 33.48 | 23.26 |
| Chinese + 100% English Data | 39.21 | 33.20 | 26.73 |
| Chinese + 20% English Data (Balanced) | 40.43 | 35.86 | 26.75 |
| Chinese + 20% English + English MCQ | 41.90 | 36.08 | 30.82 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 3ì€ ë¯¸ì„¸ ì¡°ì • ì „ëµì— ë”°ë¥¸ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ë‹¤ì–‘í•œ ë°ì´í„° êµ¬ì„±(ì „ì²´ Infinity-Instruct ë°ì´í„°ì…‹, Infinity-Instructì˜ ì¤‘êµ­ì–´ ë°ì´í„°ë§Œ ì‚¬ìš©, ì¤‘êµ­ì–´ì™€ ì˜ì–´ ë°ì´í„°ë¥¼ ê· í˜• ìˆê²Œ ì‚¬ìš©, ì˜ì–´ ë°ì´í„°ë¥¼ 20%ë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ê·¸ë¦¬ê³  ì¶”ê°€ì ìœ¼ë¡œ ì˜ì–´ MCQ ë°ì´í„°ë¥¼ í¬í•¨í•œ ê²½ìš°)ì— ë”°ë¥¸ CEVAL, CMMLU, MMLU ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ ì •í™•ë„ë¥¼ ë¹„êµ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  ê° ì „ëµì˜ ë°ì´í„° ë¶„í¬ì™€ ëª¨ë¸ ì„±ëŠ¥ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•˜ì—¬ ìµœì ì˜ ë¯¸ì„¸ ì¡°ì • ì „ëµì„ ì œì‹œí•˜ê³ ì í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 3: Performance of different fine-tuning strategies.
> </details>

{{< table-caption >}}
| Model | CEVAL | CMMLU |
|---|---|---|
| Tiny-Llama-1.1B (Zhang et al., 2024c) | 25.02 | 24.03 |
| MiniCPM-1.2B (min, 2024) | 49.14 | 46.81 |
| Qwen1.5-1.8B-Chat (Bai et al., 2023b) | 56.84 | 54.11 |
| Phi2(2B) (Abdin et al., 2023) | 23.37 | 24.18 |
| Gemma-2b-it (Gemma Team et al., 2024) | 32.30 | 33.07 |
| CT-LLM-SFT-2B (Du et al., 2024) | 41.54 | 41.48 |
| ChatGLM-6B (GLM et al., 2024) | 38.90 | 37.48 |
| Llama2-7B (Touvron et al., 2023b) | 32.42 | 31.11 |
| OLMo-7B (Groeneveld et al., 2024b) | 35.18 | 35.55 |
| Gemma-7B (Gemma Team et al., 2024) | 42.57 | 44.20 |
| MAP-Neo-7B (Zhang et al., 2024a) | 56.97 | 55.01 |
| Llama2-13B (Touvron et al., 2023b) | 37.32 | 37.06 |
| *Steel-LLM*-1B-Chat | 41.90 | 36.08 |
| *Steel-LLM*-1B-Chat-DPO | 42.04 | 36.04 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 4ëŠ” CEVAL ë° CMMLU ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•œ í‘œì…ë‹ˆë‹¤. Steel-LLM ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë‹¤ë¥¸ ì˜¤í”ˆì†ŒìŠ¤ ë° ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬ Steel-LLMì˜ ê²½ìŸë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ëª¨ë¸ ì´ë¦„, CEVAL ì •í™•ë„, CMMLU ì •í™•ë„ê°€ ì œì‹œë˜ì–´ ìˆìœ¼ë©°, Steel-LLMì´ ìœ ì‚¬í•œ ê·œëª¨ì˜ ë‹¤ë¥¸ ëª¨ë¸ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 4: Performance comparison of models on CEVAL and CMMLU benchmarks.
> </details>

{{< table-caption >}}
| Dataset | Description |
|---|---| 
| SkyPile-150B (Wei et al., 2023) | Consisting of approximately 150 billion tokens and 620 gigabytes of cleaned text data from 233 million web pages, with rigorous filtering and deduplication to ensure quality and mitigate sensitive and biased information. |
| Wanjuan1.0 (He et al., 2023) | Composed of processed data from various sources, including web pages, encyclopedias, books, patents, textbooks, and exam questions, with a total volume of data exceeding 500 million documents, amounting to over 1TB (roughly split equally between Chinese and English data) and has undergone meticulous cleaning, deduplication, and value alignment. |
| Wikipedia-cn | Based on the July 20th, 2023 Chinese Wikipedia dump, retains 254,574 high-quality entries after filtering out special types, low-quality, sensitive, and controversial content, and includes conversions between simplified and traditional Chinese. |
| Baidu Baike | Consisting of 5,630,000 uncleaned entries from Baidu Baike, with a total size of approximately 17GB. |
| Baidu QA | Including 1.5 million high-quality encyclopedia questions and answers, spanning 492 categories, with 434 categories occurring at least 10 times, suitable for training intelligent Q&A systems |
| Zhihu QA | Including 1 million entries, with 1.5GB in size. |
| BELLE (BELLEGroup, 2023) | Train_2M_CN and train_3.5M_CN, generated by ChatGPT, contain 2 million and 3.5 million dialogue entries respectively, and were both used in this project. Note that these datasets are unverified and may contain errors. |
| Moss (Sun et al., 2024) | Containing 1.1 million Chinese and English multi-turn dialogue entries. |
| Firefly (Yang, 2023) | Comprising 1.15 million entries covering 23 common Chinese NLP tasks and includes culturally relevant data such as couplets, poetry, classical Chinese translations, prose, and Jin Yong novels, resulting in a total of 1.15 million entries. |
| Starcode (Li et al., 2023) | Including 783GB of code across 86 programming languages, with 54GB of GitHub Issues, 13GB of Jupyter notebooks, and 32GB of GitHub commits. Our project used only the C++, Python, and Java data. |{{< /table-caption >}}
> ğŸ”¼ í‘œ 5ëŠ” Steel-LLM ì‚¬ì „ í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤. ê° ë°ì´í„°ì…‹ì˜ ì´ë¦„, ì¶œì²˜, í¬ê¸°, ë°ì´í„° ìœ í˜•, ê·¸ë¦¬ê³  ë°ì´í„°ì…‹ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  SkyPile-150B, Wanjuan1.0, Wikipedia-cn, Baidu Baike, Baidu QA, Zhihu QA, BELLE, Moss, Firefly, Starcode ë“± ë‹¤ì–‘í•œ ì¤‘êµ­ì–´ ë° ì˜ì–´ ë°ì´í„°ì…‹ì´ ì‚¬ìš©ë˜ì—ˆìœ¼ë©°, ê° ë°ì´í„°ì…‹ì˜ íŠ¹ì§•ê³¼ ì „ì²˜ë¦¬ ê³¼ì •ì— ëŒ€í•œ ì •ë³´ê°€ ìš”ì•½ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 5: Pretraining Data Detailed Description
> </details>

{{< table-caption >}}
| Dataset | Description |
|---|---| 
| Infinity-Instruct-7M (BAAI, 2024) | A large-scale, high-quality instruction dataset with only 0.7M of Chinese data used in this project. |
| Wanjuan1.0 (He et al., 2023) | Consistent with the one used during the pre-training stage, but with the Chinese choice question data repurposed for fine-tuning. |
| Ruozhiba (Ruozhiba, 2024) | Questions from Baidu Tieba â€œRuozhibaâ€ were answered by GPT-4, then manually reviewed and edited for formatting errors and improved responses. |
| Self-awareness Dataset (Team, 2024a) | Consisting of various â€œWho are you?â€ questions from the EmoLLM project templates. |
| Code-Feedback (Zheng et al., 2025) | A code SFT dataset consists of 66,000 entries from various open-source code datasets and LeetCode, after undergoing a series of filtering and selection processes. |
| WebInstructSub (Yue et al., 2024) | Containing 2.33 million SFT entries across fields such as mathematics, physics, biology, chemistry, and computer science. |
| OpenHermes-2.5 (Teknium, 2023) | Consisting of samples synthesized by large models and chat samples, filtered from open-source data like Airoboros, ChatBot Arena, and Evol Instruct, totaling 1 million entries. |{{< /table-caption >}}
> ğŸ”¼ í‘œ 6ì€ ë…¼ë¬¸ì˜ ì´ˆê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ë°ì´í„°ì…‹ ì´ë¦„, ê°„ëµí•œ ì„¤ëª…, ê·¸ë¦¬ê³  ì¶œì²˜ë¥¼ í¬í•¨í•˜ì—¬ ê° ë°ì´í„°ì…‹ì˜ íŠ¹ì§•ì„ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.  ë¯¸ì„¸ ì¡°ì • ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„°ì˜ ì¢…ë¥˜ì™€ ê·œëª¨ë¥¼ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 6: Supervised Finetuning Data Detailed Description
> </details>

{{< table-caption >}}
| Operator | Description | Note |
|---|---|---|
| `chinese_convert_mapper` | Converts Chinese between Traditional Chinese, Simplified Chinese and Japanese Kanji | Mode: t2s (tradition to simple) |
| `clean_email_mapper` | Removes email information | - |
| `clean_html_mapper` | Removes HTML tags and returns plain text of all the nodes | - |
| `clean_ip_mapper` | Removes IP addresses | - |
| `clean_links_mapper` | Removes links, such as those starting with http or ftp | - |
| `clean_copyright_mapper` | Removes copyright notice at the beginning of code files (must contain the word copyright) | - |
| `expand_macro_mapper` | Expands macros usually defined at the top of TeX documents | - |
| `fix_unicode_mapper` | Fixes broken Unicodes | - |
| `punctuation_normalization_mapper` | Normalizes various Unicode punctuations to their ASCII equivalents | - |
| `remove_repeat_sentences_mapper` | Remove repeat sentences in text samples | Ignore special character and sentences shorter than 2 will not be deduplicated |
| `remove_specific_chars_mapper` | Removes any user-specified characters or substrings | - |
| `whitespace_normalization_mapper` | Normalizes various Unicode whitespaces to the normal ASCII space (U+0020) | - |
| `alphanumeric_filter` | Keeps samples with alphanumeric ratio within the specified range | [0.0, 0.9] |
| `average_line_length_filter` | Keeps samples with average line length within the specified range | [10, 150] |
| `character_repetition_filter` | Keeps samples with char-level n-gram repetition ratio within the specified range | [0.0, 0.4] |
| `maximum_line_length_filter` | Keeps samples with maximum line length within the specified range | 1000 |
| `perplexity_filter` | Keeps samples with perplexity score below the specified threshold | 1500 |
| `special_characters_filter` | Keeps samples with special-char ratio within the specified range | [0.0, 0.25] |
| `text_length_filter` | Keeps samples with total text length within the specified range | [10, 100000] |
| `word_repetition_filter` | Keeps samples with word-level n-gram repetition ratio within the specified range | [0.0, 0.5] |
| `document_simhash_deduplicator` | Deduplicates samples at document-level using SimHash | Tokenization:space; window_size:6; num_blocks:6; hamming_distance:4; lowercase:true |{{< /table-caption >}}
> ğŸ”¼ í‘œ 7ì€ ë³¸ ë…¼ë¬¸ì˜ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì‚¬ìš©ëœ Data Juicer ì—°ì‚°ìë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ì—°ì‚°ìëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” íŠ¹ì • ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. í‘œì—ëŠ” ì—°ì‚°ì ì´ë¦„, ì„¤ëª…, ê·¸ë¦¬ê³  ì¶”ê°€ì ì¸ ë…¸íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  ì˜ˆë¥¼ ë“¤ì–´, `chinese_convert_mapper`ëŠ” ì¤‘êµ­ì–´ ê°„ì²´ì™€ ë²ˆì²´ ê°„ ë³€í™˜ì„ ìˆ˜í–‰í•˜ê³ , `clean_email_mapper`ëŠ” ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì œê±°í•˜ëŠ” ë“± ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ì‘ì—…ì´ ìˆ˜í–‰ë©ë‹ˆë‹¤.  ê° ì—°ì‚°ìì˜ ì…ë ¥ê°’ ë²”ìœ„ë‚˜ íŠ¹ì§• ë“± ì¶”ê°€ì ì¸ ì •ë³´ë„ í•¨ê»˜ ì œê³µë˜ì–´ ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 7: Data Juicer Operators Used for Text Processing
> </details>

{{< table-caption >}}
| Operator | Description | Note |
|---|---|---|
| clean_copyright_mapper | Removes copyright notice at the beginning of code files (must contain the word copyright) | - |
| clean_email_mapper | Removes email information | - |
| clean_links_mapper | Removes links, such as those starting with http or ftp | - |
| fix_unicode_mapper | Fixes broken Unicodes | - |
| punctuation_normalization_mapper | Normalizes various Unicode punctuations to their ASCII equivalents | - |
| alphanumeric_filter | Keeps samples with alphanumeric ratio within the specified range | [0.546, 3.65] |
| average_line_length_filter | Keeps samples with average line length within the specified range | [10, 150] |
| character_repetition_filter | Keeps samples with char-level n-gram repetition ratio within the specified range | 0.36 |
| maximum_line_length_filter | Keeps samples with maximum line length within the specified range | 1000 |
| text_length_filter | Keeps samples with total text length within the specified range | 96714 |
| words_num_filter | Keeps samples with word count within the specified range | [20,6640] |
| word_repetition_filter | Keeps samples with word-level n-gram repetition ratio within the specified range | [10, 0.357] |
| document_simhash_deduplicator | Deduplicates samples at document-level using SimHash | Tokenization:space; window_size:6; num_blocks:6; hamming_distance:4; lowercase:true |{{< /table-caption >}}
> ğŸ”¼ í‘œ 8ì€ ì½”ë“œ ì „ì²˜ë¦¬ì— ì‚¬ìš©ëœ Data Juicer ì—°ì‚°ìë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ê° ì—°ì‚°ìì˜ ì´ë¦„, ì„¤ëª…, ê·¸ë¦¬ê³  íŠ¹ì´ì‚¬í•­(ë§Œì•½ ìˆë‹¤ë©´)ì„ í¬í•¨í•˜ì—¬ ì½”ë“œ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì‚¬ìš©ëœ ì—¬ëŸ¬ ì—°ì‚°ìë“¤ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.  í‘œëŠ” ì½”ë“œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ê° ë‹¨ê³„ì˜ ê¸°ëŠ¥ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 8: Data Juicer Operators Used for Code Processing
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}