[{"content": "| Parameters | Value |\n|---|---| \n| Layers | 18 |\n| Heads | 32 |\n| KV heads | 32 |\n| Num_experts | 6 |\n| Slots_per_expert | 1 |\n| Hidden size | 1,792 |\n| Intermediate size | 1,792 |\n| Vocab size | 151,936 |", "caption": "Table 1: Key model parameters.", "description": "\ud45c 1\uc740 Steel-LLM \ubaa8\ub378\uc758 \uc8fc\uc694 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub808\uc774\uc5b4 \uc218, \ud5e4\ub4dc \uc218, \ud788\ub4e0 \uc0ac\uc774\uc988, \uc911\uac04 \uc0ac\uc774\uc988, \uc5b4\ud718 \ud06c\uae30, \uc804\ubb38\uac00 \uc218, \uc2ac\ub86f \uc218 \ub4f1\uc758 \uc8fc\uc694 \uad6c\uc870\uc801 \ub9e4\uac1c\ubcc0\uc218\uc640 \ud559\uc2b5 \uc804\ub7b5\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 Steel-LLM\uc758 \uc544\ud0a4\ud14d\ucc98\uc640 \uc131\ub2a5\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc911\uc694\ud55c \uc694\uc18c\ub4e4\uc744 \ud55c\ub208\uc5d0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3 ARCHITECTURE"}, {"content": "| Component | Exp 1 | Exp 2 | Exp 3 | Exp 4 | Exp 5 | Exp 6 | Exp 7 | Exp 8 |\n|---|---|---|---|---|---|---|---|---|\n| FlashAttention | \u2713 | \u2713 | \u00d7 | \u2713 | \u2713 | \u2713 | \u2713 | \u00d7 |\n| SelfAttention(PyTorch) | \u00d7 | \u00d7 | \u2713 | \u00d7 | \u00d7 | \u00d7 | \u00d7 | \u2713 |\n| RoPE(CUDA) | \u2713 | \u00d7 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u00d7 |\n| RoPE(PyTorch) | \u00d7 | \u2713 | \u00d7 | \u00d7 | \u00d7 | \u00d7 | \u00d7 | \u2713 |\n| RMSNorm(CUDA) | \u00d7 | \u00d7 | \u00d7 | \u00d7 | \u2713 | \u00d7 | \u2713 | \u00d7 |\n| RMSNorm(PyTorch) | \u2713 | \u2713 | \u2713 | \u2713 | \u00d7 | \u2713 | \u00d7 | \u2713 |\n| Loss Function(Triton) | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u00d7 | \u2713 | \u00d7 |\n| Loss Function(PyTorch) | \u00d7 | \u00d7 | \u00d7 | \u00d7 | \u00d7 | \u2713 | \u00d7 | \u2713 |\n| FSDP | \u2713 | \u2713 | \u2713 | \u00d7 | \u2713 | \u2713 | \u00d7 | \u2713 |\n| FSDP(no share param) | \u00d7 | \u00d7 | \u00d7 | \u2713 | \u00d7 | \u00d7 | \u2713 | \u00d7 |\n| Speed(tokens/s/gpu) | 13400 | 12500 | 10600 | 13800 | 14600 | 13000 | 15000 | 10500 |\n| GPU Memory(GB) | 65 | 65 | 69 | 69 | 61 | 75 | 66 | 75 |", "caption": "Table 2: Comparison of different training configurations.", "description": "\ud45c 2\ub294 Steel-LLM \ubaa8\ub378 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ub2e4\uc591\ud55c \uc124\uc815\ub4e4\uc758 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  FlashAttention, ROPE(CUDA/PyTorch), RMSNorm(CUDA/PyTorch), \uc190\uc2e4 \ud568\uc218(Triton/PyTorch), FSDP(\ud30c\ub77c\ubbf8\ud130 \uacf5\uc720 \uc720\ubb34) \ub4f1\uc758 \ub2e4\uc591\ud55c \ud559\uc2b5 \ucd5c\uc801\ud654 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ud1a0\ud070/\ucd08/GPU \uc18d\ub3c4\uc640 GPU \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9 \uce21\uba74\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uac01 \uae30\ubc95\ub4e4\uc758 \ud6a8\uacfc\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 Steel-LLM \ud559\uc2b5 \ud6a8\uc728\uc744 \ucd5c\ub300 50%\uae4c\uc9c0 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \uae30\uc5ec\ud55c \ucd5c\uc801\uc758 \uc124\uc815\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4 TRAINING FRAMEWORK"}, {"content": "| Experiment | CEVAL Acc. | CMMLU Acc. | MMLU Acc. |\n|---|---|---|---|\n| Full Infinity-Instruct + Wanjuan MCQ | 32.35 | 26.32 | 25.50 |\n| 700K Chinese Infinity-Instruct + Wanjuan MCQ | 38.57 | 33.48 | 23.26 |\n| Chinese + 100% English Data | 39.21 | 33.20 | 26.73 |\n| Chinese + 20% English Data (Balanced) | 40.43 | 35.86 | 26.75 |\n| Chinese + 20% English + English MCQ | 41.90 | 36.08 | 30.82 |", "caption": "Table 3: Performance of different fine-tuning strategies.", "description": "\ud45c 3\uc740 \ubbf8\uc138 \uc870\uc815 \uc804\ub7b5\uc5d0 \ub530\ub978 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ub370\uc774\ud130 \uad6c\uc131(\uc804\uccb4 Infinity-Instruct \ub370\uc774\ud130\uc14b, Infinity-Instruct\uc758 \uc911\uad6d\uc5b4 \ub370\uc774\ud130\ub9cc \uc0ac\uc6a9, \uc911\uad6d\uc5b4\uc640 \uc601\uc5b4 \ub370\uc774\ud130\ub97c \uade0\ud615 \uc788\uac8c \uc0ac\uc6a9, \uc601\uc5b4 \ub370\uc774\ud130\ub97c 20%\ub9cc \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0, \uadf8\ub9ac\uace0 \ucd94\uac00\uc801\uc73c\ub85c \uc601\uc5b4 MCQ \ub370\uc774\ud130\ub97c \ud3ec\ud568\ud55c \uacbd\uc6b0)\uc5d0 \ub530\ub978 CEVAL, CMMLU, MMLU \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uac01 \uc804\ub7b5\uc758 \ub370\uc774\ud130 \ubd84\ud3ec\uc640 \ubaa8\ub378 \uc131\ub2a5 \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ud30c\uc545\ud558\uc5ec \ucd5c\uc801\uc758 \ubbf8\uc138 \uc870\uc815 \uc804\ub7b5\uc744 \uc81c\uc2dc\ud558\uace0\uc790 \ud569\ub2c8\ub2e4.", "section": "6.2 ABLATION STUDIES AND EVALUATION"}, {"content": "| Model | CEVAL | CMMLU |\n|---|---|---|\n| Tiny-Llama-1.1B (Zhang et al., 2024c) | 25.02 | 24.03 |\n| MiniCPM-1.2B (min, 2024) | 49.14 | 46.81 |\n| Qwen1.5-1.8B-Chat (Bai et al., 2023b) | 56.84 | 54.11 |\n| Phi2(2B) (Abdin et al., 2023) | 23.37 | 24.18 |\n| Gemma-2b-it (Gemma Team et al., 2024) | 32.30 | 33.07 |\n| CT-LLM-SFT-2B (Du et al., 2024) | 41.54 | 41.48 |\n| ChatGLM-6B (GLM et al., 2024) | 38.90 | 37.48 |\n| Llama2-7B (Touvron et al., 2023b) | 32.42 | 31.11 |\n| OLMo-7B (Groeneveld et al., 2024b) | 35.18 | 35.55 |\n| Gemma-7B (Gemma Team et al., 2024) | 42.57 | 44.20 |\n| MAP-Neo-7B (Zhang et al., 2024a) | 56.97 | 55.01 |\n| Llama2-13B (Touvron et al., 2023b) | 37.32 | 37.06 |\n| *Steel-LLM*-1B-Chat | 41.90 | 36.08 |\n| *Steel-LLM*-1B-Chat-DPO | 42.04 | 36.04 |", "caption": "Table 4: Performance comparison of models on CEVAL and CMMLU benchmarks.", "description": "\ud45c 4\ub294 CEVAL \ubc0f CMMLU \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. Steel-LLM \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\ub978 \uc624\ud508\uc18c\uc2a4 \ubc0f \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec Steel-LLM\uc758 \uacbd\uc7c1\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378 \uc774\ub984, CEVAL \uc815\ud655\ub3c4, CMMLU \uc815\ud655\ub3c4\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, Steel-LLM\uc774 \uc720\uc0ac\ud55c \uaddc\ubaa8\uc758 \ub2e4\ub978 \ubaa8\ub378\ub4e4\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "6.4 \ub17c\uc758"}, {"content": "| Dataset | Description |\n|---|---| \n| SkyPile-150B (Wei et al., 2023) | Consisting of approximately 150 billion tokens and 620 gigabytes of cleaned text data from 233 million web pages, with rigorous filtering and deduplication to ensure quality and mitigate sensitive and biased information. |\n| Wanjuan1.0 (He et al., 2023) | Composed of processed data from various sources, including web pages, encyclopedias, books, patents, textbooks, and exam questions, with a total volume of data exceeding 500 million documents, amounting to over 1TB (roughly split equally between Chinese and English data) and has undergone meticulous cleaning, deduplication, and value alignment. |\n| Wikipedia-cn | Based on the July 20th, 2023 Chinese Wikipedia dump, retains 254,574 high-quality entries after filtering out special types, low-quality, sensitive, and controversial content, and includes conversions between simplified and traditional Chinese. |\n| Baidu Baike | Consisting of 5,630,000 uncleaned entries from Baidu Baike, with a total size of approximately 17GB. |\n| Baidu QA | Including 1.5 million high-quality encyclopedia questions and answers, spanning 492 categories, with 434 categories occurring at least 10 times, suitable for training intelligent Q&A systems |\n| Zhihu QA | Including 1 million entries, with 1.5GB in size. |\n| BELLE (BELLEGroup, 2023) | Train_2M_CN and train_3.5M_CN, generated by ChatGPT, contain 2 million and 3.5 million dialogue entries respectively, and were both used in this project. Note that these datasets are unverified and may contain errors. |\n| Moss (Sun et al., 2024) | Containing 1.1 million Chinese and English multi-turn dialogue entries. |\n| Firefly (Yang, 2023) | Comprising 1.15 million entries covering 23 common Chinese NLP tasks and includes culturally relevant data such as couplets, poetry, classical Chinese translations, prose, and Jin Yong novels, resulting in a total of 1.15 million entries. |\n| Starcode (Li et al., 2023) | Including 783GB of code across 86 programming languages, with 54GB of GitHub Issues, 13GB of Jupyter notebooks, and 32GB of GitHub commits. Our project used only the C++, Python, and Java data. |", "caption": "Table 5: Pretraining Data Detailed Description", "description": "\ud45c 5\ub294 Steel-LLM \uc0ac\uc804 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc0c1\uc138 \uc124\uba85\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc774\ub984, \ucd9c\ucc98, \ud06c\uae30, \ub370\uc774\ud130 \uc720\ud615, \uadf8\ub9ac\uace0 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uac04\ub7b5\ud55c \uc124\uba85\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  SkyPile-150B, Wanjuan1.0, Wikipedia-cn, Baidu Baike, Baidu QA, Zhihu QA, BELLE, Moss, Firefly, Starcode \ub4f1 \ub2e4\uc591\ud55c \uc911\uad6d\uc5b4 \ubc0f \uc601\uc5b4 \ub370\uc774\ud130\uc14b\uc774 \uc0ac\uc6a9\ub418\uc5c8\uc73c\uba70, \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5\uacfc \uc804\ucc98\ub9ac \uacfc\uc815\uc5d0 \ub300\ud55c \uc815\ubcf4\uac00 \uc694\uc57d\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5 \uc0ac\uc804 \ud559\uc2b5"}, {"content": "| Dataset | Description |\n|---|---| \n| Infinity-Instruct-7M (BAAI, 2024) | A large-scale, high-quality instruction dataset with only 0.7M of Chinese data used in this project. |\n| Wanjuan1.0 (He et al., 2023) | Consistent with the one used during the pre-training stage, but with the Chinese choice question data repurposed for fine-tuning. |\n| Ruozhiba (Ruozhiba, 2024) | Questions from Baidu Tieba \u201cRuozhiba\u201d were answered by GPT-4, then manually reviewed and edited for formatting errors and improved responses. |\n| Self-awareness Dataset (Team, 2024a) | Consisting of various \u201cWho are you?\u201d questions from the EmoLLM project templates. |\n| Code-Feedback (Zheng et al., 2025) | A code SFT dataset consists of 66,000 entries from various open-source code datasets and LeetCode, after undergoing a series of filtering and selection processes. |\n| WebInstructSub (Yue et al., 2024) | Containing 2.33 million SFT entries across fields such as mathematics, physics, biology, chemistry, and computer science. |\n| OpenHermes-2.5 (Teknium, 2023) | Consisting of samples synthesized by large models and chat samples, filtered from open-source data like Airoboros, ChatBot Arena, and Evol Instruct, totaling 1 million entries. |", "caption": "Table 6: Supervised Finetuning Data Detailed Description", "description": "\ud45c 6\uc740 \ub17c\ubb38\uc758 \ucd08\uac70\ub300 \uc5b8\uc5b4 \ubaa8\ub378 \ubbf8\uc138 \uc870\uc815 \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc0c1\uc138 \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub370\uc774\ud130\uc14b \uc774\ub984, \uac04\ub7b5\ud55c \uc124\uba85, \uadf8\ub9ac\uace0 \ucd9c\ucc98\ub97c \ud3ec\ud568\ud558\uc5ec \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5\uc744 \uc790\uc138\ud788 \uc124\uba85\ud569\ub2c8\ub2e4.  \ubbf8\uc138 \uc870\uc815 \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc758 \uc885\ub958\uc640 \uaddc\ubaa8\ub97c \ud30c\uc545\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "6. SUPERVISED FINETUNING"}, {"content": "| Operator | Description | Note |\n|---|---|---|\n| `chinese_convert_mapper` | Converts Chinese between Traditional Chinese, Simplified Chinese and Japanese Kanji | Mode: t2s (tradition to simple) |\n| `clean_email_mapper` | Removes email information | - |\n| `clean_html_mapper` | Removes HTML tags and returns plain text of all the nodes | - |\n| `clean_ip_mapper` | Removes IP addresses | - |\n| `clean_links_mapper` | Removes links, such as those starting with http or ftp | - |\n| `clean_copyright_mapper` | Removes copyright notice at the beginning of code files (must contain the word copyright) | - |\n| `expand_macro_mapper` | Expands macros usually defined at the top of TeX documents | - |\n| `fix_unicode_mapper` | Fixes broken Unicodes | - |\n| `punctuation_normalization_mapper` | Normalizes various Unicode punctuations to their ASCII equivalents | - |\n| `remove_repeat_sentences_mapper` | Remove repeat sentences in text samples | Ignore special character and sentences shorter than 2 will not be deduplicated |\n| `remove_specific_chars_mapper` | Removes any user-specified characters or substrings | - |\n| `whitespace_normalization_mapper` | Normalizes various Unicode whitespaces to the normal ASCII space (U+0020) | - |\n| `alphanumeric_filter` | Keeps samples with alphanumeric ratio within the specified range | [0.0, 0.9] |\n| `average_line_length_filter` | Keeps samples with average line length within the specified range | [10, 150] |\n| `character_repetition_filter` | Keeps samples with char-level n-gram repetition ratio within the specified range | [0.0, 0.4] |\n| `maximum_line_length_filter` | Keeps samples with maximum line length within the specified range | 1000 |\n| `perplexity_filter` | Keeps samples with perplexity score below the specified threshold | 1500 |\n| `special_characters_filter` | Keeps samples with special-char ratio within the specified range | [0.0, 0.25] |\n| `text_length_filter` | Keeps samples with total text length within the specified range | [10, 100000] |\n| `word_repetition_filter` | Keeps samples with word-level n-gram repetition ratio within the specified range | [0.0, 0.5] |\n| `document_simhash_deduplicator` | Deduplicates samples at document-level using SimHash | Tokenization:space; window_size:6; num_blocks:6; hamming_distance:4; lowercase:true |", "caption": "Table 7: Data Juicer Operators Used for Text Processing", "description": "\ud45c 7\uc740 \ubcf8 \ub17c\ubb38\uc758 \ub370\uc774\ud130 \uc804\ucc98\ub9ac \uacfc\uc815\uc5d0\uc11c \uc0ac\uc6a9\ub41c Data Juicer \uc5f0\uc0b0\uc790\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc5f0\uc0b0\uc790\ub294 \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\ub97c \uc815\uc81c\ud558\uace0 \uc804\ucc98\ub9ac\ud558\ub294 \ud2b9\uc815 \uae30\ub2a5\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uc5f0\uc0b0\uc790 \uc774\ub984, \uc124\uba85, \uadf8\ub9ac\uace0 \ucd94\uac00\uc801\uc778 \ub178\ud2b8\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc608\ub97c \ub4e4\uc5b4, `chinese_convert_mapper`\ub294 \uc911\uad6d\uc5b4 \uac04\uccb4\uc640 \ubc88\uccb4 \uac04 \ubcc0\ud658\uc744 \uc218\ud589\ud558\uace0, `clean_email_mapper`\ub294 \uc774\uba54\uc77c \uc8fc\uc18c\ub97c \uc81c\uac70\ud558\ub294 \ub4f1 \ub2e4\uc591\ud55c \uc804\ucc98\ub9ac \uc791\uc5c5\uc774 \uc218\ud589\ub429\ub2c8\ub2e4.  \uac01 \uc5f0\uc0b0\uc790\uc758 \uc785\ub825\uac12 \ubc94\uc704\ub098 \ud2b9\uc9d5 \ub4f1 \ucd94\uac00\uc801\uc778 \uc815\ubcf4\ub3c4 \ud568\uaed8 \uc81c\uacf5\ub418\uc5b4 \uc2e4\uc81c \ub370\uc774\ud130 \uc804\ucc98\ub9ac \uacfc\uc815\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "3 ARCHITECTURE"}, {"content": "| Operator | Description | Note |\n|---|---|---|\n| clean_copyright_mapper | Removes copyright notice at the beginning of code files (must contain the word copyright) | - |\n| clean_email_mapper | Removes email information | - |\n| clean_links_mapper | Removes links, such as those starting with http or ftp | - |\n| fix_unicode_mapper | Fixes broken Unicodes | - |\n| punctuation_normalization_mapper | Normalizes various Unicode punctuations to their ASCII equivalents | - |\n| alphanumeric_filter | Keeps samples with alphanumeric ratio within the specified range | [0.546, 3.65] |\n| average_line_length_filter | Keeps samples with average line length within the specified range | [10, 150] |\n| character_repetition_filter | Keeps samples with char-level n-gram repetition ratio within the specified range | 0.36 |\n| maximum_line_length_filter | Keeps samples with maximum line length within the specified range | 1000 |\n| text_length_filter | Keeps samples with total text length within the specified range | 96714 |\n| words_num_filter | Keeps samples with word count within the specified range | [20,6640] |\n| word_repetition_filter | Keeps samples with word-level n-gram repetition ratio within the specified range | [10, 0.357] |\n| document_simhash_deduplicator | Deduplicates samples at document-level using SimHash | Tokenization:space; window_size:6; num_blocks:6; hamming_distance:4; lowercase:true |", "caption": "Table 8: Data Juicer Operators Used for Code Processing", "description": "\ud45c 8\uc740 \ucf54\ub4dc \uc804\ucc98\ub9ac\uc5d0 \uc0ac\uc6a9\ub41c Data Juicer \uc5f0\uc0b0\uc790\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc5f0\uc0b0\uc790\uc758 \uc774\ub984, \uc124\uba85, \uadf8\ub9ac\uace0 \ud2b9\uc774\uc0ac\ud56d(\ub9cc\uc57d \uc788\ub2e4\uba74)\uc744 \ud3ec\ud568\ud558\uc5ec \ucf54\ub4dc \ub370\uc774\ud130\ub97c \uc815\uc81c\ud558\uace0 \uc804\ucc98\ub9ac\ud558\ub294 \uacfc\uc815\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc5ec\ub7ec \uc5f0\uc0b0\uc790\ub4e4\uc744 \uc0c1\uc138\ud788 \uc124\uba85\ud569\ub2c8\ub2e4.  \ud45c\ub294 \ucf54\ub4dc \uc804\ucc98\ub9ac \ud30c\uc774\ud504\ub77c\uc778\uc5d0\uc11c \uac01 \ub2e8\uacc4\uc758 \uae30\ub2a5\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "4. \ucf54\ub4dc \uc804\ucc98\ub9ac"}]