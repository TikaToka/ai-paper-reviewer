[{"content": "| Model | #Params | MathVista ALL | MathVista ARI | MathVista LOG | MathVista STA | MathVista VQA | MathVerse ALL | MathVerse VI | MathVerse VD | MathVerse VO | MathVerse TD | MathVerse TL | MathVerse TO |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Random | - | 17.9 | 13.8 | 13.4 | 14.3 | 26.3 | 12.4 | 12.4 | 12.4 | 12.4 | 12.4 | 12.4 | 12.4 |\n| Human | - | 60.3 | 59.2 | 40.7 | 63.9 | 55.9 | 64.9 | 61.4 | 68.3 | 66.7 | 71.2 | 70.9 | 41.7 |\n| _Open-Source General MLLMs_ |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| mPLUG-Owl2-7B [Ye et al. (2023)] | 7B | 22.2 | 19.2 | 13.5 | 21.4 | 27.9 | 10.3 | 11.1 | 9.4 | 8.0 | 11.6 | 11.4 | 13.8 |\n| MiniGPT4-7B [Zhu et al. (2023)] | 7B | 23.1 | 32.0 | 10.8 | 17.9 | 30.2 | 12.2 | 12.5 | 14.8 | 8.7 | 12.3 | 12.9 | 13.4 |\n| LLaVA-1.5-13B [Liu et al. (2024a)] | 13B | 27.7 | 28.6 | 10.8 | 22.9 | 30.2 | 12.7 | 12.6 | 12.7 | 9.0 | 17.1 | 12.0 | 22.6 |\n| SPHINX-V2-13B [Lin et al. (2023)] | 13B | 36.7 | 33.4 | 24.3 | 51.5 | 43.0 | 16.1 | 16.4 | 15.6 | 16.2 | 20.8 | 14.1 | 14.0 |\n| SPHINX-MoE [Lin et al. (2023)] | 8x7B | 42.6 | 43.0 | 14.4 | 50.8 | 43.3 | 22.8 | 21.1 | 19.6 | 18.3 | 33.3 | 21.9 | 23.1 |\n| LLaVA-NeXT-34B [Liu et al. (2024b)] | 34B | 46.5 | - | - | - | - | 34.6 | 35.2 | 28.9 | 22.4 | 49.0 | 37.6 | 30.1 |\n| InternLM-XComposer2-VL [Dong et al. (2024c)] | 7B | 57.6 | 51.6 | 13.5 | 62.8 | 39.7 | 25.9 | 20.1 | 24.4 | 19.8 | 36.9 | 28.3 | 42.5 |\n| Deepseek-VL [Lu et al. (2024)] | 7B | 34.9 | 38.8 | 18.9 | 33.2 | 34.6 | 19.3 | 20.2 | 18.4 | 11.8 | 23.0 | 23.2 | 23.1 |\n| InternVL2-8B [Chen et al. (2024d)] | 8B | 58.3 | 56.4 | 10.8 | 68.8 | 49.7 | 35.9 | 32.2 | 30.9 | 27.7 | 39.0 | 33.8 | 36.0 |\n| Qwen2-VL [Wang et al. (2024b)] | 7B | 58.9 | 57.5 | 24.3 | 43.1 | 58.1 | 33.6 | 31.3 | 30.3 | 28.1 | 37.4 | 33.5 | 35.0 |\n| _Open-Source Math MLLMs (Large-Scale Training)_ |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| G-LLaVA-7B [Gao et al. (2023)] | 7B | 25.1 | 19.4 | 15.2 | 15.1 | 28.7 | 16.6 | 17.2 | 14.6 | 9.4 | 20.9 | 20.7 | 21.1 |\n| Math-LLaVA-13B [Shi et al. (2024)] | 13B | 46.6 | 40.7 | 23.3 | 42.3 | 33.5 | 22.9 | 24.5 | 21.7 | 16.1 | 27.3 | 24.9 | 27.0 |\n| Math-PUMA-Qwen2-7B [Zhuang et al. (2024)] | 7B | 47.9 | 46.2 | 21.6 | 55.8 | 30.2 | 33.6 | 33.4 | 31.6 | 26.0 | 42.1 | 35.0 | 39.8 |\n| Math-PUMA-DeepSeek-Math [Zhuang et al. (2024)] | 7B | 44.7 | 41.9 | 8.1 | 50.8 | 31.3 | 31.8 | 33.6 | 31.6 | 14.7 | 43.4 | 35.4 | 47.5 |\n| MAVIS-7B [Zhang et al. (2024d)] | 7B | - | - | - | - | - | 35.2 | 34.1 | 29.7 | 31.8 | 43.2 | 37.2 | - |\n| InfiMM-Math [Han et al. (2024)] | 7B | - | - | - | - | - | 34.5 | 38.1 | 32.4 | 15.8 | 46.7 | 32.4 | - |\n| MultiMath-7B [Peng et al. (2024)] | 7B | 50.0 | 42.2 | 23.3 | 64.9 | 49.2 | 27.7 | 28.1 | 25.9 | 15.0 | 34.8 | 30.8 | 35.3 |\n| URSA-7B [Luo et al. (2025)] | 7B | 59.8 | 53.5 | 21.6 | 57.1 | 40.2 | 45.7 | 46.4 | 43.9 | 28.6 | 55.3 | 48.3 | 51.8 |\n| AStar (Ours, Training-free Reasoning) | 7B | 63.5 | 63.1 | 61.3 | 68.8 | 60.1 | 54.0 | 46.8 | 61.8 | 46.4 | 53.9 | 44.3 | 53.9 |", "caption": "Table 1: Evaluation of AStar\u2019s reasoning capabilities on MathVista and MathVerse testmini. The best results are highlighted in bold. For MathVista, we pick five categories: ALL (overall accuracy), ARI (arithmetic reasoning), LOG (logical reasoning), STA (statistical reasoning), and VQA (visual question answering). For MathVerse, we present seven categories: ALL (overall accuracy), VI (vision intensive), VD (vision dominant), VO (vision only), TD (text dominant), TL (text lite), and TO (text only).", "description": "\ud45c 1\uc740 MathVista\uc640 MathVerse testmini \ub370\uc774\ud130\uc14b\uc5d0\uc11c AStar\uc758 \ucd94\ub860 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. MathVista\uc758 \uacbd\uc6b0 \uc804\uccb4 \uc815\ud655\ub3c4(ALL), \uc0b0\uc220 \ucd94\ub860(ARI), \ub17c\ub9ac \ucd94\ub860(LOG), \ud1b5\uacc4 \ucd94\ub860(STA), \uc2dc\uac01\uc801 \uc9c8\ubb38 \ub2f5\ubcc0(VQA) \ub4f1 \ub2e4\uc12f \uac00\uc9c0 \ubc94\uc8fc\ub85c \ub098\ub204\uc5b4 \ud3c9\uac00\ud558\uc600\uace0, MathVerse\uc758 \uacbd\uc6b0 \uc804\uccb4 \uc815\ud655\ub3c4(ALL), \uc2dc\uac01 \uc9d1\uc911(VI), \uc2dc\uac01 \uc6b0\uc138(VD), \uc2dc\uac01 \uc804\uc6a9(VO), \ud14d\uc2a4\ud2b8 \uc6b0\uc138(TD), \ud14d\uc2a4\ud2b8 \uacbd\ub7c9(TL), \ud14d\uc2a4\ud2b8 \uc804\uc6a9(TO) \ub4f1 \uc77c\uacf1 \uac00\uc9c0 \ubc94\uc8fc\ub85c \ub098\ub204\uc5b4 \ud3c9\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4. \uac00\uc7a5 \uc88b\uc740 \uacb0\uacfc\ub294 \uad75\uc740 \uae00\uc528\uccb4\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.2. Performance Comparison with Baselines"}, {"content": "| Model | #Params | MMStar | ChartQA |\n|---|---|---|---| \n| *2B-Scale Baselines* |  |  |  |\n| Qwen2-VL [Wang et al. (2024b)] | 2B | 48.0 | 73.5 |\n| Mulberry [Yao et al. (2024)] | 2B | 51.3 | 77.7 |\n| AStar (Ours) | 2B | **51.7** | **78.3** |\n| \u2265 7B-Scale Baselines |  |  |  |\n| Deepseek-VL [Lu et al. (2024)] | 7B | 36.1 | 59.1 |\n| Qwen2-VL [Wang et al. (2024b)] | 7B | 60.7 | 83.0 |\n| InternVL2 [Chen et al. (2024d)] | 8B | 61.5 | 83.3 |\n| Insight-V [Dong et al. (2024d)] | 7B | 61.5 | 82.3 |\n| Mulberry [Yao et al. (2024)] | 7B | 61.3 | **83.9** |\n| LLaVA-CoT [Xu et al. (2024)] | 11B | 58.1 | - |\n| LlamaV-o1 [Thawakar et al. (2025)] | 11B | 59.5 | - |\n| AStar (Ours) | 7B | **61.7** | **83.9** |", "caption": "Table 2: Comparison with powerful baselines on general VQA datasets: MMStar and ChartQA.", "description": "\ud45c 2\ub294 MMStar\uc640 ChartQA\ub77c\ub294 \ub450 \uac00\uc9c0 \uc77c\ubc18\uc801\uc778 VQA(Visual Question Answering) \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uac15\ub825\ud55c \uae30\uc900 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud55c AStar\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  AStar \ubaa8\ub378\uc740 2B\uc640 7B \ud30c\ub77c\ubbf8\ud130 \ud06c\uae30\uc758 \ub450 \uac00\uc9c0 \ubc84\uc804\uc73c\ub85c \ud3c9\uac00\ub418\uc5c8\uc73c\uba70, \uac01 \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \uc815\ud655\ub3c4(Accuracy)\ub97c \ube44\uad50\ud558\uc5ec AStar\uc758 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud55c \ube44\uad50\ub97c \ud1b5\ud574 AStar\uc758 \ud655\uc7a5\uc131\uacfc \ud6a8\uc728\uc131\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \ud2b9\ud788, \uae30\uc874\uc758 \uac15\ub825\ud55c VQA \ubaa8\ub378\ub4e4\uacfc\uc758 \ube44\uad50\ub97c \ud1b5\ud574 AStar\uc758 \uacbd\uc7c1\ub825\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "4.2. \uae30\uc900 \ubaa8\ub378\uacfc\uc758 \uc131\ub2a5 \ube44\uad50"}, {"content": "| Methods | Open-Source Only | Training-Free | Data Volume | Search Iter. \u2193 | MathVista Acc. \u2191 | MMStar Acc. \u2191 | GAOKAO Acc. \u2191 |\n|---|---|---|---|---|---|---|---| \n| AR-MCTS [Dong et al. (2024a)](https://arxiv.org/html/2502.02339/bib13.png) | \u2713 | \u2717 | 34.5K | 32.0 | **64.1** | - | 37.4 |\n| Mulberry [Yao et al. (2024)](https://arxiv.org/html/2502.02339/bib73.png) | \u2717 | \u2717 | 260K | - | 63.1 | 61.3 | - |\n| Ours | \u2713 | \u2713 | 0.5K | **5.0** | 63.5 | **61.7<sup>0.4\u2191</sup>** | **49.7<sup>12.3\u2191</sup>** |", "caption": "Table 3: Comparison with leading multimodal tree search methods. AStar achieves competitive performance with only 0.5K prior data and 5 search iterations per sample, demonstrating superior efficiency and effectiveness.", "description": "\ud45c 3\uc740 \uc120\ud589 \uc5f0\uad6c\ub4e4\uc758 \ub2e4\uc591\ud55c \ub2e4\uc911 \ubaa8\ub4dc \ud2b8\ub9ac \uae30\ubc18 \uac80\uc0c9 \ubc29\ubc95\ub4e4\uacfc AStar \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  AStar\ub294 \uae30\uc874 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \ud6e8\uc52c \uc801\uc740 \ub370\uc774\ud130(0.5K)\uc640 \uc0d8\ud50c \ub2f9 5\ubc88\uc758 \uac80\uc0c9 \ubc18\ubcf5\ub9cc\uc73c\ub85c\ub3c4 \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\uc5ec \ud6a8\uc728\uc131\uacfc \ud6a8\uacfc\uc131 \uba74\uc5d0\uc11c \uc6b0\uc218\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 AStar\uac00 \uc81c\ud55c\ub41c \ub370\uc774\ud130\ub85c\ub3c4 \uace0\ucc28\uc6d0\uc801 \uc0ac\uace0 \ud328\ud134\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ucd94\ucd9c\ud558\uace0 \ud65c\uc6a9\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Model Setting | MathVista | MathVerse | Average |\n|---|---|---|---|\n| AStar | **63.5** | **54.0** | **58.8** |\n| - w/o thought cards (RA) | 58.9 | 48.5 | 53.7<sup>**-5.1**\u2193</sup> |\n| - w/o card match (RC) | 61.3 | 50.3 | 55.8<sup>**-3.0**\u2193</sup> |\n| - w/o verification (RS) | 62.0 | 51.6 | 56.8<sup>**-2.0**\u2193</sup> |\n| - w/o verification (SC) | 63.0 | 51.8 | 57.4<sup>**-1.4**\u2193</sup> |", "caption": "Table 4: Ablation results on AStar-7B. \u2018RA\u2019, \u2018RC\u2019, \u2018RS\u2019, \u2018SC\u2019 denotes \u2018random actions\u2019, \u2018random card\u2019, \u2018random selection\u2019, and \u2018self-consistency\u2019, respectively. We observe that every component is important for optimal performance.", "description": "\ud45c 4\ub294 AStar-7B \ubaa8\ub378\uc5d0 \ub300\ud55c ablation study \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uad6c\uc131 \uc694\uc18c(thought card \uc0dd\uc131, card \ub9e4\uce6d, \ucd94\ub860 \ub2e8\uacc4 \uc120\ud0dd, self-consistency \uac80\uc99d)\uc758 \uc911\uc694\uc131\uc744 \ud655\uc778\ud558\uae30 \uc704\ud574, \uac01 \uad6c\uc131 \uc694\uc18c\ub97c \uc81c\uac70\ud55c \uacbd\uc6b0\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4.  \uacb0\uacfc\uc801\uc73c\ub85c \ubaa8\ub4e0 \uad6c\uc131 \uc694\uc18c\uac00 \ucd5c\uc801\uc758 \uc131\ub2a5 \ub2ec\uc131\uc5d0 \uc911\uc694\ud558\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'RA'\ub294 \ubb34\uc791\uc704 \ud589\ub3d9, 'RC'\ub294 \ubb34\uc791\uc704 \uce74\ub4dc, 'RS'\ub294 \ubb34\uc791\uc704 \uc120\ud0dd, 'SC'\ub294 self-consistency\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uac01 \uc694\uc18c\ub97c \uc81c\uac70\ud588\uc744 \ub54c MathVista\uc640 MathVerse \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \uc815\ud655\ub3c4\uac00 \ubaa8\ub450 \uac10\uc18c\ud558\uc5ec, \uac01 \uad6c\uc131 \uc694\uc18c\uc758 \uc911\uc694\uc131\uc744 \uc785\uc99d\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Character | Meaning |\n|---|---| \n| \\(\\pi_{\\theta}\\) | policy MLLM |\n| I | task instruction |\n| \\(\\mathcal{D}_{I}\\) | demonstration examples of I, which is \\(\\phi\\) in zero-shot settings |\n| \\(D_{s}\\) | seed data |\n| \\(D_{t}\\) | test data |\n| \\(x_{t}\\) | multimodal test problem, consisting of input question \\(q\\) and images \\(i\\) |\n| \\(y_{d}\\) | decoded answer |\n| \\(y_{g}\\) | gold standard answer |\n| \\(y_{t}\\) | reasoning trajectory / solution |\n| T | number of reasoning steps |\n| \\(T_{d}\\) | number of tokens in decoded answer \\(y_{p}\\) |\n| \\(a_{t}\\) | t-th decoded answer token of \\(y_{d}\\) |\n| \\(s_{t}\\) | t-th reasoning step of trajectory \\(y_{t}\\) |\n| \\(S_{t}\\) | t-th state, which consists of input x and preceding reasoning steps \\((s_{1},s_{2},...,s_{t-1})\\) |\n| \\(a_{t}\\) | t-th action based on the previous state \\(S_{t-1}\\) |\n| s | node s in the tree structure |\n| p | parent node of s |\n| \\(Q(s)\\) | reward value of node s |\n| \\(p_{\\varphi}\\) | process reward model |\n| \\(o_{\\psi}\\) | outcome reward model |", "caption": "Table 5: Notation Table", "description": "\ud45c 5\ub294 \ub17c\ubb38\uc758 A. Preliminaries \uc139\uc158\uc5d0 \uc788\ub294 \ud45c\ub85c, \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \ub2e4\uc591\ud55c \uae30\ud638\uc640 \uc57d\uc5b4\uc5d0 \ub300\ud55c \uc815\uc758\ub97c \ub2f4\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uae30\ud638\ub294 \uc601\uc5b4\ub85c \ub41c \uc758\ubbf8\uc640 \ud568\uaed8 \uc124\uba85\ub418\uc5b4 \uc788\uc5b4 \ub17c\ubb38 \uc804\uccb4\ub97c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.  \ud2b9\ud788, MLLM \ucd94\ub860, MCTS, \uac80\uc99d \ubc29\ubc95 \ub4f1 \ub17c\ubb38\uc758 \ud575\uc2ec \uac1c\ub150\uc744 \uc774\ud574\ud558\ub294 \ub370 \ud544\uc694\ud55c \uae30\ud638\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "A. Preliminaries"}, {"content": "| Method | Open-Source Only | Data Volume | Training-Free | Training Cost \u2193 | Inference Cost \u2193 |\n|---|---|---|---|---|---| \n| AR-MCTS [Dong et al. (2024a)](https://arxiv.org/html/2502.02339/bib13.png) | \u2713 | 34.5K | \u2713 | - | High |\n| Mulberry [Yao et al. (2024)](https://arxiv.org/html/2502.02339/bib73.png) | \u2717 | 260K | \u2717 | High | Low |\n| LLaVA-CoT [Xu et al. (2024)](https://arxiv.org/html/2502.02339/bib70.png) | \u2717 | 100K | \u2717 | High | Low |\n| URSA [Luo et al. (2025)](https://arxiv.org/html/2502.02339/bib43.png) | \u2717 | 1100K | \u2717 | High | Low |\n| LlamaV-o1 [Thawakar et al. (2025)](https://arxiv.org/html/2502.02339/bib59.png) | \u2717 | 118K | \u2717 | High | Low |\n| AStar (Ours) | \u2713 | 0.5K | \u2713 | Low | Low |", "caption": "Table 6: Method Comparison: Recent Multimodal Structured Reasoning Approaches.", "description": "\ud45c 6\uc740 \ucd5c\uadfc \ub2e4\uc591\ud55c \uc5f0\uad6c\uc5d0\uc11c \uc81c\uc2dc\ub41c \ub2e4\uc911 \ubaa8\ub4dc \uad6c\uc870\uc801 \ucd94\ub860 \ubc29\ubc95\ub4e4\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  \uac01 \ubc29\ubc95\uc758 \uc624\ud508\uc18c\uc2a4 \uc5ec\ubd80, \uc0ac\uc6a9\ub41c \ub370\uc774\ud130 \ud06c\uae30, \uc0ac\uc804 \ud559\uc2b5 \uc5c6\uc774 \uc0ac\uc6a9 \uac00\ub2a5\ud55c\uc9c0 \uc5ec\ubd80, \ud559\uc2b5 \ube44\uc6a9, \ucd94\ub860 \ube44\uc6a9 \ub4f1 \ub2e4\uc12f \uac00\uc9c0 \uce21\uba74\uc744 \ube44\uad50\ud558\uc5ec \uac01 \ubc29\ubc95\uc758 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  AStar\ub97c \ud3ec\ud568\ud55c \uc5ec\ub7ec \ubc29\ubc95\ub4e4\uc774 \ube44\uad50 \ub300\uc0c1\uc5d0 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, AStar\uc758 \ud6a8\uc728\uc131 \ubc0f \uc131\ub2a5 \uc6b0\uc704\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"content": "| Type | Method | Action Space |\n|---|---|---|\n| LLM | Tree-of-Thought Yao et al. (2023) | a<sub>3</sub>: one-step thought |\n|  | RAP Hao et al. (2023) | a<sub>5</sub>: divide and conquer |\n|  | ReST-MCTS* Zhang et al. (2024b) | a<sub>3</sub>: one-step thought |\n|  | MCTSr Zhang et al. (2024a) | a<sub>4</sub>: chain-of-thought, a<sub>6</sub>: self-reflection |\n| MLLM | AR-MCTS Dong et al. (2024a) | a<sub>3</sub>: one-step thought |\n|  | LLaVA-CoT Xu et al. (2024) | a<sub>4</sub>: chain-of-thought |\n|  | URSA Luo et al. (2025) | a<sub>4</sub>: chain-of-thought |\n| Ours |  | a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>, a<sub>4</sub>, a<sub>5</sub>, a<sub>6</sub> |", "caption": "Table 7: Comparison with other search-based LLM and MLLM methods. Note that, most method contain limited space. In contrast, we define a rich set of reasoning actions, thus enhancing the upper bound of model reasoning capabilities.", "description": "\ud45c 7\uc740 \ub2e4\uc591\ud55c \uac80\uc0c9 \uae30\ubc18 LLM \ubc0f MLLM \ubc29\ubc95\ub4e4\uacfc\uc758 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub300\ubd80\ubd84\uc758 \uae30\uc874 \ubc29\ubc95\ub4e4\uc740 \uc81c\ud55c\ub41c \ud589\ub3d9 \uacf5\uac04\uc744 \uac00\uc9c0\uace0 \uc788\ub294 \ubc18\uba74, \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ud558\ub294 AStar \ubc29\ubc95\uc740 \ud48d\ubd80\ud55c \ucd94\ub860 \ud589\ub3d9 \uc9d1\ud569\uc744 \uc815\uc758\ud558\uc5ec \ubaa8\ub378\uc758 \ucd94\ub860 \ub2a5\ub825 \uc0c1\ud55c\uc120\uc744 \ub192\uc785\ub2c8\ub2e4.  \uc989, AStar\ub294 \ub2e4\uc591\ud558\uace0 \ubcf5\uc7a1\ud55c \ucd94\ub860 \uc791\uc5c5\uc5d0 \uc720\uc5f0\ud558\uac8c \ub300\ucc98\ud560 \uc218 \uc788\ub3c4\ub85d \ub354\uc6b1 \ud655\uc7a5\ub41c \ud589\ub3d9 \uc138\ud2b8\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \ubc29\ubc95\uc758 \uc885\ub958(LLM \ub610\ub294 MLLM), \uc0ac\uc6a9\ub41c \ubc29\ubc95, \ud589\ub3d9 \uacf5\uac04 \ub4f1\uc744 \ube44\uad50\ud558\uc5ec AStar\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "B. \uc54c\uace0\ub9ac\uc998 \uc138\ubd80 \uc815\ubcf4"}, {"content": "| Type | Dataset | Evaluation Dimensions |\n|---|---|---|\n| Visual Question Answering | ChartQA Masry et al. (2022) | chart understanding and reasoning |\n|  | MMStar Chen et al. (2024b) | 6 core capabilities, like scientific reasoning |\n| Mathematics | MathVista Lu et al. (2023) | 12 core capabilities, like arithmetic reasoning |\n|  | MathVerse Zhang et al. (2025) | 6 distinct versions-text-dominant |\n|  | MathVision Wang et al. (2024a) | 16 mathematical domains |\n| Commonsense and science | GAOKAO-MM Zong & Qiu (2024) | 8 subjects, like history |", "caption": "Table 8: Comparison with other search-based LLM and MLLM methods. Note that, most method contain limited space. In contrast, we define a rich set of reasoning actions, thus enhancing the upper bound of model reasoning capabilities.", "description": "\ud45c 8\uc740 \ub2e4\uc591\ud55c \uac80\uc0c9 \uae30\ubc18 LLM \ubc0f MLLM \ubc29\ubc95\ub4e4\uacfc\uc758 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub300\ubd80\ubd84\uc758 \uae30\uc874 \ubc29\ubc95\ub4e4\uc740 \uc81c\ud55c\ub41c \ud589\ub3d9 \uacf5\uac04\uc744 \uac00\uc9c0\uace0 \uc788\ub294 \ubc18\uba74, \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ud558\ub294 AStar\ub294 \ud48d\ubd80\ud55c \ucd94\ub860 \ud589\ub3d9 \uc9d1\ud569\uc744 \uc815\uc758\ud558\uc5ec \ubaa8\ub378\uc758 \ucd94\ub860 \ub2a5\ub825 \uc0c1\ud55c\uc120\uc744 \ub192\uc600\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub97c \ud1b5\ud574 AStar\uac00 \uc5b4\ub5bb\uac8c \ub2e4\uc591\ud55c \ucd94\ub860 \uc791\uc5c5\uc5d0 \uc801\ud569\ud55c\uc9c0, \uadf8\ub9ac\uace0 \uae30\uc874 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294\uc9c0 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "7. Related Work"}, {"content": "| Hyperparameter | Value | Description |\n|---|---|---|\n| temperature | 0.8 | vllm inference settings |\n| top-p | 0.9 |  |\n| top-k | 40 |  |\n| repetition penalty | 1.0 |  |\n| max tokens | 1024 |  |\n| maximum tree depth  $d_{max}$ | 5 | MCTS |\n| exploration weight  $w$ | 2.0 |  |\n| predefined terminal threshold  $c$ | 0.90 |  |\n| balance factor  $k$ | 0.95 | VOC-based optimal path selection in *Sec. [3.2](https://arxiv.org/html/2502.02339v1#S3.SS2)* Phase 2 |", "caption": "Table 9: All hyperparameters utilized in this paper.", "description": "\ud45c 9\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ubaa8\ub4e0 \ucd08\ub9e4\uac1c\ubcc0\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ucd08\ub9e4\uac1c\ubcc0\uc218\ub294  \ubaa8\ub378\uc758 \ucd94\ub860 \uacfc\uc815\uc5d0\uc11c \uc5b4\ub5a4 \uc5ed\ud560\uc744 \ud558\ub294\uc9c0, \uadf8\ub9ac\uace0 \ud574\ub2f9 \uac12\uc774 \uc5b4\ub5bb\uac8c \uc124\uc815\ub418\uc5c8\ub294\uc9c0\uc5d0 \ub300\ud55c \uc124\uba85\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc628\ub3c4(temperature), top-p, top-k\uc640 \uac19\uc740 \ucd08\ub9e4\uac1c\ubcc0\uc218\ub294 \ubaa8\ub378\uc758 \ud655\ub960\uc801 \ud589\ub3d9\uc744 \uc81c\uc5b4\ud558\uba70, \ubc18\ubcf5 \ud398\ub110\ud2f0(repetition penalty)\ub294 \uc0dd\uc131 \ud14d\uc2a4\ud2b8\uc758 \ubc18\ubcf5\uc744 \uc81c\ud55c\ud569\ub2c8\ub2e4.  \ucd5c\ub300 \ud1a0\ud070 \uc218(max tokens)\ub294 \ubaa8\ub378\uc774 \uc0dd\uc131\ud558\ub294 \ud1a0\ud070\uc758 \ucd5c\ub300 \uac1c\uc218\ub97c \uc9c0\uc815\ud558\uba70, \ucd5c\ub300 \ud2b8\ub9ac \uae4a\uc774(maximum tree depth)\ub294 MCTS \uc54c\uace0\ub9ac\uc998\uc5d0\uc11c \ud2b8\ub9ac\uc758 \ucd5c\ub300 \uae4a\uc774\ub97c \uc81c\ud55c\ud569\ub2c8\ub2e4. \ud0d0\ud5d8 \uac00\uc911\uce58(exploration weight)\ub294 MCTS \uc54c\uace0\ub9ac\uc998\uc758 \ud0d0\ud5d8-\ud65c\uc6a9 \uade0\ud615\uc744 \uc870\uc808\ud558\uba70, \ubbf8\ub9ac \uc815\uc758\ub41c \uc885\uacb0 \uc784\uacc4\uac12(predefined terminal threshold)\uc740 MCTS \uc54c\uace0\ub9ac\uc998\uc774 \uc885\ub8cc\ub420 \uc870\uac74\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c, \uade0\ud615 \uc694\uc18c(balance factor)\ub294 \ube44\uc6a9\uacfc \uc774\uc810 \uc0ac\uc774\uc758 \uade0\ud615\uc744 \ub9de\ucd94\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "C.3. \uad6c\ud604 \uc138\ubd80 \uc815\ubcf4"}, {"content": "| Model | #Params | ALL | ALG | ARI | GEO | LOG | NUM | SCI | STA |\n|---|---|---|---|---|---|---|---|---|---| \n| *Baselines* |  |  |  |  |  |  |  |  |  |\n| Random Choice | - | 17.9 | 25.8 | 13.8 | 22.7 | 13.4 | 8.8 | 15.8 | 14.3 |\n| Human Performance | - | 60.3 | 50.9 | 59.2 | 51.4 | 40.7 | 53.8 | 64.9 | 63.9 |\n| *Closed-source MLLMs* |  |  |  |  |  |  |  |  |  |\n| Qwen-VL-Plus [Bai et al. (2023)] | - | 43.3 | 39.1 | 32.0 | 39.3 | 18.9 | 26.4 | 59.0 | 56.1 |\n| GPT-4V [OpenAI (2023)] | - | 49.9 | 53.0 | 49.0 | 51.0 | 21.6 | 20.1 | 63.1 | 55.8 |\n| *Open-source Genreral MLLMs* |  |  |  |  |  |  |  |  |  |\n| mPLUG-Owl2-7B [Ye et al. (2023)] | 7B | 22.2 | 23.6 | 19.2 | 23.9 | 13.5 | 12.7 | 26.3 | 21.4 |\n| LLaVA-1.5-13B [Liu et al. (2024b)] | 13B | 25.7 | 19.6 | 28.6 | 17.6 | 10.8 | 27.8 | 33.6 | 22.9 |\n| MiniGPT-v2 [Chen et al. (2023)] | 7B | 23.1 | 28.1 | 21.0 | 24.7 | 16.2 | 16.7 | 25.4 | 17.9 |\n| InternLM-XComposer2-VL-7B [Dong et al. (2024c)] | 7B | 47.8 | 32.0 | 51.6 | 30.5 | 13.5 | 43.8 | 37.7 | 62.8 |\n| SPHINX-MoE [Lin et al. (2023)] | 8\\times7B | 42.3 | 31.7 | 41.6 | 30.5 | 16.2 | 27.1 | 50.8 | 50.8 |\n| DeepSeek-VL [Lu et al. (2024)] | 7B | 34.9 | 29.2 | 38.8 | 27.2 | 18.9 | 43.1 | 35.3 | 33.2 |\n| InternVL2-8B [Chen et al. (2024d)] | 8B | 58.3 | 59.8 | 56.4 | 60.3 | 10.8 | 30.6 | 59.0 | 68.8 |\n| Qwen2-VL [Wang et al. (2024b)] | 7B | 58.9 | 44.1 | 57.5 | 43.1 | 24.3 | 41.7 | 66.4 | 75.1 |\n| *Open-source Math MLLMs (Large-Scale Training)* |  |  |  |  |  |  |  |  |  |\n| G-LLaVA [Gao et al. (2024)] | 7B | 25.1 | 36.0 | 19.4 | 37.6 | 15.2 | 17.7 | 21.0 | 15.1 |\n| Math-LLaVA [Shi et al. (2024)] | 13B | 46.6 | 51.5 | 40.7 | 56.2 | 23.3 | 34.7 | 47.7 | 42.3 |\n| Multimath-7B [Peng et al. (2024)] | 7B | 50.0 | 61.9 | 42.2 | 64.9 | 23.3 | 32.6 | 42.6 | 49.2 |\n| Math-PUMA-Qwen2-7B [Zhuang et al. (2024)] | 7B | 47.9 | 47.7 | 46.2 | 47.3 | 21.6 | 32.6 | 42.6 | 55.8 |\n| URSA-7B [Luo et al. (2025)] | 7B | 59.8 | 74.0 | 53.5 | 77.4 | 21.6 | 35.4 | 58.2 | 57.1 |\n| AStar (Ours, Training-free Reasoning) | 7B | 63.5 | 69.0 | 63.1 | 71.7 | 61.3 | 60.0 | 48.2 | 68.8 |\n| AStar (Ours, Training-free Reasoning) | 2B | 49.6 | 51.0 | 49.9 | 53.1 | 34.7 | 46.8 | 41.3 | 54.3 |", "caption": "Table 10: Results on MathVista testmini detailed mathematics capabilities. The best results of closed-source MLLMs are highlighted in green. The best and second-best results of open-source MLLMs are highlighted in red and blue respectively.", "description": "\ud45c 10\uc740 MathVista testmini \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ud55c \ub2e4\uc591\ud55c \uc218\ud559\uc801 \ub2a5\ub825\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\uc11c \ud3d0\uc1c4\ud615(closed-source) MLLM\uc758 \ucd5c\uace0 \uc131\ub2a5\uc740 \ub179\uc0c9\uc73c\ub85c, \uac1c\ubc29\ud615(open-source) MLLM\uc758 \ucd5c\uace0 \ubc0f \ucc28\uace0 \uc131\ub2a5\uc740 \uac01\uac01 \ube68\uac04\uc0c9\uacfc \ud30c\ub780\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ubaa8\ub378 \ud06c\uae30\uc5d0 \uad00\uacc4\uc5c6\uc774 AStar \ucd94\ub860 \ud504\ub808\uc784\uc6cc\ud06c\uac00 \ub2e4\uc591\ud55c \uc218\ud559\uc801 \ub2a5\ub825\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \ucd94\ub860 \ubc31\ubcf8\uc73c\ub85c Qwen2-VL-2B\ub97c \uc0ac\uc6a9\ud55c 2B \ubaa8\ub378\uc740 InternLM-XComposer2-VL-7B \ubc0f Math-LLaVA\uc640 \uac19\uc740 \ub300\ud615 \ubaa8\ub378\uc744 \ub2a5\uac00\ud558\uc5ec GPT-4V\uc640 \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.", "section": "4.2 Performance Comparison with Baselines"}, {"content": "| Model | MathVista | MathVerse | MathVision | Average |\n|---|---|---|---|---|\n| **Closed-Source Models** |  |  |  |  |\n| GPT-4o-0513 [OpenAI (2024)] | 63.8 | 50.2 | 30.4 | 48.2 |\n| GPT-4V [OpenAI (2023)] | 49.9 | 54.4 | 24.8 | 43.1 |\n| Gemini-1.5-Pro [Team et al. (2023)] | 63.9 | 35.3 | 19.2 | 39.5 |\n| Claude-3.5-Sonnet [Anthropic (2024)] | 67.7 | - | - | - |\n| Qwen-VL-Plus [Bai et al. (2023)] | 43.3 | 21.3 | 10.8 | 25.2 |\n| **Open-Source Models** |  |  |  |  |\n| Qwen2-VL-72B [Wang et al. (2024b)] | **70.5** | - | 25.9 | - |\n| LLaVA-OneVision-72B [Li et al. (2024)] | 67.5 | 39.1 | - | - |\n| InternVL2.5-26B [Chen et al. (2024d)] | 67.7 | 40.1 | 28.0 | 45.3 |\n| InternVL2-Llama3-76B [Chen et al. (2024d)] | 65.5 | 42.8 | 23.7 | 44.0 |\n| Astar (Ours) | 63.5 | **54.0** | **32.4** | **50.0** |", "caption": "Table 11: Comparison with leading LLMs. The best results are highlighted in bold. Results for off-the-shelf models are sourced from corresponding official websites.", "description": "\ud45c 11\uc740 \uc8fc\uc694 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\ub4e4\uacfc AStar \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  MathVista, MathVerse, MathVision \uc138 \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec AStar\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  AStar\ub294 \ud2b9\ud788 MathVerse\uc640 MathVision\uacfc \uac19\uc740 \ubcf5\uc7a1\ud55c \ubb38\uc81c\uc5d0\uc11c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ubc18\uba74, \uae30\uc874\uc758 \uc624\ud508\uc18c\uc2a4 \ubc0f \ud074\ub85c\uc988\ub4dc\uc18c\uc2a4 LLM \ubaa8\ub378\ub4e4\uacfc\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\uc758 \uacb0\uacfc\ub294 \ud574\ub2f9 \ubaa8\ub378\uc758 \uacf5\uc2dd \uc6f9\uc0ac\uc774\ud2b8\uc5d0\uc11c \uac00\uc838\uc654\uc2b5\ub2c8\ub2e4.", "section": "4.2. \uae30\uc900 \ubaa8\ub378\uacfc\uc758 \uc131\ub2a5 \ube44\uad50"}, {"content": "| Model | Size | MMStar | ChartQA | MathVista | MathVerse |\n|---|---|---|---|---|---| \n| *2B-Scale Baselines* |  |  |  |  |  |\n| Mulberry Yao et al. (2024) | 2B | 51.3 | 77.7 | **51.7** | - |\n| Astar (Ours) | 2B | **51.7** | **78.3** | 49.6 | **33.7** |\n| \u2265 7B-Scale Baselines |  |  |  |  |  |\n| Insight-V Dong et al. (2024d) | 7B | 61.5 | 81.5 | 59.9 | - |\n| AR-MCTS Dong et al. (2024a) | 7B | - | - | **64.1** | - |\n| Mulberry Yao et al. (2024) | 7B | 61.3 | **83.9** | 63.1 | - |\n| LLaVA-CoT OpenAI (2024) | 11B | 57.6 | - | 54.8 | - |\n| LlamaV-o1 Thawakar et al. (2025) | 11B | 59.6 | - | 54.4 | - |\n| URSA Luo et al. (2025) | 7B | - | - | 59.8 | 45.7 |\n| Virgo Luo et al. (2025) | 7B | - | - | - | 37.5 |\n| Astar (Ours) | 7B | **61.7** | **83.9** | 63.5 | **54.0** |", "caption": "Table 12: Comparison with recent works targeting enhanced multimodal reasoning through structured thinking. We list 2B and \u2265\\geq\u2265 7B-scale baselines. The best results in each box are highlighted in bold. Our method demonstrates significant performance improvements.", "description": "\ud45c 12\ub294 \uad6c\uc870\ud654\ub41c \ucd94\ub860\uc744 \ud1b5\ud574 \ud5a5\uc0c1\ub41c \ub2e4\uc911 \ubaa8\ub4dc \ucd94\ub860\uc744 \ubaa9\ud45c\ub85c \ud558\ub294 \ucd5c\uadfc \uc5f0\uad6c\ub4e4\uacfc\uc758 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 2B \ubc0f 7B \uc774\uc0c1 \uaddc\ubaa8\uc758 \uae30\uc900 \ubaa8\ub378\ub4e4\uc744 \ub098\uc5f4\ud558\uc5ec \uc81c\uc2dc\ud558\uace0 \uc788\uc73c\uba70, \uac01 \uc0c1\uc790\uc5d0 \ud45c\uc2dc\ub41c \ucd5c\uace0 \uc131\ub2a5 \uacb0\uacfc\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ubcf8 \uc5f0\uad6c\uc758 \ubc29\ubc95\ub860\uc774 \uc131\ub2a5\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ucf30\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2. \uae30\uc900 \ubaa8\ub378\ub4e4\uacfc\uc758 \uc131\ub2a5 \ube44\uad50"}, {"content": "| Backbone | 1 | 2 | 3 | 4 | 5 | Average |\n|---|---|---|---|---|---|---|\n| Qwen2-VL-2B-Base | 20.8 | 20.7 | 21.4 | 17.8 | 10.2 | 18.2 |\n| Qwen2-VL-2B | **25.0** | 20.0 | 15.6 | 16.7 | **17.6** | **22.3** |\n| Qwen2-VL-7B-Base | 26.8 | 20.6 | 23.7 | 26.7 | 22.4 | 23.4 |\n| Qwen2-VL-7B | 26.5 | **21.3** | **26.2** | **29.8** | **23.8** | **25.5** |", "caption": "Table 13: Integration With SFT on MathVision. We evaluate the AStar framework using Qwen2-VL-2B and Qwen2-VL-7B as backbone models, comparing both pre-trained base models and SFT variants across different difficulty levels.", "description": "\ud45c 13\uc740 MathVision \ub370\uc774\ud130\uc14b\uc5d0\uc11c SFT(Supervised Fine-Tuning)\uc640 \ud1b5\ud569\ub41c AStar \ud504\ub808\uc784\uc6cc\ud06c\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Qwen2-VL-2B\uc640 Qwen2-VL-7B \ubaa8\ub378\uc744 \ubc31\ubcf8 \ubaa8\ub378\ub85c \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc804 \ud6c8\ub828\ub41c \uae30\ubcf8 \ubaa8\ub378\uacfc SFT \ubcc0\ud615 \ubaa8\ub378 \ubaa8\ub450\ub97c \ub2e4\uc591\ud55c \ub09c\uc774\ub3c4 \uc218\uc900\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.  \ud45c\ub294 \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \ub09c\uc774\ub3c4(1~5)\uc5d0 \ub530\ub77c \ud3c9\uade0 \uc131\ub2a5\uacfc \ud568\uaed8 \uc81c\uc2dc\ud558\uc5ec SFT\uc640 AStar \ud504\ub808\uc784\uc6cc\ud06c\uc758 \uc2dc\ub108\uc9c0 \ud6a8\uacfc \ubc0f \uac01 \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ud30c\uc545\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "4. Experiments"}]