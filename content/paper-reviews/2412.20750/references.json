{"references": [{"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "publication_date": "2023-00-00", "reason": "This paper introduces BLIP-2, a significant advancement in vision-language models that is frequently referenced and used as a benchmark in the field."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual Instruction Tuning", "publication_date": "2023-00-00", "reason": "This paper presents Visual Instruction Tuning (VIT), a crucial technique for aligning vision and language models, which is extensively used and cited in the paper."}, {"fullname_first_author": "Haotian Liu", "paper_title": "LLaVA-Next: Improved reasoning, OCR, and world knowledge", "publication_date": "2024-00-00", "reason": "LLaVA-Next is another significant model that improves upon prior work and is frequently used as a comparison in the paper."}, {"fullname_first_author": "Byung-Kwan Lee", "paper_title": "Phantom of Latent for Large Language and Vision Models", "publication_date": "2024-00-00", "reason": "This paper proposes the Phantom model, which is a key model compared against in the experimental results section of the paper."}, {"fullname_first_author": "Yuan Yao", "paper_title": "MiniCPM-V: A GPT-4V level MLLM on your phone", "publication_date": "2024-00-00", "reason": "This paper introduces MiniCPM-V, another key model used for comparison in the experimental section of the paper."}]}