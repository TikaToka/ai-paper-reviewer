[{"figure_path": "https://arxiv.org/html/2412.20750/x1.png", "caption": "Figure 1: Multi-vision sensor related question and response examples of recent VLMs\u00a0[39, 53]. Note that, this example underscores the difficulty that VLMs face in understanding physical properties unique to multi-vision sensors.", "description": "\uadf8\ub9bc 1\uc740 \ucd5c\uadfc VLMs [39, 53]\uc758 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \uad00\ub828 \uc9c8\ubb38\uacfc \uc751\ub2f5 \uc0ac\ub840\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uc608\uc2dc\ub294 \uc5f4\ud654\uc0c1 \uce74\uba54\ub77c\ub97c \uc774\uc6a9\ud55c \uc0ac\uc9c4\uc5d0\uc11c \uc0ac\uc2b4\uc774 \ubc30\uacbd\ubcf4\ub2e4 \ub354 \ubc1d\uac8c \ub098\ud0c0\ub098\ub294 \uc774\uc720\ub97c \ubb3b\ub294 \uc9c8\ubb38\uc5d0 \ub300\ud574, VLM\uc774 \ud587\ube5b \ubc18\uc0ac\ub85c \uc624\ub2f5\uc744 \ub0b4\ub9ac\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 VLM\uc774 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c\uc758 \uace0\uc720\ud55c \ubb3c\ub9ac\uc801 \ud2b9\uc131\uc744 \uc81c\ub300\ub85c \uc774\ud574\ud558\uc9c0 \ubabb\ud55c\ub2e4\ub294 \uc5b4\ub824\uc6c0\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.  \uc989, \ub2e8\uc21c\ud788 \uc774\ubbf8\uc9c0\uc758 \ubc1d\uae30 \ucc28\uc774\ub9cc \uc778\uc2dd\ud558\uace0, \uc5f4 \ubc29\ucd9c\uc774\ub77c\ub294 \ubb3c\ub9ac\uc801 \ud604\uc0c1\uc744 \uc774\ud574\ud558\uc9c0 \ubabb\ud558\ub294 VLM\uc758 \ud55c\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc785\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.20750/x2.png", "caption": "Figure 2: Data samples of Multi-vision Sensor Perception and Reasoning (MS-PR) benchmark for evaluating the abilities of VLMs in multi-vision sensor understanding, which covers four types of multi-vision perception tasks (Existence, Counting, Position, and General Description) and two types of multi-vision reasoning tasks (Contextual Reasoning and Sensory Reasoning).", "description": "\uadf8\ub9bc 2\ub294 \ub17c\ubb38\uc758 MS-PR \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ub370\uc774\ud130 \uc0d8\ud50c\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MS-PR \ubca4\uce58\ub9c8\ud06c\ub294 VLMs\uc758 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \uc774\ud574 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \ubca4\uce58\ub9c8\ud06c\ub85c, \ub2e4\uc911 \ube44\uc804 \uc9c0\uac01 \uacfc\uc81c(\uc874\uc7ac, \uacc4\uc0b0, \uc704\uce58, \uc77c\ubc18\uc801\uc778 \uc124\uba85) 4\uac00\uc9c0 \uc720\ud615\uacfc \ub2e4\uc911 \ube44\uc804 \ucd94\ub860 \uacfc\uc81c(\ub9e5\ub77d\uc801 \ucd94\ub860 \ubc0f \uac10\uac01\uc801 \ucd94\ub860) 2\uac00\uc9c0 \uc720\ud615\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \uac01 \uacfc\uc81c \uc720\ud615\uc5d0 \ub300\ud55c \uc9c8\ubb38\uacfc \ub2f5\ubcc0\uc758 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc8fc\uc5b4, MS-PR \ubca4\uce58\ub9c8\ud06c\uac00 VLMs\uc758 \ub2e4\uc911 \uc13c\uc11c \ub370\uc774\ud130 \ud574\uc11d \ubc0f \ucd94\ub860 \ub2a5\ub825\uc744 \uc5b4\ub5bb\uac8c \ud3c9\uac00\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Multi-Vision Sensor Perception and Reasoning (MS-PR) Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.20750/x3.png", "caption": "Figure 3: Distribution of data sources of the MS-PR benchmark. In MS-PR, we demonstrate six core multi-vision sensor tasks in the outer ring, and the inner ring displays the number of samples for each specific task.", "description": "\uadf8\ub9bc 3\uc740 MS-PR \ubca4\uce58\ub9c8\ud06c\uc758 \ub370\uc774\ud130 \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubc14\uae65\ucabd \uc6d0\uc740 6\uac00\uc9c0 \ud575\uc2ec \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \uc791\uc5c5(\uc874\uc7ac, \uacc4\uc0b0, \uc704\uce58, \uc77c\ubc18 \uc124\uba85, \uc0c1\ud669 \ucd94\ub860, \uac10\uac01 \ucd94\ub860)\uc744 \ub098\ud0c0\ub0b4\uace0, \uc548\ucabd \uc6d0\uc740 \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc0d8\ud50c \uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 MS-PR \ubca4\uce58\ub9c8\ud06c\uc5d0 \uc0ac\uc6a9\ub41c \uac01 \uc13c\uc11c \uc720\ud615(\uc5f4\ud654\uc0c1, \uae4a\uc774, X\uc120)\uc758 \ub370\uc774\ud130 \uc591\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\uc5b4, \ubca4\uce58\ub9c8\ud06c\uc758 \uad6c\uc131\uacfc \uade0\ud615\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4. \uac01 \uc791\uc5c5\uc758 \uc0d8\ud50c \uc218\ub294 \ub370\uc774\ud130 \uc14b\uc758 \ud06c\uae30\uc640 \uade0\ud615\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc9c0\ud45c\uc785\ub2c8\ub2e4.", "section": "3. Multi-Vision Sensor Perception and Reasoning (MS-PR) Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.20750/x4.png", "caption": "Figure 4: Overview of the pipeline for generating the proposed benchmark dataset. Based on the prompts corresponding to knowledge on multi-vision sensors and tasks, ChatGPT/GPT-4o generates challenging question and answer set. We refine the dataset further by utilizing human annotators to construct positive and negative sets, allowing each pair to be classified into a specific evaluation dimension.", "description": "\uadf8\ub9bc 4\ub294 \uc81c\uc548\ub41c \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b \uc0dd\uc131 \ud30c\uc774\ud504\ub77c\uc778\uc758 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uba40\ud2f0\ube44\uc804 \uc13c\uc11c\uc640 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc9c0\uc2dd\uc774 \ub2f4\uae34 \ud504\ub86c\ud504\ud2b8\ub97c \uae30\ubc18\uc73c\ub85c ChatGPT/GPT-4\uac00 \uc5b4\ub824\uc6b4 \uc9c8\ubb38\uacfc \ub2f5\ubcc0 \uc138\ud2b8\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uc774\ud6c4,  \uc778\uac04 \uc5b4\ub178\ud14c\uc774\ud130\ub97c \ud65c\uc6a9\ud558\uc5ec \ub370\uc774\ud130\uc14b\uc744 \ub354\uc6b1 \uac1c\uc120\ud558\uace0,  \uae0d\uc815\uc801 \ubc0f \ubd80\uc815\uc801 \ub2f5\ubcc0 \uc138\ud2b8\ub97c \uad6c\uc131\ud558\uc5ec \uac01 \uc30d\uc744 \ud2b9\uc815 \ud3c9\uac00 \ucc28\uc6d0\uc73c\ub85c \ubd84\ub958\ud569\ub2c8\ub2e4. \uc774\ub294 \uc2e4\uc81c \ud658\uacbd\uc744 \uc815\ud655\ud558\uac8c \ubc18\uc601\ud558\ub294 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc744 \ub9cc\ub4dc\ub294 \ub370 \uc911\uc694\ud55c \uacfc\uc815\uc785\ub2c8\ub2e4.", "section": "3. Multi-Vision Sensor Perception and Reasoning (MS-PR) Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.20750/x5.png", "caption": "Figure 5: Description of sensor knowledge information and questions types and examples.", "description": "\uadf8\ub9bc 5\ub294 \ub2e4\uc591\ud55c \uba40\ud2f0 \ube44\uc804 \uc13c\uc11c(\uc5f4\ud654\uc0c1, \uc2ec\ub3c4, X\uc120)\uc5d0 \ub300\ud55c \uc9c0\uc2dd \uc815\ubcf4\uc640 \uc9c8\ubb38 \uc720\ud615 \ubc0f \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc13c\uc11c\uc758 \ubb3c\ub9ac\uc801 \ud2b9\uc131\uacfc \uc751\uc6a9 \ubd84\uc57c\uc5d0 \ub300\ud55c \uc124\uba85\uacfc \ud568\uaed8, \uac01 \uc13c\uc11c \uc720\ud615\uc5d0 \uc801\ud569\ud55c \ub2e4\uc591\ud55c \uc9c8\ubb38 \uc720\ud615(\uac1d\uccb4 \uc778\uc2dd, \uac1c\uc218 \uc138\uae30, \uc704\uce58 \uad00\uacc4, \uc7a5\uba74 \uc124\uba85, \uc0c1\ud669 \ucd94\ub860, \uc13c\uc11c \ucd94\ub860)\uacfc \uad6c\uccb4\uc801\uc778 \uc9c8\ubb38 \uc608\uc2dc\ub97c \uc81c\uc2dc\ud558\uc5ec, \uba40\ud2f0 \ube44\uc804 \uc13c\uc11c \uc774\ud574\ub97c \uc704\ud55c \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b \uad6c\ucd95\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ub3d5\uc2b5\ub2c8\ub2e4.", "section": "3. \uba40\ud2f0 \ube44\uc804 \uc13c\uc11c \uc778\uc2dd \ubc0f \ucd94\ub860(MS-PR) \ubca4\uce58\ub9c8\ud06c"}, {"figure_path": "https://arxiv.org/html/2412.20750/x6.png", "caption": "Figure 6: Description of prompts for generating challenging multiple-choice questions and answers for multi-vision sensor tasks", "description": "\uadf8\ub9bc 6\uc740 \ub2e4\uc591\ud55c \uba40\ud2f0 \ube44\uc804 \uc13c\uc11c \uc791\uc5c5\uc5d0 \ub300\ud55c \uc5b4\ub824\uc6b4 \uac1d\uad00\uc2dd \uc9c8\ubb38\uacfc \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud558\uae30 \uc704\ud55c \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud55c \uc124\uba85\uc785\ub2c8\ub2e4. \uc774 \ud504\ub86c\ud504\ud2b8\ub294 \uc13c\uc11c \uc9c0\uc2dd, \uc791\uc5c5 \uc720\ud615 \ubc0f \uc608\uc2dc \uc9c8\ubb38\uc744 \ud3ec\ud568\ud558\uc5ec \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \uc13c\uc11c \uc720\ud615(\uc5f4, \uae4a\uc774, X\uc120)\uc758 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud55c \uc9c8\ubb38\uc744 \uc0dd\uc131\ud558\uace0, \uc815\ub2f5\uacfc \ud568\uaed8 \uc5ec\ub7ec \uac1c\uc758 \uadf8\ub7f4\ub4ef\ud558\uc9c0\ub9cc \ud2c0\ub9b0 \ub2f5\ubcc0\uc744 \uc81c\uacf5\ud558\ub3c4\ub85d \uc548\ub0b4\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uba40\ud2f0 \ube44\uc804 \uc13c\uc11c\uc5d0 \ub300\ud55c \uc2ec\uce35\uc801\uc778 \uc774\ud574 \ub2a5\ub825\uc744 \ud3c9\uac00\ud560 \uc218 \uc788\ub294 \ub370\uc774\ud130\uc14b\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \uc0dd\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. Multi-Vision Sensor Perception and Reasoning (MS-PR) Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.20750/x8.png", "caption": "Figure 7: Human Agreement Results on Multi-Vision Sensory Reasoning Performance Across Diverse VLMs", "description": "\uadf8\ub9bc 7\uc740 \ub2e4\uc591\ud55c Vision-Language Model(VLM)\uc5d0 \uac78\uccd0 \uc778\uac04\uc758 \ub2e4\uc911 \ube44\uc804 \uac10\uac01 \ucd94\ub860 \uc131\uacfc\uc5d0 \ub300\ud55c \ud569\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c VLM(LLaVA-1.5-7B, Qwen2-VL-7B, InternVL2-8B, Phantom-7B, GPT-40)\uc774 \ub2e4\uc911 \ube44\uc804 \uac10\uac01 \ucd94\ub860 \uc791\uc5c5\uc5d0\uc11c \uc5bc\ub9c8\ub098 \uc798 \uc218\ud589\ud558\ub294\uc9c0, \uadf8\ub9ac\uace0 \uc774\ub4e4\uc758 \uc131\ub2a5\uc774 \uc778\uac04\uc758 \uc218\ud589 \ub2a5\ub825\uacfc \uc5b4\ub5bb\uac8c \ube44\uad50\ub418\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ub9c9\ub300 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ubc31\ubd84\uc728\ub85c \uc81c\uc2dc\ud558\uc5ec, \ubaa8\ub378\ub4e4\uc758 \uc0c1\ub300\uc801 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uace0 \uc778\uac04\uc758 \ud3c9\uade0 \uc815\ud655\ub3c4\uc640 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. \uc774\ub294 \ub2e4\uc911 \ube44\uc804 \ub370\uc774\ud130\uc5d0 \ub300\ud55c VLMs\uc758 \uc774\ud574\ub3c4\uc640 \ucd94\ub860 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2412.20750/x9.png", "caption": "Figure 8: The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Perception task (Existence). Green font denotes the correct answer, while red font denotes the incorrect answer.", "description": "\uadf8\ub9bc 8\uc740 \ub17c\ubb38\uc758 \"3.1 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \uc778\uc2dd \ubc0f \ucd94\ub860(MS-PR) \ubca4\uce58\ub9c8\ud06c\" \uc139\uc158\uc5d0 \uc18d\ud558\uba70, \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c(\uc5f4\ud654\uc0c1, \uae4a\uc774, X\uc120)\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ucd5c\ucca8\ub2e8 VLMs\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uac83\uc785\ub2c8\ub2e4. \ud2b9\ud788, \"\uc874\uc7ac(Existence)\"\ub77c\ub294 \ub2e4\uc911 \ube44\uc804 \uc778\uc2dd \uc791\uc5c5\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd94\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uc13c\uc11c \uc720\ud615\uc5d0 \ub300\ud574 \uc9c8\ubb38\uacfc \ubcf4\uae30\uac00 \uc81c\uc2dc\ub418\uace0, \uc815\ub2f5\uc740 \ub179\uc0c9, \uc624\ub2f5\uc740 \ube68\uac04\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub418\uc5b4 VLMs\uc758 \uc815\ud655\uc131\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c VLMs\uc774 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \ub370\uc774\ud130\ub97c \uc5bc\ub9c8\ub098 \uc798 \uc774\ud574\ud558\uace0 \ud574\uc11d\ud558\ub294\uc9c0, \ud2b9\ud788 \uac1c\uccb4\uc758 \uc874\uc7ac \uc5ec\ubd80\ub97c \ud30c\uc545\ud558\ub294 \ub2a5\ub825\uc744 \ube44\uad50\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3.1 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \uc778\uc2dd \ubc0f \ucd94\ub860(MS-PR) \ubca4\uce58\ub9c8\ud06c"}, {"figure_path": "https://arxiv.org/html/2412.20750/x10.png", "caption": "Figure 9: The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Perception task (Counting). Green font denotes the correct answer, while red font denotes the incorrect answer.", "description": "\uadf8\ub9bc 9\ub294 \ub17c\ubb38\uc758 \"3.1 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \uc778\uc2dd \ubc0f \ucd94\ub860(MS-PR) \ubca4\uce58\ub9c8\ud06c\" \uc139\uc158\uc5d0 \ud3ec\ud568\ub41c \uadf8\ub9bc\uc73c\ub85c, \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c(\uc5f4\ud654\uc0c1, \uae4a\uc774, X\uc120) \ub370\uc774\ud130\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ucd5c\ucca8\ub2e8 \ube44\uc804-\uc5b8\uc5b4 \ubaa8\ub378(VLMs)\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uac83\uc785\ub2c8\ub2e4.  \ud2b9\ud788, '\uac1c\uc218 \uc138\uae30'\ub77c\ub294 \uc778\uc2dd \uacfc\uc81c\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd94\uace0 \uc788\uc73c\uba70, \uac01 \uc13c\uc11c \uc720\ud615\uc5d0 \ub530\ub978 \uc9c8\ubb38\uacfc \uc815\ub2f5\uc5d0 \ub300\ud574 \ubaa8\ub378\uc758 \uc815\ud655\uc131\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub179\uc0c9 \uae00\uaf34\uc740 \uc815\ub2f5\uc744, \ube68\uac04\uc0c9 \uae00\uaf34\uc740 \uc624\ub2f5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc744 \ud1b5\ud574 \uac01 \ubaa8\ub378\uc774 \uc11c\ub85c \ub2e4\ub978 \uc13c\uc11c \uc720\ud615\uc758 \ub370\uc774\ud130\ub97c \uc5bc\ub9c8\ub098 \uc798 \uc774\ud574\ud558\uace0 \ucc98\ub9ac\ud558\ub294\uc9c0, \uadf8\ub9ac\uace0 \uac01 \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.1 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \uc778\uc2dd \ubc0f \ucd94\ub860(MS-PR) \ubca4\uce58\ub9c8\ud06c"}, {"figure_path": "https://arxiv.org/html/2412.20750/x11.png", "caption": "Figure 10: The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Perception task (Position). Green font denotes the correct answer, while red font denotes the incorrect answer.", "description": "\uadf8\ub9bc 10\uc740 \ub2e4\uc591\ud55c VLMs(Vision-Language Models)\uc758 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c(\uc5f4\ud654\uc0c1, \uae4a\uc774, X\uc120)\uc5d0 \ub300\ud55c \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \ub2e4\uc911 \ube44\uc804 \uc9c0\uac01 \uacfc\uc81c \uc911 '\uc704\uce58'\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd94\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc815\ub2f5\uc740 \ub179\uc0c9, \uc624\ub2f5\uc740 \ubd89\uc740\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub418\uc5b4 VLMs\uc758 \uc815\ud655\ub3c4\ub97c \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc13c\uc11c \uc720\ud615\uc5d0 \ub530\ub77c \ubaa8\ub378\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ube44\uad50\ud558\uc5ec, \uc5b4\ub5a4 \uc720\ud615\uc758 \uc13c\uc11c \ub370\uc774\ud130\uc5d0\uc11c \ud2b9\uc815 VLM\uc774 \uc798 \uc791\ub3d9\ud558\uace0 \uc5b4\ub5a4 \uc720\ud615\uc5d0\uc11c\ub294 \uc798 \uc791\ub3d9\ud558\uc9c0 \uc54a\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.1 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \uc9c0\uac01 \ubc0f \ucd94\ub860 \uacfc\uc81c"}, {"figure_path": "https://arxiv.org/html/2412.20750/x12.png", "caption": "Figure 11: The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Perception task (General Description). Green font denotes the correct answer, while red font denotes the incorrect answer.", "description": "\uadf8\ub9bc 11\uc740 \ub2e4\uc591\ud55c \uba40\ud2f0 \ube44\uc804 \uc13c\uc11c(\uc5f4\ud654\uc0c1, \uae4a\uc774, X\uc120)\uc5d0 \ub300\ud55c \ub300\ud45c\uc801\uc778 VLM(Vision-Language Model)\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uac83\uc785\ub2c8\ub2e4.  \uba40\ud2f0 \ube44\uc804 \uc9c0\uac01 \uacfc\uc81c \uc911 \uc77c\ubc18\uc801\uc778 \uc124\uba85(General Description)\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd94\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uc9c8\ubb38\uc5d0 \ub300\ud574 VLM\uc774 \uc120\ud0dd\ud55c \ub2f5\ubcc0\uc774 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uc815\ub2f5\uc740 \ub179\uc0c9, \uc624\ub2f5\uc740 \ube68\uac04\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub418\uc5b4 VLMs\uc758 \uc131\ub2a5\uc744 \uc9c1\uad00\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc13c\uc11c \uc720\ud615\ubcc4\ub85c VLM\uc774 \uc5bc\ub9c8\ub098 \uc815\ud655\ud558\uac8c \uc774\ubbf8\uc9c0\uc758 \ub0b4\uc6a9\uc744 \uc124\uba85\ud558\ub294\uc9c0, \uc989 \uc77c\ubc18\uc801\uc778 \uc2dc\uac01\uc801 \ud2b9\uc9d5\uc744 \uc774\ud574\ud558\uace0 \uae30\uc220\ud558\ub294 \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "3.1 Multi-vision Sensor Perception Tasks"}, {"figure_path": "https://arxiv.org/html/2412.20750/x13.png", "caption": "Figure 12: The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Reasoning task (Contextual Reasoning). Green font denotes the correct answer, while red font denotes the incorrect answer.", "description": "\uadf8\ub9bc 12\ub294 \ub2e4\uc591\ud55c \uba40\ud2f0 \ube44\uc804 \uc13c\uc11c(\uc5f4\ud654\uc0c1, \uae4a\uc774, X\uc120)\uc5d0 \ub300\ud574 \ub300\ud45c\uc801\uc778 VLMs(LLaVA, InternVL2, Phantom, DNA \uc801\uc6a9 Phantom)\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 \uba40\ud2f0 \ube44\uc804 \ucd94\ub860 \uacfc\uc81c \uc911 \ubb38\ub9e5\uc801 \ucd94\ub860\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd94\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uc13c\uc11c \uc720\ud615\uc5d0 \ub300\ud55c \uc9c8\ubb38\uacfc \ubcf4\uae30\uac00 \uc81c\uc2dc\ub418\uace0, \uc815\ub2f5\uc740 \ub179\uc0c9, \uc624\ub2f5\uc740 \ube68\uac04\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub418\uc5b4 VLMs\uc758 \ubb38\ub9e5\uc801 \ucd94\ub860 \ub2a5\ub825\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "3.1.2 \uba40\ud2f0 \ube44\uc804 \ucd94\ub860"}]