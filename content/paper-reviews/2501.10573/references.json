{"references": [{"fullname_first_author": "J. Vuckovic", "paper_title": "A mathematical theory of attention", "publication_date": "2020-07-00", "reason": "This paper provides a mathematical framework for understanding the attention mechanism in transformer models, a core component of the models studied in the current work."}, {"fullname_first_author": "B. Geshkovski", "paper_title": "A mathematical perspective on transformers", "publication_date": "2024-00-00", "reason": "This paper offers a mathematical analysis of transformers, viewing them as dynamical systems, which is directly relevant to the current paper's focus on the geometric properties of token representations within these models."}, {"fullname_first_author": "A. Cowsik", "paper_title": "Geometric dynamics of signal propagation predict trainability of transformers", "publication_date": "2024-03-00", "reason": "This paper uses geometric analysis to study how signals propagate in transformers and relates this to the model's trainability, providing a complementary perspective to the current work's investigation of token geometry."}, {"fullname_first_author": "A. Agrachev", "paper_title": "Generic controllability of equivariant systems and applications to particle systems and neural networks", "publication_date": "2024-04-00", "reason": "This paper provides a theoretical framework for understanding the controllability of equivariant systems, which is relevant to the current paper's study of how the token distribution evolves across layers."}, {"fullname_first_author": "B. Geshkovski", "paper_title": "The emergence of clusters in self-attention dynamics", "publication_date": "2023-00-00", "reason": "This paper studies the clustering behavior of tokens in transformer models, a phenomenon related to the observed rank collapse, and offers insights into how this clustering behavior is related to the next token prediction task."}]}