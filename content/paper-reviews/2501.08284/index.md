---
title: "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for African Languages"
summary: "AfriHate: 15개 아프리카 언어의 증오심 표현 및 악용 언어에 대한 최초의 대규모 다국어 데이터 세트 공개!"
categories: ["AI Generated", "🤗 Daily Papers"]
tags: ["Natural Language Processing", "Text Classification", "🏢 Imperial College London",]
showSummary: true
date: 2025-01-14
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2501.08284 {{< /keyword >}}
{{< keyword icon="writer" >}} Shamsuddeen Hassan Muhammad et el. {{< /keyword >}}
 
{{< keyword >}} 🤗 2025-01-15 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2501.08284" target="_self" >}}
↗ arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2501.08284" target="_self" >}}
↗ Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/afrihate-a-multilingual-collection-of-hate" target="_self" >}}
↗ Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

온라인상의 증오심 표현과 악용 언어는 전 세계적인 문제이며, 특히 아프리카와 같은 저자원 지역에서는 이러한 현상을 이해하고 완화하기 위한 고품질 데이터가 부족합니다. 기존의 연구는 주로 영어에 집중했으며, 지역 사회의 참여가 부족하여 문화적 맥락을 제대로 반영하지 못했습니다. 이러한 한계점을 해결하기 위해, 연구자들은 15개 아프리카 언어로 된 증오심 표현 및 악용 언어에 대한 새로운 다국어 데이터 세트를 구축했습니다. 이 데이터 세트는 현지 문화에 대한 깊은 이해를 가진 원어민에 의해 주석이 달렸기 때문에 정확하고 신뢰할 수 있습니다.

본 연구는 15개 아프리카 언어에 대한 새로운 다국어 데이터 세트인 AfriHate를 제시하여 이러한 문제를 해결합니다. AfriHate는 현지 문화에 정통한 원어민에 의해 주석이 달렸으며, 다양한 기준 모델과 LLM(대규모 언어 모델)을 사용한 기준 결과도 제시합니다. 이 연구는 저자원 언어의 증오심 표현 연구에 크게 기여할 것이며, 이 분야의 연구를 발전시키는 중요한 리소스를 제공할 것입니다. 특히 다국어 모델이 저자원 언어 설정에서 성능을 향상시키는 데 도움이 될 수 있음을 보여줍니다.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} 15개 아프리카 언어를 위한 새로운 다국어 증오심 표현 및 악용 언어 데이터 세트인 AfriHate를 공개합니다. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} 현지 문화에 익숙한 원어민이 데이터 세트에 주석을 달아 정확성과 문화적 맥락을 보장합니다. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} AfriHate는 저자원 언어의 증오심 표현 연구를 위한 중요한 리소스가 될 것이며, 이 분야의 연구를 크게 발전시킬 것입니다. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
**아프리카 언어**의 증오심 표현 및 악용 언어에 대한 고품질 데이터 세트의 부족은 이 분야의 연구를 크게 저해해 왔습니다. 이 논문은 **15개의 아프리카 언어**에 대한 새로운 다국어 데이터 세트인 AfriHate를 제시하여 이 문제를 해결합니다. 이 데이터 세트는 현지 문화에 정통한 원어민에 의해 주석이 달렸으며, 이는 **저자원 언어**에 대한 증오심 표현 연구를 크게 발전시키는 것입니다. AfriHate는 연구자들이 증오심 표현 및 악용 언어를 더 잘 이해하고 완화하는 데 도움이 되는 새로운 도구와 방법을 개발하는 데 사용될 수 있습니다.

------
#### Visual Insights



![](https://arxiv.org/html/2501.08284/x2.png)

> 🔼 이 그림은 논문의 데이터 주석 지침과 정의를 보여줍니다. 그림에는 주석 작업에 사용된 세 가지 범주(증오, 모욕적, 중립)와 각 범주에 해당하는 정의가 자세히 설명되어 있습니다. 또한, 각 범주에 대한 예시와 함께 특정 속성(인종, 성별, 종교 등)에 따른 대상에 대한 추가 레이블 지정에 대한 설명이 포함되어 있습니다. 추가적으로, 언어 식별이 불가능하거나 다른 언어로 작성된 트윗에 대한 처리 방법도 제시되어 있습니다.
> <details>
> <summary>read the caption</summary>
> Figure 1: Annotation Guidelines and Definitions
> </details>





{{< table-caption >}}
| Language | Code | Subregion | Spoken in | Script |
|---|---|---|---|---|
| Algerian Arabic/Darja | `arq` | North Africa | Algeria | Arabic |
| Amharic | `amh` | East Africa | Ethiopia, Eritrea | Ethiopic |
| Hausa | `hau` | West Africa | Northern Nigeria, Niger, Ghana, and Cameroon, | Latin |
| Igbo | `ibo` | West Africa | Southeastern Nigeria | Latin |
| Kinyarwanda | `kin` | East Africa | Rwanda | Latin |
| Moroccan Arabic/Darija | `ary` | North Africa | Morocco | Arabic/Latin |
| Nigerian Pidgin | `pcm` | West Africa | Nigeria, Ghana, Cameroon, | Latin |
| Oromo | `orm` | East Africa | Ethiopia, Kenya, Somalia | Latin |
| Somali | `som` | East Africa | Somalia, Ethiopia, Djibouti, Kenya | Latin |
| Swahili | `swa` | East Africa | Kenya, Tanzania, Uganda, DR Congo, Rwanda, Burundi, Mozambique | Latin |
| Tigrinya | `tir` | East Africa | Ethiopia, Eritrea | Ethiopic |
| Twi | `twi` | West Africa | Ghana | Latin |
| Xhosa | `tso` | Southern Africa | Mozambique, South Africa, Zimbabwe, Eswatini | Latin |
| Yorùbá | `yor` | West Africa | Southwestern and Central Nigeria, Benin, and Togo | Latin |
| Zulu | `zul` | Southern Afric | Southern Africa | Latin |{{< /table-caption >}}

> 🔼 AfriHate 데이터셋에 포함된 증오 표현 및 모욕적인 언어의 예시를 보여주는 표입니다. 각 예시는 어떤 언어로 작성되었는지, 어떤 종류의 증오 표현인지 (인종, 성별, 종교, 정치적 성향 등), 그리고 해당 표현이 어떤 대상을 겨냥하는지를 보여줍니다. 모든 증오 표현에는 표적이 할당되어 있습니다.
> <details>
> <summary>read the caption</summary>
> Table 1: Examples of hateful and abusive instances in AfriHate. All hateful posts are assigned targets.
> </details>





### In-depth insights


#### AfriHate Dataset
AfriHate 데이터셋은 아프리카 15개 언어의 증오 표현 및 모욕적 언어를 담은 다국어 말뭉치입니다. **다양한 아프리카 언어를 포괄**하여 기존 연구의 지역적 편향성을 해소하고, **현지 문화적 맥락을 고려한 어노테이션**을 통해 더욱 정확한 분석을 가능하게 합니다.  데이터 수집 과정에서 **트위터 API의 제약 및 언어 식별의 어려움**과 같은 난관을 극복하고, 현지어 사용자들의 참여를 통해 데이터 품질을 높였습니다.  **다양한 기준선 성능 평가**를 통해 저자들은 다국어 모델이 저자원 언어 환경에서 효과적임을 보여주었습니다. 하지만, **데이터 불균형 및 주관적 어노테이션**의 한계를 인지하고,  향후 연구에서는 더욱 정교한 어노테이션 및 데이터 균형화 작업이 필요합니다.  **공개된 데이터셋과 사전에 정의된 타겟 속성**은 향후 아프리카 언어의 증오 표현 연구에 귀중한 자원이 될 것입니다.

#### Data Collection
본 논문에서 데이터 수집 방법에 대한 설명은 아프리카 언어의 특성과 제약을 고려하여 다각적인 접근 방식을 취했다는 점을 보여줍니다. **트위터 API의 제한**으로 인해 직접적인 키워드 기반 수집에 어려움이 있었고, **대체 전략**으로 언어별 키워드 목록을 **크라우드소싱**을 통해 확보하고 **수동 수집 및 기존 데이터셋 활용** 등의 방법을 병행하였습니다.  특히, 저자들은 각 언어의 사회문화적 맥락과 특징을 고려하여 언어별 전략을 수립한 점을 강조하고 있는데, 이는 데이터의 질 향상에 중요한 요소로 작용했을 것입니다. **다양한 보완 전략**은 데이터 수집 과정의 어려움을 극복하고 다양한 데이터를 확보하는데 기여했을 것으로 예상되지만, **선택적 편향**의 가능성도 존재함을 인지해야 합니다.

#### Annotation Methods
본 논문에서 다루는 주제인 '주석 방법론'에 대한 심층적인 분석은 **다국어 지원이 부족한 아프리카 언어의 특성을 고려하여 진행**되었다는 점을 보여줍니다.  **전문적인 어노테이터를 활용**하고 **엄격한 품질 관리 절차**를 거쳤으며, 이는 데이터 품질 향상에 크게 기여했을 것으로 예상됩니다.  **다양한 주석 가이드라인**과 **표준화된 분류 체계**를 통해 주석의 일관성과 신뢰도를 확보하려는 노력이 엿보입니다. **문화적 맥락**을 고려한 주석 작업이 강조된 점 또한 인상적입니다.  **데이터 불균형 문제**에 대한 해결 방안 또한 제시되었는데, 이는 실제 응용에 있어서 중요한 고려 사항입니다.  **새로운 주석 도구 및 플랫폼**의 개발 및 활용 또한 언급되어, 향후 관련 연구에 대한 시사점을 제공합니다.  **다만, 주석 작업의 어려움과 한계점** 또한 논의되어, 향후 연구 개선 방향을 제시하고 있습니다.

#### Model Evaluation
본 논문에서는 다양한 아프리카 언어에 대한 증오 표현 및 모욕적 언어 데이터셋을 구축하고 평가하는 과정을 자세하게 다루고 있습니다.  모델 평가는 **다양한 지표**를 사용하여 이루어졌을 것이며, 특히 저자원 언어 환경에서의 성능을 중점적으로 평가했을 것으로 예상됩니다. **정확도, 재현율, F1 점수** 등의 전통적인 지표 외에도, **다양한 언어 간의 성능 차이**를 분석하는 등 다각적인 평가가 수행되었을 가능성이 높습니다.  또한, 단순한 성능 비교 뿐만 아니라, **모델의 편향성 및 공정성**에 대한 분석도 포함되어 있을 것으로 예상되며, **사회 문화적 맥락**을 고려한 평가가 중요한 부분을 차지했을 것입니다.  특히, 저자들이 언급한 도전 과제(데이터 불균형, 언어 간의 다양성 등)를 고려했을 때, **로버스트한 평가 방법론**을 채택했을 것이며, 이를 통해 **신뢰할 수 있는 결과**를 제시하려 했을 것으로 예상됩니다.

#### Research Gaps
아프리카 언어를 사용하는 **저자원 언어**에 대한 고품질 데이터 부족은 아프리카 언어의 증오 표현 및 모욕적 언어 감지에 대한 연구의 주요한 **연구 격차**입니다. 기존 연구는 주로 영어와 같은 자원이 풍부한 언어에 집중되어 있어, 아프리카 언어의 특수한 사회문화적 맥락을 고려하지 못하는 경우가 많습니다. **지역 사회의 참여 부족** 또한 중요한 문제입니다. 데이터 수집, 주석 달기 및 조정 과정에 현지 사회가 참여하지 않으면 편향된 데이터와 문화적으로 적절하지 않은 모델이 생성될 수 있습니다.  **기존 연구에서 사용된 이진 분류 방식** (증오/비증오)도 제한적입니다. 아프리카 언어의 뉘앙스와 복잡성을 제대로 이해하려면 세분화된 분류체계가 필요합니다.  **대규모 다국어 데이터셋**의 부족과 **LLM 활용의 부재**도 해결해야 할 연구 격차에 해당됩니다. 이러한 격차들을 해결하기 위해서는 아프리카 현지 연구자들의 협력과 다양한 아프리카 언어에 대한 고품질 데이터 구축, 그리고 LLM을 포함한 최첨단 기술의 활용이 필수적입니다.


### More visual insights




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
|       | amh | arq | ary | hau | ibo | kin | oro | pcm | som | swa | tir | twi | xho | yor | zul |
| :---: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **Hate** | 88 | 62 | 41 | 36 | 46 | 264 | 126 | 23 | 45 | 24 | 58 | 20 | 42 | 68 | 31 |
| **Abusive** | 74 | 40 | 0 | 149 | 118 | 362 | 159 | 26 | 67 | 12 | 66 | 86 | 177 | 109 | 118 |{{< /table-caption >}}
> 🔼 AfriHate 데이터셋에 포함된 15개 아프리카 언어의 정보를 담은 표입니다. 각 언어의 ISO 코드, 아프리카 대륙 내 하위 지역 분류, 주로 사용되는 국가, 그리고 AfriHate 데이터셋에 포함된 해당 언어의 문자 시스템을 보여줍니다.  이 표는 AfriHate 데이터셋의 언어적 다양성과 데이터 수집 과정에서 고려된 언어적 특징을 이해하는 데 도움을 줍니다.
> <details>
> <summary>read the caption</summary>
> Table 2: Information about the AfriHate languages: their ISO codes, subregions, countries in which they are mainly spoken, and the writing scripts included in AfriHate.
> </details>

{{< table-caption >}}
| Language | amh | arq | ary | hau | ibo | kin | oro | pcm | som | swa | tir | twi | xho | yor | zul |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| **Manually Collected** | ✗ | ✗ | ✗ | ✓ | ✗ | ✓ | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ | ✗ |
| **Pre-Annotation** | ✗ | ✗ | ✓ | ✓ | ✓ | ✓ | ✗ | ✓ | ✗ | ✗ | ✗ | ✓ | ✓ | ✗ | ✓ |
| **Total Annotators** | 11 | 7 | 3 | 3 | 6 | 3 | 9 | 3 | 7 | 5 | 8 | 3 | 3 | 4 | 3 |
| **Annotators per Instance** | 4 | 3 | 3 | 3 | 3 | 3 | 4 | 3 | 4 | 3 | 4 | 3 | 3 | 4 | 3 |
| **Free-Marginal Multirater Kappa** ↑ | 0.63 | 0.68 | 0.61 | 0.75 | 0.80 | 0.81 | 0.63 | 0.65 | 0.46 | 0.55 | 0.46 | 0.75 | 0.62 | 0.68 | 0.81 |{{< /table-caption >}}
> 🔼 표 3은 각 언어에 대해 데이터 수집에 사용된 키워드의 수를 보여줍니다. 알제리 아랍어(arq)의 경우, 수집에는 증오심이나 모욕적인 내용이 아닌 논란이 되는 주제도 포함됩니다. 이 표는 데이터 수집 과정에서 사용된 키워드 전략과 각 언어의 특성을 반영하고 있습니다.  특히, 특정 언어에 대해서는 키워드의 수가 다른 언어에 비해 상대적으로 적거나 많은 경우가 있는데, 이는 해당 언어의 특징과 사용되는 플랫폼, 그리고 문화적 맥락에 따라 다를 수 있습니다.
> <details>
> <summary>read the caption</summary>
> Table 3: Number of keywords used for data collection. For Algerian Arabic (arq), the collection includes controversial topics that are neither hateful nor offensive.
> </details>

{{< table-caption >}}
| Class | amh | ary | arq | hau | ibo | kin | oro | pcm | som | swa | tir | twi | xho | yor | zul |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| **Hate** | 2,246 | 162 | 674 | 343 | 251 | 1,268 | 2,293 | 1,177 | 388 | 3,974 | 2,940 | 443 | 210 | 150 | 147 |
| **Abusive** | 1,353 | 778 | 2,270 | 2,336 | 3,510 | 1,146 | 667 | 5,238 | 1,404 | 7,708 | 1,070 | 3,278 | 1,550 | 2,655 | 1,839 |
| **Neutral** | 1,359 | 310 | 1,690 | 3,965 | 1,242 | 2,308 | 2,072 | 4,184 | 2,868 | 9,410 | 1,062 | 180 | 1,923 | 2,074 | 2,322 |{{< /table-caption >}}
> 🔼 AfriHate 데이터셋의 수집 및 주석 작업에 대한 세부 정보를 보여주는 표입니다. 수동으로 수집되었는지, 사전 주석 단계가 수행되었는지, 총 주석자 수, 인스턴스당 주석자 수, 주석자 간 일치도(자유-주변 다중 평가자 카파) 등이 포함되어 있습니다.  이 표는 데이터셋 생성 과정의 다양한 측면을 보여주어 데이터 품질과 신뢰성을 평가하는 데 도움이 됩니다.
> <details>
> <summary>read the caption</summary>
> Table 4: Collection and annotation details for the AfriHate dataset. The table shows if the data was manually collected, whether a pre-annotation step was conducted, the total number of annotators, the number of annotators per instance, and the inter-annotator agreement (Free-Marginal Multirater Kappa).
> </details>

{{< table-caption >}}
| Hate Target | amh | arq | ary | hau | ibo | kin | oro | pcm | som | swa | tir | twi | xho | yor | zul |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| **Disability** | 0 | 0 | 26 | 0 | 0 | 6 | 0 | 1 | 0 | 112 | 0 | 32 | 3 | 2 | 0 |
| **Ethnicity** | 902 | 269 | 150 | 32 | 383 | 94 | 462 | 537 | 74 | 2799 | 573 | 245 | 94 | 103 | 241 |
| **Gender** | 15 | 2 | 6 | 2 | 3 | 24 | 12 | 50 | 17 | 117 | 0 | 12 | 111 | 13 | 0 |
| **Politics** | 1247 | 118 | 63 | 0 | 6 | 1096 | 1550 | 87 | 703 | 233 | 2251 | 32 | 0 | 33 | 0 |
| **Religion** | 130 | 6 | 207 | 38 | 0 | 10 | 31 | 105 | 19 | 336 | 16 | 14 | 0 | 22 | 0 |
| **Others** | 199 | 0 | 0 | 0 | 0 | 0 | 767 | 15 | 208 | 501 | 85 | 14 | 0 | 76 | 12 |{{< /table-caption >}}
> 🔼 표 5는 AfriHate 데이터셋에 있는 각 언어별 데이터의 클래스별 인스턴스 수를 보여줍니다.  'Hate'(혐오), 'Abusive/Offensive'(모욕적/공격적), 'Neutral'(중립) 세 가지 클래스로 분류된 데이터의 개수를 각 언어별로 나타냅니다.  이 표를 통해 각 언어 데이터셋의 클래스 불균형 정도를 파악할 수 있습니다.
> <details>
> <summary>read the caption</summary>
> Table 5: Number of instances per class in each dataset.
> </details>

{{< table-caption >}}
| Split | amh | ary | arq | hau | ibo | kin | oro | pcm | som | swa | tir | twi | xho | yor | zul |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| **Train** | 3,467 | 3,240 | 716 | 4,566 | 3,419 | 3,302 | 3,517 | 7,416 | 3,174 | 14,760 | 3,547 | 2,564 | 2,502 | 3,336 | 2,940 |
| **Dev** | 744 | 695 | 211 | 1,029 | 774 | 706 | 763 | 1,590 | 741 | 3,164 | 760 | 639 | 559 | 724 | 640 |
| **Test** | 747 | 699 | 323 | 1,049 | 821 | 714 | 759 | 1,593 | 745 | 3,168 | 765 | 698 | 622 | 819 | 728 |{{< /table-caption >}}
> 🔼 AfriHate 데이터셋에서 각 언어별 증오 표현의 대상이 되는 특징(장애, 민족, 성별, 정치, 종교, 기타)별 분포를 보여주는 표입니다. 각 범주에 속하는 트윗의 개수를 언어별로 나열하여, AfriHate 데이터셋에서 증오 표현의 표적이 되는 특징의 분포를 자세히 보여줍니다. 이를 통해 특정 언어 또는 특징에 대한 증오 표현의 편향성을 파악하는 데 도움이 됩니다.
> <details>
> <summary>read the caption</summary>
> Table 6: Data distribution of hate speech targets in AfriHate.
> </details>

{{< table-caption >}}
| Model | amh | ary | arq | hau | ibo | kin | oro | pcm | som | swa | tir | twi | xho | yor | zul | avg. |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| **Monolingual** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| **AfriBERTa** | 69.54 | 67.93 | 30.48 | 82.28 | 89.53 | 79.43 | 73.43 | 66.90 | 65.52 | 91.36 | 73.07 | 74.54 | 81.07 | 72.37 | 83.75 | 72.33 |
| **AfriTeVa V2** | 73.91 | 76.71 | 25.25 | 79.06 | 83.95 | 77.60 | 71.61 | 68.69 | 69.65 | 90.68 | 72.36 | 64.96 | 54.67 | 79.88 | 69.05 | 68.73 |
| **AfroXLMR** | 70.65 | 80.16 | 61.18 | 81.93 | 89.30 | 80.72 | 72.11 | 67.98 | 66.84 | 91.44 | 74.52 | 77.17 | 82.49 | 72.15 | 83.44 | 76.15 |
| **AfroXLMR-76L** | 74.36 | 80.05 | 53.52 | 82.78 | 89.59 | 79.58 | 76.63 | 68.38 | 71.09 | 91.72 | 76.27 | 76.65 | 84.40 | 72.35 | 84.65 | 76.45 |
| **Multilingual** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| **AfroXLMR-76L** | 75.25 | 80.76 | 63.31 | 82.20 | 89.85 | 79.56 | 77.62 | 69.20 | 72.26 | 91.22 | 77.55 | 78.68 | 86.83 | 74.32 | 86.81 | 78.16 |{{< /table-caption >}}
> 🔼 표 7은 AfriHate 데이터셋의 다양한 언어에 대한 학습(train), 개발(dev), 테스트(test) 분할에 포함된 인스턴스 수를 보여줍니다. 각 언어에 대해 세 가지 분할(학습, 개발, 테스트)의 인스턴스 수가 나열되어 있어, 모델 훈련 및 평가에 사용되는 데이터의 양과 분포를 파악하는 데 유용합니다.  이 표는 데이터셋의 규모와 구성을 보여주는 중요한 정보를 제공하며, 각 언어에 대한 데이터 분할이 균형 있게 이루어졌는지 확인하는 데 도움을 줍니다.
> <details>
> <summary>read the caption</summary>
> Table 7: Number of instances included in the training (train), development (dev), and test splits of the different datasets.
> </details>

{{< table-caption >}}
| Lang. | Monolingual AfroXLMR-76L |  |  |  |  | Multilingual AfroXLMR-76L |  |  |  |  | GPT-4o |  |  |  |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|  | Macro F1 | Abuse | Hate | Neutral |  | Macro F1 | Abuse | Hate | Neutral |  | Macro F1 | Abuse | Hate | Neutral |
| amh | 73.83 | 70.20 | 77.27 | 74.02 |  | **75.55** | **71.92** | **78.18** | **76.54** |  | 65.70 | 59.21 | 67.91 | 70.00 |
| ary | 78.01 | **86.81** | 66.98 | 80.25 |  | **79.05** | 86.59 | **68.66** | 81.91 |  | 75.93 | 84.29 | 61.09 | **82.43** |
| arq | 57.05 | 77.35 | 27.42 | 66.38 |  | 68.99 | **78.29** | 55.95 | **72.73** |  | **73.67** | 77.63 | **71.43** | 71.96 |
| hau | **81.53** | **77.60** | **80.45** | **86.54** |  | 78.05 | 76.45 | 72.83 | 84.88 |  | 59.44 | 70.61 | 40.77 | 66.95 |
| ibo | 88.00 | 92.18 | **91.18** | 80.64 |  | **88.33** | **92.41** | **91.18** | **81.40** |  | 76.85 | 84.13 | 75.37 | 71.04 |
| kin | **77.96** | **70.09** | 80.87 | **82.90** |  | 77.11 | 70.00 | 78.90 | 82.43 |  | 74.27 | 65.45 | **82.01** | 75.37 |
| orm | **70.07** | 47.19 | **81.17** | **81.86** |  | 69.93 | **48.89** | 80.11 | 80.78 |  | 65.30 | 44.12 | 72.78 | 79.00 |
| pcm | **65.40** | **71.09** | 52.82 | 72.31 |  | 63.71 | 69.66 | 49.10 | 72.39 |  | 63.53 | 57.38 | **56.67** | **76.54** |
| som | 59.53 | 67.00 | 30.77 | 80.83 |  | 60.91 | **68.81** | 32.94 | **81.00** |  | **62.94** | 63.85 | **44.91** | 80.05 |
| swa | 88.00 | **92.18** | **91.18** | 80.64 |  | **89.50** | 89.35 | 88.33 | **90.83** |  | 83.29 | 85.48 | 80.49 | 83.91 |
| tir | 72.32 | 72.29 | 82.60 | 62.07 |  | **74.45** | **75.23** | **84.97** | **63.16** |  | 56.23 | 51.73 | 64.38 | 52.59 |
| twi | 58.30 | **91.45** | **61.40** | 22.05 |  | **63.66** | 91.04 | 60.18 | 39.76 |  | 62.64 | 85.90 | 54.45 | **47.58** |
| xho | 79.37 | **86.33** | 66.23 | 85.53 |  | **81.57** | 85.14 | **71.95** | **87.63** |  | 57.74 | 70.82 | 38.46 | 63.94 |
| yor | 57.36 | **84.24** | 6.90 | 80.95 |  | 59.41 | 84.03 | 11.76 | **82.44** |  | **71.48** | 81.56 | **53.01** | 79.88 |
| zul | 80.97 | **87.54** | 68.79 | 86.57 |  | **84.34** | 85.46 | **79.55** | **88.01** |  | 70.63 | 74.60 | 67.78 | 69.51 |{{< /table-caption >}}
> 🔼 표 8은 BERT 기반 언어 모델 미세 조정 후 모델 성능을 보여줍니다. 각 언어에 대한 최고 성능은 굵게 표시되어 있습니다. 이 표는 다양한 아프리카 언어에 대한 여러 언어 모델의 성능을 비교 분석하여, 특정 언어에 가장 적합한 모델을 선택하는 데 도움을 줍니다.  모델 성능은 미세 조정 후 각 언어에 대해 평가되었으며, 최고 성능을 달성한 모델이 굵게 강조 표시되었습니다. 이를 통해 연구자들은 다양한 언어에 대한 모델 성능을 비교하고, 특정 애플리케이션에 가장 적합한 모델을 선택할 수 있습니다.
> <details>
> <summary>read the caption</summary>
> Table 8: Model performances after fine-tuning BERT-based LMs. The best performance for each language is highlighted in bold.
> </details>

{{< table-caption >}}
| Model | # Shots | amh | ary | arq | hau | ibo | kin | oro | pcm | som | swa | tir | twi | xho | yor | zul | Avg. |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| 
| **SetFit** | 0 | 33.79 | 46.54 | 27.4 | 23.57 | 36.9 | 25.29 | 26.26 | 43.46 | 32.18 | 44.38 | 49.3 | 51.13 | 37.38 | 48.46 | 35.08 | 37.41 |
|  | 5 | 44.89 | 33.82 | 36.07 | 49.93 | 51.52 | 55.35 | 36.56 | 53.28 | 48.09 | 54.9 | 35.36 | 33.23 | 47.49 | 34.36 | 33.18 | 43.2 |
|  | 10 | 49.42 | 35.81 | 46.55 | 49.72 | 39.31 | 53.1 | 39.1 | 57 | 42.29 | 62.64 | 39.16 | 45.76 | 43.52 | 49.73 | 40.33 | 46.23 |
|  | 20 | 50.13 | 40.49 | 54.24 | 55.4 | 65.15 | 57.35 | 40.91 | 59.09 | 48.95 | 74.93 | 40.04 | 53.72 | 46.92 | 56.54 | 50.67 | 52.97 |
| **Mistral-7B-v0.1** | 0 | 21.18 | 17.46 | 11.98 | 6.84 | 8.04 | 15.18 | 20.82 | 6.76 | 8.16 | 18.88 | 25.62 | 8.38 | 9.72 | 8.22 | 8.92 | 13.08 |
|  | 5 | 37.68 | 31.6 | 44.54 | 34.9 | 36.84 | 46.98 | 39.46 | 50 | 34.12 | 58.58 | 35.64 | 32.04 | 34.24 | 41.04 | 32.54 | 39.35 |
|  | 10 | 39.68 | 36.5 | 50.56 | 37.82 | 36.94 | 45.72 | 39.02 | 52.1 | 31.82 | 63.18 | 35.84 | 30.82 | 32.64 | 42.9 | 32.46 | 40.53 |
|  | 20 | 36.64 | 35.46 | 52.94 | 38.44 | 38.98 | 52.28 | 39.24 | 54.6 | 33.28 | 66.28 | 37.42 | 30.44 | 34.12 | 46.4 | 35 | 42.1 |
| **aya-23-35B** | 0 | 17.04 | 19.2 | 17.2 | 20.34 | 14.04 | 21.34 | 20.38 | 20.02 | 18.56 | 34.04 | 17.1 | 10.68 | 20.32 | 17.48 | 20.24 | 19.2 |
|  | 5 | 37.78 | 56.4 | 57 | 39.7 | 42.7 | 48.78 | 39.24 | 57.9 | 38.72 | 69.84 | 35.68 | 40.24 | 36.76 | 48.88 | 44.58 | 46.28 |
|  | 10 | 38.5 | 53.96 | 61.82 | 44.42 | 45.8 | 52.82 | 42.16 | 59.26 | 37.6 | 74.1 | 35.88 | 39.92 | 36.18 | 48.44 | 45 | 47.72 |
|  | 20 | 38.1 | 52.9 | 63.76 | 46.14 | 46.52 | 57.62 | 42.26 | 61.68 | 38.68 | 75.68 | 38.12 | 38.54 | 37.74 | 51.52 | 44.52 | 48.92 |
| **Gemma-2-9B** | 0 | 27.42 | 28.36 | 25.46 | 14.48 | 15 | 24.08 | 23.74 | 24.74 | 10.92 | 37.12 | 25.52 | 19.5 | 13.4 | 21.66 | 12.52 | 21.59 |
|  | 5 | 56.6 | 57.68 | 61.14 | 49.98 | 48.12 | 60.26 | 45.1 | 60.3 | 46.42 | 74.78 | 48.4 | 46.5 | 35.82 | 55.96 | 46.6 | 52.91 |
|  | 10 | 57.84 | 61.08 | 61.72 | 56.62 | 53.78 | 63.54 | 45.78 | 60.54 | 51.72 | 78.16 | 45.48 | 48.16 | 38.82 | 60.22 | 52.58 | 55.74 |
|  | 20 | 60.96 | 59.94 | 64.38 | 55.9 | 54.56 | 64.14 | 44.7 | 62.54 | 52.22 | 79.24 | 44.94 | 48.92 | 39.02 | 60.5 | 53.68 | 56.38 |
| **Gemma-2-27B** | 0 | 41.78 | 41.24 | 41.12 | 28.96 | 31.36 | 42.08 | 33.46 | 49.78 | 31.1 | 59.88 | 30.84 | 27.74 | 25.64 | 41.46 | 28.02 | 36.96 |
|  | 5 | 59.62 | 64.14 | 65.62 | 54.62 | 56.14 | 61.08 | 46.76 | 61.78 | 54.12 | 81.18 | 52.12 | 47.26 | 40.48 | 61.72 | 54.28 | 57.39 |
|  | 10 | 60.7 | 61.36 | 64.88 | 58.9 | 56.84 | 64.86 | 46.96 | 61.6 | 52.86 | 81.94 | 49.82 | 50.86 | 41.9 | 59.66 | 56.94 | 58.01 |
|  | 20 | 62.28 | 59.86 | 65.8 | 59.6 | 58.76 | 66.06 | 48.9 | 63.24 | 52.9 | 82.98 | 53.36 | 50.34 | 43.08 | 59.1 | 56.88 | 58.88 |
| **Llama-3.1-70B** | 0 | 36.34 | 43.34 | 42.64 | 35.64 | 32.52 | 38.52 | 31.54 | 48.66 | 31.14 | 60.14 | 27.08 | 25.54 | 28.98 | 40.64 | 35.5 | 37.21 |
|  | 5 | 58.52 | 66.24 | 62.88 | 52.86 | 52.88 | 58.14 | 45.5 | 64.38 | 52.72 | 75.76 | 44.42 | 43.9 | 39.02 | 58.08 | 55.24 | 55.37 |
|  | 10 | 61.18 | 64.46 | 62.2 | 56.4 | 55.1 | 59 | 46.6 | 63.58 | 54.4 | 78.74 | 49.62 | 45.76 | 39.04 | 57.96 | 55.74 | 56.65 |
|  | 20 | 60.38 | 61.36 | 63.8 | 57.4 | 53.8 | 62.48 | 49.02 | 63.18 | 51.82 | 80.02 | 53.9 | 47.5 | 40.34 | 57.94 | 56.36 | 57.29 |
| **GPT-4o** | 0 | 61.78 | 66.41 | 73.75 | 56.91 | 68.82 | 62.53 | 60.01 | 65.94 | 63.42 | 73.66 | 45.75 | 52.72 | 58.92 | 75.21 | 54.47 | 62.69 |
|  | 5 | 66.73 | 73.53 | 77.33 | 55.44 | 76.73 | 72.27 | 70.94 | 66.29 | 59.33 | 80.95 | 61.11 | 73.21 | 60.45 | 76.98 | 59.34 | 68.71 |
|  | 10 | 67.94 | 75.54 | 77.54 | 58.15 | 80.08 | 75.07 | 72.27 | 67.14 | 62.75 | 84.19 | 57.52 | 72.28 | 65.75 | 76.74 | 65.05 | 70.53 |
|  | 20 | 68.08 | 76.16 | 78.69 | 58.34 | 80.81 | 74.86 | 72.33 | 65.41 | 66.44 | 84.61 | 59.55 | 75.86 | 66.08 | 77.11 | 71.36 | 71.71 |
| **Lang. avg.** | - | 48.32 | 50.74 | 54.04 | 44.91 | 47.79 | 52.88 | 43.18 | 54.44 | 43.1 | 67.53 | 41.95 | 42.53 | 39.06 | 51.25 | 44.18 | 48.39 |{{< /table-caption >}}
> 🔼 표 9는 세 가지 모델(단일 언어 AfroXLMR-76L, 다국어 AfroXLMR-76L, GPT-4o)의 성능을 클래스별(Hate, Abuse, Neutral)로 비교 분석한 표입니다.  단일 언어 및 다국어 AfroXLMR-76L 모델은 각 언어에 대해 한 번만 실행되었고, GPT-4o 모델은 프롬프트 템플릿 1을 사용하여 20개의 샷으로 평가되었습니다. 각 언어별 최고 성능은 굵게 표시되어 있습니다. 이 표는 다국어 모델의 우수성과 특정 언어에 대한 모델의 강점과 약점을 보여줍니다.
> <details>
> <summary>read the caption</summary>
> Table 9: Macro F1 scores per class for monolingual and multilingual AfroXLMR-76L [only one run] vs. GPT-4o [prompt template 1; 20 shots]. The best performance for each language is highlighted in bold.
> </details>

{{< table-caption >}}
Model | # Shots | amh | ary | arq | hau | ibo | kin | oro | pcm | som | swa | tir | twi | xho | yor | zul | Avg. 
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---
SetFit | 0 | 33.79 | 46.54 | 27.4 | 23.57 | 36.9 | 25.29 | 26.26 | 43.46 | 32.18 | 44.38 | 49.3 | 51.13 | 37.38 | 48.46 | 35.08 | 37.41
 | 5 | 44.89 | 33.82 | 36.07 | 49.93 | 51.52 | 55.35 | 36.56 | 53.28 | 48.09 | 54.9 | 35.36 | 33.23 | 47.49 | 34.36 | 33.18 | 43.2
 | 10 | 49.42 | 35.81 | 46.55 | 49.72 | 39.31 | 53.1 | 39.1 | 57 | 42.29 | 62.64 | 39.16 | 45.76 | 43.52 | 49.73 | 40.33 | 46.23
 | 20 | 50.13 | 40.49 | 54.24 | 55.4 | 65.15 | 57.35 | 40.91 | 59.09 | 48.95 | 74.93 | 40.04 | 53.72 | 46.92 | 56.54 | 50.67 | 52.97
InkubaLM-0.4B | 0 | 22.96 | 19.38 | 18.14 | 20.02 | 18.42 | 22.44 | 26.16 | 16.86 | 20.76 | 20.74 | 25.1 | 17.44 | 20.58 | 19.26 | 20.38 | 20.58
 | 5 | 28.04 | 24.7 | 20.06 | 21.78 | 21.98 | 25.78 | 26.86 | 19.08 | 25.06 | 20.48 | 29.26 | 24.5 | 23.04 | 20.98 | 21.68 | 23.55
 | 10 | 27 | 26.46 | 23.92 | 24.48 | 23.52 | 27.22 | 29.14 | 23.68 | 25.98 | 24.58 | 27.92 | 24.02 | 24.68 | 23.68 | 25.06 | 25.42
 | 20 | 27.06 | 26.42 | 24.6 | 24.3 | 23.86 | 26.96 | 28.58 | 23.76 | 25.88 | 24.64 | 30.04 | 23.32 | 23.56 | 23.14 | 25.2 | 25.42
mt0-small | 0 | 18.86 | 16.98 | 13.42 | 12.22 | 9.72 | 17.4 | 20.68 | 12.4 | 15.16 | 14.07 | 19.3 | 8.56 | 14.2 | 12.04 | 13.62 | 14.58
 | 5 | 26.38 | 25.76 | 21.66 | 19.94 | 21.28 | 23.04 | 27.78 | 25.98 | 21.4 | 20.3 | 26.2 | 12.66 | 19.96 | 17.72 | 19.5 | 21.97
 | 10 | 27.38 | 23.72 | 20.4 | 20.94 | 22.36 | 21.08 | 27.94 | 25.58 | 19.88 | 16.93 | 26.62 | 12.08 | 18.56 | 19.9 | 18.88 | 21.48
 | 20 | 26.28 | 23.38 | 19.86 | 21.6 | 22.14 | 21.18 | 28.16 | 25.06 | 19.96 | 20.05 | 28.04 | 13.44 | 19.12 | 19.76 | 18.5 | 21.77
bloomz-7b1-mt | 0 | 17.18 | 21.86 | 21.86 | 18.6 | 18.42 | 21.98 | 19.38 | 21.6 | 16.74 | 25.1 | 16.92 | 17.7 | 19.24 | 22.36 | 19.8 | 19.92
 | 5 | 22.74 | 21.4 | 25.88 | 23.1 | 27.88 | 25.36 | 26.22 | 26.66 | 23.54 | 27.03 | 25.64 | 28.34 | 23.26 | 24.8 | 22.62 | 24.96
 | 10 | 27.86 | 21.06 | 25.88 | 24.4 | 27.9 | 22.54 | 27.86 | 26.24 | 25.48 | 19.83 | 28.58 | 28.5 | 23.56 | 24.16 | 23.92 | 25.18
 | 20 | 25.46 | 19.92 | 24.64 | 24.96 | 27.72 | 24.18 | 26.64 | 26.82 | 25.76 | 24.5 | 28.48 | 28.84 | 23.24 | 25.28 | 23.88 | 25.35
Mistral-7B-v0.1 | 0 | 21.18 | 17.46 | 11.98 | 6.84 | 8.04 | 15.18 | 20.82 | 6.76 | 8.16 | 10.64 | 25.62 | 8.38 | 9.72 | 8.22 | 8.92 | 12.53
 | 5 | 37.68 | 31.6 | 44.54 | 34.9 | 36.84 | 46.98 | 39.46 | 50 | 34.12 | 49.58 | 35.64 | 32.04 | 34.24 | 41.04 | 32.54 | 38.75
 | 10 | 39.68 | 36.5 | 50.56 | 37.82 | 36.94 | 45.72 | 39.02 | 52.1 | 31.82 | 54.34 | 35.84 | 30.82 | 32.64 | 42.9 | 32.46 | 39.94
 | 20 | 36.64 | 35.46 | 52.94 | 38.44 | 38.98 | 52.28 | 39.24 | 54.6 | 33.28 | 55.98 | 37.42 | 30.44 | 34.12 | 46.4 | 35 | 41.41
aya-23-35B | 0 | 17.04 | 19.2 | 17.2 | 20.34 | 14.04 | 21.34 | 20.38 | 20.02 | 18.56 | 21.2 | 17.1 | 10.68 | 20.32 | 17.48 | 20.24 | 18.34
 | 5 | 37.78 | 56.4 | 57 | 39.7 | 42.7 | 48.78 | 39.24 | 57.9 | 38.72 | 56.32 | 35.68 | 40.24 | 36.76 | 48.88 | 44.58 | 45.38
 | 10 | 38.5 | 53.96 | 61.82 | 44.42 | 45.8 | 52.82 | 42.16 | 59.26 | 37.6 | 61.08 | 35.88 | 39.92 | 36.18 | 48.44 | 45 | 46.86
 | 20 | 38.1 | 52.9 | 63.76 | 46.14 | 46.52 | 57.62 | 42.26 | 61.68 | 38.68 | 73.68 | 38.12 | 38.54 | 37.74 | 51.52 | 44.52 | 48.79
Gemma-2-9B | 0 | 27.42 | 28.36 | 25.46 | 14.48 | 15 | 24.08 | 23.74 | 24.74 | 10.92 | 32.12 | 25.52 | 19.5 | 13.4 | 21.66 | 12.52 | 21.26
 | 5 | 56.6 | 57.68 | 61.14 | 49.98 | 48.12 | 60.26 | 45.1 | 60.3 | 46.42 | 72.88 | 48.4 | 46.5 | 35.82 | 55.96 | 46.6 | 52.78
 | 10 | 57.84 | 61.08 | 61.72 | 56.62 | 53.78 | 63.54 | 45.78 | 60.54 | 51.72 | 76.5 | 45.48 | 48.16 | 38.82 | 60.22 | 52.58 | 55.63
 | 20 | 60.96 | 59.94 | 64.38 | 55.9 | 54.56 | 64.14 | 44.7 | 62.54 | 52.22 | 77.56 | 44.94 | 48.92 | 39.02 | 60.5 | 53.68 | 56.26
Gemma-2-27B | 0 | 41.78 | 41.24 | 41.12 | 28.96 | 31.36 | 42.08 | 33.46 | 49.78 | 31.1 | 54.54 | 30.84 | 27.74 | 25.64 | 41.46 | 28.02 | 36.61
 | 5 | 59.62 | 64.14 | 65.62 | 54.62 | 56.14 | 61.08 | 46.76 | 61.78 | 54.12 | 77.96 | 52.12 | 47.26 | 40.48 | 61.72 | 54.28 | 57.18
 | 10 | 60.7 | 61.36 | 64.88 | 58.9 | 56.84 | 64.86 | 46.96 | 61.6 | 52.86 | 80.04 | 49.82 | 50.86 | 41.9 | 59.66 | 56.94 | 57.88
 | 20 | 62.28 | 59.86 | 65.8 | 59.6 | 58.76 | 66.06 | 48.9 | 63.24 | 52.9 | 81.18 | 53.36 | 50.34 | 43.08 | 59.1 | 56.88 | 58.76
Llama-3.1-8B | 0 | 17.36 | 19.34 | 18.82 | 24.36 | 12.7 | 23.9 | 22.56 | 20.1 | 24.86 | 20.82 | 14.18 | 9.02 | 21.42 | 18.6 | 21.94 | 19.33
 | 5 | 38.08 | 40.88 | 51.68 | 36.78 | 35.9 | 38.78 | 31.24 | 52.38 | 30.62 | 58.1 | 30.2 | 34.82 | 29.22 | 40.42 | 30.9 | 38.67
 | 10 | 42.04 | 43.1 | 54.76 | 37.78 | 37.78 | 43.02 | 35.92 | 53.88 | 32.98 | 64.96 | 31.92 | 34.52 | 28.2 | 38.98 | 30.72 | 40.7
 | 20 | 47.74 | 40.84 | 57.92 | 41.64 | 37.76 | 48.86 | 39.78 | 55.36 | 30.26 | 69.38 | 37.12 | 33.6 | 27.84 | 42.68 | 29.12 | 42.66
Llama-3.1-70B | 0 | 36.34 | 43.34 | 42.64 | 35.64 | 32.52 | 38.52 | 31.54 | 48.66 | 31.14 | 49.36 | 27.08 | 25.54 | 28.98 | 40.64 | 35.5 | 36.5
 | 5 | 58.52 | 66.24 | 62.88 | 52.86 | 52.88 | 58.14 | 45.5 | 64.38 | 52.72 | 73.48 | 44.42 | 43.9 | 39.02 | 58.08 | 55.24 | 55.22
 | 10 | 61.18 | 64.46 | 62.2 | 56.4 | 55.1 | 59 | 46.6 | 63.58 | 54.4 | 76.5 | 49.62 | 45.76 | 39.04 | 57.96 | 55.74 | 56.5
 | 20 | 60.38 | 61.36 | 63.8 | 57.4 | 53.8 | 62.48 | 49.02 | 63.18 | 51.82 | 77.72 | 53.9 | 47.5 | 40.34 | 57.94 | 56.36 | 57.13
GPT-4o | 0 | 61.78 | 66.41 | 73.75 | 56.91 | 68.82 | 62.53 | 60.01 | 65.94 | 63.42 | 73.66 | 45.75 | 52.72 | 58.68 | 75.21 | 54.47 | 62.67
 | 5 | 66.73 | 73.53 | 77.33 | 55.44 | 76.73 | 72.27 | 70.94 | 66.29 | 59.33 | 80.95 | 61.11 | 73.21 | 60.45 | 76.98 | 59.34 | 68.71
 | 10 | 67.94 | 75.54 | 77.54 | 58.15 | 80.08 | 75.07 | 72.27 | 67.14 | 62.75 | 84.19 | 57.52 | 72.28 | 65.75 | 76.74 | 65.05 | 70.53
 | 20 | 68.08 | 76.16 | 78.69 | 58.34 | 80.81 | 74.86 | 72.33 | 65.41 | 66.44 | 84.61 | 59.55 | 75.86 | 66.08 | 77.11 | 71.36 | 71.71
Lang. avg. | - | 40.8 | 41.73 | 44.47 | 37.6 | 39.26 | 43.51 | 37.59 | 44.99 | 36.16 | 51.01 | 36.37 | 35.05 | 33.03 | 41.56 | 36.43 | 39.97{{< /table-caption >}}
> 🔼 표 10은 AfriHate 데이터셋의 14개 언어에 대해 제로샷 및 퓨샷 분류기의 성능(Macro F1 점수)을 보여줍니다. 각 언어별 최고 성능은 굵게 표시되어 있으며, 5개의 프롬프트 템플릿에 대한 평균 결과입니다. 이 표는 다양한 크기의 사전 훈련된 언어 모델을 사용한 제로샷 및 퓨샷 학습 설정에서의 모델 성능을 비교 분석하는 데 사용됩니다.
> <details>
> <summary>read the caption</summary>
> Table 10: Model performance (Macro F1-score) for zero- and-few shot classifiers across the different languages in AfriHate. The best performance for each language is highlighted in bold. This is an average over 5 prompt templates.
> </details>

{{< table-caption >}}
model
#shots
`amh`
`ary`
`arq`
`hau`
`ibo`
`kin`
`oro`
`pcm`
`som`
`swa`
`tir`
`twi`
`xho`
`yor`
`zul`
Avg.
SetFit
0
33.20
42.92
26.63
24.02
38.61
21.29
28.72
39.23
29.26
35.26
47.32
52.15
37.62
41.27
35.03
35.50
5
45.11
35.91
31.89
50.81
50.30
51.26
36.50
52.54
49.93
54.83
31.63
38.40
46.14
31.62
35.71
42.84
10
49.26
37.05
44.27
50.43
40.80
48.60
42.56
56.87
40.27
62.88
39.74
49.28
42.60
49.21
42.03
46.39
20
50.33
40.92
53.25
54.34
65.65
57.28
43.08
58.44
48.86
74.68
38.95
55.87
46.78
53.85
51.10
52.89
InkubaLM-0.4B
0
39.74
23.62
30.26
22.66
23.04
30.96
41.14
19.84
28.30
27.02
47.24
23.16
27.30
23.70
27.38
29.02
5
39.12
23.72
31.88
25.28
34.36
32.60
44.88
23.74
30.86
25.62
48.12
43.96
28.14
26.88
27.52
32.45
10
37.20
29.52
32.64
38.38
27.74
37.58
43.26
28.82
41.26
31.22
45.84
32.90
34.22
31.54
35.76
35.19
20
37.64
29.18
32.68
38.64
28.70
37.84
43.06
28.98
41.34
31.37
48.88
32.30
33.82
30.84
36.12
35.43
mt0-small
0
38.14
23.36
31.76
28.70
17.02
35.56
43.74
22.70
31.34
27.47
42.80
14.66
28.32
23.16
27.62
29.09
5
36.70
24.90
34.12
29.18
29.48
31.36
41.10
32.06
25.26
27.90
42.56
16.20
33.04
28.38
32.86
31.01
10
37.40
25.74
35.06
26.36
33.44
28.30
40.46
31.98
24.00
23.37
43.04
15.88
31.32
29.72
32.00
30.54
20
36.50
25.98
35.08
26.20
34.50
28.44
40.30
31.54
23.86
23.40
44.76
17.08
31.68
28.92
32.30
30.70
bloomz-7b1-mt
0
34.62
34.38
33.08
32.66
29.18
36.20
37.30
32.76
33.90
38.53
35.82
28.88
31.94
33.70
32.24
33.68
5
32.36
37.34
46.74
37.52
62.56
30.88
31.96
48.14
33.90
40.97
36.68
68.50
39.36
48.86
39.64
42.36
10
35.16
36.96
45.92
37.72
62.40
27.74
33.06
47.10
35.04
36.97
38.70
67.94
38.56
47.66
39.44
42.02
20
33.16
35.60
44.18
37.88
63.12
29.12
31.82
47.04
34.78
37.93
38.72
68.84
38.14
49.08
39.08
41.90
Mistral-7B-v0.1
0
42.26
30.62
17.28
10.52
13.00
27.62
45.42
11.14
13.86
18.88
55.02
14.28
16.36
13.44
14.54
22.95
5
37.92
43.46
57.14
51.96
58.42
50.40
45.70
59.32
52.64
58.58
41.36
69.52
47.50
57.72
45.96
51.84
10
40.42
46.68
60.66
56.38
61.74
50.00
46.70
60.42
51.50
63.18
44.88
70.26
46.40
60.50
46.42
53.74
20
39.48
45.08
61.70
56.98
65.10
55.64
48.82
61.22
54.02
64.28
51.38
70.04
48.64
64.94
50.16
55.83
aya-23-35B
0
34.54
32.76
28.52
37.76
18.66
41.04
42.48
30.34
40.76
34.04
35.84
14.40
35.42
28.22
35.96
32.72
5
42.20
61.86
64.38
55.32
56.42
56.56
49.96
61.12
57.46
69.84
45.70
63.86
48.96
63.48
51.14
56.55
10
45.74
61.22
69.00
59.78
63.98
58.66
52.30
62.08
56.98
74.10
51.08
69.58
49.06
66.50
52.44
59.50
20
46.76
59.70
71.40
59.84
68.30
62.06
54.14
65.44
57.78
75.68
56.90
71.20
51.72
70.54
56.12
61.84
Llama-3.1-8B
0
29.14
34.10
36.04
55.82
22.60
48.60
42.96
39.84
55.00
49.04
23.18
15.20
46.18
38.02
47.42
38.88
5
41.50
50.12
63.90
46.56
64.48
42.24
34.94
61.22
39.08
63.88
35.80
68.12
43.66
58.14
43.30
50.46
10
45.16
51.70
66.04
47.72
66.12
44.60
39.16
62.70
36.90
68.54
39.88
69.84
43.20
60.88
42.68
52.34
20
50.20
50.56
67.92
48.18
66.92
48.50
44.10
64.28
38.70
72.00
48.26
70.40
42.72
62.24
43.16
54.54
Gemma-2-9B
0
48.26
37.78
26.88
17.60
16.24
34.34
46.86
26.68
16.22
37.12
57.98
27.20
18.20
22.96
16.58
30.06
5
61.10
63.62
65.72
55.20
62.84
63.24
56.34
63.36
54.46
74.78
62.12
65.44
46.24
70.14
52.12
61.11
10
61.26
63.78
65.98
60.64
66.38
64.60
56.40
64.08
57.28
78.16
61.20
69.76
49.66
73.02
54.74
63.13
20
64.14
61.86
67.90
60.26
69.38
65.48
57.24
64.50
58.62
79.24
62.98
70.68
50.02
73.96
56.00
64.15
Gemma-2-27B
0
53.40
46.38
44.10
33.62
31.08
49.02
50.82
54.20
38.16
59.88
55.58
31.34
29.66
45.96
31.90
43.67
5
61.40
66.00
69.40
60.44
68.30
63.26
56.34
64.44
62.34
80.18
59.90
64.34
51.36
75.60
56.24
63.97
10
62.44
64.76
69.74
64.70
70.62
66.22
56.24
64.22
61.24
81.94
59.52
70.82
53.18
75.34
59.16
65.34
20
64.12
63.60
70.24
65.74
72.84
67.72
59.18
66.06
63.20
82.98
64.44
73.28
56.04
76.50
61.18
67.14
Llama-3.1-70B
0
46.28
49.84
52.30
51.26
43.72
50.38
48.30
56.50
46.38
60.14
43.64
35.68
42.64
51.34
47.00
48.36
5
59.10
68.32
67.12
58.96
66.88
60.90
54.24
67.20
61.20
75.76
45.18
66.18
50.18
72.60
57.30
62.07
10
61.36
67.80
67.70
63.00
68.78
61.50
55.86
66.62
63.38
78.74
50.48
68.34
52.04
73.22
58.92
63.85
20
60.72
65.62
69.48
64.00
70.02
65.08
58.78
67.02
64.46
80.02
56.40
72.52
53.50
74.98
61.26
65.59
GPT-4o
0
61.70
56.05
66.45
45.90
44.21
44.92
44.93
49.67
38.50
57.24
46.01
33.90
36.72
48.62
36.34
47.41
5
66.69
73.26
71.03
53.24
74.23
72.39
65.30
55.93
56.19
79.02
57.64
61.36
51.81
69.65
58.45
64.41
10
67.09
75.12
73.81
55.28
78.02
74.47
66.97
56.47
58.34
72.83
56.50
61.11
61.61
70.78
65.12
66.23
20
67.22
76.05
75.00
56.08
78.17
74.85
66.01
50.33
61.95
84.14
56.91
51.67
62.78
72.22
61.84
66.35
Lang. avg.
-
47.21
47.15
51.73
45.76
51.10
48.39
47.03
49.50
44.74
56.22
47.65
50.37
42.15
51.59
43.85
48.30{{< /table-caption >}}
> 🔼 표 11은 AfriHate 데이터셋의 14개 언어에 대해 제로샷 및 퓨샷 분류기의 성능을 보여줍니다. 매크로 F1 점수를 사용하여 모델 성능을 평가했으며, 각 언어별 최고 성능은 굵게 표시했습니다. 이 표는 다양한 크기의 사전 훈련된 언어 모델과 몇몇 퓨샷 학습 방법의 성능을 비교 분석하여 AfriHate 데이터셋에서의 낮은 자원 언어 처리에 대한 통찰력을 제공합니다.
> <details>
> <summary>read the caption</summary>
> Table 11: Model performance (Macro F1-score) for zero- and few shot classifiers across the 14 languages in AfriHate. Best performance for each language is highlighted in bold.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}