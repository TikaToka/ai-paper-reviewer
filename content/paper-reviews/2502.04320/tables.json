[{"content": "| Method | Architecture | ImageNet-Segmentation Acc \u2191 | ImageNet-Segmentation mIoU \u2191 | ImageNet-Segmentation mAP \u2191 | PascalVOC (Single Class) Acc \u2191 | PascalVOC (Single Class) mIoU \u2191 | PascalVOC (Single Class) mAP \u2191 |\n|---|---|---|---|---|---|---|---|\n| LRP (Binder et al., 2016) | CLIP ViT | 51.09 | 32.89 | 55.68 | 48.77 | 31.44 | 52.89 |\n| Partial-LRP (Binder et al., 2016) | CLIP ViT | 76.31 | 57.94 | 84.67 | 71.52 | 51.39 | 84.86 |\n| Rollout (Abnar & Zuidema, 2020) | CLIP ViT | 73.54 | 55.42 | 84.76 | 69.81 | 51.26 | 85.34 |\n| ViT Attention (Dosovitskiy et al., 2021) | CLIP ViT | 67.84 | 46.37 | 80.24 | 68.51 | 44.81 | 83.63 |\n| GradCAM (Selvaraju et al., 2020) | CLIP ViT | 64.44 | 40.82 | 71.60 | 70.44 | 44.90 | 76.80 |\n| TextSpan (Gandelsman et al., 2024) | CLIP ViT | 75.21 | 54.50 | 81.61 | 75.00 | 56.24 | 84.79 |\n| TransInterp (Chefer et al., 2021) | CLIP ViT | 79.70 | 61.95 | 86.03 | 76.90 | 57.08 | 86.74 |\n| DINO Attention (Caron et al., 2021) | DINO ViT | 81.97 | 69.44 | 86.12 | 80.71 | 64.33 | 88.90 |\n| DAAM (Tang et al., 2022) | SDXL UNet | 78.47 | 64.56 | 88.79 | 72.76 | 55.95 | 88.34 |\n| DAAM (Tang et al., 2022) | SD2 UNet | 64.52 | 47.62 | 78.01 | 64.28 | 45.01 | 83.04 |\n| Flux Cross Attention | Flux DiT | 74.92 | 59.90 | 87.23 | 80.37 | 54.77 | 89.08 |\n| ConceptAttention | Flux DiT | **83.07** | **71.04** | **90.45** | **87.85** | **76.45** | **90.19** |", "caption": "Table 1: ConceptAttention outperforms a variety of Diffusion, DINO, and CLIP ViT interpretability methods on ImageNet-Segmentation and PascalVOC (Single Class).", "description": "\ud45c 1\uc740 ImageNet-Segmentation \ub370\uc774\ud130\uc14b\uacfc Pascal VOC \ub370\uc774\ud130\uc14b(\ub2e8\uc77c \ud074\ub798\uc2a4)\uc5d0\uc11c ConceptAttention\uc774 \ub2e4\uc591\ud55c Diffusion, DINO, CLIP ViT \ud574\uc11d \uac00\ub2a5\uc131 \ubc29\ubc95\ub4e4\uc744 \uc131\ub2a5 \uba74\uc5d0\uc11c \ub2a5\uac00\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ConceptAttention\uc758 \uc815\ud655\ub3c4(Accuracy), \ud3c9\uade0 IoU(mIoU), \ud3c9\uade0 \uc815\ubc00\ub3c4(mAP)\ub97c \ub2e4\ub978 \uc5ec\ub7ec \ubc29\ubc95\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc81c\uc2dc\ub41c \ubc29\ubc95\ub4e4\uc758 zero-shot \uc774\ubbf8\uc9c0 \ubd84\ud560 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec ConceptAttention\uc758 \uc6b0\uc218\uc131\uc744 \uc218\uce58\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uae30\ubc18 \ubaa8\ub378(Diffusion, DINO, CLIP)\uc744 \uc0ac\uc6a9\ud55c \uc5ec\ub7ec \ud574\uc11d \ubc29\ubc95\ub4e4\uacfc \ube44\uad50\ud558\uc5ec ConceptAttention\uc758 \uc131\ub2a5 \uc6b0\uc704\ub97c \uba85\ud655\ud788 \uc81c\uc2dc\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8"}, {"content": "| Space | Softmax | Acc\u2191 | mIoU\u2191 | mAP\u2191 |\n|---|---|---|---|---|\n| CA |  | 66.59 | 49.91 | 73.17 |\n| CA | \u2713 | 74.92 | 59.90 | 87.23 |\n| Value |  | 45.93 | 29.81 | 65.79 |\n| Value | \u2713 | 45.78 | 29.68 | 39.61 |\n| Output |  | 78.75 | 64.95 | 88.39 |\n| Output | \u2713 | **83.07** | **71.04** | **90.45** |", "caption": "Table 2: The output space of DiT attention layers produces more transferable representations than cross attentions.  We explore the transferability of several representation spaces of a DiT: the cross attentions (CA), the value space, and the output space. We performed linear projections of the image patches and concept vectors in each of these spaces and evaluated their performance on ImageNet-Segmentation.", "description": "\ubcf8 \ud45c\ub294 DiT(Diffusion Transformer)\uc758 \uc5ec\ub7ec \ud45c\ud604 \uacf5\uac04(cross-attention, value space, output space)\uc758 ImageNet-Segmentation \uc791\uc5c5\uc5d0\uc11c\uc758 \uc804\uc774 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uacf5\uac04\uc5d0\uc11c \uc774\ubbf8\uc9c0 \ud328\uce58\uc640 \uac1c\ub150 \ubca1\ud130\uc5d0 \ub300\ud55c \uc120\ud615 \ud22c\uc601\uc744 \uc218\ud589\ud558\uace0 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uc5ec, DiT \uc5b4\ud150\uc158 \ub808\uc774\uc5b4\uc758 \ucd9c\ub825 \uacf5\uac04\uc774 cross-attention\ubcf4\ub2e4 \ub354 \ub098\uc740 \uc804\uc774 \uac00\ub2a5\ud55c \ud45c\ud604\uc744 \uc0dd\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8"}, {"content": "| CA | SA | Acc \u2191 | mIoU \u2191 | mAP \u2191 |\n|---|---|---|---|---|\n|  |  | 52.63 | 35.72 | 70.21 |\n|  | \u2713 | 51.68 | 34.85 | 69.36 |\n| \u2713 |  | 76.51 | 61.96 | 86.73 |\n| \u2713 | \u2713 | **83.07** | **71.04** | **90.45** |", "caption": "Table 3: ConceptAttention performs best when we utilize both cross and self attention. We tested the effectiveness of performing just a cross attention operation between the concepts and image tokens, just a self attention among the concepts, both cross and self attention, and neither. We found that doing both operations leads to the best results. Metrics are computed on the ImageNet Segmentation benchmark.", "description": "\ud45c 3\uc740 \uac1c\ub150 \uc5b4\ud150\uc158\uc5d0\uc11c \uc0c1\ud638 \uc5b4\ud150\uc158\uacfc \uc790\uae30 \uc5b4\ud150\uc158\uc744 \ubaa8\ub450 \uc0ac\uc6a9\ud588\uc744 \ub54c \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac1c\ub150\uacfc \uc774\ubbf8\uc9c0 \ud1a0\ud070 \uac04\uc758 \uc0c1\ud638 \uc5b4\ud150\uc158 \uc5f0\uc0b0\ub9cc \uc218\ud589, \uac1c\ub150 \uac04\uc758 \uc790\uae30 \uc5b4\ud150\uc158\ub9cc \uc218\ud589, \uc0c1\ud638 \ubc0f \uc790\uae30 \uc5b4\ud150\uc158 \ubaa8\ub450 \uc218\ud589, \ub458 \ub2e4 \uc218\ud589\ud558\uc9c0 \uc54a\uc74c \ub4f1 \ub2e4\uc591\ud55c \ubc29\ubc95\uc744 \ud14c\uc2a4\ud2b8\ud588\uc2b5\ub2c8\ub2e4. \ub450 \uc5f0\uc0b0\uc744 \ubaa8\ub450 \uc218\ud589\ud558\ub294 \uac83\uc774 \uac00\uc7a5 \uc88b\uc740 \uacb0\uacfc\ub97c \uc5bb\uc5c8\uc2b5\ub2c8\ub2e4. \uba54\ud2b8\ub9ad\uc740 ImageNet Segmentation \ubca4\uce58\ub9c8\ud06c\ub97c \uae30\ubc18\uc73c\ub85c \uacc4\uc0b0\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "4.1 CONCEPTATTENTION \uc0ac\uc6a9"}, {"content": "| Method | Acc\u2191 | mIoU\u2191 |\n|---|---|---|\n| TextSpan | 73.84 | 38.10 |\n| DAAM | 62.89 | 10.97 |\n| Flux Cross Attention | 79.52 | 27.04 |\n| ConceptAttention | **86.99** | **51.39** |", "caption": "Table 4: ConceptAttention outperforms alternative methods on images with multiple classes from PascalVOC. Notably, the margin between ConceptAttention and other methods is even higher for this task than when a single class is in each image.", "description": "\ud45c 4\ub294 Pascal VOC \ub370\uc774\ud130\uc14b\uc758 \uc5ec\ub7ec \ud074\ub798\uc2a4\uac00 \ud3ec\ud568\ub41c \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 ConceptAttention\uc774 \ub2e4\ub978 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc6b0\uc218\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \uac01 \uc774\ubbf8\uc9c0\uc5d0 \ub2e8\uc77c \ud074\ub798\uc2a4\ub9cc \uc788\ub294 \uacbd\uc6b0\ubcf4\ub2e4 \uc5ec\ub7ec \ud074\ub798\uc2a4\uac00 \uc788\ub294 \uacbd\uc6b0 ConceptAttention\uacfc \ub2e4\ub978 \ubc29\ubc95\ub4e4 \uac04\uc758 \uc131\ub2a5 \ucc28\uc774\uac00 \ub354 \ud06c\ub2e4\ub294 \uc810\uc5d0 \uc8fc\ubaa9\ud560 \ub9cc\ud569\ub2c8\ub2e4. \uc774\ub294 ConceptAttention\uc774 \ub2e4\uc591\ud55c \ud074\ub798\uc2a4\uac00 \uc874\uc7ac\ud558\ub294 \ubcf5\uc7a1\ud55c \uc774\ubbf8\uc9c0\uc5d0\uc11c\ub3c4 \uac1c\ubcc4 \uac1c\uccb4\ub97c \uc815\ud655\ud558\uac8c \uc2dd\ubcc4\ud558\uace0 \uad6c\ubd84\ud558\ub294 \ub2a5\ub825\uc774 \ub6f0\uc5b4\ub0a8\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8"}]