{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper is foundational for the field of large language models (LLMs), introducing the concept of few-shot learning which is fundamental to many of the advancements discussed in the target paper."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-00-00", "reason": "This paper established the scaling laws that govern the performance of LLMs, providing crucial insights into the relationship between model size, compute, and accuracy, a topic central to the target paper's investigation."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-00-00", "reason": "This paper introduced the chain-of-thought prompting technique, which directly relates to the core methodology and findings of the target paper regarding the impact of reasoning length on model performance."}, {"fullname_first_author": "Niklas Muennighoff", "paper_title": "s1: Simple test-time scaling", "publication_date": "2025-00-00", "reason": "This paper directly addresses test-time compute scaling which is a crucial aspect of the target paper's analysis on how efficient reasoning is impacted by computational resources."}, {"fullname_first_author": "Bofei Gao", "paper_title": "Omni-math: A universal olympiad level mathematic benchmark for large language models", "publication_date": "2024-00-00", "reason": "This paper introduces the Omni-MATH benchmark dataset, which is the primary dataset used in the target paper, making it essential for understanding the context and results of the study."}]}