---
title: "Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis"
summary: "Infinity, a novel bitwise autoregressive model, sets new records in high-resolution image synthesis, outperforming top diffusion models in speed and quality."
categories: ["AI Generated", "ü§ó Daily Papers"]
tags: ["Computer Vision", "Image Generation", "üè¢ ByteDance",]
showSummary: true
date: 2024-12-05
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2412.04431 {{< /keyword >}}
{{< keyword icon="writer" >}} Jian Han et el. {{< /keyword >}}
 
{{< keyword >}} ü§ó 2024-12-06 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2412.04431" target="_self" >}}
‚Üó arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2412.04431" target="_self" >}}
‚Üó Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/infinity-scaling-bitwise-autoregressive" target="_self" >}}
‚Üó Papers with Code
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/2412.04431/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

Current text-to-image models face challenges such as limited detail in high-resolution images and slow generation speeds, especially for autoregressive models.  Existing models often rely on index-wise tokenization, which restricts scalability and can lead to quantization errors impacting image quality.  Furthermore, training methods like teacher-forcing can introduce inconsistencies between training and inference.

The researchers introduced Infinity, a bitwise autoregressive model that tackles these issues. By utilizing a bitwise visual tokenizer, infinite-vocabulary classifier, and a bitwise self-correction mechanism, Infinity significantly improves generation capacity and detail.  The model establishes new benchmarks for autoregressive text-to-image models, outperforming leading diffusion models in multiple evaluation metrics while achieving much faster inference speeds. The use of bitwise tokens offers a potential pathway to more efficient and scalable visual generation.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Infinity achieves state-of-the-art results in high-resolution image synthesis, surpassing leading diffusion models. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The novel bitwise modeling approach significantly improves the scalability and detail of autoregressive models. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Infinity demonstrates impressive speed, generating high-quality 1024x1024 images in under a second. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
This paper is crucial because it **significantly advances high-resolution image synthesis**, achieving state-of-the-art results and surpassing leading diffusion models in speed and quality.  It introduces novel bitwise modeling techniques opening new avenues for visual generation research.  This **pushes the boundaries of autoregressive models** and offers a potentially more efficient and scalable approach for future advancements in image generation.

------
#### Visual Insights



![](https://arxiv.org/html/2412.04431/x1.png)

> üîº Figure 1 presents several high-resolution images generated by the Infinity model, demonstrating its ability to accurately interpret and generate images based on diverse textual prompts.  The images showcase various capabilities of the model: precise adherence to the instructions within prompts (prompt following), creation of images with realistic spatial relationships between objects (spatial reasoning), accurate rendering of text with different styles and fonts (text rendering), and high aesthetic quality across different artistic styles and aspect ratios. This visual demonstration highlights Infinity's superior performance in advanced image synthesis.
> <details>
> <summary>read the caption</summary>
> Figure 1: High-resolution image synthesis results from Infinity, showcasing its capabilities in precise prompt following, spatial reasoning, text rendering, and aesthetics across different styles and aspect ratios.
> </details>





{{< table-caption >}}
| Methods | # Params | GenEval Two Obj. | GenEval Position | GenEval Color Attri. | GenEval Overall | DPG Global | DPG Relation | DPG Overall |
|---|---|---|---|---|---|---|---|---|
| Diffusion Models |  |  |  |  |  |  |  |  |
| LDM [49] | 1.4B | 0.29 | 0.02 | 0.05 | 0.37 | - | - | - |
| SDv1.5 [49] | 0.9B | 0.38 | 0.04 | 0.06 | 0.43 | 74.63 | 73.49 | 63.18 |
| PixArt-alpha [13] | 0.6B | 0.50 | 0.08 | 0.07 | 0.48 | 74.97 | 82.57 | 71.11 |
| SDv2.1 [49] | 0.9B | 0.51 | 0.07 | 0.17 | 0.50 | 77.67 | 80.72 | 68.09 |
| DALL-E 2 [45] | 6.5B | 0.66 | 0.10 | 0.19 | 0.52 | - | - | - |
| DALL-E 3 [7] | - | - | - | - | 0.67‚Ä† | 90.97 | 90.58 | 83.50 |
| SDXL [43] | 2.6B | 0.74 | 0.15 | 0.23 | 0.55 | 83.27 | 86.76 | 74.65 |
| PixArt-Sigma [12] | 0.6B | 0.62 | 0.14 | 0.27 | 0.55 | 86.89 | 86.59 | 80.54 |
| SD3 (d=24) [21] | 2B | 0.74 | 0.34 | 0.36 | 0.62 | - | - | 84.08 |
| SD3 (d=38) [21] | 8B | 0.89 | 0.34 | 0.47 | 0.71 | - | - | - |
| AutoRegressive Models |  |  |  |  |  |  |  |  |
| LlamaGen [55] | 0.8B | 0.34 | 0.07 | 0.04 | 0.32 |  |  | 65.16 |
| Chameleon [59] | 7B | - | - | - | 0.39 | - | - | - |
| HART [58] | 732M | - | - | - | 0.56 | - | - | 80.89 |
| Show-o [70] | 1.3B | 0.80 | 0.31 | 0.50 | 0.68 | - | - | 67.48 |
| Emu3 [66] | 8.5B | 0.81‚Ä† | 0.49‚Ä† | 0.45‚Ä† | 0.66‚Ä† | - | - | 81.60 |
| Infinity | 2B | 0.85‚Ä† | 0.49‚Ä† | 0.57‚Ä† | 0.73‚Ä† | 93.11 | 90.76 | 83.46 |{{< /table-caption >}}

> üîº This table presents a quantitative comparison of different text-to-image generation models on two benchmark datasets: GenEval and DPG.  GenEval assesses the model's ability to generate images that accurately reflect the given prompt, focusing on aspects like object presence, attributes, and relationships.  The DPG benchmark evaluates the overall quality of the generated images.  The table shows each model's performance in terms of generating images with two objects, accurate positioning, correct color representation, and appropriate attributes.  The overall scores for each dataset are displayed, along with a breakdown of the scores on global and relational aspects of the image generation task.  A symbol ( ‚Ä† ) indicates that prompt rewriting was used for that particular model's evaluation.
> <details>
> <summary>read the caption</summary>
> Table 1: Evaluation on the GenEval¬†[24] and DPG¬†[29] benchmark. ‚Ä†‚Ä†\dagger‚Ä† result is with prompt rewriting.
> </details>





### In-depth insights


#### Bitwise Autoregressive
The concept of "Bitwise Autoregressive" modeling presents a novel approach to image synthesis.  It leverages the strengths of autoregressive models, particularly their scalability and generalizability, while addressing limitations in high-resolution image generation.  **The core innovation lies in replacing traditional index-wise tokenization with bitwise tokenization.** This allows for a vastly expanded vocabulary size, effectively approaching an infinite vocabulary, leading to more precise and detailed image reconstructions.  Furthermore, **the bitwise framework enables the development of an infinite-vocabulary classifier**, drastically reducing the number of parameters required compared to traditional classifiers and alleviating computational constraints associated with extremely large vocabularies.  The incorporation of **bitwise self-correction mechanisms enhances training stability and improves generation quality by mitigating the train-test discrepancy common in autoregressive models.** This novel approach, therefore, represents a significant advancement in visual autoregressive modeling, offering improved scalability, efficiency, and image quality compared to previous methods.

#### Infinite Vocabulary
The concept of "Infinite Vocabulary" in the context of image generation models signifies a significant advancement in autoregressive modeling.  Traditional methods relied on fixed-size vocabularies for representing image tokens, leading to limitations in capturing intricate details and diverse visual styles.  **An infinite vocabulary overcomes this constraint by allowing the model to represent an unbounded number of visual tokens**, essentially handling any image feature without quantization limitations. This approach theoretically eliminates the information loss inherent in mapping continuous visual data to a discrete vocabulary, paving the way for **higher-fidelity image generation with richer details and more nuanced aesthetics**.  **The practical implementation likely involves techniques like bitwise modeling**, which encodes visual features using a sequence of bits instead of fixed-length indices. This permits an exponentially larger vocabulary size while maintaining computational efficiency.  **This innovative method significantly enhances the model's expressive power, allowing it to handle highly detailed and complex images more effectively**, achieving near-continuous representation similar to variational autoencoders but with the added advantage of causal autoregressive modeling, thus significantly improving generation quality and enabling more robust image synthesis.

#### Self-Correction Training
Self-correction training, in the context of visual autoregressive models, addresses the inherent limitations of traditional teacher-forcing methods.  Teacher-forcing, while effective in initial training, often leads to a significant **train-test discrepancy**, hindering the model's ability to generalize to unseen data.  The core idea behind self-correction is to introduce controlled noise or errors into the training process, forcing the model to learn to identify and correct these mistakes. This is achieved by deliberately perturbing the training data, either randomly or systematically, and allowing the model to recover from these perturbations. This approach is especially valuable in high-resolution image generation where cumulative errors during sequential prediction can severely impact the final result.  By incorporating self-correction, the model learns to be more robust and less susceptible to cascading errors, leading to improved image quality and detail preservation. **Bitwise self-correction**, as presented in the research, offers a particularly efficient and effective mechanism by manipulating individual bits within the discrete token representations. This allows for fine-grained control over the introduced noise, enabling the model to learn more effectively from subtle errors. The resulting models demonstrate improved generation quality, achieving a higher degree of visual fidelity and adhering more precisely to the given textual prompts.  The process also enhances the models' capacity for generating images across diverse styles and aspect ratios.

#### Scaling Laws in VAR
The concept of "Scaling Laws in VAR" (Visual Autoregressive models) explores how performance changes as key model parameters, like the number of parameters, dataset size, and computational resources, increase.  **Understanding these scaling laws is crucial for efficient model development.**  A key insight is that simply increasing model size without considering other factors may not yield proportional improvements.  Instead, **optimal scaling involves a balanced increase across all relevant parameters**, achieving a synergy where the combined impact exceeds the sum of individual effects.  This often requires carefully curated datasets and efficient training strategies. **Research into VAR scaling laws aims to identify optimal resource allocation for achieving target performance levels with maximal efficiency.**  The potential for substantial performance gains through strategic scaling makes understanding scaling laws a critical area for improving visual autoregressive model capabilities and reducing computational costs.  **Further research could focus on quantifying the relationships between different parameters and exploring novel training techniques to optimize scaling behavior.** This improved understanding could lead to a more efficient design of future VAR models, ultimately enabling the generation of higher-quality images with less computational expense.

#### High-Res Image Synth
High-resolution image synthesis (High-Res Image Synth) is a challenging area of research, aiming to generate realistic and detailed images at high resolutions.  The advancements discussed in the paper significantly impact this field. **Bitwise autoregressive modeling**, a novel approach, is presented as a key innovation, enabling the generation of high-quality 1024x1024 images efficiently. This method surpasses traditional index-wise tokenization, leading to better detail and faster generation.  The **Infinite-Vocabulary Classifier** is another significant contribution, resolving computational limitations associated with very large vocabularies in autoregressive models.  Furthermore, the technique of **bitwise self-correction** addresses training issues, enhancing the model's ability to generate consistent and accurate results.  **Dynamic aspect ratio handling** expands the capabilities of the model, enabling it to generate images of various sizes, rather than being limited to square images.  The paper's empirical results and benchmark comparisons showcase that this method achieves state-of-the-art performance, outperforming leading diffusion models in speed and quality, suggesting significant progress in the field of high-resolution image synthesis.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2412.04431/x2.png)

> üîº Figure 2 illustrates the core concept of the visual tokenizer used in Infinity.  The left side shows a conventional classifier that predicts indices from continuous features.  A small change in the continuous features can result in a large change in the predicted index, leading to instability and requiring an exponentially growing number of parameters as the number of bits increases (d). The right side shows Infinity's Infinite-Vocabulary Classifier (IVC) which predicts individual bits instead of indices.  This approach makes the classifier significantly more robust to small perturbations of continuous values, with only a small change in predicted bit values resulting from small changes in input values.  Crucially, the number of parameters required by the IVC increases only linearly with the number of bits, making it highly scalable even with a massive vocabulary, in contrast to the exponentially growing needs of the conventional classifier. The example of d=32 and h=2048 illustrates the huge difference in parameter counts: 8.8 trillion for the conventional classifier versus 0.13 million for the IVC.
> <details>
> <summary>read the caption</summary>
> Figure 2: Visual tokenizer quantizes continuous features and then gets index labels. Conventional classifier (left) predicts 2dsuperscript2ùëë2^{d}2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT indices. Infinite-Vocabulary Classifier (right) predicts dùëëditalic_d bits instead. Slight perturbations to near-zero values in continuous features cause a complete change of index labels. Bit labels (i.e. quantized features) change subtly and still provide steady supervision. Besides, parameters of conventional classifiers grow exponentially as dùëëditalic_d increases, while IVC grows linearly. If d=32ùëë32d=32italic_d = 32 and h=2048‚Ñé2048h=2048italic_h = 2048, the conventional classifier requires 8.8 trillion parameters, exceeding current compute limits. By contrast, IVC only requires 0.13M parameters.
> </details>



![](https://arxiv.org/html/2412.04431/x3.png)

> üîº Figure 3 illustrates the architecture of the Infinity model, which uses bitwise modeling.  The core components are:  1. **Bitwise Multi-Scale Visual Tokenizer:** This tokenizer encodes the input image into a sequence of residual feature maps (R1, R2, ..., Rk) at different resolutions (scales).  Each Rk represents the residual information at a particular scale. This is a departure from traditional tokenization methods that use a single, fixed vocabulary.  2. **Infinite-Vocabulary Classifier (IVC):**  Instead of predicting indices from a fixed-size vocabulary (like in conventional models), the IVC predicts the bits that compose each token directly.  This allows for a theoretically infinite vocabulary size, significantly increasing model capacity and enabling finer-grained control over image details.  3. **Bitwise Self-Correction:** This mechanism introduces noise or errors into the predictions of earlier residual maps (R1, R2, ..., Rk-1).  The model then attempts to correct these errors, leading to more robust and accurate image generation.  This technique helps to mitigate cumulative errors from the autoregressive nature of the model.  The process is autoregressive; each residual map Rk is predicted based on the preceding maps (R1, R2, ..., Rk-1) and the text prompt, using a cross-attention mechanism.  Importantly, the prediction is performed at the bit level instead of the index level, leading to a more efficient and powerful model.
> <details>
> <summary>read the caption</summary>
> Figure 3: Framework of Infinity. Infinity introduces bitwise modeling, which incorporates a bitwise multi-scale visual tokenizer, Infinite-Vocabulary Classifier (IVC), and Bitwise Self-Correction. When predicting ùëπksubscriptùëπùëò\bm{R}_{k}bold_italic_R start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, the sequence (ùëπ1,ùëπ2,‚Ä¶,ùëπk‚àí1)subscriptùëπ1subscriptùëπ2‚Ä¶subscriptùëπùëò1(\bm{R}_{1},{\bm{R}}_{2},...,\bm{R}_{k-1})( bold_italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_R start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT ) serves as the prefixed context and the text condition guides the prediction through a cross attention mechanism. Different from VAR, Infinity performs next-scale prediction with bit labels.
> </details>



![](https://arxiv.org/html/2412.04431/x4.png)

> üîº This figure showcases examples of high-resolution images generated by the Infinity model.  Each image is accompanied by a short description of the prompt used to generate it. The images demonstrate Infinity's capabilities in various aspects of image synthesis including precise prompt following, accurate text rendering, diverse artistic styles and accurate depiction of spatial relationships. The variety of scenes and styles exemplifies the model's versatility and ability to generate photorealistic results.
> <details>
> <summary>read the caption</summary>
> Figure 4: Qualitative results from Infinity.
> </details>



![](https://arxiv.org/html/2412.04431/extracted/6046736/images/human_preference.png)

> üîº This figure displays the results of a human preference evaluation comparing the image generation quality of the Infinity model to several other open-source models. Participants were shown pairs of images, one generated by Infinity and one by a competing model, and asked to choose the image they preferred based on three criteria: overall quality, prompt following, and visual aesthetics.  The bar chart visualizes the win rate (percentage of times each model was preferred) for each model across these three criteria, showing that Infinity was significantly preferred by the participants compared to the other models.
> <details>
> <summary>read the caption</summary>
> Figure 5: Human Preference Evaluation. We ask users to select the better one in a side-by-side comparison in terms of Overall Quality, Prompt Following, and Visual Aesthetics. Infinity is more preferred by humans compared to other open-source models.
> </details>



![](https://arxiv.org/html/2412.04431/x5.png)

> üîº This figure presents a qualitative comparison of prompt-following capabilities across six different text-to-image models, including Infinity-2B and four others.  Each row shows the same prompt given to each model, and the generated images are displayed side-by-side. Key phrases from the prompt are highlighted in red to emphasize instances where Infinity-2B faithfully renders the text while other models fail to do so. This demonstrates Infinity-2B's superior ability to accurately follow detailed instructions provided in prompts.
> <details>
> <summary>read the caption</summary>
> Figure 6: Prompt-following qualitative comparison. We highlight text in red that Infinity-2B consistently adheres to while the other four models fail to follow. Zoom in for better comparison.
> </details>



![](https://arxiv.org/html/2412.04431/x6.png)

> üîº Figure 7 showcases the text rendering capabilities of the Infinity-2B model.  It presents several examples of images generated from diverse prompts, highlighting the model's ability to accurately render text in various styles, fonts, and contexts.  The prompts range from simple text phrases to more complex descriptions requiring specific styles, settings, and subject matter. The figure demonstrates the model's strong performance in generating text-consistent images regardless of the prompt's complexity or creative demands.
> <details>
> <summary>read the caption</summary>
> Figure 7: Text rendering results from our Infinity-2B model. Infinity-2B could generate text-consistent images following user prompts across diverse categories.
> </details>



![](https://arxiv.org/html/2412.04431/x7.png)

> üîº Figure 8 demonstrates the impact of using an Infinite-Vocabulary Classifier (IVC) for image generation. The left side shows results from a conventional classifier predicting index-wise labels, while the right side displays results from the IVC predicting bitwise labels. The comparison highlights that the IVC produces images with significantly richer details and finer visual qualities. This improvement is attributed to the ability of the IVC to handle a much larger vocabulary space effectively, leading to more precise and nuanced representation of image features during the generation process.
> <details>
> <summary>read the caption</summary>
> Figure 8: Impact of Infinite-Vocabulary Classifier. Predicting bitwise labels with the Infinite-Vocabulary Classifier (Right) generates images with richer details compared to predicting index-wise labels using a conventional classifier (Left).
> </details>



![](https://arxiv.org/html/2412.04431/extracted/6046736/images/scaling_vae_bits_three_column.jpg)

> üîº Figure 9 illustrates the impact of increasing the vocabulary size on model performance in text-to-image generation.  The experiment uses consistent training hyperparameters across different model sizes, focusing on the effect of changing the vocabulary size, represented as Vd (where d is the exponent of 2, indicating the number of bits used in the tokenizer).  Smaller models (125M and 361M parameters) converge faster and achieve better results when the vocabulary size is Vd=2^16. However, when using a larger model (2.2B parameters), using Vd=2^32 outperforms Vd=2^16, demonstrating that the optimal vocabulary size is dependent on the model's complexity. The data used for this experiment comprises 5 million high-quality image-text pairs, and all images were 256x256 pixels.
> <details>
> <summary>read the caption</summary>
> Figure 9: Effects of Scaling Up the Vocabulary. We analyze the impact of scaling the vocabulary size under consistent training hyperparameters throughout. Vocabulary size Vd=216subscriptùëâùëësuperscript216V_{d}=2^{16}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT converges faster and achieves better results for small models (125M and 361M parameters). As we scale up the model size to 2.2B, Infinity with a vocabulary size Vd=232subscriptùëâùëësuperscript232V_{d}=2^{32}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 32 end_POSTSUPERSCRIPT beats that one with Vd=216subscriptùëâùëësuperscript216V_{d}=2^{16}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT. Experiment with 5M high-quality image-text pair data under 256√ó256256256256\times 256256 √ó 256 resolution.
> </details>



![](https://arxiv.org/html/2412.04431/extracted/6046736/images/scaling_models.jpg)

> üîº This figure shows the impact of scaling the model size on the performance of visual autoregressive modeling.  The experiment involved training models with 10 million pre-training data points and images at a 256x256 resolution.  The key finding is that as the model size increases, the validation loss consistently decreases, indicating improved model performance.  Furthermore, the validation loss serves as a strong indicator of overall model performance, exhibiting a high correlation with holistic image evaluation metrics.  This supports the idea that larger models trained for longer durations lead to better results.
> <details>
> <summary>read the caption</summary>
> Figure 10: Effects of Scaling Visual AutoRegressive Modeling. We analyze the impact of scaling model size under consistent training hyperparameters throughout (Experiment with 10M pre-training data and 256√ó256256256256\times 256256 √ó 256 resolution). Validation loss smoothly decreases as a function of the model size and training iterations. Besides, Validation loss is a strong predictor of overall model performance. There is a strong correlation between validation loss and holistic image evaluation metrics.
> </details>



![](https://arxiv.org/html/2412.04431/x8.png)

> üîº Figure 11 visually demonstrates the positive impact of increased model size and training computational resources on the quality of image generation.  As model size and training compute scale up, both the semantic accuracy (how well the generated image reflects the intended meaning) and the visual fidelity (sharpness, detail, and overall realism) show consistent and significant improvement. The figure showcases a series of images generated under various scaling conditions, enabling a clear visual comparison of the quality enhancements.
> <details>
> <summary>read the caption</summary>
> Figure 11: Semantics and visual quality improve consistently with scaling up model size and training compute. Zoom in for better comparison.
> </details>



![](https://arxiv.org/html/2412.04431/x9.png)

> üîº Figure 12 demonstrates the effectiveness of Bitwise Self-Correction in mitigating the train-test discrepancy inherent in teacher-forcing training. The left side shows the results of using teacher-forcing training, where the generated images suffer from significant discrepancies between training and inference. This leads to degraded performance during actual inference. In contrast, the right side illustrates how Bitwise Self-Correction effectively addresses this issue by automatically correcting mistakes during the generation process, thus resulting in significantly improved image quality. The generation parameters used here are œÑ=1 and cfg=3.
> <details>
> <summary>read the caption</summary>
> Figure 12: Impact of Self-Correction. Teacher-forcing training introduces great train-test discrepancy which degrades performance during inference (left). Bitwise Self-Correction auto-corrects mistakes and thus generates better results (right). Decoding with œÑ=1ùúè1\tau=1italic_œÑ = 1 and c‚Å¢f‚Å¢g=3ùëêùëìùëî3cfg=3italic_c italic_f italic_g = 3.
> </details>



![](https://arxiv.org/html/2412.04431/extracted/6046736/images/pe_ablation.jpg)

> üîº Figure 13 presents a comparison of training performance between two approaches to positional encoding in visual autoregressive models: learnable Absolute Position Embeddings (APE) as used in previous work, and the method proposed in the paper, which combines Rotary Position Embedding (RoPE2d) with learnable scale embeddings. The results show that the proposed method, applied to features at each scale, leads to faster convergence and higher training accuracy.  This is likely because RoPE2d preserves the intrinsic 2D structure of images, while learnable scale embeddings efficiently adapt to varying sequence lengths caused by differing aspect ratios at various scales. The learnable APE approach, in contrast, can be less effective due to a much larger number of parameters that need to be optimized and the inherent difficulty of distinguishing features across different resolutions.
> <details>
> <summary>read the caption</summary>
> Figure 13: Comparison between learnable APE and our positional embeddings. Our method, i.e., applying RoPE2d along with learnable scale embeddings on features of each scale, converges faster and reaches higher training accuracy.
> </details>



![](https://arxiv.org/html/2412.04431/x10.png)

> üîº Figure 14 presents a comparison of image generation results using different sampling methods: Greedy Sampling, Normal Sampling, Pyramid CFG, and the authors' proposed method.  The figure visually demonstrates that the authors' method produces images with significantly richer details and a stronger alignment between the generated image content and the input text prompt compared to the other three sampling techniques.  The improved detail and alignment suggest that the authors' sampling strategy is more effective at capturing the nuances of the text prompt and translating them into high-quality, faithful image generation.
> <details>
> <summary>read the caption</summary>
> Figure 14: Comparison of different sampling methods. In contrast to Greedy Sample, Normal Sample and Pyramid Sample, our method could generate images with richer details and higher text-image alignments.
> </details>



![](https://arxiv.org/html/2412.04431/extracted/6046736/images/Categories.png)

> üîº This figure shows a pie chart visualizing the distribution of prompt categories used in the paper's experiments.  The categories represent different subject matters or themes of the image generation prompts, such as human, animal, foods, plants, architecture, products/artifacts, text rendering, landscape, and interior scenes. Each slice of the pie chart is proportionally sized to represent the percentage of prompts belonging to each category. This gives a reader a clear overview of the types of images the model was trained and evaluated on, highlighting the variety and balance in the dataset.
> <details>
> <summary>read the caption</summary>
> Figure 15: Distribution of Prompt Categories
> </details>



![](https://arxiv.org/html/2412.04431/extracted/6046736/images/Prompts_Challenges.png)

> üîº This figure shows the distribution of challenges present in the prompts used to evaluate the image generation models.  The pie chart breaks down the percentage of prompts that fall into various categories of difficulty, such as simple prompts, complex prompts, those requiring nuanced understanding of semantics, specific color requests, precise positioning or perspective, particular artistic styles, detailed descriptions, or creative imagination. The analysis provides insight into the range and complexity of the prompts used for model evaluation, ensuring a comprehensive assessment of the models' capabilities.
> <details>
> <summary>read the caption</summary>
> Figure 16: Distribution of Prompts Challenges
> </details>



![](https://arxiv.org/html/2412.04431/x11.png)

> üîº Figure 17 presents a qualitative comparison of text-to-image (T2I) generation results. It compares the Infinity-2B model against four other open-source models: three diffusion models (Flux Schnell, SD3-Medium, and PixArt Sigma) and one autoregressive model (HART).  The figure showcases several example prompts and their corresponding generated images from each model, highlighting the differences in image quality, detail, adherence to the prompt, and overall visual aesthetics.  By visually examining the generated images for each prompt, one can assess the relative strengths and weaknesses of the different models in terms of their image generation capabilities.  The differences in style, detail, and faithfulness to the prompt provide insights into the various techniques employed in each model. Zooming in on the images enhances the ability to compare fine details and subtle differences between the outputs.
> <details>
> <summary>read the caption</summary>
> Figure 17: T2I qualitative comparison among our Infinity-2B model and the other four open-source models. Here we select three diffusion models (Flux Schnell, SD3-Medium and PixArt Sigma), one AR model (HART) for comparison. Zoom in for better comparsion.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Methods | # Params | ImageReward ‚Üë |  |  | HPSv2.1 ‚Üë |  |  | Latency ‚Üì |  |  |
|---|---|---|---|---|---|---|---|---|---|---|
| SD-XL [43] | 2.6B | 4 | 0.600 | 4 | 30.06 | 4 | 2.7s |  |  |  |
| SD3-Medium [21] | 2B | 3 | 0.871 | 3 | 30.91 | 3 | 2.1s |  |  |  |
| PixArt Sigma [12] | 630M | 2 | 0.872 | 2 | 31.47 | 2 | 1.1s |  |  |  |
| **Infinity** | 2B | 1 | **0.962** | 1 | **32.25** | 1 | **0.8s** |  |  |  |{{< /table-caption >}}
> üîº This table presents a comparison of different text-to-image generation models, including Infinity, in terms of human preference metrics (ImageReward and HPSv2.1) and inference latency.  Higher scores in ImageReward and HPSv2.1 indicate better human preference. Lower latency values indicate faster generation speed.  The comparison focuses on state-of-the-art (SoTA) open-source models to benchmark Infinity's performance.  The results demonstrate that Infinity achieves the highest scores in terms of human preference for both metrics and simultaneously boasts the fastest inference time.
> <details>
> <summary>read the caption</summary>
> Table 2:  Human Preference Metrics and Inference Latency. We compared our method with SoTA open-source models. Infinity achieved the best human preference results with the fastest speed.
> </details>

{{< table-caption >}}
| Quantizer | d=16 | d=18 | d=20 | d=32 | d=64 |
|---|---|---|---|---|---| 
| LFQ | 37.6 | 53.7 | OOM | OOM | OOM |
| **BSQ** | 32.4 | 32.4 | 32.4 | 32.4 | 32.4 |{{< /table-caption >}}
> üîº This table compares the memory usage (in gigabytes) of two different quantization methods, LFQ and BSQ, during the training process of a visual autoregressive model.  The comparison is made across various codebook dimensions (d), representing the number of bits used to represent each visual token. The results demonstrate that as the codebook dimension increases, the memory efficiency of BSQ (Bitwise Spherical Quantizer) significantly surpasses that of LFQ (Learned Fixed-Point Quantizer). This superior memory efficiency of BSQ enables the model to scale to an extremely large vocabulary size of 2<sup>64</sup>, which is crucial for generating high-resolution images.
> <details>
> <summary>read the caption</summary>
> Table 3:  Comparison of memory consumption (GB) between different quantizers during training. As codebook dimension dùëëditalic_d increases, MSR-BSQ shows significant advantages over MSR-LFQ, enabling nearly infinite vocabulary size of 264superscript2642^{64}2 start_POSTSUPERSCRIPT 64 end_POSTSUPERSCRIPT.
> </details>

{{< table-caption >}}
| VAE (stride=16) | TYPE | IN-256 rFID‚Üì | IN-512 rFID‚Üì |
|---|---|---|---|
| $V_{d}=2^{16}$ | Discrete | 1.22 | 0.31 |
| $V_{d}=2^{24}$ | Discrete | 0.75 | 0.30 |
| $V_{d}=2^{32}$ | Discrete | 0.61 | 0.23 |
| $V_{d}=2^{64}$ | Discrete | 0.33 | 0.15 |
| SD VAE [49] | Contiguous | 0.87 | N/A |{{< /table-caption >}}
> üîº This table presents a comparison of the ImageNet-rFID scores achieved by different visual tokenizers (discrete and continuous) in the context of VAEs (Variational Autoencoders).  By increasing the vocabulary size of the discrete tokenizer, the results demonstrate that a discrete tokenizer can surpass the performance of a continuous VAE.  The table showcases how scaling the vocabulary of the discrete tokenizer leads to improved results on ImageNet-rFID, suggesting the benefits of high-capacity discrete representations for image reconstruction.
> <details>
> <summary>read the caption</summary>
> Table 4:  By scaling up visual tokenizer‚Äôs vocabulary, discrete tokenizer surpasses continuous VAE of SD¬†[48] on ImageNet-rFID.
> </details>

{{< table-caption >}}
| Classifier | # Params | vRAM | Recons. Loss ‚Üì | FID ‚Üì | ImageReward ‚Üë | HPSv2.1 ‚Üë |
|---|---|---|---|---|---|---|
| Convention | 124M | 2GB | 0.184 | 4.49 | 0.79 | 31.95 |
| IVC | 0.65M | 10MB | 0.180 | 3.83 | 0.91 | 32.31 |{{< /table-caption >}}
> üîº Table 5 presents a comparison of the Infinite-Vocabulary Classifier (IVC) against a conventional classifier.  Both classifiers are tasked with predicting visual tokens, but IVC uses a significantly more efficient architecture. The table shows that IVC reduces the number of parameters by 99.95% while achieving lower reconstruction loss and FID (Fr√©chet Inception Distance), a metric assessing the quality of generated images, and higher ImageReward and HPSv2.1 scores (measures of human preference).  This demonstrates IVC's superior performance in terms of both efficiency and image generation quality, particularly when considering a large vocabulary size (2^16).
> <details>
> <summary>read the caption</summary>
> Table 5:  IVC saves 99.95% params and gets better performance to conventional classifier (Vd=216)V_{d}=2^{16})italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT )
> </details>

{{< table-caption >}}
| # Params | GFLOPs | Hidden Dimension | Heads | Layers |
|---|---|---|---|---|
| 125M | 30 | 768 | 8 | 12 |
| 361M | 440 | 1152 | 12 | 16 |
| 940M | 780 | 1536 | 16 | 24 |
| 2.2B | 1500 | 2080 | 20 | 32 |
| 4.7B | 2600 | 2688 | 24 | 40 |{{< /table-caption >}}
> üîº This table presents the model architectures used in the scaling experiments for visual autoregressive modeling.  It shows how the number of parameters, GFLOPs (floating-point operations), hidden dimension, number of attention heads, and number of transformer layers were varied to explore the effects of model scaling on performance.  Note that the GFLOPs are approximate because the computational cost is influenced by the length of the text prompt used in each image generation task.
> <details>
> <summary>read the caption</summary>
> Table 6:  Model architectures for scaling visual autoregressive modeling. Note that GFLOPs are rough values since they are affected by the length of the text prompt.
> </details>

{{< table-caption >}}
| Method | FID ‚Üì | ImageReward ‚Üë | HPSv2.1 ‚Üë |
|---|---|---|---|
| Baseline | 9.76 | 0.52 | 29.53 |
| Baseline + Random Flip | 9.69 | 0.52 | 29.20 |
| Baseline + Bitwise Self-Correction | 3.48 | 0.76 | 30.71 |{{< /table-caption >}}
> üîº This table presents the results of an ablation study evaluating the impact of Bitwise Self-Correction (BSC) on the performance of the Infinity model.  The experiment used 5 million high-quality images with a resolution of 512x512 pixels. The FID (Fr√©chet Inception Distance) metric was calculated on a validation set of 40,000 images.  The decoding parameters used were œÑ (tau) = 1 and cfg (classifier-free guidance) = 3.  The table compares the performance of the baseline model (without BSC), a model with random bit flips (simulating errors), and the model with BSC. The metrics reported are FID, ImageReward, and HPSv2.1, indicating the impact of BSC on image quality, adherence to prompts, and overall aesthetic appeal.
> <details>
> <summary>read the caption</summary>
> Table 7:  Bitwise Self-Correction makes significant improvements. Experiment with 5M high-quality data and 512√ó512512512512\times 512512 √ó 512 resolution. FID is measured on the validation set with 40K images. Decoding with œÑ=1ùúè1\tau=1italic_œÑ = 1 and c‚Å¢f‚Å¢g=3ùëêùëìùëî3cfg=3italic_c italic_f italic_g = 3.
> </details>

{{< table-caption >}}
| Method | FID‚Üì | ImageReward‚Üë | HPSv2.1‚Üë |
|---|---|---|---|
| w/o Bitwise Self-Correction | 9.76 | 0.515 | 29.53 |
| Bitwise Self-Correction (p=10%) | 3.45 | 0.751 | 30.47 |
| Bitwise Self-Correction (p=20%) | 3.48 | 0.763 | 30.71 |
| Bitwise Self-Correction (p=30%) | **3.33** | **0.775** | **31.05** |{{< /table-caption >}}
> üîº This table presents the ablation study results on the impact of different Bitwise Self-Correction (BSC) strengths on the model's performance.  The experiment used 5 million high-quality images with a resolution of 512x512 pixels. The decoding parameters were set to œÑ (tau) = 1 and cfg = 3. The table shows how varying the probability of randomly flipping bits during BSC (10%, 20%, and 30%) affects the FID score, ImageReward, and HPSv2.1 scores, which are metrics evaluating image quality and human preference.
> <details>
> <summary>read the caption</summary>
> Table 8:  Comparison between different strengths of Bitwise Self-Correction. Experiment with 5M high-quality data and 512√ó512512512512\times 512512 √ó 512 resolution. Decoding with œÑ=1ùúè1\tau=1italic_œÑ = 1 and c‚Å¢f‚Å¢g=3ùëêùëìùëî3cfg=3italic_c italic_f italic_g = 3.
> </details>

{{< table-caption >}}
| Method | Param | FID ‚Üì | ImageReward ‚Üë | HPSv2.1 ‚Üë |
|---|---|---|---|---|
| Greedy Sampling | œÑ=0.01,cfg=1 | 9.97 | 0.397 | 30.98 |
| Normal Sampling | œÑ=1.00,cfg=1 | 4.84 | 0.706 | 31.59 |
| Pyramid CFG | œÑ=1.00,cfg=1‚Üí3 | 3.48 | 0.872 | **32.48** |
| Pyramid CFG | œÑ=1.00,cfg=1‚Üí5 | 2.98 | 0.929 | 32.32 |
| CFG on features | œÑ=1.00,cfg=3 | 3.00 | 0.953 | 32.13 |
| CFG on logits | œÑ=1.00,cfg=3 | 2.91 | 0.952 | 32.31 |
| CFG on logits (Ours) | œÑ=1.00,cfg=4 | **2.82** | **0.962** | 32.25 |{{< /table-caption >}}
> üîº This table compares different decoding methods used in the Infinity model for text-to-image generation, showing their performance across various metrics such as FID (Fr√©chet Inception Distance), ImageReward, and HPSv2.1.  The methods compared include Greedy Sampling, Normal Sampling, different configurations of Classifier-Free Guidance (CFG), and the authors' proposed method.  The results demonstrate the impact of different decoding strategies on the quality and fidelity of the generated images.
> <details>
> <summary>read the caption</summary>
> Table 9:  Comparison between different decoding methods.
> </details>

{{< table-caption >}}
| Aspect Ratio | Resolution | Scale Schedule |
|---|---|---|
| 1.000 (1:1) | 1024 √ó 1024 | (1,1), (2,2), (4,4), (6,6), (8,8), (12,12), (16,16), (20,20), (24,24), (32,32), (40,40), (48,48), (64,64) |
| 0.800 (4:5) | 896 √ó 1120 | (1,1), (2,2), (3,3), (4,5), (8,10), (12,15), (16,20), (20,25), (24,30), (28,35), (36,45), (44,55), (56,70) |
| 1.250 (5:4) | 1120 √ó 896 | (1,1), (2,2), (3,3), (5,4), (10,8), (15,12), (20,16), (25,20), (30,24), (35,28), (45,36), (55,44), (70,56) |
| 0.750 (3:4) | 864 √ó 1152 | (1,1), (2,2), (3,4), (6,8), (9,12), (12,16), (15,20), (18,24), (21,28), (27,36), (36,48), (45,60), (54,72) |
| 1.333 (4:3) | 1152 √ó 864 | (1,1), (2,2), (4,3), (8,6), (12,9), (16,12), (20,15), (24,18), (28,21), (36,27), (48,36), (60,45), (72,54) |
| 0.666 (2:3) | 832 √ó 1248 | (1,1), (2,2), (2,3), (4,6), (6,9), (10,15), (14,21), (18,27), (22,33), (26,39), (32,48), (42,63), (52,78) |
| 1.500 (3:2) | 1248 √ó 832 | (1,1), (2,2), (3,2), (6,4), (9,6), (15,10), (21,14), (27,18), (33,22), (39,26), (48,32), (63,42), (78,52) |
| 0.571 (4:7) | 768 √ó 1344 | (1,1), (2,2), (3,3), (4,7), (6,11), (8,14), (12,21), (16,28), (20,35), (24,42), (32,56), (40,70), (48,84) |
| 1.750 (7:4) | 1344 √ó 768 | (1,1), (2,2), (3,3), (7,4), (11,6), (14,8), (21,12), (28,16), (35,20), (42,24), (56,32), (70,40), (84,48) |
| 0.500 (1:2) | 720 √ó 1440 | (1,1), (2,2), (2,4), (3,6), (5,10), (8,16), (11,22), (15,30), (19,38), (23,46), (30,60), (37,74), (45,90) |
| 2.000 (2:1) | 1440 √ó 720 | (1,1), (2,2), (4,2), (6,3), (10,5), (16,8), (22,11), (30,15), (38,19), (46,23), (60,30), (74,37), (90,45) |
| 0.400 (2:5) | 640 √ó 1600 | (1,1), (2,2), (2,5), (4,10), (6,15), (8,20), (10,25), (12,30), (16,40), (20,50), (26,65), (32,80), (40,100) |
| 2.500 (5:2) | 1600 √ó 640 | (1,1), (2,2), (5,2), (10,4), (15,6), (20,8), (25,10), (30,12), (40,16), (50,20), (65,26), (80,32), (100,40) |
| 0.333 (1:3) | 592 √ó 1776 | (1,1), (2,2), (2,6), (3,9), (5,15), (7,21), (9,27), (12,36), (15,45), (18,54), (24,72), (30,90), (37,111) |
| 3.000 (3:1) | 1776 √ó 592 | (1,1), (2,2), (6,2), (9,3), (15,5), (21,7), (27,9), (36,12), (45,15), (54,18), (72,24), (90,30), (111,37) |{{< /table-caption >}}
> üîº This table shows predefined scale schedules used by the Infinity model for generating images with various aspect ratios.  Each row represents a different aspect ratio (e.g., 1:1, 4:3, 16:9), and the columns list the height and width resolutions (h, w) at each of the 13 scales (K=13) used in the model's next-scale prediction process.  The model starts with small resolution images at the first scale and gradually upsamples them to 1024x1024 (or other specified resolutions) over 13 prediction steps. This pre-defined schedule ensures consistent training and efficient generation across different aspect ratios.
> <details>
> <summary>read the caption</summary>
> Table 10: Predefined scale schedules {(h1r,w1r),‚Ä¶,(hKr,wKr)}subscriptsuperscript‚Ñéùëü1subscriptsuperscriptùë§ùëü1‚Ä¶subscriptsuperscript‚Ñéùëüùêæsubscriptsuperscriptùë§ùëüùêæ\{(h^{r}_{1},w^{r}_{1}),...,(h^{r}_{K},w^{r}_{K})\}{ ( italic_h start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_w start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ‚Ä¶ , ( italic_h start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , italic_w start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ) } for different aspect ratios. Following the text guided next-scale prediction scheme, Infinity takes KùêæKitalic_K=13 scales to generate a 1024√ó1024102410241024\times 10241024 √ó 1024 (or other aspect ratio) image.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2412.04431/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2412.04431/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}