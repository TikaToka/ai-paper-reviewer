<table id='0' style='font-size:18px'><tr><td>Data Source</td><td>Size</td><td>Type</td></tr><tr><td>Emu2 (Sun et al., 2024b)</td><td>10M</td><td>Caption</td></tr><tr><td>LVIS-Instruct(Gupta et al., 2019)</td><td>223K</td><td>General</td></tr><tr><td>LLaVA-CC3M-Pretrain-595K(Li et al., 2024b)</td><td>595K</td><td>General</td></tr><tr><td>Visdial(Das et al., 2017)</td><td>116K</td><td>General</td></tr><tr><td>Sharegpt4(Chen et al., 2023)</td><td>3.2M</td><td>General</td></tr><tr><td>STVQA(Agrawal et al., 2024)</td><td>43K</td><td>General</td></tr><tr><td>MMC-INST(Liu et al., 2024a)</td><td>500K</td><td>Doc/Chart/Screen</td></tr><tr><td>MathV360K(Shi et al., 2024)</td><td>338K</td><td>Math/Reasoning</td></tr><tr><td>MMC-Alignment(Liu et al., 2024a)</td><td>250K</td><td>Doc/Chart/Screen</td></tr><tr><td>DocReason(Ye et al., 2024)</td><td>26K</td><td>Doc/Chart/Screen</td></tr><tr><td>ALLaVA(Chen et al., 2024a)</td><td>1.7M</td><td>General</td></tr><tr><td>Cocotext(Havard et al., 2017)</td><td>163K</td><td>General</td></tr><tr><td>Docvqa(Ye et al., 2024)</td><td>16K</td><td>Doc/Chart/Screen</td></tr><tr><td>Geoqa+(Chen et al., 2021)</td><td>72K</td><td>Math/Reasoning</td></tr><tr><td>DocDownstream(Ye et al., 2024)</td><td>700K</td><td>Doc/Chart/Screen</td></tr><tr><td>Cambrian (Tong et al., 2024)</td><td>8.3M</td><td>General, General OCR, Math/Reasoning Doc/Chart/Screen, Text Instruct</td></tr><tr><td>DocStruct4M(Ye et al., 2024)</td><td>4M</td><td>General OCR, Doc/Chart/Screen</td></tr><tr><td>LLaVA-onevision (Li et al., 2024a)</td><td>4M</td><td>General, General OCR, Math/Reasoning Doc/Chart/Screen, Text Instruct</td></tr><tr><td>Docmatix(Laurenï¿½on et al., 2024)</td><td>1.2M</td><td>Doc VQA</td></tr><tr><td>Infinity-Instruct (BAAI, 2024b)</td><td>7M</td><td>Text Instruct</td></tr><tr><td>Our Synthetic Data</td><td>0.8M</td><td>Fine-grained Perception(single-instance) Attribute Reasoning Fine-grained Perception(Cross-instance) Relation Reasoning Coarse Perception, Logic Reasoning</td></tr></table>