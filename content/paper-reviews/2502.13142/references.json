{"references": [{"fullname_first_author": "Brown, T.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field of large language models and its impact on few-shot learning, which is a central concept in the current paper."}, {"fullname_first_author": "Alayrac, J.-B.", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-04-14", "reason": "This paper introduces Flamingo, a multimodal model demonstrating the power of pre-training on massive datasets for few-shot learning capabilities, directly relevant to the robotics context."}, {"fullname_first_author": "Brohan, A.", "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control", "publication_date": "2023-07-15", "reason": "This paper introduces RT-2, a vision-language-action model that leverages web knowledge for robotic control, establishing a strong baseline for comparison in the current work."}, {"fullname_first_author": "Kim, M.J.", "paper_title": "OpenVLA: An open-source vision-language-action model", "publication_date": "2024-06-09", "reason": "OpenVLA is a directly comparable model to the one proposed in this paper, providing a strong benchmark for evaluating the performance and contribution."}, {"fullname_first_author": "Collaboration, O.", "paper_title": "Open X-Embodiment: Robotic learning datasets and RT-X models", "publication_date": "2023-10-08", "reason": "This paper provides a large-scale dataset for robotic learning, which is crucial for training and evaluating robotic models and is directly relevant to this work's contribution."}]}