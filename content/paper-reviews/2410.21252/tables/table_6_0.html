<table id='0' style='font-size:16px'><tr><td>Dataset</td><td>Task Type</td><td>#Data</td><td>Avg Len</td><td>Language</td><td>Metric</td><td>Judge Model</td></tr><tr><td colspan="7">Long-context Benchmark</td></tr><tr><td>LongBench-Chat</td><td>Multi-Task</td><td>50</td><td>35,571</td><td>English/Chinese</td><td>Point-wise Rate</td><td>GPT-4o</td></tr><tr><td></td><td>Single-Doc QA</td><td>750</td><td>8,573</td><td>English/Chinese</td><td>Point-wise Rate</td><td>GPT-4o</td></tr><tr><td>LongBench</td><td>Multi-Doc QA</td><td>800</td><td>1,0255</td><td>English/Chinese</td><td>Point-wise Rate</td><td>GPT-4o</td></tr><tr><td></td><td>Summarization</td><td>800</td><td>9,210</td><td>English/Chinese</td><td>Point-wise Rate</td><td>GPT-4o</td></tr><tr><td colspan="7">Short-context Benchmark</td></tr><tr><td>MT-Bench</td><td>Instruction Following</td><td>80</td><td>-</td><td>English</td><td>Point-wise Rate</td><td>GPT-4</td></tr><tr><td>AlpacaEval2</td><td>Instruction Following</td><td>805</td><td>-</td><td>English</td><td>LC Win Rate</td><td>GPT-4-turbo</td></tr></table>