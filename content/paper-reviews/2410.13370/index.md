---
title: "MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models"
summary: "MagicTailor empowers text-to-image models with component-level control over personalized concepts, enabling fine-grained customization and high-quality image generation."
categories: ["AI Generated"]
tags: ["ðŸ”– 24-10-17", "ðŸ¤— 24-10-21"]
showSummary: true
date: 2024-10-17
draft: false
---

### TL;DR


{{< lead >}}

This research introduces a novel approach called 'component-controllable personalization' for improving text-to-image generation.  Current methods can generate images based on text prompts, but struggle to precisely control individual components within a concept (e.g., changing only the hair color in a person's image).  This paper introduces MagicTailor, a new framework that solves this problem. MagicTailor uses two key techniques: Dynamic Masked Degradation (DM-Deg), which removes unwanted elements from training images, and Dual-Stream Balancing (DS-Bal), which ensures even learning of all components within a concept.  Experiments show MagicTailor generates higher-quality images with better control over individual components than existing methods. The results indicate that MagicTailor also works well with other image-generation tools, paving the way for more creative image generation applications.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.13370" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}
<br><br>
{{< button href="https://huggingface.co/papers/2410.13370" target="_self" >}}
{{< icon "hf-logo" >}} &nbsp; on Hugging Face
{{< /button >}}

#### Why does it matter?
This paper is highly important for researchers working on text-to-image diffusion models and personalization.  It introduces a novel task of component-controllable personalization, addressing a significant limitation of existing methods. The proposed MagicTailor framework offers a powerful solution to this challenging problem, opening new avenues for research and practical applications in image generation and manipulation.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} MagicTailor enables component-controllable personalization in text-to-image diffusion models, allowing users to precisely modify specific components within concepts. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The framework tackles the challenges of semantic pollution and imbalance through Dynamic Masked Degradation (DM-Deg) and Dual-Stream Balancing (DS-Bal). {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} MagicTailor shows state-of-the-art performance and demonstrates significant potential for diverse applications, including collaboration with other generative tools. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](figures/figures_1_0.png)

> ðŸ”¼ The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showing examples of images generated by the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>





![](charts/charts_8_0.png)

> ðŸ”¼ The chart shows the ablation study of loss weights (Apres and Aattn) on CLIP-T and DreamSim metrics, highlighting MagicTailor's robustness to different loss weight settings.
> <details>
> <summary>read the caption</summary>
> Figure 7: Ablation of loss weights. We report CLIP-T for text alignment, and DreamSim for identity fidelity as it is most similar to human judgments (Fu et al., 2023). For reference, we also present the results of the second-best method in Table 1, highlighting our robustness on loss weights.
> </details>





{{< table-caption >}}
<table id='2' style='font-size:16px'><tr><td rowspan="2">Methods</td><td colspan="4">Automatic Metrics</td><td colspan="3">User Study</td></tr><tr><td>CLIP-Tâ†‘</td><td>CLIP-I â†‘</td><td>DINO â†‘</td><td>DreamSim â†“</td><td>Text Align. â†‘</td><td>Id. Fidelity â†‘</td><td>Gen. Quality â†‘</td></tr><tr><td>Textual Inversion (Gal et al., 2022)</td><td>0.236</td><td>0.742</td><td>0.620</td><td>0.558</td><td>5.8%</td><td>2.5%</td><td>5.2%</td></tr><tr><td>DreamBooth (Ruiz et al., 2023)</td><td>0.266</td><td>0.841</td><td>0.798</td><td>0.323</td><td>15.3%</td><td>14.7%</td><td>12.5%</td></tr><tr><td>Custom Diffusion (Kumari et al., 2023)</td><td>0.251</td><td>0.797</td><td>0.750</td><td>0.407</td><td>7.1%</td><td>7.7%</td><td>9.8%</td></tr><tr><td>Break-A-Scene (Avrahami et al., 2023)</td><td>0.259</td><td>0.840</td><td>0.780</td><td>0.338</td><td>10.8%</td><td>12.1%</td><td>22.8%</td></tr><tr><td>CLiC (Safaee et al., 2024)</td><td>0.263</td><td>0.764</td><td>0.663</td><td>0.499</td><td>4.5%</td><td>5.1%</td><td>6.2%</td></tr><tr><td>MagicTailor (Ours)</td><td>0.270</td><td>0.854</td><td>0.813</td><td>0.279</td><td>56.5%</td><td>57.9%</td><td>43.4%</td></tr></table>{{< /table-caption >}}

> ðŸ”¼ Table 1 presents a quantitative comparison of MagicTailor against state-of-the-art methods for personalization, using both automatic metrics and a user study to evaluate text alignment, identity fidelity, and generation quality.
> <details>
> <summary>read the caption</summary>
> Table 1: Quantitative comparisons. We compare our MagicTailor with SOTA methods of personalization based on automatic metrics and user study. The best results are marked in bold.
> </details>



### More visual insights

<details>
<summary>More on figures
</summary>


![](figures/figures_3_0.png)

> ðŸ”¼ Figure 2 illustrates the two main challenges in component-controllable personalization: semantic pollution, where unwanted visual elements corrupt the concept; and semantic imbalance, where disproportionate learning of the concept and component occurs.
> <details>
> <summary>read the caption</summary>
> Figure 2: Major challenges in component-controllable personalization. (a) Semantic pollution: (i) Undesired visual elements may inadvertently disturb the personalized concept. (ii) A simple mask-out strategy is ineffective and causes unintended compositions, whereas (iii) our DM-Deg effectively suppresses unwanted visual semantics, preventing such pollution. (b) Semantic imbalance: (i) Simultaneously learning the concept and component can lead to imbalance, resulting in concept or component distortion (here we present a case for the former). (ii) Our DS-Bal ensures balanced learning, enhancing personalization performance.
> </details>



![](figures/figures_3_1.png)

> ðŸ”¼ Figure 1 illustrates personalization in text-to-image diffusion models, showing how to modify specific components of visual concepts using reference images and the effectiveness of MagicTailor in achieving this.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_4_0.png)

> ðŸ”¼ The figure illustrates the pipeline of MagicTailor, a framework that enables component-controllable personalization for text-to-image diffusion models by using dynamic masked degradation and dual-stream balancing.
> <details>
> <summary>read the caption</summary>
> Figure 3: Pipeline overview of MagicTailor. Using reference images as the inputs, MagicTailor fine-tunes a T2I diffusion model to learn both the target concept and component, enabling the generation of images that seamlessly integrate the component into the concept. Two key techniques, Dynamic Masked Degradation (DM-Deg, see Section 3.2) and Dual-Stream Balancing (DS-Bal, see Section 3.3), address the challenges of semantic pollution and semantic imbalance, respectively. For clarity, only one image per concept/component is presented and the warm-up stage is not depicted.
> </details>



![](figures/figures_5_0.png)

> ðŸ”¼ Figure 1 illustrates the concept of personalization in text-to-image diffusion models, highlighting the differences between standard personalization and the novel component-controllable personalization proposed in the paper, and shows example outputs of the proposed method.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_6_0.png)

> ðŸ”¼ Figure 1 illustrates personalization and component-controllable personalization using text-to-image diffusion models, and shows example images generated by the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_6_1.png)

> ðŸ”¼ Figure 5 visualizes how the dual-stream balancing (DS-Bal) technique in MagicTailor effectively addresses semantic imbalance in component-controllable personalization, contrasting it with the unbalanced learning of a vanilla approach.
> <details>
> <summary>read the caption</summary>
> Figure 5: Visualization of the learning process. (a) The vanilla learning paradigm lapses into overemphasizing the easier one. (b) DS-Bal effectively balances the learning of the concept and component.
> </details>



![](figures/figures_7_0.png)

> ðŸ”¼ Figure 6 shows a qualitative comparison of images generated by MagicTailor and other state-of-the-art methods across various domains, highlighting MagicTailor's superior performance in text alignment, identity fidelity, and image quality.
> <details>
> <summary>read the caption</summary>
> Figure 6: Qualitative comparisons. We present images generated by MagicTailor and the compared methods for various domains. MagicTailor generally achieves promising text alignment, strong identity fidelity, and high generation quality. More results are provided in Appendix D.
> </details>



![](figures/figures_9_0.png)

> ðŸ”¼ Figure 1 illustrates the concept of personalization in text-to-image diffusion models, showing how to modify specific visual components using reference images, and provides example images generated by MagicTailor.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_9_1.png)

> ðŸ”¼ The figure illustrates the concepts of personalization and component-controllable personalization in text-to-image diffusion models, and showcases example images generated by the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_9_2.png)

> ðŸ”¼ Figure 1 illustrates personalization and component-controllable personalization, showing how text-to-image diffusion models can learn and modify visual concepts with example images generated by MagicTailor.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_10_0.png)

> ðŸ”¼ This figure illustrates the MagicTailor pipeline, which fine-tunes a text-to-image diffusion model to learn concepts and components from reference images, addressing semantic pollution and imbalance.
> <details>
> <summary>read the caption</summary>
> Figure 3: Pipeline overview of MagicTailor. Using reference images as the inputs, MagicTailor fine-tunes a T2I diffusion model to learn both the target concept and component, enabling the generation of images that seamlessly integrate the component into the concept. Two key techniques, Dynamic Masked Degradation (DM-Deg, see Section 3.2) and Dual-Stream Balancing (DS-Bal, see Section 3.3), address the challenges of semantic pollution and semantic imbalance, respectively. For clarity, only one image per concept/component is presented and the warm-up stage is not depicted.
> </details>



![](figures/figures_10_1.png)

> ðŸ”¼ The figure illustrates the concept of personalization, component-controllable personalization, and example images generated by the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_10_2.png)

> ðŸ”¼ The figure shows how MagicTailor can be used to enhance other generative tools by adding the ability to control a concept's component.
> <details>
> <summary>read the caption</summary>
> Figure 9: Enhancing other generative tools. MagicTailor can conveniently collaborate with a variety of generative tools that focus on other tasks, equipping them with an additional ability to control the concept's component in their pipelines.
> </details>



![](figures/figures_10_3.png)

> ðŸ”¼ This figure illustrates the pipeline of MagicTailor, a novel framework that enables component-controllable personalization for text-to-image diffusion models, highlighting its key techniques: Dynamic Masked Degradation and Dual-Stream Balancing.
> <details>
> <summary>read the caption</summary>
> Figure 3: Pipeline overview of MagicTailor. Using reference images as the inputs, MagicTailor fine-tunes a T2I diffusion model to learn both the target concept and component, enabling the generation of images that seamlessly integrate the component into the concept. Two key techniques, Dynamic Masked Degradation (DM-Deg, see Section 3.2) and Dual-Stream Balancing (DS-Bal, see Section 3.3), address the challenges of semantic pollution and semantic imbalance, respectively. For clarity, only one image per concept/component is presented and the warm-up stage is not depicted.
> </details>



![](figures/figures_10_5.png)

> ðŸ”¼ The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showcasing examples generated by the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_10_6.png)

> ðŸ”¼ Figure 1 illustrates personalization, component-controllable personalization, and example images generated by MagicTailor, highlighting its effectiveness in adapting T2I diffusion models for component-controllable personalization.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_10_7.png)

> ðŸ”¼ Figure 4 shows the comparison of using fixed and dynamic intensity in the DM-Deg, illustrating how dynamic intensity mitigates noise memorization during training.
> <details>
> <summary>read the caption</summary>
> Figure 4: Motivation of dynamic intensity. (a) Fixed intensity (ad = 0.5 here) could cause noisy generated images. (b) Our dynamic intensity helps to mitigate noise memorization.
> </details>



![](figures/figures_17_0.png)

> ðŸ”¼ This figure illustrates the concept of personalization and component-controllable personalization, showing how text-to-image diffusion models can modify specific components of a visual concept and provides example images generated by the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_17_1.png)

> ðŸ”¼ Figure 1 illustrates the concept of personalization in text-to-image diffusion models, showing how to modify specific components of a visual concept and examples of images generated by MagicTailor.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_17_2.png)

> ðŸ”¼ The figure illustrates the two main challenges in component-controllable personalization: semantic pollution and semantic imbalance, showing how the proposed methods, DM-Deg and DS-Bal, address these issues.
> <details>
> <summary>read the caption</summary>
> Figure 2: Major challenges in component-controllable personalization. (a) Semantic pollution: (i) Undesired visual elements may inadvertently disturb the personalized concept. (ii) A simple mask-out strategy is ineffective and causes unintended compositions, whereas (iii) our DM-Deg effectively suppresses unwanted visual semantics, preventing such pollution. (b) Semantic imbalance: (i) Simultaneously learning the concept and component can lead to imbalance, resulting in concept or component distortion (here we present a case for the former). (ii) Our DS-Bal ensures balanced learning, enhancing personalization performance.
> </details>



![](figures/figures_17_3.png)

> ðŸ”¼ The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showing examples of images generated by the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_17_4.png)

> ðŸ”¼ The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showcasing examples of image generation using the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_17_5.png)

> ðŸ”¼ Figure 1 illustrates personalization and component-controllable personalization in text-to-image diffusion models, showing how MagicTailor modifies a specific visual concept component using reference images.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_18_0.png)

> ðŸ”¼ The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showing examples of images generated using the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_18_1.png)

> ðŸ”¼ Figure 1 illustrates personalization, component-controllable personalization, and example images generated by MagicTailor to showcase its effectiveness in adapting T2I diffusion models for component-controllable personalization.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_18_2.png)

> ðŸ”¼ Figure 1 illustrates the tasks of personalization and component-controllable personalization, and shows example images generated by the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_18_3.png)

> ðŸ”¼ Figure 1 illustrates personalization, component-controllable personalization, and example images generated by MagicTailor, highlighting the effectiveness of the proposed framework for adapting T2I diffusion models.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_18_4.png)

> ðŸ”¼ The figure illustrates personalization, component-controllable personalization, and example images generated by MagicTailor, highlighting the differences between standard personalization and the proposed component-controllable approach.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_18_5.png)

> ðŸ”¼ The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showing examples of images generated by MagicTailor.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_19_0.png)

> ðŸ”¼ The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showing examples of images generated by the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_19_1.png)

> ðŸ”¼ Figure 1 illustrates personalization and component-controllable personalization tasks, showing how text-to-image diffusion models can learn and modify specific visual concepts using reference images, and provides example images generated by the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_19_2.png)

> ðŸ”¼ The figure illustrates the concept of personalization and component-controllable personalization in text-to-image diffusion models, showing how MagicTailor modifies a specific component of a visual concept.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_19_3.png)

> ðŸ”¼ Figure 1 illustrates personalization and component-controllable personalization in text-to-image diffusion models, showing examples of generated images using the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_19_4.png)

> ðŸ”¼ The figure illustrates personalization, component-controllable personalization, and example images generated by the proposed MagicTailor framework for text-to-image diffusion models.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_19_6.png)

> ðŸ”¼ Figure 1 illustrates the personalization and component-controllable personalization tasks, and shows example images generated by the proposed MagicTailor.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_19_7.png)

> ðŸ”¼ Figure 1 illustrates the concept of personalization in text-to-image diffusion models, showcasing component-controllable personalization as a new task and example images generated by the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_19_8.png)

> ðŸ”¼ Figure 1 illustrates the personalization and component-controllable personalization tasks, and shows example images generated by the proposed MagicTailor framework.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_19_9.png)

> ðŸ”¼ Figure 1 illustrates personalization, component-controllable personalization, and examples of images generated by MagicTailor, highlighting its effectiveness in component-controllable personalization.
> <details>
> <summary>read the caption</summary>
> Figure 1: (a) Illustration of personalization, demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images. (b) Illustration of component-controllable personalization, depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization. (c) Example images generated by MagicTailor, showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization. For clarity, the red and blue circles are used to highlight the target concept and component, respectively.
> </details>



![](figures/figures_20_0.png)

> ðŸ”¼ Figure 6 shows a qualitative comparison of images generated by MagicTailor and other state-of-the-art methods across different domains, highlighting MagicTailor's superior performance in terms of text alignment, identity preservation, and image quality.
> <details>
> <summary>read the caption</summary>
> Figure 6: Qualitative comparisons. We present images generated by MagicTailor and the compared methods for various domains. MagicTailor generally achieves promising text alignment, strong identity fidelity, and high generation quality. More results are provided in Appendix D.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table id='1' style='font-size:18px'><tr><td colspan="11">Table 2: Ablation of key techniques. Our DM- Table 4: Ablation of DM-Deg. We compare Deg and DS-Bal effectively contribute to a supe- DM-Deg with its variants and the mask-out strat- rior performance trade-off. egy. Our DM-Deg attains superior overall perfor-</td></tr><tr><td>DM-Deg DS-Bal</td><td></td><td>CLIP-Tâ†‘</td><td>CLIP-I â†‘</td><td colspan="2">DINO â†‘ DreamSim â†“</td><td>mance on text alignment and identity fidelity.</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>0.275</td><td>0.837</td><td>0.798</td><td colspan="2">0.317</td><td>Intensity Variants</td><td>CLIP-Tâ†‘</td><td>CLIP-Iâ†‘</td><td>DINO â†‘</td><td>DreamSim â†“</td></tr><tr><td></td><td>0.276</td><td>0.848</td><td>0.809</td><td colspan="2">0.294</td><td>Mask-Out Startegy</td><td>0.270</td><td>0.818</td><td>0.760</td><td>0.375</td></tr><tr><td></td><td>0.270</td><td>0.845</td><td>0.802</td><td colspan="2">0.304</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>V</td><td>0.270</td><td>0.854</td><td>0.813</td><td colspan="2">0.279</td><td>Fixed (a = 0.4)</td><td>0.270 0.271</td><td>0.849</td><td>0.800</td><td>0.297 0.310</td></tr><tr><td colspan="6">Table 3: Ablation of DS-Bal. We compare DS- Bal with its variants, showing its excellence.</td><td>Fixed (a = 0.6)</td><td>0.271</td><td>0.845 0.846</td><td>0.794 0.796</td><td>0.305</td></tr><tr><td colspan="6"></td><td>Fixed (a = 0.8) Linear (Ascent)</td><td>0.270</td><td>0.846</td><td>0.797</td><td>0.307</td></tr><tr><td>U-Net Variants</td><td>CLIP-Tâ†‘</td><td>CLIP-I â†‘</td><td>DINO â†‘</td><td colspan="2">DreamSim â†“</td><td>Linear (Descent)</td><td>0.261</td><td>0.851</td><td>0.802</td><td>0.300</td></tr><tr><td>Fixed (B = 0)</td><td>0.268</td><td>0.850</td><td>0.803</td><td colspan="2">0.293</td><td>Dynamic (Y = 8)</td><td>0.266</td><td>0.850</td><td>0.806</td><td>0.289</td></tr><tr><td>Fixed (B = 1)</td><td>0.270</td><td>0.851</td><td>0.808</td><td colspan="2">0.286</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Momentum (B = 0.5)</td><td>0.268</td><td>0.850</td><td>0.805</td><td colspan="2">0.290</td><td>Dynamic (Y = 16)</td><td>0.268</td><td>0.854</td><td>0.813</td><td>0.282</td></tr><tr><td>Momentum (B = 0.9)</td><td>0.269</td><td>0.850</td><td>0.808</td><td colspan="2">0.288</td><td>Dynamic (Y = 64)</td><td>0.271</td><td>0.852</td><td>0.812</td><td>0.283</td></tr><tr><td>Momentum (Ours)</td><td>0.270</td><td>0.854</td><td>0.813</td><td colspan="2">0.279</td><td>Dynamic (Ours)</td><td>0.270</td><td>0.854</td><td>0.813</td><td>0.279</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 1 quantitatively compares MagicTailor's performance against state-of-the-art methods in personalization using automatic metrics and a user study.


{{< table-caption >}}
<table id='2' style='font-size:14px'><tr><td>Recontextualization</td><td>Restylization</td></tr><tr><td>' <placeholder>, on the beach" ' ' <placeholder>, in the snow" " <placeholder>, at night" <placeholder>, in autumn"</td><td>"<placeholder>, watercolor painting" Â· <placeholder>, Ukiyo-e painting" ' <placeholder>, in Pixel Art style" "<placeholder>, in Von Gogh style" ' ' <placeholder>, in a comic book"</td></tr><tr><td>' <placeholder>, in the jungle" Interaction</td><td>Property Modification</td></tr><tr><td><placeholder>, with clouds in the background" <placeholder>, with flowers in the background"</td><td>"<placeholder>, from 3D rendering" "<placeholder>, in a far view" in a close view"</td></tr><tr><td><placeholder>, near the Eiffel Tower" <placeholder>, on top of water" <placeholder>, in front of the Mount Fuji"</td><td><placeholder>, <placeholder>, made of clay" <placeholder>, made of plastic"</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 1 quantitatively compares MagicTailor's performance against state-of-the-art methods in personalization using automatic metrics and a user study.


{{< table-caption >}}
<table id='22' style='font-size:18px'><tr><td>Warm-up Variants</td><td>CLIP-Tâ†‘</td><td>CLIP-Iâ†‘</td><td>DINO â†‘</td><td>DreamSim â†“</td></tr><tr><td>w/o Warm-up</td><td>0.272</td><td>0.844</td><td>0.793</td><td>0.320</td></tr><tr><td>w/ Warm-up (Ours)</td><td>0.270</td><td>0.854</td><td>0.813</td><td>0.279</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ {{ table.description }}
> <details>
> <summary>read the caption</summary>
> {{ table.caption }}
> </details>


> Table 2 shows the impact of the Dynamic Masked Degradation (DM-Deg) and Dual-Stream Balancing (DS-Bal) techniques on the performance of the MagicTailor model.


</details>


### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}