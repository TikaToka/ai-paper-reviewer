---
title: "Fast Best-of-N Decoding via Speculative Rejection"
summary: "Speculative Rejection: A novel algorithm achieves fast, high-quality LLM decoding by strategically rejecting low-scoring partial generations, offering 16-32x speedup over Best-of-N."
categories: ["AI Generated"]
tags: ["üîñ 24-10-26", "ü§ó 24-10-29"]
showSummary: true
date: 2024-10-26
draft: false
---

{{< keyword >}} 2410.20290 {{< /keyword >}}

### TL;DR


{{< lead >}}

Large Language Models (LLMs) require alignment to ensure safe and reliable outputs. Current alignment techniques often involve complex post-training, slowing down deployment.  Inference-time alignment methods, such as Best-of-N, avoid post-training but suffer from high computational costs at inference. This limits their applicability in real-world scenarios. 

The paper introduces Speculative Rejection, a novel inference-time alignment algorithm. It addresses the efficiency issues of Best-of-N by selectively rejecting low-scoring partial sequences during generation.  **By dynamically adjusting the batch size, it makes high-N Best-of-N decoding computationally viable, even on a single GPU.**  Experiments demonstrate a significant speedup (16-32x) compared to Best-of-N, while maintaining comparable alignment quality. This opens new possibilities for efficient and large-scale LLM deployments.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.20290" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}
<br><br>
{{< button href="https://huggingface.co/papers/2410.20290" target="_self" >}}
{{< icon "hf-logo" >}} &nbsp; on Hugging Face
{{< /button >}}

#### Why does it matter?
This paper is crucial for researchers working on LLM alignment and efficient decoding.  It introduces a novel and computationally efficient method for inference-time alignment, addressing a major bottleneck in deploying LLMs. The findings are highly relevant to current trends in making LLMs more efficient and practical for real-world applications, opening up new avenues for optimization and further research into speed-accuracy trade-offs in inference-time LLM alignment.  **Its efficiency improvements, especially the potential for running high-N Best-of-N decoding on a single GPU, are highly impactful**.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Speculative Rejection significantly speeds up Best-of-N decoding by up to 16-32 times. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The proposed method effectively utilizes GPU resources by dynamically adjusting batch sizes. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Speculative Rejection achieves comparable alignment quality to Best-of-N, while being significantly more efficient. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](https://ai-paper-reviewer.com/2410.20290/figures_2_0.png)

> üîº The figure illustrates how Speculative Rejection improves upon Best-of-N by strategically halting low-quality generations early, leading to greater efficiency and higher reward scores.
> <details>
> <summary>read the caption</summary>
> Figure 1: Left: An illustration of our method. Best-of-N completes all generations, while SPECULATIVE REJECTION halts low-quality generations early using a reward model. Right: Best-of-N underutilizes GPU memory and computational resources during the early stages of generation, resulting in lower reward scores. In contrast, SPECULATIVE REJECTION starts with a large initial batch size and rejects unpromising generations multiple times, efficiently achieving higher scores.
> </details>





![](https://ai-paper-reviewer.com/2410.20290/charts_5_0.png)

> üîº The chart shows the correlation between partial and final rewards for 1000 responses generated by Llama-3-8B-Instruct, evaluated by Mistral-7B-RM, illustrating that early generation scores are predictive of final scores.
> <details>
> <summary>read the caption</summary>
> Figure 2: Partial and final reward for an example. We generate N = 1000 responses via Llama-3-8B-Instruct and evaluate the partial rewards (at T = 256) and final rewards via Mistral-7B-RM. Blue line: the Ordinary Least Square fit. Red dot: the scores for the best response. Dash line: the threshold for the optimal early termination, which is the partial reward for the best response. Blue area: the confidence set for the OLS fit.
> </details>





{{< table-caption >}}
<br><table id='1' style='font-size:14px'><tr><td>Input:</td><td>An auto-regressive generative model p, a reward model S, stopping fraction a E (0,1), a prompt X.</td></tr><tr><td>1:</td><td>Decide the initial batch size as binit based on the GPU memory capacity and the prompt length.</td></tr><tr><td>2:</td><td>b ‚Üê binit, I = ‚åÄ. b ></td></tr><tr><td>3:</td><td>while 0 do For 1 ‚â§ k ‚â§ b, generate (Y1 , Yk2 , . ", Yk) from model</td></tr><tr><td>4:</td><td>p and Tk := min{ÔøΩ, lk}, where Tk is the number of generated tokens before OOM and lk is the number of tokens in Yk.</td></tr><tr><td>5:</td><td>Evaluate all partial rewards from s and compute the cutoff threshold via</td></tr><tr><td>6:</td><td>Compute the set of accepted index Iaccepted via add completed sequences to I.</td></tr><tr><td>7:</td><td>Update the batch size using Iaccepted: b ‚Üê |Zaccepted|.</td></tr><tr><td>8:</td><td>end while</td></tr><tr><td>Output:</td><td>YSR = Yk* with k* = arg maxkET s(Yk).</td></tr></table>{{< /table-caption >}}

> üîº The table presents win-rate and length-controlled win-rate results for different language models and varying numbers of generations, comparing Best-of-N with the proposed Speculative Rejection method.
> <details>
> <summary>read the caption</summary>
> Table 1: Win-rate results across various settings for the Mistral-7B, Llama-3-8B, and Llama-3-8B-Instruct models, scored by the reward model ArmoRM-Llama-3-8B and evaluated using GPT-4-Turbo. 'WR' refers to win-rate, and 'LC-WR' refers to length-controlled win-rate.
> </details>



### More visual insights



<details>
<summary>More on charts
</summary>


![](https://ai-paper-reviewer.com/2410.20290/charts_8_0.png)

> üîº The chart displays the relative GPU compute and improvement score for Best-of-N and SPECULATIVE REJECTION across various generative models and reward models, showing that SPECULATIVE REJECTION achieves higher reward scores with fewer computational resources.
> <details>
> <summary>read the caption</summary>
> Figure 3: We evaluate our efficient implementation of SPECULATIVE REJECTION on the AlpacaFarm-Eval dataset using various generative models and reward models. The numbers indicate N for Best-of-N and rejection rate Œ± for SPECULATIVE REJECTION. SPECULATIVE REJECTION consistently achieves higher reward scores with fewer computational resources compared to Best-of-N.
> </details>


![](https://ai-paper-reviewer.com/2410.20290/charts_16_0.png)

> üîº The figure shows the Pearson and Kendall's tau correlation coefficients between partial and final rewards for different decision token numbers, indicating a positive correlation between them.
> <details>
> <summary>read the caption</summary>
> Figure 4: Pearson correlation (left) and Kendall's tau correlation coefficient (right) for the partial and final rewards. We randomly sample 100 prompts in the AlpacaFarm-Eval dataset. The responses are generated via Llama3-8b-Instruct and rewards are evaluated via Mistral-7B-RM.
> </details>


</details>



<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<br><table id='1' style='font-size:20px'><tr><td rowspan="2">Methods</td><td colspan="2">Mistral-7B</td><td colspan="2">Llama-3-8B</td><td colspan="2">Llama-3-8B-Instruct</td><td colspan="2">Average</td></tr><tr><td>WR</td><td>LC-WR</td><td>WR</td><td>LC-WR</td><td>WR</td><td>LC-WR</td><td>WR</td><td>LC-WR</td></tr><tr><td>Bo120</td><td>50.00</td><td>50.00</td><td>50.00</td><td>50.00</td><td>50.00</td><td>50.00</td><td>50.00</td><td>50.00</td></tr><tr><td>Bo240</td><td>60.69</td><td>60.07</td><td>50.45</td><td>50.27</td><td>49.92</td><td>52.89</td><td>53.69</td><td>54.41</td></tr><tr><td>Bo480</td><td>61.28</td><td>61.84</td><td>58.90</td><td>59.93</td><td>50.49</td><td>53.11</td><td>56.89</td><td>58.29</td></tr><tr><td>Bo960</td><td>67.50</td><td>68.07</td><td>59.20</td><td>60.26</td><td>50.39</td><td>51.64</td><td>59.03</td><td>59.99</td></tr><tr><td>Bo1920</td><td>75.20</td><td>76.27</td><td>60.57</td><td>61.05</td><td>51.86</td><td>53.13</td><td>62.54</td><td>63.48</td></tr><tr><td>Bo3840</td><td>76.13</td><td>77.21</td><td>59.19</td><td>57.91</td><td>53.36</td><td>54.01</td><td>62.89</td><td>63.04</td></tr><tr><td>Ours (a = 0.5)</td><td>69.42</td><td>73.31</td><td>73.60</td><td>77.91</td><td>55.50</td><td>58.80</td><td>66.17</td><td>70.01</td></tr></table>{{< /table-caption >}}
> üîº Table 1 shows the win rates and length-controlled win rates of different models and methods, comparing Best-of-N and Speculative Rejection, evaluated by GPT-4-Turbo.
> <details>
> <summary>read the caption</summary>
> Table 1: Win-rate results across various settings for the Mistral-7B, Llama-3-8B, and Llama-3-8B-Instruct models, scored by the reward model ArmoRM-Llama-3-8B and evaluated using GPT-4-Turbo. 'WR' refers to win-rate, and 'LC-WR' refers to length-controlled win-rate.
> </details>

{{< table-caption >}}
<br><table id='1' style='font-size:16px'><tr><td rowspan="2">Methods</td><td colspan="2">Mistral-7B</td><td colspan="2">Llama-3-8B</td><td colspan="2">Llama-3-8B-Instruct</td><td colspan="2">Average</td></tr><tr><td>PPL</td><td>Speedup</td><td>PPL</td><td>Speedup</td><td>PPL</td><td>Speedup</td><td>PPL</td><td>Speedup</td></tr><tr><td>Bo120</td><td>2.316</td><td>33.3x</td><td>2.020</td><td>31.9x</td><td>2.885</td><td>29.5x</td><td>2.407</td><td>31.6x</td></tr><tr><td>Bo240</td><td>2.143</td><td>15.9x</td><td>1.775</td><td>16.0x</td><td>2.718</td><td>15.9x</td><td>2.212</td><td>15.9x</td></tr><tr><td>Bo480</td><td>1.919</td><td>8.0x</td><td>1.595</td><td>8.1x</td><td>2.618</td><td>7.6x</td><td>2.044</td><td>7.9x</td></tr><tr><td>Bo960</td><td>1.744</td><td>4.0x</td><td>1.506</td><td>4.0x</td><td>2.533</td><td>4.1x</td><td>1.928</td><td>4.0x</td></tr><tr><td>Bo1920</td><td>1.637</td><td>2.0x</td><td>1.394</td><td>2.0x</td><td>2.449</td><td>2.0x</td><td>1.827</td><td>2.0x</td></tr><tr><td>Bo3840</td><td>1.488</td><td>1.0x</td><td>1.288</td><td>1.0x</td><td>2.318</td><td>1.0x</td><td>1.698</td><td>1.0x</td></tr><tr><td>Ours (a = 0.5)</td><td>1.476</td><td>76.9x</td><td>1.299</td><td>30.6x</td><td>1.887</td><td>12.1x</td><td>1.554</td><td>39.9x</td></tr></table>{{< /table-caption >}}
> üîº Table 1 presents win-rate results for different language models, comparing the performance of SPECULATIVE REJECTION against Best-of-N using different settings and metrics.
> <details>
> <summary>read the caption</summary>
> Table 1: Win-rate results across various settings for the Mistral-7B, Llama-3-8B, and Llama-3-8B-Instruct models, scored by the reward model ArmoRM-Llama-3-8B and evaluated using GPT-4-Turbo. 'WR' refers to win-rate, and 'LC-WR' refers to length-controlled win-rate.
> </details>

</details>


### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2410.20290/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20290/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}