{"references": [{"fullname_first_author": "Nathan Lambert", "paper_title": "RewardBench: Evaluating reward models for language modeling", "publication_date": "2024-03-13", "reason": "It is a widely used benchmark for evaluating reward models, providing a basis for comparison with the RAG-RewardBench."}, {"fullname_first_author": "Akari Asai", "paper_title": "Self-RAG: Learning to retrieve, generate, and critique through self-reflection", "publication_date": "2024-05-07", "reason": "It is a key example of preference-aligned RAG training, which is discussed as a paradigm shift in the paper."}, {"fullname_first_author": "Zhuoran Jin", "paper_title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment", "publication_date": "2024-12-18", "reason": "This is the main focus of the current paper, and hence is a crucial reference."}, {"fullname_first_author": "Yushi Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-04-05", "reason": "It introduces the concept of helpfulness and harmlessness, which is directly relevant to the RAG-RewardBench scenarios."}, {"fullname_first_author": "Ori Ram", "paper_title": "In-context retrieval-augmented language models", "publication_date": "2023-11-16", "reason": "It describes the direct approach to building RALMs using in-context learning, which is contrasted with the preference-aligned paradigm."}]}