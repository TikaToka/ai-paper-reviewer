{"references": [{"fullname_first_author": "Sanjeev Arora", "paper_title": "A latent variable model approach to PMI-based word embeddings", "publication_date": "2016-00-00", "reason": "This paper introduced a latent variable model approach for PMI-based word embeddings, a foundational concept in the field of word embeddings that is relevant to the study of anisotropy in LLMs."}, {"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper introduced the concept of few-shot learning in language models and is highly influential in the development and application of LLMs, which are directly relevant to the paper's study of anisotropic embeddings."}, {"fullname_first_author": "Daniel Bi\u015b", "paper_title": "Too much in common: Shifting of embeddings in transformer language models and its implications", "publication_date": "2021-00-00", "reason": "This paper directly addresses the problem of anisotropic embeddings in transformer language models, providing crucial insights into the root causes and potential solutions that are central to the current research."}, {"fullname_first_author": "Jun Gao", "paper_title": "Representation degeneration problem in training natural language generation models", "publication_date": "2019-00-00", "reason": "This paper was among the first to describe and analyze the phenomenon of representation degeneration (anisotropy) in language models, which is the central focus of the current research."}, {"fullname_first_author": "Diederik Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014-00-00", "reason": "This paper introduced the Adam optimizer, a widely used optimization algorithm in deep learning, whose role in the creation of anisotropic embeddings is investigated in the current study."}]}