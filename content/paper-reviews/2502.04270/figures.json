[{"figure_path": "https://arxiv.org/html/2502.04270/x1.png", "caption": "Figure 1: Overview of our approach. (a) We consider a full RLHF training setup, where a language model (LM) policy is iteratively refined through active data collection. Our goal is to develop an optimal response sampling method for preference labeling. (b) We introduce PILAF, which generates responses by interpolating between the current policy and a reference policy, balancing exploration and exploitation. (c) Our theoretical analysis shows that T-PILAF aligns the parameter gradient with the steepest direction for maximizing human values and achieves more favorable convergence in regions of high sensitivity.", "description": "\ubcf8 \uadf8\ub9bc\uc740 PILAF \uc811\uadfc \ubc29\uc2dd\uc5d0 \ub300\ud55c \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 RLHF \ud559\uc2b5 \uacfc\uc815\uc758 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc5b8\uc5b4 \ubaa8\ub378 \uc815\ucc45(LM policy)\uc740 \uc120\ud638\ub3c4 \ub808\uc774\ube14\ub9c1\uc744 \uc704\ud574 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\ub294 \uc801\uadf9\uc801\uc778 \ub370\uc774\ud130 \uc218\uc9d1\uc744 \ud1b5\ud574 \ubc18\ubcf5\uc801\uc73c\ub85c \uac1c\uc120\ub429\ub2c8\ub2e4. PILAF\uc758 \ubaa9\ud45c\ub294 \ucd5c\uc801\uc758 \uc751\ub2f5 \uc0d8\ud50c\ub9c1 \ubc29\ubc95\uc744 \uac1c\ubc1c\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. (b)\ub294 PILAF\ub97c \uc18c\uac1c\ud569\ub2c8\ub2e4. PILAF\ub294 \ud604\uc7ac \uc815\ucc45\uacfc \ucc38\uc870 \uc815\ucc45 \uac04\uc758 \ubcf4\uac04\uc744 \ud1b5\ud574 \uc751\ub2f5\uc744 \uc0dd\uc131\ud558\uc5ec \ud0d0\uc0c9\uacfc \ud65c\uc6a9 \uac04\uc758 \uade0\ud615\uc744 \ub9de\ucda5\ub2c8\ub2e4. (c)\ub294 T-PILAF\uac00 \ub9e4\uac1c\ubcc0\uc218 \uae30\uc6b8\uae30\ub97c \uc778\uac04\uc758 \uac00\uce58\ub97c \uadf9\ub300\ud654\ud558\ub294 \uac00\uc7a5 \uac00\ud30c\ub978 \ubc29\ud5a5\uacfc \uc815\ub82c\ud558\uace0, \ubbfc\uac10\ub3c4\uac00 \ub192\uc740 \uc601\uc5ed\uc5d0\uc11c \ub354 \ub098\uc740 \uc218\ub834\uc744 \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc774\ub860\uc801 \ubd84\uc11d\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04270/x2.png", "caption": "Figure 2: Reward-KL curve for Iterative DPO. All training runs start from the same model obtained at the end of the first iteration via Vanilla Sampling. Each dot represents an evaluation performed every 50 training steps.", "description": "\uadf8\ub9bc 2\ub294 \ubc18\ubcf5\uc801 DPO(Direct Preference Optimization) \uc124\uc815\uc5d0\uc11c \ubcf4\uc0c1-KL \uace1\uc120\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \ud6c8\ub828 \uc2e4\ud589\uc740 Vanilla Sampling\uc744 \ud1b5\ud574 \uccab \ubc88\uc9f8 \ubc18\ubcf5\uc758 \ub05d\uc5d0\uc11c \uc5bb\uc740 \ub3d9\uc77c\ud55c \ubaa8\ub378\uc5d0\uc11c \uc2dc\uc791\ud569\ub2c8\ub2e4. \uac01 \uc810\uc740 50\ubc88\uc758 \ud6c8\ub828 \ub2e8\uacc4\ub9c8\ub2e4 \uc218\ud589\ub41c \ud3c9\uac00\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \uadf8\ub798\ud504\ub294 PILAF(Policy-Interpolated Learning for Aligned Feedback)\uac00 \ubc18\ubcf5\uc801 DPO \ud658\uacbd\uc5d0\uc11c \ubcf4\uc0c1\uc744 \uadf9\ub300\ud654\ud558\uace0 \ucc38\uc870 \ubaa8\ub378\uacfc\uc758 KL(Kullback-Leibler) \ubc1c\uc0b0\uc744 \uc904\uc774\ub294 \ub370 \ud6a8\uacfc\uc801\uc784\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6.1 \ubc18\ubcf5\uc801 DPO"}, {"figure_path": "https://arxiv.org/html/2502.04270/x3.png", "caption": "Figure 3: Reward-KL curve for Online DPO. Each dot represents an evaluation performed every 50 training steps.", "description": "\uadf8\ub9bc 3\uc740 \uc628\ub77c\uc778 DPO \uc124\uc815\uc5d0\uc11c\uc758 \ubcf4\uc0c1-KL \uace1\uc120\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc810\uc740 50\ubc88\uc758 \ud559\uc2b5 \ub2e8\uacc4\ub9c8\ub2e4 \uc218\ud589\ub41c \ud3c9\uac00\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \uadf8\ub798\ud504\ub294 \uc628\ub77c\uc778 \ud559\uc2b5 \ud658\uacbd\uc5d0\uc11c PILAF \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub370 \uc911\uc810\uc744 \ub461\ub2c8\ub2e4. \uc989, \ubaa8\ub378\uc774 \ud559\uc2b5\ud558\ub294 \ub3d9\uc548 \uc9c0\uc18d\uc801\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ub418\uace0 \uc0c8\ub85c\uc6b4 \uc120\ud638\ub3c4 \ub370\uc774\ud130\uac00 \uc218\uc9d1\ub429\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc774 \uc778\uac04\uc758 \uc120\ud638\ub3c4\uc5d0 \ub9de\ucdb0 \ud6a8\uc728\uc801\uc73c\ub85c \ud559\uc2b5\ub420 \uc218 \uc788\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc2dc\uac01\uc801 \uc790\ub8cc\uc785\ub2c8\ub2e4.  Vanilla, Hybrid, Best-of-N \ub4f1\uc758 \ub2e4\ub978 \ubc29\ubc95\uacfc \ube44\uad50\ud558\uc5ec PILAF \ubc29\ubc95\uc774 \ubcf4\uc0c1\uc744 \ub192\uc774\uace0 \ucc38\uc870 \ubaa8\ub378\uacfc\uc758 KL \ubc1c\uc0b0\uc744 \ub0ae\ucd94\ub294 \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6.2 Online DPO"}, {"figure_path": "https://arxiv.org/html/2502.04270/x4.png", "caption": "Figure 4: Online DPO with an overfitted initial policy. Each dot represents an evaluation performed every 50 training steps. Color saturation indicates the training step, with darker colors representing later steps.", "description": "\uadf8\ub9bc 4\ub294 \uacfc\uc801\ud569\ub41c \ucd08\uae30 \uc815\ucc45\uc744 \uc0ac\uc6a9\ud55c \uc628\ub77c\uc778 DPO\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc810\uc740 50\ubc88\uc758 \ud559\uc2b5 \ub2e8\uacc4\ub9c8\ub2e4 \uc218\ud589\ub41c \ud3c9\uac00\ub97c \ub098\ud0c0\ub0b4\uba70, \uc0c9\uc0c1\uc758 \ucc44\ub3c4\ub294 \ud559\uc2b5 \ub2e8\uacc4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc5b4\ub450\uc6b4 \uc0c9\uc0c1\uc77c\uc218\ub85d \ub354 \ub2a6\uc740 \ub2e8\uacc4\uc784\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \uacfc\uc801\ud569\ub41c \ucd08\uae30 \uc815\ucc45\uc73c\ub85c \ud559\uc2b5\uc744 \uc2dc\uc791\ud588\uc744 \ub54c, PILAF\uac00 \uae30\uc874 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \uc5bc\ub9c8\ub098 \ub354 \uac15\uac74\ud558\uace0 \ud6a8\uc728\uc801\uc778\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Vanilla Sampling\uc758 \uacbd\uc6b0 KL divergence\uac00 \uae09\uaca9\ud788 \uc99d\uac00\ud558\uace0 reward \ud5a5\uc0c1\uc740 \uc815\uccb4\ub418\ub294 \ubc18\uba74, PILAF\ub294 \ucd08\uae30\uc5d0\ub294 KL divergence \ubcc0\ub3d9\uc774 \uc788\uc9c0\ub9cc, \uacb0\uad6d \ub354 \ub192\uc740 reward\uc640 \ub354 \ub0ae\uc740 KL divergence\ub97c \ub2ec\uc131\ud569\ub2c8\ub2e4. \uc774\ub294 PILAF\uc758 \ud0d0\uc0c9 \uc804\ub7b5\uc774 \uacfc\uc801\ud569\ub41c \ucd08\uae30 \uc815\ucc45\uc73c\ub85c \uc778\ud574 \ubc1c\uc0dd\ud558\ub294 \uc9c0\uc5ed\uc801 \ucd5c\uc801\ud654 \ubb38\uc81c\uc5d0\uc11c \ubc97\uc5b4\ub098\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub428\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "6.2 Online DPO"}, {"figure_path": "https://arxiv.org/html/2502.04270/x5.png", "caption": "Figure 5: Online DPO with an overfitted initial policy. Full results of the Figure\u00a04. Each dot represents an evaluation performed every 50 training steps. Color saturation indicates the training step, with darker colors representing later steps.", "description": "\uadf8\ub9bc 5\ub294 \uacfc\uc801\ud569\ub41c \ucd08\uae30 \uc815\ucc45\uc744 \uc0ac\uc6a9\ud55c \uc628\ub77c\uc778 DPO\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc 4\uc758 \uc804\uccb4 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc774 \uadf8\ub9bc\uc740 \uac01 \uc810\uc774 50\ud68c\uc758 \ud559\uc2b5 \ub2e8\uacc4\ub9c8\ub2e4 \uc218\ud589\ub41c \ud3c9\uac00\ub97c \ub098\ud0c0\ub0b4\uace0, \uc0c9\uc0c1 \ucc44\ub3c4\ub294 \ud559\uc2b5 \ub2e8\uacc4\ub97c \ub098\ud0c0\ub0b4\uba70, \uc5b4\ub450\uc6b4 \uc0c9\uc0c1\uc77c\uc218\ub85d \ub354 \ub2a6\uc740 \ud559\uc2b5 \ub2e8\uacc4\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uacfc\uc801\ud569\ub41c \ucd08\uae30 \uc815\ucc45\uc73c\ub85c \uc2dc\uc791\ud558\uc5ec Vanilla Sampling\uacfc PILAF\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc, PILAF\ub294 \ucd08\uae30 \ub2e8\uacc4\uc5d0\uc11c KL \ubc1c\uc0b0\uc774 \ubcc0\ub3d9\ud558\uc9c0\ub9cc, \ub354 \ub192\uc740 \ubcf4\uc0c1\uc744 \ub2ec\uc131\ud558\ub294 \uc815\ucc45\uc744 \uc5bb\uace0, Vanilla Sampling\ubcf4\ub2e4 KL \ubc1c\uc0b0\uc774 \ud6e8\uc52c \ub0ae\uc558\uc2b5\ub2c8\ub2e4. \uc774\ub294 PILAF\uc758 \ud0d0\uc0c9 \uc804\ub7b5\uc774 \uacfc\uc801\ud569\ub41c \ucd08\uae30 \uc815\ucc45\uc758 \ucd5c\uc801\ud654 \uacfc\uc815\uc5d0\uc11c \ub3c4\uc6c0\uc774 \ub418\uc5c8\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "6.2 Online DPO"}]