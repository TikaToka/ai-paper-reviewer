[{"content": "| Yunzhen Feng<sup>\u2020</sup> | Ariel Kwiatkowski<sup>\u2217</sup> | Kunhao Zheng<sup>\u2217</sup> | Julia Kempe<sup>\u22c4</sup> | Yaqi Duan<sup>\u22c4</sup> |\n|---|---|---|---|---|\n| NYU | Meta FAIR | Meta FAIR | Meta FAIR & NYU | NYU |", "caption": "Table 1:  A cost summary of PILAF and sampling methods from related works. Best-of-N method in Xiong et\u00a0al. (2024) uses the oracle reward to score all candidate responses, then selects the highest- and lowest-scoring ones\u2014instead of providing a preference label for only two responses. We restrict the oracle to providing only preference labels. Thus, we create a Best-of-N variant that uses the DPO internal reward for selection and then applies preference labeling, with an annotation cost of 2. We compare with this variant in the experiment.", "description": "\ud45c 1\uc740 \uc81c\uc548\ub41c PILAF \ubc29\ubc95\uacfc \uad00\ub828 \uc5f0\uad6c\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub2e4\ub978 \uc0d8\ud50c\ub9c1 \ubc29\ubc95\ub4e4\uc758 \ube44\uc6a9\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  Xiong et al. (2024)\uc758 Best-of-N \ubc29\ubc95\uc740 \uc624\ub77c\ud074 \ubcf4\uc0c1\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub4e0 \ud6c4\ubcf4 \uc751\ub2f5\uc744 \ud3c9\uac00\ud55c \ud6c4 \ucd5c\uace0\uc810\uacfc \ucd5c\uc800\uc810\uc744 \uc120\ud0dd\ud558\ub294 \ubc29\uc2dd\uc774\uc9c0\ub9cc, \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 \uc624\ub77c\ud074\uc774 \ub450 \uac1c\uc758 \uc751\ub2f5\ub9cc \ube44\uad50\ud558\uc5ec \uc120\ud638\ub3c4 \ub808\uc774\ube14\uc744 \uc81c\uacf5\ud558\ub3c4\ub85d \uc81c\ud55c\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c, \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 DPO \ub0b4\ubd80 \ubcf4\uc0c1\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc751\ub2f5\uc744 \uc120\ud0dd\ud55c \ud6c4 \uc120\ud638\ub3c4 \ub808\uc774\ube14\ub9c1\uc744 \uc801\uc6a9\ud558\ub294 Best-of-N \ubcc0\ud615 \ubc29\uc2dd\uc744 \ucd94\uac00\ud558\uc5ec \uc81c\uc548\ub41c PILAF \ubc29\ubc95\uacfc \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ubc29\ubc95\uc758 \uc0d8\ud50c\ub9c1 \ube44\uc6a9\uacfc \uc8fc\uc11d \ube44\uc6a9\uc774 \uc694\uc57d\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ud2b9\ud788 PILAF \ubc29\ubc95\uc758 \ud6a8\uc728\uc131\uc744 \uac15\uc870\ud558\uae30 \uc704\ud574 \ube44\uc6a9\uc744 \uc0c1\uc138\ud788 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "5 PILAF Algorithm"}, {"content": "| Method | \n\\vec{\\boldsymbol{y}}\\!\\,^{a} | \n\\vec{\\boldsymbol{y}}\\!\\,^{b} | Sampling Cost | Annotation Cost |\n|---|---|---|---|---|\n| *Vanilla* (Rafailov et al., 2023) | \n\\uppi_{\\theta} | \n\\uppi_{\\theta} | 2 | 2 |\n| *Best-of-N* (Xiong et al., 2024), N=8 | best of \\uppi_{\\theta} | worst of \\uppi_{\\theta} | 8 | 8* |\n| *Best-of-N* (with DPO reward), N=8 | best of \\uppi_{\\theta} | worst of \\uppi_{\\theta} | 8 | 2 |\n| *Hybrid* (Xie et al., 2024) | \\uppi_{\\theta} | \\uppi_{\\text{ref}} | 2 | 2 |\n| *PILAF* (OURS) | \\uppi_{\\theta}^{+}/\\uppi_{\\theta} | \\uppi_{\\theta}^{-}/\\uppi_{\\theta} | 3 | 2 |", "caption": "Table 2: Results of Iterative DPO. We report the average reward, KL divergence from the reference model, and objective J\ud835\udc3dJitalic_J on the testset. Higher reward and J\ud835\udc3dJitalic_J are better, while lower KL divergence is better. We use boldface to indicate the best result and underline to denote the second-best result.", "description": "\ud45c 2\ub294 \ubc18\ubcf5\uc801 DPO(Direct Preference Optimization) \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0\uc11c \ud3c9\uade0 \ubcf4\uc0c1, \ucc38\uc870 \ubaa8\ub378\uacfc\uc758 KL(Kullback-Leibler) \ubc1c\uc0b0, \uadf8\ub9ac\uace0 \ubaa9\uc801 \ud568\uc218 J\uc758 \uac12\uc744 \ubcf4\uace0\ud569\ub2c8\ub2e4. \ubcf4\uc0c1\uacfc \ubaa9\uc801 \ud568\uc218 J \uac12\uc774 \ub192\uc744\uc218\ub85d, KL \ubc1c\uc0b0\uc774 \ub0ae\uc744\uc218\ub85d \uc131\ub2a5\uc774 \uc88b\uc2b5\ub2c8\ub2e4. \uac00\uc7a5 \uc88b\uc740 \uacb0\uacfc\ub294 \uad75\uac8c \ud45c\uc2dc\ud558\uace0, \ub450 \ubc88\uc9f8\ub85c \uc88b\uc740 \uacb0\uacfc\ub294 \ubc11\uc904\ub85c \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubc18\ubcf5\uc801 DPO \uc124\uc815\uc5d0\uc11c PILAF(Policy-Interpolated Learning for Aligned Feedback) \ubc29\ubc95\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "6.1 \ubc18\ubcf5\uc801 DPO"}, {"content": "| Method | Reward (\u2191) | KL (\u2193) | J (\u2191) |\n|---|---|---|---|\n| *Vanilla* | -10.16 | 35.20 | -13.68 |\n| *Best-of-N* | -10.13 | 32.38 | -13.37 |\n| *Hybrid* | -10.51 | 22.86 | -12.80 |\n| *PILAF* (Ours) | -9.80 | 25.01 | -12.30 |", "caption": "Table 3: Results of Online DPO. We report the average reward, KL divergence from the reference model, and objective J\ud835\udc3dJitalic_J on the testset.", "description": "\ud45c 3\uc740 \ubcf8 \ub17c\ubb38\uc758 \uc628\ub77c\uc778 DPO(Direct Preference Optimization) \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud14c\uc2a4\ud2b8 \uc138\ud2b8\ub97c \uae30\uc900\uc73c\ub85c \ud3c9\uade0 \ubcf4\uc0c1, \ucc38\uc870 \ubaa8\ub378(reference model)\uacfc\uc758 KL(Kullback-Leibler) \ubc1c\uc0b0, \uadf8\ub9ac\uace0 \ubaa9\uc801 \ud568\uc218 J\uc758 \uac12\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \ub354 \ub192\uc740 \ubcf4\uc0c1\uacfc J \uac12\uc740 \ub354 \uc88b\uc740 \uc131\ub2a5\uc744, \ub0ae\uc740 KL \ubc1c\uc0b0\uc740 \ucc38\uc870 \ubaa8\ub378\uacfc\uc758 \ub354 \ub192\uc740 \uc720\uc0ac\uc131\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 PILAF \ubc29\ubc95\uc774 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc5d0 \ube44\ud574 \ub354 \ub192\uc740 \ubcf4\uc0c1\uacfc \ub354 \ub0ae\uc740 KL \ubc1c\uc0b0\uc744 \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.  \uc989, PILAF\uac00 \uc778\uac04\uc758 \uc120\ud638\ub3c4\ub97c \ub354 \uc798 \ubc18\uc601\ud558\ub294 \uc815\ucc45\uc744 \ud559\uc2b5\ud55c\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "6.2 Online DPO"}]