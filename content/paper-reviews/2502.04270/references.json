{"references": [{"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-31", "reason": "This paper is foundational to RLHF, the core technique of this paper."}, {"fullname_first_author": "Rafailov, R.", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-12-31", "reason": "This paper introduces DPO, an important efficiency improvement on RLHF, directly addressed by this paper."}, {"fullname_first_author": "Touvron, H.", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-01", "reason": "This paper introduces Llama-2, a model used in the experiments of this paper."}, {"fullname_first_author": "Xiong, W.", "paper_title": "Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint", "publication_date": "2024-12-31", "reason": "This paper is one of the baselines used for comparison in the experiments of this paper."}, {"fullname_first_author": "Guo, S.", "paper_title": "Direct language model alignment from online feedback", "publication_date": "2024-02-01", "reason": "This paper provides another baseline for comparison in the experiments of this paper."}]}