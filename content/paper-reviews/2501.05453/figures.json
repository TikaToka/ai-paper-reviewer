[{"figure_path": "https://arxiv.org/html/2501.05453/x1.png", "caption": "Figure 1: Overall Framework. Starting with images and video frames from a collection of datasets, we tokenize each frame/image into discrete visual tokens independently.\nWe pre-train the transformer by predicting the next visual tokens, with a context length of 4K tokens of images or video frames. Once trained, we take the intermediate representations and evaluate them on various tasks.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ud558\ub294 \uc804\uccb4 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b\uc73c\ub85c\ubd80\ud130 \uc774\ubbf8\uc9c0\uc640 \ube44\ub514\uc624 \ud504\ub808\uc784\uc744 \uac00\uc838\uc640 \uac01 \ud504\ub808\uc784/\uc774\ubbf8\uc9c0\ub97c \ub3c5\ub9bd\uc801\uc73c\ub85c \uc774\uc0b0\uc801\uc778 \uc2dc\uac01\uc801 \ud1a0\ud070\uc73c\ub85c \ubcc0\ud658\ud569\ub2c8\ub2e4. \ubcc0\ud658\ub41c \ud1a0\ud070\ub4e4\uc744 \uc0ac\uc6a9\ud558\uc5ec, 4K \ud1a0\ud070 \uae38\uc774(\uc774\ubbf8\uc9c0 \ub610\ub294 \ube44\ub514\uc624 \ud504\ub808\uc784)\uc758 \ubb38\ub9e5\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc74c \uc2dc\uac01\uc801 \ud1a0\ud070\uc744 \uc608\uce21\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub97c \uc0ac\uc804 \ud6c8\ub828\ud569\ub2c8\ub2e4. \ud6c8\ub828\uc774 \uc644\ub8cc\ub418\uba74 \uc911\uac04 \ud45c\ud604\uc744 \ucd94\ucd9c\ud558\uc5ec \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5\uc5d0 \ub300\ud574 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/toto_blue.png", "caption": "Figure 2: Training Loss Curves: We show the training loss curves for base, large, and 1b models trained with tokens from dVAE\u00a0(Ramesh et\u00a0al., 2021) with a vocabulary size of 8k and context length of 4k tokens (equivalent to 16 images or video frames).", "description": "\uadf8\ub9bc 2\ub294 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \ubaa8\ub378(\uae30\ubcf8, \ub300\ud615, 10\uc5b5 \ub9e4\uac1c\ubcc0\uc218)\uc5d0 \ub300\ud55c \ud559\uc2b5 \uc190\uc2e4 \uace1\uc120\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubaa8\ub378\uc740 dVAE \ud1a0\ud070\ud654\uae30\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub418\uc5c8\uc73c\uba70, \uc5b4\ud718 \ud06c\uae30\ub294 8,000\uc774\uace0 \ubb38\ub9e5 \uae38\uc774\ub294 4,000\ud1a0\ud070(16\uac1c\uc758 \uc774\ubbf8\uc9c0 \ub610\ub294 \ube44\ub514\uc624 \ud504\ub808\uc784\uc5d0 \ud574\ub2f9)\uc785\ub2c8\ub2e4. \uc774 \uadf8\ub798\ud504\ub294 \ubaa8\ub378 \ud06c\uae30\uc5d0 \ub530\ub978 \ud559\uc2b5 \uc9c4\ud589 \uc0c1\ud669\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2dc\uac01\uc801 \uc790\ub8cc\uc785\ub2c8\ub2e4.  \uac01 \uace1\uc120\uc740 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uc190\uc2e4 \uac12\uc774 \uc5b4\ub5bb\uac8c \uac10\uc18c\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\uba70, \ubaa8\ub378\uc758 \ud06c\uae30\uac00 \ud074\uc218\ub85d \uc190\uc2e4 \uac10\uc18c \uc18d\ub3c4\uac00 \ube60\ub974\ub2e4\ub294 \uac83\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub294 \ub354 \ud070 \ubaa8\ub378\uc774 \ub354 \ud6a8\uc728\uc801\uc73c\ub85c \ud559\uc2b5 \ub370\uc774\ud130\ub97c \ud559\uc2b5\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3 \uc811\uadfc \ubc29\uc2dd"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/tokens1gram_blue.png", "caption": "Figure 3: 1-gram Distribution of Various Tokens: This Figure shows the distribution of 1-gram tokens of various tokenizers (dVAE\u00a0(Ramesh et\u00a0al., 2021), VQGAN-1k, VQGAN-16k\u00a0(Esser et\u00a0al., 2020)) on Imagenet validation set. Note that, dVAE has almost full convergence of the tokens while VQGAN has less than 50% coverage of the tokens.", "description": "\uc774 \uadf8\ub9bc\uc740 ImageNet \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c \ub2e4\uc591\ud55c \ud1a0\ud070 \uc0dd\uc131\uae30(dVAE (Ramesh et al., 2021), VQGAN-1k, VQGAN-16k (Esser et al., 2020))\uc758 1-\uadf8\ub7a8 \ud1a0\ud070 \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. dVAE\ub294 \uac70\uc758 \ubaa8\ub4e0 \ud1a0\ud070\uc5d0 \ub300\ud574 \uc218\ub834\ud558\ub294 \ubc18\uba74, VQGAN\uc740 50% \ubbf8\ub9cc\uc758 \ud1a0\ud070\ub9cc\uc744 \ud3ec\ud568\ud55c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, dVAE \ud1a0\ud070\ud654 \ubc29\uc2dd\uc740 VQGAN\uc5d0 \ube44\ud574 ImageNet \ub370\uc774\ud130\uc14b\uc744 \ud6e8\uc52c \ub354 \uc798 \ud3ec\uad04\uc801\uc73c\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774\uac83\uc740 \ud1a0\ud070\uc758 \ub2e4\uc591\uc131\uacfc \ud45c\ud604\ub825 \uce21\uba74\uc5d0\uc11c dVAE\uc758 \uc6b0\uc218\uc131\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.1 Design Choices"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/franka_pick_toto_blue.png", "caption": "Table 4: Token Resolution: While the performance is lower for a low-resolution model, when finetuned for next-patch prediction at a higher resolution, its performance surpasses the full-resolution pre-trained model. \u2020\u2020{}^{\\text{\\textdagger}}start_FLOATSUPERSCRIPT \u2020 end_FLOATSUPERSCRIPT Base values of the RoPE is 50,000.", "description": "\ud45c 4\ub294 \ud1a0\ud070 \ud574\uc0c1\ub3c4\uc758 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub0ae\uc740 \ud574\uc0c1\ub3c4\ub85c \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc740 \uc131\ub2a5\uc774 \ub0ae\uc9c0\ub9cc, \ub354 \ub192\uc740 \ud574\uc0c1\ub3c4\uc5d0\uc11c \ub2e4\uc74c \ud328\uce58 \uc608\uce21\uc744 \uc704\ud574 \ubbf8\uc138 \uc870\uc815\ud558\uba74 \uc804\uccb4 \ud574\uc0c1\ub3c4\ub85c \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc6b0\uc218\ud574\uc9d0\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ROPE(Rotary Position Embedding)\uc758 \uae30\ubcf8\uac12\uc740 50,000\uc785\ub2c8\ub2e4.", "section": "4.1 \ub514\uc790\uc778 \uc120\ud0dd"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/kuka_pick_toto_blue.png", "caption": "Table 5: Attention vs Average Pooling: When probed at the same layers, attention pooling performs much better than average pooling of intermediate tokens.", "description": "\uc774 \ud45c\ub294 \uac19\uc740 \ub808\uc774\uc5b4\uc5d0\uc11c \ud3c9\uac00\ud588\uc744 \ub54c \uc5b4\ud150\uc158 \ud480\ub9c1\uc774 \uc911\uac04 \ud1a0\ud070\uc758 \ud3c9\uade0 \ud480\ub9c1\ubcf4\ub2e4 \ud6e8\uc52c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc900\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc5b4\ud150\uc158 \ud480\ub9c1\uc740 \ud1a0\ud070\uc758 \uc911\uc694\ub3c4\ub97c \ub3d9\uc801\uc73c\ub85c \uac00\uc911\uce58\ub97c \ubd80\uc5ec\ud558\uc5ec \uc911\uac04 \ud45c\ud604\uc744 \uc0dd\uc131\ud558\ub294 \ubc18\uba74, \ud3c9\uade0 \ud480\ub9c1\uc740 \ubaa8\ub4e0 \ud1a0\ud070\uc5d0 \ub3d9\uc77c\ud55c \uac00\uc911\uce58\ub97c \ubd80\uc5ec\ud569\ub2c8\ub2e4.  \ub530\ub77c\uc11c, \uc5b4\ud150\uc158 \ud480\ub9c1\uc740 \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc5d0 \ub354 \uc801\ud569\ud55c \ud45c\ud604\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.", "section": "4.1 \ub514\uc790\uc778 \uc120\ud0dd"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/franka_cabinet_toto_blue.png", "caption": "Figure 4: Probing at Different Layers: We show the attention-probing performance at each layer of our three models. Peak performance is observed at around 50% depth of the models.", "description": "\uadf8\ub9bc 4\ub294 \uc138 \uac00\uc9c0 \ud06c\uae30\uc758 \ubaa8\ub378(\uae30\ubcf8, \ud070, 10\uc5b5 \ub9e4\uac1c\ubcc0\uc218)\uc5d0 \ub300\ud55c \uac01 \ub808\uc774\uc5b4\uc758 \uc5b4\ud150\uc158 \ud504\ub85c\ube59 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc138 \uac00\uc9c0 \ubaa8\ub378 \ubaa8\ub450\uc5d0\uc11c \ucd5c\uace0 \uc131\ub2a5\uc740 \ubaa8\ub378\uc758 \uc57d 50% \uae4a\uc774\uc5d0\uc11c \uad00\ucc30\ub429\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc758 \uc911\uac04 \ub808\uc774\uc5b4\uac00 \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc5d0 \uac00\uc7a5 \uc720\uc6a9\ud55c \ud45c\ud604\uc744 \ud559\uc2b5\ud55c\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  \ucd08\ubc18 \ub808\uc774\uc5b4\ub294 \uc800\uc218\uc900 \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud558\uace0, \ud6c4\ubc18 \ub808\uc774\uc5b4\ub294 \uacfc\uc801\ud569\ub420 \uc218 \uc788\uc73c\ubbc0\ub85c \uc911\uac04 \ub808\uc774\uc5b4\uac00 \ucd5c\uc801\uc758 \uade0\ud615\uc744 \uc774\ub8f9\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/kuka_cabinet_toto_blue.png", "caption": "Figure 5: Semi-Supervised Tracking: We follow the protocol in STC\u00a0(Jabri et\u00a0al., 2020), start with the GT segmentation mask, and propagate the labels using the features computed by Toto-large. The mask was propagated up to 60 frames without losing much information.", "description": "\uadf8\ub9bc 5\ub294 \uc900\uc9c0\ub3c4 \ud559\uc2b5 \uae30\ubc18\uc758 \ube44\ub514\uc624 \uac1d\uccb4 \ucd94\uc801 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Jabri \ub4f1\uc758 \uc5f0\uad6c(2020)\uc5d0\uc11c \uc81c\uc2dc\ub41c \ubc29\ubc95\uc744 \ub530\ub77c, \uc815\ub2f5(GT) \ubd84\ud560 \ub9c8\uc2a4\ud06c\ub85c \uc2dc\uc791\ud558\uc5ec Toto-large \ubaa8\ub378\uc774 \uacc4\uc0b0\ud55c \ud2b9\uc9d5\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub808\uc774\ube14\uc744 \uc804\ud30c\ud569\ub2c8\ub2e4.  \ub9c8\uc2a4\ud06c\ub294 \uc815\ubcf4 \uc190\uc2e4 \uc5c6\uc774 \ucd5c\ub300 60\ud504\ub808\uc784\uae4c\uc9c0 \uc804\ud30c\ub429\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ubaa8\ub378\uc774 \uc2dc\uac04\uc5d0 \uac78\uccd0 \uac1d\uccb4\ub97c \uc815\ud655\ud558\uac8c \ucd94\uc801\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2dc\uac01\uc801 \uc99d\uac70\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4.5 \ube44\ub514\uc624 \ucd94\uc801"}, {"figure_path": "https://arxiv.org/html/2501.05453/x2.png", "caption": "(a) Franka Pick", "description": "\uadf8\ub9bc 6(a)\ub294 \uac15\ud654 \ud559\uc2b5\uc744 \uc0ac\uc6a9\ud55c \ub85c\ubd07 \uc870\uc791 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788, Franka \ub85c\ubd07\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubb3c\uccb4\ub97c \uc9d1\ub294 \uc791\uc5c5\uc758 \uc131\uacf5\ub960\uc744 \uc2dc\uac04\uc5d0 \ub530\ub77c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  Toto-base \ubaa8\ub378\uacfc MVP-base \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec Toto \ubaa8\ub378\uc774 \uc791\uc5c5 \ud559\uc2b5\uc744 \ub354 \ube60\ub974\uac8c \uc218\ud589\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub450 \ubaa8\ub378 \ubaa8\ub450 \uac15\ud654 \ud559\uc2b5\uc744 \ud1b5\ud574 \ud6c8\ub828\ub418\uc5c8\uc73c\uba70, \uc131\uacf5\ub960\uc740 \uc5ec\ub7ec \ubc88\uc758 \uc2dc\ub3c4\uc5d0 \uac78\uce5c \ud3c9\uade0\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4.5 \ube44\ub514\uc624 \ucd94\uc801"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot2x_vgpt_blue.png", "caption": "(b) Kuka Pick", "description": "\uadf8\ub9bc (b)\ub294 \ucfe0\uce74 \ub85c\ubd07 \uc554\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubb3c\uccb4\ub97c \uc9d1\ub294 \uc791\uc5c5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac15\ud654 \ud559\uc2b5\uc744 \ud1b5\ud574 \ud559\uc2b5\ub41c \ub85c\ubd07 \uc81c\uc5b4 \uc815\ucc45\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \ub2e4\uc591\ud55c \uc2dc\ub3c4\ub97c \uac70\uce5c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \uadf8\ub798\ud504\ub294 \uc2dc\ub3c4 \ud69f\uc218\uc5d0 \ub530\ub978 \uc131\uacf5\ub960\uc744 \ub098\ud0c0\ub0b4\uba70, Toto \uae30\ubc18 \ubaa8\ub378\uacfc \uae30\uc874 MAE \uae30\ubc18 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. Toto \ubaa8\ub378\uc774 \uc791\uc5c5 \ud559\uc2b5\uc5d0 \ub354 \ube60\ub974\uace0 \ud6a8\uc728\uc801\uc778 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc758 \ub85c\ubd07 \uc81c\uc5b4 \ubd80\ubd84\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc2dc\ubbac\ub808\uc774\uc158 \ud658\uacbd\uc5d0\uc11c\uc758 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.5 \ube44\ub514\uc624 \ucd94\uc801"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot3_toto_blue.png", "caption": "(c) Franka Cabinet", "description": "\uadf8\ub9bc (c)\ub294 \ub85c\ubd07 \uc870\uc791 \uc2e4\ud5d8 \uc911 \ud504\ub791\uce74 \uce90\ube44\ub2db \uc791\uc5c5\uc5d0 \ub300\ud55c \uc131\uacf5\ub960\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac15\ud654 \ud559\uc2b5\uc744 \ud1b5\ud574 \ub85c\ubd07\uc774 \uce90\ube44\ub2db\uc744 \uc5ec\ub294 \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub294 \uc131\uacf5\ub960\uc744 \uc5ec\ub7ec \ub2e8\uacc4\uc5d0 \uac78\uccd0 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. Toto-base \ubaa8\ub378\uacfc MVP-base \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec Toto-base \ubaa8\ub378\uc774 \uc791\uc5c5 \ud559\uc2b5 \uc18d\ub3c4 \uba74\uc5d0\uc11c \ub354 \uc6b0\uc218\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.5 \ube44\ub514\uc624 \ucd94\uc801"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot4_toto_blue.png", "caption": "(d) Kuka Cabinet", "description": "\uadf8\ub9bc (d)\ub294 \ucfe0\uce74 \ub85c\ubd07 \ud314\uc774 \uce90\ube44\ub2db \ubb38\uc744 \uc5ec\ub294 \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub294 \uac15\ud654 \ud559\uc2b5 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Toto-base \ubaa8\ub378\uacfc MVP-base \ubaa8\ub378\uc758 \uc131\uacf5\ub960\uc744 \ube44\uad50\ud558\uc5ec Toto \ubaa8\ub378\uc774 \uc791\uc5c5 \ud559\uc2b5\uc5d0 \ub354 \ube60\ub974\uace0 \ud6a8\uc728\uc801\uc784\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub9c9\ub300 \uadf8\ub798\ud504\ub294 \ud2b9\uc815 \ub2e8\uacc4\uc5d0\uc11c\uc758 \uc131\uacf5\ub960\uc744 \ub098\ud0c0\ub0b4\uba70, Toto-base \ubaa8\ub378\uc774 \uc804\ubc18\uc801\uc73c\ub85c \ub354 \ub192\uc740 \uc131\uacf5\ub960\uc744 \ub2ec\uc131\ud568\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 Toto \ubaa8\ub378\uc774 \ube44\ub514\uc624 \ub370\uc774\ud130\ub85c \uc0ac\uc804 \ud6c8\ub828\ub418\uc5b4 \uc2dc\uac01\uc801 \uc815\ubcf4\ub97c \ub354 \uc798 \ucc98\ub9ac\ud558\uace0 \uac15\ud654 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \ub354 \ud6a8\uacfc\uc801\uc73c\ub85c \ud559\uc2b5\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.5 \ube44\ub514\uc624 \ucd94\uc801"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot5_vgpt_blue.png", "caption": "Figure 6: Robot Manipulation with Reinforcement Learning: We compare MAE-base\u00a0(Radosavovic et\u00a0al., 2022) with Toto-base pre-trained models in simulation following\u00a0Xiao et\u00a0al. (2022). We evaluate each model the mean success rate over training steps. Toto was able to learn these tasks faster than MAE, across two robots and two tasks.", "description": "\uadf8\ub9bc 6\uc740 \uac15\ud654 \ud559\uc2b5\uc744 \uc0ac\uc6a9\ud55c \ub85c\ubd07 \uc870\uc791 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Radosavovic et al.(2022)\uc758 MAE-base \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec Toto-base \uc0ac\uc804 \ud6c8\ub828 \ubaa8\ub378\uc744 \uc2dc\ubbac\ub808\uc774\uc158 \ud658\uacbd\uc5d0\uc11c Xiao et al.(2022)\uc758 \ubc29\ubc95\uc744 \ub530\ub77c \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \ud3c9\uade0 \uc131\uacf5\ub960\uc744 \ud6c8\ub828 \ub2e8\uacc4\uc5d0 \ub530\ub77c \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. Toto \ubaa8\ub378\uc740 MAE \ubaa8\ub378\ubcf4\ub2e4 \ub450 \uac00\uc9c0 \ub85c\ubd07\uacfc \ub450 \uac00\uc9c0 \uc791\uc5c5\uc5d0\uc11c \ubaa8\ub450 \ud559\uc2b5 \uc18d\ub3c4\uac00 \ub354 \ube68\ub790\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ub124 \uac00\uc9c0 \ub2e4\ub978 \ub85c\ubd07 \uc870\uc791 \uc791\uc5c5(Franka Pick, Kuka Pick, Franka Cabinet, Kuka Cabinet)\uc5d0 \ub300\ud55c \uc131\uacf5\ub960 \uadf8\ub798\ud504\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uadf8\ub798\ud504\ub294 Toto-base \ubaa8\ub378\uacfc MAE-base \ubaa8\ub378\uc758 \uc131\uacf5\ub960\uc744 \ud6c8\ub828 \ub2e8\uacc4\uc5d0 \ub530\ub77c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 Toto \ubaa8\ub378\uc774 MAE \ubaa8\ub378\ubcf4\ub2e4 \ud559\uc2b5 \uc18d\ub3c4\uac00 \ube60\ub974\ub2e4\ub294 \uac83\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.5 \ube44\ub514\uc624 \ucd94\uc801"}, {"figure_path": "https://arxiv.org/html/2501.05453/x3.png", "caption": "Figure 7: Real-world Deployment: We show an example episode of our policy performing the cube picking task on a Franka robot in the real world. We use Toto-base to run the robot at real time, despite being a small model, Toto was able to achieve about 63% success rate in real world setting.", "description": "\uc774 \uadf8\ub9bc\uc740 \uc2e4\uc81c \ub85c\ubd07 \ud658\uacbd\uc5d0\uc11c Toto-base \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud050\ube0c \uc9d1\ub294 \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub294 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Toto-base\ub294 \ube44\uad50\uc801 \uc791\uc740 \ubaa8\ub378\uc774\uc9c0\ub9cc, \uc2e4\uc2dc\uac04\uc73c\ub85c \ub85c\ubd07\uc744 \uc81c\uc5b4\ud558\uc5ec \uc57d 63%\uc758 \uc131\uacf5\ub960\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 Toto \ubaa8\ub378\uc758 \uc2e4\uc81c \uc138\uacc4 \uc801\uc6a9 \uac00\ub2a5\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc99d\uac70\uc785\ub2c8\ub2e4.  Franka \ub85c\ubd07\uc774 \ud050\ube0c\ub97c \uc9d1\ub294 \uacfc\uc815\uc758 \uc5ec\ub7ec \ub2e8\uacc4\uac00 \uc21c\ucc28\uc801\uc73c\ub85c \ub098\ud0c0\ub098 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.6 \ub85c\ubcf4\ud2f1\uc2a4"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/toto-large-k400-val-set.png", "caption": "Figure 8: Probing Across Layers, Models, and Tasks: We study the behavior of our models across multiple layers and tasks. For image classification, action recognition, and object tracking, all the models behave similarly and peak around 50% of the model depth. This behavior is observed across all model sizes. Robot tasks show a similar behaviour, where the middle layers perform good at picking the objects, but last layers also perform good as middle layers. These plots suggests, in decoder-only model, first half of the model starts to behave like an encoder, and compress the information, and then rest of the model, projects the compressed semantic features back to input space.", "description": "\uadf8\ub9bc 8\uc740 \ub2e4\uc591\ud55c \uc791\uc5c5\uc5d0 \ub300\ud574 \ubaa8\ub378\uc758 \uc5ec\ub7ec \uacc4\uce35\uc5d0\uc11c\uc758 \ub3d9\uc791\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ubbf8\uc9c0 \ubd84\ub958, \ub3d9\uc791 \uc778\uc2dd \ubc0f \uac1d\uccb4 \ucd94\uc801\uc758 \uacbd\uc6b0 \ubaa8\ub4e0 \ubaa8\ub378\uc740 \uc720\uc0ac\ud558\uac8c \ub3d9\uc791\ud558\uba70 \ubaa8\ub378 \uae4a\uc774\uc758 \uc57d 50%\uc5d0\uc11c \ucd5c\uace0 \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ub3d9\uc791\uc740 \ubaa8\ub4e0 \ubaa8\ub378 \ud06c\uae30\uc5d0\uc11c \uad00\ucc30\ub429\ub2c8\ub2e4. \ub85c\ubd07 \uc791\uc5c5\uc740 \uc911\uac04 \uacc4\uce35\uc774 \ubb3c\uccb4\ub97c \uc9d1\ub294 \ub370 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\uc9c0\ub9cc \ub9c8\uc9c0\ub9c9 \uacc4\uce35\ub3c4 \uc911\uac04 \uacc4\uce35\ub9cc\ud07c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uc720\uc0ac\ud55c \ub3d9\uc791\uc744 \ubcf4\uc785\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uadf8\ub798\ud504\ub294 \ub514\ucf54\ub354 \uc804\uc6a9 \ubaa8\ub378\uc5d0\uc11c \ubaa8\ub378\uc758 \uc804\ubc18\ubd80\uac00 \uc778\ucf54\ub354\ucc98\ub7fc \ub3d9\uc791\ud558\uc5ec \uc815\ubcf4\ub97c \uc555\ucd95\ud55c \ub2e4\uc74c \ubaa8\ub378\uc758 \ud6c4\ubc18\ubd80\uac00 \uc555\ucd95\ub41c \uc758\ubbf8\uc801 \ud2b9\uc9d5\uc744 \uc785\ub825 \uacf5\uac04\uc73c\ub85c \ud22c\uc601\ud55c\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.8 \uacc4\uce35\ubcc4 \uc870\uc0ac"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/tokens2gram_blue.png", "caption": "Figure 9: Scaling Toto: We train multiple variants of Toto, with increasing hidden size and depth, with optimal learning rates. We plot the validation loss vs the compute spent on training in MACs. This shows a clear scaling behavior with optimal compute.", "description": "\uadf8\ub9bc 9\ub294 \ubaa8\ub378\uc758 \ud06c\uae30(\uc740\ub2c9\uce35 \ud06c\uae30 \ubc0f \uae4a\uc774)\ub97c \uc99d\uac00\uc2dc\ud0a4\uba74\uc11c \ucd5c\uc801\uc758 \ud559\uc2b5\ub960\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5ec\ub7ec \uac00\uc9c0 \ubcc0\ud615\ub41c Toto \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \uc5f0\uc0b0\ub7c9(MACs, Multiply-Accumulate operations)\uc744 \ub098\ud0c0\ub0b4\uace0, y\ucd95\uc740 \uac80\uc99d \uc190\uc2e4\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \uadf8\ub798\ud504\ub294 \ucd5c\uc801\uc758 \uc5f0\uc0b0\ub7c9\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c \uba85\ud655\ud55c \ud655\uc7a5\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, \uc5f0\uc0b0\ub7c9\uc774 \uc99d\uac00\ud560\uc218\ub85d \uac80\uc99d \uc190\uc2e4\uc774 \uac10\uc18c\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub428\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc758 \ud06c\uae30\ub97c \ud0a4\uc6b0\ub294 \uac83\uc774 \ud6a8\uacfc\uc801\uc784\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.9 Compute Optimal Scaling"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/tokens3gram_blue.png", "caption": "Figure 10: Average Validation Loss Over Tokens: We show the average loss per token for kinetics validation set. It clearly shows the redundancy in videos, as the first frame has higher prediction loss, and rest of the frames on average has lower loss than the first frame.", "description": "\uadf8\ub9bc 10\uc740 Kinetics \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c \ud1a0\ud070\ub2f9 \ud3c9\uade0 \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uccab \ubc88\uc9f8 \ud504\ub808\uc784\uc740 \uc608\uce21 \uc190\uc2e4\uc774 \ub354 \ub192\uace0, \ub098\uba38\uc9c0 \ud504\ub808\uc784\uc740 \ud3c9\uade0\uc801\uc73c\ub85c \uccab \ubc88\uc9f8 \ud504\ub808\uc784\ubcf4\ub2e4 \uc190\uc2e4\uc774 \ub354 \ub0ae\ub2e4\ub294 \uac83\uc744 \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ube44\ub514\uc624\uc758 \uc911\ubcf5\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc785\ub2c8\ub2e4. \uc989, \ub2e4\uc74c \ud504\ub808\uc784\uc744 \uc608\uce21\ud558\ub294 \uac83\uc774 \uc0c1\ub300\uc801\uc73c\ub85c \uc26c\uc6cc\uc11c \uc190\uc2e4\uc774 \ub0ae\uc544\uc9c0\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774\ub294 \ube44\ub514\uc624 \ud504\ub808\uc784 \uac04\uc758 \ub192\uc740 \uc0c1\uad00\uad00\uacc4 \ub54c\ubb38\uc5d0 \ubc1c\uc0dd\ud569\ub2c8\ub2e4.", "section": "3.3 \ub370\uc774\ud130\uc14b"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/frames1.png", "caption": "Table 15: Toto Varients: We scale Toto models by increasing hidden dimension and number of layers linearly while keeping number of heads constant following (Yang et\u00a0al., 2022; Touvron et\u00a0al., 2023).", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 4.9\uc808 \ucef4\ud4e8\ud305 \ucd5c\uc801 \ubc30\uc728 \uc870\uc815(Compute Optimal Scaling) \uc139\uc158\uc5d0 \uc788\ub294 \uadf8\ub9bc\uc73c\ub85c,  Toto \ubaa8\ub378\uc758 \ud06c\uae30\ub97c \uc870\uc815\ud558\ub294 \ub2e4\uc591\ud55c \ubcc0\ud615 \ubaa8\ub378\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Toto \ubaa8\ub378\uc758 \ud06c\uae30 \uc870\uc815\uc740  (Yang et al., 2022; Touvron et al., 2023)\uc758 \uc5f0\uad6c\ub97c \ub530\ub985\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c,  \uc740\ub2c9 \ucc28\uc6d0(hidden dimension)\uacfc \ub808\uc774\uc5b4 \uc218(number of layers)\ub97c \uc120\ud615\uc801\uc73c\ub85c \uc99d\uac00\uc2dc\ud0a4\uba74\uc11c \ud5e4\ub4dc \uc218(number of heads)\ub294 \uc77c\uc815\ud558\uac8c \uc720\uc9c0\ud569\ub2c8\ub2e4.  \uac01 \ubaa8\ub378 \ubcc0\ud615\uc5d0 \ub300\ud55c \ub9e4\uac1c\ubcc0\uc218 \uc218(Params), \ucc28\uc6d0(Dimension), \ud5e4\ub4dc \uc218(Heads), \ub808\uc774\uc5b4 \uc218(Layers)\ub97c \uba85\uc2dc\uc801\uc73c\ub85c \uc81c\uc2dc\ud558\uc5ec  \ubaa8\ub378 \ud06c\uae30 \ubcc0\ud654\uc5d0 \ub530\ub978 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e8\uc21c\ud788 \ubaa8\ub378\uc758 \ud06c\uae30 \uc815\ubcf4\ub9cc \uc81c\uacf5\ud558\ub294 \uac83\uc774 \uc544\ub2c8\ub77c, \ud6c4\uc18d \uc2e4\ud5d8\uc5d0\uc11c \ubaa8\ub378 \ud06c\uae30 \ubcc0\ud654\uc5d0 \ub530\ub978 \uc131\ub2a5 \ubcc0\ud654 \ubd84\uc11d \ubc0f \ucef4\ud4e8\ud305 \uc790\uc6d0 \ud6a8\uc728\uc131 \ud3c9\uac00\uc758 \uae30\ubc18\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4.9 Compute Optimal Scaling"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/frames2.png", "caption": "Figure 11: \u03bc\ud835\udf07\\muitalic_\u03bc-Parameterization Learning Rate: We show that \u03bc\ud835\udf07\\muitalic_\u03bc-Parameterization\u00a0(Yang et\u00a0al., 2022), we can train all width Toto models, with an single optimal learning rate of 2\u22127superscript272^{-7}2 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT.", "description": "\uadf8\ub9bc 11\uc740 \u03bc-\ud30c\ub77c\ubbf8\ud130\ud654(Yang et al., 2022)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \ub108\ube44\uc758 Toto \ubaa8\ub378\uc744 \ub2e8\uc77c \ucd5c\uc801 \ud559\uc2b5\ub960 \ubc94\uc704(2\u207b\u2077)\ub85c \ud559\uc2b5\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc758 \ub108\ube44\uc5d0 \uad00\uacc4\uc5c6\uc774 \uc77c\uad00\ub41c \ucd5c\uc801 \ud559\uc2b5\ub960\uc744 \ucc3e\uc744 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud558\uba70, \u03bc-\ud30c\ub77c\ubbf8\ud130\ud654 \uae30\ubc95\uc774 \ubaa8\ub378 \uc2a4\ucf00\uc77c\ub9c1\uc5d0 \ud6a8\uacfc\uc801\uc784\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ub108\ube44\uc758 Toto \ubaa8\ub378\uc5d0 \ub300\ud55c \ucd5c\uc801 \ud559\uc2b5\ub960 \uace1\uc120\uc744 \ubcf4\uc5ec\uc8fc\uba70, \uac01 \uace1\uc120\uc758 \ucd5c\uc800\uc810\uc774 \ucd5c\uc801 \ud559\uc2b5\ub960\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ubaa8\ub4e0 \uace1\uc120\uc758 \ucd5c\uc800\uc810\uc774 \ube44\uc2b7\ud55c \ubc94\uc704\uc5d0 \uc788\uc73c\ubbc0\ub85c, \u03bc-\ud30c\ub77c\ubbf8\ud130\ud654\ub97c \ud1b5\ud574 \ub2e8\uc77c \ucd5c\uc801 \ud559\uc2b5\ub960\ub85c \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \ubaa8\ub378\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \ud559\uc2b5\ud560 \uc218 \uc788\uc74c\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.9 Compute Optimal Scaling"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/frames3.png", "caption": "Figure 12: 2-gram Distribution of Various Tokens: We compute the 2-gram distribution on 10000 images from the ImageNet validation set. Compared to VQGAN 1k and 16k vocabulary tokenizers, the dVAE tokenizer has a larger set of token combinations.", "description": "\uc774 \uadf8\ub9bc\uc740 ImageNet \uac80\uc99d \uc138\ud2b8\uc758 10,000\uac1c \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 \uacc4\uc0b0\ub41c \ub2e4\uc591\ud55c \ud1a0\ud070\ud654 \ubc29\ubc95(dVAE, VQGAN-1k, VQGAN-16k)\uc758 2-gram \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  dVAE \ud1a0\ud070\ud654 \ubc29\uc2dd\uc740 VQGAN-1k \ubc0f VQGAN-16k \ud1a0\ud070\ud654 \ubc29\uc2dd\uacfc \ube44\uad50\ud558\uc5ec \ub354 \ub9ce\uc740 \ud1a0\ud070 \uc870\ud569\uc744 \uac00\uc9c0\uace0 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, dVAE\ub294 \uc774\ubbf8\uc9c0\uc758 \ub2e4\uc591\ud55c \ud2b9\uc9d5\ub4e4\uc744 \ub354\uc6b1 \uc138\ubd84\ud654\ud558\uc5ec \ub098\ud0c0\ub0b4\ub294 \ub354\uc6b1 \ud48d\ubd80\ud55c \ud1a0\ud070 \uc9d1\ud569\uc744 \uc0dd\uc131\ud55c\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3.4 Tokenization"}, {"figure_path": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot2_extra.png", "caption": "Figure 13: 3-gram Distribution of Various Tokens: We compute the 3-gram distribution on 10000 images from the ImageNet validation set. All the tokenizers has similar almost flat distribution when it comes to 3-gram tokens.", "description": "\uadf8\ub9bc 13\uc740 ImageNet \uac80\uc99d \uc138\ud2b8\uc758 10,000\uac1c \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 3-gram \ubd84\ud3ec\ub97c \uacc4\uc0b0\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc138 \uac00\uc9c0 \ud1a0\ud070 \uc0dd\uc131\uae30(dVAE, VQGAN-1k, VQGAN-16k) \ubaa8\ub450 3-gram \ud1a0\ud070\uc5d0 \ub300\ud574 \uac70\uc758 \ube44\uc2b7\ud55c \ud3c9\ud3c9\ud55c \ubd84\ud3ec\ub97c \ubcf4\uc785\ub2c8\ub2e4. \uc774\ub294 3-gram \uc218\uc900\uc5d0\uc11c\ub294 \ud1a0\ud070 \uc0dd\uc131\uae30\uc758 \ucc28\uc774\uac00 \ud06c\uc9c0 \uc54a\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  dVAE \ud1a0\ud070 \uc0dd\uc131\uae30\ub294 2-gram \uc218\uc900\uc5d0\uc11c\ub294 \ub354 \ub2e4\uc591\ud55c \uc870\ud569\uc744 \ubcf4\uc600\uc73c\ub098, 3-gram \uc218\uc900\uc5d0\uc11c\ub294 VQGAN \ud1a0\ud070 \uc0dd\uc131\uae30\uc640 \uc720\uc0ac\ud55c \ubd84\ud3ec\ub97c \ubcf4\uc785\ub2c8\ub2e4.", "section": "3.4 \ud1a0\ud070\ud654"}]