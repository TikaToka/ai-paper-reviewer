[{"content": "| Model | Params | Dimension | Heads | Layers |\n|---|---|---|---|---|\n| base | 120m | 768 | 12 | 12 |\n| large | 280m | 1024 | 16 | 16 |\n| 1b | 1.1b | 2048 | 16 | 22 |", "caption": "Table 1: Model Architecture: We pre-train models at different scales, only on visual tokens from images and videos.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 3.2\uc808(Architecture)\uc5d0\uc11c \ub2e4\uc591\ud55c \uaddc\ubaa8\uc758 \ubaa8\ub378\uc5d0 \ub300\ud55c \uad6c\uc870 \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'base', 'large', '1b' \uc138 \uac00\uc9c0 \ud06c\uae30\uc758 \ubaa8\ub378\uc774 \uc788\uc73c\uba70, \uac01 \ubaa8\ub378\uc740 \ub9e4\uac1c\ubcc0\uc218 \uc218, \ucc28\uc6d0, \ud5e4\ub4dc \uc218, \ub808\uc774\uc5b4 \uc218 \ub4f1\uc758 \uc8fc\uc694 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\uac00 \ub2e4\ub985\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubaa8\ub378\ub4e4\uc740 \uc774\ubbf8\uc9c0\uc640 \ube44\ub514\uc624\uc758 \uc2dc\uac01\uc801 \ud1a0\ud070\ub4e4\ub85c\ub9cc \uc0ac\uc804 \ud6c8\ub828\ub418\uc5c8\ub2e4\ub294 \uc810\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.  \uc989,  \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\ub294 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4. \ubaa8\ub378 \ud06c\uae30\uac00 \ucee4\uc9d0\uc5d0 \ub530\ub77c \ub9e4\uac1c\ubcc0\uc218 \uc218, \ucc28\uc6d0, \ud5e4\ub4dc \uc218, \ub808\uc774\uc5b4 \uc218 \ubaa8\ub450 \uc99d\uac00\ud558\uc5ec \ubaa8\ub378\uc758 \ud45c\ud604 \ub2a5\ub825\uc774 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3 Approach"}, {"content": "| Datasets | Instances | Tokens | Hours |\n|---|---|---|---|\n| ImageNet | 13.9M | 3.6B | - |\n| Kinetics-600 | 0.53M | 41.3B | 1496 |\n| Ego4D | 52.1K | 103B | 3750 |\n| HowTo100m | 1.172M | 2560B | 92627 |", "caption": "Table 2: Pre-training Dataset: We use both image datasets (Imagenet\u00a0(Russakovsky et\u00a0al., 2015)) and video datasets (Kinetics600\u00a0(Carreira et\u00a0al., 2019), Ego4D\u00a0(Grauman et\u00a0al., 2022), HowTo100m\u00a0(Miech et\u00a0al., 2019)) with different mixing ratios during the pre-training of our models. The whole training data contains about 100,000 hours of videos.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 \ub370\uc774\ud130\uc14b \uad6c\uc131\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ImageNet, Kinetics600, Ego4D, HowTo100M \ub4f1 \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0 \ubc0f \ube44\ub514\uc624 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud588\uc73c\uba70, \uc774\ub4e4\uc758 \ube44\uc728\uc744 \ub2e4\ub974\uac8c \uc870\uc815\ud558\uc5ec \ubaa8\ub378\uc744 \uc0ac\uc804 \ud6c8\ub828\uc2dc\ucf30\uc2b5\ub2c8\ub2e4.  \uc804\uccb4 \ud6c8\ub828 \ub370\uc774\ud130\ub294 \uc57d 10\ub9cc \uc2dc\uac04 \ubd84\ub7c9\uc758 \ube44\ub514\uc624\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc778\uc2a4\ud134\uc2a4 \uc218, \ud1a0\ud070 \uc218, \ube44\ub514\uc624 \uc2dc\uac04(\uc2dc\uac04)\uc774 \ud45c\uc5d0 \uc790\uc138\ud788 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.3 Dataset"}, {"content": "| Input-Target | Tokens | Vocabulary | Top1 |\n|---|---|---|---| \n| VQGAN-VQGAN | 16x16 | 16k | 61.3 |\n| VQGAN-VQGAN | 16x16 | 1k | 61.1 |\n| dVAE-dVAE | 32x32 | 8k | 61.2 |\n| dVAE-dVAE | 16x16 | 8k | 53.2 |\n| patch-patch | 16x16 | - | 60.6 |\n| patch-dVAE | 16x16 | 8k | 58.5 |", "caption": "Table 3: ImageNet Linear Probing Accuracy with Various Tokenizers: We compare discrete (dVAE, VQGAN) and patch embedding as input and target for pre-training our models. ImageNet top-1 accuracies are computed by linear probing at the 9th layer of the large model.", "description": "\uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud1a0\ud06c\ub098\uc774\uc800(dVAE, VQGAN, \ud328\uce58 \uc784\ubca0\ub529)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \uc0ac\uc804 \ud6c8\ub828\uc2dc\ud0a8 \ud6c4 ImageNet \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc120\ud615 \ud0d0\uc0c9(linear probing)\uc744 \ud1b5\ud574 \uc5bb\uc740 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, dVAE\uc640 VQGAN \ud1a0\ud06c\ub098\uc774\uc800\ub294 \ubd88\uc5f0\uc18d\uc801\uc778 \uc2dc\uac01\uc801 \ud1a0\ud070\uc744 \uc0dd\uc131\ud558\ub294 \ubc18\uba74, \ud328\uce58 \uc784\ubca0\ub529\uc740 \uc5f0\uc18d\uc801\uc778 \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc785\ub825\uacfc \ucd9c\ub825\uc73c\ub85c \uc0ac\uc6a9\ud588\uc744 \ub54c, \ubaa8\ub378\uc758 9\ubc88\uc9f8 \ub808\uc774\uc5b4\uc5d0\uc11c \uc120\ud615 \ud0d0\uc0c9\uc744 \uc218\ud589\ud558\uc5ec \uc5bb\uc740 ImageNet \ubd84\ub958 \uc815\ud655\ub3c4(top-1 \uc815\ud655\ub3c4)\uac00 \ub098\ud0c0\ub098 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \ud1a0\ud06c\ub098\uc774\uc800\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uace0, \ubaa8\ub378\uc758 \uc0ac\uc804 \ud6c8\ub828\uc5d0 \ub300\ud55c \ucd5c\uc801\uc758 \ud1a0\ud06c\ub098\uc774\uc800 \uc120\ud0dd\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.1 \ub514\uc790\uc778 \uc120\ud0dd (Design Choices)"}, {"content": "| Method | Compute | Top1 |\n|---|---|---|\ndVAE/16 | 1.42e+17 | 53.2 |\ndVAE/32 | 5.68e+17 | 61.2 |\ndVAE/16\u219232 | 2.13e+17 | 63.2 |\ndVAE/16\u219232\u2020 | 2.13e+17 | 64.4 |", "caption": "Table 6: Architecture: We compare sequence modeling architectures LLaMA\u00a0Touvron et\u00a0al. (2023), GPT2\u00a0Radford et\u00a0al. (2019), and non-transformer models, Mamba\u00a0Gu & Dao (2023) on ImageNet linear probing task.", "description": "\ud45c 6\uc740 ImageNet \uc120\ud615 \ud504\ub85c\ube59 \uc791\uc5c5\uc5d0\uc11c \ub2e4\uc591\ud55c \uc2dc\ud000\uc2a4 \ubaa8\ub378\ub9c1 \uc544\ud0a4\ud14d\ucc98(LLaMA, GPT2, Mamba)\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218\uc640 ImageNet \uc120\ud615 \ud504\ub85c\ube59 \uc791\uc5c5\uc5d0\uc11c \ub2ec\uc131\ud55c \uc0c1\uc704 1\uc704 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec,  \ub2e4\uc591\ud55c \uc544\ud0a4\ud14d\ucc98\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 \uc11c\ub85c \ub2e4\ub978 \uc544\ud0a4\ud14d\ucc98\uac00 \uc2dc\uac01\uc801 \ud45c\ud604 \ud559\uc2b5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8"}, {"content": "| Method | Tokens | Pooling | Top1 |\n|---|---|---|---| \n| dVAE | 16x16 | Average | 53.2 |\n| dVAE | 16x16 | Attention | 61.1 |", "caption": "Table 7: ImageNet Results: We compare discriminative and generative models on ImageNet\u00a0(Deng et\u00a0al., 2009) recognition task. While achieving comparable performance among generative models, our models model achieves the highest accuracy on autoregressive modeling. \u2020models are evaluated with linear probing.", "description": "\ud45c 7\uc740 ImageNet (Deng et al., 2009) \uc774\ubbf8\uc9c0 \ubd84\ub958 \uc791\uc5c5\uc5d0\uc11c \ub2e4\uc591\ud55c \ucc28\ubcc4\uc801 \ubc0f \uc0dd\uc131\uc801 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc790\uc138\ud788 \uc0b4\ud3b4\ubcf4\uba74, \ubcf8 \ub17c\ubb38\uc758 \ubaa8\ub378\ub4e4\uc774 \uc0dd\uc131\uc801 \ubaa8\ub378\ub4e4 \uc911\uc5d0\uc11c\ub294 \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\uc9c0\ub9cc, \ud2b9\ud788 \uc790\uae30\ud68c\uadc0\uc801 \ubaa8\ub378\ub9c1 \uce21\uba74\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud588\uc74c\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \u2020 \ud45c\uc2dc\ub294 \uc120\ud615 \ud0d0\uc0c9(linear probing) \ubc29\uc2dd\uc73c\ub85c \ud3c9\uac00\ub41c \ubaa8\ub378\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc989, \uae30\uc874\uc5d0 \ud559\uc2b5\ub41c \ubaa8\ub378\uc758 \ud2b9\uc9d5 \ucd94\ucd9c \ubd80\ubd84\ub9cc\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0c8\ub85c\uc6b4 \ubd84\ub958 \uc791\uc5c5\uc5d0 \uc801\uc6a9\ud55c \uacb0\uacfc\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "4.2 Image Recognition"}, {"content": "| Model | Params | Top1 |\n|---|---|---|\n| GPT2 [Radford et al. (2019)] | 280 m | 48.5 |\n| Mamba [Gu & Dao (2023)] | 290 m | 40.7 |\n| LLaMA [Touvron et al. (2023)] | 280 m | 53.2 |", "caption": "Table 8: K400 Results: We compare discriminative and generative models on Kinetics-400\u00a0(Kay et\u00a0al., 2017) action recognition task. While achieving comparable performance among generative models, our models are the first to show the competitive performance on K400 with autoregressive pre-training, and shows scaling with large model sizes.", "description": "\ud45c 8\uc740 Kinetics-400 (Kay et al., 2017) \ub3d9\uc791 \uc778\uc2dd \uc791\uc5c5\uc5d0\uc11c \ucc28\ubcc4\uc801 \ubaa8\ub378\uacfc \uc0dd\uc131\uc801 \ubaa8\ub378\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0dd\uc131\uc801 \ubaa8\ub378\ub4e4 \uac04\uc758 \uc131\ub2a5\uc774 \ube44\uc2b7\ud55c \uc218\uc900\uc784\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0, \ubcf8 \uc5f0\uad6c\uc758 \ubaa8\ub378\ub4e4\uc740 \uc624\ud1a0\ub9ac\uadf8\ub808\uc2dc\ube0c \uc0ac\uc804 \ud559\uc2b5\uc744 \ud1b5\ud574 Kinetics-400\uc5d0\uc11c \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc900 \ucd5c\ucd08\uc758 \ubaa8\ub378\uc774\uba70, \ubaa8\ub378 \ud06c\uae30\uac00 \ucee4\uc9d0\uc5d0 \ub530\ub77c \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\ub294 \uacbd\ud5a5\uc744 \ubcf4\uc785\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8"}, {"content": "| Method | Arch | #\u03b8 | Top1 |\n|---|---|---|---| \n| *Discriminative Approaches* |  |  |  |\n| SimCLR (Chen et al., 2020b)\u2020 | RN50x2 | 94 | 74.2 |\n| BYOL (Grill et al., 2020)\u2020 | RN50x2 | 94 | 77.4 |\n| SwAV (Caron et al., 2020)\u2020 | RN50x2 | 94 | 73.5 |\n| DINO (Caron et al., 2021) | ViT-B/8 | 86 | 80.1 |\n| DINOv2 (Oquab et al., 2023) | ViT-g/14 | 1011 | 86.4 |\n| *Generative Approaches* |  |  |  |\n| BEiT-L (Bao et al., 2021) | ViT-L/14 | 307 | 62.2 |\n| AIM (El-Nouby et al., 2024) | ViT-1B/14 | 1200 | 80.6 |\n| MAE (He et al., 2022) | ViT-H/14 | 632 | 80.9 |\n| iGPT-L (Chen et al., 2020a)\u2020 | GPT-2 | 1386 | 65.2 |\n| iGPT-XL (Chen et al., 2020a)\u2020 | GPT-2 | 6801 | 72.0 |\n| *Toto*-base | LLaMA | 120 | 64.7 |\n| *Toto*-large | LLaMA | 280 | 71.1 |\n| *Toto*-1b | LLaMA | 1100 | 75.3 |", "caption": "Table 9: Ego4D Results: Our model achieves comparable mean-average precision compared to previous work. We compare our method with, FRCNN+Rnd\u00a0(Grauman et\u00a0al., 2022), FRCNN+SF\u00a0(Grauman et\u00a0al., 2022), Hiera\u00a0(Ryali et\u00a0al., 2023), StillFast\u00a0(Ragusa et\u00a0al., 2023), VideoMAE\u00a0(Wang et\u00a0al., 2023a), and MAE-ST\u00a0(Feichtenhofer et\u00a0al., 2022).", "description": "\ud45c 9\ub294 Ego4D \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud55c \ub2e8\uae30 \ud589\ub3d9 \uc608\uce21 \uc791\uc5c5\uc5d0\uc11c \uc81c\uc548\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc81c\uc548\ub41c \ubaa8\ub378\uc740 \uae30\uc874 \uc5f0\uad6c\ub4e4(FRCNN+Rnd, FRCNN+SF, Hiera, StillFast, VideoMAE, MAE-ST)\uacfc \ube44\uad50\ud558\uc5ec \ud3c9\uade0 \uc815\ubc00\ub3c4 \uce21\uba74\uc5d0\uc11c \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub378\uc758 Noun, N+V, N+TTC, \uadf8\ub9ac\uace0 \uc804\uccb4 \ud3c9\uade0 \uc815\ubc00\ub3c4 \uc810\uc218\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc81c\uc548\ub41c \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \ucc99\ub3c4\uc5d0\uc11c \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.4 Action Forecasting"}, {"content": "| Method | Arch | Top1 |\n|---|---|---|\n| *Discriminative Approaches* |  |  |\n| I-JEPA (Assran et al., 2023) | ViT-H/16 | 74.5 |\n| OpenCLIP (Cherti et al., 2023) | ViT-G/14 | 83.3 |\n| DINOv2 (Oquab et al., 2023) | ViT-g/14 | 84.4 |\n| InternVideo (Wang et al., 2022) | - | 73.7 |\n| *Generative Approaches* |  |  |\n| Hiera (Ryali et al., 2023) | Hiera-H/14 | 77.0 |\n| MVD (Wang et al., 2023b) | ViT-H/14 | 79.4 |\n| VideoMAE (Wang et al., 2023a) | ViT-L/14 | 79.8 |\n| *Toto*-base | LLaMA | 59.3 |\n| *Toto*-large | LLaMA | 65.3 |\n| *Toto*-1b | LLaMA | 74.4 |", "caption": "Table 10: DAVIS Tracking: We report J, F, and J&F scores at the peak layers of each model. We achieves comparable performance as DINO and at large resolution (512), it outperforms all methods.", "description": "\ud45c 10\uc740 DAVIS(Densely Annotated Video Segmentation) \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud55c \ube44\ub514\uc624 \uac1d\uccb4 \ucd94\uc801 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0b4\ub294 \uc9c0\ud45c\ub85c J, F, J&F \uc810\uc218\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.  J\ub294 \uac1d\uccb4\uc758 \uc704\uce58 \uc815\ud655\ub3c4, F\ub294 \uac1d\uccb4\uc758 \ubd84\ud560 \uc815\ud655\ub3c4, J&F\ub294 \ub450 \uc9c0\ud45c\uc758 \uc870\ud654 \ud3c9\uade0\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud45c\ub294 \uac01 \ubaa8\ub378\uc5d0\uc11c \uc131\ub2a5\uc774 \uac00\uc7a5 \uc88b\uc740 \ub808\uc774\uc5b4\uc758 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uacb0\uacfc\uc5d0 \ub530\ub974\uba74, \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c Toto \ubaa8\ub378\uc740 DINO\uc640 \uc720\uc0ac\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\uc73c\uba70, \ud2b9\ud788 \uace0\ud574\uc0c1\ub3c4(512)\uc5d0\uc11c\ub294 \ub2e4\ub978 \ubaa8\ub4e0 \ubc29\ubc95\ub4e4\uc744 \ub2a5\uac00\ud558\ub294 \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4.  \uc989, Toto \ubaa8\ub378\uc774 \ube44\ub514\uc624 \uac1d\uccb4 \ucd94\uc801 \uc791\uc5c5\uc5d0\uc11c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.5 \ube44\ub514\uc624 \ucd94\uc801"}, {"content": "| Method | Noun | N+V | N+TTC | Overall |\n|---|---|---|---|---|\n| FRCNN+Rnd (Grauman et al., 2022) | 17.55 | 1.56 | 3.21 | 0.34 |\n| FRCNN+SF (Grauman et al., 2022) | 17.55 | 5.19 | 5.37 | 2.07 |\n| Hiera-large (Ryali et al., 2023) | 14.05 | 6.03 | 4.53 | 2.12 |\n| StillFast (Ragusa et al., 2023) | 16.20 | 7.47 | 4.94 | 2.48 |\n| VideoMAE-large (Wang et al., 2023a) | 15.16 | 6.72 | 5.26 | 2.55 |\n| MAE-ST-large (Feichtenhofer et al., 2022) | 13.71 | 6.63 | 4.94 | 2.60 |\n| *Toto*-large | 15.20 | 6.75 | 5.41 | 2.70 |", "caption": "Table 11: Robotics, Real-world Experiments: We compare MVP\u00a0(Radosavovic et\u00a0al., 2022) and Toto on a Franka cube-picking task in the real world. Features from both models are pre-trained, frozen, and passed into a learning module trained with behavior cloning using the same demonstrations.\nWe see that our approach performs comparably to the state-of-the-art vision backbone for robotics, despite not being designed with the robotic application in mind.", "description": "\ud45c 11\uc740 \uc2e4\uc81c \ud658\uacbd\uc5d0\uc11c\uc758 \ub85c\ubd07 \uc870\uc791 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788 Franka \ub85c\ubd07\uc744 \uc774\uc6a9\ud55c \ud050\ube0c \uc9d1\uae30 \uc791\uc5c5\uc5d0\uc11c MVP(Radosavovic et al., 2022) \uc640 Toto \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. \ub450 \ubaa8\ub378 \ubaa8\ub450 \uc0ac\uc804 \ud6c8\ub828\ub41c \ud2b9\uc9d5\uc744 \uc0ac\uc6a9\ud558\uba70, \ub3d9\uc77c\ud55c \ub370\ubaa8 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud589\ub3d9 \ubcf5\uc81c \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ub41c \ubaa8\ub4c8\uc5d0 \uc774\ub7ec\ud55c \ud2b9\uc9d5\uc744 \uc804\ub2ec\ud569\ub2c8\ub2e4.  \uc2e4\ud5d8 \uacb0\uacfc, Toto \ubaa8\ub378\uc774 \ub85c\ubd07 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud574 \ud2b9\ubcc4\ud788 \uc124\uacc4\ub418\uc9c0 \uc54a\uc558\uc74c\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ucd5c\ucca8\ub2e8 \ub85c\ubd07 \ube44\uc804 \ubc31\ubcf8\uacfc \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \ubcf8 \uc5f0\uad6c\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ubc29\ubc95\uc774 \ub85c\ubd07 \uc870\uc791 \uc791\uc5c5\uc5d0 \ud6a8\uacfc\uc801\uc784\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.6 \ub85c\ubcf4\ud2f1\uc2a4"}, {"content": "| Method (Res/Patch) | J&F | J | F |\n|---|---|---|---| \n| DINO-base (224/8) | 54.3 | 52.5 | 56.1 |\n| DINO-base (224/16) | 33.1 | 36.2 | 30.1 |\n| MAE-base (224/16) | 31.5 | 34.1 | 28.9 |\n| *Toto*-base (256/8) | 42.0 | 41.2 | 43.1 |\n| *Toto*-large (256/8) | 44.8 | 44.4 | 45.1 |\n| *Toto*-1b (256/8) | 46.1 | 45.8 | 46.4 |\n| *Toto*-large (512/8) | 62.4 | 59.2 | 65.6 |", "caption": "Table 12: Object Permanence: CATER\u00a0(Girdhar & Ramanan, 2019) object localization task, where the object is hidden under or obstructed by other objects. The model is trained to predict its coarse location. Our model performs better than previous methods on snitch localization task at 16, 32 temporal resolutions.", "description": "\ubcf8 \ud45c\ub294 CATER \ub370\uc774\ud130\uc14b(Girdhar & Ramanan, 2019)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac1d\uccb4\uc758 \uc601\uc18d\uc131(Object Permanence) \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  CATER \ub370\uc774\ud130\uc14b\uc740 \ubb3c\uccb4\uac00 \ub2e4\ub978 \ubb3c\uccb4\uc5d0 \uc758\ud574 \uac00\ub824\uc9c0\uac70\ub098 \uc228\uaca8\uc9c4 \uc0c1\ud669\uc5d0\uc11c \uac1d\uccb4\uc758 \uc704\uce58\ub97c \uc608\uce21\ud558\ub294 \uacfc\uc81c\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \ubc29\ubc95(V3D, TFC-V3D, Toto-large)\uc744 \uc0ac\uc6a9\ud55c \uc2e4\ud5d8 \uacb0\uacfc\uac00 \ub098\uc640 \uc788\uc73c\uba70, \ud2b9\ud788 Toto-large \ubaa8\ub378\uc774 16\ud504\ub808\uc784\uacfc 32\ud504\ub808\uc784 \ubaa8\ub450\uc5d0\uc11c \uae30\uc874 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub294 Toto \ubaa8\ub378\uc774 \uac1d\uccb4\uc758 \uc704\uce58\ub97c \uc608\uce21\ud558\ub294 \ub370 \uc788\uc5b4\uc11c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \uac00\uc9c0\uace0 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.7 \uac1d\uccb4 \uc601\uc18d\uc131"}, {"content": "| Model | # Traj | Success |\n|---|---|---|\n| MVP | 240 | 75% |\n| Toto-base | 240 | 63% |", "caption": "Table 13: Full Fine Tuning Performance: Comparison of different methods performance on ImageNet-1K.", "description": "\ud45c 13\uc740 ImageNet-1K \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \ubc29\ubc95\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  '\uc804\uccb4 \ubbf8\uc138 \uc870\uc815 \uc131\ub2a5' \uc774\ub77c\ub294 \uc81c\ubaa9\ucc98\ub7fc, \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc744 ImageNet-1K \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574 \ubbf8\uc138 \uc870\uc815\ud588\uc744 \ub54c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  DINO, MoCo v3, BEIT, MAE\uc640 \uac19\uc740 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uacfc Toto \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec,  Toto \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uacfc \ube44\uc2b7\ud558\uac70\ub098 \uadf8 \uc774\uc0c1\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 \ub514\uc790\uc778 \uc120\ud0dd"}, {"content": "| Method | Model | 16 | 32 |\n|---|---|---|---| \n| V3D | ResNet | 55.2 | 69.7 |\n| TFC V3D | ResNet | 54.6 | 70.2 |\n| *Toto*-large | LLaMa | 62.8 | 72.9 |", "caption": "Table 14: ImageNet Linear Probing Results: Toto performs better than similar size iGPT models.", "description": "\ubcf8 \ud45c\ub294 ImageNet \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc120\ud615 \ud504\ub85c\ube59 \ubc29\uc2dd\uc73c\ub85c \ud3c9\uac00\ud55c Toto \ubaa8\ub378\uacfc \uc720\uc0ac\ud55c \ud06c\uae30\uc758 iGPT \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Toto \ubaa8\ub378\uc774 iGPT \ubaa8\ub378\ubcf4\ub2e4 \ub354 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud588\uc74c\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c\ub294, \ube44\uc2b7\ud55c \ud06c\uae30\uc758 \ubaa8\ub378(\uc57d 10\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130)\uc5d0\uc11c Toto \ubaa8\ub378\uc774 iGPT \ubaa8\ub378\ubcf4\ub2e4 ImageNet \ubd84\ub958 \uc815\ud655\ub3c4\uac00 \ub354 \ub192\uc558\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 Toto \ubaa8\ub378\uc758 \uc124\uacc4 \ubcc0\uacbd\uc774 iGPT \ubaa8\ub378\uc5d0 \ube44\ud574 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud588\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.2 Image Recognition"}, {"content": "| DINO | MoCo v3 | BEiT | MAE | Toto |\n|---|---|---|---|---|\n| 82.8 | 83.2 | 83.2 | 83.6 | 82.6 |", "caption": "Table 16: K400 Results: We evaluate our models using cross attention and MLP layer as the classification head. Overall using a high-capacity head improves the performance across all models.", "description": "\ud45c 16\uc740 Kinetics-400 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \ub3d9\uc791 \uc778\uc2dd \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ud06c\ub85c\uc2a4 \uc5b4\ud150\uc158\uacfc \ub2e4\uce35 \ud37c\uc149\ud2b8\ub860(MLP) \ub808\uc774\uc5b4\ub97c \ubd84\ub958 \ud5e4\ub4dc\ub85c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud45c\uc5d0\uc11c \uc54c \uc218 \uc788\ub4ef\uc774, \uace0\uc6a9\ub7c9 \ud5e4\ub4dc\ub97c \uc0ac\uc6a9\ud558\uba74 \ubaa8\ub4e0 \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub429\ub2c8\ub2e4.  \uc989, \ub354 \ubcf5\uc7a1\ud55c \ubd84\ub958 \ud5e4\ub4dc\ub97c \uc0ac\uc6a9\ud558\uba74 \ubaa8\ub378\uc774 \ub3d9\uc791\uc744 \ub354 \uc815\ud655\ud558\uac8c \uc778\uc2dd\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ud2b9\ud788 \ub300\uaddc\ubaa8 \ubaa8\ub378\uc5d0\uc11c \ub354\uc6b1 \ub450\ub4dc\ub7ec\uc9c0\uac8c \ub098\ud0c0\ub0a9\ub2c8\ub2e4.", "section": "4.3 \ub3d9\uc791 \uc778\uc2dd"}]