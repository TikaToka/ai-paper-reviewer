{"references": [{"fullname_first_author": "Yuntao Bai", "paper_title": "Constitutional AI: Harmlessness from AI Feedback", "publication_date": "2022-12-08", "reason": "This paper introduces Constitutional AI, a framework for aligning AI models with human values, which is a foundational concept for the evaluation of LLMs."}, {"fullname_first_author": "Collin Burns", "paper_title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities with Weak Supervision", "publication_date": "2023-12-09", "reason": "This paper explores methods for improving the generalization capabilities of LLMs through weak supervision, which is relevant to the problem of LLM evaluation."}, {"fullname_first_author": "Dawei Li", "paper_title": "From Generation to Judgement: Opportunities and Challenges of LLM-as-a-Judge", "publication_date": "2024-11-16", "reason": "This paper directly addresses the topic of LLM-as-a-judge, providing a comprehensive overview of the opportunities and challenges of this evaluation approach."}, {"fullname_first_author": "Tu Vu", "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "publication_date": "2024-07-10", "reason": "This paper introduces Foundational Autoraters, a novel approach for evaluating LLMs that is directly related to the methods used in the current paper."}, {"fullname_first_author": "Peifeng Wang", "paper_title": "Direct Judgement Preference Optimization", "publication_date": "2024-09-14", "reason": "This paper introduces a novel optimization technique, Direct Judgement Preference Optimization (DPO), which is the core method used for training the Atla Selene Mini model."}]}