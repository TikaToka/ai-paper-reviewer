[{"content": "| name | date | size | type | Dutch-specific | data transparency | training transparency | finetuned from | wiki fertility | wiki tps | wiki s |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **fietje-2b** | 4/24 | 2.8B | base | yes | yes | yes | phi-2 | 9501.41 \u00b1 0.66 | 2.05 | 440.41 \u00b1 0.03 |\n| **fietje-2b-chat** | 4/24 | 2.8B | chat | yes | yes | yes | fietje-2b-instruct | 9501.41 \u00b1 0.66 | 2.05 | 440.41 \u00b1 0.03 |\n| **fietje-2b-instruct** | 4/24 | 2.8B | instruct | yes | yes | yes | fietje-2b | 9501.70 \u00b1 4.72 | 2.05 | 440.40 \u00b1 0.22 |\n| **GEITje-7B-ultra** | 1/24 | 7.2B | chat | yes | yes | yes | GEITje-7B | 4035.27 \u00b1 0.64 | 1.97 | 999.42 \u00b1 0.16 |\n| **Llama-3.2-3B-Instruct** | 9/24 | 3.2B | instruct | no | partly | partly | Llama-3.2-3B | 7884.63 \u00b1 0.36 | 1.74 | 451.97 \u00b1 0.02 |\n| **phi-2** | 12/23 | 2.8B | base | no | no | no | none | 9631.95 \u00b1 16.12 | 2.05 | 434.44 \u00b1 0.73 |\n| **Phi-3.5-mini-instruct** | 8/24 | 3.8B | instruct | underspecified | no | no | none | 6633.85 \u00b1 0.68 | 1.89 | 584.14 \u00b1 0.06 |\n| **Mistral-7B-Instruct-v0.1** | 9/23 | 7.2B | instruct | no | unclear<sup class=\"ltx_note_mark\">16</sup> | no | none | 4027.81 \u00b1 1.14 | 1.97 | 1001.27 \u00b1 0.28 |\n| **Mistral-7B-v0.1** | 9/23 | 7.2B | base | no | no | no | Mistral-7B-v0.1 | 4046.46 \u00b1 0.67 | 1.97 | 996.66 \u00b1 0.16 |\n| **Qwen2.5-3B-Instruct** | 9/24 | 3.1B | instruct | underspecified | no | no | Qwen2.5-3B | 8094.26 \u00b1 0.53 | 1.82 | 459.94 \u00b1 0.03 |\n| **GEITje-7B** | 12/23 | 7.2B | base | yes | partly | yes | Mistral-7B-v0.1 | 4021.61 \u00b1 1.64 | 1.97 | 1002.82 \u00b1 0.41 |\n| **tweety-7b-dutch-v24a** | 5/24 | 7.4B | base | yes | yes | partly | Mistral-7B-v0.1 | 3979.88 \u00b1 2.12 | 1.41 | 728.56 \u00b1 0.39 |\n| **Boreas-7B** | 4/24 | 7.2B | base | yes | partly | partly | Mistral-7B-v0.1 | 4032.05 \u00b1 15.28 | 1.97 | 1000.23 \u00b1 3.78 |\n| **Boreas-7B-chat** | 4/24 | 7.2B | instruct | yes | partly | partly | Boreas-7B | 4034.36 \u00b1 0.69 | 1.97 | 999.65 \u00b1 0.17 |", "caption": "Table 1: Overview of benchmarked models. Dutch-specific: did the model undergo (re-)training specifically for Dutch? data/training transparency: are the data and training procedure described in detail (reproducible) and is the data and training code publicly available? wiki fertility: how many tokens are needed on average to encode one word, calculated on full Dutch Wikipedia. Lower = more efficient. wiki tps: Tokens-per-second throughput on first 10,000 Wikipedia documents. How many tokens can the model process per second. wiki s: Processing time of first 10,000 Wikipedia documents. Lower = faster. wiki tps and wiki s were calculated on an isolated RTX 3090 in bfloat16 with Flash Attention 2 enabled. Batch size was 1 and all models\u2019 maximum context length was used, or 8192 at most. The reported mean metrics and their CI are based on the results of three runs.", "description": "\ud45c 1\uc740 \ubca4\uce58\ub9c8\ud0b9\uc5d0 \uc0ac\uc6a9\ub41c \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc744 \uac1c\uad04\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ud2b9\uc9d5\uacfc \uc131\ub2a5 \uc9c0\ud45c\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \ub124\ub35c\ub780\ub4dc\uc5b4\uc5d0 \ud2b9\ud654\ub41c \ubaa8\ub378 \uc5ec\ubd80, \ub370\uc774\ud130 \ubc0f \ud559\uc2b5 \uacfc\uc815\uc758 \ud22c\uba85\uc131,  \ud1a0\ud070 \ud6a8\uc728\uc131(wiki fertility), \ucd08\ub2f9 \ud1a0\ud070 \ucc98\ub9ac\ub7c9(wiki tps),  Wikipedia \ucc98\ub9ac \uc2dc\uac04(wiki s) \ub4f1\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  wiki tps \uc640 wiki s \ub294 \ub2e8\uc77c RTX 3090 GPU\ub97c \uc0ac\uc6a9\ud558\uc5ec bfloat16 \ud615\uc2dd\uacfc Flash Attention 2\ub97c \ud65c\uc131\ud654\ud55c \uc0c1\ud0dc\uc5d0\uc11c \uce21\uc815\ub418\uc5c8\uace0, \ubc30\uce58 \ud06c\uae30\ub294 1\uc774\uba70, \ubaa8\ub4e0 \ubaa8\ub378\uc758 \ucd5c\ub300 \ubb38\ub9e5 \uae38\uc774 \ub610\ub294 \ucd5c\ub300 8192 \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \ubcf4\uace0\ub41c \ud3c9\uade0 \uc9c0\ud45c\uc640 \uc2e0\ub8b0 \uad6c\uac04\uc740 \uc138 \ubc88\uc758 \uc2e4\ud589 \uacb0\uacfc\ub97c \uae30\ubc18\uc73c\ub85c \ud569\ub2c8\ub2e4.", "section": "4. Model overview"}, {"content": "| Model                      | Global MMLU | Global MMLU rank | Global DBRD | Global DBRD rank | Dutch CoLA | Dutch CoLA rank | Dutch ARC | Dutch ARC rank | XLWIC | XLWIC rank | median rank |\n|------------------------------|-------------|-----------------|-------------|-----------------|------------|-----------------|-----------|-----------------|-------|-------------|-------------|\n| **Phi-3.5-mini-instruct**   | 48.34 \u00b1 0.10 | 2               | 92.31 \u00b1 0.13 | 2               | 58.43 \u00b1 0.09 | 2               | 65.31 \u00b1 0.22 | 2               | 37.39 \u00b1 0.31 | 8           | 1.0         |\n| **Qwen2.5-3B-Instruct**     | 50.33 \u00b1 0.14 | 1               | 91.70 \u00b1 0.15 | 3               | 63.74 \u00b1 0.17 | 1               | 66.97 \u00b1 0.45 | 1               | 36.05 \u00b1 0.38 | 12          | 2.0         |\n| **Boreas-7B-chat**          | 44.93 \u00b1 0.15 | 3               | 94.38 \u00b1 0.27 | 1               | 52.87 \u00b1 0.42 | 4               | 59.88 \u00b1 0.66 | 3               | 33.78 \u00b1 0.34 | 14          | 3.5         |\n| **Llama-3.2-3B-Instruct**   | 35.59 \u00b1 0.22 | 4               | 59.74 \u00b1 0.97 | 8               | 55.35 \u00b1 1.45 | 3               | 42.80 \u00b1 0.93 | 4               | 42.72 \u00b1 0.76 | 3           | 3.5         |\n| **Mistral-7B-Instruct-v0.1** | 32.30 \u00b1 0.38 | 5               | 79.73 \u00b1 0.67 | 5               | 40.29 \u00b1 0.75 | 14              | 36.72 \u00b1 1.06 | 5               | 40.03 \u00b1 0.53 | 4           | 5.0         |\n| **GEITje-7B-ultra**        | 24.39 \u00b1 0.18 | 12              | 90.00 \u00b1 0.37 | 4               | 46.57 \u00b1 0.59 | 9               | 29.10 \u00b1 0.56 | 8               | 44.45 \u00b1 0.79 | 1           | 6.0         |\n| **tweety-7b-dutch-v24a**   | 27.36 \u00b1 0.32 | 7               | 40.22 \u00b1 1.04 | 14              | 51.27 \u00b1 1.00 | 5               | 29.46 \u00b1 1.25 | 7               | 43.23 \u00b1 1.07 | 2           | 7.0         |\n| **fietje-2b-chat**         | 26.36 \u00b1 0.25 | 9               | 58.78 \u00b1 0.58 | 9               | 45.45 \u00b1 0.64 | 10              | 31.56 \u00b1 0.78 | 6               | 39.24 \u00b1 0.94 | 5           | 8.0         |\n| **Boreas-7B**              | 27.02 \u00b1 0.63 | 8               | 70.33 \u00b1 1.12 | 6               | 49.34 \u00b1 0.51 | 6               | 26.19 \u00b1 0.85 | 12              | 37.24 \u00b1 0.98 | 10          | 9.5         |\n| **fietje-2b-instruct**     | 24.93 \u00b1 0.27 | 11              | 51.38 \u00b1 0.76 | 12              | 49.31 \u00b1 0.78 | 7               | 28.70 \u00b1 0.82 | 9               | 38.61 \u00b1 1.00 | 6           | 9.5         |\n| **Mistral-7B-v0.1**        | 27.51 \u00b1 0.15 | 6               | 63.69 \u00b1 0.85 | 7               | 48.00 \u00b1 0.51 | 8               | 26.82 \u00b1 0.85 | 11              | 37.27 \u00b1 0.88 | 9           | 11.0        |\n| **GEITje-7B**              | 25.12 \u00b1 0.37 | 10              | 46.28 \u00b1 0.78 | 13              | 43.67 \u00b1 0.72 | 11              | 27.61 \u00b1 1.30 | 10              | 37.64 \u00b1 0.75 | 7           | 12.0        |\n| **phi-2**                   | 20.82 \u00b1 0.28 | 14              | 51.45 \u00b1 0.66 | 11              | 42.29 \u00b1 0.75 | 12              | 18.07 \u00b1 0.52 | 14              | 36.55 \u00b1 0.97 | 11          | 13.0        |\n| **fietje-2b**              | 24.09 \u00b1 0.43 | 13              | 52.44 \u00b1 1.23 | 10              | 41.41 \u00b1 0.37 | 13              | 24.44 \u00b1 0.89 | 13              | 34.28 \u00b1 0.70 | 13          | 14.0        |", "caption": "Table 2: Benchmark results, showing weighted F1 score and the 95% confidence interval (obtained by running each benchmark five times on each model). Models\u2019 ranks are also given, although they should be taken with a grain of salt considering overlapping confidence intervals. The last column illustrates the final median ranking across all benchmarks.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ud1b5\ud574 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\uc911\uce58\uac00 \ubd80\uc5ec\ub41c F1 \uc810\uc218\uc640 95% \uc2e0\ub8b0 \uad6c\uac04\uc744 \uc81c\uc2dc\ud558\uc5ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc815\ud655\ud558\uac8c \ud3c9\uac00\ud569\ub2c8\ub2e4. \uac01 \ubca4\uce58\ub9c8\ud06c\ub294 5\ubc88 \uc2e4\ud589\ud558\uc5ec \uc2e0\ub8b0 \uad6c\uac04\uc744 \uacc4\uc0b0\ud588\uc2b5\ub2c8\ub2e4. \ubaa8\ub378 \uc21c\uc704 \ub610\ud55c \uc81c\uc2dc\ud558\uc9c0\ub9cc, \uc2e0\ub8b0 \uad6c\uac04\uc758 \uc911\ubcf5\uc73c\ub85c \uc778\ud574 \uc21c\uc704\uc5d0 \ub300\ud55c \ud574\uc11d\uc740 \uc8fc\uc758\ud574\uc57c \ud569\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9 \uc5f4\uc740 \ubaa8\ub4e0 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ucd5c\uc885 \uc911\uac04 \uc21c\uc704\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. \uacb0\uacfc"}]