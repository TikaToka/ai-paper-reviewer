<table id='0' style='font-size:14px'><tr><td>Model</td><td>All tasks</td><td>RAG Reranking</td><td>RAG Reader</td><td>Perplexity</td></tr><tr><td colspan="5">7B parameters models:</td></tr><tr><td>berkeley-nest/Starling-LM-7B-alpha</td><td>47.46</td><td>75.73</td><td>82.86</td><td>1438.04</td></tr><tr><td>openchat/openchat-3.5-0106</td><td>47.32</td><td>74.71</td><td>83.60</td><td>1106.56</td></tr><tr><td>Nexusflow/Starling-LM-7B-beta</td><td>45.69</td><td>74.58</td><td>81.22</td><td>1161.54</td></tr><tr><td>openchat/openchat-3.5-1210</td><td>44.17</td><td>71.76</td><td>82.15</td><td>1923.83</td></tr><tr><td>teknium/OpenHermes-2.5-Mistral-7B</td><td>42.64</td><td>70.63</td><td>80.25</td><td>1463.00</td></tr><tr><td>mistralai/Mistral-7B-Instruct-v0.2</td><td>40.29</td><td>72.58</td><td>79.39</td><td>2088.08</td></tr><tr><td>Bielik-7B-Instruct-v0.1</td><td>39.28</td><td>61.89</td><td>86.00</td><td>277.92</td></tr><tr><td>internlm/internlm2-chat-7b</td><td>37.64</td><td>72.29</td><td>71.17</td><td>3892.50</td></tr><tr><td>internlm/internlm2-chat-7b-sft</td><td>36.97</td><td>73.22</td><td>69.96</td><td>4269.63</td></tr><tr><td>HuggingFaceH4/zephyr-7b-alpha</td><td>33.97</td><td>71.47</td><td>73.35</td><td>4464.45</td></tr><tr><td>HuggingFaceH4/zephyr-7b-beta</td><td>33.15</td><td>71.65</td><td>71.27</td><td>3613.14</td></tr><tr><td>szymonrucinski/Curie-7B-v1</td><td>26.72</td><td>55.58</td><td>85.19</td><td>389.17</td></tr><tr><td>mistralai/Mistral-7B-Instruct-v0.1</td><td>26.42</td><td>56.35</td><td>73.68</td><td>6909.94</td></tr><tr><td>meta-Ilama/Llama-2-7b-chat-hf</td><td>21.04</td><td>54.65</td><td>72.93</td><td>4018.74</td></tr><tr><td>Voicelab/trurl-2-7b</td><td>18.85</td><td>60.67</td><td>77.19</td><td>1098.88</td></tr><tr><td>Baseline (majority class)</td><td>0.00</td><td>53.36</td><td>-</td><td>-</td></tr><tr><td colspan="5">Models with different sizes:</td></tr><tr><td>upstage/SOLAR-10.7B-Instruct-v1.0 (10.7B)</td><td>46.07</td><td>76.93</td><td>82.86</td><td>789.58</td></tr><tr><td>Voicelab/trurl-2-13b-academic (13B)</td><td>29.45</td><td>68.19</td><td>79.88</td><td>733.91</td></tr><tr><td>Azurro/APT3-1B-Instruct-v1 (1B)</td><td>-13.80</td><td>52.11</td><td>12.23</td><td>739.09</td></tr><tr><td colspan="5">7B parameters pretrained and continuously pretrained models:</td></tr><tr><td>alpindale/Mistral-7B-v0.2-hf</td><td>33.05</td><td>60.23</td><td>85.21</td><td>932.60</td></tr><tr><td>internlm/internlm2-7b</td><td>33.03</td><td>69.39</td><td>73.63</td><td>5498.23</td></tr><tr><td>mistralai/Mistral-7B-v0.1</td><td>30.67</td><td>60.35</td><td>85.39</td><td>857.32</td></tr><tr><td>Bielik-7B-v0.1</td><td>29.38</td><td>62.13</td><td>88.39</td><td>123.31</td></tr><tr><td>internlm/intermlm2-base-7b</td><td>20.68</td><td>52.39</td><td>69.85</td><td>3110.92</td></tr><tr><td>meta-llama/Llama-2-7b-hf</td><td>12.73</td><td>54.02</td><td>77.92</td><td>850.45</td></tr><tr><td>OPI-PG/Qra-7b</td><td>11.13</td><td>54.40</td><td>75.25</td><td>203.36</td></tr></table>