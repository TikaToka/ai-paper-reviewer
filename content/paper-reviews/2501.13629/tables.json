[{"content": "| Model | Overall | Hella. | ObQA | Wino. | ARC. | PIQA | SciQ | Bool. | Logi. | LAMB. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **MHA** ($n_{k}^{h}$=$n_{v}^{h}$=32) | **52.40** | 55.6 | 37.6 | 57.6 | 36.0 | 73.9 | 85.5 | 59.6 | 28.9 | 36.8 |\n| -50% V Heads ($n_{v}^{h}$=16) | **51.74** (<span class=\"ltx_font_medium\">\u21930.66)</span> | 55.5 | 39.6 | 55.0 | 35.9 | 71.6 | 85.9 | 56.9 | 28.3 | 37.0 |\n| -50% K Heads ($n_{k}^{h}$=16) | **52.83** (<span class=\"ltx_font_medium\"><span class=\"ltx_framed ltx_framed_underline\">\u21910.43)</span></span> | 55.1 | 38.8 | 56.4 | 35.8 | 71.9 | 85.2 | 63.6 | 29.0 | 39.7 |\n| **GQA** ($n_{k}^{h}$=$n_{v}^{h}$=16) | **52.14** | 55.1 | 39.6 | 56.3 | 35.4 | 71.9 | 85.0 | 61.4 | 27.8 | 36.8 |\n| -75% V Heads ($n_{v}^{h}$=4) | **51.76** (<span class=\"ltx_font_medium\">\u21930.38)</span> | 54.0 | 38.2 | 55.6 | 34.8 | 72.7 | 85.0 | 60.3 | 29.9 | 35.3 |\n| -75% K Heads ($n_{k}^{h}$=4) | **51.97** (<span class=\"ltx_font_medium\"><span class=\"ltx_framed ltx_framed_underline\">\u21930.17)</span></span> | 54.6 | 37.8 | 57.1 | 35.1 | 72.3 | 84.1 | 62.5 | 28.3 | 36.0 |\n| **GQA** ($n_{k}^{h}$=$n_{v}^{h}$=4) | **51.66** | 54.0 | 38.0 | 56.0 | 37.5 | 72.3 | 82.0 | 61.3 | 28.6 | 35.4 |\n| -75% V Heads ($n_{v}^{h}$=1) | **51.03** (<span class=\"ltx_font_medium\">\u21930.63)</span> | 53.5 | 38.4 | 57.0 | 35.1 | 72.1 | 82.6 | 56.9 | 28.4 | 35.1 |\n| -75% K Heads ($n_{k}^{h}$=1) | **51.67** (<span class=\"ltx_font_medium\"><span class=\"ltx_framed ltx_framed_underline\">\u21910.01)</span></span> | 53.9 | 36.2 | 58.6 | 36.9 | 71.1 | 83.5 | 60.7 | 28.7 | 35.5 |", "caption": "Table 1: Comparisons of model performance when reducing the same number of K heads versus V heads. The number of Q heads is 32 for all models (nqh=32superscriptsubscript\ud835\udc5b\ud835\udc5e\u210e32n_{q}^{h}=32italic_n start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT = 32). The results show that compressing the number of K heads has a relatively smaller impact on the overall model performance.", "description": "\uc774 \ud45c\ub294 \ub3d9\uc77c\ud55c \uc218\uc758 K \ud5e4\ub4dc\uc640 V \ud5e4\ub4dc\ub97c \uc904\uc600\uc744 \ub54c \ubaa8\ub378 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \ubaa8\ub378\uc5d0 \ub300\ud574 Q \ud5e4\ub4dc\uc758 \uc218\ub294 32\ub85c \uace0\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uacb0\uacfc\ub294 K \ud5e4\ub4dc\uc758 \uc218\ub97c \uc904\uc774\ub294 \uac83\uc774 \uc804\uccb4 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc774 \uc0c1\ub300\uc801\uc73c\ub85c \uc791\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, K \ud5e4\ub4dc\uc758 \ucc28\uc6d0\uacfc \uac1c\uc218\ub97c \uc904\uc774\ub294 \uac83\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc774 V \ud5e4\ub4dc\ubcf4\ub2e4 \uc0c1\ub300\uc801\uc73c\ub85c \uc801\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "2 DiffQKV Attention"}, {"content": "| Model | Overall | Hella. | ObQA | Wino. | ARC. | PIQA | SciQ | Bool. | Logi. | LAMB. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **MHA** (<span class=\"ltx_text ltx_font_bold\">n<sub>k</sub><sup>h</sup></span>=<span class=\"ltx_text ltx_font_bold\">n<sub>v</sub><sup>h</sup></span>=32) | **52.40** | 55.6 | 37.6 | 57.6 | 36.0 | 73.9 | 85.5 | 59.6 | 28.9 | 36.8 |\n| _w/_ Half K Dim. | **52.56** (<span class=\"ltx_text ltx_font_medium\">\u21910.16</span>) | 55.2 | 39.4 | 56.9 | 36.9 | 72.7 | 84.1 | 63.3 | 27.8 | 36.8 |\n| **GQA** (<span class=\"ltx_text ltx_font_bold\">n<sub>k</sub><sup>h</sup></span>=<span class=\"ltx_text ltx_font_bold\">n<sub>v</sub><sup>h</sup></span>=16) | **52.14** | 55.1 | 39.6 | 56.3 | 35.4 | 71.9 | 85.0 | 61.4 | 27.8 | 36.8 |\n| _w/_ Half K Dim. | **52.06** (<span class=\"ltx_text ltx_font_medium\">\u21930.08</span>) | 54.3 | 39.8 | 56.9 | 36.7 | 72.0 | 83.9 | 59.5 | 29.2 | 36.2 |\n| **GQA** (<span class=\"ltx_text ltx_font_bold\">n<sub>k</sub><sup>h</sup></span>=<span class=\"ltx_text ltx_font_bold\">n<sub>v</sub><sup>h</sup></span>=4) | **51.66** | 54.0 | 38.0 | 56.0 | 37.5 | 72.3 | 82.0 | 61.3 | 28.6 | 35.4 |\n| _w/_ Half K Dim. | **51.92** (<span class=\"ltx_text ltx_font_medium\">\u21910.26</span>) | 53.4 | 39.2 | 56.6 | 35.6 | 72.5 | 84.0 | 62.3 | 28.9 | 34.8 |", "caption": "Table 2: The ablation studies of halving the K head dimension. The results indicate that this adjustment, while largely improving the inference efficiency by reducing the size of KV cache, does not significantly compromise performance. The number of Q heads is 32 for all models (nqh=32superscriptsubscript\ud835\udc5b\ud835\udc5e\u210e32n_{q}^{h}=32italic_n start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT = 32).", "description": "\ud45c 2\ub294 K \ud5e4\ub4dc\uc758 \ucc28\uc6d0\uc744 \uc808\ubc18\uc73c\ub85c \uc904\uc600\uc744 \ub54c\uc758 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uacb0\uacfc\ub294 KV \uce90\uc2dc \ud06c\uae30\ub97c \uc904\uc784\uc73c\ub85c\uc368 \ucd94\ub860 \ud6a8\uc728\uc131\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub3d9\uc2dc\uc5d0 \uc131\ub2a5 \uc800\ud558\ub97c \ud06c\uac8c \uc720\ubc1c\ud558\uc9c0 \uc54a\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \ubaa8\ub378\uc5d0\uc11c Q \ud5e4\ub4dc\uc758 \uc218\ub294 32\uac1c\ub85c \ub3d9\uc77c\ud558\uac8c \uc720\uc9c0\ub418\uc5c8\uc2b5\ub2c8\ub2e4(nqh=32). \uc774 \ud45c\ub294 DiffQKV \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc5d0\uc11c K \ud5e4\ub4dc \ucc28\uc6d0\uc758 \uac10\uc18c\uac00 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud55c \uac83\uc785\ub2c8\ub2e4. K \ud5e4\ub4dc\uc758 \ucc28\uc6d0\uc744 \uc904\uc774\uba74 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc774 \uc904\uc5b4\ub4e4\uc5b4 \ucd94\ub860 \uc18d\ub3c4\uac00 \ube68\ub77c\uc9c8 \uc218 \uc788\uc9c0\ub9cc, \ub3d9\uc2dc\uc5d0 \ubaa8\ub378\uc758 \ud45c\ud604 \ub2a5\ub825\uc774 \uac10\uc18c\ud558\uc5ec \uc131\ub2a5\uc774 \uc800\ud558\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uadf8\ub7ec\ud55c \uc808\ucda9\uc810\uc744 \ubd84\uc11d\ud558\uace0 \ucd5c\uc801\uc758 K \ud5e4\ub4dc \ucc28\uc6d0\uc744 \ucc3e\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "2 DIFFQKV ATTENTION"}, {"content": "| Model | Overall | Hella. | ObQA | Wino. | ARC. | PIQA | SciQ | Bool. | Logi. | LAMB. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **MHA**  (n<sub>k</sub><sup>h</sup>=n<sub>v</sub><sup>h</sup>=32) | **52.40** | 55.6 | 37.6 | 57.6 | 36.0 | 73.9 | 85.5 | 59.6 | 28.9 | 36.8 |\n| + Sel.V-top100 | **52.10** (\u21930.30) | 55.6 | 37.6 | 57.6 | 36.0 | 74.0 | 84.8 | 59.3 | 27.0 | 36.9 |\n| **GQA** (n<sub>k</sub><sup>h</sup>=n<sub>v</sub><sup>h</sup>=16) | **52.14** | 55.1 | 39.6 | 56.3 | 35.4 | 71.9 | 85.0 | 61.4 | 27.8 | 36.8 |\n| + Sel.V-top100 | **52.08** (\u21930.06) | 55.2 | 39.6 | 56.3 | 35.4 | 71.8 | 84.4 | 61.6 | 27.8 | 36.7 |\n| **GQA** (n<sub>k</sub><sup>h</sup>=n<sub>v</sub><sup>h</sup>=4) | **51.66** | 54.0 | 38.0 | 56.0 | 37.5 | 72.3 | 82.0 | 61.3 | 28.6 | 35.4 |\n| + Sel.V-top100 | **51.67** (\u21910.01) | 54.0 | 38.2 | 55.9 | 37.5 | 72.2 | 82.0 | 61.2 | 28.6 | 35.4 |", "caption": "Table 3: The ablation studies of the model performance when only selectively loading the V vectors corresponding to the highest attention scores for approximate calculation. This operation significantly enhances inference efficiency by reducing memory usage. The number of Q heads is 32 for all models in the table (nqh=32superscriptsubscript\ud835\udc5b\ud835\udc5e\u210e32n_{q}^{h}=32italic_n start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT = 32).", "description": "\ud45c 3\uc740 \uc5b4\ud150\uc158 \uc810\uc218\uac00 \uac00\uc7a5 \ub192\uc740 V \ubca1\ud130\ub9cc \uc120\ud0dd\uc801\uc73c\ub85c \ub85c\ub4dc\ud558\uc5ec \uadfc\uc0ac \uacc4\uc0b0\uc744 \uc218\ud589\ud588\uc744 \ub54c \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ub300\ud55c \ucd94\uac00 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ubc29\ubc95\uc740 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \uc904\uc5ec \ucd94\ub860 \ud6a8\uc728\uc131\uc744 \ud06c\uac8c \ub192\uc785\ub2c8\ub2e4. \ud45c\uc5d0 \uc788\ub294 \ubaa8\ub4e0 \ubaa8\ub378\uc758 Q \ud5e4\ub4dc \uc218\ub294 32\uac1c\uc785\ub2c8\ub2e4(nqh=32). \uc774 \ud45c\ub294 \uc120\ud0dd\uc801 V \ubca1\ud130 \ub85c\ub529\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ubd84\uc11d\ud558\uace0, \uba54\ubaa8\ub9ac \ud6a8\uc728\uacfc \uc131\ub2a5 \uac04\uc758 \uade0\ud615\uc744 \ucc3e\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub418\ub294 \ub2e4\uc591\ud55c \uc124\uc815\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c\ub294, V \ubca1\ud130\uc758 \uc77c\ubd80\ub9cc \uc120\ud0dd\uc801\uc73c\ub85c \ub85c\ub4dc\ud558\uc5ec \ucd94\ub860 \uc18d\ub3c4\ub97c \uac1c\uc120\ud558\ub294 \uc804\ub7b5\uc744 \uc81c\uc2dc\ud558\uace0, \uc2e4\ud5d8\uc744 \ud1b5\ud574 \uc774 \uc804\ub7b5\uc758 \uc720\ud6a8\uc131\uc744 \uac80\uc99d\ud569\ub2c8\ub2e4. \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \ucd5c\uc801\uc758 \uc124\uc815\uc744 \uc81c\uc2dc\ud558\uace0, SIGMA \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uc744 \ub4b7\ubc1b\uce68\ud558\ub294 \uc911\uc694\ud55c \uadfc\uac70\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "2 DIFFQKV ATTENTION"}, {"content": "| Model | Overall | Hella. | ObQA | Wino. | ARC. | PIQA | SciQ | Bool. | Logi. | LAMB. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **MHA** | **52.40** | 55.6 | 37.6 | 57.6 | 36.0 | 73.9 | 85.5 | 59.6 | 28.9 | 36.8 |\n| + AugQ ($d_{q}^{h}$=5632) | **53.03**<sup>**\u21910.63**</sup> | 57.4 | 38.0 | 57.9 | 39.4 | 72.9 | 85.9 | 60.1 | 27.3 | 38.3 |\n| **GQA** ($n_{k}^{h}$=$n_{v}^{h}$=16) | **52.14** | 55.1 | 39.6 | 56.3 | 35.4 | 71.9 | 85.0 | 61.4 | 27.8 | 36.8 |\n| + AugQ ($d_{q}^{h}$=3072) | **53.38**<sup>**\u21911.24**</sup> | 56.6 | 40.2 | 56.1 | 40.0 | 73.2 | 87.3 | 61.0 | 28.3 | 37.7 |\n| + AugQ ($d_{q}^{h}$=4096) | **52.93**<sup>**\u21910.79**</sup> | 56.7 | 40.8 | 56.9 | 37.1 | 73.6 | 83.5 | 61.7 | 28.0 | 38.1 |\n| + AugQ ($d_{q}^{h}$=5632) | **53.07**<sup>**\u21910.93**</sup> | 57.3 | 39.8 | 57.3 | 36.4 | 74.2 | 83.6 | 61.4 | 28.7 | 39.0 |\n| **GQA** ($n_{k}^{h}$=$n_{v}^{h}$=4) | **51.66** | 54.0 | 38.0 | 56.0 | 37.5 | 72.3 | 82.0 | 61.3 | 28.6 | 35.4 |\n| + AugQ ($d_{q}^{h}$=5632) | **53.13**<sup>**\u21911.47**</sup> | 56.5 | 40.8 | 58.2 | 37.6 | 73.6 | 84.7 | 61.2 | 27.5 | 37.9 |", "caption": "Table 4: Comparisons between the baseline model architectures and those incorporating augmented Q. dqhsuperscriptsubscript\ud835\udc51\ud835\udc5e\u210ed_{q}^{h}italic_d start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT refers to the intermediate Q head dimension. The number of Q heads is 32 for all models in the table (nqh=32superscriptsubscript\ud835\udc5b\ud835\udc5e\u210e32n_{q}^{h}=32italic_n start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT = 32). For the baseline without AugQ, the intermediate dimension of Q head is dqh=2048superscriptsubscript\ud835\udc51\ud835\udc5e\u210e2048d_{q}^{h}=2048italic_d start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT = 2048.", "description": "\ud45c 4\ub294 \uae30\ubcf8 \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\uc640 \uc99d\uac15\ub41c Q\ub97c \ud1b5\ud569\ud55c \uc544\ud0a4\ud14d\ucc98 \uac04\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  dqh(d_{q}^{h})\ub294 \uc911\uac04 Q \ud5e4\ub4dc \ucc28\uc6d0\uc744 \ub098\ud0c0\ub0b4\uba70, \ud45c\uc758 \ubaa8\ub4e0 \ubaa8\ub378\uc5d0\uc11c Q \ud5e4\ub4dc\uc758 \uc218\ub294 32(n_{q}^{h}=32)\ub85c \uc77c\uc815\ud569\ub2c8\ub2e4. AugQ\uac00 \uc5c6\ub294 \uae30\ubcf8 \ubaa8\ub378\uc758 \uacbd\uc6b0, Q \ud5e4\ub4dc\uc758 \uc911\uac04 \ucc28\uc6d0\uc740 dqh=2048\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 Q \ud5e4\ub4dc\uc758 \ucc28\uc6d0\uc744 \uc99d\uac00\uc2dc\ud0a4\ub294 \uac83\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\uae30 \uc704\ud55c \uac83\uc785\ub2c8\ub2e4.", "section": "2 DiffQKV \uc5b4\ud150\uc158"}, {"content": "Model|Overall|Hella.|ObQA|Wino.|ARC.|PIQA|SciQ|Bool.|Logi.|LAMB.|LM\n---|---|---|---|---|---|---|---|---|---|---\n**GQA**|**52.14**|55.1|39.6|56.3|35.4|71.9|85.0|61.4|27.8|36.8\n+ AugF (\u0394dF=\u03b4)|**53.26 (\u21911.12)**|57.6|39.6|57.3|38.5|73.2|87.3|59.0|27.6|39.2\n+ AugQ (dqh=\u03b4)|**53.38 (\u21911.24)**|56.6|40.2|56.1|40.0|73.2|87.3|61.0|28.3|37.7\n+ AugF (\u0394dF=2\u03b4)|**53.16 (\u21911.02)**|59.3|40.0|57.0|38.3|73.9|85.2|59.8|24.9|40.1\n+ AugF (\u0394dF=\u03b4) & AugQ (dqh=\u03b4)|**54.55 (\u21912.41)**|58.8|41.8|57.5|39.7|74.4|86.9|62.4|28.1|41.5\n+ AugF (\u0394dF=3\u03b4)|**54.50 (\u21912.36)**|60.5|42.4|59.8|39.8|74.7|87.3|59.9|27.0|39.2\n+ AugF (\u0394dF=2\u03b4) & AugQ (dqh=\u03b4)|**54.67 (\u21912.53)**|60.5|41.6|57.4|39.9|74.7|87.3|59.6|27.3|43.8\n+ AugF (\u0394dF=5\u03b4)|**55.08 (\u21912.94)**|62.4|41.4|57.3|41.3|75.3|88.3|60.6|26.6|42.5\n+ AugF (\u0394dF=3\u03b4) & AugQ (dqh=\u03b4)|**55.09 (\u21912.95)**|61.6|39.8|61.0|40.5|75.1|88.7|59.6|28.3|41.2", "caption": "Table 5: Comparisons of the model performance when incorporating the augmented Q component (AugQ) with different sizes and enlarging the FFN module (AugF). The baseline method is GQA, with the FFN dimension being 5632 and nkhsuperscriptsubscript\ud835\udc5b\ud835\udc58\u210en_{k}^{h}italic_n start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT=nvhsuperscriptsubscript\ud835\udc5b\ud835\udc63\u210en_{v}^{h}italic_n start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT=16. \u0394\u2062dF\u0394subscript\ud835\udc51\ud835\udc39\\Delta d_{F}roman_\u0394 italic_d start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT denotes the enlarged dimension for the FFN module, while dqhsuperscriptsubscript\ud835\udc51\ud835\udc5e\u210ed_{q}^{h}italic_d start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT represents the intermediate Q head dimension (\u03b4\ud835\udeff\\deltaitalic_\u03b4=3072307230723072).", "description": "\ud45c 5\ub294 \uc99d\uac15\ub41c Q \uad6c\uc131 \uc694\uc18c(AugQ)\ub97c \ub2e4\uc591\ud55c \ud06c\uae30\ub85c \ud1b5\ud569\ud558\uace0 FFN \ubaa8\ub4c8(AugF)\uc744 \ud655\uc7a5\ud560 \ub54c \ubaa8\ub378 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uae30\uc900 \ubc29\ubc95\uc740 FFN \ucc28\uc6d0\uc774 5632\uc774\uace0 nk=nv=16\uc778 GQA\uc785\ub2c8\ub2e4. \u0394dF\ub294 FFN \ubaa8\ub4c8\uc758 \ud655\uc7a5\ub41c \ucc28\uc6d0\uc744 \ub098\ud0c0\ub0b4\uace0, dq\ub294 \uc911\uac04 Q \ud5e4\ub4dc \ucc28\uc6d0(\u03b4=3072)\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 AugQ\uc640 AugF\uc758 \ud06c\uae30\ub97c \ub2ec\ub9ac\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "2 DiffQKV Attention"}, {"content": "| Model Config ($n_k^h = n_v^h = 16$) | Overall | Commonsense & Comprehension | Continued | LM |\n|---|---|---|---|---|\n| **AugQ** | **Overall** | **Commonsense & Comprehension** | **Continued** | **LM** |\n| -75% K Heads | Hella. | ObQA | Wino. | ARC. | PIQA | SciQ | Bool. | Logi. | LAMB. |\n| Half K Dim |  |  |  |  |  |  |  |  |  |\n| ($d_q^h = 3072$) | 52.14 | 55.1 | 39.6 | 56.3 | 35.4 | 71.9 | 85.0 | 61.4 | 27.8 | 36.8 |\n| ($n_k^h = 4$) | **52.97** | 56.0 | 38.4 | 57.7 | 37.0 | 72.5 | 83.8 | 62.5 | 30.1 | 38.8 |\n| ($d_k^h = d_v^h/2$) | **52.72** | 55.9 | 39.4 | 59.1 | 37.5 | 73.1 | 84.3 | 60.6 | 27.0 | 37.4 |\n|  | **51.74** | 54.3 | 37.4 | 57.5 | 36.3 | 72.9 | 85.5 | 60.5 | 26.3 | 35.1 |\n|  | **52.61** | 55.8 | 40.2 | 54.9 | 38.5 | 74.0 | 84.6 | 61.1 | 26.9 | 37.6 |", "caption": "Table 6: Combinations of three strategies for optimizing the self-attention architecture: augmented Q, compressing the number of K heads, and compressing K head dimension. \u2713 and \u2717 represent whether the corresponding strategy is used or not respectively. If the strategy is not used, the standard model setting is adopted (i.e. nkhsuperscriptsubscript\ud835\udc5b\ud835\udc58\u210en_{k}^{h}italic_n start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT=nvhsuperscriptsubscript\ud835\udc5b\ud835\udc63\u210en_{v}^{h}italic_n start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT=16, and Q is not augmented).", "description": "\ud45c 6\uc740 \uc790\uae30 \uc8fc\uc758 \ub124\ud2b8\uc6cc\ud06c \uad6c\uc870\ub97c \ucd5c\uc801\ud654\ud558\uae30 \uc704\ud55c \uc138 \uac00\uc9c0 \uc804\ub7b5(\uc99d\uac15\ub41c Q, K \ud5e4\ub4dc \uc218 \uc555\ucd95, K \ud5e4\ub4dc \ucc28\uc6d0 \uc555\ucd95)\uc758 \uc870\ud569\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \u2713\uc640 \u2717\ub294 \uac01 \uc804\ub7b5\uc774 \uc0ac\uc6a9\ub418\uc5c8\ub294\uc9c0 \uc5ec\ubd80\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc804\ub7b5\uc774 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\uc740 \uacbd\uc6b0 \ud45c\uc900 \ubaa8\ub378 \uc124\uc815(\uc989,  \ud835\udc5b\ud835\udc58\u210e=\ud835\udc5b\ud835\udc63\u210e=16\uc774\uace0 Q\ub294 \uc99d\uac15\ub418\uc9c0 \uc54a\uc74c)\uc774 \uc801\uc6a9\ub429\ub2c8\ub2e4.", "section": "2 DiffQKV \uc8fc\uc758"}, {"content": "| Parameter \\\\ Scale | 1.5B | 10B |\n|---|---|---|\n| Layers | 26 | 32 |\n| Hidden Dimension | 2,048 | 4,096 |\n| FFN Dimension | 6,144 | 14,336 |\n| Aug Q Dimension | 3,072 | 6,144 |\n| Attention Heads | 32 | 32 |\n| Key Heads | 4 | 4 |\n| Value Heads | 16 | 16 |\n| Peak Learning Rate | 4.0e-4 | 1.5e-4 |\n| Activation Function | SwiGLU | SwiGLU |\n| Vocabulary Size | 128,256 | 128,256 |\n| Positional Embeddings | ROPE (\u03b8=50,000) | ROPE (\u03b8=500,000) |", "caption": "Table 7: Key configurations and hyperparameters of Sigma 1.5B and 10B.", "description": "\ud45c 7\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c SIGMA \ubaa8\ub378\uc758 \ub450 \uac00\uc9c0 \ud06c\uae30, \uc989 15\uc5b5 \uac1c\uc640 100\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uac00\uc9c4 SIGMA-1.5B\uc640 SIGMA-10B\uc5d0 \ub300\ud55c \uc8fc\uc694 \uad6c\uc131 \ubc0f \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub808\uc774\uc5b4 \uc218, \uc740\ub2c9 \ucc28\uc6d0, FFN \ucc28\uc6d0, \uc99d\uac15\ub41c Q \ucc28\uc6d0, \uc5b4\ud150\uc158 \ud5e4\ub4dc \uc218, \ud0a4 \ud5e4\ub4dc \uc218, \ubc38\ub958 \ud5e4\ub4dc \uc218, \ucd5c\uace0 \ud559\uc2b5\ub960, \ud65c\uc131\ud654 \ud568\uc218, \uc5b4\ud718 \ud06c\uae30, \uc704\uce58 \uc784\ubca0\ub529\uacfc \uac19\uc740 \uc138\ubd80 \uc815\ubcf4\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 SIGMA \ubaa8\ub378\uc758 \uc544\ud0a4\ud14d\ucc98\uc640 \uc131\ub2a5\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ub3d5\uae30 \uc704\ud574 \uc790\uc138\ud55c \uc124\uc815 \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "2.4 SIGMA \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98"}, {"content": "| Prefix | Std Split | Std Combine | Std Total Cost | Sigma 1.5B Split | Sigma 1.5B Combine | Sigma 1.5B Total Cost | Relative Improvement Split | Relative Improvement Combine | Relative Improvement Total Cost |\n|---|---|---|---|---|---|---|---|---|---| \n| **Length** |  |  |  |  |  |  |  |  |  |\n| **2k** | 2.53E+6 | 1.88E+6 | 4.41E+6 | 2.50E+6 | 1.85E+6 | 4.34E+6 | 1.17% | 1.68% | 1.39% |\n| **4k** | 4.68E+6 | 1.91E+6 | 6.59E+6 | 3.49E+6 | 1.91E+6 | 5.40E+6 | 25.33% | 0.08% | 18.02% |\n| **16k** | 1.52E+7 | 1.94E+6 | 1.72E+7 | 1.12E+7 | 1.94E+6 | 1.31E+7 | 26.30% | 0.25% | 23.35% |\n| **32k** | 2.75E+7 | 1.99E+6 | 2.95E+7 | 2.00E+7 | 2.01E+6 | 2.21E+7 | 27.21% | -0.93% | 25.31% |", "caption": "Table 8: KET Results (ns) with the prefix length increase from 2k to 32k, keeping the output length as 10. Split represents the split kernel and Combine represents the combine kernel.", "description": "\ud45c 8\uc740 \uc811\ub450\uc0ac \uae38\uc774\uac00 2k\uc5d0\uc11c 32k\ub85c \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ubd84\ud560 \ucee4\ub110\uacfc \uacb0\ud569 \ucee4\ub110\uc758 \uc2e4\ud589 \uc2dc\uac04(ns)\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ucd9c\ub825 \uae38\uc774\ub294 10\uc73c\ub85c \uace0\uc815\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. 'Split'\uc740 \ubd84\ud560 \ucee4\ub110\uc744 \ub098\ud0c0\ub0b4\uace0, 'Combine'\uc740 \uacb0\ud569 \ucee4\ub110\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 SIGMA \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc774\uba70, \uc811\ub450\uc0ac \uae38\uc774 \ubcc0\ud654\uc5d0 \ub530\ub978 \uac01 \ucee4\ub110\uc758 \uc2e4\ud589 \uc2dc\uac04\uc744 \uce21\uc815\ud558\uc5ec SIGMA \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "3.3.2 \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Data Type | Sources | Size | # Tokens |\n|---|---|---|---| \n| **General System** | CCF Ranking list | 14.0 G | 3.3 B |\n|  | arXiv | 33.0 G | 5.4 B |\n| **Design Capability** | Technical blogs & Developer forums | 14.5 G | 3.2 B |\n| **Debug Capability** | Stack Overflow | 38.9 G | 7.6 B |", "caption": "Table 9: Composition of the system domain pre-training data for Sigma.", "description": "\ud45c 9\ub294 SIGMA \uc0ac\uc804 \ud6c8\ub828\uc744 \uc704\ud574 \uc0ac\uc6a9\ub41c \uc2dc\uc2a4\ud15c \ub3c4\uba54\uc778 \ub370\uc774\ud130\uc758 \uad6c\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc77c\ubc18\uc801\uc778 \uc2dc\uc2a4\ud15c \ub370\uc774\ud130, \uc124\uacc4 \ub2a5\ub825 \uad00\ub828 \ub370\uc774\ud130, \ub514\ubc84\uae45 \ub2a5\ub825 \uad00\ub828 \ub370\uc774\ud130 \ub4f1 \ub370\uc774\ud130 \uc720\ud615\ubcc4\ub85c \ub370\uc774\ud130 \uc18c\uc2a4, \ud06c\uae30, \ud1a0\ud070 \uc218\ub97c \uc790\uc138\ud788 \uc81c\uc2dc\ud558\uc5ec SIGMA \ubaa8\ub378 \ud6c8\ub828\uc5d0 \uc0ac\uc6a9\ub41c \uc2dc\uc2a4\ud15c \ub3c4\uba54\uc778 \ub370\uc774\ud130\uc758 \uaddc\ubaa8\uc640 \ub2e4\uc591\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 \uc2dc\uc2a4\ud15c \ub3c4\uba54\uc778 \uc0ac\uc804 \ud6c8\ub828 \ubc0f AIMICIUS \ubca4\uce58\ub9c8\ud06c"}, {"content": "| Model | CMD Score | Output Score | Calibration Score | Exact Match | Success Ratio | Accuracy |\n|---|---|---|---|---|---|---|\n| **GPT-3.5** | 70.0 | 36.0 | 21.0 | 6.0 | 11.0 | 13.0 |\n| **GPT-4** | 84.0 | 61.0 | 62.0 | 13.0 | 21.0 | 25.0 |\n| **Mistral-7B-S** | 80.6 | 58.7 | 62.0 | 24.9 | 19.0 | 30.7 |\n| **Mistral-7B-P-S** | 83.4 | 65.3 | 66.3 | 23.9 | 21.5 | 32.2 |\n| **Llama3-8B-S** | 86.4 | 69.1 | 64.4 | 42.0 | 32.7 | 50.7 |\n| **Llama3-8B-P-S** | 87.5 | 72.2 | 69.3 | 46.3 | 37.1 | 57.1 |\n| **Codegemma-7B-P-S** | 84.2 | 61.8 | 65.9 | 23.9 | 21.0 | 32.7 |\n| **Starcoder2-7B-P-S** | 86.5 | 66.5 | 64.9 | 31.2 | 23.4 | 38.1 |\n| **DeepSeekCoder1.5-7B-P-S** | 86.3 | 68.4 | 63.9 | 41.0 | 30.7 | 49.3 |\n| **Gemma2-9B-P-S** | 90.3 | 72.2 | 78.1 | 34.2 | 26.8 | 43.9 |\n| Sigma-System-10B | 87.5 | 80.9 | 78.0 | 57.0 | 74.0 | 74.5 |", "caption": "Table 10: Performance of different models on CMDGen NVIDIA subtask in AIMicius. The postfix of \u201c-S\u201d indicates that the model has been SFTed using a preliminary version of our SFT dataset, while \u201c-P\u201d denotes that the model has been pre-trained on our system-domain pre-training dataset.", "description": "\ud45c 10\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c AIMICIUS \ubca4\uce58\ub9c8\ud06c\uc758 CMDGen NVIDIA \ud558\uc704 \uc791\uc5c5\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \"-S\" \uc811\ubbf8\uc0ac\ub294 \ud574\ub2f9 \ubaa8\ub378\uc774 \uc5f0\uad6c\ud300\uc758 SFT(Supervised Fine-Tuning) \ub370\uc774\ud130\uc14b\uc758 \ucd08\uae30 \ubc84\uc804\uc744 \uc0ac\uc6a9\ud558\uc5ec SFT(Supervised Fine-Tuning) \ud559\uc2b5\ub418\uc5c8\uc74c\uc744 \ub098\ud0c0\ub0b4\uace0, \"-P\" \uc811\ubbf8\uc0ac\ub294 \uc2dc\uc2a4\ud15c \ub3c4\uba54\uc778 \uc0ac\uc804 \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uc73c\ub85c \uc0ac\uc804 \ud559\uc2b5\ub418\uc5c8\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub378\uc758 CMD \uc810\uc218, \ucd9c\ub825 \uc810\uc218, \ubcf4\uc815 \uc810\uc218, \uc815\ud655\ud788 \uc77c\uce58\ud558\ub294 \ube44\uc728, \uc131\uacf5 \ube44\uc728, \uc815\ud655\ub3c4 \ub4f1 \ub2e4\uc591\ud55c \uc9c0\ud45c\uac00 \ud3ec\ud568\ub418\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\uac01\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "5.2 \uc2dc\uc2a4\ud15c \ub3c4\uba54\uc778 \uc131\ub2a5"}, {"content": "| CMD | Score |\n|---|---|", "caption": "Table 11: Evaluation results on the AIMicius benchmark. The baselines include GPT-4, Gemma2-9B-Instruct, Deepseek-Coder-7b-Instruct-v1.5, Qwen2.5-Coder-7B-Instruct, and Llama3-8B-Instruct. All metrics are normalized to a scale of 0 to 100, with higher values indicating better performance. Bolded metrics represent the most critical evaluation criteria for each task. Sigma-System 10B is fine-tuned (SFT) using our proprietary dataset.", "description": "\ud45c 11\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c AIMICIUS \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\uc900 \ubaa8\ub378\ub85c GPT-4, Gemma2-9B-Instruct, Deepseek-Coder-7b-Instruct-v1.5, Qwen2.5-Coder-7B-Instruct, Llama3-8B-Instruct\ub97c \uc0ac\uc6a9\ud588\uc73c\uba70, \ubaa8\ub4e0 \uc9c0\ud45c\ub294 0\uc5d0\uc11c 100\uae4c\uc9c0 \uc815\uaddc\ud654\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ub192\uc740 \uac12\uc77c\uc218\ub85d \uc131\ub2a5\uc774 \ub354 \uc88b\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uac01 \uacfc\uc81c\uc5d0\uc11c \uac00\uc7a5 \uc911\uc694\ud55c \ud3c9\uac00 \uae30\uc900\uc740 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  Sigma-System 10B \ubaa8\ub378\uc740 \uc5f0\uad6c\ud300\uc774 \ub3c5\uc790\uc801\uc73c\ub85c \ub9cc\ub4e0 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubbf8\uc138 \uc870\uc815(Fine-tuned, SFT)\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "5. \uc131\ub2a5 \ud3c9\uac00"}, {"content": "| Output | Score |\n|---|---|", "caption": "Table 12: Comparisons with baseline models on commonsense reasoning and text understanding tasks. Differences with original reports in the baseline models are due to our unified re-evaluations for fair comparisons.", "description": "\ud45c 12\ub294 commonsense \ucd94\ub860 \ubc0f text \uc774\ud574 \uc791\uc5c5\uc5d0 \ub300\ud55c \uae30\uc900 \ubaa8\ub378\uacfc SIGMA \ubaa8\ub378\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc740 HellaSwag, OpenBookQA, WinoGrande, ARC Challenge, PIQA, SciQ, BoolQ, LogiQA \uc640 \uac19\uc740 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218, \ud3c9\uade0 \uc810\uc218, \uadf8\ub9ac\uace0 \uac01 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc138\ubd80 \uc810\uc218\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ud45c\uc758 \uacb0\uacfc\ub294 \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574 \uc800\uc790\ub4e4\uc774 \ud1b5\uc77c\ub41c \ubc29\uc2dd\uc73c\ub85c \uc7ac\ud3c9\uac00\ud55c \uacb0\uacfc\uc784\uc744 \uc8fc\ubaa9\ud574\uc57c \ud569\ub2c8\ub2e4. \uc6d0 \ub17c\ubb38\uc5d0\uc11c \ubcf4\uace0\ub41c \uacb0\uacfc\uc640\uc758 \ucc28\uc774\uc810\uc740 \uc7ac\ud3c9\uac00 \uacfc\uc815\uc5d0\uc11c \ubc1c\uc0dd\ud55c \uac83\uc73c\ub85c, \ubaa8\ub378 \uc790\uccb4\uc758 \uc131\ub2a5 \ucc28\uc774\uac00 \uc544\ub2d9\ub2c8\ub2e4.", "section": "5.3 \uc77c\ubc18 \ub3c4\uba54\uc778 \uc131\ub2a5"}, {"content": "| Calibration | Score |\n|---|---|", "caption": "Table 13: Comparisons with baseline models on general, coding, and math problem-solving tasks. Differences with original reports in the baseline models are due to our unified re-evaluations for fair comparisons.", "description": "\ud45c 13\uc740 \ub2e4\uc591\ud55c \uae30\uc900 \ubaa8\ub378\ub4e4\uacfc SIGMA \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc77c\ubc18\uc801\uc778 \ubb38\uc81c \ud574\uacb0, \ucf54\ub529, \uc218\ud559 \ubb38\uc81c \ud574\uacb0 \uc791\uc5c5\uc5d0 \ub300\ud574 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  \uae30\uc874 \uc5f0\uad6c\uc5d0\uc11c \ubcf4\uace0\ub41c \uae30\uc900 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uacfc \ucc28\uc774\uac00 \uc788\ub294\ub370, \uc774\ub294 \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \ud1b5\uc77c\ub41c \uc7ac\ud3c9\uac00\ub97c \uc218\ud589\ud588\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218\uc640 \ub2e4\uc591\ud55c \uc791\uc5c5(MMLU, MMLU-Pro, BBH, HumanEval, MBPP, MATH, GSM8K)\uc5d0\uc11c\uc758 \ud3c9\uade0 \uc131\ub2a5 \uc810\uc218\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uc791\uc5c5\uc758 \uc131\ub2a5 \uc810\uc218\ub294 \ud574\ub2f9 \uc791\uc5c5\uc758 \ud2b9\uc131\uc5d0 \ub530\ub77c \ub2e4\ub97c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.3 \uc77c\ubc18 \ub3c4\uba54\uc778 \uc131\ub2a5"}]