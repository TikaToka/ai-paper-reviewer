{"references": [{"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-12-01", "reason": "This paper introduces Grouped-Query Attention (GQA), a key technique compared against in SIGMA's efficiency analysis."}, {"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness", "publication_date": "2022-12-01", "reason": "FlashAttention is the attention mechanism implementation used in SIGMA's efficiency analysis, providing a baseline for comparison."}, {"fullname_first_author": "S\u00e9bastien Bubeck", "paper_title": "Sparks of artificial general intelligence: Early experiments with GPT-4", "publication_date": "2023-03-15", "reason": "This widely cited paper provides a comprehensive overview of the capabilities of large language models, establishing a context for SIGMA's development."}, {"fullname_first_author": "Yonatan Bisk", "paper_title": "PiQA: Reasoning about physical commonsense in natural language", "publication_date": "2020-01-01", "reason": "PiQA is a benchmark dataset used to evaluate SIGMA's performance in commonsense reasoning, a critical component of the paper's evaluation."}, {"fullname_first_author": "Rowan Zellers", "paper_title": "HellaSwag: Can a machine really finish your sentence?", "publication_date": "2019-05-01", "reason": "HellaSwag is another benchmark dataset used in the evaluation, demonstrating SIGMA's performance on commonsense reasoning tasks."}]}