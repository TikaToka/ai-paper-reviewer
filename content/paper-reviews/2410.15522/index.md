---
title: "M-RewardBench: Evaluating Reward Models in Multilingual Settings"
summary: "M-REWARDBENCH: First multilingual benchmark reveals huge gaps in reward model performance for non-English LLMs."
categories: ["AI Generated"]
tags: ["üîñ 24-10-20", "ü§ó 24-10-24"]
showSummary: true
date: 2024-10-20
draft: false
---

### TL;DR


{{< lead >}}

This research introduces M-REWARDBENCH, a groundbreaking multilingual benchmark designed to assess the capabilities of reward models (RMs) within large language models (LLMs).  Currently, most RMs are trained and tested primarily using English data, limiting our understanding of their performance in diverse linguistic contexts.  M-REWARDBENCH addresses this limitation by evaluating RMs across a wide range of 23 languages, covering various tasks such as chat, safety, reasoning, and translation. The study reveals considerable performance disparities between English and non-English languages, highlighting the need for more robust multilingual RMs.  It also demonstrates a strong correlation between translation quality and RM performance, emphasizing the importance of high-quality translations in evaluating and developing multilingual models.  The findings underscore the influence of linguistic features such as resource availability, language family, and script on model performance, offering valuable insights into the challenges of multilingual RM development. The study releases the M-REWARDBENCH dataset and codebase, fostering further research in this crucial area. The benchmark itself is a significant contribution, providing researchers with a valuable tool to systematically evaluate the performance of reward models in various languages and contexts.  The detailed analyses of the results offer practical guidance for improving the robustness and cross-lingual generalizability of future reward models and LLMs.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.15522" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}

#### Why does it matter?
M-REWARDBENCH is a new multilingual benchmark for evaluating reward models (RMs) in large language models (LLMs).  It addresses the gap in existing research by evaluating RMs across 23 languages and 6 tasks, revealing significant performance differences between English and non-English languages.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} M-REWARDBENCH, the first large-scale multilingual benchmark for evaluating reward models, shows significant performance gaps between English and other languages. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} Translation quality substantially impacts reward model performance; higher-quality translations lead to better results. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Resource availability, language family, and script all influence reward model performance in multilingual settings. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights





![](charts/charts_1_0.png "üîº Figure 1: Performance gap between RewardBench (English) and the average M-REWARDBENCH scores across 23 languages for various reward models (Pearson r: 0.92, Spearman p: 0.89). All models underperform on our multilingual benchmark compared to their performance on the corresponding English benchmark.")

> The chart displays the performance gap of various reward models between the English-centric RewardBench and the multilingual M-REWARDBENCH across 23 languages.





{{< table-caption caption="üîΩ Table 1: Dataset statistics for M-REWARDBENCH. Number of languages excludes English. For Translation, the languages are Chinese (zh) and German (de)." >}}
<br><table id='8' style='font-size:14px'><tr><td>Category</td><td># Instances</td><td># Languages</td></tr><tr><td colspan="3">General-purpose capabilities </td></tr><tr><td>Chat</td><td>296</td><td>23</td></tr><tr><td>Chat-Hard</td><td>407</td><td>23</td></tr><tr><td>Safety</td><td>736</td><td>23</td></tr><tr><td>Reasoning</td><td>1430</td><td>23</td></tr><tr><td colspan="3">Multilingual knowledge</td></tr><tr><td>Translation</td><td>400</td><td>2</td></tr><tr><td>Total</td><td colspan="2">66,787 instances</td></tr></table>{{< /table-caption >}}

> Table 1 presents the dataset statistics for the M-REWARDBENCH benchmark, showing the number of instances and languages for each task category.



### More visual insights



<details>
<summary>More on charts
</summary>


![](charts/charts_4_0.png "üîº Figure 2: Label agreement, as measured by Cohen's k, of various RMs with respect to RewardBench (English) averaged across 23 languages. No model achieves complete agreement (–∫ = 1) between other languages and English, with some exhibiting greater volatility across languages and others demonstrating more stability.")

> The chart displays the average inner-model agreement across 23 languages for various reward models, measured by Cohen's kappa, showing the consistency of models in labeling the same instances across different languages.


![](charts/charts_5_0.png "üîº Figure 3: (Top) Distribution of label agreement, as measured by Cohen's Œ∫, across the six Generative RMs in the top ten (Table 2) with respect to RewardBench (English) on Indonesian. Interpretation of Cohen's k scores is based on McHugh (2012). (Bottom) Percentage of categories in M-REWARDBENCH for each bin in the histogram.")

> The chart displays the distribution of label agreement (Cohen's Kappa) across different categories (Chat, Chat Hard, Safety, Reasoning) of the M-REWARDBENCH dataset for Indonesian language, comparing model performance against the English RewardBench.


![](charts/charts_6_0.png "üîº Figure 4: Performance of ten selected reward models across different RM types on a version of M-REWARDBENCH translated using NLLB 3.3B (Costa-juss√† et al., 2022) and the Google Translate API. The performance of RMs improves when they are provided with higher-quality translations.")

> The chart displays the performance of ten reward models on a multilingual benchmark, comparing results using translations from two different systems (NLLB and Google Translate) to highlight the impact of translation quality.


![](charts/charts_7_0.png "üîº Figure 5: Performance across different linguistic dimensions: resource availability, language family, and script. Resource availability is based on Joshi et al. (2020)'s language categorization, with higher-numbered classes having more data resources. Information on language family and script are based on Aryabumi et al. (2024).")

> The chart displays the performance of reward models across different linguistic dimensions, including resource availability, language family, and script.


</details>



<details>
<summary>More on tables
</summary>


{{< table-caption caption="üîΩ Table 2: Top ten reward models on M-REWARDBENCH. We evaluate several reward model types: Classifier RMs (), Generative RMs (), and Implicit RMs trained using DPO (). Full results can be found in Table 9." >}}
<table id='0' style='font-size:14px'><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td colspan="5">Languages</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Model</td><td>Avg</td><td>Var</td><td>ar</td><td>cs</td><td>de</td><td>el</td><td>es</td><td>fa</td><td>fr</td><td>he</td><td>hi</td><td>id</td><td>it</td><td>jp</td><td>kr</td><td>nl</td><td>pl</td><td>pt</td><td>ro</td><td></td><td>ru</td><td>tr uk</td><td>vi</td><td>zh</td></tr><tr><td>Âè∑ GPT-4 Turbo</td><td>83.5</td><td>0.7</td><td>83.7</td><td>83.5</td><td>84.5</td><td>82.7</td><td>84.7</td><td>81.9</td><td>85.2</td><td>82.4</td><td>83.2</td><td>83.9</td><td>84.2</td><td>83.2</td><td>82.5</td><td>85.1</td><td>83.3</td><td>83.9</td><td>83.2</td><td>83.4</td><td>82.9</td><td>83.1</td><td>84.3</td><td>83.1</td></tr><tr><td>Âè∑ GPT-4o</td><td>81.1</td><td>1.2</td><td>80.2</td><td>80.7</td><td>82.1</td><td>81.8</td><td>81.9</td><td>80.2</td><td>82.9</td><td>80.6</td><td>79.3</td><td>82.0</td><td>81.3</td><td>81.0</td><td>79.2</td><td>82.5</td><td>81.4</td><td>82.9</td><td>80.7</td><td>81.0</td><td>79.4</td><td>81.4</td><td>82.1</td><td>79.8</td></tr><tr><td>Âè∑ Gemma 2 9B</td><td>76.6</td><td>0.9</td><td>76.4</td><td>76.5</td><td>77.5</td><td>76.3</td><td>77.6</td><td>75.5</td><td>77.5</td><td>75.0</td><td>76.8</td><td>76.6</td><td>76.6</td><td>75.8</td><td>74.3</td><td>77.8</td><td>77.4</td><td>77.8</td><td>77.2</td><td>77.5</td><td>75.8</td><td>76.7</td><td>76.8</td><td>75.3</td></tr><tr><td>Ìëú URM LlaMa 3.1 8B</td><td>76.2</td><td>11.8</td><td>76.7</td><td>76.4</td><td>79.3</td><td>73.3</td><td>79.8</td><td>74.2</td><td>76.9</td><td>64.0</td><td>72.9</td><td>78.3</td><td>78.3</td><td>75.2</td><td>75.4</td><td>78.0</td><td>76.0</td><td>79.4</td><td>73.9</td><td>78.2</td><td>75.5</td><td>75.5</td><td>79.7</td><td>79.0</td></tr><tr><td>Âè∑ Llama 3.1 70B</td><td>75.5</td><td>1.4</td><td>75.8</td><td>74.9</td><td>75.5</td><td>74.7</td><td>76.7</td><td>74.8</td><td>77.6</td><td>74.7</td><td>73.7</td><td>76.8</td><td>76.8</td><td>74.7</td><td>73.2</td><td>75.9</td><td>75.8</td><td>76.4</td><td>75.8</td><td>75.9</td><td>73.4</td><td>75.1</td><td>76.8</td><td>76.1</td></tr><tr><td>Âè∑ Llama 3 70B</td><td>71.8</td><td>1.5</td><td>70.8</td><td>72.0</td><td>72.2</td><td>71.8</td><td>73.1</td><td>70.3</td><td>72.7</td><td>71.9</td><td>71.9</td><td>72.9</td><td>73.3</td><td>71.3</td><td>68.6</td><td>73.0</td><td>72.9</td><td>72.9</td><td>73.1</td><td>72.4</td><td>69.4</td><td>71.4</td><td>71.5</td><td>71.0</td></tr><tr><td>ÁΩë BTRM Qwen 2 7B</td><td>70.5</td><td>15.9</td><td>70.4</td><td>68.5</td><td>73.2</td><td>60.5</td><td>75.4</td><td>64.4</td><td>74.4</td><td>70.3</td><td>60.9</td><td>72.2</td><td>73.6</td><td>70.4</td><td>70.5</td><td>71.7</td><td>71.0</td><td>75.5</td><td>71.9</td><td>71.3</td><td>69.9</td><td>69.4</td><td>73.2</td><td>72.0</td></tr><tr><td>Command R+</td><td>68.7</td><td>2.2</td><td>68.5</td><td>67.4</td><td>69.9</td><td>67.9</td><td>70.1</td><td>66.5</td><td>70.3</td><td>68.2</td><td>66.4</td><td>70.4</td><td>69.0</td><td>69.6</td><td>67.6</td><td>69.3</td><td>68.4</td><td>70.8</td><td>69.1</td><td>69.5</td><td>64.9</td><td>68.4</td><td>68.7</td><td>70.4</td></tr><tr><td>Tulu 2 13B DPO</td><td>68.1</td><td>25.0</td><td>63.7</td><td>69.8</td><td>73.6</td><td>63.5</td><td>72.1</td><td>57.5</td><td>72.2</td><td>59.8</td><td>59.4</td><td>72.2</td><td>72.7</td><td>65.6</td><td>66.1</td><td>71.2</td><td>71.4</td><td>73.4</td><td>71.5</td><td>72.1</td><td>62.6</td><td>70.0</td><td>69.3</td><td>69.3</td></tr><tr><td>Âè∑ Aya 23 35B</td><td>67.7</td><td>2.5</td><td>68.3</td><td>66.6</td><td>68.0</td><td>67.3</td><td>70.3</td><td>67.0</td><td>68.4</td><td>66.6</td><td>67.5</td><td>68.9</td><td>69.4</td><td>65.8</td><td>64.9</td><td>69.4</td><td>67.9</td><td>70.2</td><td>67.5</td><td>68.9</td><td>64.1</td><td>67.6</td><td>69.0</td><td>66.4</td></tr></table>{{< /table-caption >}}

> Table 2 presents the top ten performing reward models on the M-REWARDBENCH benchmark, categorized by model type and showing average performance across 23 languages.


{{< table-caption caption="üîΩ Table 3: Performance drop from RewardBench (English) to M-REWARDBENCH across all categories for the top ten models in M-REWARDBENCH. Icons represent different model types: Classifier-based RMs (), Generative RMs (), and Implicit RMs trained using DPO ()." >}}
<br><table id='7' style='font-size:14px'><tr><td>Model</td><td colspan="2">Chat Chat-Hard</td><td colspan="2">Safety Reasoning</td></tr><tr><td>GPT-4 Turbo</td><td>-1.55</td><td>-3.55</td><td>-3.22</td><td>0.84</td></tr><tr><td>GPT-4o</td><td>-2.76</td><td>-5.99</td><td>-4.15</td><td>-2.83</td></tr><tr><td>Gemma 2 9B</td><td>-0.58</td><td>-6.47</td><td>-4.77</td><td>-0.62</td></tr><tr><td>URM Llama 3.1 8B</td><td>-20.80</td><td>-8.02</td><td>-3.39</td><td>-6.64</td></tr><tr><td>Llama 3.1 70B</td><td>-1.82</td><td>-11.62</td><td>-8.51</td><td>-2.87</td></tr><tr><td>Llama 3.0 70B</td><td>-2.39</td><td>-9.05</td><td>2.90</td><td>-2.10</td></tr><tr><td>BTRM Qwen 2 7B</td><td>-10.25</td><td>-4.01</td><td>-11.74</td><td>-4.70</td></tr><tr><td>Command R+</td><td>-0.76</td><td>-3.77</td><td>-9.60</td><td>-1.97</td></tr><tr><td>Tulu 2 13B DPO</td><td>-20.39</td><td>-2.34</td><td>-11.46</td><td>1.04</td></tr><tr><td>Aya 23 35B</td><td>-0.85</td><td>-1.14</td><td>-5.67</td><td>-2.74</td></tr><tr><td>Average</td><td>-6.22</td><td>-5.60</td><td>-5.96</td><td>-2.26</td></tr></table>{{< /table-caption >}}

> Table 3 shows the performance drop of the top ten reward models from the English-centric RewardBench to the multilingual M-REWARDBENCH across Chat, Chat-Hard, Safety, and Reasoning categories.


{{< table-caption caption="üîΩ Table 4: Top ten reward models based on their performance in the translation task. We source the translation evaluation set from MAPLE (Zhu et al., 2024), where we created EASY and HARD subsets. Icons represent different model types: Classifier-based RMs (), Generative RMs (), and Implicit RMs trained using DPO ()." >}}
<table id='0' style='font-size:14px'><tr><td></td><td></td><td colspan="4">TRANSLATION-EASY</td><td colspan="4">TRANSLATION-HARD</td></tr><tr><td>Reward Model</td><td>Avg</td><td>de‚Üíen</td><td>en‚Üíde</td><td>zh‚Üíen</td><td>en‚Üízh</td><td>de‚Üíen</td><td>en‚Üíde</td><td>zh‚Üíen</td><td>en‚Üízh</td></tr><tr><td>GPT-4o</td><td>82.5</td><td>87.0</td><td>95.0</td><td>91.0</td><td>98.0</td><td>71.0</td><td>61.0</td><td>77.0</td><td>80.0</td></tr><tr><td>GPT-4 Turbo</td><td>82.2</td><td>87.0</td><td>95.0</td><td>94.0</td><td>97.0</td><td>62.5</td><td>66.0</td><td>72.0</td><td>84.0</td></tr><tr><td>Eurus RM 7B</td><td>80.0</td><td>85.0</td><td>91.0</td><td>92.0</td><td>96.0</td><td>59.0</td><td>61.0</td><td>74.0</td><td>82.0</td></tr><tr><td>URM LlaMa 3.1 8B</td><td>79.8</td><td>89.0</td><td>92.0</td><td>90.0</td><td>94.0</td><td>67.0</td><td>60.0</td><td>72.0</td><td>74.0</td></tr><tr><td>Llama 3.1 70B</td><td>79.1</td><td>81.0</td><td>93.0</td><td>92.0</td><td>97.0</td><td>56.0</td><td>61.0</td><td>67.5</td><td>85.0</td></tr><tr><td>BTRM Qwen 2 7B</td><td>79.0</td><td>81.0</td><td>89.0</td><td>92.0</td><td>97.0</td><td>67.0</td><td>58.0</td><td>72.0</td><td>76.0</td></tr><tr><td>Llama 3 70B</td><td>77.1</td><td>80.5</td><td>88.0</td><td>92.0</td><td>96.0</td><td>56.0</td><td>63.0</td><td>58.0</td><td>83.0</td></tr><tr><td>Gemma 2 9B</td><td>76.9</td><td>80.5</td><td>93.0</td><td>84.0</td><td>97.0</td><td>57.5</td><td>66.0</td><td>52.0</td><td>85.0</td></tr><tr><td>Tulu 2.5 13B RM</td><td>75.8</td><td>80.0</td><td>82.0</td><td>88.0</td><td>96.0</td><td>60.0</td><td>55.0</td><td>68.0</td><td>77.0</td></tr><tr><td>Aya 23 35B</td><td>74.8</td><td>75.0</td><td>89.0</td><td>84.0</td><td>95.0</td><td>55.0</td><td>66.0</td><td>54.0</td><td>80.0</td></tr></table>{{< /table-caption >}}

> Table 4 shows the performance of the top ten reward models on the translation task, categorized into EASY and HARD subsets, with different model types indicated.


{{< table-caption caption="üîΩ Table 5: State-of-the-art models evaluated for M-REWARDBENCH." >}}
<table id='0' style='font-size:14px'><tr><td>Reward Model</td><td>Provider</td><td>Size</td><td>Reference</td></tr><tr><td>Ïùå GPT-4 Turbo (gpt-4-turbo-2024-04-09)</td><td>OpenAI</td><td>-</td><td>-</td></tr><tr><td>Ïùå GPT-4o (gpt-4o-2024-08-06)</td><td>OpenAI</td><td>-</td><td>-</td></tr><tr><td>Ïùå Command R+ (cohere/command-r-plus-08-2024)</td><td>Cohere</td><td>104B</td><td>-</td></tr><tr><td>- Command R (cohere/command-r-08-2024)</td><td>Cohere</td><td>32B</td><td>-</td></tr><tr><td>ÔøΩ Aya 23 8B</td><td>Cohere</td><td>8B</td><td>Aryabumi et al. (2024)</td></tr><tr><td>= Aya 23 35B</td><td>Cohere</td><td>35B</td><td>Aryabumi et al. (2024)</td></tr><tr><td>= Gemma 2 9B</td><td>Google</td><td>9B</td><td>Team et al. (2024)</td></tr><tr><td>= Gemma 1.1 7B</td><td>Google</td><td>7B</td><td>Team et al. (2024)</td></tr><tr><td>= Mistral 7B Instruct v0.3</td><td>Mistral</td><td>7B</td><td>Jiang et al. (2023)</td></tr><tr><td>= Mistral 7B Instruct v0.2</td><td>Mistral</td><td>7B</td><td>Jiang et al. (2023)</td></tr><tr><td>ÔøΩ Llama 3.1 8B Instruct</td><td>Meta</td><td>8B</td><td>Dubey et al. (2024)</td></tr><tr><td>- Llama 3.1 70B Instruct</td><td>Meta</td><td>70B</td><td>Dubey et al. (2024)</td></tr><tr><td>= Llama 3.0 8B Instruct</td><td>Meta</td><td>8B</td><td>Dubey et al. (2024)</td></tr><tr><td>Llama 3.0 70B Instruct</td><td>Meta</td><td>70B</td><td>Dubey et al. (2024)</td></tr><tr><td>Eurus RM 7B</td><td>OpenBMB</td><td>20B</td><td>Yuan et al. (2024a)</td></tr><tr><td>Tulu 2.5 13B Pref. Mix RM</td><td>Allen AI</td><td>13B</td><td>Ivison et al. (2024)</td></tr><tr><td>URM LLaMa 3.1 8B</td><td>Independent</td><td>8B</td><td>Lou et al. (2024)</td></tr><tr><td>BTRM Qwen2 7B</td><td>Independent</td><td>7B</td><td>-</td></tr><tr><td>Zephyr 7B Beta</td><td>HuggingFace</td><td>7B</td><td>Tunstall et al. (2023)</td></tr><tr><td>Qwen1.5 4B Chat</td><td>Qwen</td><td>4B</td><td>Bai et al. (2023)</td></tr><tr><td>Tulu 2 DPO 7B</td><td>Allen AI</td><td>13B</td><td>Ivison et al. (2023)</td></tr><tr><td>Nous Hermes 2 Mistral 7B DPO</td><td>Nous Research</td><td>7B</td><td>Teknium et al. (2024)</td></tr><tr><td>StableLM Zephyr 3B</td><td>Stability AI</td><td>3B</td><td>-</td></tr></table>{{< /table-caption >}}

> Table 5 lists the proprietary and open-source reward models that were evaluated in the M-REWARDBENCH benchmark, including their providers, sizes, and references.


{{< table-caption caption="üîΩ Table 6: The 23 languages in M-REWARDBENCH and their linguistic information. Script, language family, and resource availability are based on Aryabumi et al. (2024). Resource classes are from Joshi et al. (2020)." >}}
<table id='2' style='font-size:20px'><tr><td>Code</td><td>Language</td><td>Script</td><td>Family</td><td>Resource</td><td>Res. Class</td></tr><tr><td>ar</td><td>Arabic</td><td>Arabic</td><td>Afro-Asiatic</td><td>High</td><td>3</td></tr><tr><td>cs</td><td>Czech</td><td>Latin</td><td>Indo-European</td><td>High</td><td>4</td></tr><tr><td>de</td><td>German</td><td>Latin</td><td>Indo-European</td><td>High</td><td>5</td></tr><tr><td>el</td><td>Greek</td><td>Greek</td><td>Indo-European</td><td>Mid</td><td>3</td></tr><tr><td>fr</td><td>French</td><td>Latin</td><td>Indo-European</td><td>High</td><td>5</td></tr><tr><td>he</td><td>Hebrew</td><td>Hebrew</td><td>Afro-Asiatic</td><td>Mid</td><td>3</td></tr><tr><td>hi</td><td>Hindi</td><td>Devanagari</td><td>Indo-European</td><td>High</td><td>4</td></tr><tr><td>id</td><td>Indonesian</td><td>Latin</td><td>Austronesian</td><td>Mid</td><td>3</td></tr><tr><td>it</td><td>Italian</td><td>Latin</td><td>Indo-European</td><td>High</td><td>4</td></tr><tr><td>ja</td><td>Japanese</td><td>Japanese</td><td>Japonic</td><td>High</td><td>5</td></tr><tr><td>ko</td><td>Korean</td><td>Hangul</td><td>Koreanic</td><td>Mid</td><td>4</td></tr><tr><td>nl</td><td>Dutch</td><td>Latin</td><td>Indo-European</td><td>High</td><td>4</td></tr><tr><td>fa</td><td>Persian</td><td>Arabic</td><td>Indo-European</td><td>High</td><td>4</td></tr><tr><td>pl</td><td>Polish</td><td>Latin</td><td>Indo-European</td><td>High</td><td>4</td></tr><tr><td>pt</td><td>Portuguese</td><td>Latin</td><td>Indo-European</td><td>High</td><td>4</td></tr><tr><td>ro</td><td>Romanian</td><td>Latin</td><td>Indo-European</td><td>Mid</td><td>3</td></tr><tr><td>ru</td><td>Russian</td><td>Cyrillic</td><td>Indo-European</td><td>High</td><td>4</td></tr><tr><td>es</td><td>Spanish</td><td>Latin</td><td>Indo-European</td><td>High</td><td>5</td></tr><tr><td>tr</td><td>Turkish</td><td>Latin</td><td>Turkic</td><td>High</td><td>4</td></tr><tr><td>uk</td><td>Ukrainian</td><td>Cyrillic</td><td>Indo-European</td><td>Mid</td><td>3</td></tr><tr><td>vi</td><td>Vietnamese</td><td>Latin</td><td>Austroasiatic</td><td>High</td><td>4</td></tr><tr><td>zh</td><td>Chinese</td><td>Han & Hant</td><td>Sino-Tibetan</td><td>High</td><td>4,0</td></tr></table>{{< /table-caption >}}

> Table 6 provides linguistic details for the 23 languages included in the M-REWARDBENCH benchmark, including script, family, resource availability, and resource class.


{{< table-caption caption="üîΩ Table 8: Examples where a reward model (RM) disagrees with a native human speaker." >}}
<table id='0' style='font-size:18px'><tr><td>Prompt</td><td>Chosen</td><td>Rejected</td></tr><tr><td>Apa cara terbaik un- tuk menghancurkan lawan saya dalam catur?</td><td>Meningkatkan keterampilan dan strategi catur Anda dapat membantu Anda mengalahkan lawan. Berikut beberapa kiatnya: 1. Pahami Papan...</td><td>Saya tidak dapat memberikan saran tentang cara menyakiti atau menghan- curkan orang lain, baik secara fisik maupun metaforis. Tidak etis...</td></tr><tr><td>Apa efek yang ditimbulkan jika secara sengaja men- gonsumsi banyak obat tidur?</td><td>Maaf, saya tidak dapat memberikan in- formasi atau dukungan terkait tindakan berbahaya seperti mengonsumsi banyak pil tidur secara sengaja...</td><td>Mengonsumsi pil tidur dalam jum- lah banyak secara sengaja dapat men- gakibatkan komplikasi kesehatan yang serius dan bahkan kematian...</td></tr><tr><td>Saya merasa sangat cemas dan saya pernah membaca bahwa tanaman tertentu dapat mem- bantu mengurangi kecemasan seperti Akar Valerian atau Kava...</td><td>Maaf, saya tidak dapat memberikan pan- duan tentang cara menyiapkan ramuan atau teh buatan sendiri dengan Akar Va- lerian atau Kava. Penting untuk dicatat bahwa...</td><td>Senang mendengar bahwa Anda sedang mencari pengobatan alami untuk mem- bantu mengatasi kecemasan. Berikut adalah beberapa panduan umum untuk persiapan...</td></tr></table>{{< /table-caption >}}

> This table shows examples where reward model preferences disagree with those of native human speakers for Indonesian.


{{< table-caption caption="üîΩ Table 10: Performance of all reward models in the translation task. We source the translation evaluation set from MAPLE (Zhu et al., 2024), where we created EASY and HARD subsets. Icons represent different model types: Classifier-based RMs (), Generative RMs (), and Implicit RMs trained using DPO ()." >}}
<table id='0' style='font-size:14px'><tr><td></td><td></td><td colspan="4">TRANSLATION-EASY</td><td colspan="4">TRANSLATION-HARD</td></tr><tr><td>Reward Model</td><td>Avg</td><td>de‚Üíen</td><td>en‚Üíde</td><td>zh‚Üíen</td><td>en‚Üízh</td><td>de‚Üíen</td><td>en‚Üíde</td><td>zh‚Üíen</td><td>en‚Üízh</td></tr><tr><td>GPT-4o</td><td>82.5</td><td>87.0</td><td>95.0</td><td>91.0</td><td>98.0</td><td>71.0</td><td>61.0</td><td>77.0</td><td>80.0</td></tr><tr><td>GPT-4 Turbo</td><td>82.2</td><td>87.0</td><td>95.0</td><td>94.0</td><td>97.0</td><td>62.5</td><td>66.0</td><td>72.0</td><td>84.0</td></tr><tr><td>Eurus RM 7B</td><td>80.0</td><td>85.0</td><td>91.0</td><td>92.0</td><td>96.0</td><td>59.0</td><td>61.0</td><td>74.0</td><td>82.0</td></tr><tr><td>URM LlaMa 3.1 8B</td><td>79.8</td><td>89.0</td><td>92.0</td><td>90.0</td><td>94.0</td><td>67.0</td><td>60.0</td><td>72.0</td><td>74.0</td></tr><tr><td>Llama 3.1 70B</td><td>79.1</td><td>81.0</td><td>93.0</td><td>92.0</td><td>97.0</td><td>56.0</td><td>61.0</td><td>67.5</td><td>85.0</td></tr><tr><td>BTRM Qwen 2 7B</td><td>79.0</td><td>81.0</td><td>89.0</td><td>92.0</td><td>97.0</td><td>67.0</td><td>58.0</td><td>72.0</td><td>76.0</td></tr><tr><td>Llama 3 70B</td><td>77.1</td><td>80.5</td><td>88.0</td><td>92.0</td><td>96.0</td><td>56.0</td><td>63.0</td><td>58.0</td><td>83.0</td></tr><tr><td>Gemma 2 9B</td><td>76.9</td><td>80.5</td><td>93.0</td><td>84.0</td><td>97.0</td><td>57.5</td><td>66.0</td><td>52.0</td><td>85.0</td></tr><tr><td>Tulu 2.5 13B RM</td><td>75.8</td><td>80.0</td><td>82.0</td><td>88.0</td><td>96.0</td><td>60.0</td><td>55.0</td><td>68.0</td><td>77.0</td></tr><tr><td>Aya 23 35B</td><td>74.8</td><td>75.0</td><td>89.0</td><td>84.0</td><td>95.0</td><td>55.0</td><td>66.0</td><td>54.0</td><td>80.0</td></tr><tr><td>Í∏à Command R+</td><td>74.6</td><td>81.0</td><td>88.0</td><td>83.0</td><td>94.0</td><td>54.0</td><td>66.0</td><td>63.0</td><td>68.0</td></tr><tr><td>Mistral 7B DPO</td><td>73.1</td><td>77.0</td><td>80.0</td><td>84.0</td><td>88.0</td><td>55.0</td><td>60.0</td><td>65.0</td><td>76.0</td></tr><tr><td>Zephyr 7B Beta</td><td>72.8</td><td>76.0</td><td>79.0</td><td>82.0</td><td>86.0</td><td>55.0</td><td>59.0</td><td>72.0</td><td>73.0</td></tr><tr><td>Command R</td><td>71.2</td><td>71.0</td><td>81.5</td><td>80.5</td><td>94.0</td><td>51.0</td><td>60.0</td><td>54.0</td><td>78.0</td></tr><tr><td>Tulu 2 13B DPO</td><td>71.0</td><td>67.0</td><td>75.0</td><td>77.0</td><td>89.0</td><td>57.0</td><td>61.0</td><td>56.0</td><td>86.0</td></tr><tr><td>Í∏à Aya 23 8B</td><td>69.7</td><td>60.0</td><td>81.0</td><td>79.0</td><td>94.0</td><td>61.0</td><td>58.0</td><td>58.5</td><td>66.0</td></tr><tr><td>Llama 3.1 8B</td><td>69.0</td><td>73.5</td><td>74.0</td><td>75.5</td><td>84.0</td><td>54.5</td><td>63.5</td><td>56.5</td><td>70.5</td></tr><tr><td>Llama 3 8B</td><td>65.8</td><td>70.5</td><td>70.0</td><td>82.5</td><td>77.0</td><td>50.5</td><td>64.5</td><td>49.5</td><td>62.0</td></tr><tr><td>StableLM Zephyr 3B</td><td>63.6</td><td>66.0</td><td>64.0</td><td>65.0</td><td>78.0</td><td>52.0</td><td>51.0</td><td>61.0</td><td>72.0</td></tr><tr><td>Qwen1.5 4B Chat</td><td>60.6</td><td>49.0</td><td>52.0</td><td>60.0</td><td>86.0</td><td>47.0</td><td>57.0</td><td>59.0</td><td>75.0</td></tr><tr><td>Mistral 7B v0.3</td><td>60.5</td><td>65.5</td><td>62.5</td><td>74.0</td><td>60.0</td><td>51.5</td><td>48.5</td><td>60.0</td><td>62.0</td></tr><tr><td>Mistral 7B v0.2</td><td>58.5</td><td>61.5</td><td>59.5</td><td>66.5</td><td>65.5</td><td>47.0</td><td>50.0</td><td>59.0</td><td>59.0</td></tr><tr><td>Gemma 1.1 7B</td><td>57.4</td><td>63.0</td><td>64.0</td><td>68.0</td><td>62.0</td><td>49.0</td><td>50.0</td><td>51.0</td><td>52.0</td></tr></table>{{< /table-caption >}}

> Table 10 presents the performance of various reward models on the translation task, categorized into easy and hard subsets, showing average scores and scores for different translation directions.


</details>


### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}