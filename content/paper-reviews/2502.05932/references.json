{"references": [{"fullname_first_author": "Rishabh Agarwal", "paper_title": "Reincarnating reinforcement learning: Reusing prior computation to accelerate progress", "publication_date": "2022-12-01", "reason": "This paper introduces the concept of reusing prior computation to accelerate reinforcement learning, which is directly relevant to the core idea of PSEC."}, {"fullname_first_author": "Anurag Ajay", "paper_title": "Is conditional generative modeling all you need for decision making?", "publication_date": "2023-05-01", "reason": "This paper explores the use of conditional generative models for decision making, a related approach to skill composition that PSEC builds upon."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-04-07", "reason": "The D4RL benchmark is used for experimental evaluation in this paper, making it a crucial reference for understanding the context and evaluation of PSEC."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "The LoRA technique, central to PSEC's efficient skill expansion mechanism, is introduced in this paper."}, {"fullname_first_author": "Tianhe Yu", "paper_title": "MoPO: Model-based offline policy optimization", "publication_date": "2020-11-01", "reason": "MoPO is a strong baseline used for comparison in the experimental evaluation, highlighting its importance in assessing PSEC's performance."}]}