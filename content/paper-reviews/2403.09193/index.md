---
title: "Are Vision Language Models Texture or Shape Biased and Can We Steer Them?"
summary: "VLMsì˜ í…ìŠ¤ì²˜-í˜•íƒœ í¸í–¥ì„±ì„ ì–¸ì–´ë¡œ ì¡°ì ˆ ê°€ëŠ¥"
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Multimodal Learning", "Vision-Language Models", "ğŸ¢ Google DeepMind",]
showSummary: true
date: 2024-03-14
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2403.09193 {{< /keyword >}}
{{< keyword icon="writer" >}} Paul Gavrikov et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2025-01-28 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2403.09193" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2403.09193" target="_self" >}}
â†— Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/are-vision-language-models-texture-or-shape" target="_self" >}}
â†— Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

ë³¸ ì—°êµ¬ëŠ” ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLMs)ì˜ ì‹œê°ì  í¸í–¥ì„±, íŠ¹íˆ í…ìŠ¤ì²˜ì™€ í˜•íƒœì— ëŒ€í•œ í¸í–¥ì„±ì„ ë¶„ì„í•˜ê³  ì´ë¥¼ ì–¸ì–´ë¥¼ í†µí•´ ì¡°ì ˆí•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ì—ì„œëŠ” ì£¼ë¡œ ìˆœìˆ˜ ë¹„ì „ ëª¨ë¸ì˜ ì‹œê°ì  í¸í–¥ì„±ì— ì´ˆì ì„ ë§ì·„ì§€ë§Œ, ë³¸ ì—°êµ¬ëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì¸ VLMsì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì–¸ì–´ì  ìš”ì†Œê°€ ì‹œê°ì  í¸í–¥ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.  ì´ë¥¼ í†µí•´ VLMsê°€ ì¸ê°„ì˜ ì‹œê°ì  í¸í–¥ì„±ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ë©€í‹°ëª¨ë‹¬ ìœµí•©ì„ í†µí•´ ì–´ë–»ê²Œ ì‹œê°ì  í¸í–¥ì„±ì´ í˜•ì„±ë˜ëŠ”ì§€ì— ëŒ€í•œ ì´í•´ë¥¼ ë†’ì´ê³ ì í•©ë‹ˆë‹¤.

ì—°êµ¬ì§„ì€ ë‹¤ì–‘í•œ VLMsì„ ëŒ€ìƒìœ¼ë¡œ í…ìŠ¤ì²˜-í˜•íƒœ í¸í–¥ì„±ì„ ì¸¡ì •í•˜ê³ , ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ í¸í–¥ì„±ì„ ì¡°ì ˆí•˜ëŠ” ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼, VLMsëŠ” ìˆœìˆ˜ ë¹„ì „ ëª¨ë¸ë³´ë‹¤ í˜•íƒœì— ë” ì¹˜ìš°ì¹œ í¸í–¥ì„±ì„ ê°€ì§€ë©°, ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ í¸í–¥ì„±ì„ ì–´ëŠ ì •ë„ ì¡°ì ˆí•  ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ì¸ê°„ì˜ ì‹œê°ì  í¸í–¥ì„±ê³¼ëŠ” ì—¬ì „íˆ ì°¨ì´ê°€ ìˆìŒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” VLMsì˜ ì‹œê°ì  í¸í–¥ì„±ì„ ì´í•´í•˜ê³  ì œì–´í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì‹œí•˜ë©°, ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì˜ ì‹œê°ì  í¸í–¥ì„± ì—°êµ¬ì— ì¤‘ìš”í•œ ê¸°ì—¬ë¥¼ í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} VLMsëŠ” ê¸°ì¡´ì˜ ìˆœìˆ˜ ë¹„ì „ ëª¨ë¸ë³´ë‹¤ í˜•íƒœì— ë” ì¹˜ìš°ì¹œ í¸í–¥ì„±ì„ ê°€ì§„ë‹¤. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ VLMsì˜ í…ìŠ¤ì²˜-í˜•íƒœ í¸í–¥ì„±ì„ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} ì¸ê°„ì˜ ì‹œê°ì  í¸í–¥ì„±ê³¼ VLMsì˜ í¸í–¥ì„± ì‚¬ì´ì—ëŠ” ì—¬ì „íˆ ì°¨ì´ê°€ ì¡´ì¬í•œë‹¤. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
ë³¸ ë…¼ë¬¸ì€ **ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLMs)**ì˜ **í…ìŠ¤ì²˜ ë° í˜•íƒœ í¸í–¥ì„±**ì„ ì—°êµ¬í•˜ê³  ì´ë¥¼ **ì–¸ì–´ë¥¼ í†µí•´ ì¡°ì ˆ**í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì¡°ì‚¬í•˜ì—¬, **ë‹¤ì–‘í•œ VLMsì—ì„œì˜ í¸í–¥ì„±ì„ ì¸¡ì •**í•˜ê³  **ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ í¸í–¥ì„±ì„ ì¡°ì ˆí•˜ëŠ” ë°©ë²•**ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ëŠ” **ì¸ê°„ì˜ ì‹œê°ì  í¸í–¥ì„±ê³¼ì˜ ì •ë ¬**ì„ ê°œì„ í•˜ê³  **VLMsì˜ ì‘ìš© ê°€ëŠ¥ì„±ì„ í™•ì¥**í•˜ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆìœ¼ë©°, **ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì˜ í¸í–¥ì„± ì´í•´ ë° ì œì–´**ì— ëŒ€í•œ ìƒˆë¡œìš´ ë°©í–¥ì„ ì œì‹œí•˜ëŠ” ì¤‘ìš”í•œ ì—°êµ¬ì…ë‹ˆë‹¤.  **íŠ¹íˆ, ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•œ ì‹œê°ì  í¸í–¥ì„± ì¡°ì ˆ ê°€ëŠ¥ì„±**ì€ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì˜ ìƒˆë¡œìš´ ì—°êµ¬ ë°©í–¥ì„ ì œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

------
#### Visual Insights



![](https://arxiv.org/html/2403.09193/x1.png)

> ğŸ”¼ ê·¸ë¦¼ 1ì€ ë‹¤ì–‘í•œ VLMs(Vision Language Models)ì˜ ì§ˆê°-í˜•íƒœ í¸í–¥(texture-shape bias)ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.  ë‹¨ì¼ ëª¨ë“œ ëª¨ë¸ë“¤ê³¼ ë‹¬ë¦¬ VLMsëŠ” ê°ì²´ ì¸ì‹ ì‹œ ì§ˆê°ë³´ë‹¤ëŠ” í˜•íƒœë¥¼ ë” ì„ í˜¸í•˜ì§€ë§Œ, ì¸ê°„ì˜ ìˆ˜ì¤€ì—ëŠ” ë¯¸ì¹˜ì§€ ëª»í•©ë‹ˆë‹¤.  í¥ë¯¸ë¡­ê²Œë„, VLMsì˜ ì§ˆê°-í˜•íƒœ í¸í–¥ì€ ì‹œê° ì •ë³´ë¿ë§Œ ì•„ë‹ˆë¼ ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë§Œìœ¼ë¡œë„ ì¡°ì ˆ ê°€ëŠ¥í•˜ë©°, ì´ëŠ” InternVL-Chat 1.1ì˜ ì˜ˆì‹œë¥¼ í†µí•´ ê°•ì¡°ë©ë‹ˆë‹¤.  ì´ ê·¸ë¦¼ì€ VLMsê°€ ì‹œê°ì  í¸í–¥ì„ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ê³  ì–¸ì–´ë¥¼ í†µí•´ ì¡°ì ˆí•  ìˆ˜ ìˆëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ì¤‘ìš”í•œ ì‹œê°ì  ìë£Œì…ë‹ˆë‹¤.  ê° VLMì— ëŒ€í•œ ì§ˆê° ë° í˜•íƒœ í¸í–¥ì˜ ì •ë„ê°€ ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ í‘œí˜„ë˜ì–´ ìˆê³ , InternVL-Chat 1.1ì˜ ê²½ìš° ì–¸ì–´ ì¡°ì ˆì„ í†µí•´ í˜•íƒœ í¸í–¥ì„ ì–´ë–»ê²Œ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ì¶”ê°€ ê·¸ë˜í”„ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 1: Unlike many unimodal models, vision language models (VLMs) prefer shape over texture for object recognition, but not to the same extent as humans. Further, we find that the (visual) texture/shape bias [1] can be steered through language alone, albeit not to the extent as through vision. Here we visualize the texture/shape bias of some exemplary VLMs, and highlight the steerability of InternVL-Chat 1.1 [2].
> </details>





{{< table-caption >}}
| Model                     | Description                                                                                                                                                                                  |
|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `Qwen-VL-Chat` [63]         | Adds vision capabilities to `Qwen-7B` [79]. We set a repetition penalty of 1.2 for this model.                                                                                              |
| `Qwen-VL Plus/Max` [26]     | Alibabaâ€™s proprietary larger variants of `Qwen-VL-Chat`. Access only via API.                                                                                                                   |
| `CogAgent` [64]             | A special model for interaction with graphical user interfaces (GUIs) at high-resolution.                                                                                                       |
| `CogVLM` [65]               | Adds "trainable visual expert module" in LLM layers to combine vision and language.                                                                                                              |
| `Emu2` [66]                 | The 37B model claims "strong multimodal in-context learning abilities".                                                                                                                         |
| `InstructBLIP` [23]         | Connects frozen vision encoders and LLMs through a trainable Q-Former. Uses `Vicuna` or `FLAN-T5` as LLMs.                                                                                      |
| `LLaVA v1.5` [61]           | Improvements of `LLaVA` with modifications on the image encoder, the projector, and task-specific data. Uses `Vicuna-7/13B` as LLM.                                                                  |
| `LLaVA-NeXT` [62]           | Successor of `LLaVA v1.5` supporting higher resolutions through patching, and using better SFT training data for training, claiming â€œimproved reasoning, OCR, and world knowledgeâ€ [62]. The 34B version switches from `Vicuna-7/13B` to `Nous Hermes 2 Yi 34B`. |
| `MoE-LLaVA v1.5` [67]       | Variants of `LLaVA v1.5` employing 4 sparsely activated Mixture-of-Experts (MoE), and smaller LLMs (`Qwen`, `Phi-2`, `StableLM`).                                                                     |
| `LLaVA-RLHF` [69]           | Variants of `LLaVA v1.5` aligned with Factually Augmented RLHF (Fact-RLHF) [69].                                                                                                                  |
| `UForm-Gen Chat` [68]       | A small (1.5B) model for VQA and image captioning finetuned for multimodal chat.                                                                                                                 |
| `Gemini 1.0 Pro Vision` [25] | Googleâ€™s proprietary multi-modal model based on the Gemini Pro LLM. Access only via API.                                                                                                           |
| `InternVL Chat 1.1/1.2+` [2] | An open-source effort to provide an alternative to `ViT-22B` [80]. V1.1 is based on a 6B ViT and `Vicuna-13B`, V1.2+ uses `Nous Hermes 2 Yi 34B` as LLM including additional SFT on 10x more data. |
| `GPT-4V (Preview)` [12]      | OpenAIâ€™s proprietary multi-modal model based on the GPT-4 LLM. Access only via API. Often considered to be the most powerful model.                                                                    |{{< /table-caption >}}

> ğŸ”¼ í‘œ 2ëŠ” ë‹¤ì–‘í•œ VLMsì— ëŒ€í•œ VQA ë¶„ë¥˜ ë˜ëŠ” ì´ë¯¸ì§€ ì„¤ëª… ì‘ì—…ì—ì„œ í ì¶©ëŒ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ í˜•íƒœ í¸í–¥ ë° í•´ë‹¹ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ë¯¸ì§€ ì„¤ëª… ì‘ì—…ì˜ ê²½ìš°, ë¹„ì¿ ëƒ í† í¬ë‚˜ì´ì €ê°€ ìƒì„±í•œ í† í°ì˜ í‰ê·  ìˆ˜ì™€ ë³„ë„ì˜ LLMì— ì˜í•´ íŒë‹¨ëœ ë‹¨ì¼ í´ë˜ìŠ¤ë§Œ í¬í•¨í•˜ê±°ë‚˜ ì¼ë°˜ì ì¸(í´ë˜ìŠ¤ë¥¼ ì–¸ê¸‰í•˜ì§€ ì•ŠìŒ) ì‘ë‹µì˜ ë¹„ìœ¨ì„ ì¶”ê°€ë¡œ ì œê³µí•©ë‹ˆë‹¤.  '-'ëŠ” VQAì— ëŒ€í•œ ì§€ì‹œ ì‚¬í•­ì„ ë”°ë¥´ì§€ ì•Šì•„ í‰ê°€í•  ìˆ˜ ì—†ëŠ” ëª¨ë¸ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 2: The shape bias and respective accuracy on the cue-conflict dataset for various VLMs in VQA classification or image description tasks. For the image description task, we additionally provide the average number of tokens generated by Vicunaâ€™s tokenizer and the ratio of responses that only contain a single class or are generic (do not mention any class) as judged by a separate LLM. â€œ-â€ indicates models that did not follow instructions on VQA and could, thus, not be evaluated.
> </details>





### In-depth insights


#### VLM Bias Study
ë³¸ ë…¼ë¬¸ì€ **ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLM)**ì˜ **í…ìŠ¤ì²˜ ë° í˜•íƒœ í¸í–¥ì„±(texture vs. shape bias)**ì„ ì‹¬ë„ ìˆê²Œ ì—°êµ¬í•©ë‹ˆë‹¤.  íŠ¹íˆ, ì¸ê°„ì˜ ì‹œê°ì  í¸í–¥ì„±ê³¼ VLMì˜ í¸í–¥ì„± ê°„ì˜ ì°¨ì´ì ê³¼, ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ VLMì˜ í¸í–¥ì„±ì„ ì¡°ì ˆí•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤. **í¥ë¯¸ë¡­ê²Œë„, VLMì€ ìˆœìˆ˜ ë¹„ì „ ëª¨ë¸ë³´ë‹¤ í˜•íƒœì— ë” ì¹˜ìš°ì¹œ ê²½í–¥**ì„ ë³´ì´ë©°, ì´ëŠ” **í…ìŠ¤íŠ¸ê°€ ì‹œê°ì  í¸í–¥ì„±ì„ ì¡°ì ˆí•˜ëŠ” ì—­í• **ì„ í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  **ì‹¤í—˜ ê²°ê³¼ëŠ” ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ í˜•íƒœ í¸í–¥ì„±ì„ ì¡°ì ˆí•  ìˆ˜ ìˆìŒ**ì„ ë³´ì—¬ì£¼ì§€ë§Œ, ì¸ê°„ì˜ ê°•ë ¥í•œ í˜•íƒœ í¸í–¥ì„±ì—ëŠ” ë¯¸ì¹˜ì§€ ëª»í•¨ì„ í™•ì¸í•©ë‹ˆë‹¤.  **ë‹¤ì–‘í•œ VLMê³¼ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•ì„ í™œìš©**í•˜ì—¬ ì´ëŸ¬í•œ í¸í–¥ì„±ì˜ ì›ì¸ê³¼ ì¡°ì ˆ ê°€ëŠ¥ì„±ì„ íƒêµ¬í•˜ëŠ” í¬ê´„ì ì¸ ì—°êµ¬ì…ë‹ˆë‹¤.  **ê²°ê³¼ëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì—ì„œ ì‹œê°ì  í¸í–¥ì„±ì´ ë‹¨ìˆœíˆ ë¹„ì „ ì¸ì½”ë”ì—ì„œ ìƒì†ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì–¸ì–´ì™€ì˜ ìƒí˜¸ì‘ìš©ì— ì˜í•´ ì¡°ì ˆë¨**ì„ ê°•ì¡°í•©ë‹ˆë‹¤.

#### Prompt Steering
ë³¸ ë…¼ë¬¸ì—ì„œ ë‹¤ë£¬ í”„ë¡¬í”„íŠ¸ ìŠ¤í‹°ì–´ë§ì€ **ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLM)ì˜ ì‹œê°ì  í¸í–¥(texture/shape bias)ì„ ì–¸ì–´ì  ê°œì…ì„ í†µí•´ ì¡°ì ˆí•˜ëŠ” ê¸°ë²•**ì…ë‹ˆë‹¤.  ë‹¨ìˆœíˆ ì‹œê° ì •ë³´ë§Œìœ¼ë¡œ í¸í–¥ì„ ì¡°ì •í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, **í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì‹œê°ì  í•´ì„ ë°©ì‹ì„ ë°”ê¿€ ìˆ˜ ìˆë‹¤ëŠ” ì **ì´ í•µì‹¬ì…ë‹ˆë‹¤.  ì‹¤í—˜ ê²°ê³¼ëŠ” í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ shape biasê°€ ìµœëŒ€ 72%ê¹Œì§€ ì¦ê°€í•˜ê³ , texture biasëŠ” 49%ê¹Œì§€ ê°ì†Œí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  **ì¸ê°„ì˜ ì‹œê°ì  í¸í–¥(shape bias 96%)ì„ ì™„ì „íˆ ì¬í˜„í•˜ì§€ëŠ” ëª»í•˜ì§€ë§Œ**,  í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•´ VLMì˜ ì‹œê°ì  íŒë‹¨ì— ìƒë‹¹í•œ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  **ìë™í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„± ê¸°ë²•**ì„ í†µí•´ ë”ìš± íš¨ê³¼ì ì¸ ìŠ¤í‹°ì–´ë§ ê°€ëŠ¥ì„±ì„ í™•ì¸í•œ ê²ƒë„ ì¤‘ìš”í•œ ê²°ê³¼ì…ë‹ˆë‹¤.  ì´ëŸ¬í•œ ì—°êµ¬ëŠ” **í–¥í›„ VLM ê°œë°œ ë° ì‘ìš©ì— ìˆì–´ì„œ ì‹œê°ì  í¸í–¥ì„ ì œì–´í•˜ê³  ì¸ê°„ì˜ ì‹œê° ì¸ì§€ì™€ ë”ìš± ì¼ì¹˜ì‹œí‚¤ëŠ” ë° ì¤‘ìš”í•œ ì „ëµì  ì˜ë¯¸**ë¥¼ ê°–ìŠµë‹ˆë‹¤.

#### Vision's Role
ì´ ë…¼ë¬¸ì—ì„œ 'ë¹„ì „ì˜ ì—­í• 'ì— ëŒ€í•œ ì‹¬ì¸µì ì¸ ê³ ì°°ì€ ì‹œê° ì–¸ì–´ ëª¨ë¸(VLMs)ì˜ ì‹œê°ì  í¸í–¥ í˜•ì„±ì— ìˆì–´ì„œ ë¹„ì „ ì¸ì½”ë”ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.  **VLMsëŠ” ìˆœìˆ˜ ì‹œê° ëª¨ë¸ê³¼ ë‹¬ë¦¬ ì–¸ì–´ë¥¼ í†µí•´ ì‹œê°ì  ì •ë³´ì— ì ‘ê·¼**í•˜ë©°, ì´ëŸ¬í•œ ë‹¤ì¤‘ ëª¨ë‹¬ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ì‹œê°ì  í¸í–¥ì´ ë‹¨ìˆœíˆ ë¹„ì „ ì¸ì½”ë”ì—ì„œ ìƒì†ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì–¸ì–´ì™€ì˜ ìƒí˜¸ì‘ìš©ì„ í†µí•´ ì¡°ì ˆëœë‹¤ëŠ” ì ì´ í•µì‹¬ì…ë‹ˆë‹¤.  **í…ìŠ¤ì²˜ ëŒ€ë¹„ í˜•íƒœ í¸í–¥**ê³¼ ê°™ì€ ì‹œê°ì  í¸í–¥ì€ ë¹„ì „ ì¸ì½”ë”ì˜ ì˜í–¥ì„ ë°›ì§€ë§Œ, ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ì¡°ì • ê°€ëŠ¥í•˜ë‹¤ëŠ” ì‹¤í—˜ì  ì¦ê±°ê°€ ì œì‹œë©ë‹ˆë‹¤.  **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•´ í˜•íƒœ í¸í–¥ì„ 49%ì—ì„œ 72%ê¹Œì§€ ì¡°ì ˆ**í•  ìˆ˜ ìˆë‹¤ëŠ” ê²°ê³¼ëŠ” ì–¸ì–´ê°€ ì‹œê°ì  í¸í–¥ì„ ì¡°ì •í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  **ë¹„ì „ ì¸ì½”ë”ì˜ ì—­í• ì„ ë¶„ë¦¬ ë¶„ì„**í•˜ëŠ” ì‹¤í—˜ì„ í†µí•´ ì‹œê°ì  í¸í–¥ì´ ë‹¨ìˆœíˆ ìƒì†ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë©°, ë‹¤ì¤‘ ëª¨ë‹¬ ìœµí•©ì„ í†µí•´ ëª¨ë¸ì˜ ì‹œê°ì  í¸í–¥ì´ ë³€í™”ë  ìˆ˜ ìˆìŒì„ í™•ì¸í•©ë‹ˆë‹¤.  **ì¸ê°„ì˜ ì‹œê°ì  í¸í–¥ê³¼ì˜ ë¹„êµ**ë¥¼ í†µí•´ VLMsì˜ ì‹œê°ì  í¸í–¥ì´ ì¸ê°„ì˜ í¸í–¥ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ì§€ ì•ŠìŒì„ ë³´ì—¬ì£¼ì§€ë§Œ, ì–¸ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•œ ì¡°ì • ê°€ëŠ¥ì„±ì„ ì œì‹œí•˜ë©°, **í–¥í›„ ì¸ê°„ ì‹œê°ê³¼ì˜ ì¡°í™”ë¥¼ ìœ„í•œ ì—°êµ¬ ë°©í–¥**ì„ ì œì‹œí•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ë¹„ì „ì€ ë‹¨ìˆœíˆ ì •ë³´ ì…ë ¥ì˜ ìˆ˜ë‹¨ì„ ë„˜ì–´, ì–¸ì–´ì™€ ìƒí˜¸ì‘ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì‹œê°ì  ì´í•´ì™€ íŒë‹¨ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•µì‹¬ ìš”ì†Œì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### RLHF's Impact
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” RLHF(Reinforcement Learning from Human Feedback)ê°€ ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLMs)ì˜ í…ìŠ¤ì²˜-ì‰ì´í”„ í¸í–¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.  **RLHFëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•˜ì§€ë§Œ, í…ìŠ¤ì²˜-ì‰ì´í”„ í¸í–¥ ìì²´ë¥¼ ì¸ê°„ ìˆ˜ì¤€ìœ¼ë¡œ ì¤„ì´ì§€ëŠ” ëª»í•œë‹¤ëŠ” ì ì„ ê°•ì¡°**í•©ë‹ˆë‹¤.  ì¼ë¶€ ëª¨ë¸ì—ì„œ RLHF ì ìš© í›„ í¸í–¥ì´ ë³€í™”í•˜ê±°ë‚˜, ë³€í™”í•˜ì§€ ì•Šê±°ë‚˜, ì‹¬ì§€ì–´ ì˜¤íˆë ¤ ê°ì†Œí•˜ëŠ” ë“± ì¼ê´€ëœ ê²½í–¥ì´ ì—†ë‹¤ëŠ” ì ì„ ë³´ì—¬ì£¼ë©°, **RLHFê°€ ê¼­ ì‹œê°ì  ì„ í˜¸ë„ë¥¼ ì¸ê°„ ìˆ˜ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ëŠ” ë³´ì¥ì€ ì—†ìŒ**ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  ë”°ë¼ì„œ RLHFëŠ” VLMsì˜ ì„±ëŠ¥ ê°œì„ ì—ëŠ” íš¨ê³¼ì ì´ì§€ë§Œ, ì‹œê°ì  í¸í–¥ ì¡°ì •ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì—°êµ¬ì™€ ì „ëµì´ í•„ìš”í•¨ì„ ê°•ì¡°í•©ë‹ˆë‹¤.  **ê²°ë¡ ì ìœ¼ë¡œ, RLHFëŠ” VLMsì˜ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ë˜ì§€ë§Œ, ì¸ê°„ ìˆ˜ì¤€ì˜ ì‹œê°ì  í¸í–¥ ì •ë ¬ì—ëŠ” ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ëŠ” ì ì´ ì¤‘ìš”í•œ ì‹œì‚¬ì **ì…ë‹ˆë‹¤.  ì¶”ê°€ ì—°êµ¬ë¥¼ í†µí•´ RLHFì˜ í•œê³„ì™€ ê°œì„  ë°©í–¥ì„ ëª¨ìƒ‰í•˜ì—¬, ë”ìš± ì¸ê°„ ì¹œí™”ì ì¸ VLMs ê°œë°œì— ê¸°ì—¬í•´ì•¼ í•©ë‹ˆë‹¤.

#### Future Work
ë³¸ ë…¼ë¬¸ì€ ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLMs)ì˜ ì§ˆê°-í˜•íƒœ í¸í–¥ì„±ì„ ì¡°ì‚¬í•˜ê³ , ì´ë¥¼ ì–¸ì–´ë¥¼ í†µí•´ ì œì–´í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤.  **ë¯¸ë˜ ì—°êµ¬ëŠ” ëª‡ ê°€ì§€ ì¤‘ìš”í•œ ë°©í–¥**ìœ¼ë¡œ ë‚˜ì•„ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì²«ì§¸, **ë”ìš± ë‹¤ì–‘í•œ VLMsê³¼ ë‹¤ì–‘í•œ ë¹„ì „ í¸í–¥ì„±**ì„ ì¡°ì‚¬í•˜ì—¬, ë³¸ ë…¼ë¬¸ì˜ ë°œê²¬ì´ ì–¼ë§ˆë‚˜ ì¼ë°˜ì ì¸ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ë‘˜ì§¸, **ì œì–´ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë“¤ì„ ë”ìš± ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„**í•˜ì—¬ ì–¸ì–´ì  ì¡°ì‘ì„ í†µí•œ í¸í–¥ì„± ì œì–´ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ëª…í™•íˆ ë°í˜€ì•¼ í•©ë‹ˆë‹¤.  ì…‹ì§¸, **ë³¸ ì—°êµ¬ì—ì„œ ì‚¬ìš©ëœ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•ì„ ê°œì„ **í•˜ì—¬, ì¸ê°„ ìˆ˜ì¤€ì˜ í˜•íƒœ í¸í–¥ì„±ì— ë„ë‹¬í•  ìˆ˜ ìˆëŠ”ì§€ íƒìƒ‰í•´ì•¼ í•©ë‹ˆë‹¤.  ë§ˆì§€ë§‰ìœ¼ë¡œ, **ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ë¹„ì „ í¸í–¥ì„±ì—ë„ ë³¸ ì—°êµ¬ì˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì ìš©**í•˜ì—¬, ë³´ë‹¤ í¬ê´„ì ì¸ ë¹„ì „ ì–¸ì–´ ëª¨ë¸ì˜ í¸í–¥ì„± ì´í•´ ë° ì œì–´ ë°©ì•ˆì„ ëª¨ìƒ‰í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¶”ê°€ ì—°êµ¬ëŠ” VLMsì˜ ì‹ ë¢°ì„± ë° ê³µì •ì„± í–¥ìƒì— ê¸°ì—¬í•  ê²ƒì…ë‹ˆë‹¤.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2403.09193/x2.png)

> ğŸ”¼ ê·¸ë¦¼ 2ëŠ” ë‹¤ì–‘í•œ VLMs(Vision Language Models)ì˜ ì§ˆê°-í˜•íƒœ í¸í–¥(texture-shape bias)ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. VQA(Visual Question Answering) ì‘ì—…ê³¼ ì´ë¯¸ì§€ ì„¤ëª… ì‘ì—…ì—ì„œ VLMsì˜ í˜•íƒœ í¸í–¥ì„ ì¸¡ì •í•˜ì—¬ ë¹„êµ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ VLMsëŠ” ì•½ê°„ í˜•íƒœ ì¤‘ì‹¬ì ì´ì§€ë§Œ, ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ë¼ëŠ” ìš”ì²­ì„ ë°›ì•˜ì„ ë•Œ VQAì™€ ë¹„êµí•˜ì—¬ ì°¨ì´ë¥¼ ë³´ì´ëŠ” ëª¨ë¸ë“¤ë„ ìˆìŠµë‹ˆë‹¤.  ë¹„êµë¥¼ ìœ„í•´ ì´ˆê¸° ì§ˆê°-í˜•íƒœ í¸í–¥ ì—°êµ¬ì—ì„œ ì‚¬ìš©ëœ ResNet-50 [59], ì œë¡œìƒ· ë¶„ë¥˜(CLIP ViT-L/14 [11]), ê·¸ë¦¬ê³  10ëª…ì˜ ì‚¬ëŒì˜ í‰ê·  ê²°ê³¼ë„ í•¨ê»˜ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ VLMsê°€ í˜•íƒœ ì •ë³´ì— ë” ì˜ì¡´í•˜ëŠ” ê²½í–¥ì„ ë³´ì´ì§€ë§Œ, ì‚¬ëŒì˜ í˜•íƒœ í¸í–¥(96%)ì—ëŠ” ë¯¸ì¹˜ì§€ ëª»í•˜ë©°, ì‘ì—… ìœ í˜•ì— ë”°ë¼ í˜•íƒœ í¸í–¥ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 2: Most VLMs are slightly shape-biased but some models show differences when asked to describe an image compared to VQA. We measure the shape bias on the cue-conflict dataset [1]. For reference, we also provide measurements on ResNet-50 [59] from the initial shape bias study [1], zero-shot classification (CLIP ViT-L/14 [11]), and a human average (over 10 subjects [1]).
> </details>



![](https://arxiv.org/html/2403.09193/x3.png)

> ğŸ”¼ ê·¸ë¦¼ (a)ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì‹ ë¢°ë„ì— ë”°ë¥¸ í…ìŠ¤ì²˜ ë° ëª¨ì–‘ í† í°ì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•œ í…ìŠ¤ì²˜ì™€ ëª¨ì–‘ í† í°ì˜ ì‹ ë¢°ë„ ë¶„í¬, ì •ë‹µ ì˜ˆì¸¡(ì¦‰, ì˜ˆì¸¡ ë ˆì´ë¸”ì´ ëª¨ì–‘ ë˜ëŠ” ì§ˆê°ê³¼ ì¼ì¹˜)ì—ì„œì˜ ì‹ ë¢°ë„ ë¶„í¬, ì‹ ë¢°ë„ ì„ê³„ê°’ì„ ë³€ê²½í–ˆì„ ë•Œ ëª¨ì–‘ í¸í–¥ì˜ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ì–´ë–»ê²Œ ê²°ì •ì„ ë‚´ë¦¬ëŠ”ì§€, ê·¸ë¦¬ê³  ì‹ ë¢°ë„ê°€ ê²°ì •ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ, ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ë‹µë³€ì€ ëª¨ì–‘ ë‹¨ì„œì—ë§Œ í•´ë‹¹ëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (a)
> </details>



![](https://arxiv.org/html/2403.09193/x4.png)

> ğŸ”¼ ê·¸ë¦¼ (b)ëŠ” ì •í™•í•œ ì˜ˆì¸¡(ì¦‰, ì˜ˆì¸¡ ë ˆì´ë¸”ì´ ëª¨ì–‘ ë˜ëŠ” ì§ˆê°ê³¼ ì¼ì¹˜)ì— ëŒ€í•œ ì‹ ë¢°ë„ ë¶„í¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ëª¨ë¸ì´ ì‘ë‹µì— ì‚¬ìš©í•œ ë‹¨ì„œì— ëŒ€í•œ ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„ë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ íŠ¹ì§•ì…ë‹ˆë‹¤. ì´ëŠ” ë‹¤ë¥¸ ë‹¨ì„œì—ì„œ ë‚˜ì˜¨ ì •ë³´ê°€ ì‹¤ì œë¡œ ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì†ì‹¤ëœë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  ë‘ ë²ˆì§¸(ìƒë‹¹íˆ ì‹ ë¢°ë„ê°€ ë‚®ì€) ì˜ˆì¸¡ í† í°ì€ ì¢…ì¢…(70.7%) ìƒë°˜ë˜ëŠ” ë‹¨ì„œì— ì†í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  ìƒìœ„ 2ê°œ ìŒ ì¤‘ 17.7%ë§Œì´ ëª¨ì–‘ê³¼ ì§ˆê° ëª¨ë‘ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. í‰ê· ì ìœ¼ë¡œ ëª¨ë¸ì€ ì§ˆê° ê²°ì •ì— ëŒ€í•´ ì•½ê°„ ëœ í™•ì‹ í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤(p=0.87 ëŒ€ 0.92). ì‹ ë¢°ë„ ì„ê³„ê°’ ì´ìƒì˜ ì‘ë‹µë§Œ ë¶„ì„í•  ë•Œ ì´ëŠ” ë”ìš± ë¶„ëª…í•´ì§‘ë‹ˆë‹¤.  p=0.5ê¹Œì§€ëŠ” ì¸¡ì •ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì§€ë§Œ, ê·¸ ì´í›„ì—ëŠ” ì§ˆê° ê²°ì •ì˜ ë¹„ìœ¨ì´ ê°ì†Œí•˜ì§€ ì•Šê³  ëª¨ì–‘ ê²°ì •ì˜ ë¹„ìœ¨ì´ ì¦ê°€í•˜ì—¬ ëª¨ì–‘ í¸í–¥ì´ ì»¤ì§‘ë‹ˆë‹¤.  p=0.9ë¥¼ ë„˜ì–´ì„œë©´ ëª¨ë¸ì€ ëª¨ì–‘ ë‹¨ì„œë§Œì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ê°€ ë‹¤ë¥¸ ëª¨ë¸ì—ë„ ì¼ë°˜í™”ë˜ëŠ”ì§€ëŠ” ë¶ˆë¶„ëª…í•˜ì§€ë§Œ, LLaVAëŠ” í˜„ì¬ ë§ì€ í›„ì† ëª¨ë¸ì˜ ê¸°ë³¸ì´ ë˜ë¯€ë¡œ ì´ëŸ¬í•œ í¸í–¥ì„ ê·¸ëŒ€ë¡œ ì´ì–´ë°›ì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (b)
> </details>



![](https://arxiv.org/html/2403.09193/x5.png)

> ğŸ”¼ ê·¸ë¦¼ 3ì€ VLMsì´ í•œ ê°€ì§€ íŠ¹ì • ë‹¨ì„œë¥¼ ì„ í˜¸í•˜ê³  ë‹¤ë¥¸ ë‹¨ì„œë¥¼ ë¬´ì‹œí•˜ëŠ” í…ŒìŠ¤íŠ¸ì—ì„œ ë†’ì€ ì‹ ë¢°ë„ì˜ ë‹µë³€ì„ ì œê³µí•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‹ ë¢°ë„ëŠ” ë‹¨ì„œì— ë”°ë¼ ì•½ê°„ ë‹¤ë¥´ì§€ë§Œ, ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ë¥¼ ê°€ì§„ ë‹µë³€ì€ ì „ì ìœ¼ë¡œ ëª¨ì–‘ ë‹¨ì„œì— ì†í•©ë‹ˆë‹¤. (a) ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•œ ëª¨ì–‘ ë° ì§ˆê° í† í°ì˜ ì‹ ë¢°ë„ ë¶„í¬, (b) ì •í™•í•œ ì˜ˆì¸¡(ì¦‰, ì˜ˆì¸¡ëœ ë ˆì´ë¸”ì´ ëª¨ì–‘ ë˜ëŠ” ì§ˆê°ê³¼ ì¼ì¹˜í•˜ëŠ” ê²½ìš°)ì˜ ì‹ ë¢°ë„ ë¶„í¬, (c) ì‹ ë¢°ë„ë¥¼ í•„í„°ë§í•  ë•Œ ëª¨ì–‘ í¸í–¥ì˜ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. LLaVA-NeXT 7Bì˜ VQA ì‘ì—…ì— ëŒ€í•´ ì¸¡ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (c)
> </details>



![](https://arxiv.org/html/2403.09193/x6.png)

> ğŸ”¼ ê·¸ë¦¼ 3ì€ ì‹œê° ì–¸ì–´ ëª¨ë¸(VLMs)ì´ ë†’ì€ ì‹ ë¢°ë„ë¡œ ë‹µë³€ì„ ì œì‹œí•˜ë©° íŠ¹ì • ë‹¨ì„œë¥¼ ì„ í˜¸í•˜ê³  ë‹¤ë¥¸ ë‹¨ì„œë¥¼ ë¬´ì‹œí•˜ëŠ” ê²½í–¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‹ ë¢°ë„ëŠ” ë‹¨ì„œì— ë”°ë¼ ì•½ê°„ì”© ë‹¤ë¥´ì§€ë§Œ, ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ë¥¼ ê°€ì§„ ë‹µë³€ì€ ëª¨ì–‘ ë‹¨ì„œì—ë§Œ êµ­í•œë©ë‹ˆë‹¤. (a)ëŠ” ëª¨ë“  í‘œë³¸ì— ëŒ€í•œ ëª¨ì–‘ ë° ì§ˆê° í† í°ì˜ ì‹ ë¢°ë„ ë¶„í¬ë¥¼, (b)ëŠ” ì˜¬ë°”ë¥¸ ì˜ˆì¸¡(ì¦‰, ì˜ˆì¸¡ ë ˆì´ë¸”ì´ ëª¨ì–‘ ë˜ëŠ” ì§ˆê°ê³¼ ì¼ì¹˜)ì— ëŒ€í•œ ì‹ ë¢°ë„ ë¶„í¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. (c)ëŠ” ì‹ ë¢°ë„ë¥¼ í•„í„°ë§í•  ê²½ìš° ëª¨ì–‘ í¸í–¥ì˜ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. LLaVA-NeXT 7B ëª¨ë¸ì˜ VQA ì‘ì—…ì„ ê¸°ì¤€ìœ¼ë¡œ ì¸¡ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 3: VLMs provide high-confidence answers in our test that prefer one specific cue and ignore the other. Confidence only slightly differs by cue, but answers with the highest confidence exclusively belong to shape decisions. (a) Confidence distribution of shape and texture tokens for all samples, (b) Confidence distribution in correct predictions (i.e., the predicted label matches either shape or texture), (c) Evolution of shape bias if filtering for confidence. Measured on LLaVA-NeXT 7B for the VQA task.
> </details>



![](https://arxiv.org/html/2403.09193/x7.png)

> ğŸ”¼ ê·¸ë¦¼ 4ëŠ” ë‹¤ì–‘í•œ ë¹„ì „ ì–¸ì–´ ëª¨ë¸(VLM), ì´ë¯¸ì§€ë„· ê¸°ë°˜ ëª¨ë¸, ì´ë¯¸ì§€ë„· ë¯¸ì„¸ ì¡°ì • ë° ì œë¡œìƒ· ë¶„ë¥˜ë¥¼ ê±°ì¹œ VLM ì¸ì½”ë”, ê·¸ë¦¬ê³  10ëª…ì˜ ì‚¬ëŒì˜ ì‘ë‹µì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ê° ëª¨ë¸ì˜ ì‘ë‹µì€ VQA ì‘ì—…ì—ì„œ ìˆ˜í–‰ë˜ì—ˆìœ¼ë©°, ê·¸ë¦¼ì—ì„œ 'ì˜¤ë¥˜ ì¼ê´€ì„±'(Error Consistency)ì´ë¼ëŠ” ì§€í‘œë¥¼ í†µí•´ ëª¨ë¸ ê°„ ì˜¤ë¥˜ì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜ëŠ” í˜•íƒœ í´ë˜ìŠ¤ì— ì†í•˜ì§€ ì•ŠëŠ” ëª¨ë“  ì‘ë‹µì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ VLMì´ ì´ë¯¸ì§€ë„· ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ì‚¬ëŒê³¼ ìœ ì‚¬í•œ ì˜¤ë¥˜ë¥¼ ë²”í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ VLMì˜ ì˜¤ë¥˜ëŠ” ê·¸ë“¤ì˜ ë¹„ì „ ì¸ì½”ë”ì™€ ìœ ì‚¬ì„±ì„ ê³µìœ í•¨ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 4: VLMs make similar errors on the cue-conflict datasets and share similarities with their vision encoders. In terms of errors, VLMs are also more similar to humans than ImageNet-trained/finetuned models. We measure the pair-wise error consistency [71] between predictions. For this analysis, an error is any answer that does not belong to the shape class (analogous to [29]). Shown responses belong to LLM-based VLMs (under the VQA task), other selected models including ImageNet models, (some) VLM encoders under ImageNet-finetuning and zero-shot classification, and ten human subjects.
> </details>



![](https://arxiv.org/html/2403.09193/x8.png)

> ğŸ”¼ ê·¸ë¦¼ 5ëŠ” ì´ë¯¸ì§€ ì „ì²˜ë¦¬ê°€ ì§ˆê°-í˜•íƒœ í¸í–¥ì— í° ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì™¼ìª½ íŒ¨ë„ì€ ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ê³  íŒ¨ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ë©´ ì§ˆê° ì •ë³´ëŠ” ìœ ì§€ë˜ì§€ë§Œ í˜•íƒœ ì •ë³´ê°€ ì†ì‹¤ë˜ì–´ ì§ˆê° í¸í–¥ì´ ê°•í•´ì§ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì˜¤ë¥¸ìª½ íŒ¨ë„ì€ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆë¥¼ ì¦ê°€ì‹œí‚¤ë©´ í˜•íƒœ ì •ë³´ëŠ” ìœ ì§€ë˜ì§€ë§Œ ì§ˆê° ì •ë³´ê°€ ì†ì‹¤ë˜ì–´ í˜•íƒœ í¸í–¥ì´ ê°•í•´ì§ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ ë‹¤ì–‘í•œ íŒ¨ì¹˜ í¬ê¸°ì™€ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ê°•ë„ì— ë”°ë¥¸ ì§ˆê°-í˜•íƒœ í¸í–¥ì˜ ë³€í™”ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ê·¸ë˜í”„ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 5: Image preprocessing can strongly steer texture/shape bias. Left: Shuffling image patches with decreasing patch size results in a strong texture bias, Right: Increasing Gaussian noise introduces a strong shape bias.
> </details>



![](https://arxiv.org/html/2403.09193/x9.png)

> ğŸ”¼ ê·¸ë¦¼ 6ì€ í”„ë¡¬í”„íŠ¸ê°€ ì§ˆê°/í˜•íƒœ í¸í–¥ì„ ì–´ëŠ ì •ë„ ì¡°ì •í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì—¬ëŸ¬ ëª¨ë¸ì—ì„œ ë™ì¼í•œ ì§ˆê°/í˜•íƒœ í¸í–¥ ì§€ì‹œë¬¸ì„ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì¼ë¶€ ê²°ì •(ì£¼ë¡œ ì§ˆê° ìª½ìœ¼ë¡œ)ì„ ì´ë¯¸ ë³€ê²½í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. InternVL 1.1 ë° LLaVA-NeXT 7Bì˜ ê²½ìš° ë™ì˜ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆê°/í˜•íƒœì— ëŒ€í•œ ì´í•´ë„ë¥¼ ì¶”ê°€ë¡œ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, LLMì„ ì‚¬ìš©í•˜ì—¬ ì–´ëŠ ë°©í–¥ìœ¼ë¡œë“  ìµœì í™”í•  íŠ¹ì • í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 6: Prompts can steer the texture/shape bias to some extent. We test the same texture/shape-biased instructions on multiple models, showing that these can already shift some decisions (usually in favor of texture). For InternVL 1.1 and LLaVA-NeXT 7B we additionally test the understanding of texture/shape by using synonyms. Furthermore, we use an LLM to automatically search for specific prompts to optimize in either direction.
> </details>



![](https://arxiv.org/html/2403.09193/x10.png)

> ğŸ”¼ ê·¸ë¦¼ 7ì€ í¸í–¥ëœ VQA í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ë™ì˜ì–´ë¥¼ ì‚¬ìš©í•œ ì„¸ë¶€ì ì¸ í˜•íƒœ í¸í–¥ ì¸¡ì • ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì´ ê·¸ë¦¼ì€ 'ì§ˆê°' ë˜ëŠ” 'í˜•íƒœ'ë¼ëŠ” ë‹¨ì–´ ëŒ€ì‹  ë™ì˜ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆê° ë˜ëŠ” í˜•íƒœì— ëŒ€í•œ ì–¸ê¸‰ ë°©ì‹ì„ ë°”ê¾¸ì—ˆì„ ë•Œ, ëª¨ë¸ì˜ í˜•íƒœ í¸í–¥ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.  ê° ë™ì˜ì–´ì— ëŒ€í•œ í˜•íƒœ í¸í–¥ì˜ ë³€í™”ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì œì‹œí•˜ë©°,  íŠ¹ì • ë™ì˜ì–´ê°€ í˜•íƒœ ë˜ëŠ” ì§ˆê° ì¤‘ ì–´ëŠ ìª½ì— ë” í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.  ë‹¤ì–‘í•œ ëª¨ë¸ì—ì„œ í˜•íƒœ í¸í–¥ì— ëŒ€í•œ ë™ì˜ì–´ ì‚¬ìš©ì˜ íš¨ê³¼ë¥¼ ë³´ì—¬ì¤Œìœ¼ë¡œì¨, ì–¸ì–´ì  ë³€í™”ë¥¼ í†µí•œ ì‹œê°ì  í¸í–¥ ì¡°ì • ê°€ëŠ¥ì„±ì„ ì‹œê°ì ìœ¼ë¡œ ì œì‹œí•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 7: Detailed shape bias measurements under synonyms for biased VQA prompts.
> </details>



![](https://arxiv.org/html/2403.09193/x11.png)

> ğŸ”¼ ë³¸ ê·¸ë¦¼ì€ ì˜¨ë„ ì¡°ì ˆì´ VQAì™€ ì´ë¯¸ì§€ ìº¡ì…˜ ì‘ì—… ëª¨ë‘ì—ì„œ í˜•íƒœ í¸í–¥ì— ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì§€ë§Œ, ë†’ì€ ìˆ˜ì¤€ì—ì„œëŠ” ì •í™•ë„ë¥¼ ë–¨ì–´ëœ¨ë¦°ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. LLaVA-NeXT 7B ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìœ¼ë©°, ì´ë¯¸ì§€ ìº¡ì…˜ ì‘ì—…ì˜ ì˜¨ë„ê°€ 0 ë˜ëŠ” 1ì¸ ê²½ìš°ë¥¼ ì œì™¸í•˜ê³ ëŠ” 3ê°œì˜ ì‹œë“œë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì˜¨ë„ê°€ ë†’ì•„ì§ì— ë”°ë¼ ëª¨ë¸ì˜ ì°½ì˜ì„±ì´ ì¦ê°€í•˜ì§€ë§Œ, ë™ì‹œì— ë¹„í˜„ì‹¤ì ì¸ ê²°ê³¼ë¥¼ ìƒì„±í•  ê°€ëŠ¥ì„±ë„ ì»¤ì§‘ë‹ˆë‹¤. ê·¸ë¦¼ì€ ì˜¨ë„ ë³€í™”ì— ë”°ë¥¸ í˜•íƒœ í¸í–¥ê³¼ ì •í™•ë„ ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ì–´, ì˜¨ë„ ì¡°ì ˆì´ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ëª…í™•í•˜ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 8: Temperature scaling has no significant effect on shape bias neither under VQA (left) nor Image Captioning (right) tasks but starts to decrease accuracy at higher levels. Experiments performed on LLaVA-NeXT 7B with 3 seeds (except Temperature = 0 and Temperature = 1 of Image Captioning where we use a single seed).
> </details>



![](https://arxiv.org/html/2403.09193/x12.png)

> ğŸ”¼ ê·¸ë¦¼ 9ëŠ” ë‹¤ì–‘í•œ VLMsì— ëŒ€í•œ ì§ˆê° ëŒ€ í˜•íƒœ í¸í–¥ì„ VQA(ì™¼ìª½) ë° ì´ë¯¸ì§€ ìë§‰ ìƒì„±(ì˜¤ë¥¸ìª½) ì‘ì—…ì— ëŒ€í•´ í´ë˜ìŠ¤ë³„ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ì ì€ íŠ¹ì • í´ë˜ìŠ¤ì˜ ì§ˆê° ë° í˜•íƒœ ê²°ì • ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚´ë©°, ëª¨ë¸ì´ ì–´ë–¤ ì¸¡ë©´ì— ë” ì¤‘ì ì„ ë‘ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤. ì™¼ìª½ íŒ¨ë„(VQA)ì—ì„œëŠ” ì§ˆê° ëŒ€ í˜•íƒœ ê²°ì • ë¹„ìœ¨ì´ ëª¨ë¸ê³¼ ì‘ì—… ê°„ì— ìƒë‹¹íˆ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ì¼ë¶€ ëª¨ë¸ì€ í˜•íƒœì— ë” ì¤‘ì ì„ ë‘ê³ , ë‹¤ë¥¸ ëª¨ë¸ì€ ì§ˆê°ì— ë” ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ì˜¤ë¥¸ìª½ íŒ¨ë„(ì´ë¯¸ì§€ ìë§‰ ìƒì„±)ì—ì„œëŠ” ëª¨ë¸ ê°„ ì¼ê´€ì„±ì´ ë” ë†’ìœ¼ë©°, ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì—ì„œ í˜•íƒœì— ëŒ€í•œ ê°•í•œ í¸í–¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì˜ ê²°ì • ê³¼ì •ì—ì„œì˜ ì°¨ì´ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ë©°, VQAì™€ ì´ë¯¸ì§€ ìë§‰ ìƒì„± ì‘ì—… ê°„ì˜ ì°¨ì´ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (a) VQA
> </details>



![](https://arxiv.org/html/2403.09193/x13.png)

> ğŸ”¼ ê·¸ë¦¼ (b)ëŠ” ë…¼ë¬¸ì˜ 4.1ì ˆ 'ì£¼ìš” ê²°ê³¼' ì„¹ì…˜ì— ìˆëŠ” ê·¸ë¦¼ìœ¼ë¡œ, ë‹¤ì–‘í•œ VLMs(Vision Language Models)ì˜ í…ìŠ¤ì²˜-ì‰ì´í”„ í¸í–¥(texture-shape bias)ì„ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‘ì—…ì—ì„œ ì¸¡ì •í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  xì¶•ì€ ì‰ì´í”„(shape) ê²°ì • ë¹„ìœ¨ì„, yì¶•ì€ í…ìŠ¤ì²˜(texture) ê²°ì • ë¹„ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê° ì ì€ íŠ¹ì • VLM ëª¨ë¸ì„ ë‚˜íƒ€ë‚´ë©°, ì ì˜ ìœ„ì¹˜ëŠ” í•´ë‹¹ ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ ìº¡ì…”ë‹í•  ë•Œ ì‰ì´í”„ ì •ë³´ì™€ í…ìŠ¤ì²˜ ì •ë³´ ì¤‘ ì–´ëŠ ìª½ì— ë” ì˜ì¡´í•˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì ì´ xì¶•ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì‰ì´í”„ì— ë” ì˜ì¡´í•˜ê³ , yì¶•ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í…ìŠ¤ì²˜ì— ë” ì˜ì¡´í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  ì´ ê·¸ë¦¼ì€ ë‹¤ì–‘í•œ VLM ëª¨ë¸ë“¤ì´ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‘ì—…ì—ì„œ ì‰ì´í”„ì— ëŒ€í•œ í¸í–¥ì„ ë³´ì´ëŠ” ì •ë„ê°€ ë‹¤ë¦„ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ ì¸ê°„ì˜ ì‰ì´í”„ í¸í–¥(96%)ê³¼ ë¹„êµí•˜ì—¬ VLM ëª¨ë¸ë“¤ì˜ í¸í–¥ ì •ë„ë¥¼ ë¹„êµ ë¶„ì„í•˜ëŠ” ë°ì—ë„ í™œìš©ë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (b) Image Captioning
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Model | VQA |  | Image Captioning |  |  |  |  |
|---|---|---|---|---|---|---|---|---|
|  | Shape | Accuracy [%] | Shape | Accuracy [%] | Avg. Tokens | Single Class Ratio [%] | Generic Ratio [%] |
|---|---|---|---|---|---|---|---|---|
| Gemini 1.0 Pro Vision [25] | 64.1 | 82.33 | 63.2 | 68.00 | 18.9 | 63.0 | 32.3 |
| GPT-4V (Preview) [12] | 47.9 | 69.75 | 53.6 | 52.67 | 44.8 | 37.2 | 60.4 |
| Qwen-VL Plus [26] | 64.8 | 82.92 | 67.9 | 65.50 | 21.9 | 59.2 | 36.0 |
| Qwen-VL Max [26] | 62.4 | 85.50 | 69.7 | 68.50 | 151.9 | 52.1 | 41.0 |
| Qwen-VL Chat [63] | - | - | 38.2 | 67.42 | 27.3 | 59.1 | 33.2 |
| InternVL Chat 1.1 [2] | 68.3 | 89.33 | 73.2 | 75.58 | 16.9 | 74.9 | 19.4 |
| InternVL Chat 1.2+ [2] | 61.1 | 90.83 | 61.3 | 82.42 | 15.8 | 80.4 | 11.4 |
| LLaVA v1.5 7B [61] | 61.4 | 80.75 | 61.4 | 76.08 | 12.1 | 73.8 | 19.2 |
| LLaVA v1.5 13B [61] | 64.1 | 80.25 | 62.7 | 75.58 | 28.9 | 65.8 | 23.8 |
| LLaVA-RLHF 7B [69] | 61.7 | 68.08 | 63.0 | 71.83 | 47.9 | 65.1 | 24.7 |
| LLaVA-RLHF 13B [69] | 63.4 | 80.42 | 62.3 | 73.25 | 38.3 | 64.7 | 27.7 |
| LLaVA-NeXT 7B [62] | 59.2 | 82.58 | 64.0 | 65.08 | 20.2 | 55.5 | 39.5 |
| LLaVA-NeXT 13B [62] | 57.2 | 83.42 | 63.5 | 65.25 | 48.8 | 52.6 | 40.9 |
| LLaVA-NeXT 34B [62] | 56.0 | 73.83 | 66.2 | 57.50 | 93.4 | 36.2 | 59.1 |
| MoE-LLaVA-StableLM [67] | 59.1 | 80.08 | 63.0 | 73.92 | 24.1 | 67.4 | 21.6 |
| MoE-LLaVA-Qwen [67] | 62.9 | 59.50 | 63.2 | 75.33 | 13.3 | 69.4 | 20.7 |
| MoE-LLaVA-Phi2 [67] | 59.6 | 82.33 | 61.1 | 75.42 | 34.9 | 67.0 | 18.6 |
| InstructBLIP Flan-T5-xl [23] | 68.2 | 79.58 | 67.1 | 81.50 | 116.7 | 57.0 | 22.3 |
| InstructBLIP Vicuna-7B [23] | 73.8 | 72.25 | 67.7 | 80.67 | 94.0 | 60.9 | 28.0 |
| Emu2-Chat [66] | 52.9 | 75.08 | 59.6 | 65.00 | 13.6 | 63.0 | 34.0 |
| CogAgent Chat [64] | - | - | 67.4 | 60.33 | 40.1 | 49.6 | 47.7 |
| CogVLM Chat [65] | - | - | 57.6 | 66.58 | 35.8 | 53.2 | 40.1 |
| UForm Gen Chat [68] | - | - | 38.8 | 64.50 | 30.2 | 59.3 | 33.0 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 3ì€ VQA(Visual Question Answering) ì‘ì—…ì—ì„œ ì‚¬ìš©ëœ ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¥¼ íƒìƒ‰í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë¹„ë¡¯í•˜ì—¬, CLIP ìŠ¤íƒ€ì¼ì˜ ì˜µì…˜ì„ ì‚¬ìš©í•œ í”„ë¡¬í”„íŠ¸, ì´ë¯¸ì§€ ë‚´ ê°ì²´ì— ëŒ€í•œ ì„¤ëª…ì„ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸, CLIP ìŠ¤íƒ€ì¼ ì˜µì…˜ê³¼ í•¨ê»˜ ê°ì²´ ì„¤ëª…ì„ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸, ê·¸ë¦¬ê³  ì§€ì‹œì‚¬í•­ ì—†ì´ ì˜µì…˜ë§Œ ì œê³µí•˜ëŠ” í”„ë¡¬í”„íŠ¸ ë“± ë‹¤ì–‘í•œ ë³€í˜•ì´ ì‹œë„ë˜ì—ˆìŠµë‹ˆë‹¤.  ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì •í™•ë„ì™€ í˜•íƒœ í¸í–¥(shape bias)ì„ ì¸¡ì •í•˜ì—¬ ë¹„êµ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.  ê²°ê³¼ì ìœ¼ë¡œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ê°€ ê°€ì¥ ì¢‹ì€ ì •í™•ë„ì™€ í˜•íƒœ í¸í–¥ì˜ ê· í˜•ì„ ì´ë£¨ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 3: Exploration of alternative VQA prompts.
> </details>

{{< table-caption >}}
| Prompt | Bias [%] | Accuracy [%] |
|---|---|---|
| "Which option best describes the image? [...]" (default) | 59.2 | 82.58 |
| Default with CLIP-style options | 59.5 | 81.92 |
| "Describe the object in the image: [...]" | 60.2 | 81.33 |
| "Describe the object in the image: [...]" |  |  |
| with CLIP-style options | 59.4 | 80.17 |
| Empty instruction (just options) | 59.5 | 81.33 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ì´ë¯¸ì§€ ìº¡ì…˜ ì‘ì—…ì„ ìœ„í•œ ì—¬ëŸ¬ ê°€ì§€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‹œí—˜í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì™¸ì—ë„ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ì ì¸ ì„¤ëª…ì„ ë„£ê±°ë‚˜,  ì§§ê²Œ ì‘ë‹µí•˜ë„ë¡ ìš”ì²­í•˜ëŠ” ë“± ë‹¤ì–‘í•œ ë³€í˜•ì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤. ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì •í™•ë„ì™€ í˜•íƒœ í¸í–¥ì„ ì¸¡ì •í•˜ì—¬ ë¹„êµ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.  ê²°ê³¼ì ìœ¼ë¡œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ê°€ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜, ì¶”ê°€ì ì¸ ì„¤ëª…ì´ë‚˜ ì§€ì‹œì–´ê°€ ì •í™•ë„ì™€ í˜•íƒœ í¸í–¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 4: Exploration of alternative Image Captioning prompts.
> </details>

{{< table-caption >}}
| Prompt | Bias [%] | Accuracy [%] | Avg. Tokens | Generic Ratio [%] |
|---|---|---|---|---|
| "Describe the image. Keep your response short." (default) | 64.0 | 65.08 | 55.5 | 39.5 |
| "Describe the image." | 63.6 | 68.25 | 202.9 | 46.8 |
| "Describe the image. Be precise." | 67.3 | 64.50 | 166.2 | 50.6 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 5ëŠ” LLM ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ê²€ìƒ‰ì„ ìœ„í•œ ì˜ˆì‹œ ëŒ€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ë³„í‘œ(*)ëŠ” ëª¨ì˜ ëŒ€í™”ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  ì´ í‘œëŠ” ì‚¬ìš©ìê°€ LLMì—ê²Œ ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í•˜ì—¬ í˜•íƒœ í¸í–¥ì„ ìµœì†Œí™”í•˜ê³  ì •í™•ë„ë¥¼ ìœ ì§€í•˜ëŠ” ë°©ë²•ì„ ìš”ì²­í•˜ëŠ” ëŒ€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì‚¬ìš©ìëŠ” LLMì´ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ì •í™•ë„ì™€ í˜•íƒœ í¸í–¥ì„ ë°˜í™˜í•˜ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•´ ë‚˜ê°€ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ëŒ€í™”ëŠ” ì—¬ëŸ¬ ë¼ìš´ë“œë¡œ ì§„í–‰ë˜ë©°, LLMì€ ë‹¤ì–‘í•œ ê¸°ë²•(ë³´ìƒ ì œê³µ, ë¬¸ë§¥ ë‚´ ì˜ˆì‹œ ì¶”ê°€, ì´ì „ ì‹œë„ ìš”ì•½ ë“±)ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.  ê²°ê³¼ì ìœ¼ë¡œ,  LLMì€ ìˆ˜ë™ìœ¼ë¡œ ì‘ì„±í•œ í”„ë¡¬í”„íŠ¸ë³´ë‹¤ í˜•íƒœ í¸í–¥ì„ ë” ì¤„ì´ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 5: Example conversation for LLM-guided Prompt Search. *{}^{*}start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPTMock conversation.
> </details>

{{< table-caption >}}
| User* | Assistant* |
|---|---| 
| Your task is to help me design an ideal image classification prompt for a vision/language model to MINIMIZE the shape bias without significantly hurting accuracy. The provided photos show natural objects modified to contain shape and texture of conflicting object classes. E.g., it could be a photo of an elephant (shape) with dog fur (texture). The model should classify the image as â€™dogâ€™ based on the texture of the object and ignore the shape. The modelâ€™s accuracy is the percentage of correctly classified images. The shape bias is the ratio of how often the model classified based on shape over texture. You can test your prompt by outputting a single new line starting with â€™PROMPT: â€™. Do not list options - the system will provide them automatically. Try to keep the prompt as short and simple as possible but be creative. It might be reasonable to summarize insights of previous attempts and to outline your goals before responding with a new prompt, but make sure that only the prompt starts with â€™PROMPT:â€™. In response to the prompt you will be told the accuracy and shape bias. Then you will refine the prompt and we will continue until I say stop. Letâ€™s go! | PROMPT: Which option best describes the image? |
| Prompt: [â€¦], Accuracy: 82.58 %, Shape Bias: 59.43 %. What is your next prompt? | To minimize the shape bias while maintaining accuracy, letâ€™s encourage the model to focus more on textures than shapes. Hereâ€™s my next prompt: PROMPT: Based on the surface characteristics and patterns, which category does the image most resemble? |
| Prompt: [â€¦], Accuracy: 78.25 %, Shape Bias: 50.9 %. Can you improve this? | I have provided various prompts and explored different approaches to minimize the shape bias while maintaining high accuracy. The current optimal prompt is: [â€¦] Here are some additional suggestions for minimizing the shape bias: [â€¦] These methods may require more computational resources and time investment. If you have other questions or need guidance on different topics, please donâ€™t hesitate to reach out. Wishing you the best in your continued efforts! At this point, I believe that we have explored various approaches, and the current prompt is the best-performing one among those tested. If you decide to pursue any of the mentioned alternatives, I am confident that you will find ways to improve the modelâ€™s performance and minimize the shape bias. Good luck with your continued work! As our conversation has concluded, I would like to thank you for engaging in this exercise, and I hope I have been helpful in providing prompts and suggestions. If you have other questions or need guidance on different topics in the future, please donâ€™t hesitate to reach out. Wishing you the best in your projects and research! |{{< /table-caption >}}
> ğŸ”¼ í‘œ 6ì€ ë‹¤ì–‘í•œ CLIP ìœ ì‚¬ ê²°í•© ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í ì¶©ëŒ ë°ì´í„°ì…‹ì—ì„œ ì œë¡œìƒ· ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì„¸ ê°€ì§€ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì „ëµ(80ê°œì˜ í”„ë¡¬í”„íŠ¸, 'a photo of a {class}', '{class}')ì„ ì‚¬ìš©í•˜ì—¬ ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤.  ì´ í‘œëŠ” ê° í”„ë¡¬í”„íŠ¸ ì „ëµì— ë”°ë¥¸ ê° ëª¨ë¸ì˜ í˜•íƒœ í¸í–¥(Shape Bias)ê³¼ ì •í™•ë„(Accuracy)ë¥¼ ë³´ì—¬ì£¼ì–´, ëª¨ë¸ì˜ ì‹œê°ì  íŠ¹ì§• ì¸ì‹ ë°©ì‹ì„ ë¹„êµ ë¶„ì„í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.  CLIP ëª¨ë¸ì˜ í˜•íƒœ í¸í–¥ ë° ì •í™•ë„ì— ëŒ€í•œ ìì„¸í•œ ë¶„ì„ ê²°ê³¼ê°€ ì œì‹œë˜ì–´ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ì „ëµì— ë”°ë¥¸ ê²°ê³¼ ì°¨ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 6: Zero-shot classification on cue-conflict with different CLIP(-like) joint embedding models.
> </details>

{{< table-caption >}}
| Model | Prompt | Shape Bias [%] | Accuracy [%] |
|---|---|---|---|
| EVA01-CLIP-g/14 [17] | 80 Prompts [11] | 66.03 | 87.83 |
| EVA01-CLIP-g/14 [17] | "a photo of a {class}." | 66.03 | 87.08 |
| EVA01-CLIP-g/14 [17] | "{class}" | 66.44 | 86.67 |
| EVA02-CLIP-8B@448px [82] | 80 Prompts [11] | 58.26 | 91.83 |
| EVA02-CLIP-8B@448px [82] | "a photo of a {class}." | 57.58 | 89.00 |
| EVA02-CLIP-8B@448px [82] | "{class}" | 56.60 | 88.33 |
| EVA02-CLIP-E/14+ [17] | 80 Prompts [11] | 65.62 | 90.67 |
| EVA02-CLIP-E/14+ [17] | "a photo of a {class}." | 64.44 | 89.75 |
| EVA02-CLIP-E/14+ [17] | "{class}" | 62.48 | 86.17 |
| CLIP-ViT-L/14 [11] | 80 Prompts [11] | 60.95 | 84.08 |
| CLIP-ViT-L/14 [11] | "a photo of a {class}." | 60.20 | 84.17 |
| CLIP-ViT-L/14 [11] | "{class}" | 60.16 | 81.17 |
| CLIP-ViT-L/14@336px [11] | 80 Prompts [11] | 61.52 | 86.83 |
| CLIP-ViT-L/14@336px [11] | "a photo of a {class}." | 60.56 | 86.42 |
| CLIP-ViT-L/14@336px [11] | "{class}" | 59.80 | 83.75 |
| CLIP-ResNet-50 [11] | 80 Prompts [11] | 19.70 | 77.83 |
| CLIP-ResNet-50 [11] | "a photo of a {class}." | 20.96 | 72.75 |
| CLIP-ResNet-50 [11] | "{class}" | 20.77 | 71.83 |
| CLIP-ResNet-101 [11] | 80 Prompts [11] | 25.50 | 74.83 |
| CLIP-ResNet-101 [11] | "a photo of a {class}." | 25.23 | 71.00 |
| CLIP-ResNet-101 [11] | "{class}" | 25.41 | 70.83 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 7ì€ VQA ì‘ì—…ì—ì„œ í ì¶©ëŒ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ VLMs(Vision Language Models)ì™€ ê°ê°ì˜ ë¹„ì „ ì¸ì½”ë”(CLIP zero-shot classification)ì˜ í˜•íƒœ ë°”ì´ì–´ìŠ¤ì™€ ì •í™•ë„ ê°„ì˜ ì •ëŸ‰ì  ë¹„êµë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ê° VLMì— ëŒ€í•´ í˜•íƒœ ë°”ì´ì–´ìŠ¤ì™€ ì •í™•ë„ì˜ ë³€í™”ìœ¨(ë°±ë¶„ìœ¨)ì´ í‘œì‹œë©ë‹ˆë‹¤.  ì´ í‘œëŠ” VLMsê°€ ë¹„ì „ ì¸ì½”ë”ì˜ í˜•íƒœ ë°”ì´ì–´ìŠ¤ë¥¼ ë‹¨ìˆœíˆ ìƒì†í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë‹¤ì¤‘ ëª¨ë“œ ìœµí•©ì„ í†µí•´ í˜•íƒœ ë°”ì´ì–´ìŠ¤ê°€ ì–´ë–»ê²Œ ë³€í˜•ë  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” ì‹¤í—˜ ê²°ê³¼ë¥¼ ìš”ì•½í•œ ê²ƒì…ë‹ˆë‹¤.  ìŒìˆ˜ ê°’ì€ VLMì´ ì¸ì½”ë”ë³´ë‹¤ í˜•íƒœ ë°”ì´ì–´ìŠ¤ê°€ ê°ì†Œí–ˆìŒì„, ì–‘ìˆ˜ ê°’ì€ ì¦ê°€í–ˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 7: Quantitative comparison between shape bias and accuracy of VLMs (VQA) and their respective vision encoders (CLIP zero-shot classification) on cue-conflict.
> </details>

{{< table-caption >}}
| VLM | Vision Encoder | Accuracy | Shape-Bias |
|---|---|---|---| 
| **LLaVA v1.5 7B** <a href="https://arxiv.org/html/2403.09193v1#bib.bib61">[61]</a> | â„ CLIP ViT-L/14@336px <a href="https://arxiv.org/html/2403.09193v1#bib.bib11">[11]</a> | -3.50 | 4.2 |
| **LLaVA v1.5 13B** <a href="https://arxiv.org/html/2403.09193v1#bib.bib61">[61]</a> |  | -3.00 | 1.5 |
| **LLaVA-NeXT 7B** <a href="https://arxiv.org/html/2403.09193v1#bib.bib62">[62]</a> |  | -9.92 | -3.9 |
| **LLaVA-NeXT 13B** <a href="https://arxiv.org/html/2403.09193v1#bib.bib62">[62]</a> |  | -0.33 | -2.7 |
| **LLaVA-NeXT 34B** <a href="https://arxiv.org/html/2403.09193v1#bib.bib62">[62]</a> |  | -1.08 | -0.2 |
| **MoE-LLaVA v1.5 Phi2 x4** <a href="https://arxiv.org/html/2403.09193v1#bib.bib67">[67]</a> |  | -1.42 | -0.3 |
| **MoE-LLaVA v1.5 Qwen x4** <a href="https://arxiv.org/html/2403.09193v1#bib.bib67">[67]</a> |  | -24.25 | 3.0 |
| **MoE-LLaVA v1.5 StableLM x4** <a href="https://arxiv.org/html/2403.09193v1#bib.bib67">[67]</a> |  | -3.67 | -0.8 |
| **InstructBLIP FLAN-T5-XL** <a href="https://arxiv.org/html/2403.09193v1#bib.bib23">[23]</a> | â„ EVA-01-CLIP ViT-g/14@224px <a href="https://arxiv.org/html/2403.09193v1#bib.bib17">[17]</a> | -6.83 | 1.8 |
| **InstructBLIP Vicuna-7B** <a href="https://arxiv.org/html/2403.09193v1#bib.bib23">[23]</a> |  | -14.42 | 7.4 |
| **Emu2-Chat** <a href="https://arxiv.org/html/2403.09193v1#bib.bib66">[66]</a> | â„ EVA-02-CLIP-E/14+@448px <a href="https://arxiv.org/html/2403.09193v1#bib.bib17">[17]</a> | -11.08 | -9.5 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 8ì€ ImageNetìœ¼ë¡œ í•™ìŠµë˜ê±°ë‚˜ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ë“¤ì— ëŒ€í•œ í ì¶©ëŒ(cue-conflict) ë°ì´í„°ì…‹ì—ì„œì˜ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  'Shape Bias' ì—´ì€ ëª¨ë¸ì´ í˜•íƒœ(shape) ì •ë³´ì— ì–¼ë§ˆë‚˜ ì¹˜ìš°ì³ì ¸ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°±ë¶„ìœ¨ì´ë©°, 'Accuracy' ì—´ì€ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°±ë¶„ìœ¨ì…ë‹ˆë‹¤.  ë‹¤ì–‘í•œ ëª¨ë¸ì˜ í˜•íƒœ í¸í–¥ê³¼ ì •í™•ë„ë¥¼ ë¹„êµí•˜ì—¬ ImageNet í•™ìŠµì˜ ì˜í–¥ì„ ë¶„ì„í•˜ê³ ì í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 8: Classification on cue-conflict with ImageNet-trained/finetuned models.
> </details>

{{< table-caption >}}
| Model | Shape Bias [%] | Accuracy [%] |
|---|---|---|
| ResNet-50 [59] | 22.3 | 67.33 |
| ResNet-50 (timm) [83] | 23.1 | 65.42 |
| ResNet-152 (timm) [83] | 28.6 | 65.83 |
| ViT-B/16 (ImageNet-21k pretraining) [32] | 45.4 | 63.67 |
| ImageNet-finetuned CLIP (ViT-L/14@336px) [83] | 32.1 | 82.75 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ì´ë¯¸ì§€ ìº¡ì…˜ ì‘ì—…ì— ëŒ€í•œ ëª¨ë“  ì‘ë‹µê³¼ LLMì´ ì¼ë°˜ì ì´ì§€ ì•Šë‹¤ê³  ë¶„ë¥˜í•˜ì§€ ì•Šì€ ì‘ë‹µì— ëŒ€í•´ í˜•íƒœ í¸í–¥ ë° ì •í™•ë„ë¥¼ ë¹„êµí•œ ê²ƒì…ë‹ˆë‹¤.  LLMì´ ì¼ë°˜ì ì´ì§€ ì•Šë‹¤ê³  ë¶„ë¥˜í•œ ì‘ë‹µì€ ì¦‰, ëª¨ë¸ì´ ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ ë¬˜ì‚¬í•˜ì§€ ëª»í•œ ì‘ë‹µì„ ì˜ë¯¸í•©ë‹ˆë‹¤. í‘œëŠ” ëª¨ë“  ì‘ë‹µì„ ê³ ë ¤í•œ ê²°ê³¼ì™€ ì¼ë°˜ì ì´ì§€ ì•Šì€ ì‘ë‹µë§Œ ê³ ë ¤í•œ ê²°ê³¼ë¥¼ ëª¨ë‘ ë³´ì—¬ì£¼ì–´, ì¼ë°˜ì ì¸ ì‘ë‹µê³¼ ëª…í™•í•œ ì‘ë‹µ ì‚¬ì´ì˜ í˜•íƒœ í¸í–¥ ë° ì •í™•ë„ ì°¨ì´ë¥¼ ë¹„êµ ë¶„ì„í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 9: Comparison of shape bias and accuracy for the Image Captioning tasks for all responses and only responses which an LLM did not classify as generic.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}