[{"content": "| Model                     | Description                                                                                                                                                                                  |\n|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `Qwen-VL-Chat` [63]         | Adds vision capabilities to `Qwen-7B` [79]. We set a repetition penalty of 1.2 for this model.                                                                                              |\n| `Qwen-VL Plus/Max` [26]     | Alibaba\u2019s proprietary larger variants of `Qwen-VL-Chat`. Access only via API.                                                                                                                   |\n| `CogAgent` [64]             | A special model for interaction with graphical user interfaces (GUIs) at high-resolution.                                                                                                       |\n| `CogVLM` [65]               | Adds \"trainable visual expert module\" in LLM layers to combine vision and language.                                                                                                              |\n| `Emu2` [66]                 | The 37B model claims \"strong multimodal in-context learning abilities\".                                                                                                                         |\n| `InstructBLIP` [23]         | Connects frozen vision encoders and LLMs through a trainable Q-Former. Uses `Vicuna` or `FLAN-T5` as LLMs.                                                                                      |\n| `LLaVA v1.5` [61]           | Improvements of `LLaVA` with modifications on the image encoder, the projector, and task-specific data. Uses `Vicuna-7/13B` as LLM.                                                                  |\n| `LLaVA-NeXT` [62]           | Successor of `LLaVA v1.5` supporting higher resolutions through patching, and using better SFT training data for training, claiming \u201cimproved reasoning, OCR, and world knowledge\u201d [62]. The 34B version switches from `Vicuna-7/13B` to `Nous Hermes 2 Yi 34B`. |\n| `MoE-LLaVA v1.5` [67]       | Variants of `LLaVA v1.5` employing 4 sparsely activated Mixture-of-Experts (MoE), and smaller LLMs (`Qwen`, `Phi-2`, `StableLM`).                                                                     |\n| `LLaVA-RLHF` [69]           | Variants of `LLaVA v1.5` aligned with Factually Augmented RLHF (Fact-RLHF) [69].                                                                                                                  |\n| `UForm-Gen Chat` [68]       | A small (1.5B) model for VQA and image captioning finetuned for multimodal chat.                                                                                                                 |\n| `Gemini 1.0 Pro Vision` [25] | Google\u2019s proprietary multi-modal model based on the Gemini Pro LLM. Access only via API.                                                                                                           |\n| `InternVL Chat 1.1/1.2+` [2] | An open-source effort to provide an alternative to `ViT-22B` [80]. V1.1 is based on a 6B ViT and `Vicuna-13B`, V1.2+ uses `Nous Hermes 2 Yi 34B` as LLM including additional SFT on 10x more data. |\n| `GPT-4V (Preview)` [12]      | OpenAI\u2019s proprietary multi-modal model based on the GPT-4 LLM. Access only via API. Often considered to be the most powerful model.                                                                    |", "caption": "Table 2: The shape bias and respective accuracy on the cue-conflict dataset for various VLMs in VQA classification or image description tasks. For the image description task, we additionally provide the average number of tokens generated by Vicuna\u2019s tokenizer and the ratio of responses that only contain a single class or are generic (do not mention any class) as judged by a separate LLM. \u201c-\u201d indicates models that did not follow instructions on VQA and could, thus, not be evaluated.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c VLMs\uc5d0 \ub300\ud55c VQA \ubd84\ub958 \ub610\ub294 \uc774\ubbf8\uc9c0 \uc124\uba85 \uc791\uc5c5\uc5d0\uc11c \ud050 \ucda9\ub3cc \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \ub300\ud55c \ud615\ud0dc \ud3b8\ud5a5 \ubc0f \ud574\ub2f9 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ubbf8\uc9c0 \uc124\uba85 \uc791\uc5c5\uc758 \uacbd\uc6b0, \ube44\ucfe0\ub0d0 \ud1a0\ud06c\ub098\uc774\uc800\uac00 \uc0dd\uc131\ud55c \ud1a0\ud070\uc758 \ud3c9\uade0 \uc218\uc640 \ubcc4\ub3c4\uc758 LLM\uc5d0 \uc758\ud574 \ud310\ub2e8\ub41c \ub2e8\uc77c \ud074\ub798\uc2a4\ub9cc \ud3ec\ud568\ud558\uac70\ub098 \uc77c\ubc18\uc801\uc778(\ud074\ub798\uc2a4\ub97c \uc5b8\uae09\ud558\uc9c0 \uc54a\uc74c) \uc751\ub2f5\uc758 \ube44\uc728\uc744 \ucd94\uac00\ub85c \uc81c\uacf5\ud569\ub2c8\ub2e4.  \"-\"\ub294 VQA\uc5d0 \ub300\ud55c \uc9c0\uc2dc \uc0ac\ud56d\uc744 \ub530\ub974\uc9c0 \uc54a\uc544 \ud3c9\uac00\ud560 \uc218 \uc5c6\ub294 \ubaa8\ub378\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4.1 \uc8fc\uc694 \uacb0\uacfc"}, {"content": "| Model | VQA |  | Image Captioning |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n|  | Shape | Accuracy [%] | Shape | Accuracy [%] | Avg. Tokens | Single Class Ratio [%] | Generic Ratio [%] |\n|---|---|---|---|---|---|---|---|---|\n| Gemini 1.0 Pro Vision [25] | 64.1 | 82.33 | 63.2 | 68.00 | 18.9 | 63.0 | 32.3 |\n| GPT-4V (Preview) [12] | 47.9 | 69.75 | 53.6 | 52.67 | 44.8 | 37.2 | 60.4 |\n| Qwen-VL Plus [26] | 64.8 | 82.92 | 67.9 | 65.50 | 21.9 | 59.2 | 36.0 |\n| Qwen-VL Max [26] | 62.4 | 85.50 | 69.7 | 68.50 | 151.9 | 52.1 | 41.0 |\n| Qwen-VL Chat [63] | - | - | 38.2 | 67.42 | 27.3 | 59.1 | 33.2 |\n| InternVL Chat 1.1 [2] | 68.3 | 89.33 | 73.2 | 75.58 | 16.9 | 74.9 | 19.4 |\n| InternVL Chat 1.2+ [2] | 61.1 | 90.83 | 61.3 | 82.42 | 15.8 | 80.4 | 11.4 |\n| LLaVA v1.5 7B [61] | 61.4 | 80.75 | 61.4 | 76.08 | 12.1 | 73.8 | 19.2 |\n| LLaVA v1.5 13B [61] | 64.1 | 80.25 | 62.7 | 75.58 | 28.9 | 65.8 | 23.8 |\n| LLaVA-RLHF 7B [69] | 61.7 | 68.08 | 63.0 | 71.83 | 47.9 | 65.1 | 24.7 |\n| LLaVA-RLHF 13B [69] | 63.4 | 80.42 | 62.3 | 73.25 | 38.3 | 64.7 | 27.7 |\n| LLaVA-NeXT 7B [62] | 59.2 | 82.58 | 64.0 | 65.08 | 20.2 | 55.5 | 39.5 |\n| LLaVA-NeXT 13B [62] | 57.2 | 83.42 | 63.5 | 65.25 | 48.8 | 52.6 | 40.9 |\n| LLaVA-NeXT 34B [62] | 56.0 | 73.83 | 66.2 | 57.50 | 93.4 | 36.2 | 59.1 |\n| MoE-LLaVA-StableLM [67] | 59.1 | 80.08 | 63.0 | 73.92 | 24.1 | 67.4 | 21.6 |\n| MoE-LLaVA-Qwen [67] | 62.9 | 59.50 | 63.2 | 75.33 | 13.3 | 69.4 | 20.7 |\n| MoE-LLaVA-Phi2 [67] | 59.6 | 82.33 | 61.1 | 75.42 | 34.9 | 67.0 | 18.6 |\n| InstructBLIP Flan-T5-xl [23] | 68.2 | 79.58 | 67.1 | 81.50 | 116.7 | 57.0 | 22.3 |\n| InstructBLIP Vicuna-7B [23] | 73.8 | 72.25 | 67.7 | 80.67 | 94.0 | 60.9 | 28.0 |\n| Emu2-Chat [66] | 52.9 | 75.08 | 59.6 | 65.00 | 13.6 | 63.0 | 34.0 |\n| CogAgent Chat [64] | - | - | 67.4 | 60.33 | 40.1 | 49.6 | 47.7 |\n| CogVLM Chat [65] | - | - | 57.6 | 66.58 | 35.8 | 53.2 | 40.1 |\n| UForm Gen Chat [68] | - | - | 38.8 | 64.50 | 30.2 | 59.3 | 33.0 |", "caption": "Table 3: Exploration of alternative VQA prompts.", "description": "\ud45c 3\uc740 VQA(Visual Question Answering) \uc791\uc5c5\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub2e4\uc591\ud55c \ud504\ub86c\ud504\ud2b8\ub97c \ud0d0\uc0c9\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\ubcf8 \ud504\ub86c\ud504\ud2b8\ub97c \ube44\ub86f\ud558\uc5ec, CLIP \uc2a4\ud0c0\uc77c\uc758 \uc635\uc158\uc744 \uc0ac\uc6a9\ud55c \ud504\ub86c\ud504\ud2b8, \uc774\ubbf8\uc9c0 \ub0b4 \uac1d\uccb4\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \uc694\uccad\ud558\ub294 \ud504\ub86c\ud504\ud2b8, CLIP \uc2a4\ud0c0\uc77c \uc635\uc158\uacfc \ud568\uaed8 \uac1d\uccb4 \uc124\uba85\uc744 \uc694\uccad\ud558\ub294 \ud504\ub86c\ud504\ud2b8, \uadf8\ub9ac\uace0 \uc9c0\uc2dc\uc0ac\ud56d \uc5c6\uc774 \uc635\uc158\ub9cc \uc81c\uacf5\ud558\ub294 \ud504\ub86c\ud504\ud2b8 \ub4f1 \ub2e4\uc591\ud55c \ubcc0\ud615\uc774 \uc2dc\ub3c4\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \uac01 \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\uc640 \ud615\ud0dc \ud3b8\ud5a5(shape bias)\uc744 \uce21\uc815\ud558\uc5ec \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.  \uacb0\uacfc\uc801\uc73c\ub85c \uae30\ubcf8 \ud504\ub86c\ud504\ud2b8\uac00 \uac00\uc7a5 \uc88b\uc740 \uc815\ud655\ub3c4\uc640 \ud615\ud0dc \ud3b8\ud5a5\uc758 \uade0\ud615\uc744 \uc774\ub8e8\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4.", "section": "3.2 VLM Tasks"}, {"content": "| Prompt | Bias [%] | Accuracy [%] |\n|---|---|---|\n| \"Which option best describes the image? [...]\" (default) | 59.2 | 82.58 |\n| Default with CLIP-style options | 59.5 | 81.92 |\n| \"Describe the object in the image: [...]\" | 60.2 | 81.33 |\n| \"Describe the object in the image: [...]\" |  |  |\n| with CLIP-style options | 59.4 | 80.17 |\n| Empty instruction (just options) | 59.5 | 81.33 |", "caption": "Table 4: Exploration of alternative Image Captioning prompts.", "description": "\uc774 \ud45c\ub294 \uc774\ubbf8\uc9c0 \ucea1\uc158 \uc791\uc5c5\uc744 \uc704\ud55c \uc5ec\ub7ec \uac00\uc9c0 \ud504\ub86c\ud504\ud2b8\ub97c \uc2dc\ud5d8\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\ubcf8 \ud504\ub86c\ud504\ud2b8 \uc678\uc5d0\ub3c4 \ud504\ub86c\ud504\ud2b8\uc5d0 \ucd94\uac00\uc801\uc778 \uc124\uba85\uc744 \ub123\uac70\ub098,  \uc9e7\uac8c \uc751\ub2f5\ud558\ub3c4\ub85d \uc694\uccad\ud558\ub294 \ub4f1 \ub2e4\uc591\ud55c \ubcc0\ud615\uc744 \uc2dc\ub3c4\ud588\uc2b5\ub2c8\ub2e4. \uac01 \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\uc640 \ud615\ud0dc \ud3b8\ud5a5\uc744 \uce21\uc815\ud558\uc5ec \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.  \uacb0\uacfc\uc801\uc73c\ub85c \uae30\ubcf8 \ud504\ub86c\ud504\ud2b8\uac00 \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uc73c\ub098, \ucd94\uac00\uc801\uc778 \uc124\uba85\uc774\ub098 \uc9c0\uc2dc\uc5b4\uac00 \uc815\ud655\ub3c4\uc640 \ud615\ud0dc \ud3b8\ud5a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "3.2 VLM Tasks"}, {"content": "| Prompt | Bias [%] | Accuracy [%] | Avg. Tokens | Generic Ratio [%] |\n|---|---|---|---|---|\n| \"Describe the image. Keep your response short.\" (default) | 64.0 | 65.08 | 55.5 | 39.5 |\n| \"Describe the image.\" | 63.6 | 68.25 | 202.9 | 46.8 |\n| \"Describe the image. Be precise.\" | 67.3 | 64.50 | 166.2 | 50.6 |", "caption": "Table 5: Example conversation for LLM-guided Prompt Search. *{}^{*}start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPTMock conversation.", "description": "\ud45c 5\ub294 LLM \uae30\ubc18 \ud504\ub86c\ud504\ud2b8 \uac80\uc0c9\uc744 \uc704\ud55c \uc608\uc2dc \ub300\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubcc4\ud45c(*)\ub294 \ubaa8\uc758 \ub300\ud654\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uc0ac\uc6a9\uc790\uac00 LLM\uc5d0\uac8c \uc774\ubbf8\uc9c0 \ubd84\ub958 \ud504\ub86c\ud504\ud2b8\ub97c \ucd5c\uc801\ud654\ud558\uc5ec \ud615\ud0dc \ud3b8\ud5a5\uc744 \ucd5c\uc18c\ud654\ud558\uace0 \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud558\ub294 \ubc29\ubc95\uc744 \uc694\uccad\ud558\ub294 \ub300\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc0ac\uc6a9\uc790\ub294 LLM\uc774 \ud504\ub86c\ud504\ud2b8\ub97c \uc0dd\uc131\ud558\uace0, \uc815\ud655\ub3c4\uc640 \ud615\ud0dc \ud3b8\ud5a5\uc744 \ubc18\ud658\ud558\uba70, \uc774\ub97c \ubc14\ud0d5\uc73c\ub85c \ud504\ub86c\ud504\ud2b8\ub97c \uac1c\uc120\ud574 \ub098\uac00\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub300\ud654\ub294 \uc5ec\ub7ec \ub77c\uc6b4\ub4dc\ub85c \uc9c4\ud589\ub418\uba70, LLM\uc740 \ub2e4\uc591\ud55c \uae30\ubc95(\ubcf4\uc0c1 \uc81c\uacf5, \ubb38\ub9e5 \ub0b4 \uc608\uc2dc \ucd94\uac00, \uc774\uc804 \uc2dc\ub3c4 \uc694\uc57d \ub4f1)\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud504\ub86c\ud504\ud2b8\ub97c \uac1c\uc120\ud569\ub2c8\ub2e4.  \uacb0\uacfc\uc801\uc73c\ub85c,  LLM\uc740 \uc218\ub3d9\uc73c\ub85c \uc791\uc131\ud55c \ud504\ub86c\ud504\ud2b8\ubcf4\ub2e4 \ud615\ud0dc \ud3b8\ud5a5\uc744 \ub354 \uc904\uc774\ub294 \ud504\ub86c\ud504\ud2b8\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.", "section": "5.2 \uc5b8\uc5b4\uc801 \uc870\uc815: \ud504\ub86c\ud504\ud2b8 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1"}, {"content": "| User* | Assistant* |\n|---|---| \n| Your task is to help me design an ideal image classification prompt for a vision/language model to MINIMIZE the shape bias without significantly hurting accuracy. The provided photos show natural objects modified to contain shape and texture of conflicting object classes. E.g., it could be a photo of an elephant (shape) with dog fur (texture). The model should classify the image as \u2019dog\u2019 based on the texture of the object and ignore the shape. The model\u2019s accuracy is the percentage of correctly classified images. The shape bias is the ratio of how often the model classified based on shape over texture. You can test your prompt by outputting a single new line starting with \u2019PROMPT: \u2019. Do not list options - the system will provide them automatically. Try to keep the prompt as short and simple as possible but be creative. It might be reasonable to summarize insights of previous attempts and to outline your goals before responding with a new prompt, but make sure that only the prompt starts with \u2019PROMPT:\u2019. In response to the prompt you will be told the accuracy and shape bias. Then you will refine the prompt and we will continue until I say stop. Let\u2019s go! | PROMPT: Which option best describes the image? |\n| Prompt: [\u2026], Accuracy: 82.58 %, Shape Bias: 59.43 %. What is your next prompt? | To minimize the shape bias while maintaining accuracy, let\u2019s encourage the model to focus more on textures than shapes. Here\u2019s my next prompt: PROMPT: Based on the surface characteristics and patterns, which category does the image most resemble? |\n| Prompt: [\u2026], Accuracy: 78.25 %, Shape Bias: 50.9 %. Can you improve this? | I have provided various prompts and explored different approaches to minimize the shape bias while maintaining high accuracy. The current optimal prompt is: [\u2026] Here are some additional suggestions for minimizing the shape bias: [\u2026] These methods may require more computational resources and time investment. If you have other questions or need guidance on different topics, please don\u2019t hesitate to reach out. Wishing you the best in your continued efforts! At this point, I believe that we have explored various approaches, and the current prompt is the best-performing one among those tested. If you decide to pursue any of the mentioned alternatives, I am confident that you will find ways to improve the model\u2019s performance and minimize the shape bias. Good luck with your continued work! As our conversation has concluded, I would like to thank you for engaging in this exercise, and I hope I have been helpful in providing prompts and suggestions. If you have other questions or need guidance on different topics in the future, please don\u2019t hesitate to reach out. Wishing you the best in your projects and research! |", "caption": "Table 6: Zero-shot classification on cue-conflict with different CLIP(-like) joint embedding models.", "description": "\ud45c 6\uc740 \ub2e4\uc591\ud55c CLIP \uc720\uc0ac \uacb0\ud569 \uc784\ubca0\ub529 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud050 \ucda9\ub3cc \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc81c\ub85c\uc0f7 \ubd84\ub958\ub97c \uc218\ud589\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc138 \uac00\uc9c0 \ub2e4\ub978 \ud504\ub86c\ud504\ud2b8 \uc804\ub7b5(80\uac1c\uc758 \ud504\ub86c\ud504\ud2b8, \"a photo of a {class}\", \"{class}\")\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uac01 \ud504\ub86c\ud504\ud2b8 \uc804\ub7b5\uc5d0 \ub530\ub978 \uac01 \ubaa8\ub378\uc758 \ud615\ud0dc \ud3b8\ud5a5(Shape Bias)\uacfc \uc815\ud655\ub3c4(Accuracy)\ub97c \ubcf4\uc5ec\uc8fc\uc5b4, \ubaa8\ub378\uc758 \uc2dc\uac01\uc801 \ud2b9\uc9d5 \uc778\uc2dd \ubc29\uc2dd\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.  CLIP \ubaa8\ub378\uc758 \ud615\ud0dc \ud3b8\ud5a5 \ubc0f \uc815\ud655\ub3c4\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ubd84\uc11d \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \ub2e4\uc591\ud55c \ud504\ub86c\ud504\ud2b8 \uc804\ub7b5\uc5d0 \ub530\ub978 \uacb0\uacfc \ucc28\uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "G CLIP \ubaa8\ub378 \uacb0\uacfc"}, {"content": "| Model | Prompt | Shape Bias [%] | Accuracy [%] |\n|---|---|---|---|\n| EVA01-CLIP-g/14 [17] | 80 Prompts [11] | 66.03 | 87.83 |\n| EVA01-CLIP-g/14 [17] | \"a photo of a {class}.\" | 66.03 | 87.08 |\n| EVA01-CLIP-g/14 [17] | \"{class}\" | 66.44 | 86.67 |\n| EVA02-CLIP-8B@448px [82] | 80 Prompts [11] | 58.26 | 91.83 |\n| EVA02-CLIP-8B@448px [82] | \"a photo of a {class}.\" | 57.58 | 89.00 |\n| EVA02-CLIP-8B@448px [82] | \"{class}\" | 56.60 | 88.33 |\n| EVA02-CLIP-E/14+ [17] | 80 Prompts [11] | 65.62 | 90.67 |\n| EVA02-CLIP-E/14+ [17] | \"a photo of a {class}.\" | 64.44 | 89.75 |\n| EVA02-CLIP-E/14+ [17] | \"{class}\" | 62.48 | 86.17 |\n| CLIP-ViT-L/14 [11] | 80 Prompts [11] | 60.95 | 84.08 |\n| CLIP-ViT-L/14 [11] | \"a photo of a {class}.\" | 60.20 | 84.17 |\n| CLIP-ViT-L/14 [11] | \"{class}\" | 60.16 | 81.17 |\n| CLIP-ViT-L/14@336px [11] | 80 Prompts [11] | 61.52 | 86.83 |\n| CLIP-ViT-L/14@336px [11] | \"a photo of a {class}.\" | 60.56 | 86.42 |\n| CLIP-ViT-L/14@336px [11] | \"{class}\" | 59.80 | 83.75 |\n| CLIP-ResNet-50 [11] | 80 Prompts [11] | 19.70 | 77.83 |\n| CLIP-ResNet-50 [11] | \"a photo of a {class}.\" | 20.96 | 72.75 |\n| CLIP-ResNet-50 [11] | \"{class}\" | 20.77 | 71.83 |\n| CLIP-ResNet-101 [11] | 80 Prompts [11] | 25.50 | 74.83 |\n| CLIP-ResNet-101 [11] | \"a photo of a {class}.\" | 25.23 | 71.00 |\n| CLIP-ResNet-101 [11] | \"{class}\" | 25.41 | 70.83 |", "caption": "Table 7: Quantitative comparison between shape bias and accuracy of VLMs (VQA) and their respective vision encoders (CLIP zero-shot classification) on cue-conflict.", "description": "\ud45c 7\uc740 VQA \uc791\uc5c5\uc5d0\uc11c \ud050 \ucda9\ub3cc \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec VLMs(Vision Language Models)\uc640 \uac01\uac01\uc758 \ube44\uc804 \uc778\ucf54\ub354(CLIP zero-shot classification)\uc758 \ud615\ud0dc \ubc14\uc774\uc5b4\uc2a4\uc640 \uc815\ud655\ub3c4 \uac04\uc758 \uc815\ub7c9\uc801 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 VLM\uc5d0 \ub300\ud574 \ud615\ud0dc \ubc14\uc774\uc5b4\uc2a4\uc640 \uc815\ud655\ub3c4\uc758 \ubcc0\ud654\uc728(\ubc31\ubd84\uc728)\uc774 \ud45c\uc2dc\ub429\ub2c8\ub2e4.  \uc774 \ud45c\ub294 VLMs\uac00 \ube44\uc804 \uc778\ucf54\ub354\uc758 \ud615\ud0dc \ubc14\uc774\uc5b4\uc2a4\ub97c \ub2e8\uc21c\ud788 \uc0c1\uc18d\ud558\ub294 \uac83\uc774 \uc544\ub2c8\ub77c, \ub2e4\uc911 \ubaa8\ub4dc \uc735\ud569\uc744 \ud1b5\ud574 \ud615\ud0dc \ubc14\uc774\uc5b4\uc2a4\uac00 \uc5b4\ub5bb\uac8c \ubcc0\ud615\ub420 \uc218 \uc788\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc694\uc57d\ud55c \uac83\uc785\ub2c8\ub2e4.  \uc74c\uc218 \uac12\uc740 VLM\uc774 \uc778\ucf54\ub354\ubcf4\ub2e4 \ud615\ud0dc \ubc14\uc774\uc5b4\uc2a4\uac00 \uac10\uc18c\ud588\uc74c\uc744, \uc591\uc218 \uac12\uc740 \uc99d\uac00\ud588\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "4.3 \ube44\uc804 \uc778\ucf54\ub354\uc758 \uc5ed\ud560"}, {"content": "| VLM | Vision Encoder | Accuracy | Shape-Bias |\n|---|---|---|---| \n| **LLaVA v1.5 7B** <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib61\">[61]</a> | \u2744 CLIP ViT-L/14@336px <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib11\">[11]</a> | -3.50 | 4.2 |\n| **LLaVA v1.5 13B** <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib61\">[61]</a> |  | -3.00 | 1.5 |\n| **LLaVA-NeXT 7B** <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib62\">[62]</a> |  | -9.92 | -3.9 |\n| **LLaVA-NeXT 13B** <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib62\">[62]</a> |  | -0.33 | -2.7 |\n| **LLaVA-NeXT 34B** <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib62\">[62]</a> |  | -1.08 | -0.2 |\n| **MoE-LLaVA v1.5 Phi2 x4** <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib67\">[67]</a> |  | -1.42 | -0.3 |\n| **MoE-LLaVA v1.5 Qwen x4** <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib67\">[67]</a> |  | -24.25 | 3.0 |\n| **MoE-LLaVA v1.5 StableLM x4** <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib67\">[67]</a> |  | -3.67 | -0.8 |\n| **InstructBLIP FLAN-T5-XL** <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib23\">[23]</a> | \u2744 EVA-01-CLIP ViT-g/14@224px <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib17\">[17]</a> | -6.83 | 1.8 |\n| **InstructBLIP Vicuna-7B** <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib23\">[23]</a> |  | -14.42 | 7.4 |\n| **Emu2-Chat** <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib66\">[66]</a> | \u2744 EVA-02-CLIP-E/14+@448px <a href=\"https://arxiv.org/html/2403.09193v1#bib.bib17\">[17]</a> | -11.08 | -9.5 |", "caption": "Table 8: Classification on cue-conflict with ImageNet-trained/finetuned models.", "description": "\ud45c 8\uc740 ImageNet\uc73c\ub85c \ud559\uc2b5\ub418\uac70\ub098 \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud55c \ud050 \ucda9\ub3cc(cue-conflict) \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \ubd84\ub958 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'Shape Bias' \uc5f4\uc740 \ubaa8\ub378\uc774 \ud615\ud0dc(shape) \uc815\ubcf4\uc5d0 \uc5bc\ub9c8\ub098 \uce58\uc6b0\uccd0\uc838 \uc788\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 \ubc31\ubd84\uc728\uc774\uba70, 'Accuracy' \uc5f4\uc740 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0b4\ub294 \ubc31\ubd84\uc728\uc785\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \ud615\ud0dc \ud3b8\ud5a5\uacfc \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec ImageNet \ud559\uc2b5\uc758 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\uace0\uc790 \ud569\ub2c8\ub2e4.", "section": "H. ImageNet-trained Models"}, {"content": "| Model | Shape Bias [%] | Accuracy [%] |\n|---|---|---|\n| ResNet-50 [59] | 22.3 | 67.33 |\n| ResNet-50 (timm) [83] | 23.1 | 65.42 |\n| ResNet-152 (timm) [83] | 28.6 | 65.83 |\n| ViT-B/16 (ImageNet-21k pretraining) [32] | 45.4 | 63.67 |\n| ImageNet-finetuned CLIP (ViT-L/14@336px) [83] | 32.1 | 82.75 |", "caption": "Table 9: Comparison of shape bias and accuracy for the Image Captioning tasks for all responses and only responses which an LLM did not classify as generic.", "description": "\uc774 \ud45c\ub294 \uc774\ubbf8\uc9c0 \ucea1\uc158 \uc791\uc5c5\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \uc751\ub2f5\uacfc LLM\uc774 \uc77c\ubc18\uc801\uc774\uc9c0 \uc54a\ub2e4\uace0 \ubd84\ub958\ud558\uc9c0 \uc54a\uc740 \uc751\ub2f5\uc5d0 \ub300\ud574 \ud615\ud0dc \ud3b8\ud5a5 \ubc0f \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4.  LLM\uc774 \uc77c\ubc18\uc801\uc774\uc9c0 \uc54a\ub2e4\uace0 \ubd84\ub958\ud55c \uc751\ub2f5\uc740 \uc989, \ubaa8\ub378\uc774 \uc774\ubbf8\uc9c0\uc758 \ub0b4\uc6a9\uc744 \uba85\ud655\ud558\uac8c \ubb18\uc0ac\ud558\uc9c0 \ubabb\ud55c \uc751\ub2f5\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \ud45c\ub294 \ubaa8\ub4e0 \uc751\ub2f5\uc744 \uace0\ub824\ud55c \uacb0\uacfc\uc640 \uc77c\ubc18\uc801\uc774\uc9c0 \uc54a\uc740 \uc751\ub2f5\ub9cc \uace0\ub824\ud55c \uacb0\uacfc\ub97c \ubaa8\ub450 \ubcf4\uc5ec\uc8fc\uc5b4, \uc77c\ubc18\uc801\uc778 \uc751\ub2f5\uacfc \uba85\ud655\ud55c \uc751\ub2f5 \uc0ac\uc774\uc758 \ud615\ud0dc \ud3b8\ud5a5 \ubc0f \uc815\ud655\ub3c4 \ucc28\uc774\ub97c \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "4.1 \uc8fc\uc694 \uacb0\uacfc"}]