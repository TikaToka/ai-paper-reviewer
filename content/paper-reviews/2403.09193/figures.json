[{"figure_path": "https://arxiv.org/html/2403.09193/x1.png", "caption": "Figure 1: Unlike many unimodal models, vision language models (VLMs) prefer shape over texture for object recognition, but not to the same extent as humans. Further, we find that the (visual) texture/shape bias [1] can be steered through language alone, albeit not to the extent as through vision. Here we visualize the texture/shape bias of some exemplary VLMs, and highlight the steerability of InternVL-Chat 1.1 [2].", "description": "\uadf8\ub9bc 1\uc740 \ub2e4\uc591\ud55c VLMs(Vision Language Models)\uc758 \uc9c8\uac10-\ud615\ud0dc \ud3b8\ud5a5(texture-shape bias)\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e8\uc77c \ubaa8\ub4dc \ubaa8\ub378\ub4e4\uacfc \ub2ec\ub9ac VLMs\ub294 \uac1d\uccb4 \uc778\uc2dd \uc2dc \uc9c8\uac10\ubcf4\ub2e4\ub294 \ud615\ud0dc\ub97c \ub354 \uc120\ud638\ud558\uc9c0\ub9cc, \uc778\uac04\uc758 \uc218\uc900\uc5d0\ub294 \ubbf8\uce58\uc9c0 \ubabb\ud569\ub2c8\ub2e4.  \ud765\ubbf8\ub86d\uac8c\ub3c4, VLMs\uc758 \uc9c8\uac10-\ud615\ud0dc \ud3b8\ud5a5\uc740 \uc2dc\uac01 \uc815\ubcf4\ubfd0\ub9cc \uc544\ub2c8\ub77c \uc5b8\uc5b4 \ud504\ub86c\ud504\ud2b8\ub9cc\uc73c\ub85c\ub3c4 \uc870\uc808 \uac00\ub2a5\ud558\uba70, \uc774\ub294 InternVL-Chat 1.1\uc758 \uc608\uc2dc\ub97c \ud1b5\ud574 \uac15\uc870\ub429\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 VLMs\uac00 \uc2dc\uac01\uc801 \ud3b8\ud5a5\uc744 \uc5b4\ub5bb\uac8c \ucc98\ub9ac\ud558\uace0 \uc5b8\uc5b4\ub97c \ud1b5\ud574 \uc870\uc808\ud560 \uc218 \uc788\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc911\uc694\ud55c \uc2dc\uac01\uc801 \uc790\ub8cc\uc785\ub2c8\ub2e4.  \uac01 VLM\uc5d0 \ub300\ud55c \uc9c8\uac10 \ubc0f \ud615\ud0dc \ud3b8\ud5a5\uc758 \uc815\ub3c4\uac00 \ub9c9\ub300 \uadf8\ub798\ud504\ub85c \ud45c\ud604\ub418\uc5b4 \uc788\uace0, InternVL-Chat 1.1\uc758 \uacbd\uc6b0 \uc5b8\uc5b4 \uc870\uc808\uc744 \ud1b5\ud574 \ud615\ud0dc \ud3b8\ud5a5\uc744 \uc5b4\ub5bb\uac8c \uc99d\uac00\uc2dc\ud0ac \uc218 \uc788\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \ucd94\uac00 \uadf8\ub798\ud504\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2403.09193/x2.png", "caption": "Figure 2: Most VLMs are slightly shape-biased but some models show differences when asked to describe an image compared to VQA. We measure the shape bias on the cue-conflict dataset [1]. For reference, we also provide measurements on ResNet-50 [59] from the initial shape bias study [1], zero-shot classification (CLIP ViT-L/14 [11]), and a human average (over 10 subjects [1]).", "description": "\uadf8\ub9bc 2\ub294 \ub2e4\uc591\ud55c VLMs(Vision Language Models)\uc758 \uc9c8\uac10-\ud615\ud0dc \ud3b8\ud5a5(texture-shape bias)\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. VQA(Visual Question Answering) \uc791\uc5c5\uacfc \uc774\ubbf8\uc9c0 \uc124\uba85 \uc791\uc5c5\uc5d0\uc11c VLMs\uc758 \ud615\ud0dc \ud3b8\ud5a5\uc744 \uce21\uc815\ud558\uc5ec \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4. \ub300\ubd80\ubd84\uc758 VLMs\ub294 \uc57d\uac04 \ud615\ud0dc \uc911\uc2ec\uc801\uc774\uc9c0\ub9cc, \uc774\ubbf8\uc9c0\ub97c \uc124\uba85\ud558\ub77c\ub294 \uc694\uccad\uc744 \ubc1b\uc558\uc744 \ub54c VQA\uc640 \ube44\uad50\ud558\uc5ec \ucc28\uc774\ub97c \ubcf4\uc774\ub294 \ubaa8\ub378\ub4e4\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.  \ube44\uad50\ub97c \uc704\ud574 \ucd08\uae30 \uc9c8\uac10-\ud615\ud0dc \ud3b8\ud5a5 \uc5f0\uad6c\uc5d0\uc11c \uc0ac\uc6a9\ub41c ResNet-50 [59], \uc81c\ub85c\uc0f7 \ubd84\ub958(CLIP ViT-L/14 [11]), \uadf8\ub9ac\uace0 10\uba85\uc758 \uc0ac\ub78c\uc758 \ud3c9\uade0 \uacb0\uacfc\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ud588\uc2b5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 VLMs\uac00 \ud615\ud0dc \uc815\ubcf4\uc5d0 \ub354 \uc758\uc874\ud558\ub294 \uacbd\ud5a5\uc744 \ubcf4\uc774\uc9c0\ub9cc, \uc0ac\ub78c\uc758 \ud615\ud0dc \ud3b8\ud5a5(96%)\uc5d0\ub294 \ubbf8\uce58\uc9c0 \ubabb\ud558\uba70, \uc791\uc5c5 \uc720\ud615\uc5d0 \ub530\ub77c \ud615\ud0dc \ud3b8\ud5a5\uc774 \ub2ec\ub77c\uc9c8 \uc218 \uc788\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 Are VLMs Biased towards Texture or Shape?"}, {"figure_path": "https://arxiv.org/html/2403.09193/x3.png", "caption": "(a)", "description": "\uadf8\ub9bc (a)\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uc608\uce21 \uc2e0\ub8b0\ub3c4\uc5d0 \ub530\ub978 \ud14d\uc2a4\ucc98 \ubc0f \ubaa8\uc591 \ud1a0\ud070\uc758 \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \uc0d8\ud50c\uc5d0 \ub300\ud55c \ud14d\uc2a4\ucc98\uc640 \ubaa8\uc591 \ud1a0\ud070\uc758 \uc2e0\ub8b0\ub3c4 \ubd84\ud3ec, \uc815\ub2f5 \uc608\uce21(\uc989, \uc608\uce21 \ub808\uc774\ube14\uc774 \ubaa8\uc591 \ub610\ub294 \uc9c8\uac10\uacfc \uc77c\uce58)\uc5d0\uc11c\uc758 \uc2e0\ub8b0\ub3c4 \ubd84\ud3ec, \uc2e0\ub8b0\ub3c4 \uc784\uacc4\uac12\uc744 \ubcc0\uacbd\ud588\uc744 \ub54c \ubaa8\uc591 \ud3b8\ud5a5\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc774 \uc5b4\ub5bb\uac8c \uacb0\uc815\uc744 \ub0b4\ub9ac\ub294\uc9c0, \uadf8\ub9ac\uace0 \uc2e0\ub8b0\ub3c4\uac00 \uacb0\uc815\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \uac00\uc7a5 \ub192\uc740 \uc2e0\ub8b0\ub3c4\uc758 \ub2f5\ubcc0\uc740 \ubaa8\uc591 \ub2e8\uc11c\uc5d0\ub9cc \ud574\ub2f9\ub41c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc774 \uc911\uc694\ud569\ub2c8\ub2e4.", "section": "4.2 \uc758\uc0ac\uacb0\uc815 \ud615\uc131 \uac80\ud1a0"}, {"figure_path": "https://arxiv.org/html/2403.09193/x4.png", "caption": "(b)", "description": "\uadf8\ub9bc (b)\ub294 \uc815\ud655\ud55c \uc608\uce21(\uc989, \uc608\uce21 \ub808\uc774\ube14\uc774 \ubaa8\uc591 \ub610\ub294 \uc9c8\uac10\uacfc \uc77c\uce58)\uc5d0 \ub300\ud55c \uc2e0\ub8b0\ub3c4 \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378\uc774 \uc751\ub2f5\uc5d0 \uc0ac\uc6a9\ud55c \ub2e8\uc11c\uc5d0 \ub300\ud55c \ub9e4\uc6b0 \ub192\uc740 \uc2e0\ub8b0\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc774 \ud2b9\uc9d5\uc785\ub2c8\ub2e4. \uc774\ub294 \ub2e4\ub978 \ub2e8\uc11c\uc5d0\uc11c \ub098\uc628 \uc815\ubcf4\uac00 \uc2e4\uc81c\ub85c \ucc98\ub9ac \uacfc\uc815\uc5d0\uc11c \uc190\uc2e4\ub41c\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  \ub450 \ubc88\uc9f8(\uc0c1\ub2f9\ud788 \uc2e0\ub8b0\ub3c4\uac00 \ub0ae\uc740) \uc608\uce21 \ud1a0\ud070\uc740 \uc885\uc885(70.7%) \uc0c1\ubc18\ub418\ub294 \ub2e8\uc11c\uc5d0 \uc18d\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.  \uc0c1\uc704 2\uac1c \uc30d \uc911 17.7%\ub9cc\uc774 \ubaa8\uc591\uacfc \uc9c8\uac10 \ubaa8\ub450\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. \ud3c9\uade0\uc801\uc73c\ub85c \ubaa8\ub378\uc740 \uc9c8\uac10 \uacb0\uc815\uc5d0 \ub300\ud574 \uc57d\uac04 \ub35c \ud655\uc2e0\ud558\ub294 \uacbd\ud5a5\uc774 \uc788\uc2b5\ub2c8\ub2e4(p=0.87 \ub300 0.92). \uc2e0\ub8b0\ub3c4 \uc784\uacc4\uac12 \uc774\uc0c1\uc758 \uc751\ub2f5\ub9cc \ubd84\uc11d\ud560 \ub54c \uc774\ub294 \ub354\uc6b1 \ubd84\uba85\ud574\uc9d1\ub2c8\ub2e4.  p=0.5\uae4c\uc9c0\ub294 \uce21\uc815\uc5d0 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce58\uc9c0 \uc54a\uc9c0\ub9cc, \uadf8 \uc774\ud6c4\uc5d0\ub294 \uc9c8\uac10 \uacb0\uc815\uc758 \ube44\uc728\uc774 \uac10\uc18c\ud558\uc9c0 \uc54a\uace0 \ubaa8\uc591 \uacb0\uc815\uc758 \ube44\uc728\uc774 \uc99d\uac00\ud558\uc5ec \ubaa8\uc591 \ud3b8\ud5a5\uc774 \ucee4\uc9d1\ub2c8\ub2e4.  p=0.9\ub97c \ub118\uc5b4\uc11c\uba74 \ubaa8\ub378\uc740 \ubaa8\uc591 \ub2e8\uc11c\ub9cc\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucd94\ub860\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacb0\uacfc\uac00 \ub2e4\ub978 \ubaa8\ub378\uc5d0\ub3c4 \uc77c\ubc18\ud654\ub418\ub294\uc9c0\ub294 \ubd88\ubd84\uba85\ud558\uc9c0\ub9cc, LLaVA\ub294 \ud604\uc7ac \ub9ce\uc740 \ud6c4\uc18d \ubaa8\ub378\uc758 \uae30\ubcf8\uc774 \ub418\ubbc0\ub85c \uc774\ub7ec\ud55c \ud3b8\ud5a5\uc744 \uadf8\ub300\ub85c \uc774\uc5b4\ubc1b\uc744 \uac00\ub2a5\uc131\uc774 \ub192\uc2b5\ub2c8\ub2e4.", "section": "4.2 \uc758\uc0ac\uacb0\uc815 \ud615\uc131 \uacfc\uc815 \uc0b4\ud3b4\ubcf4\uae30"}, {"figure_path": "https://arxiv.org/html/2403.09193/x5.png", "caption": "(c)", "description": "\uadf8\ub9bc 3\uc740 VLMs\uc774 \ud55c \uac00\uc9c0 \ud2b9\uc815 \ub2e8\uc11c\ub97c \uc120\ud638\ud558\uace0 \ub2e4\ub978 \ub2e8\uc11c\ub97c \ubb34\uc2dc\ud558\ub294 \ud14c\uc2a4\ud2b8\uc5d0\uc11c \ub192\uc740 \uc2e0\ub8b0\ub3c4\uc758 \ub2f5\ubcc0\uc744 \uc81c\uacf5\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc2e0\ub8b0\ub3c4\ub294 \ub2e8\uc11c\uc5d0 \ub530\ub77c \uc57d\uac04 \ub2e4\ub974\uc9c0\ub9cc, \uac00\uc7a5 \ub192\uc740 \uc2e0\ub8b0\ub3c4\ub97c \uac00\uc9c4 \ub2f5\ubcc0\uc740 \uc804\uc801\uc73c\ub85c \ubaa8\uc591 \ub2e8\uc11c\uc5d0 \uc18d\ud569\ub2c8\ub2e4. (a) \ubaa8\ub4e0 \uc0d8\ud50c\uc5d0 \ub300\ud55c \ubaa8\uc591 \ubc0f \uc9c8\uac10 \ud1a0\ud070\uc758 \uc2e0\ub8b0\ub3c4 \ubd84\ud3ec, (b) \uc815\ud655\ud55c \uc608\uce21(\uc989, \uc608\uce21\ub41c \ub808\uc774\ube14\uc774 \ubaa8\uc591 \ub610\ub294 \uc9c8\uac10\uacfc \uc77c\uce58\ud558\ub294 \uacbd\uc6b0)\uc758 \uc2e0\ub8b0\ub3c4 \ubd84\ud3ec, (c) \uc2e0\ub8b0\ub3c4\ub97c \ud544\ud130\ub9c1\ud560 \ub54c \ubaa8\uc591 \ud3b8\ud5a5\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. LLaVA-NeXT 7B\uc758 VQA \uc791\uc5c5\uc5d0 \ub300\ud574 \uce21\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "4.2 \uc758\uc0ac\uacb0\uc815 \ud615\uc131 \uacfc\uc815 \uc0b4\ud3b4\ubcf4\uae30"}, {"figure_path": "https://arxiv.org/html/2403.09193/x6.png", "caption": "Figure 3: VLMs provide high-confidence answers in our test that prefer one specific cue and ignore the other. Confidence only slightly differs by cue, but answers with the highest confidence exclusively belong to shape decisions. (a) Confidence distribution of shape and texture tokens for all samples, (b) Confidence distribution in correct predictions (i.e., the predicted label matches either shape or texture), (c) Evolution of shape bias if filtering for confidence. Measured on LLaVA-NeXT 7B for the VQA task.", "description": "\uadf8\ub9bc 3\uc740 \uc2dc\uac01 \uc5b8\uc5b4 \ubaa8\ub378(VLMs)\uc774 \ub192\uc740 \uc2e0\ub8b0\ub3c4\ub85c \ub2f5\ubcc0\uc744 \uc81c\uc2dc\ud558\uba70 \ud2b9\uc815 \ub2e8\uc11c\ub97c \uc120\ud638\ud558\uace0 \ub2e4\ub978 \ub2e8\uc11c\ub97c \ubb34\uc2dc\ud558\ub294 \uacbd\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc2e0\ub8b0\ub3c4\ub294 \ub2e8\uc11c\uc5d0 \ub530\ub77c \uc57d\uac04\uc529 \ub2e4\ub974\uc9c0\ub9cc, \uac00\uc7a5 \ub192\uc740 \uc2e0\ub8b0\ub3c4\ub97c \uac00\uc9c4 \ub2f5\ubcc0\uc740 \ubaa8\uc591 \ub2e8\uc11c\uc5d0\ub9cc \uad6d\ud55c\ub429\ub2c8\ub2e4. (a)\ub294 \ubaa8\ub4e0 \ud45c\ubcf8\uc5d0 \ub300\ud55c \ubaa8\uc591 \ubc0f \uc9c8\uac10 \ud1a0\ud070\uc758 \uc2e0\ub8b0\ub3c4 \ubd84\ud3ec\ub97c, (b)\ub294 \uc62c\ubc14\ub978 \uc608\uce21(\uc989, \uc608\uce21 \ub808\uc774\ube14\uc774 \ubaa8\uc591 \ub610\ub294 \uc9c8\uac10\uacfc \uc77c\uce58)\uc5d0 \ub300\ud55c \uc2e0\ub8b0\ub3c4 \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (c)\ub294 \uc2e0\ub8b0\ub3c4\ub97c \ud544\ud130\ub9c1\ud560 \uacbd\uc6b0 \ubaa8\uc591 \ud3b8\ud5a5\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. LLaVA-NeXT 7B \ubaa8\ub378\uc758 VQA \uc791\uc5c5\uc744 \uae30\uc900\uc73c\ub85c \uce21\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "4 Are VLMs Biased towards Texture or Shape?"}, {"figure_path": "https://arxiv.org/html/2403.09193/x7.png", "caption": "Figure 4: VLMs make similar errors on the cue-conflict datasets and share similarities with their vision encoders. In terms of errors, VLMs are also more similar to humans than ImageNet-trained/finetuned models. We measure the pair-wise error consistency [71] between predictions. For this analysis, an error is any answer that does not belong to the shape class (analogous to [29]).\nShown responses belong to LLM-based VLMs (under the VQA task), other selected models including ImageNet models, (some) VLM encoders under ImageNet-finetuning and zero-shot classification, and ten human subjects.", "description": "\uadf8\ub9bc 4\ub294 \ub2e4\uc591\ud55c \ube44\uc804 \uc5b8\uc5b4 \ubaa8\ub378(VLM), \uc774\ubbf8\uc9c0\ub137 \uae30\ubc18 \ubaa8\ub378, \uc774\ubbf8\uc9c0\ub137 \ubbf8\uc138 \uc870\uc815 \ubc0f \uc81c\ub85c\uc0f7 \ubd84\ub958\ub97c \uac70\uce5c VLM \uc778\ucf54\ub354, \uadf8\ub9ac\uace0 10\uba85\uc758 \uc0ac\ub78c\uc758 \uc751\ub2f5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc751\ub2f5\uc740 VQA \uc791\uc5c5\uc5d0\uc11c \uc218\ud589\ub418\uc5c8\uc73c\uba70, \uadf8\ub9bc\uc5d0\uc11c \"\uc624\ub958 \uc77c\uad00\uc131\"(Error Consistency)\uc774\ub77c\ub294 \uc9c0\ud45c\ub97c \ud1b5\ud574 \ubaa8\ub378 \uac04 \uc624\ub958\uc758 \uc720\uc0ac\uc131\uc744 \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4. \uc624\ub958\ub294 \ud615\ud0dc \ud074\ub798\uc2a4\uc5d0 \uc18d\ud558\uc9c0 \uc54a\ub294 \ubaa8\ub4e0 \uc751\ub2f5\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 VLM\uc774 \uc774\ubbf8\uc9c0\ub137 \uae30\ubc18 \ubaa8\ub378\ubcf4\ub2e4 \uc0ac\ub78c\uacfc \uc720\uc0ac\ud55c \uc624\ub958\ub97c \ubc94\ud55c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c VLM\uc758 \uc624\ub958\ub294 \uadf8\ub4e4\uc758 \ube44\uc804 \uc778\ucf54\ub354\uc640 \uc720\uc0ac\uc131\uc744 \uacf5\uc720\ud568\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. VLMs\ub294 \uc9c8\uac10 \ub610\ub294 \ud615\ud0dc\uc5d0 \uce58\uc6b0\uccd0\uc838 \uc788\uc2b5\ub2c8\uae4c?"}, {"figure_path": "https://arxiv.org/html/2403.09193/x8.png", "caption": "Figure 5: Image preprocessing can strongly steer texture/shape bias. Left: Shuffling image patches with decreasing patch size results in a strong texture bias, Right: Increasing Gaussian noise introduces a strong shape bias.", "description": "\uadf8\ub9bc 5\ub294 \uc774\ubbf8\uc9c0 \uc804\ucc98\ub9ac\uac00 \uc9c8\uac10-\ud615\ud0dc \ud3b8\ud5a5\uc5d0 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce60 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd \ud328\ub110\uc740 \uc774\ubbf8\uc9c0 \ud328\uce58\ub97c \ubb34\uc791\uc704\ub85c \uc11e\uace0 \ud328\uce58 \ud06c\uae30\ub97c \uc904\uc774\uba74 \uc9c8\uac10 \uc815\ubcf4\ub294 \uc720\uc9c0\ub418\uc9c0\ub9cc \ud615\ud0dc \uc815\ubcf4\uac00 \uc190\uc2e4\ub418\uc5b4 \uc9c8\uac10 \ud3b8\ud5a5\uc774 \uac15\ud574\uc9d0\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc624\ub978\ucabd \ud328\ub110\uc740 \uac00\uc6b0\uc2dc\uc548 \ub178\uc774\uc988\ub97c \uc99d\uac00\uc2dc\ud0a4\uba74 \ud615\ud0dc \uc815\ubcf4\ub294 \uc720\uc9c0\ub418\uc9c0\ub9cc \uc9c8\uac10 \uc815\ubcf4\uac00 \uc190\uc2e4\ub418\uc5b4 \ud615\ud0dc \ud3b8\ud5a5\uc774 \uac15\ud574\uc9d0\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ud328\uce58 \ud06c\uae30\uc640 \uac00\uc6b0\uc2dc\uc548 \ub178\uc774\uc988 \uac15\ub3c4\uc5d0 \ub530\ub978 \uc9c8\uac10-\ud615\ud0dc \ud3b8\ud5a5\uc758 \ubcc0\ud654\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.1 \ube44\uc804 \uc81c\uc5b4"}, {"figure_path": "https://arxiv.org/html/2403.09193/x9.png", "caption": "Figure 6: Prompts can steer the texture/shape bias to some extent. We test the same texture/shape-biased instructions on multiple models, showing that these can already shift some decisions (usually in favor of texture). For InternVL 1.1 and LLaVA-NeXT 7B we additionally test the understanding of texture/shape by using synonyms. Furthermore, we use an LLM to automatically search for specific prompts to optimize in either direction.", "description": "\uadf8\ub9bc 6\uc740 \ud504\ub86c\ud504\ud2b8\uac00 \uc9c8\uac10/\ud615\ud0dc \ud3b8\ud5a5\uc744 \uc5b4\ub290 \uc815\ub3c4 \uc870\uc815\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc5ec\ub7ec \ubaa8\ub378\uc5d0\uc11c \ub3d9\uc77c\ud55c \uc9c8\uac10/\ud615\ud0dc \ud3b8\ud5a5 \uc9c0\uc2dc\ubb38\uc744 \ud14c\uc2a4\ud2b8\ud558\uc5ec \uc77c\ubd80 \uacb0\uc815(\uc8fc\ub85c \uc9c8\uac10 \ucabd\uc73c\ub85c)\uc744 \uc774\ubbf8 \ubcc0\uacbd\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. InternVL 1.1 \ubc0f LLaVA-NeXT 7B\uc758 \uacbd\uc6b0 \ub3d9\uc758\uc5b4\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc9c8\uac10/\ud615\ud0dc\uc5d0 \ub300\ud55c \uc774\ud574\ub3c4\ub97c \ucd94\uac00\ub85c \ud14c\uc2a4\ud2b8\ud588\uc2b5\ub2c8\ub2e4. \ub610\ud55c, LLM\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5b4\ub290 \ubc29\ud5a5\uc73c\ub85c\ub4e0 \ucd5c\uc801\ud654\ud560 \ud2b9\uc815 \ud504\ub86c\ud504\ud2b8\ub97c \uc790\ub3d9\uc73c\ub85c \uac80\uc0c9\ud588\uc2b5\ub2c8\ub2e4.", "section": "5.2 \uc5b8\uc5b4\uc5d0\uc11c \uc870\uc815: \ud504\ub86c\ud504\ud2b8 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1"}, {"figure_path": "https://arxiv.org/html/2403.09193/x10.png", "caption": "Figure 7: Detailed shape bias measurements under synonyms for biased VQA prompts.", "description": "\uadf8\ub9bc 7\uc740 \ud3b8\ud5a5\ub41c VQA \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud55c \ub3d9\uc758\uc5b4\ub97c \uc0ac\uc6a9\ud55c \uc138\ubd80\uc801\uc778 \ud615\ud0dc \ud3b8\ud5a5 \uce21\uc815 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 '\uc9c8\uac10' \ub610\ub294 '\ud615\ud0dc'\ub77c\ub294 \ub2e8\uc5b4 \ub300\uc2e0 \ub3d9\uc758\uc5b4\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc9c8\uac10 \ub610\ub294 \ud615\ud0dc\uc5d0 \ub300\ud55c \uc5b8\uae09 \ubc29\uc2dd\uc744 \ubc14\uafb8\uc5c8\uc744 \ub54c, \ubaa8\ub378\uc758 \ud615\ud0dc \ud3b8\ud5a5\uc774 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub3d9\uc758\uc5b4\uc5d0 \ub300\ud55c \ud615\ud0dc \ud3b8\ud5a5\uc758 \ubcc0\ud654\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \uc81c\uc2dc\ud558\uba70,  \ud2b9\uc815 \ub3d9\uc758\uc5b4\uac00 \ud615\ud0dc \ub610\ub294 \uc9c8\uac10 \uc911 \uc5b4\ub290 \ucabd\uc5d0 \ub354 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ubaa8\ub378\uc5d0\uc11c \ud615\ud0dc \ud3b8\ud5a5\uc5d0 \ub300\ud55c \ub3d9\uc758\uc5b4 \uc0ac\uc6a9\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90c\uc73c\ub85c\uc368, \uc5b8\uc5b4\uc801 \ubcc0\ud654\ub97c \ud1b5\ud55c \uc2dc\uac01\uc801 \ud3b8\ud5a5 \uc870\uc815 \uac00\ub2a5\uc131\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "5.2 \uc5b8\uc5b4\uc801 \uc870\uc815: \ud504\ub86c\ud504\ud2b8 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1"}, {"figure_path": "https://arxiv.org/html/2403.09193/x11.png", "caption": "Figure 8: Temperature scaling has no significant effect on shape bias neither under VQA (left) nor Image Captioning (right) tasks but starts to decrease accuracy at higher levels. Experiments performed on LLaVA-NeXT 7B with 3 seeds (except Temperature = 0 and Temperature = 1 of Image Captioning where we use a single seed).", "description": "\ubcf8 \uadf8\ub9bc\uc740 \uc628\ub3c4 \uc870\uc808\uc774 VQA\uc640 \uc774\ubbf8\uc9c0 \ucea1\uc158 \uc791\uc5c5 \ubaa8\ub450\uc5d0\uc11c \ud615\ud0dc \ud3b8\ud5a5\uc5d0 \uc720\uc758\ubbf8\ud55c \uc601\ud5a5\uc744 \ubbf8\uce58\uc9c0 \uc54a\uc9c0\ub9cc, \ub192\uc740 \uc218\uc900\uc5d0\uc11c\ub294 \uc815\ud655\ub3c4\ub97c \ub5a8\uc5b4\ub728\ub9b0\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. LLaVA-NeXT 7B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud5d8\uc744 \uc218\ud589\ud588\uc73c\uba70, \uc774\ubbf8\uc9c0 \ucea1\uc158 \uc791\uc5c5\uc758 \uc628\ub3c4\uac00 0 \ub610\ub294 1\uc778 \uacbd\uc6b0\ub97c \uc81c\uc678\ud558\uace0\ub294 3\uac1c\uc758 \uc2dc\ub4dc\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uc628\ub3c4\uac00 \ub192\uc544\uc9d0\uc5d0 \ub530\ub77c \ubaa8\ub378\uc758 \ucc3d\uc758\uc131\uc774 \uc99d\uac00\ud558\uc9c0\ub9cc, \ub3d9\uc2dc\uc5d0 \ube44\ud604\uc2e4\uc801\uc778 \uacb0\uacfc\ub97c \uc0dd\uc131\ud560 \uac00\ub2a5\uc131\ub3c4 \ucee4\uc9d1\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \uc628\ub3c4 \ubcc0\ud654\uc5d0 \ub530\ub978 \ud615\ud0dc \ud3b8\ud5a5\uacfc \uc815\ud655\ub3c4 \ubcc0\ud654\ub97c \uadf8\ub798\ud504\ub85c \ub098\ud0c0\ub0b4\uc5b4, \uc628\ub3c4 \uc870\uc808\uc774 \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 Are VLMs Biased towards Texture or Shape?"}, {"figure_path": "https://arxiv.org/html/2403.09193/x12.png", "caption": "(a) VQA", "description": "\uadf8\ub9bc 9\ub294 \ub2e4\uc591\ud55c VLMs\uc5d0 \ub300\ud55c \uc9c8\uac10 \ub300 \ud615\ud0dc \ud3b8\ud5a5\uc744 VQA(\uc67c\ucabd) \ubc0f \uc774\ubbf8\uc9c0 \uc790\ub9c9 \uc0dd\uc131(\uc624\ub978\ucabd) \uc791\uc5c5\uc5d0 \ub300\ud574 \ud074\ub798\uc2a4\ubcc4\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc810\uc740 \ud2b9\uc815 \ud074\ub798\uc2a4\uc758 \uc9c8\uac10 \ubc0f \ud615\ud0dc \uacb0\uc815 \ube44\uc728\uc744 \ub098\ud0c0\ub0b4\uba70, \ubaa8\ub378\uc774 \uc5b4\ub5a4 \uce21\uba74\uc5d0 \ub354 \uc911\uc810\uc744 \ub450\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd \ud328\ub110(VQA)\uc5d0\uc11c\ub294 \uc9c8\uac10 \ub300 \ud615\ud0dc \uacb0\uc815 \ube44\uc728\uc774 \ubaa8\ub378\uacfc \uc791\uc5c5 \uac04\uc5d0 \uc0c1\ub2f9\ud788 \ub2e4\ub974\uac8c \ub098\ud0c0\ub0a9\ub2c8\ub2e4. \uc77c\ubd80 \ubaa8\ub378\uc740 \ud615\ud0dc\uc5d0 \ub354 \uc911\uc810\uc744 \ub450\uace0, \ub2e4\ub978 \ubaa8\ub378\uc740 \uc9c8\uac10\uc5d0 \ub354 \uc911\uc810\uc744 \ub461\ub2c8\ub2e4. \uc624\ub978\ucabd \ud328\ub110(\uc774\ubbf8\uc9c0 \uc790\ub9c9 \uc0dd\uc131)\uc5d0\uc11c\ub294 \ubaa8\ub378 \uac04 \uc77c\uad00\uc131\uc774 \ub354 \ub192\uc73c\uba70, \ub300\ubd80\ubd84\uc758 \ubaa8\ub378\uc5d0\uc11c \ud615\ud0dc\uc5d0 \ub300\ud55c \uac15\ud55c \ud3b8\ud5a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \uc11c\ub85c \ub2e4\ub978 \ubaa8\ub378\uc758 \uacb0\uc815 \uacfc\uc815\uc5d0\uc11c\uc758 \ucc28\uc774\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\uba70, VQA\uc640 \uc774\ubbf8\uc9c0 \uc790\ub9c9 \uc0dd\uc131 \uc791\uc5c5 \uac04\uc758 \ucc28\uc774\uc810\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "4.1 \uc8fc\uc694 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2403.09193/x13.png", "caption": "(b) Image Captioning", "description": "\uadf8\ub9bc (b)\ub294 \ub17c\ubb38\uc758 4.1\uc808 \"\uc8fc\uc694 \uacb0\uacfc\" \uc139\uc158\uc5d0 \uc788\ub294 \uadf8\ub9bc\uc73c\ub85c, \ub2e4\uc591\ud55c VLMs(Vision Language Models)\uc758 \ud14d\uc2a4\ucc98-\uc250\uc774\ud504 \ud3b8\ud5a5(texture-shape bias)\uc744 \uc774\ubbf8\uc9c0 \ucea1\uc154\ub2dd \uc791\uc5c5\uc5d0\uc11c \uce21\uc815\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 \uc250\uc774\ud504(shape) \uacb0\uc815 \ube44\uc728\uc744, y\ucd95\uc740 \ud14d\uc2a4\ucc98(texture) \uacb0\uc815 \ube44\uc728\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uac01 \uc810\uc740 \ud2b9\uc815 VLM \ubaa8\ub378\uc744 \ub098\ud0c0\ub0b4\uba70, \uc810\uc758 \uc704\uce58\ub294 \ud574\ub2f9 \ubaa8\ub378\uc774 \uc774\ubbf8\uc9c0\ub97c \ucea1\uc154\ub2dd\ud560 \ub54c \uc250\uc774\ud504 \uc815\ubcf4\uc640 \ud14d\uc2a4\ucc98 \uc815\ubcf4 \uc911 \uc5b4\ub290 \ucabd\uc5d0 \ub354 \uc758\uc874\ud558\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc810\uc774 x\ucd95\uc5d0 \uac00\uae4c\uc6b8\uc218\ub85d \uc250\uc774\ud504\uc5d0 \ub354 \uc758\uc874\ud558\uace0, y\ucd95\uc5d0 \uac00\uae4c\uc6b8\uc218\ub85d \ud14d\uc2a4\ucc98\uc5d0 \ub354 \uc758\uc874\ud568\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c VLM \ubaa8\ub378\ub4e4\uc774 \uc774\ubbf8\uc9c0 \ucea1\uc154\ub2dd \uc791\uc5c5\uc5d0\uc11c \uc250\uc774\ud504\uc5d0 \ub300\ud55c \ud3b8\ud5a5\uc744 \ubcf4\uc774\ub294 \uc815\ub3c4\uac00 \ub2e4\ub984\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c \uc778\uac04\uc758 \uc250\uc774\ud504 \ud3b8\ud5a5(96%)\uacfc \ube44\uad50\ud558\uc5ec VLM \ubaa8\ub378\ub4e4\uc758 \ud3b8\ud5a5 \uc815\ub3c4\ub97c \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370\uc5d0\ub3c4 \ud65c\uc6a9\ub429\ub2c8\ub2e4.", "section": "4.1 \uc8fc\uc694 \uacb0\uacfc"}]