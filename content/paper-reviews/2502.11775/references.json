{"references": [{"fullname_first_author": "Cobbe, K.", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-26", "reason": "This paper introduced a novel approach to enhancing reasoning in LLMs by training verifiers, which significantly influenced the development of reasoning optimization methods discussed in the target paper."}, {"fullname_first_author": "Uesato, J.", "paper_title": "Solving math word problems with process- and outcome-based feedback", "publication_date": "2022-11-18", "reason": "This paper introduced process reward models (PRMs) that significantly advanced reasoning optimization in LLMs and directly inspired the development of the process DPO method in the target paper."}, {"fullname_first_author": "Wang, P.", "paper_title": "Math-Shepherd: Verify and reinforce LLMs step-by-step without human annotations", "publication_date": "2024-00-00", "reason": "This paper presented Math-Shepherd, a method for improving LLM reasoning without manual annotations, which provided valuable insights and alternative approaches to reasoning optimization explored in the target paper."}, {"fullname_first_author": "Zhang, R.", "paper_title": "Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward", "publication_date": "2024-04-09", "reason": "This paper introduced direct preference optimization (DPO), a key method in the target paper's reasoning enhancement technique, providing a crucial foundation for the proposed pDPO method."}, {"fullname_first_author": "Li, B.", "paper_title": "LLaVA-OneVision: Easy visual task transfer", "publication_date": "2024-08-07", "reason": "This paper introduced LLaVA-OneVision, a strong baseline model in visual language models, against which the target paper's video-SALMONN-01 model is compared and improved upon."}]}