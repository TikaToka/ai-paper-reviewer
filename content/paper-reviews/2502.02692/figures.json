[{"figure_path": "https://arxiv.org/html/2502.02692/extracted/6178801/Figures/Opportunities.jpg", "caption": "Figure 1: Opportunities for Intelligent Sensing-to-Action: In sensing-to-action loops, significant gains can be achieved by selectively sensing critical environmental regions while predicting less critical areas based on training data. This frugal sensing strategy is especially beneficial for resource-intensive modalities, such as LiDAR, enhancing task accuracy without unnecessary overhead. Similarly, action-to-sensing optimizations can adjust control variables to opportunistically reduce sensing demands based on task relevance. While these frameworks improve loop efficiency, ensuring reliability requires robust and computationally efficient monitors to continuously assess fidelity and support aggressive optimizations. In multi-agent sensing-action loops, agents can collaborate by sharing sensing tasks or complementing each other\u2019s sensing capabilities. Moreover, emerging paradigms, such as neuromorphic sensing-action loops, offer unified frameworks by adapting sensing and processing rates based on event dynamics, enabling seamless sensing and control.", "description": "\uadf8\ub9bc 1\uc740 \uc9c0\ub2a5\ud615 \uac10\uc9c0-\ud589\ub3d9 \ub8e8\ud504\uc758 \uae30\ud68c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud6c8\ub828 \ub370\uc774\ud130\ub97c \uae30\ubc18\uc73c\ub85c \uc911\uc694\ud55c \ud658\uacbd \uc601\uc5ed\uc744 \uc120\ud0dd\uc801\uc73c\ub85c \uac10\uc9c0\ud558\uace0 \ub35c \uc911\uc694\ud55c \uc601\uc5ed\uc740 \uc608\uce21\ud568\uc73c\ub85c\uc368 \uc790\uc6d0 \uc9d1\uc57d\uc801\uc778 \ubaa8\ub2ec\ub9ac\ud2f0(\uc608: LiDAR)\uc758 \ud6a8\uc728\uc131\uc744 \ub192\uc774\uace0 \ubd88\ud544\uc694\ud55c \uc624\ubc84\ud5e4\ub4dc \uc5c6\uc774 \uc791\uc5c5 \uc815\ud655\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \ub9c8\ucc2c\uac00\uc9c0\ub85c, \uc561\uc158-\ud22c-\uc13c\uc2f1 \ucd5c\uc801\ud654\ub97c \ud1b5\ud574 \uc791\uc5c5 \uad00\ub828\uc131\uc5d0 \ub530\ub77c \uac10\uc9c0 \uc694\uad6c \uc0ac\ud56d\uc744 \ub3d9\uc801\uc73c\ub85c \uc904\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub7ec\ud55c \ud504\ub808\uc784\uc6cc\ud06c\ub294 \ub8e8\ud504 \ud6a8\uc728\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uc9c0\ub9cc, \uc2e0\ub8b0\uc131\uc744 \ubcf4\uc7a5\ud558\ub824\uba74 \uc815\ud655\ub3c4\ub97c \uc9c0\uc18d\uc801\uc73c\ub85c \ud3c9\uac00\ud558\uace0 \uacf5\uaca9\uc801\uc778 \ucd5c\uc801\ud654\ub97c \uc9c0\uc6d0\ud558\ub294 \uac15\ub825\ud558\uace0 \uacc4\uc0b0 \ud6a8\uc728\uc801\uc778 \ubaa8\ub2c8\ud130\uac00 \ud544\uc694\ud569\ub2c8\ub2e4.  \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uac10\uc9c0-\ud589\ub3d9 \ub8e8\ud504\uc5d0\uc11c \uc5d0\uc774\uc804\ud2b8\ub294 \uac10\uc9c0 \uc791\uc5c5\uc744 \uacf5\uc720\ud558\uac70\ub098 \uc11c\ub85c\uc758 \uac10\uc9c0 \uae30\ub2a5\uc744 \ubcf4\uc644\ud558\uc5ec \ud611\ub825\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \ub610\ud55c \ub274\ub85c\ubaa8\ud53d \uac10\uc9c0-\ud589\ub3d9 \ub8e8\ud504\uc640 \uac19\uc740 \uc0c8\ub85c\uc6b4 \ud328\ub7ec\ub2e4\uc784\uc740 \uc774\ubca4\ud2b8 \ub3d9\uc5ed\ud559\uc5d0 \ub530\ub77c \uac10\uc9c0 \ubc0f \ucc98\ub9ac \uc18d\ub3c4\ub97c \uc870\uc815\ud558\uc5ec \uc6d0\ud65c\ud55c \uac10\uc9c0 \ubc0f \uc81c\uc5b4\ub97c \uac00\ub2a5\ud558\uac8c \ud558\ub294 \ud1b5\ud569\ub41c \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "II. INTELLIGENT SENSING-TO-ACTION"}, {"figure_path": "https://arxiv.org/html/2502.02692/x1.png", "caption": "Figure 2: An end-to-end computing pipeline comparison sensing-processing-action loop between a biological and a neuromorphic system. In a biological system, inputs are perceived as changes in intensity (events and frames) and color (frames) by the eye. In contrast, a neuromorphic system uses frame cameras to capture analog intensity at low rates and event cameras to detect motion-induced variations, generating events. The brain\u2019s parallel and recurrent connections enable computation within memory. Neuromorphic system emulates this by combining ANNs, SNNs, and hybrid ANN-SNN models to balance accuracy and efficiency. These algorithms also benefit from hardware acceleration via in-memory (IMC) and near-memory (NMC) computing by efficiently implementing synaptic functionality and, work alongside CPU/GPU architectures to enhance efficiency and reduce latency.", "description": "\uadf8\ub9bc 2\ub294 \uc0dd\ubb3c\ud559\uc801 \uc2dc\uc2a4\ud15c\uacfc \ub274\ub85c\ubaa8\ud53d \uc2dc\uc2a4\ud15c \uac04\uc758 \uac10\uc9c0-\ucc98\ub9ac-\uc791\ub3d9 \ub8e8\ud504\ub97c \ube44\uad50\ud558\uc5ec \uc5d4\ub4dc-\ud22c-\uc5d4\ub4dc \ucef4\ud4e8\ud305 \ud30c\uc774\ud504\ub77c\uc778\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0dd\ubb3c\ud559\uc801 \uc2dc\uc2a4\ud15c\uc5d0\uc11c\ub294 \ub208\uc774 \uac15\ub3c4(\uc774\ubca4\ud2b8\uc640 \ud504\ub808\uc784)\uc640 \uc0c9\uc0c1(\ud504\ub808\uc784)\uc758 \ubcc0\ud654\ub97c \uac10\uc9c0\ud569\ub2c8\ub2e4. \ubc18\uba74\uc5d0 \ub274\ub85c\ubaa8\ud53d \uc2dc\uc2a4\ud15c\uc740 \ud504\ub808\uc784 \uce74\uba54\ub77c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub0ae\uc740 \uc18d\ub3c4\ub85c \uc544\ub0a0\ub85c\uadf8 \uac15\ub3c4\ub97c \ud3ec\ucc29\ud558\uace0, \uc774\ubca4\ud2b8 \uce74\uba54\ub77c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc6c0\uc9c1\uc784\uc5d0 \uc758\ud55c \ubcc0\ud654\ub97c \uac10\uc9c0\ud558\uc5ec \uc774\ubca4\ud2b8\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ub1cc\uc758 \ubcd1\ub82c \ubc0f \uc21c\ud658 \uc5f0\uacb0\uc740 \uba54\ubaa8\ub9ac \ub0b4\uc5d0\uc11c \uacc4\uc0b0\uc744 \uac00\ub2a5\ud558\uac8c \ud569\ub2c8\ub2e4. \ub274\ub85c\ubaa8\ud53d \uc2dc\uc2a4\ud15c\uc740 \uc815\ud655\uc131\uacfc \ud6a8\uc728\uc131\uc758 \uade0\ud615\uc744 \ub9de\ucd94\uae30 \uc704\ud574 ANN, SNN \ubc0f \ud558\uc774\ube0c\ub9ac\ub4dc ANN-SNN \ubaa8\ub378\uc744 \uacb0\ud569\ud558\uc5ec \uc774\ub97c \uc5d0\ubbac\ub808\uc774\ud2b8\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc54c\uace0\ub9ac\uc998\uc740 \uba54\ubaa8\ub9ac \ub0b4(IMC) \ubc0f \uadfc\uc811 \uba54\ubaa8\ub9ac(NMC) \ucef4\ud4e8\ud305\uc744 \ud1b5\ud55c \ud558\ub4dc\uc6e8\uc5b4 \uac00\uc18d\uc73c\ub85c\ubd80\ud130 \uc774\uc810\uc744 \uc5bb\uc5b4 \uc2dc\ub0c5\uc2a4 \uae30\ub2a5\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \uad6c\ud604\ud558\uace0 CPU/GPU \uc544\ud0a4\ud14d\ucc98\uc640 \ud568\uaed8 \uc791\ub3d9\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \ub192\uc774\uace0 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \uc904\uc785\ub2c8\ub2e4.", "section": "II. INTELLIGENT SENSING-TO-ACTION"}, {"figure_path": "https://arxiv.org/html/2502.02692/extracted/6178801/Figures/ICASSP_GenSense.png", "caption": "Figure 3: Generative Sensing: Sense only what you really need: Generative sensing optimizes resource use by focusing on essential environmental features, reducing unnecessary data collection and enhancing real-time responsiveness. For LiDAR proessing, in this approach, the input point cloud is voxelized and radially masked based on voxel distance from the sensor to minimize redundant information. A 3D spatially sparse convolutional encoder extracts latent features, while a decoder reconstructs the 3D scene, enabling efficient perception that supports adaptive sensing-to-action strategies.", "description": "\uadf8\ub9bc 3\uc740 \uc0dd\uc131\uc801 \uac10\uc9c0(Generative Sensing) \uac1c\ub150\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uae30\ubc95\uc740 \ubd88\ud544\uc694\ud55c \ub370\uc774\ud130 \uc218\uc9d1\uc744 \uc904\uc774\uace0 \uc2e4\uc2dc\uac04 \ubc18\uc751\uc131\uc744 \ub192\uc774\uae30 \uc704\ud574 \ud544\uc218\uc801\uc778 \ud658\uacbd \ud2b9\uc9d5\uc5d0 \uc9d1\uc911\ud558\uc5ec \uc790\uc6d0 \uc0ac\uc6a9\uc744 \ucd5c\uc801\ud654\ud569\ub2c8\ub2e4. LiDAR \ucc98\ub9ac\uc758 \uacbd\uc6b0 \uc785\ub825 \uc810\uad70(point cloud)\uc744 \ubcf5\uc140(voxel)\ub85c \ub098\ub204\uace0 \uc13c\uc11c\uc640\uc758 \uac70\ub9ac\uc5d0 \ub530\ub77c \ubc18\uacbd \ubc29\ud5a5\uc73c\ub85c \ub9c8\uc2a4\ud06c \ucc98\ub9ac\ud558\uc5ec \uc911\ubcf5 \uc815\ubcf4\ub97c \ucd5c\uc18c\ud654\ud569\ub2c8\ub2e4. 3\ucc28\uc6d0 \uacf5\uac04\uc801\uc73c\ub85c \ub4dc\ubb38\ub4dc\ubb38\ud55c(spatially sparse) \ud569\uc131\uacf1 \uc778\ucf54\ub354\ub294 \uc7a0\uc7ac \ud2b9\uc9d5(latent feature)\uc744 \ucd94\ucd9c\ud558\uace0, \ub514\ucf54\ub354\ub294 3\ucc28\uc6d0 \uc7a5\uba74\uc744 \uc7ac\uad6c\uc131\ud558\uc5ec \uc801\uc751\ud615 \uac10\uc9c0-\uc791\ub3d9 \uc804\ub7b5\uc744 \uc9c0\uc6d0\ud558\ub294 \ud6a8\uc728\uc801\uc778 \uc778\uc9c0\ub97c \uac00\ub2a5\ud558\uac8c \ud569\ub2c8\ub2e4. \uc989, \uc13c\uc11c\uac00 \ubaa8\ub4e0 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\ub294 \ub300\uc2e0, \uc911\uc694\ud55c \ub370\uc774\ud130\ub9cc \uc218\uc9d1\ud558\uace0 \ub098\uba38\uc9c0\ub294 \ubaa8\ub378\uc774 \uc608\uce21\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \ub192\uc774\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4.", "section": "III. OPTIMIZING \"SENSING-TO-ACTION\" PATHWAYS"}, {"figure_path": "https://arxiv.org/html/2502.02692/x2.png", "caption": "Figure 4: Our approach conditions visual representations on the task policy by incorporating contrastive spectral Koopman encoding and reinforcement learning (RL)-guided control. This high-level framework unifies perception and control, enabling task-aware sensing adjustments. The RoboKoop model leverages these representations to dynamically adjust sensing parameters based on control objectives. (Adapted from RoboKoop[18])", "description": "\uadf8\ub9bc 4\ub294 RoboKoop \ubaa8\ub378\uc758 \uace0\ucc28\uc6d0 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud504\ub808\uc784\uc6cc\ud06c\ub294 \ub300\uc870\uc801\uc778 \uc2a4\ud399\ud2b8\ub7f4 \ucff1\ub9cc \uc778\ucf54\ub529\uacfc \uac15\ud654 \ud559\uc2b5 \uae30\ubc18 \uc81c\uc5b4\ub97c \ud1b5\ud569\ud558\uc5ec \uc791\uc5c5 \uc815\ucc45\uc5d0 \uc2dc\uac01\uc801 \ud45c\ud604\uc744 \uc870\uac74\ud654\ud569\ub2c8\ub2e4.  \uc989, \uc2dc\uac01 \uc815\ubcf4\ub97c \uc791\uc5c5 \ubaa9\ud45c\uc5d0 \ub530\ub77c \uc870\uc815\ud558\uc5ec \uc778\uc2dd\uacfc \uc81c\uc5b4\ub97c \ud1b5\ud569\ud558\ub294 \uac83\uc785\ub2c8\ub2e4.  RoboKoop \ubaa8\ub378\uc740 \uc774\ub7ec\ud55c \ud45c\ud604\uc744 \ud65c\uc6a9\ud558\uc5ec \uc81c\uc5b4 \ubaa9\ud45c\uc5d0 \ub530\ub77c \uac10\uc9c0 \ub9e4\uac1c\ubcc0\uc218\ub97c \ub3d9\uc801\uc73c\ub85c \uc870\uc815\ud569\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uc2dc\uc2a4\ud15c\uc740 \uc791\uc5c5\uc5d0 \ud544\uc694\ud55c \uc815\ubcf4\ub9cc\uc744 \uc120\ud0dd\uc801\uc73c\ub85c \uac10\uc9c0\ud558\uace0, \ubd88\ud544\uc694\ud55c \uac10\uc9c0 \uc791\uc5c5\uc744 \uc904\uc784\uc73c\ub85c\uc368 \uc790\uc6d0\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "IV. OPTIMIZING \"ACTION-TO-SENSING\" PATHWAYS"}, {"figure_path": "https://arxiv.org/html/2502.02692/extracted/6178801/Figures/control_op.png", "caption": "Figure 5: (a) Computational load of state-of-the-art dynamical models. (b) Performance under external disturbances. (Adapted from RoboKoop[18])", "description": "\uadf8\ub9bc 5\ub294 \uc678\ubd80 \ud658\uacbd \ubcc0\ud654\uc5d0 \ub300\ud55c \uac15\uac74\uc131\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \ucd5c\ucca8\ub2e8 \ub3d9\uc801 \ubaa8\ub378\ub4e4\uc758 \uacc4\uc0b0 \ube44\uc6a9\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uadf8\ub798\ud504\uc774\uba70, (b)\ub294 \uc678\ubd80 \uac04\uc12d\uc774 \uc788\uc744 \ub54c \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. RoboKoop \ub17c\ubb38[18]\uc5d0\uc11c \ubc1c\ucdcc\ud55c \ub0b4\uc6a9\uc744 \ubc14\ud0d5\uc73c\ub85c \uc791\uc131\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  (a)\uc5d0\uc11c\ub294 \uc81c\uc548\ub41c \ubaa8\ub378(Spectral Koopman)\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \ud6e8\uc52c \uc801\uc740 \uacc4\uc0b0\ub7c9\uc73c\ub85c \ub3d9\uc791\ud568\uc744 \ubcf4\uc5ec\uc8fc\uba70, (b)\uc5d0\uc11c\ub294 \uc678\ubd80 \uac04\uc12d\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \uc81c\uc548\ub41c \ubaa8\ub378\uc774 \uac00\uc7a5 \uc548\uc815\uc801\uc778 \uc131\ub2a5\uc744 \uc720\uc9c0\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "IV. OPTIMIZING \"ACTION-TO-SENSING\" PATHWAYS"}, {"figure_path": "https://arxiv.org/html/2502.02692/extracted/6178801/Figures/plot_action_prob.png", "caption": "Figure 6: Ensuring Sensing-Action Loop Reliability: STARNet enhances the reliability of sensing-to-action loops by ingesting feature representations from primary task networks. A VAE models the typical distribution of these features, and during inference, STARNet uses gradient-free optimization to compute likelihood regret, identifying discrepancies between sensed and learned distributions to alert the system to potential inaccuracies.", "description": "\uadf8\ub9bc 6\uc740 STARNet\uc774 \uc8fc\uc694 \uc791\uc5c5 \ub124\ud2b8\uc6cc\ud06c\uc758 \ud2b9\uc9d5 \ud45c\ud604\uc744 \ud65c\uc6a9\ud558\uc5ec \uac10\uc9c0-\uc791\uc5c5 \ub8e8\ud504\uc758 \uc548\uc815\uc131\uc744 \ub192\uc774\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. VAE(Variational Autoencoder)\ub294 \uc774\ub7ec\ud55c \ud2b9\uc9d5\ub4e4\uc758 \uc77c\ubc18\uc801\uc778 \ubd84\ud3ec\ub97c \ubaa8\ub378\ub9c1\ud558\uace0, \ucd94\ub860 \uc911\uc5d0 STARNet\uc740 \uae30\uc6b8\uae30 \uc5c6\ub294 \ucd5c\uc801\ud654\ub97c \uc0ac\uc6a9\ud558\uc5ec Likelihood Regret\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uac10\uc9c0\ub41c \ubd84\ud3ec\uc640 \ud559\uc2b5\ub41c \ubd84\ud3ec \uac04\uc758 \ubd88\uc77c\uce58\ub97c \ud30c\uc545\ud558\uc5ec \uc2dc\uc2a4\ud15c\uc5d0 \uc7a0\uc7ac\uc801\uc778 \ubd80\uc815\ud655\uc131\uc744 \uc54c\ub9bd\ub2c8\ub2e4.  \uc989, STARNet\uc740 \uc2dc\uc2a4\ud15c\uc758 \uc2e0\ub8b0\ub3c4\ub97c \ub192\uc774\uae30 \uc704\ud574 \uc608\uc0c1\uce58 \ubabb\ud55c \uc624\ub958\ub098 \ubcc0\ud654\ub97c \uac10\uc9c0\ud558\uace0 \uc774\uc5d0 \ub300\uc751\ud558\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "V. SENSING-TO-ACTION LOOPS\uc758 \uc548\uc815\uc131"}, {"figure_path": "https://arxiv.org/html/2502.02692/x3.png", "caption": "Figure 7: Object Detection Accuracy for KITTI Dataset: The VAE-based approach analyzes LiDAR point clouds and object labels, producing bounding boxes for cars, pedestrians, and cyclists. The network was tested under challenging conditions, such as varying snow intensities and other corruptions.", "description": "\uadf8\ub9bc 7\uc740 KITTI \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec VAE \uae30\ubc18 \uc811\uadfc \ubc29\uc2dd\uc758 \uac1d\uccb4 \uac10\uc9c0 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uc811\uadfc \ubc29\uc2dd\uc740 LiDAR \uc810 \uad6c\ub984\uacfc \uac1d\uccb4 \ub808\uc774\ube14\uc744 \ubd84\uc11d\ud558\uc5ec \uc790\ub3d9\ucc28, \ubcf4\ud589\uc790, \uc790\uc804\uac70\ub97c \uc704\ud55c \ubc14\uc6b4\ub529 \ubc15\uc2a4\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ub124\ud2b8\uc6cc\ud06c\ub294 \ub2e4\uc591\ud55c \uac15\uc124 \uac15\ub3c4\uc640 \uae30\ud0c0 \uc190\uc0c1\uacfc \uac19\uc740 \uc5b4\ub824\uc6b4 \uc870\uac74\uc5d0\uc11c \ud14c\uc2a4\ud2b8\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc190\uc0c1 \uc720\ud615(\ub208, \ud06c\ub85c\uc2a4 \uc13c\uc11c, \ubaa8\uc158 \ube14\ub7ec \ub4f1) \ud558\uc5d0\uc11c\uc758 \uac1d\uccb4 \uac10\uc9c0 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ucc28\ud2b8\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. VAE \uae30\ubc18 \uc811\uadfc \ubc29\uc2dd\uc740 \ub2e4\uc591\ud55c \uc190\uc0c1 \uc720\ud615\uc5d0\uc11c\ub3c4 \uc0c1\ub2f9\ud55c \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud558\uba70, \ud2b9\ud788 \ub208\uc774 \ub9ce\uc774 \ub0b4\ub9ac\ub294 \uc0c1\ud669\uc5d0\uc11c\ub3c4 \uc131\ub2a5\uc774 \uc6b0\uc218\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "V. RELIABILITY OF SENSING-TO-ACTION LOOPS"}, {"figure_path": "https://arxiv.org/html/2502.02692/x4.png", "caption": "Figure 8: Neuromorphic sensing-action loop architectures. a) Full-ANN\u00a0[48], Full-SNN\u00a0[49], and Hybrid SNN-ANN\u00a0[50] models for optical flow estimation using event data. b) Fusion-FlowNet\u00a0[51] integrates event-based and frame-based modalities for enhanced feature extraction. Outputs are aggregated at the final SNN layer. (Adapted from Fusion-FlowNet\u00a0[51]).", "description": "\uadf8\ub9bc 8\uc740 \ub274\ub85c\ubaa8\ud53d \uac10\uc9c0-\uc561\uc158 \ub8e8\ud504 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \uc774\ubca4\ud2b8 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud55c \uad11\ud559 \ud750\ub984 \ucd94\uc815\uc744 \uc704\ud55c \uc804\ud1b5\uc801\uc778 \uc644\uc804 ANN [48], \uc644\uc804 SNN [49], \uadf8\ub9ac\uace0 \ud558\uc774\ube0c\ub9ac\ub4dc SNN-ANN [50] \ubaa8\ub378\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. (b)\ub294 \ud5a5\uc0c1\ub41c \ud2b9\uc9d5 \ucd94\ucd9c\uc744 \uc704\ud574 \uc774\ubca4\ud2b8 \uae30\ubc18 \ubc0f \ud504\ub808\uc784 \uae30\ubc18 \ubaa8\ub2ec\ub9ac\ud2f0\ub97c \ud1b5\ud569\ud558\ub294 Fusion-FlowNet [51]\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ucd9c\ub825\uc740 \ucd5c\uc885 SNN \uacc4\uce35\uc5d0\uc11c \uc9d1\uacc4\ub429\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 Fusion-FlowNet [51]\uc5d0\uc11c \uac01\uc0c9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "VI. \ub274\ub85c\ubaa8\ud53d \uc2dc\uc2a4\ud15c\uc758 \uac10\uc9c0-\uc561\uc158 \ub8e8\ud504"}, {"figure_path": "https://arxiv.org/html/2502.02692/x5.png", "caption": "Figure 9: Average Endpoint Error (AEE) comparison for Optical flow estimation on the MVSEC\u00a0[66] dataset. The left shows the Average Endpoint Error (AEE) for baseline models, EvFlow-Net (EvF)\u00a0[48], Spike-FlowNet (SpF)\u00a0[50], and Fusion-FlowNet (FF)\u00a0[51]). The right showcases how AEE varies with model size for Adaptive-SpikeNet and corresponding full-ANN models. (Adapted from Adaptive-SpikeNet\u00a0[49]).", "description": "\uadf8\ub9bc 9\ub294 MVSEC [66] \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uad11\ud559 \ud750\ub984 \ucd94\uc815\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc885\ucc29\uc810 \uc624\ucc28(AEE) \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc740 \uae30\uc900 \ubaa8\ub378, EvFlow-Net(EvF)[48], Spike-FlowNet(SpF)[50], Fusion-FlowNet(FF)[51]\uc758 AEE\ub97c \ubcf4\uc5ec\uc8fc\uace0, \uc624\ub978\ucabd\uc740 Adaptive-SpikeNet\uacfc \ud574\ub2f9\ud558\ub294 \uc804\uccb4 ANN \ubaa8\ub378\uc758 \ubaa8\ub378 \ud06c\uae30\uc5d0 \ub530\ub978 AEE \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc2e0\uacbd\ub9dd \uc544\ud0a4\ud14d\ucc98(\uc804\uccb4 ANN, \uc804\uccb4 SNN, \ud558\uc774\ube0c\ub9ac\ub4dc ANN-SNN)\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uace0, \uc774\ubca4\ud2b8 \uae30\ubc18 \uc13c\uc11c \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\ub294 Adaptive-SpikeNet\uc758 \ud6a8\uc728\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4. \ud2b9\ud788, Adaptive-SpikeNet\uc740 \ubaa8\ub378 \ud06c\uae30\uac00 \uc791\uc544\ub3c4 \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uc5b4 \uc5d0\ub108\uc9c0 \ud6a8\uc728\uc801\uc778 \uc5d0\uc9c0 \uc2dc\uc2a4\ud15c\uc5d0 \uc801\ud569\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "VII. \uc5f0\ud569 \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uac10\uc9c0-\uc561\uc158 \ub8e8\ud504"}, {"figure_path": "https://arxiv.org/html/2502.02692/x6.png", "caption": "Figure 10: Key aspects of dynamic multi-agent systems: resource heterogeneity, adaptable architectures, hardware-aware optimization, and workload management across server-client interactions.", "description": "\uadf8\ub9bc 10\uc740 \ub3d9\uc801\uc778 \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uc2dc\uc2a4\ud15c\uc758 \ud575\uc2ec \uc694\uc18c\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc11c\ubc84-\ud074\ub77c\uc774\uc5b8\ud2b8 \uc0c1\ud638 \uc791\uc6a9 \uc804\ubc18\uc5d0 \uac78\uccd0 \uc790\uc6d0 \uc774\uc9c8\uc131, \uc801\uc751\ud615 \uc544\ud0a4\ud14d\ucc98, \ud558\ub4dc\uc6e8\uc5b4 \uc778\uc2dd \ucd5c\uc801\ud654 \ubc0f \uc791\uc5c5 \ubd80\ud558 \uad00\ub9ac\uac00 \ud3ec\ud568\ub429\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud074\ub77c\uc774\uc5b8\ud2b8 \uc7a5\uce58\uc758 \uc81c\ud55c\ub41c \uc790\uc6d0(\uc5f0\uc0b0 \ub2a5\ub825, \uba54\ubaa8\ub9ac, \uc5d0\ub108\uc9c0)\uc744 \uace0\ub824\ud558\uc5ec \uc2dc\uc2a4\ud15c\uc758 \ud6a8\uc728\uc131\uc744 \uadf9\ub300\ud654\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc785\ub2c8\ub2e4. \uc11c\ubc84\ub294 \uc911\uc559 \uc9d1\uc911\uc2dd \ud559\uc2b5 \ubc0f \ubaa8\ub378 \uc5c5\ub370\uc774\ud2b8\ub97c \ub2f4\ub2f9\ud558\ub294 \ubc18\uba74, \ud074\ub77c\uc774\uc5b8\ud2b8\ub294 \uac1c\ubcc4\uc801\uc73c\ub85c \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\uace0 \ucc98\ub9ac\ud558\uba70, \uc774\ub54c \uc5d0\uc774\uc804\ud2b8\ub4e4\uc740 \uc790\uc6d0 \uc81c\uc57d, \ub124\ud2b8\uc6cc\ud06c \uc9c0\uc5f0, \ud558\ub4dc\uc6e8\uc5b4 \uc774\uc9c8\uc131 \ub4f1\uc758 \uc5b4\ub824\uc6c0\uc5d0 \uc9c1\uba74\ud558\uac8c \ub429\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \uc774\ub7ec\ud55c \uacfc\uc81c\ub4e4\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574 \uc801\uc751\ud615 \uc544\ud0a4\ud14d\ucc98, \ud558\ub4dc\uc6e8\uc5b4 \uc778\uc2dd \ucd5c\uc801\ud654, \uc791\uc5c5 \ubd80\ud558 \uad00\ub9ac \ub4f1\uc758 \uc804\ub7b5\uc774 \ud544\uc694\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "VII. \uc5f0\ud569 \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uac10\uc9c0-\uc791\uc6a9 \ub8e8\ud504"}, {"figure_path": "https://arxiv.org/html/2502.02692/x7.png", "caption": "Figure 11: Performance comparison of DC-NAS and HaLo-FL on the CIFAR-10 dataset, showing relative reductions in energy, latency, and area with adaptive model optimization.", "description": "\uadf8\ub9bc 11\uc740 CIFAR-10 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc801\uc751\ud615 \ubaa8\ub378 \ucd5c\uc801\ud654\ub97c \uc0ac\uc6a9\ud558\uc5ec DC-NAS\uc640 HaLo-FL\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc5d0\ub108\uc9c0, \uc9c0\uc5f0 \uc2dc\uac04 \ubc0f \uba74\uc801 \uce21\uba74\uc5d0\uc11c \uc0c1\ub300\uc801\uc778 \uac10\uc18c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  DC-NAS\uc640 HaLo-FL\uc740 \ubaa8\ub450 \uc5f0\ud569 \ud559\uc2b5 \ud658\uacbd\uc5d0\uc11c \ub9ac\uc18c\uc2a4 \uc81c\uc57d\uc774 \uc788\ub294 \ub2e4\uc591\ud55c \uc7a5\uce58\uc5d0 \uc801\uc751\ud558\ub294 \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\uae30 \uc704\ud55c \ucd5c\uc801\ud654\ub41c \ubc29\ubc95\ub860\uc785\ub2c8\ub2e4.  DC-NAS\ub294 \ub124\ud2b8\uc6cc\ud06c \ud1a0\ud3f4\ub85c\uc9c0\uc640 \ucc44\ub110\uc744 \ub3d9\uc801\uc73c\ub85c \uc870\uc815\ud558\uc5ec \uac01 \uc7a5\uce58\uc758 \uc81c\uc57d\uc5d0 \ub9de\ucd98 \ubaa8\ub378\uc744 \uc0dd\uc131\ud558\uace0, HaLo-FL\uc740 \ud558\ub4dc\uc6e8\uc5b4 \uc778\uc2dd \uc815\ubc00\ub3c4 \uc120\ud0dd\uc744 \ud1b5\ud574 \uac00\uc911\uce58, \ud65c\uc131\ud654 \ubc0f \uae30\uc6b8\uae30\ub97c \ucd5c\uc801\ud654\ud558\uc5ec \uc5d0\ub108\uc9c0 \uc18c\ube44\uc640 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \uc904\uc785\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub450 \ubc29\ubc95 \ubaa8\ub450 \uc5d0\ub108\uc9c0 \ud6a8\uc728\uc131, \uc9c0\uc5f0 \uc2dc\uac04 \ub2e8\ucd95, \uadf8\ub9ac\uace0 \uba74\uc801 \uac10\uc18c\uc5d0 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2dc\uac01\uc801 \uc99d\uac70\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "VII. \uc5f0\ud569, \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uac10\uc9c0-\uc561\uc158 \ub8e8\ud504"}]