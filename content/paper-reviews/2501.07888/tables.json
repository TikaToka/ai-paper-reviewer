[{"content": "| Benchmark | Previous SOTA |\n|---|---| \n| DREAM-1K [105] | Tarsier-7B [105] |\n| MVBench [57] | InternVL2.5-8B [20] |\n| TVBench [25] | IXC-2.5 7B [124] |\n| TOMATO [94] | Qwen2-VL-7B [106] |\n| Vinoground [123] | LLaVA-OV-7B [53] |\n| TempCompass [69] | Qwen2-VL-7B [106] |\n| Video-MME [31] | NVILA-7B [70] |\n| LongVideoBench [110] | Apollo-7B [130] |\n| TemporalBench [12] | LLaVA-Video-7B [127] |\n| MLVU [128] | InternVL2.5-8B [20] |\n| MMBench-Video [30] | MiniCPM-V-2.6 [119] |\n| VideoHallucer [109] | Qwen2-VL-7B [106] |\n| EventHallusion [122] | Tarsier-7B [105] |\n| E.T. Bench [67] | E.T. Chat [67] |", "caption": "Table 1: Evaluation results on DREAM-1K. We report F1/Precision/Recall scores for each category and for the overall dataset. For open-source models, all results are tested with their official checkpoint and inference code under recommended setting. SOTA results of comparable scale (<<<10B) are bolded and overall best results are underlined.", "description": "DREAM-1K \ube44\ub514\uc624 \ucea1\uc158\ub9c1 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c Tarsier2\uc758 \uc131\ub2a5 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \uce74\ud14c\uace0\ub9ac\uc640 \uc804\uccb4 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c F1 \uc810\uc218, \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728\uc774 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378\uc758 \uacbd\uc6b0, \uacf5\uc2dd \uccb4\ud06c\ud3ec\uc778\ud2b8\uc640 \ucd94\ub860 \ucf54\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \uad8c\uc7a5 \uc124\uc815\uc73c\ub85c \ubaa8\ub4e0 \uacb0\uacfc\ub97c \ud14c\uc2a4\ud2b8\ud588\uc2b5\ub2c8\ub2e4. \ube44\uc2b7\ud55c \uaddc\ubaa8(10B \ubbf8\ub9cc)\uc758 \ucd5c\ucca8\ub2e8(SOTA) \uacb0\uacfc\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5c8\uace0, \ucd5c\uace0\uc758 \uacb0\uacfc\ub294 \ubc11\uc904\uc774 \uadf8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ube44\ub514\uc624\uc5d0 \ub300\ud55c \ucea1\uc158 \uc0dd\uc131 \ub2a5\ub825\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "4.1 \uc815\ub7c9\uc801 \uacb0\uacfc"}, {"content": "| Model | Live-action | Animation | Stock | YouTube | Shorts | Overall |\n|---|---|---|---|---|---|---|\n| *Proprietary models* |  |  |  |  |  |  |\n| GPT-4V [5] | 34.8/39.2/31.3 | 27.4/31.9/24.0 | 40.7/46.7/36.1 | 33.8/40.1/29.2 | 34.8/46.1/28.0 | 34.4/40.8/29.7 |\n| GPT-4o [41] | 39.8/42.1/37.8 | 35.8/39.1/33.1 | 44.0/46.6/41.7 | 35.9/41.5/31.7 | 39.9/47.9/34.2 | 39.2/43.4/35.7 |\n| Gemini-1.5-Flash [102] | 34.8/36.4/33.3 | 29.2/32.5/26.5 | 39.4/39.7/39.1 | 34.3/38.6/30.9 | 35.6/42.4/30.7 | 34.8/37.9/32.1 |\n| Gemini-1.5-Pro [102] | 36.4/36.4/36.4 | 30.7/31.8/29.7 | 42.2/40.7/43.8 | 34.0/36.7/31.6 | 37.0/42.4/32.7 | 36.2/37.6/34.8 |\n| *Open-source models (>10B)* |  |  |  |  |  |  |\n| PLLaVA-34B [114] | 29.3/34.9/25.2 | 20.9/32.0/15.6 | 35.1/42.5/29.9 | 28.9/40.8/22.3 | 25.6/41.9/18.4 | 28.2/38.4/22.3 |\n| VideoLLaMA2-72B [23] | 27.3/29.3/25.6 | 19.7/21.7/18.1 | 33.9/37.0/31.3 | 27.7/33.0/23.8 | 26.5/33.1/22.1 | 27.1/30.8/24.2 |\n| LLaVA-OV-72B [53] | 31.7/32.8/30.7 | 27.7/30.6/25.2 | 38.0/39.6/36.6 | 34.1/34.7/33.5 | 33.8/41.8/28.4 | 33.2/35.9/30.9 |\n| LLaVA-Video-72B [127] | 33.5/36.3/31.1 | 28.6/31.7/26.1 | 39.3/41.1/37.6 | 32.8/34.7/31.1 | 35.7/42.8/30.6 | 34.0/37.3/31.3 |\n| Qwen2-VL-72B [106] | 32.1/33.7/30.6 | 27.6/32.6/23.9 | 41.1/41.2/41.1 | 32.0/38.1/27.7 | 32.1/41.0/26.4 | 33.2/37.3/29.9 |\n| InternVL2.5-78B [20] | 25.3/31.5/21.1 | 21.8/28.8/17.6 | 33.5/38.1/29.9 | 31.0/38.5/25.9 | 31.1/41.7/24.8 | 28.6/35.7/23.9 |\n| Tarsier-34B [105] | 38.5/39.6/37.5 | 32.2/35.8/29.2 | 41.7/46.4/37.8 | 34.5/41.1/29.7 | 34.0/44.1/27.7 | 36.3/41.4/32.4 |\n| *Open-source models (<10B)* |  |  |  |  |  |  |\n| Video-LLaVA-7B [61] | 19.4/24.3/16.2 | 15.3/21.2/11.9 | 27.0/33.5/22.7 | 21.2/31.9/15.8 | 18.5/29.4/13.5 | 20.4/28.1/16.0 |\n| VideoLLaMA2-7B [23] | 25.1/28.7/22.2 | 20.4/25.5/17.0 | 32.6/35.5/30.2 | 27.5/33.5/23.4 | 24.5/34.1/19.2 | 26.2/31.5/22.4 |\n| LLaVA-OV-7B [53] | 31.2/33.2/29.3 | 26.8/29.0/25.0 | 38.1/39.1/37.1 | 30.6/32.1/29.2 | 31.4/38.3/26.6 | 31.7/34.3/29.4 |\n| LLaVA-Video-7B [127] | 31.4/35.2/28.4 | 27.6/32.9/23.8 | 36.7/39.7/34.1 | 33.0/39.5/28.3 | 33.4/42.5/27.5 | 32.5/37.9/28.4 |\n| Qwen2-VL-7B [106] | 27.7/32.5/24.2 | 22.2/28.0/18.4 | 37.0/36.1/38.0 | 30.7/35.5/27.0 | 29.1/37.6/23.8 | 29.6/33.9/26.3 |\n| InternVL2.5-8B [20] | 26.6/32.0/22.8 | 21.3/28.9/16.9 | 32.7/37.2/29.1 | 27.9/35.4/23.0 | 28.9/39.9/22.7 | 27.6/34.7/22.9 |\n| Tarsier-7B [105] | 36.6/38.5/34.8 | 29.3/34.6/25.5 | 39.6/44.7/35.5 | 33.0/39.2/28.4 | 33.6/44.6/26.9 | 34.6/40.3/30.2 |\n| Tarsier2-7B | 44.4/41.9/47.3 | 39.3/39.5/39.1 | 45.7/45.4/46.0 | 36.0/38.4/33.9 | 43.7/48.9/39.4 | 42.0/42.8/41.1 |", "caption": "Table 2: Evaluation results on E.T. Bench-Captioning. Results marked in gray are tested on a subset. \u2020\u2020{\\dagger}\u2020 denotes the model is fine-tuned on E.T. Instruct 164K. All results are transcribed from the official benchmark, except for LLaVA-OV, LLaVA-Video and Qwen2-VL, which are our evaluation using the official checkpoint and inference code.", "description": "\ud45c 2\ub294 E.T. Bench-Captioning \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud68c\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub41c \uacb0\uacfc\ub294 \ud558\uc704 \uc9d1\ud569\uc5d0 \ub300\ud574 \ud14c\uc2a4\ud2b8\ub418\uc5c8\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \u2020 \uae30\ud638\ub294 \ubaa8\ub378\uc774 E.T. Instruct 164K \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ub418\uc5c8\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ubaa8\ub4e0 \uacb0\uacfc\ub294 \uacf5\uc2dd \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc804\uc0ac\ub418\uc5c8\uc9c0\ub9cc, LLaVA-OV, LLaVA-Video \ubc0f Qwen2-VL\uc5d0 \ub300\ud55c \uacb0\uacfc\ub294 \uacf5\uc2dd \uccb4\ud06c\ud3ec\uc778\ud2b8\uc640 \ucd94\ub860 \ucf54\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc800\uc790\ub4e4\uc774 \uc9c1\uc811 \ud3c9\uac00\ud55c \uac83\uc785\ub2c8\ub2e4. \ud45c\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 DVCF1, DVCSim, SLCF1, SLCSim, AvgF1, AvgSim \uc131\ub2a5 \uc9c0\ud45c\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uac01 \ubaa8\ub378\uc758 \ube44\ub514\uc624 \ucea1\uc158 \uc0dd\uc131 \ub2a5\ub825\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "4.1 \uc815\ub7c9\uc801 \uacb0\uacfc"}, {"content": "| Model | DVC<sub>F1</sub> | DVC<sub>Sim</sub> | SLC<sub>F1</sub> | SLC<sub>Sim</sub> | Avg<sub>F1</sub> | Avg<sub>Sim</sub> |\n|---|---|---|---|---|---|---|\n| *Proprietary models* |  |  |  |  |  |  |\n| GPT-4V [5] | <span style=\"color:#BFBFBF;\">16.1</span> | <span style=\"color:#BFBFBF;\">19.4</span> | <span style=\"color:#BFBFBF;\">21.9</span> | <span style=\"color:#BFBFBF;\">13.5</span> | <span style=\"color:#BFBFBF;\">19.0</span> | <span style=\"color:#BFBFBF;\">16.4</span> |\n| GPT-4o [41] | <span style=\"color:#BFBFBF;\">46.9</span> | <span style=\"color:#BFBFBF;\">22.3</span> | <span style=\"color:#BFBFBF;\">23.1</span> | <span style=\"color:#BFBFBF;\">14.9</span> | <span style=\"color:#BFBFBF;\">35.0</span> | <span style=\"color:#BFBFBF;\">18.6</span> |\n| Gemini-1.5-Flash [102] | <span style=\"color:#BFBFBF;\">31.6</span> | <span style=\"color:#BFBFBF;\">14.9</span> | <span style=\"color:#BFBFBF;\">16.5</span> | <span style=\"color:#BFBFBF;\">13.3</span> | <span style=\"color:#BFBFBF;\">24.1</span> | <span style=\"color:#BFBFBF;\">14.1</span> |\n| Gemini-1.5-Pro [102] | <span style=\"color:#BFBFBF;\">24.0</span> | <span style=\"color:#BFBFBF;\">17.5</span> | <span style=\"color:#BFBFBF;\">5.8</span> | <span style=\"color:#BFBFBF;\">9.8</span> | <span style=\"color:#BFBFBF;\">14.9</span> | <span style=\"color:#BFBFBF;\">13.7</span> |\n| *Open-source models (>10B)* |  |  |  |  |  |  |\n| PLLaVA-34B [114] | 13.3 | 10.6 | 9.7 | 11.8 | 11.5 | 11.2 |\n| LLaVA-OV-72B [53] | 41.9 | 16.3 | 25.6 | 13.9 | 33.8 | 15.1 |\n| LLaVA-Video-72B [127] | 37.0 | 15.7 | 20.4 | 13.5 | 28.7 | 14.6 |\n| Qwen2-VL-72B [106] | 15.3 | 13.9 | 11.0 | 12.8 | 13.2 | 13.4 |\n| *Open-source models (\u226410B)* |  |  |  |  |  |  |\n| VideoLLaMA2-7B [23] | 0.6 | 14.5 | 0.0 | 15.2 | 0.3 | 14.8 |\n| Video-LLaVA-7B [61] | 28.0 | 15.0 | 0.9 | 8.3 | 14.4 | 11.7 |\n| LLaVA-OV-7B [53] | 22.0 | 15.1 | 9.5 | 10.6 | 15.8 | 12.8 |\n| LLaVA-Video-7B [127] | 20.6 | 14.7 | 6.5 | 13.4 | 13.6 | 14.1 |\n| E.T. Chat [67]<sup>\u2020</sup> | 38.4 | 19.7 | 24.4 | 14.6 | 31.4 | 17.1 |\n| Qwen2-VL-7B [106]<sup>\u2020</sup> | 44.3 | 25.3 | 25.7 | 15.6 | 35.0 | 20.4 |\n| Tarsier-7B [105]<sup>\u2020</sup> | 42.8 | 19.1 | 23.7 | 15.2 | 33.2 | 17.1 |\n| Tarsier2-7B<sup>\u2020</sup> | 46.5 | 28.8 | 24.6 | 16.4 | 35.5 | 22.6 |", "caption": "Table 3: Evaluation results on short video question answering benchmarks. * indicates that the training set has been observed in the training data mixture.", "description": "\ud45c 3\uc740 \uc9e7\uc740 \ube44\ub514\uc624 \uc9c8\ubb38 \ub2f5\ubcc0 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c(MVBench, PerceptionTest, TVBench, TOMATO, Vinoground, TempCompass)\uc5d0\uc11c Tarsier2-7B \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\ub978 \ucd5c\ucca8\ub2e8 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubca4\uce58\ub9c8\ud06c\uc758 \ud558\uc704 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4 \uc810\uc218\uc640 \ub354\ubd88\uc5b4 \uc804\uccb4 \ud3c9\uade0 \uc810\uc218\ub3c4 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  * \ud45c\uc2dc\ub294 \ud574\ub2f9 \ubaa8\ub378\uc758 \ud559\uc2b5 \ub370\uc774\ud130\uc5d0 \ubca4\uce58\ub9c8\ud06c\uc758 \ud559\uc2b5 \ub370\uc774\ud130\uac00 \ud3ec\ud568\ub418\uc5c8\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4.1 \uc815\ub7c9\uc801 \uacb0\uacfc"}, {"content": "| Model | MVBench [57] | PerceptionTest [86] | TVBench [25] | TOMATO [94] | Vinoground [123] | TempCompass [69] |\n|---|---|---|---|---|---|---|\n| **Model** | test | val | test | test | Text/Video/Group | mc/yn/cm/cg |\n| *Proprietary models* |  |  |  |  |  |  |\n| GPT-4o [41] | 57.5 | - | 39.6 | 37.7 | 54.0/38.2/24.6 | 71.0/73.7/80.8/70.8 |\n| Gemini-1.5-Pro [102] | - | - | 46.5 | 36.1 | 35.8/22.6/10.2 | 63.9/70.3/77.5/57.9 |\n| *Open-source models (>10B)* |  |  |  |  |  |  |\n| LLaVA-OV-72B [53] | 59.4 | 66.9 | 45.9 | 28.6 | 48.4/35.2/21.8 | 67.6/72.6/78.2/52.6 |\n| LLaVA-Video-72B [127] | 64.1 | 74.3* | 50.0 | 28.2 | 52.0/35.6/20.8 | 69.9/73.0/80.9/54.4 |\n| Qwen2-VL-72B [106] | 73.6 | 66.5 | 52.7 | 37.9 | 50.4/32.6/17.4 | 76.0/75.9/84.6/58.6 |\n| Tarsier-34B [105] | 67.6 | 60.4 | 53.8 | 34.3 | 37.8/32.0/15.0 | 69.8/74.0/73.0/60.9 |\n| *Open-source models (\u226410B)* |  |  |  |  |  |  |\n| LLaVA-OV-7B [53] | 56.7 | 57.1 | 45.6 | 25.5 | 41.6/29.4/14.6 | 64.8/69.7/73.8/49.9 |\n| LLaVA-Video-7B [127] | 58.6 | 67.9* | 45.6 | 24.9 | 36.8/29.0/12.8 | 56.3/68.7/76.8/53.0 |\n| Qwen2-VL-7B [106] | 67.0 | - | 43.8 | 31.5 | 40.0/23.4/12.4 | 68.5/72.8/77.3/54.2 |\n| Tarsier-7B [105] | 62.6 | 53.9 | 45.8 | 28.6 | 29.8/22.2/8.6 | 58.7/58.0/54.2/55.3 |\n| Previous SOTA | 72.0 [20] | 70.0* [72] | 51.6 [124] | 31.5 [106] | 41.6/29.4/14.6 [52] | 68.5/72.8/77.3/54.2 [106] |\n| Tarsier2-7B | 71.5 | 71.6* | 54.7 | 42.0 | 65.8/38.0/28.8 | 75.3/75.1/80.6/66.6 |", "caption": "Table 4: Evaluation results on long-video question answering benchmarks. We list the number of frames used for each benchmark during evaluating Tarsier2.", "description": "\ud45c 4\ub294 \uc7a5\uc2dc\uac04 \ube44\ub514\uc624 \uc9c8\ubb38 \uc751\ub2f5 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c Tarsier2\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Tarsier2 \ud3c9\uac00 \uc2dc \uac01 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ud504\ub808\uc784 \uc218\ub97c \ud568\uaed8 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uacbd\uc7c1 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud558\uc5ec Tarsier2\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \uc9c0\ud45c(\uc815\ud655\ub3c4, \ud3c9\uade0 \uc815\ud655\ub3c4 \ub4f1)\ub85c \ub098\ud0c0\ub0b4\uc5b4 \uc7a5\uc2dc\uac04 \ube44\ub514\uc624 \uc774\ud574 \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "4.1.3 Long-Video Question Answering"}, {"content": "| Model | Video-MME<sup>[31]</sup> | LongVideoBench<sup>[110]</sup> | TemporalBench<sup>[12]</sup> | MLVU<sup>[128]</sup> | MMBench-Video<sup>[30]</sup> |\n|---|---|---|---|---|---| \n|  | w/o subs | val | Binary Accuracy | M-Avg | val |\n|---|---|---|---|---|---| \n| *Proprietary models* |  |  |  |  |  |\n| GPT-4o<sup>[41]</sup> | 71.9 | 66.7 | 73.2 | 64.6 | 1.87 |\n| Gemini-1.5-Pro<sup>[102]</sup> | 75.0 | 64.0 | 66.4 | - | 1.30 |\n| *Open-source models (>10B)* |  |  |  |  |  |\n| VILA-1.5-40B<sup>[62]</sup> | 60.1 | - | - | 56.7 | 1.61 |\n| LLaVA-Video-72B<sup>[127]</sup> | 70.5 | 61.9 | 72.4 | 74.4 | 1.71 |\n| Qwen2-VL-72B<sup>[106]</sup> | 71.2 | - | 70.2 | - | 1.70 |\n| InternVL2.5-78B<sup>[20]</sup> | 72.1 | 63.6 | - | 75.7 | 1.97 |\n| Tarsier-34B<sup>[105]</sup> | 52.3 | 54.2 | 66.7 | 58.2 | 1.46 |\n| *Open-source models (\u226410B)* |  |  |  |  |  |\n| LLaVA-Video-7B<sup>[127]</sup> | 63.3 | 58.2 | 63.6 | 70.8 | 1.60 |\n| Qwen2-VL-7B<sup>[106]</sup> | 63.3 | 55.6 | 62.0 | - | 1.44 |\n| InternVL2.5-8B<sup>[20]</sup> | 64.2 | 60.0 | - | 68.9 | 1.68 |\n| Tarsier-7B<sup>[105]</sup> | 42.2 | 39.8 | 56.9 | 49.3 | - |\n| Previous SOTA | 64.2<sup>[70]</sup> | 60.0<sup>[20]</sup> | 63.6<sup>[127]</sup> | 70.9<sup>[130]</sup> | 1.70<sup>[119]</sup> |\n| Tarsier2-7B | 64.5 (128f) | 58.6 (128f) | 65.3 (128f) | 67.9 (256f) | 1.82 (128f) |", "caption": "Table 5: Evaluation results on hallucination benchmarks.", "description": "\ud45c 5\ub294 \ube44\ub514\uc624 \ud658\uac01 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub450 \uac00\uc9c0 \ud658\uac01 \ubca4\uce58\ub9c8\ud06c\uc778 VideoHallucer\uc640 EventHallusion\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \uc81c\uc2dc\ub429\ub2c8\ub2e4.  \uac01 \ubca4\uce58\ub9c8\ud06c\ub294 \uc5ec\ub7ec \ud558\uc704 \uc791\uc5c5(\uc608: Yes/No QA, \uc124\uba85 \uc0dd\uc131)\uc73c\ub85c \uad6c\uc131\ub418\uba70,  \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728, F1 \uc810\uc218\uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uc9c0\ud45c\uac00 \uc81c\uacf5\ub429\ub2c8\ub2e4.  \ube44\ub514\uc624 \uc124\uba85 \uc0dd\uc131\uc758 \ud658\uac01 \uc5ec\ubd80\ub97c \uc9c1\uc811\uc801\uc73c\ub85c \ud3c9\uac00\ud558\ub294 \uc791\uc5c5\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub3c5\uc810 \ubaa8\ub378(GPT-4, Gemini)\uacfc \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378(LLaVA, Tarsier \ub4f1)\uc758 \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, Tarsier2 \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.1 \uc815\ub7c9\uc801 \uacb0\uacfc"}, {"content": "| Model | VideoHallucer [109] | EventHallusion [122] | EventHallusion [122] |\n|---|---|---|---|\n|  | Yes/No QA | Yes/No QA | Desc GPT |\n|  | Basic/Hallucinated/Overall | Entire/Interleave/Misleading/Overall | Entire/Interleave/Misleading/Overall |\n| *Proprietary models* |  |  |  |\n| GPT-4o [41] | 75.1/74.2/53.3 | 65.8/90.7/92.2/84.1 | 34.9/54.9/83.2/56.2 |\n| Gemini-1.5-Pro [102] | 83.6/42.3/37.8 | 70.2/77.7/96.1/80.2 | 38.5/40.9/80.0/49.6 |\n| *Open-Source models (>10B)* |  |  |  |\n| Qwen2-VL-72B [106] | 87.1/79.4/70.2 | 33.3/77.7/56.4/60.0 | 16.5/25.4/70.2/33.6 |\n| LLaVA-OV-72B [53] | 88.3/62.6/55.2 | 47.4/26.9/90.1/48.3 | 24.8/34.7/71.3/40.7 |\n| LLaVA-Video-72B [127] | 88.2/73.5/64.6 | 57.9/11.9/96.0/45.6 | 32.1/35.8/75.5/44.2 |\n| InternVL2.5-78B [20] | 82.5/82.5/67.8 | 57.9/67.9/88.2/70.2 | 45.0/43.0/76.8/51.6 |\n| Tarsier-34B [105] | 84.8/80.0/67.7 | 49.1/92.7/69.6/74.8 | 38.5/40.4/83.2/50.1 |\n| *Open-Source models (\u226410B)* |  |  |  |\n| LLaVA-OV-7B [53] | 81.1/69.6/53.8 | 46.5/67.4/86.1/66.2 | 22.0/26.4/73.4/36.4 |\n| LLaVA-Video-7B [127] | 82.4/70.6/56.0 | 61.4/48.7/96.0/64.0 | 27.5/32.6/75.5/41.4 |\n| Qwen2-VL-7B [106] | 85.0/70.8/59.3 | 35.1/94.3/57.4/68.6 | 14.7/16.1/67.0/27.8 |\n| InternVL2.5-8B [20] | 72.7/78.3/53.6 | 46.5/69.2/90.2/68.2 | 23.9/20.7/60.0/31.0 |\n| Tarsier-7B [105] | 76.4/60.8/41.4 | 43.9/82.4/79.4/70.9 | 35.8/29.5/72.6/41.6 |\n| Tarsier2-7B | 86.5/78.3/67.0 | 60.5/93.3/95.1/84.6 | 54.6/53.1/93.7/63.3 |", "caption": "Table 6: Evaluation results on E.T. Bench-Grounding. Results marked in gray are tested on a subset. \u2020\u2020{\\dagger}\u2020 denotes the model is fine-tuned on E.T. Instruct 164K.", "description": "\ud45c 6\uc740 E.T. Bench-Grounding \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ube44\ub514\uc624 \uc9c0\uc0c1\ud654(grounding) \uc791\uc5c5\uc5d0 \ub300\ud55c \uc5ec\ub7ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. \ud68c\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub41c \uacb0\uacfc\ub294 \uc77c\ubd80 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574\uc11c\ub9cc \ud14c\uc2a4\ud2b8\ub41c \uacb0\uacfc\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \u2020 \uae30\ud638\ub294 \ubaa8\ub378\uc774 E.T. Instruct 164K \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubbf8\uc138 \uc870\uc815\ub418\uc5c8\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 TVGF1, EPMF1, TALF1, EVSF1, VHDF1 \ubc0f MeanF1 \ub4f1 \uc5ec\ub7ec \uc9c0\ud45c\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \uc9c0\ud45c\ub294 \ube44\ub514\uc624 \uc9c0\uc0c1\ud654 \uc791\uc5c5\uc758 \ub2e4\uc591\ud55c \uce21\uba74\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uc5b4\ub5a4 \ubaa8\ub378\uc774 \ud2b9\uc815 \uc791\uc5c5\uc5d0\uc11c \uac00\uc7a5 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.1 \uc815\ub7c9\uc801 \uacb0\uacfc"}, {"content": "| Model | TVG<sub>F1</sub> | EPM<sub>F1</sub> | TAL<sub>F1</sub> | EVS<sub>F1</sub> | VHD<sub>F1</sub> | Mean<sub>F1</sub> |\n|---|---|---|---|---|---|---|\n| *Proprietary models* |\n| GPT-4V [5] | <span style=\"color:#BFBFBF;\">27.0</span> | <span style=\"color:#BFBFBF;\">1.8</span> | <span style=\"color:#BFBFBF;\">18.0</span> | <span style=\"color:#BFBFBF;\">28.6</span> | <span style=\"color:#BFBFBF;\">55.1</span> | <span style=\"color:#BFBFBF;\">26.1</span> |\n| GPT-4o [41] | <span style=\"color:#BFBFBF;\">40.4</span> | <span style=\"color:#BFBFBF;\">4.5</span> | <span style=\"color:#BFBFBF;\">20.0</span> | <span style=\"color:#BFBFBF;\">17.6</span> | <span style=\"color:#BFBFBF;\">56.9</span> | <span style=\"color:#BFBFBF;\">27.9</span> |\n| Gemini-1.5-Flash [102] | <span style=\"color:#BFBFBF;\">43.9</span> | <span style=\"color:#BFBFBF;\">5.4</span> | <span style=\"color:#BFBFBF;\">27.0</span> | <span style=\"color:#BFBFBF;\">5.4</span> | <span style=\"color:#BFBFBF;\">60.8</span> | <span style=\"color:#BFBFBF;\">28.5</span> |\n| Gemini-1.5-Pro [102] | <span style=\"color:#BFBFBF;\">43.1</span> | <span style=\"color:#BFBFBF;\">6.2</span> | <span style=\"color:#BFBFBF;\">33.8</span> | <span style=\"color:#BFBFBF;\">7.9</span> | <span style=\"color:#BFBFBF;\">47.0</span> | <span style=\"color:#BFBFBF;\">27.6</span> |\n| *Open-source models (&lt;10B)* |\n| LITA [39] | 22.2 | 4.6 | 18.0 | 29.7 | 23.9 | 19.7 |\n| VTG-LLM [37] | 15.9 | 3.7 | 14.4 | 26.8 | 48.2 | 21.8 |\n| TimeChat [91]<sup>\u2020</sup> | - | - | - | - | - | 24.3 |\n| E.T. Chat [67]<sup>\u2020</sup> | 38.6 | 10.2 | 30.8 | 25.4 | 62.5 | 33.5 |\n| Tarsier-7B [105]<sup>\u2020</sup> | 39.6 | 9.0 | 25.0 | 25.4 | 47.6 | 30.9 |\n| Qwen2-VL-7B [106]<sup>\u2020</sup> | 39.7 | 7.0 | 26.9 | 17.1 | 66.9 | 33.5 |\n| Tarsier2-7B<sup>\u2020</sup> | 38.4 | 11.0 | 31.8 | 19.4 | 66.8 | 35.5 |", "caption": "Table 7: Evaluation results on embodied question-answering tasks, including EgoTaskQA, RoboVQA and OpenEQA.", "description": "\ud45c 7\uc740 EgoTaskQA, RoboVQA, OpenEQA \uc138 \uac00\uc9c0 \uc784\ubca0\ub514\ub4dc \uc9c8\uc758\uc751\ub2f5 \uacfc\uc81c\uc5d0 \ub300\ud55c Tarsier2 \ubaa8\ub378\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uacfc\uc81c\ub294 \uc2e4\uc81c \ub85c\ubd07 \uc2dc\ub098\ub9ac\uc624\ub97c \uae30\ubc18\uc73c\ub85c \ud558\uba70, \ubaa8\ub378\uc758 \uc2e4\uc81c \ud658\uacbd \uc774\ud574 \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  \uacb0\uacfc\ub294 EgoTaskQA\uc758 \uacbd\uc6b0 \uc815\ud655\ub3c4 \uc77c\uce58\uc728(Exact Match Accuracy), RoboVQA\uc758 \uacbd\uc6b0 BLEU \uc810\uc218, OpenEQA\uc758 \uacbd\uc6b0 GPT-4\ub97c \uc0ac\uc6a9\ud55c \uc815\ud655\ub3c4 \uc810\uc218\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 Tarsier2\uac00 \uc77c\ubc18\uc801\uc778 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378\uacfc \uc804\ubb38 \ubaa8\ub378 \ubaa8\ub450\ub97c \ub2a5\uac00\ud558\ub294 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 \uc815\ub7c9\uc801 \uacb0\uacfc"}, {"content": "| Model | EgoTaskQA Exact Match |\n|---|---| \n| Human | 80.0 |\n| HCRN [50] | 42.2 |\n| GF [9] | 44.3 |\n| EgoVLPv2 [88] | 46.3 |\n| Tarsier2 | **77.5** |", "caption": "Table 8: Results of the ablation study for pre-training. Tarsier1-7b-Qwen stands for the model where the base model is upgraded to Qwen2-VL, while the pre-training dataset remains the same as Tarsier1. Tarsier2 is trained from Qwen2-VL with an expanded pre-training dataset, growing from 13 million in Tarsier1 to 40 million samples.", "description": "\ud45c 8\uc740 \uc0ac\uc804 \ud6c8\ub828\uc5d0 \ub300\ud55c ablation \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Tarsier1-7B-Qwen\uc740 \uae30\ubcf8 \ubaa8\ub378\uc774 Qwen2-VL\ub85c \uc5c5\uadf8\ub808\uc774\ub4dc\ub418\uc5c8\uc9c0\ub9cc \uc0ac\uc804 \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc740 Tarsier1\uacfc \ub3d9\uc77c\ud55c \ubaa8\ub378\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. Tarsier2\ub294 Qwen2-VL\uc744 \uae30\ubc18\uc73c\ub85c \uc0ac\uc804 \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc774 Tarsier1\uc758 1,300\ub9cc \uac1c\uc5d0\uc11c 4,000\ub9cc \uac1c\ub85c \ud655\uc7a5\ub418\uc5b4 \ud6c8\ub828\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uae30\ubcf8 \ubaa8\ub378, \uc0ac\uc804 \ud6c8\ub828 \ub370\uc774\ud130\uc14b \ud06c\uae30, \ud6c8\ub828 \ub2e8\uacc4 \ub4f1 \uc5ec\ub7ec \uc694\uc18c\ub4e4\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2.1 \uc0ac\uc804 \ud6c8\ub828"}, {"content": "| Model | RoboVQA (BLEU-1/2/3/4)| \n|---|---| \n| LLaMA-AdapterV2 [33] | 27.8/16.0/10.9/8.1 | \n| LLaVA-OV-7B [53] | 38.1/33.6/31.8/31.0 | \n| RoboMamba [66] | 54.9/44.2/39.5/36.3 | \n| MLCD [3] | 73.2/66.4/60.6/56.6 | \n| Tarsier2 | 77.1/67.4/61.5/56.8 |", "caption": "Table 9: Ablation study of temporal grounding dataset during the SFT phase. Tarsier2-7B-SFT refers to the model after the SFT phase. w/o SFT refers to the model after pre-training; w/o grounding refers to the model fine-tinued without grounding information.", "description": "\ud45c 9\ub294 SFT(Supervised Fine-Tuning) \ub2e8\uacc4\uc5d0\uc11c \uc2dc\uac04\uc801 \uae30\ubc18 \ub370\uc774\ud130 \uc138\ud2b8\uc758 \uc601\ud5a5\uc744 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Tarsier2-7B-SFT\ub294 SFT \ub2e8\uacc4\ub97c \uac70\uce5c \ubaa8\ub378\uc774\uace0, w/o SFT\ub294 \uc0ac\uc804 \ud559\uc2b5\ub9cc \uac70\uce5c \ubaa8\ub378\uc774\uba70, w/o grounding\uc740 \uc2dc\uac04\uc801 \uae30\ubc18 \uc815\ubcf4 \uc5c6\uc774 \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ucea1\uc158 \uc0dd\uc131, \uc9e7\uc740 \ube44\ub514\uc624 \uc9c8\ubb38 \ub2f5\ubcc0, \uae34 \ube44\ub514\uc624 \uc9c8\ubb38 \ub2f5\ubcc0, \uadf8\ub9ac\uace0 \ud658\uac01(hallucination) \ud14c\uc2a4\ud2b8\uc758 \uc131\ub2a5\uc744 DREAM-1K, TempCompass-cg, Vinoground-Text \ub4f1 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c \uae30\uc900\uc73c\ub85c \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.2.2 SFT"}, {"content": "| Model | OpenEQA |\n|---|---| \n| **Human** | 86.8 |\n| GPT-4V<sup>[5]</sup> | 55.3 |\n| Gemini-1.5-Pro<sup>[102]</sup> | 44.9 |\n| MLCD<sup>[3]</sup> | 48.8 |\n| Tarsier2 | **58.7** |", "caption": "Table 10: Ablation study for DPO training phase, negative sampling (NS) and preference data filtering (PF) strategies.", "description": "\ud45c 10\uc740 DPO(Direct Preference Optimization) \ud6c8\ub828 \ub2e8\uacc4\uc5d0\uc11c \uc74c\uc131 \uc0d8\ud50c\ub9c1(Negative Sampling)\uacfc \uc120\ud638\ub3c4 \ub370\uc774\ud130 \ud544\ud130\ub9c1(Preference Filtering) \uc804\ub7b5\uc744 \uc81c\uac70\ud588\uc744 \ub54c\uc758 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ucd94\uac00 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc5f4\uc740 DREAM-1K, TempCompass-cg, Vinoground-Text, Short Video QA, Long Video QA, Hallucination\uc758 \uc131\ub2a5 \uc9c0\ud45c\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uac01 \ud589\uc740 Tarsier2-7B \ubaa8\ub378\uc758 \uae30\uc900 \uc124\uc815\uacfc \uc74c\uc131 \uc0d8\ud50c\ub9c1 \uc81c\uac70, \uc120\ud638\ub3c4 \ub370\uc774\ud130 \ud544\ud130\ub9c1 \uc81c\uac70, DPO \uc81c\uac70 \ub4f1\uc758 \ubcc0\ud615 \uc124\uc815\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uac01 \uad6c\uc131 \uc694\uc18c\uc758 \uc131\ub2a5\uc5d0 \ub300\ud55c \uae30\uc5ec\ub3c4\ub97c \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.2 Ablation Study"}, {"content": "| Model | DREAM-1K | TempCompass-cg | Vinoground-Text | Short | Long | Hallucination |\n|---|---|---|---|---|---|---|\n| Tarsier1-7B | 34.6 | 55.3 | 29.8 | 45.6 | 46.3 | 56.3 |\n| Tarsier1-7B-Qwen <br> _upgrading model_ | 38.4 (\u21913.8) | 59.3 (\u21914.0) | 48.6 (\u219118.8) | 52.4 (\u21916.8) | 57.6 (\u219111.3) | 62.1 (\u21915.8) |\n| Tarsier2-7B <br> _upgrading model_ + _data_ | 40.8 (\u21916.2) | 60.1 (\u21914.8) | 60.2 (\u219130.4) | 55.3 (\u21919.7) | 64.1 (\u219117.8) | 63.5 (\u21917.2) |", "caption": "Table 11: The experimental results of recaptioning. \u201cRecaption FT\u201d represents fine-tune the model on the Tarsier2-Recap-585K dataset. \u201cOriginal FT\u201d represents fine-tune the model with the same videos as Tarsier2-Recap-585K but taking their original labels as target output.", "description": "\ud45c 11\uc740 Tarsier2-Recap-585K \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud55c \uacb0\uacfc\uc640 \ub3d9\uc77c\ud55c \ube44\ub514\uc624\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc6d0\ub798 \ub808\uc774\ube14\uc744 \ubaa9\ud45c \ucd9c\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \"Recaption FT\"\ub294 Tarsier2-Recap-585K \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574 \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud55c \uac83\uc744 \ub098\ud0c0\ub0b4\uace0, \"Original FT\"\ub294 Tarsier2-Recap-585K\uc640 \ub3d9\uc77c\ud55c \ube44\ub514\uc624\ub97c \uc0ac\uc6a9\ud558\uc9c0\ub9cc \uc6d0\ub798 \ub808\uc774\ube14\uc744 \ubaa9\ud45c \ucd9c\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud55c \uac83\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc989,  \uc6d0\ubcf8 \uc790\ub9c9\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc7ac\ud559\uc2b5\ud55c \uacbd\uc6b0\uc640 \uc0c8\ub85c\uc6b4 \uc790\ub9c9\uc73c\ub85c \uc7ac\ud559\uc2b5\ud55c \uacbd\uc6b0\uc758 \uc131\ub2a5 \ube44\uad50\ud45c\uc785\ub2c8\ub2e4.", "section": "4.3 Video Recaptioning using Tarsier2"}, {"content": "| Model | Caption | Caption | Caption | Video QA | Video QA | Hallucination |\n|---|---|---|---|---|---|---|\n| **Model** | **DREAM-1K** | **TempCompass-cg** | **Vinoground-Text** | **Short** | **Long** | **Hallucination** |\n| Tarsier2-7B-SFT | 40.8 | 60.1 | 60.2 | 56.2 | 63.2 | 71.9 |\n| *w/o SFT* | 35.2 (\u21935.6) | 50.5 (\u21939.6) | 57.2 (\u21933.0) | 55.3 (\u21930.9) | 64.1 (\u21910.9) | 63.5 (\u21938.4) |\n| *w/o grounding* | 37.4 (\u21933.4) | 50.2 (\u21939.9) | 60.6 (\u21910.4) | 55.9 (\u21930.3) | 61.9 (\u21931.3) | 68.6 (\u21933.3) |", "caption": "Table 12: Training hyper-parameters of Tarsier2", "description": "Tarsier2 \ubaa8\ub378\uc758 \ud559\uc2b5 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.  \uc804\uccb4 \ud559\uc2b5 \uacfc\uc815(Pre-training, SFT-1, SFT-2, DPO)\uc5d0 \uc0ac\uc6a9\ub41c Optimizer, \ud559\uc2b5\ub960, \ubc30\uce58 \ud06c\uae30, \uac00\uc911\uce58 \uac10\uc18c, \ub4dc\ub86d\uc544\uc6c3 \ube44\uc728, \ucd5c\ub300 \ud53d\uc140 \uc218, \ube44\ub514\uc624\ub2f9 \ud504\ub808\uc784 \uc218, \uc218\uce58\uc801 \uc815\ubc00\ub3c4 \ub4f1\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac12\uc744 \uc0c1\uc138\ud788 \uc81c\uc2dc\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ub2e8\uacc4\ubcc4\ub85c \ub2e4\ub978 \uc124\uc815\uac12\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378 \ud559\uc2b5\uc758 \uc138\ubd80\uc801\uc778 \ub0b4\uc6a9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3 Approach"}, {"content": "| Model | Caption DREAM-1K | Caption TempCompass-cg | Caption Vinoground-Text | Video QA Short | Video QA Long | Hallucination |\n|---|---|---|---|---|---|---|\n| Tarsier2-7B | 42.0 | 66.6 | 65.8 | 56.1 | 62.8 | 74.0 |\n| _w/o DPO_ | 40.8 (\u21931.2) | 62.1 (\u21936.5) | 60.6 (\u21935.6) | 56.2 (\u21910.1) | 63.2 (\u21910.4) | 71.9 (\u21932.1) |\n| _w/o NS_ | 41.5 (\u21930.5) | 61.1 (\u21935.5) | 59.8 (\u21936.0) | 56.1 (\u21930.0) | 62.8 (\u21930.0) | 72.9 (\u21931.1) |\n| _w/o PF_ | 40.5 (\u21931.5) | 65.1 (\u21931.5) | 67.6 (\u21911.8) | 56.0 (\u21930.1) | 62.3 (\u21930.5) | 74.2 (\u21910.2) |", "caption": "Table 13: Datasets and their sizes used in Tarsier2 pre-training. \u2020\u2020\\dagger\u2020 indicates in-house datasets.", "description": "Tarsier2 \uc0ac\uc804 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \uc885\ub958\uc640 \ud06c\uae30\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ube44\ub514\uc624 \ucea1\uc158, \uc561\uc158 \uc778\uc2dd, \ube44\ub514\uc624 \uc9c8\uc758\uc751\ub2f5, \uadf8\ub77c\uc6b4\ub529, \ube44\ub514\uc624 \uc790\uae30 \uc9c0\ub3c4 \ud559\uc2b5, \uc774\ubbf8\uc9c0 \uc774\ud574, \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \ub4f1 \ub2e4\uc591\ud55c \uc791\uc5c5\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\ub294 \uad04\ud638 \uc548\uc5d0 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \u2020\u2020\n\uae30\ud638\ub294 \uc790\uccb4\uc801\uc73c\ub85c \uc218\uc9d1\ud55c \ub370\uc774\ud130\uc14b\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 Tarsier2 \ubaa8\ub378\uc758 \uc0ac\uc804 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uc5b4\ub5a4 \uc885\ub958\uc640 \uc591\uc758 \ub370\uc774\ud130\uac00 \uc0ac\uc6a9\ub418\uc5c8\ub294\uc9c0 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3.1 \uc0ac\uc804 \ud559\uc2b5"}, {"content": "| Model | Caption DREAM-1K | Caption TempCompass-cg | Caption Vinoground-Text | Video QA Short | Video QA Long | Hallucination |\n|---|---|---|---|---|---|---|\n| Qwen2-VL-7B [106] | 31.2 | 54.2 | 40.0 | 49.4 | 60.3 | 51.9 |\n| Qwen2-VL-7B [106] + *Original FT* | 35.2 (\u21914.0) | 49.9 (\u21934.3) | 39.0 (\u21931.0) | 46.9 (\u21932.5) | 55.4 (\u21934.9) | 43.0 (\u21938.9) |\n| Qwen2-VL-7B [106] + *Recaption FT* | 39.5 (\u21918.3) | 67.7 (\u219113.5) | 55.0 (\u219115.0) | 52.5 (\u21913.1) | 56.8 (\u21933.5) | 68.5 (\u219116.6) |", "caption": "Table 14: Detailed results of the ablation study for pre-training. For the captioning task, results are reported after the SFT stage. For other tasks, results are reported after the pre-training stage.", "description": "\ud45c 14\ub294 \uc0ac\uc804 \ud6c8\ub828\uc5d0 \ub300\ud55c ablation study \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ucea1\uc158 \uc791\uc5c5\uc758 \uacbd\uc6b0 SFT \ub2e8\uacc4 \uc774\ud6c4\uc758 \uacb0\uacfc\uac00 \ubcf4\uace0\ub418\uace0, \ub2e4\ub978 \uc791\uc5c5\uc758 \uacbd\uc6b0 \uc0ac\uc804 \ud6c8\ub828 \ub2e8\uacc4 \uc774\ud6c4\uc758 \uacb0\uacfc\uac00 \ubcf4\uace0\ub429\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ucea1\uc158 \uc0dd\uc131, \uc9e7\uc740 \ube44\ub514\uc624 QA, \uae34 \ube44\ub514\uc624 QA, \ud658\uac01 \ud14c\uc2a4\ud2b8 \uc138 \uac00\uc9c0 \ub2a5\ub825 \ubc94\uc8fc\uc5d0 \ub300\ud55c \uc815\ub7c9\uc801 \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ubc94\uc8fc\uc5d0 \ub300\ud574 \uc5ec\ub7ec \uac1c\uc758 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uace0, Tarsier1-7B, Tarsier1-7B-Qwen(\ubaa8\ub378 \uc5c5\uadf8\ub808\uc774\ub4dc), Tarsier2-7B(\ub370\uc774\ud130 \ubc0f \ubaa8\ub378 \uc5c5\uadf8\ub808\uc774\ub4dc) \uc138 \uac00\uc9c0 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uc0ac\uc804 \ud6c8\ub828 \ub370\uc774\ud130 \ud06c\uae30 \ubc0f \uae30\ubcf8 \ubaa8\ub378\uc758 \uc601\ud5a5\uc744 \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2.1 \uc0ac\uc804 \ud6c8\ub828"}, {"content": "| Configuration | Pre-training | SFT-1 | SFT-2 | DPO |\n|---|---|---|---|---|\n| VLM init. | Qwen2-VL-7B | Tarsier2-Pre-trian | Tarsier2-SFT-1 | Tarsier2-SFT-2 |\n| Optimizer name | AdamW | AdamW | AdamW | AdamW |\n| Optimizer \\$\\beta_{1}$ | 0.9 | 0.9 | 0.9 | 0.9 |\n| Optimizer \\$\\beta_{2}$ | 0.999 | 0.999 | 0.999 | 0.999 |\n| Optimizer eps | 1e^{-6} | 1e^{-6} | 1e^{-6} | 1e^{-6} |\n| Learning rate | 2e^{-5} | 2e^{-5} | 2e^{-6} | 1e^{-6} |\n| Learning rate schedule | cosine | cosine | cosine | cosine |\n| Training steps | 200,000 | 5,000 | 5,000 | 1,000 |\n| Warm-up steps | 1,000 | 250 | 250 | 100 |\n| Weight decay | 0.01 | 0.01 | 0.01 | 0.01 |\n| Gradient clip | 1.0 | 1.0 | 1.0 | 1.0 |\n| Dropout rate | 0.0 | 0.0 | 0.0 | 0.0 |\n| Global batch size | 384 | 64 | 64 | 64 |\n| Max pixels | 460,800 | 460,800 | 460,800 | 460,800 |\n| Frames per video | [8,128] | 16 | 16 | 16 |\n| Numerical precision | bfloat16 | bfloat16 | bfloat16 | bfloat16 |", "caption": "Table 15: Detailed results of the ablation study for SFT.", "description": "\ud45c 15\ub294 SFT(Supervised Fine-Tuning) \ub2e8\uacc4\uc758 ablation study \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  SFT \ub2e8\uacc4\uc5d0\uc11c fine-grained temporal grounding \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c\uc640 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \ube44\ub514\uc624 \uc774\ud574 \uc791\uc5c5(\ucea1\uc158 \uc0dd\uc131, \uc9e7\uc740 \ube44\ub514\uc624 \uc9c8\uc758\uc751\ub2f5, \uae34 \ube44\ub514\uc624 \uc9c8\uc758\uc751\ub2f5, \ud658\uac01)\uc5d0 \ub300\ud55c \uc815\ub7c9\uc801 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uc138 \uac00\uc9c0 \ubaa8\ub378(Tarsier2-7B-SFT, grounding \uc815\ubcf4 \uc5c6\uc774 fine-tuning\ub41c \ubaa8\ub378, SFT \ub2e8\uacc4 \uc5c6\uc774 pre-training\ub9cc \uc9c4\ud589\ub41c \ubaa8\ub378)\uc744 \ube44\uad50\ud558\uc5ec fine-grained temporal grounding\uc758 \uc911\uc694\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "4.2 SFT"}, {"content": "| Task                     | Dataset             | # Samples | Notes                                   |\n|--------------------------|----------------------|------------|----------------------------------------|\n| **Video Captioning**      |                       |            |                                        |\n| WebVid                    | [10]                 | 2.9M       |                                        |\n| LSMDC                     | [92]                 | 109K       |                                        |\n| TGIF                      | [59]                 | 105K       |                                        |\n| ActivityNet               | [47]                 | 38K        |                                        |\n| Charades                  | [97]                 | 16K        |                                        |\n| Charades-Ego              | [96]                 | 6K         |                                        |\n| YouCook2                  | [129]                | 9K         |                                        |\n| TACoS                     | [90]                 | 18K        |                                        |\n| Ego4D                     | [35]                 | 1.1M       |                                        |\n| Spoken Moments            | [82]                 | 493K       |                                        |\n| Multi-Moments             | [83]                 | 997K       |                                        |\n| TREC-VTT                  | [7]                  | 64K        |                                        |\n| ShareGPT-4o-video        | [26]                 | 2K         |                                        |\n| MovieStory101            | [38]                 | 11K        |                                        |\n| GPT4o-labeled Caption\u2020   |                       | 2.5M       |                                        |\n| Human-labeled Caption\u2020   |                       | 145K       |                                        |\n| Film&TV Commentary\u2020      |                       | 11.5M      |                                        |\n| **Action Recognition**    |                       |            |                                        |\n| HMDB                      | [49]                 | 5.8K       |                                        |\n| COIN                      | [101]                | 10K        |                                        |\n| SSV2                      | [34]                 | 169K       |                                        |\n| Kinetics-700             | [13]                 | 537K       |                                        |\n| FineAction                | [68]                 | 82K        |                                        |\n| RareAct                   | [80]                 | 2K         |                                        |\n| 20BN-jester              | [79]                 | 46K        |                                        |\n| **Video QA**              |                       |            |                                        |\n| CLEVRER                   | [120]                | 83K        |                                        |\n| TGIF-QA                   | [43]                 | 72K        |                                        |\n| EgoQA                     | [29]                 | 5K         |                                        |\n| VideoInstruct             | [76]                 | 89K        |                                        |\n| LLaVA-Video-178K         | [127]                | 165K       |                                        |\n| M4-Instruct-video        | [52]                 | 255K       |                                        |\n| GPT4o-labeled QA\u2020        |                       | 16.2K      |                                        |\n| **Grounding**             |                       |            |                                        |\n| DiDeMo                    | [4]                  | 82K        |                                        |\n| AVA                       | [36]                 | 28K        |                                        |\n| E.T. Instruct 164K       | [67]                 | 147K       |                                        |\n| Object Tracking\u2020          |                       | 745K       |                                        |\n| **Video Self-Supervised Training** |                       |            |                                        |\n| Frame Order Prediction\u2020    |                       | 825K       |                                        |\n| **Intent Recognition**    |                       |            |                                        |\n| Oops!                     | [28]                 | 15K        |                                        |\n| **Multi-Image Understanding** |                       |            |                                        |\n| VIST                      | [40]                 | 38K        |                                        |\n| MMDU                      | [71]                 | 45K        |                                        |\n| M4-Instruct-image        | [52]                 | 616K       |                                        |\n| Image Retrival\u2020           |                       | 533K       |                                        |\n| **Single-Image Understanding** |                       |            |                                        |\n| ShareGPT4V                | [15]                 | 95K        |                                        |\n| LLaVA-1.5                | [64]                 | 643K       |                                        |\n| ShareGPT-4o-image        | [26]                 | 57K        |                                        |\n| MS COCO                   | [63]                 | 566K       |                                        |\n| Flicker                   | [87]                 | 145K       |                                        |\n| LLaVA-ReCap-CC3M         | [52]                 | 2.9M       |                                        |\n| Visual Genome             | [48]                 | 759K       |                                        |\n| SBU Captions              | [84]                 | 860K       |                                        |\n| GPT4o-labeled Caption\u2020   |                       | 1.13M      |                                        |\n| **Image OCR**             |                       |            |                                        |\n| RCTW-17                   | [95]                 | 8K         |                                        |\n| LSVT                      | [98]                 | 430K       |                                        |\n| ReCTS                     | [125]                | 20K        |                                        |\n| Art                       | [11]                 | 5.6K       |                                        |\n| COCOTextV2                | [103]                | 16K        |                                        |\n| CORD-v2                   | [85]                 | 1K         |                                        |\n| HierText                  | [73]                 | 10K        |                                        |\n| MSRA-TD500                | [118]                | 465        |                                        |\n| IC03                      | [74]                 | 499        |                                        |\n| SynthDoG-en              | [46]                 | 100K       |                                        |\n| SynthDoG-zh              | [46]                 | 100K       |                                        |\n| **Text Generation**       |                       |            |                                        |\n| OpenOrca                  | [60]                 | 995K       |                                        |\n| ShareGPT                  | [24]                 | 80K        |                                        |", "caption": "Table 16: Detailed results of the ablation study for DPO.", "description": "\ud45c 16\uc740 DPO(Direct Preference Optimization) \ub2e8\uacc4\uc5d0 \ub300\ud55c ablation study \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 ablation \uc2e4\ud5d8\uc740 DPO \ud6c8\ub828\uc5d0\uc11c \ud2b9\uc815 \uc694\uc18c(\uc74c\uc131 \uc0d8\ud50c\ub9c1, \uc120\ud638\ub3c4 \ud544\ud130\ub9c1)\ub97c \uc81c\uac70\ud558\uc5ec \uc218\ud589\ub418\uc5c8\uc73c\uba70,  'Caption', 'Video QA', 'Hallucination' \uc138 \uac00\uc9c0 \ub2a5\ub825\uc5d0 \ub300\ud55c \uc601\ud5a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.  \uac01 \ub2a5\ub825\uc740 \uc5ec\ub7ec \ud558\uc704 \uc791\uc5c5(\uc608: DREAM-1K, TempCompass-cg, Vinoground-Text \ub4f1)\uc758 \ud3c9\uade0 \uc810\uc218\ub85c \uce21\uc815\ub429\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uac01 \uad6c\uc131 \uc694\uc18c\uac00 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\uace0, DPO \uc804\ub7b5\uc758 \ud6a8\uacfc\ub97c \uc790\uc138\ud788 \uc774\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.2.3 DPO"}, {"content": "| Capability | Benchmark | Tarsier1-7B | Tarsier1-7B-Qwen | Tarsier2-7B |\n|---|---|---|---|---|\n| Caption | DREAM-1K | 34.6/30.2/40.3 | 38.4/40.6/36.4 | 40.8/42.5/39.3 |\n|  | TempCompass-cg | 55.3 | 59.3 | 60.1 |\n|  | Vinoground-Text | 29.8 | 48.6 | 60.2 |\n| Video QA Short | MVBench | 62.6 | 69.8 | 72.8 |\n|  | TVBench | 45.8 | 51.0 | 53.5 |\n|  | TOMATO | 28.6 | 36.5 | 39.5 |\n| Video QA Long | Video-MME | 42.2 | 58.9 | 65.3 |\n|  | LongVideoBench | 39.8 | 52.1 | 58.3 |\n|  | TemporalBench | 56.9 | 61.9 | 68.7 |\n| Hallucination | EventHallusion-Y/N | 70.9 | 75.6 | 77.8 |\n|  | EventHallusion-Desc | 41.6 | 48.6 | 49.1 |", "caption": "Table 17: Detailed results of the recaptioning experiment.", "description": "\ud45c 17\uc740 Tarsier2-Recap-585K \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud55c \uc7ac\ucea1\uc158 \uc791\uc5c5\uc758 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Tarsier2-Recap-585K\ub294 \uae30\uc874\uc758 \ube44\ub514\uc624 \ucea1\uc158 \ub370\uc774\ud130\uc14b\uc5d0 \ub450 \uac00\uc9c0 \uc561\uc158 \uc778\uc2dd \ub370\uc774\ud130\uc14b\uc744 \ucd94\uac00\ud558\uc5ec \uad6c\uc131\ub41c \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4.  \ud45c\ub294 \uc7ac\ucea1\uc158 \uc2e4\ud5d8\uc744 \uc704\ud574 \uc6d0\ubcf8 \ucea1\uc158\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubbf8\uc138 \uc870\uc815\ud55c \ubaa8\ub378\uacfc Tarsier2-Recap-585K \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec DREAM-1K, TempCompass-cg, Vinoground-Text \uc640 \uac19\uc740 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 F1 \uc810\uc218, \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728, \uc815\ud655\ub3c4 \ub4f1 \ub2e4\uc591\ud55c \uc9c0\ud45c\ub97c \ud1b5\ud574 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ube44\ub514\uc624 \uc9c8\uc758\uc751\ub2f5(Video QA)\uacfc \ud658\uac01(Hallucination) \ud14c\uc2a4\ud2b8 \uacb0\uacfc \ub610\ud55c \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.3 Video Recaptioning using Tarsier2"}, {"content": "| Capability | Benchmark | pre-train | Tarsier2-7B | SFT |\n|---|---|---|---|---|\n| Caption | DREAM-1K | 35.2/36.8/33.7 | 37.4/38.6/36.3 | 40.8/42.5/39.3 |\n|  | TempCompass-cg | 50.5 | 50.2 | 60.1 |\n|  | Vinoground-Text | 57.2 | 60.6 | 60.2 |\n| Video QA Short | MVBench | 72.8 | 71.9 | 72.5 |\n|  | TVBench | 53.5 | 54.5 | 54.2 |\n|  | TOMATO | 39.5 | 41.3 | 41.9 |\n| Video QA Long | Video-MME | 65.3 | 64.0 | 64.7 |\n|  | LongVideoBench | 58.3 | 54.7 | 58.2 |\n|  | TemporalBench | 68.7 | 66.9 | 66.6 |\n| Hallucination | EventHallusion-Y/N | 77.8 | 80.1 | 84.4 |\n|  | EventHallusion-Desc | 49.1 | 56.2 | 59.4 |", "caption": "Table 18: Data composition of Tarsier2-Recap-585K. The \u201cSplit\u201d column lists the original dataset partitioning, and we use bold to mark the parts which we sampled the video clips from to conduct recaptioning.", "description": "Tarsier2-Recap-585K \ub370\uc774\ud130\uc14b\uc758 \uad6c\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4. \uc6d0\ubcf8 \ub370\uc774\ud130\uc14b\uc758 \ubd84\ud560 \ubc29\uc2dd\uc744 \"Split\" \uc5f4\uc5d0 \ub098\ud0c0\ub0b4\uace0, \uc7ac\ucea1\uc158 \uc791\uc5c5\uc744 \uc704\ud574 \ube44\ub514\uc624 \ud074\ub9bd\uc744 \uc0d8\ud50c\ub9c1\ud55c \ubd80\ubd84\uc744 \uad75\uc740 \uae00\uc528\uccb4\ub85c \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc6d0\ubcf8 \ub808\uc774\ube14 \uc720\ud615, \uc0d8\ud50c\ub9c1\ub41c \ud074\ub9bd\uc758 \uc218, \ud3c9\uade0 \uc9c0\uc18d \uc2dc\uac04, \uc804\uccb4 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ucc28\uc9c0\ud558\ub294 \ube44\uc728 \ub4f1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.3 Video Recaptioning using Tarsier2"}]