[{"figure_path": "https://arxiv.org/html/2501.15570/extracted/6157273/arwkv-2.drawio.png", "caption": "Figure 1: replace self-attention by RWKV-7 time mixing module", "description": "\uadf8\ub9bc 1\uc740 RWKV-7\uc758 \uc2dc\uac04 \ud63c\ud569 \ubaa8\ub4c8\ub85c \uc790\uae30 \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uc744 \ub300\uccb4\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\uc874\uc758 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uad6c\uc870\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \uc790\uae30 \uc8fc\uc758(self-attention) \uba54\ucee4\ub2c8\uc998 \ub300\uc2e0 RWKV-7\uc758 \uc2dc\uac04 \ud63c\ud569 \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud558\uc5ec RNN \uae30\ubc18 \uc5b8\uc5b4 \ubaa8\ub378\uc744 \uad6c\ucd95\ud558\ub294 \ubc29\ubc95\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \uc124\uba85\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\ub294 \uc785\ub825 \uc784\ubca0\ub529\uc5d0\uc11c\ubd80\ud130 \ucd9c\ub825 \uc784\ubca0\ub529\uae4c\uc9c0\uc758 \uc804\uccb4 \uacfc\uc815\uacfc \uac01 \uad6c\uc131 \uc694\uc18c(RMSNorm, SwiGLU, Q, K, V, Time Mixing Module \ub4f1)\uc758 \uc5f0\uacb0 \uad00\uacc4\uac00 \uc790\uc138\ud788 \ub098\ud0c0\ub098 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc744 \ud1b5\ud574 \uae30\uc874\uc758 \uc790\uae30 \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uc744 RWKV-7\uc758 \uc2dc\uac04 \ud63c\ud569 \ubaa8\ub4c8\ub85c \ub300\uccb4\ud558\ub294 \ubc29\ubc95\uacfc \uadf8 \uacfc\uc815\uc5d0\uc11c\uc758 \uc8fc\uc694 \uad6c\uc131 \uc694\uc18c\ub4e4\uc758 \uc5ed\ud560\uc744 \uba85\ud655\ud558\uac8c \uc774\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3 From Transformer to RNN"}, {"figure_path": "https://arxiv.org/html/2501.15570/extracted/6157273/rwkv7.png", "caption": "Figure 2: RWKV-7 architecture.capability of attention is the key for RNN-based LLMs, which in this case is Time mixing module", "description": "\uadf8\ub9bc 2\ub294 RWKV-7 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 RNN \uae30\ubc18\uc758 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378\uc5d0\uc11c \uc5b4\ud150\uc158(attention) \uba54\ucee4\ub2c8\uc998\uc758 \uc911\uc694\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4. \ud2b9\ud788, \uc774 \ubaa8\ub378\uc5d0\uc11c\ub294 \uc2dc\uac04 \ud63c\ud569(Time mixing) \ubaa8\ub4c8\uc774 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \uc5ed\ud560\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.  \uadf8\ub9bc\uc5d0\uc11c\ub294 \uc785\ub825 \uc784\ubca0\ub529(Input Embedding)\ubd80\ud130 \ucd9c\ub825 \uc784\ubca0\ub529(Output Embedding)\uae4c\uc9c0\uc758 \uacfc\uc815\uc744 \ub2e8\uacc4\ubcc4\ub85c \ubcf4\uc5ec\uc8fc\ub294 \ub2e4\uc591\ud55c \ub808\uc774\uc5b4(Layer)\ub4e4\uc774 \ud45c\ud604\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ub808\uc774\uc5b4\ub294 \ub808\uc774\uc5b4 \uc815\uaddc\ud654(Layer Norm), \uc2dc\uac04 \ud63c\ud569 \ubaa8\ub4c8, \ucc44\ub110 \ud63c\ud569 \ubaa8\ub4c8(Channel Mix Module), \uadf8\ub9ac\uace0 ReLU \ud65c\uc131\ud654 \ud568\uc218\ub97c \uc0ac\uc6a9\ud55c \ud53c\ub4dc\ud3ec\uc6cc\ub4dc \ub124\ud2b8\uc6cc\ud06c(Feedforward Network) \ub4f1\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \uc544\ud0a4\ud14d\ucc98\ub294 \uc21c\ud658 \uc2e0\uacbd\ub9dd(RNN)\uc758 \ud2b9\uc131\uc744 \ud65c\uc6a9\ud558\uc5ec \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \ucc98\ub9ac\ud558\uace0, \uc7a5\uae30 \uc758\uc874\uc131(Long-range dependency) \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "2 Architecture"}, {"figure_path": "https://arxiv.org/html/2501.15570/extracted/6157273/arwkv-decoderlayer.png", "caption": "Figure 3: General Decoder Layer in transformer", "description": "\uadf8\ub9bc 3\uc740 Transformer\uc758 \uc77c\ubc18\uc801\uc778 \ub514\ucf54\ub354 \ub808\uc774\uc5b4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c \ub514\ucf54\ub354\ub294 \uc785\ub825 \uc2dc\ud000\uc2a4\ub97c \ucc98\ub9ac\ud558\uace0 \ucd9c\ub825 \uc2dc\ud000\uc2a4\ub97c \uc0dd\uc131\ud558\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774, \uc77c\ubc18\uc801\uc778 \ub514\ucf54\ub354 \ub808\uc774\uc5b4\ub294 \uc140\ud504 \uc5b4\ud150\uc158(self-attention), \uc794\ucc28 \uc5f0\uacb0(residual connection), \ud6c4\ucc98\ub9ac \ub808\uc774\uc5b4 \uc815\uaddc\ud654(post-layer normalization), \uadf8\ub9ac\uace0 \ub2e4\uce35 \ud37c\uc149\ud2b8\ub860(MLP)\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \uc140\ud504 \uc5b4\ud150\uc158\uc740 \uc785\ub825 \uc2dc\ud000\uc2a4 \ub0b4\uc758 \ub2e8\uc5b4\ub4e4 \uac04\uc758 \uad00\uacc4\ub97c \ud30c\uc545\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\uace0, \uc794\ucc28 \uc5f0\uacb0\uc740 \uadf8\ub798\ub514\uc5b8\ud2b8 \uc18c\uc2e4 \ubb38\uc81c\ub97c \uc644\ud654\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4. \ud6c4\ucc98\ub9ac \ub808\uc774\uc5b4 \uc815\uaddc\ud654\ub294 \ubaa8\ub378\uc758 \uc548\uc815\uc131\uc744 \ub192\uc774\uace0, MLP\ub294 \ube44\uc120\ud615 \ubcc0\ud658\uc744 \uc218\ud589\ud558\uc5ec \ubaa8\ub378\uc758 \ud45c\ud604 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.", "section": "3 From Transformer to RNN"}, {"figure_path": "https://arxiv.org/html/2501.15570/extracted/6157273/arwkv-workflow.png", "caption": "Figure 4: We replace the standard Attention with an AttentionWrapper that contains both the original self-attention mechanism and a TimeMixer. The TimeMixer is trained to minimize the gap between its output and that of the self-attention module. The final output combines the hidden states from the original self-attention with the residual difference between self-attention and TimeMixer outputs. This architecture enables the model to optimize the TimeMixer to progressively reduce the discrepancy between self-attention and TimeMixer outputs.", "description": "\uc774 \uadf8\ub9bc\uc740 \uae30\uc874\uc758 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uc790\uac00 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uacfc TimeMixer\ub97c \ubaa8\ub450 \ud3ec\ud568\ud558\ub294 AttentionWrapper\ub85c \ub300\uccb4\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. TimeMixer\ub294 \uc790\uac00 \uc5b4\ud150\uc158 \ubaa8\ub4c8\uc758 \ucd9c\ub825\uacfc\uc758 \ucc28\uc774\ub97c \ucd5c\uc18c\ud654\ud558\ub3c4\ub85d \ud559\uc2b5\ub429\ub2c8\ub2e4. \ucd5c\uc885 \ucd9c\ub825\uc740 \uc6d0\ub798 \uc790\uac00 \uc5b4\ud150\uc158\uc758 \uc740\ub2c9 \uc0c1\ud0dc\uc640 \uc790\uac00 \uc5b4\ud150\uc158 \ubc0f TimeMixer \ucd9c\ub825 \uac04\uc758 \uc794\ucc28 \ucc28\uc774\ub97c \uacb0\ud569\ud569\ub2c8\ub2e4. \uc774 \uc544\ud0a4\ud14d\ucc98\ub97c \ud1b5\ud574 \ubaa8\ub378\uc740 \uc790\uac00 \uc5b4\ud150\uc158\uacfc TimeMixer \ucd9c\ub825 \uac04\uc758 \ubd88\uc77c\uce58\ub97c \uc810\uc9c4\uc801\uc73c\ub85c \uc904\uc774\ub3c4\ub85d TimeMixer\ub97c \ucd5c\uc801\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.1 Stage 1 - Time Mixing module replacing Self-Attention"}, {"figure_path": "https://arxiv.org/html/2501.15570/extracted/6157273/stage1_no_norm_loss.png", "caption": "Figure 5: Stage-1 loss, 18 hours with one 8*h800 80G , context length 2048, 4B tokens", "description": "\uadf8\ub9bc 5\ub294 Stage-1 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c\uc758 \uc190\uc2e4 \uac12 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e8\uc77c 8x A100 80GB GPU\ub97c \uc0ac\uc6a9\ud558\uc5ec 18\uc2dc\uac04 \ub3d9\uc548 \ud559\uc2b5\uc744 \uc9c4\ud589\ud588\uc73c\uba70, \ubb38\ub9e5 \uae38\uc774(context length)\ub294 2048 \ud1a0\ud070, \ucd1d \ud1a0\ud070 \uc218\ub294 40\uc5b5 \uac1c\uc600\uc2b5\ub2c8\ub2e4. \uadf8\ub798\ud504\ub294 \ud559\uc2b5 \uc9c4\ud589\uc5d0 \ub530\ub978 \uc190\uc2e4 \uac12\uc758 \uac10\uc18c \ucd94\uc138\ub97c \ub098\ud0c0\ub0b4\uba70, \ubaa8\ub378 \ud559\uc2b5\uc758 \uc548\uc815\uc131\uacfc \ud6a8\uc728\uc131\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 \ud559\uc2b5 \ub2e8\uacc4(step)\ub97c, y\ucd95\uc740 \uc190\uc2e4 \uac12(loss)\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "3.1 Stage 1 - Time Mixing module replacing Self-Attention"}]