{"references": [{"fullname_first_author": "A. Bick", "paper_title": "Transformers to ssms: Distilling quadratic knowledge to subquadratic models", "publication_date": "2024-08-10", "reason": "This paper is foundational to the knowledge distillation method used in the current work, which is central to its approach."}, {"fullname_first_author": "X. Dong", "paper_title": "Hymba: A hybrid-head architecture for small language models", "publication_date": "2024-11-13", "reason": "This paper provides background on hybrid attention models, a key concept discussed and compared to in the current work."}, {"fullname_first_author": "R. Grazzi", "paper_title": "Unlocking state-tracking in linear rnns through negative eigenvalues", "publication_date": "2024-11-12", "reason": "This paper is crucial in explaining the properties of RWKV-7 attention, a core component of the model presented."}, {"fullname_first_author": "D. Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-12", "reason": "This paper is cited for future work, indicating its relevance to the model's development trajectory."}, {"fullname_first_author": "W. Merrill", "paper_title": "The illusion of state in state-space models", "publication_date": "2024-04-08", "reason": "This paper is referenced in relation to state tracking, a key feature of the proposed model and a point of comparison with transformers."}]}