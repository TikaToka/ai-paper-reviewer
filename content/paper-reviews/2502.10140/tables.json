[{"content": "| Model | #Params (B) | TC (\u2191) | NER (\u2191) |\n|---|---|---|---|\n| mBERT+<span class=\"ltx_text ltx_font_typewriter\">Seq_bn_inv</span> | **0.177** | **71.92** | **85.28** |\n| XLM-R+<span class=\"ltx_text ltx_font_typewriter\">Seq_bn_inv</span> | **0.279** | **80.79** | **85.42** |\n| DeepSeek-R1-D-Llama | 8 | 20.5 | - |\n| DeepSeek-R1-D-Qwen | 14 | 41.88 | - |\n| DeepSeek-R1-D-Qwen | 32 | 68.54 | - |\n| DeepSeek-R1-D-Llama | 70 | 70.72 | - |\n| LLaMA-3 | 8 | 65.8 | - |\n| LLaMA-3.1 | 8 | 65.62 | - |\n| Gemma | 7 | 60.21 | - |\n| Gemma-2 | 9 | 44.27 | - |\n| Qwen-1.5 | 7 | 40.41 | - |\n| Qwen2 | 7 | 56.82 | - |\n| GPT-3.5-turbo-0301 | - | - | 70.65 |\n| GPT-3.5-turbo-0613 | - | 45.02 | - |\n| GPT-4-0613 | - | 45.82 | - |\n| LLaMA-2 | 7 | 18.24 | - |\n| BLOOM | 7 | 13.02 | 31.35 |\n| BLOOMz | 7 | 17.51 | 20.92 |\n| mT0 | 13 | - | 17.48 |\n| Occiglot-eu5 | 7 | 28.56 |  |\n| XGLM | 7.5 | 29.98 | - |\n| Yayi | 7 | 16.88 | - |\n| LLaMAX2 Alpaca | 7 | 23.13 | - |\n| Mala-500-v2 | 10 | 5.74 | - |", "caption": "Table 1: Results for mBERT and XLM-R across 4 tasks: Topic Classification (TC), Named Entity Recognition (NER), Sentiment Analysis (SA), Masked Language Modeling (MLM). All numbers are the averages for the 30 studied LRLs and provided separately for the languages included (\u201dseen\u201d) and languages not included (\u201dunseen\u201d) in the pre-training data of a model. The baselines are the models with a single task adapter for downstream tasks, or without adapters for MLM. The full results for each task are in the Appendix.", "description": "\ud45c 1\uc740 30\uac1c\uc758 \uc800\uc790\uc6d0 \uc5b8\uc5b4(LRL)\uc5d0 \ub300\ud574 mBERT\uc640 XLM-R \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub124 \uac00\uc9c0 \uacfc\uc81c(\uc8fc\uc81c \ubd84\ub958, \uac1c\uccb4\uba85 \uc778\uc2dd, \uac10\uc815 \ubd84\uc11d, \ub9c8\uc2a4\ud06c \uc5b8\uc5b4 \ubaa8\ub378\ub9c1)\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uba70, \uac01 \uacfc\uc81c\uc5d0 \ub300\ud574 \uc0ac\uc804 \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \ud3ec\ud568\ub41c \uc5b8\uc5b4\uc640 \ud3ec\ud568\ub418\uc9c0 \uc54a\uc740 \uc5b8\uc5b4\ub85c \ub098\ub204\uc5b4 \ud3c9\uade0 \uc810\uc218\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \ubca0\uc774\uc2a4\ub77c\uc778\uc740 \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc5d0 \ub2e8\uc77c \ud0dc\uc2a4\ud06c \uc5b4\ub311\ud130\ub97c \uc0ac\uc6a9\ud558\uac70\ub098 MLM\uc758 \uacbd\uc6b0 \uc5b4\ub311\ud130\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \ubaa8\ub378\uc785\ub2c8\ub2e4. \ubd80\ub85d\uc5d0\ub294 \uac01 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc804\uccb4 \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5 Results: Small mLMs"}, {"content": "| Model | TC (\u2191) | SA (\u2191) | NER (\u2191) |\n|---|---|---|---|\n| mBERT+<tt>Seq_bn_inv</tt> | **71.92** | **73.68** | **59.32** |\n| XLM-R+<tt>Seq_bn_inv</tt> | **80.79** | **83.35** | **69.26** |\n| LLaMA-3 Baseline | 31.93 | 58.83 | 45.18 |\n| LLaMA-3+<tt>Seq_bn_inv</tt> | 60.26 | 68.68 | 45.12 |", "caption": "Table 2: Average F1 scores on overlapping LRLs for LLMs and our Glot adapter-based mLMs on TC and NER. Prompting results are 3-shot, based on Ji et\u00a0al. (2024) for TC and Asai et\u00a0al. (2023) for NER. For NER, we report averages across eight overlapping languages, while the GPT-3.5 average is based on only two. TC results for GPT-3.5 and GPT-4 are zero-shot, as reported by Adelani et\u00a0al. (2024a). DeepSeek results are zero-shot and were obtained in our evaluation. Per-language results are in Appendix U.", "description": "\ud45c 2\ub294 TC\uc640 NER \uc791\uc5c5\uc5d0 \ub300\ud574 \uc911\uad6d\uc5b4\uac00 \uc544\ub2cc \uc800\uc790\uc6d0 \uc5b8\uc5b4(LRL)\uc5d0 \ub300\ud55c \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uacfc Glot \uc5b4\ub311\ud130 \uae30\ubc18\uc758 \uc18c\uaddc\ubaa8 \ub2e4\uad6d\uc5b4 \ubaa8\ub378(mLMs)\uc758 \ud3c9\uade0 F1 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud504\ub86c\ud504\ud305 \uacb0\uacfc\ub294 Ji et al.(2024)\uc758 TC\uc640 Asai et al.(2023)\uc758 NER\uc744 \uae30\ubc18\uc73c\ub85c 3-shot\uc785\ub2c8\ub2e4. NER\uc758 \uacbd\uc6b0 8\uac1c\uc758 \uc911\ubcf5 \uc5b8\uc5b4\uc5d0 \ub300\ud55c \ud3c9\uade0\uc744 \ubcf4\uace0\ud558\uba70, GPT-3.5\uc758 \ud3c9\uade0\uc740 2\uac1c \uc5b8\uc5b4\uc5d0 \ub300\ud574\uc11c\ub9cc \uc0b0\ucd9c\ub429\ub2c8\ub2e4. Adelani et al.(2024a)\uac00 \ubcf4\uace0\ud55c \ubc14\uc640 \uac19\uc774 GPT-3.5\uc640 GPT-4\uc758 TC \uacb0\uacfc\ub294 \uc81c\ub85c\uc0f7\uc774\uba70, DeepSeek \uacb0\uacfc\ub294 \uc800\uc790\uc758 \ud3c9\uac00\ub97c \ud1b5\ud574 \uc81c\ub85c\uc0f7\uc73c\ub85c \uc5bb\uc5b4\uc84c\uc2b5\ub2c8\ub2e4. \uac01 \uc5b8\uc5b4\ubcc4 \uacb0\uacfc\ub294 \ubd80\ub85d U\uc5d0 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5 Results: Small mLMs"}, {"content": "| ConceptNet Relationship | Natural Language Predicate |\n|---|---| \n| Antonym | is the opposite of |\n| DerivedFrom | is derived from |\n| EtymologicallyDerivedFrom | is etymologically derived from |\n| EtymologicallyRelatedTo | is etymologically related to |\n| FormOf | is a form of |\n| PartOf | is a part of |\n| HasA | belongs to |\n| UsedFor | is used for |\n| AtLocation | is a typical location for |\n| Causes | causes |\n| CausesDesire | makes someone want |\n| MadeOf | is made of |\n| ReceivesAction | receives action of |\n| HasSubevent | is a subevent of |\n| HasFirstSubevent | is an event that begins with subevent |\n| HasLastSubevent | is an event that concludes with subevent |\n| HasPrerequisite | has prerequisite of |\n| HasProperty | can be described as |\n| MotivatedByGoal | is a step toward accomplishing the goal |\n| ObstructedBy | is an obstacle in the way of |\n| Desires | is a conscious entity that typically wants |\n| CreatedBy | is a process or agent that creates |\n| CapableOf | is capable of |\n| HasContext | is a word used in the context of |\n| IsA | is a type of |\n| RelatedTo | is related to |\n| SimilarTo | is similar to |\n| Synonym | is a synonym of |\n| SymbolOf | symbolically represents |\n| DefinedAs | is a more explanatory version of |\n| DistinctFrom | is distinct from |\n| MannerOf | is a specific way to do |\n| LocatedNear | is typically found near |", "caption": "Table 3: Average F1 scores over 5 selected LRLs for language adapter-tuned LLaMa-3-8B, mBERT, and XLM-R. Additionally, we present results for LLaMA3 with a single Seq_bn task adapter, similar to our baselines. Per-language results are in Appendix U.", "description": "\ubcf8 \ud45c\ub294 \ub17c\ubb38\uc758 5\uac1c \uc800\uc790\uc6d0 \uc5b8\uc5b4(LRLs)\uc5d0 \ub300\ud574 \uc5b8\uc5b4 \uc5b4\ub311\ud130\ub85c \ubbf8\uc138 \uc870\uc815\ub41c LLaMA-3-8B, mBERT, XLM-R \ubaa8\ub378\uc758 \ud3c9\uade0 F1 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LLaMA-3 \ubaa8\ub378\uc5d0 \ub300\ud55c \uae30\uc900\uc120 \uacb0\uacfc\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uc774\ub294 \ubcf8 \uc5f0\uad6c\uc758 \uae30\uc900\uc120\uacfc \uc720\uc0ac\ud558\uac8c \ub2e8\uc77c Seq_bn \uc791\uc5c5 \uc5b4\ub311\ud130\ub97c \uc0ac\uc6a9\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. \uac01 \uc5b8\uc5b4\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uacb0\uacfc\ub294 \ubd80\ub85d U\uc5d0 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5 \uacb0\uacfc: \uc18c\uaddc\ubaa8 \ub2e4\uad6d\uc5b4 \uc5b8\uc5b4 \ubaa8\ub378"}, {"content": "| Language | ISO | Language Family | CN (Sent-s) | CN (MB) | Glot (Doc-s) | Glot (MB) | mBERT? | XLM-R? | mBERT Data Size (GB) | XLM-R Data Size (GB) |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Thai | th | Kra-Dai | 123,859 | 6.95 | 2,391,253 | 977.68 | \u2713 | \u2713 | 1.29 | 85.24 |\n| Romanian | ro | Indo-European | 70,236 | 2.47 | 8,657,002 | 1002.36 | \u2713 | \u2713 | 1.22 | 83.29 |\n| Bulgarian | bg | Indo-European | 162,181 | 8.02 | 5,192,702 | 1014.73 | \u2713 | \u2713 | 1.50 | 70.37 |\n| Danish | da | Indo-European | 66,109 | 2.27 | 8,743,767 | 1006.91 | \u2713 | \u2713 | 0.81 | 62.39 |\n| Greek | el | Indo-European | 89,016 | 4.17 | 4,789,519 | 980.94 | \u2713 | \u2713 | 1.85 | 57.30 |\n| Hebrew | he | Afro-Asiatic | 41,444 | 1.62 | 5,287,428 | 991.82 | \u2713 | \u2713 | 2.73 | 40.87 |\n| Slovak | sk | Indo-European | 22,460 | 0.81 | 9,294,165 | 1006.96 | \u2713 | \u2713 | 0.61 | 31.96 |\n| Slovenian | sl | Indo-European | 85,882 | 2.98 | 9,301,902 | 1007.91 | \u2713 | \u2713 | 0.67 | 14.16 |\n| Latvian | lv | Indo-European | 66,408 | 2.4 | 8,301,651 | 988.21 | \u2713 | \u2713 | 0.33 | 11.94 |\n| Indonesian | ms | Austronesian | 175,246 | 6.21 | 8,024,827 | 1022.01 | \u2713 | \u2713 | 0.59 | 11.73 |\n| Georgian | ka | Kartvelian | 35,331 | 1.89 | 3,463,631 | 1014.24 | \u2713 | \u2713 | 0.88 | 10.55 |\n| Bengali | bn | Indo-European | 8,782 | 0.46 | 2,940,197 | 993.44 | \u2713 | \u2713 | 1.22 | 10.10 |\n| Azerbaijani | az | Turkic | 15,149 | 0.57 | 6,179,152 | 1016.68 | \u2713 | \u2713 | 0.62 | 8.33 |\n| Urdu | ur | Indo-European | 13,315 | 0.51 | 4,220,566 | 1009.42 | \u2713 | \u2713 | 0.54 | 6.97 |\n| Macedonian | mk | Indo-European | 38,116 | 1.54 | 5,037,552 | 1005.62 | \u2713 | \u2713 | 0.86 | 5.76 |\n| Telugu | te | Dravidian | 33,476 | 1.72 | 3,162,535 | 1005.55 | \u2713 | \u2713 | 0.88 | 5.46 |\n| Nepali | ne | Indo-European | 4,456 | 0.21 | 2,569,572 | 1012.63 | \u2713 | \u2713 | 0.14 | 4.32 |\n| Marathi | mr | Indo-European | 7,232 | 0.37 | 402,575 | 157.3 | \u2713 | \u2713 | 0.32 | 3.33 |\n| Swahili | sw | Niger-Congo | 12,380 | 0.39 | 2,450,753 | 323.27 | \u2713 | \u2713 | 0.10 | 2.15 |\n| Welsh | cy | Indo-European | 18,313 | 0.61 | 3,174,686 | 360.24 | \u2713 | \u2713 | 0.39 | 1.07 |\n| Uzbek | uz | Turkic | 4,362 | 0.16 | 4,018,172 | 481.49 | \u2713 | \u2713 | 0.57 | 0.95 |\n| Javanese | jv | Austronesian | 3,448 | 0.13 | 367,795 | 43.56 | \u2713 | \u2713 | 0.10 | 0.20 |\n| Sundanese | su | Austronesian | 1,880 | 0.07 | 323,610 | 43.55 | \u2713 | \u2713 | 0.06 | 0.08 |\n| Sinhala | si | Indo-European | 1,782 | 0.1 | 1,655,641 | 586.21 | \u2718 | \u2713 | \u2718 | 4.27 |\n| Amharic | am | Afro-Asiatic | 1,814 | 0.07 | 667,881 | 203.65 | \u2718 | \u2713 | \u2718 | 1.00 |\n| Kurdish | ku | Indo-European | 12,246 | 0.44 | 376,260 | 134.7 | \u2718 | \u2713 | \u2718 | 0.52 |\n| Uyghur | ug | Turkic | 1,715 | 0.06 | 976,010 | 233.61 | \u2718 | \u2713 | \u2718 | 0.43 |\n| Maltese | mt | Afro-Asiatic | 3,895 | 0.14 | 1,389,527 | 182.17 | \u2718 | \u2718 | \u2718 | \u2718 |\n| Tibetan | bo | Sino-Tibetan | 4,768 | 0.21 | 288,847 | 165.31 | \u2718 | \u2718 | \u2718 | \u2718 |\n| Yoruba | yo | Niger-Congo | 1,044 | 0.05 | 278,003 | 34.51 | \u2713 | \u2718 | 0.03 | \u2718 |", "caption": "Table 4: ConceptNet relationships and their natural language predicates. This mapping is used for converting the ConceptNet KG data into natural language text.", "description": "\ubcf8 \ud45c\ub294 ConceptNet \uc9c0\uc2dd \uadf8\ub798\ud504\uc758 \uad00\uacc4\uc640 \uc774\uc5d0 \uc0c1\uc751\ud558\ub294 \uc790\uc5f0\uc5b4\uc220\uc5b4 \uac04\uc758 \ub9e4\ud551\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. ConceptNet\uc758 \uad00\uacc4\ub294 \uc9c0\uc2dd \uadf8\ub798\ud504\uc758 \uc5d0\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 \ubc18\uba74, \uc790\uc5f0\uc5b4\uc220\uc5b4\ub294 \uc774\ub7ec\ud55c \uad00\uacc4\ub97c \ud45c\ud604\ud558\ub294 \uc0ac\ub78c\uc774 \uc774\ud574\ud558\uae30 \uc26c\uc6b4 \ubb38\uc7a5\uc73c\ub85c \ubcc0\ud658\ub41c \uac83\uc785\ub2c8\ub2e4. \uc774 \ub9e4\ud551\uc740 ConceptNet \uc9c0\uc2dd \uadf8\ub798\ud504 \ub370\uc774\ud130\ub97c \uc790\uc5f0\uc5b4 \ud14d\uc2a4\ud2b8\ub85c \ubcc0\ud658\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, 'Antonym' \uad00\uacc4\ub294 'is the opposite of' \ub77c\ub294 \uc220\uc5b4\ub85c \ub9e4\ud551\ub418\uace0, 'PartOf' \uad00\uacc4\ub294 'is part of' \ub85c \ub9e4\ud551\ub429\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub17c\ubb38\uc5d0\uc11c ConceptNet \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud55c \uc2e4\ud5d8\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ub3d5\uc2b5\ub2c8\ub2e4.", "section": "3 Methodology"}, {"content": "| Language | ISO code | #train | #val | #test |\n|---|---|---|---|---|\n| Bulgarian | bg | 20000 | 10000 | 10000 |\n| Indonesian | ms | 20000 | 1000 | 1000 |\n| Maltese | mt | 100 | 100 | 100 |\n| Nepali | ne | 100 | 100 | 100 |\n| Javanese | jv | 100 | 100 | 100 |\n| Uyghur | ug | 100 | 100 | 100 |\n| Tibetan | bo | 100 | 100 | 100 |\n| Sinhala | si | 100 | 100 | 100 |\n| Sundanese | su | 100 | 100 | 100 |\n| Amharic | am | 100 | 100 | 100 |\n| Swahili | sw | 1000 | 1000 | 1000 |\n| Georgian | ka | 10000 | 10000 | 10000 |\n| Latvian | lv | 10000 | 10000 | 10000 |\n| Slovak | sk | 20000 | 10000 | 10000 |\n| Slovenian | sl | 15000 | 10000 | 10000 |\n| Uzbek | uz | 1000 | 1000 | 1000 |\n| Yoruba | yo | 100 | 100 | 100 |\n| Urdu | ur | 20000 | 1000 | 1000 |\n| Macedonian | mk | 10000 | 1000 | 1000 |\n| Danish | da | 20000 | 10000 | 10000 |\n| Marathi | mr | 5000 | 1000 | 1000 |\n| Bengali | bn | 10000 | 1000 | 1000 |\n| Hebrew | he | 20000 | 10000 | 10000 |\n| Romanian | ro | 20000 | 10000 | 10000 |\n| Telugu | te | 1000 | 1000 | 1000 |\n| Welsh | cy | 10000 | 1000 | 1000 |\n| Azerbaijani | az | 10000 | 1000 | 1000 |\n| Greek | el | 20000 | 10000 | 10000 |\n| Kurdish | ku | 100 | 100 | 100 |\n| Thai | th | 20000 | 10000 | 10000 |", "caption": "Table 5: Number of ConceptNet triples and GlotCC documents as well as corresponding data sizes per language, sorted by Glot (Doc-s) in descending order. The last four columns indicate the inclusion of the respective language in mBERT and XLM-R pre-training data, alongside the corresponding data sizes in GB. The sizes are approximated based on the openly available CC100 and WikiPedia datasets.", "description": "\ud45c 5\ub294 \uc5b8\uc5b4\ubcc4 ConceptNet 3\uc911\ud56d \ubc0f GlotCC \ubb38\uc11c\uc758 \uc218\uc640 \ud574\ub2f9 \ub370\uc774\ud130 \ud06c\uae30\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. GlotCC \ubb38\uc11c \uc218\ub97c \uae30\uc900\uc73c\ub85c \ub0b4\ub9bc\ucc28\uc21c\uc73c\ub85c \uc815\ub82c\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9 \ub124 \uc5f4\uc740 \uac01 \uc5b8\uc5b4\uac00 mBERT \ubc0f XLM-R \uc0ac\uc804 \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \ud3ec\ud568\ub418\uc5c8\ub294\uc9c0 \uc5ec\ubd80\uc640 \ud574\ub2f9 \ub370\uc774\ud130 \ud06c\uae30(GB)\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uacf5\uac1c\uc801\uc73c\ub85c \uc0ac\uc6a9 \uac00\ub2a5\ud55c CC100 \ubc0f \uc704\ud0a4\ud53c\ub514\uc544 \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uae30\ubc18\uc73c\ub85c \ud06c\uae30\ub97c \uc5b4\ub9bc\uc7a1\uc558\uc2b5\ub2c8\ub2e4.", "section": "4 Experimental Setup"}, {"content": "| ISO | ConceptNet |  |  |  |  |  | Glot |  |  |  |  |  | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **mBERT** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **XLM-R** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **mBERT** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **XLM-R** | **Seq_bn** | **LoRA** | **Seq_bn_inv** |\n| th | 1.21 | 1.24 | 1.2 | 1.42 | 1.42 | 1.35 | 0.46 | 0.54 | 0.45 | 1.55 | 1.65 | 1.53 |\n| ro | 1.41 | 1.46 | 1.34 | 1.43 | 1.43 | 1.33 | 1.37 | 1.52 | 1.34 | 1.27 | 1.3 | 1.26 |\n| bg | 0.68 | 0.71 | 0.66 | 0.87 | 0.87 | 0.81 | 1.09 | 1.25 | 1.07 | 1.83 | 1.8 | 1.8 |\n| da | 1.24 | 1.29 | 1.19 | 1.35 | 1.36 | 1.26 | 1.39 | 1.54 | 1.36 | 1.28 | 1.36 | 1.26 |\n| el | 1.13 | 1.18 | 1.12 | 1.36 | 1.36 | 1.29 | 0.67 | 0.77 | 0.66 | 0.84 | 0.9 | 0.83 |\n| he | 1.35 | 1.38 | 1.32 | 1.47 | 1.46 | 1.4 | 1.3 | 1.41 | 1.28 | 1.29 | 1.38 | 1.28 |\n| sk | 1.22 | 1.28 | 1.16 | 1.39 | 1.39 | 1.28 | 1.09 | 1.19 | 1.06 | 1.16 | 1.19 | 1.14 |\n| sl | 0.83 | 0.91 | 0.79 | 1.05 | 1.09 | 0.98 | 1.16 | 1.28 | 1.13 | 1.22 | 1.28 | 1.21 |\n| lv | 1.32 | 1.4 | 1.25 | 1.47 | 1.51 | 1.37 | 1.11 | 1.29 | 1.07 | 1.28 | 1.37 | 1.25 |\n| ms | 1.57 | 1.63 | 1.5 | 1.59 | 1.57 | 1.47 | 1.52 | 1.65 | 1.48 | 1.55 | 1.6 | 1.54 |\n| ka | 1.15 | 1.19 | 1.14 | 1.38 | 1.35 | 1.3 | 0.79 | 0.91 | 0.77 | 1.12 | 1.18 | 1.11 |\n| bn | 0.99 | 1.03 | 0.97 | 1.37 | 1.37 | 1.3 | 1.05 | 1.16 | 1.03 | 1.44 | 1.49 | 1.42 |\n| az | 1.33 | 1.37 | 1.29 | 1.5 | 1.55 | 1.42 | 0.89 | 1.02 | 0.86 | 1.19 | 1.31 | 1.15 |\n| ur | 1.43 | 1.48 | 1.4 | 1.62 | 1.61 | 1.51 | 1.15 | 1.31 | 1.12 | 1.38 | 1.44 | 1.36 |\n| mk | 1.42 | 1.44 | 1.38 | 1.59 | 1.54 | 1.45 | 0.89 | 0.99 | 0.87 | 1.41 | 1.4 | 1.41 |\n| te | 1.09 | 1.12 | 1.07 | 1.29 | 1.29 | 1.22 | 0.83 | 0.94 | 0.81 | 1.33 | 1.4 | 1.31 |\n| ne | 1.26 | 1.31 | 1.21 | 1.53 | 1.52 | 1.42 | 0.77 | 0.9 | 0.75 | 1.38 | 1.45 | 1.35 |\n| mr | 1.08 | 1.12 | 1.04 | 1.46 | 1.45 | 1.37 | 0.94 | 1.07 | 0.92 | 1.43 | 1.49 | 1.41 |\n| sw | 1.54 | 1.63 | 1.51 | 1.64 | 1.73 | 1.56 | 0.94 | 1.13 | 0.9 | 1.13 | 1.22 | 1.1 |\n| cy | 1.55 | 1.6 | 1.48 | 1.83 | 1.91 | 1.76 | 0.81 | 0.99 | 0.77 | 0.95 | 1.06 | 0.92 |\n| uz | 1.22 | 1.3 | 1.18 | 1.55 | 1.62 | 1.45 | 0.85 | 1.01 | 0.82 | 1.06 | 1.17 | 1.03 |\n| jv | 1.44 | 1.5 | 1.4 | 1.55 | 1.56 | 1.48 | 2.11 | 2.21 | 2.08 | 2.63 | 2.66 | 2.54 |\n| su | 1.51 | 1.56 | 1.47 | 1.38 | 1.4 | 1.38 | 1.14 | 1.28 | 1.11 | 1.21 | 1.35 | 1.18 |\n| si | 1.4 | 1.33 | 1.38 | 1.31 | 1.25 | 1.25 | 0.82 | 0.88 | 0.8 | 1.21 | 1.29 | 1.19 |\n| am | 1.47 | 1.51 | 1.58 | 1.22 | 1.29 | 1.13 | 1.25 | 1.31 | 1.23 | 1.2 | 1.31 | 1.19 |\n| ku | 1.64 | 1.73 | 1.61 | 1.91 | 2.04 | 1.86 | 0.93 | 1.05 | 0.9 | 0.76 | 1.02 | 0.71 |\n| ug | 1.09 | 1.13 | 1.07 | 1.57 | 1.59 | 1.47 | 0.46 | 0.57 | 0.44 | 0.79 | 0.94 | 0.76 |\n| mt | 1.41 | 1.44 | 1.39 | 1.53 | 1.68 | 1.5 | 0.84 | 1.08 | 0.8 | 0.93 | 1.2 | 0.87 |\n| bo | 1.0 | 1.01 | 0.98 | 0.63 | 0.64 | 0.62 | 0.24 | 0.28 | 0.24 | 0.72 | 0.73 | 0.71 |\n| yo | 1.12 | 1.27 | 1.1 | 1.77 | 1.79 | 1.76 | 0.87 | 1.04 | 0.84 | 0.83 | 1.03 | 0.78 |", "caption": "Table 6:  Sentiment analysis data details.", "description": "\ud45c 6\uc740 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uac10\uc131 \ubd84\uc11d \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc138\ubd80 \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uac01 \uc5b8\uc5b4\uc5d0 \ub300\ud574 \ub370\uc774\ud130 \ucd9c\ucc98, \uae0d\uc815\uc801/\ubd80\uc815\uc801 \ub9ac\ubdf0 \uc218, \ud559\uc2b5/\uac80\uc99d/\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc758 \ubd84\ud560 \ube44\uc728\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc5b8\uc5b4\uc758 \uac10\uc131 \ubd84\uc11d \uc791\uc5c5\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\uc640 \ubd84\ud3ec\ub97c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "4.3 Task-Specific Training"}, {"content": "| Adapter Type | mBERT |  |  | XLM-R |  |  | LLaMA-3 |  |\n|---|---|---|---|---|---|---|---|---|\n|  | Seq_bn | Seq_bn_inv | LoRA | Seq_bn | Seq_bn_inv | LoRA | Seq_bn | Seq_bn_inv |\n| Trainable Params (No.) | 894,528 | 1,190,592 | 294,912 | 894,528 | 1,190,592 | 294,912 | 67,248,128 | 75,642,880 |\n| Trainable Params (%) | 0.505% | 0.672% | 0.166% | 0.322% | 0.429% | 0.106% | 0.896% | 1.008% |\n| Hyperparameters for LA | Batch Size: 16, Learning Rate: 1e-4,  Seq_bn and Seq_bn_inv: Reduction Factor = 16, LoRA: \u03b1=8, r=8 |  |  | Batch Size: 1, Learning Rate: 1e-4 |  |  |  |  |\n| Hyperparameters for TA | Batch Size: 32, Learning Rate: 1e-4, Seq_bn: Reduction Factor = 16, LoRA: \u03b1=8, r=8 |  |  | Batch Size for TC: 16; for SA and NER: 8, Learning Rate: 2e-5 |  |  |  |  |", "caption": "Table 7: Named entity recognition data details.", "description": "\ud45c 7\uc740 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c 30\uac1c \uc800\uc790\uc6d0 \uc5b8\uc5b4\uc5d0 \ub300\ud55c \uac1c\uccb4\uba85 \uc778\uc2dd(Named Entity Recognition, NER) \ub370\uc774\ud130\uc14b\uc758 \uc138\ubd80 \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc5b8\uc5b4\uc5d0 \ub300\ud574 \ud559\uc2b5, \uac80\uc99d \ubc0f \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc758 \uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubaa8\ub378\uc758 NER \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc758 \ud06c\uae30\uc640 \ubd84\ud3ec\ub97c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "4 Experimental Setup"}, {"content": "| Batch Size | Learning Rate | Seq_bn and Seq_bn_inv Reduction Factor | LoRA (\u03b1, r) |\n|---|---|---|---| \n| 16 | 1e-4 | 16 | 8, 8 |", "caption": "Table 8: Evaluation losses for language adapters by model, architecture, and language.", "description": "\uc774 \ud45c\ub294 \ubaa8\ub378, \uc544\ud0a4\ud14d\ucc98 \ubc0f \uc5b8\uc5b4\ubcc4\ub85c \uc5b8\uc5b4 \uc5b4\ub311\ud130\uc758 \ud3c9\uac00 \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud3c9\uac00 \uc190\uc2e4 \uac12\uc740 MLM \uc131\ub2a5\uc744 \uc608\uce21\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.  \uac00\uc7a5 \ub0ae\uc740 \ud3c9\uac00 \uc190\uc2e4\uc744 \uae30\ub85d\ud55c Seq_bn_inv\ub294 MLM \uc791\uc5c5\uc5d0\uc11c \uc131\ub2a5\uc774 \uc800\uc870\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ud3c9\uac00 \uc190\uc2e4\uc774 \uc2e0\ub8b0\ud560 \uc218 \uc5c6\ub294 \ud6c8\ub828 \uc9c0\ud45c\uac00 \ub420 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3 Methodology"}, {"content": "| Batch Size | Learning Rate |\n|---|---| \n| 1 | 1e-4 |\n", "caption": "Table 9: Trainable parameters and hyperparameters for different adapter types in mBERT, XLM-R, and LLaMA-3. The rest of hyperparameters are as specified in the default adapter configurations in Adapterhub. LA - Langauge adapter, TA - Task adapter.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 3.1\uc808 \ubaa8\ub378 \uc801\uc751(Model Adaptation)\uc5d0\uc11c \uc138 \uac00\uc9c0 \uc5b4\ub311\ud130 \uad6c\uc870(Sequential Bottleneck, Sequential Bottleneck with Invertible Layers, Low-Rank Adaptation)\ub97c \uc0ac\uc6a9\ud558\uc5ec mBERT\uc640 XLM-R, \uadf8\ub9ac\uace0 LLaMA-3\ub97c \uc800\uc790\uc6d0 \uc5b8\uc5b4\uc5d0 \uc801\uc751\uc2dc\ud0a4\ub294 \uacfc\uc815\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub9e4\uac1c\ubcc0\uc218\uc758 \uc218\uc640 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uacfc \uc5b4\ub311\ud130 \uc720\ud615\ubcc4\ub85c \ud559\uc2b5 \uac00\ub2a5\ud55c \ub9e4\uac1c\ubcc0\uc218\uc758 \uc218\uc640 \uc804\uccb4 \ub9e4\uac1c\ubcc0\uc218\uc5d0\uc11c \ucc28\uc9c0\ud558\ub294 \ube44\uc728\uc774 \uc81c\uc2dc\ub429\ub2c8\ub2e4. \ub610\ud55c, \uc5b8\uc5b4 \uc5b4\ub311\ud130(LA)\uc640 \uc791\uc5c5 \uc5b4\ub311\ud130(TA)\uc5d0 \ub300\ud574 \uac01\uac01 \uc0ac\uc6a9\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. Adapterhub\uc758 \uae30\ubcf8 \uc5b4\ub311\ud130 \uc124\uc815\uc744 \ub530\ub974\ub294 \ub098\uba38\uc9c0 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub294 \ud45c\uc5d0 \uba85\uc2dc\uc801\uc73c\ub85c \uc81c\uc2dc\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.", "section": "3 Methodology"}, {"content": "| Batch Size | Learning Rate | Seq_bn Reduction Factor | LoRA \u03b1 | LoRA r |\n|---|---|---|---|---|\n| 32 | 1e-4 | 16 | 8 | 8 |", "caption": "Table 10: Pseudo-perplexity scores comparison across different adapters for mBERT in ConceptNet and Glot. \u2020Language not included in mBERT pre-training. FFT denotes full fine-tuning of a base model on the target-language Glot data. The underlined FFT scores indicate that FFT outperform the best performing adapter for a respective language.", "description": "\ud45c 10\uc740 ConceptNet\uacfc Glot \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec mBERT\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \uc5b4\ub311\ud130\uc758 \ub9c8\uc2a4\ud06c \uc5b8\uc5b4 \ubaa8\ub378\ub9c1 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4.  \u2020\uae30\ud638\ub294 mBERT\uc758 \uc0ac\uc804 \ud6c8\ub828\uc5d0 \ud3ec\ud568\ub418\uc9c0 \uc54a\uc740 \uc5b8\uc5b4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. FFT\ub294 \ub300\uc0c1 \uc5b8\uc5b4\uc758 Glot \ub370\uc774\ud130\ub97c \uae30\ubc18\uc73c\ub85c \ud55c \uae30\ubcf8 \ubaa8\ub378\uc758 \uc644\uc804 \ubbf8\uc138 \uc870\uc815\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ubc11\uc904 \uce5c FFT \uc810\uc218\ub294 \ud574\ub2f9 \uc5b8\uc5b4\uc5d0 \ub300\ud574 \ucd5c\uace0 \uc131\ub2a5\uc758 \uc5b4\ub311\ud130\ubcf4\ub2e4 FFT\uc758 \uc131\ub2a5\uc774 \ub354 \uc6b0\uc218\ud568\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "5.1 \ub9c8\uc2a4\ud06c \uc5b8\uc5b4 \ubaa8\ub378\ub9c1"}, {"content": "| Batch Size for TC | Learning Rate |\n|---|---| \n| 16 for TC; 8 for SA and NER | 2e-5 |", "caption": "Table 11: Pseudo-perplexity scores comparison for XLM-R across different adapters in ConceptNet and Glot. \u2021Language not included in XLM-R pre-training. FFT denotes full fine-tuning of a base model on the target-language Glot data. The underlined FFT scores indicate that FFT outperform the best performing adapter for a respective language.", "description": "\ud45c 11\uc740 ConceptNet\uacfc Glot \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \uc5b4\ub311\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec XLM-R \ubaa8\ub378\uc758 \ub9c8\uc2a4\ud06c \uc5b8\uc5b4 \ubaa8\ub378\ub9c1(MLM) \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4.  \u2021 \ud45c\uc2dc\ub294 XLM-R\uc758 \uc0ac\uc804 \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \ud3ec\ud568\ub418\uc9c0 \uc54a\uc740 \uc5b8\uc5b4\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  FFT\ub294 \uae30\ubcf8 \ubaa8\ub378\uc744 \ub300\uc0c1 \uc5b8\uc5b4\uc758 Glot \ub370\uc774\ud130\ub85c \ubbf8\uc138 \uc870\uc815\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubc11\uc904 \uce5c FFT \uc810\uc218\ub294 \ud574\ub2f9 \uc5b8\uc5b4\uc5d0 \ub300\ud574 \ucd5c\uace0 \uc131\ub2a5\uc758 \uc5b4\ub311\ud130\ubcf4\ub2e4 FFT\uc758 \uc131\ub2a5\uc774 \ub354 \uc6b0\uc218\ud568\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "5.1 \ub9c8\uc2a4\ud06c \uc5b8\uc5b4 \ubaa8\ub378\ub9c1"}, {"content": "| ISO | mBERT | mBERT | mBERT | mBERT | mBERT | mBERT | mBERT | mBERT | mBERT |\n|---|---|---|---|---|---|---|---|---|---|\n|  | **Base** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **FFT** |\n| he | 18.36 | 19.71 | 18.29 | 19.85 | **11.09** | 12.31 | 12.51 | 8.78 |\n| el | 4.69 | 6.17 | 5.55 | 6.92 | **3.3** | 3.54 | 3.49 | 2.71 |\n| bg | 10.84 | 14.99 | 12.65 | 20.93 | **5.4** | 5.9 | 6.09 | 4.67 |\n| th | 3.87 | 4.13 | 4.29 | 4.07 | **2.94** | 3.34 | 3.18 | 2.54 |\n| ro | 11.49 | 13.47 | 12.67 | 22.39 | **5.94** | 6.59 | 8.67 | 6.75 |\n| bn | 11.97 | 14.94 | 13.53 | 15.99 | **9.11** | 10.05 | 10.32 | 8.42 |\n| te | 7.92 | 8.9 | 8.34 | 9.33 | **6.09** | 6.13 | 6.4 | 5.32 |\n| ka | 6.52 | 6.3 | 6.0 | 6.54 | **3.63** | 4.06 | 3.91 | 2.6 |\n| mk | 11.95 | 14.5 | 12.3 | 13.26 | **5.83** | 6.33 | 6.54 | 5.53 |\n| da | 19.16 | 19.29 | 25.39 | 30.87 | **11.13** | 11.8 | 13.02 | 8.76 |\n| sl | 13.57 | 18.09 | 14.32 | 26.86 | **6.68** | 7.26 | 8.58 | 4.91 |\n| az | 12.47 | 15.2 | 13.48 | 24.26 | **7.04** | 7.89 | 7.9 | 5.83 |\n| sk | 11.5 | 13.86 | 12.37 | 19.29 | **5.98** | 6.64 | 7.14 | 6.03 |\n| ms | 36.26 | 53.66 | 50.17 | 128.6 | **18.23** | 20.01 | 22.71 | 16.95 |\n| uz | 26.65 | 31.41 | 23.43 | 40.35 | **5.84** | 7.21 | 9.22 | 3.84 |\n| ur | 22.59 | 23.02 | 21.74 | 26.4 | **10.18** | 12.0 | 12.89 | 7.16 |\n| cy | 21.24 | 22.13 | 23.0 | 39.75 | **6.08** | 7.8 | 9.06 | 4.89 |\n| lv | 14.14 | 18.31 | 16.21 | 33.14 | **5.98** | 7.13 | 7.48 | 4.58 |\n| mr | 12.51 | 12.9 | 12.21 | 14.0 | **5.84** | 6.78 | 6.85 | 6.71 |\n| ne | 12.72 | 14.19 | 13.08 | 15.36 | **6.71** | 7.21 | 8.68 | 4.88 |\n| jv | 83.84 | 115.27 | 132.08 | 146.64 | **19.4** | 22.86 | 31.6 | 19.19 |\n| sw | 42.53 | 57.57 | 52.21 | 79.5 | **8.99** | 12.48 | 16.09 | 7.19 |\n| su | 102.16 | 177.27 | 183.04 | 227.87 | **20.24** | 23.2 | 34.29 | 34.93 |\n| yo | 85.21 | 293.99 | 210.43 | 370.71 | **23.14** | 31.96 | 86.79 | 38.89 |\n| Avg. | 25.17 | 41.22 | 37.37 | 55.95 | **8.95** | 10.44 | 14.31 | 9.25 |\n| mt\u2020 | 531.59 | 432.99 | 456.64 | 457.43 | **6.89** | 9.87 | 15.02 | 5.95 |\n| ku\u2020 | **72.87** | 119.29 | 101.13 | 149.74 | 1524.98 | 559.83 | 173.24 | 6381.75 |\n| ug\u2020 | 112.63 | 96.52 | 86.31 | 121.15 | **28.69** | 67.26 | 75.53 | 313.64 |\n| si\u2020 | **16.29** | 96.5 | 40.3 | 103.36 | 15640.68 | 8981.09 | 157397.73 | 443921.11 |\n| am\u2020 | **10.06** | 31.41 | 26.93 | 23.47 | 56052.75 | 34924.59 | 4223.4 | 38289.93 |\n| bo\u2020 | **4.59** | 58.78 | 47.33 | 89.81 | 57.94 | 65.03 | 1136.47 | 41.99 |\n| Avg. | **124.67** | 139.25 | 126.44 | 157.49 | 12218.65 | 7434.61 | 27170.23 | 81492.4 |\n| Total | **45.07** | 60.83 | 55.18 | 76.26 | 2450.89 | 1495.27 | 5445.49 | 16305.88 |", "caption": "Table 12: Pearson and Spearman Correlations for mBERT and XLM-R between pseudo-perplexity and amounts of pre-training and post-training data for the pre-adaptation and post-adaptation results. Post-adaptation results are based on the models with Seq_bn language adapters and denote the correlation between the sum of the pre-training and adaptation data sizes and pseudo-perplexity scores after the adaptation.", "description": "\ubcf8 \ud45c\ub294 \uc0ac\uc804 \uc801\uc751 \uc804\uacfc \ud6c4\uc758 mBERT \ubc0f XLM-R \ubaa8\ub378\uc5d0 \ub300\ud574 \uc758\uc0ac-\ud37c\ud50c\ub809\uc11c\ud2f0\uc640 \uc0ac\uc804 \ud559\uc2b5 \ubc0f \ucd94\uac00 \ud559\uc2b5 \ub370\uc774\ud130 \uc591 \uac04\uc758 \ud53c\uc5b4\uc2a8 \ubc0f \uc2a4\ud53c\uc5b4\ub9cc \uc0c1\uad00 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ucd94\uac00 \ud559\uc2b5 \uacb0\uacfc\ub294 Seq_bn \uc5b8\uc5b4 \uc5b4\ub311\ud130\ub97c \uc0ac\uc6a9\ud55c \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c \ud558\uba70, \uc0ac\uc804 \ud559\uc2b5\uacfc \ucd94\uac00 \ud559\uc2b5 \ub370\uc774\ud130 \ud06c\uae30\uc758 \ud569\uacc4\uc640 \uc801\uc751 \ud6c4 \uc758\uc0ac-\ud37c\ud50c\ub809\uc11c\ud2f0 \uc810\uc218 \uac04\uc758 \uc0c1\uad00 \uad00\uacc4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "7.4 \uc0ac\uc804 \ud559\uc2b5 \ubc0f \ucd94\uac00 \ud559\uc2b5 \ub370\uc774\ud130 \ud06c\uae30\uc758 \uc601\ud5a5"}, {"content": "| ISO | XLM-R |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n| **ISO** | **XLM-R** |  |  |  |  |  |  |  |\n|  |  | **ConceptNet** |  |  | **Glot** |  |  |  |\n|  | **Base** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **FFT** |\n| th | 7.83 | 8.67 | 8.86 | 10.11 | 8.78 | 7.97 | 9.39 | 22.16 |\n| ro | 2.97 | 3.76 | 3.79 | 4.51 | 3.42 | **2.96** | 3.25 | 6.18 |\n| bg | **3.61** | 4.88 | 5.51 | 5.4 | 3.63 | 3.7 | 3.64 | 6.12 |\n| da | 4.29 | 5.56 | 5.94 | 6.21 | 6.69 | **4.21** | 4.58 | 7.9 |\n| el | **2.56** | 3.17 | 3.1 | 3.46 | 2.97 | 2.63 | 2.87 | 3.81 |\n| he | **5.74** | 6.17 | 6.36 | 6.74 | 5.8 | 5.84 | 5.99 | 10.95 |\n| sk | 3.93 | 4.85 | 4.67 | 5.36 | 4.56 | **3.68** | 4.08 | 4.62 |\n| sl | 4.79 | 7.31 | 7.41 | 8.68 | 4.35 | **4.01** | 4.95 | 5.3 |\n| lv | 4.14 | 5.96 | 6.32 | 9.34 | 5.09 | **3.92** | 4.7 | **4.87** |\n| ms | 10.79 | 15.02 | 15.82 | 17.26 | 8.97 | **8.8** | 9.65 | 12.55 |\n| ka | **3.88** | 4.41 | 4.47 | 4.48 | 3.99 | 3.94 | 4.76 | 4.97 |\n| bn | 6.5 | 7.22 | 7.17 | 7.6 | **5.95** | 6.28 | 8.0 | 6.69 |\n| az | 7.52 | 11.21 | 11.45 | 15.95 | 8.27 | **7.58** | 9.7 | 14.11 |\n| ur | 10.17 | 12.13 | 12.82 | 12.23 | **9.53** | 9.54 | 11.12 | 12.32 |\n| mk | 5.19 | 6.74 | 7.51 | 7.28 | 4.82 | **4.78** | **4.78** | 8.14 |\n| te | 6.76 | 8.12 | 8.11 | 8.31 | **6.41** | 6.66 | 9.92 | 7.6 |\n| ne | 12.76 | 16.87 | 17.74 | 16.91 | 11.86 | **11.82** | 22.42 | 16.64 |\n| si | 7.04 | 7.97 | 8.22 | 8.26 | **5.74** | 6.37 | 11.44 | 6.74 |\n| mr | 10.25 | 11.83 | 12.12 | 12.67 | 9.11 | **8.9** | 16.42 | 21.99 |\n| sw | 15.68 | 26.99 | 27.39 | 36.78 | **7.76** | 9.61 | 11.24 | 9.18 |\n| cy | 9.37 | 13.94 | 16.05 | 17.51 | **5.08** | 5.88 | 8.11 | **4.7** |\n| am | 10.87 | 14.77 | 15.4 | 15.15 | **7.32** | 8.44 | 17.0 | 10.49 |\n| uz | 8.4 | 14.77 | 16.81 | 20.66 | **5.46** | 6.21 | 9.14 | 5.92 |\n| ku | 159.39 | 72.75 | 84.04 | 69.25 | **2.95** | 4.34 | 19.27 | 3.88 |\n| ug<sup>\u2020</sup> | 6.87 | 13.97 | 12.48 | 16.76 | **4.99** | 5.97 | 12.48 | 16.13 |\n| jv | 33.81 | 96.45 | 89.36 | 116.95 | **12.49** | 15.06 | 27.14 | 26.25 |\n| su | 57.32 | 134.71 | 128.95 | 152.14 | **10.41** | 15.22 | 29.1 | 25.16 |\n| Avg. | 15.65 | 20.01 | 20.29 | 22.81 | **6.53** | 6.83 | 10.56 | 10.57 |\n| mt<sup>\u2021</sup> | 395.18 | 283.77 | 335.23 | 275.56 | **3.19** | 5.0 | 12.01 | 3.36 |\n| bo<sup>\u2021</sup> | **9.45** | 937.1 | 2036.45 | 1209.39 | 353.49 | 274.66 | 1972.96 | 597.55 |\n| yo<sup>\u2021</sup> | 207.26 | 225.8 | 335.24 | 223.49 | **9.57** | 14.31 | 155.99 | 19.12 |\n| Avg. | 203.96 | 482.22 | 902.31 | 569.48 | 122.08 | **97.99** | 713.65 | 206.68 |\n| Total | 34.48 | 66.23 | 108.49 | 77.48 | 18.09 | **15.94** | 80.87 | 30.18 |", "caption": "Table 13: Average pseudo-perplexity scores for 30 languages across three model configurations. For the adapted XLM-R-base, we pick the adapter with the best performance.", "description": "\ud45c 13\uc740 \uc138 \uac00\uc9c0 \ubaa8\ub378 \uad6c\uc131(XLM-R \uae30\ubcf8 \ubaa8\ub378, Glot \ub370\uc774\ud130\ub85c \uc801\uc751\ub41c XLM-R \uae30\ubcf8 \ubaa8\ub378, XLM-R \ub300\ud615 \ubaa8\ub378)\uc5d0 \ub300\ud574 30\uac1c \uc5b8\uc5b4\uc758 \ud3c9\uade0 \uc758\uc0ac \ud37c\ud50c\ub809\uc11c\ud2f0 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc801\uc751\ub41c XLM-R \uae30\ubcf8 \ubaa8\ub378\uc758 \uacbd\uc6b0 \uc131\ub2a5\uc774 \uac00\uc7a5 \uc88b\uc740 \uc5b4\ub311\ud130\uc758 \uacb0\uacfc\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\uacfc \uc5b4\ub311\ud130\ub97c \uc0ac\uc6a9\ud55c \uc800\uc790\uc6d0 \uc5b8\uc5b4\uc5d0 \ub300\ud55c \uc5b8\uc5b4 \ubaa8\ub378 \uc801\uc751\uc758 \ud6a8\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "5 \uacb0\uacfc: \uc18c\uaddc\ubaa8 \ub2e4\uad6d\uc5b4 \uc5b8\uc5b4 \ubaa8\ub378"}, {"content": "| Model | Pearson (p-value) | Spearman (p-value) |\n|---|---|---|\n| **Pre-adapt** |  |  |\n| mBERT | -0.37 (0.07) | -0.51 (0.01) |\n| XLM-R | -0.32 (0.1) | -0.39 (0.04) |\n| **Post-adapt** |  |  |\n| mBERT | -0.69 (\u00a10.001) | -0.79 (\u00a10.001) |\n| XLM-R | -0.27 (0.16) | -0.79 (\u00a10.001) |", "caption": "Table 14: Pearson and Spearman Correlations for mBERT and XLM-R (Pre-Adapt and Post-Adapt) between pseudo-perplexity and task performance. Post-Adapt is represented by the models adapted with the Seq_bn language adapters.", "description": "\ubcf8 \ud45c\ub294 mBERT\uc640 XLM-R \ubaa8\ub378\uc5d0 \ub300\ud574 \uc0ac\uc804 \uc801\uc751(Pre-Adapt) \ubc0f \uc0ac\ud6c4 \uc801\uc751(Post-Adapt) \ub2e8\uacc4\uc5d0\uc11c\uc758 \uc758\uc0ac-\ud37c\ud50c\ub809\uc11c\ud2f0(pseudo-perplexity)\uc640 \uacfc\uc81c \uc131\ub2a5 \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ud53c\uc5b4\uc2a8(Pearson)\uacfc \uc2a4\ud53c\uc5b4\ub9cc(Spearman) \uc0c1\uad00 \uacc4\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc0ac\ud6c4 \uc801\uc751 \uacb0\uacfc\ub294 Seq_bn \uc5b8\uc5b4 \uc5b4\ub311\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc801\uc751\ub41c \ubaa8\ub378\uc744 \uae30\uc900\uc73c\ub85c \ud569\ub2c8\ub2e4.  \uc989, \ubaa8\ub378\uc758 \uc5b8\uc5b4 \ubaa8\ub378\ub9c1 \uc131\ub2a5(\uc758\uc0ac-\ud37c\ud50c\ub809\uc11c\ud2f0\ub85c \uce21\uc815)\uc774 \ud558\uc704 \uacfc\uc81c(\uc608: \uc8fc\uc81c \ubd84\ub958, \uac10\uc815 \ubd84\uc11d, \uac1c\uccb4\uba85 \uc778\uc2dd)\uc758 \uc131\ub2a5\uacfc \uc5bc\ub9c8\ub098 \uc5f0\uad00\ub418\uc5b4 \uc788\ub294\uc9c0 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc0ac\uc804 \uc801\uc751\uacfc \uc0ac\ud6c4 \uc801\uc751 \ubaa8\ub450\uc5d0 \ub300\ud55c \uc0c1\uad00\uad00\uacc4\uac00 \uc81c\uc2dc\ub418\uc5b4 \ubaa8\ub378 \uc801\uc751 \uc804\ud6c4\uc758 \uad00\uacc4\ub97c \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.", "section": "7.3 Language Modeling\uacfc \ud558\uc704 \uacfc\uc81c \uc131\ub2a5 \uac04\uc758 \uc0c1\uad00\uad00\uacc4"}, {"content": "| ISO | XLM-R-base | Adapted XLM-R-base | XLM-R-large | Glot-500m |\n|---|---|---|---|---|\n| th | 7.83 | 7.97 | **4.92** | 31.34 |\n| ro | 2.97 | 2.96 | **2.06** | 13.29 |\n| bg | 3.61 | 3.63 | **2.53** | 14.16 |\n| da | 4.29 | 4.21 | **2.78** | 28.06 |\n| el | 2.56 | 2.97 | **1.87** | 6.87 |\n| he | 5.74 | 5.8 | **3.19** | 32.80 |\n| sk | 3.93 | 3.68 | **2.30** | 26.36 |\n| sl | 4.79 | 4.01 | **2.60** | 41.98 |\n| lv | 4.14 | 3.92 | **2.51** | 14.55 |\n| ms | 10.79 | 8.8 | **6.71** | 38.46 |\n| ka | 3.88 | 3.94 | **2.69** | 10.77 |\n| bn | 6.50 | 5.95 | **3.99** | 19.36 |\n| az | 7.52 | 7.58 | **4.40** | 17.46 |\n| ur | 10.17 | 9.53 | **6.10** | 25.60 |\n| mk | 5.19 | 4.78 | **3.23** | 14.00 |\n| te | 6.76 | 6.41 | **4.31** | 17.19 |\n| ne | 12.76 | 11.82 | **8.06** | 23.19 |\n| mr | 10.25 | 8.9 | **5.77** | 27.95 |\n| sw | 15.68 | **7.76** | 8.90 | 44.82 |\n| cy | 9.37 | 5.08 | **4.35** | 25.74 |\n| uz | 8.40 | 5.46 | **3.92** | 15.33 |\n| jv | 33.81 | **12.49** | 17.83 | 73.46 |\n| su | 57.32 | **10.41** | 26.42 | 52.65 |\n| si | 7.04 | 5.74 | **4.50** | 15.03 |\n| am | 10.87 | 7.32 | **6.73** | 25.56 |\n| ku | 159.39 | **2.95** | 126.40 | 23.35 |\n| ug | 6.87 | 4.99 | **3.80** | 13.67 |\n| Avg. | 15.65 | **6.26** | 10.11 | 25.66 |\n| mt | 395.18 | **3.19** | 317.81 | 7.93 |\n| bo | 9.45 | 274.66 | **3.99** | 26.74 |\n| yo | 207.26 | **9.57** | 155.57 | 96.80 |\n| Avg. | 203.96 | 95.81 | 159.12 | **43.82** |\n| Total | 34.48 | **15.22** | 25.01 | 27.48 |", "caption": "Table 15: F1 scores comparison across different adapters for mBERT in ConceptNet and Glot for topic classification. All results are averaged over 3 independent runs with different random seeds.", "description": "\ud45c 15\ub294 mBERT \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec ConceptNet\uacfc Glot \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc138 \uac00\uc9c0 \ub2e4\ub978 \uc5b4\ub311\ud130(Seq-bn, LORA, Seq-bn-inv)\ub97c \uc801\uc6a9\ud55c \uc8fc\uc81c \ubd84\ub958 \uc791\uc5c5\uc758 F1 \uc810\uc218\ub97c \ube44\uad50\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. \uac01 \uacb0\uacfc\ub294 \uc11c\ub85c \ub2e4\ub978 \ub79c\ub364 \uc2dc\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec 3\ud68c \ub3c5\ub9bd\uc801\uc73c\ub85c \uc2e4\ud589\ud55c \uacb0\uacfc\uc758 \ud3c9\uade0\uac12\uc785\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc5b4\ub311\ud130 \uc544\ud0a4\ud14d\ucc98\uc640 \ub370\uc774\ud130\uc14b \uc870\ud569\uc774 \uc8fc\uc81c \ubd84\ub958 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "5.2.1 \uc8fc\uc81c \ubd84\ub958"}, {"content": "| Model | Task | Pre-Adapt Pearson (p-value) | Pre-Adapt Spearman (p-value) | Post-Adapt Pearson (p-value) | Post-Adapt Spearman (p-value) |\n|---|---|---|---|---|---| \n| mBERT | TC | -0.09 (0.62) | -0.25 (0.18) | -0.66 (\u00a10.001) | -0.42 (0.02) |\n|  | SA | -0.29 (0.12) | -0.15 (0.42) | -0.45 (0.01) | -0.23 (0.23) |\n|  | NER | -0.28 (0.13) | -0.22 (0.24) | -0.54 (0.002) | -0.49 (0.006) |\n| XLM-R | TC | -0.48 (0.007) | -0.68 (\u00a10.001) | -0.88 (\u00a10.001) | -0.20 (0.3) |\n|  | SA | -0.47 (0.009) | -0.55 (0.002) | -0.64 (\u00a10.001) | -0.38 (0.04) |\n|  | NER | -0.42 (0.02) | -0.62 (\u00a10.001) | -0.35 (0.06) | -0.28 (0.13) |", "caption": "Table 16: F1 scores comparison across different adapters for XLM-R in ConceptNet and Glot for topic classification. All results are averaged over 3 independent runs with different random seeds.", "description": "\ud45c 16\uc740 XLM-R \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec ConceptNet\uacfc Glot \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc8fc\uc81c \ubd84\ub958 \uc791\uc5c5\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \uc5b4\ub311\ud130\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc138 \uac00\uc9c0 \uc5b4\ub311\ud130 \uad6c\uc870 (Seq_bn, LORA, Seq_bn_inv)\uc640 \ub450 \uac00\uc9c0 \ub370\uc774\ud130\uc14b(ConceptNet, Glot)\uc758 \uc870\ud569\uc5d0 \ub530\ub978 F1 \uc810\uc218\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uc2e4\ud5d8\uc740 \uc11c\ub85c \ub2e4\ub978 \ub09c\uc218 \uc2dc\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc138 \ubc88 \ub3c5\ub9bd\uc801\uc73c\ub85c \uc218\ud589\ub418\uc5c8\uc73c\uba70, \ud45c\uc758 \uacb0\uacfc\ub294 \uadf8 \ud3c9\uade0\uac12\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ubcf8 \ud45c\ub294 \ub2e4\uc591\ud55c \uc5b4\ub311\ud130\uc640 \ub370\uc774\ud130\uc14b \uc870\ud569\uc744 \ud1b5\ud574 XLM-R \ubaa8\ub378\uc758 \uc8fc\uc81c \ubd84\ub958 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "5.2.1 \uc8fc\uc81c \ubd84\ub958"}, {"content": "| ISO | mBERT |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n| **ISO** | **mBERT** |  |  |  |  |  |  |  |\n|  |  | **ConceptNet** |  |  | **Glot** |  |  |  |\n|  | **Base** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **FFT** |\n| he | 79.79 | **83.99** | 82.87 | 82.11 | 83.26 | 83.43 | 83.91 | 83.24 |\n| el | **79.47** | 77.95 | 79.14 | 78.12 | 76.65 | 77.92 | 76.64 | **84.81** |\n| bg | **84.39** | 83.71 | 84.17 | 83.38 | 82.64 | 82.87 | 82.58 | **85.88** |\n| th | 74.18 | **74.66** | 73.9 | 74.42 | 71.34 | 74.47 | 72.47 | **76.44** |\n| ro | 86.95 | 87.86 | 86.45 | **88.37** | 85.8 | 86.63 | 86.8 | **89.06** |\n| bn | 76.18 | 77.65 | 74.52 | 76.69 | 77.51 | **78.09** | 77.34 | 77.3 |\n| te | 80.03 | **82.35** | 80.04 | 81.13 | 77.32 | 81.2 | 78.95 | 79.33 |\n| ka | 76.28 | 73.26 | 74.26 | 74.07 | 75.68 | **78.23** | 75.19 | **79.82** |\n| mk | 83.44 | 84.48 | 84.34 | 83.79 | 84.53 | 84.92 | **85.25** | 84.96 |\n| da | 87.06 | 86.85 | 86.63 | **87.72** | 86.03 | 86.48 | 85.5 | 85.8 |\n| sl | 83.6 | 85.07 | 83.75 | 86.22 | 86.71 | 85.39 | **86.73** | 86.43 |\n| az | 81.09 | 83.72 | 82.53 | 83.38 | 82.93 | 82.55 | **84.29** | 82.01 |\n| sk | 84.37 | 83.49 | 83.98 | **85.4** | 84.79 | 84.43 | 83.57 | 84.52 |\n| ms | 84.31 | 84.65 | 84.1 | 82.94 | **85.4** | 84.59 | 83.39 | 84.38 |\n| uz | 76.57 | 73.89 | 73.71 | 75.76 | **81.32** | 74.44 | 79.35 | **85.35** |\n| ur | 76.7 | 73.7 | 74.85 | 74.76 | 76.06 | 75.26 | **76.94** | **78.18** |\n| cy | 72.37 | 72.23 | 71.6 | 73.49 | **81.47** | 77.16 | 80.75 | **85.53** |\n| lv | 82.28 | **83.63** | 82.42 | 82.45 | 83.48 | 82.56 | 80.94 | **85.02** |\n| mr | 73.21 | **77.29** | 76.22 | 76.61 | 76.37 | 75.73 | 75.28 | **78.84** |\n| ne | 73.72 | 77.55 | 74.62 | 76.02 | **81.59** | 75.21 | 80.8 | 79.11 |\n| jv | 72.4 | 73.32 | **75.12** | 73.11 | 73.71 | 74.09 | 74.02 | **75.89** |\n| sw | 69.17 | 70.53 | 69.89 | 70.21 | 73.93 | 69.05 | **77.15** | **85.89** |\n| su | 76.15 | 77.42 | 77.62 | 77.0 | 78.21 | **79.2** | 78.63 | **79.97** |\n| yo | 54.18 | 52.11 | 52.08 | 54.89 | 55.93 | 55.93 | **58.05** | **63.66** |\n| Avg. | 77.67 | 78.39 | 77.87 | 78.42 | 79.28 | 78.74 | **79.35** | **81.73** |\n| mt<sup class=\"ltx_sup\"><i>\u2020</i></sup> | 69.86 | 69.83 | 69.85 | 68.79 | 78.0 | 78.09 | **79.8** | **83.32** |\n| ku<sup class=\"ltx_sup\"><i>\u2020</i></sup> | 28.76 | 23.78 | 15.71 | 19.93 | 46.41 | 40.22 | **46.85** | **52.82** |\n| ug<sup class=\"ltx_sup\"><i>\u2020</i></sup> | 23.4 | 22.21 | 20.9 | 22.17 | 47.18 | 31.68 | **48.91** | **56.26** |\n| si<sup class=\"ltx_sup\"><i>\u2020</i></sup> | 17.45 | 14.3 | 14.88 | 14.95 | **21.53** | 21.25 | 20.4 | 19.08 |\n| am<sup class=\"ltx_sup\"><i>\u2020</i></sup> | 17.75 | 14.01 | 18.47 | 12.94 | 18.74 | **20.3** | 18.07 | 16.88 |\n| bo<sup class=\"ltx_sup\"><i>\u2020</i></sup> | 12.59 | 11.08 | 9.48 | 6.33 | 36.67 | 28.36 | **39.17** | 33.53 |\n| Avg. | 28.72 | 25.87 | 24.88 | 24.18 | 41.42 | 36.65 | **42.2** | **43.65** |\n| Total avg. | 67.88 | 67.89 | 67.27 | 67.57 | 71.71 | 70.32 | **71.92** | **74.11** |", "caption": "Table 17: Pearson and Spearman Correlations for mBERT and XLM-R (Pre-Adapt and Post-Adapt) between task performance and data amounts. Post-Adapt is represented by the models adapted with the Seq_bn_inv language adapters and denote the correlation between the sum of the pre-training and adaptation data sizes and downstream task performance scores after the adaptation.", "description": "\ubcf8 \ud45c\ub294 mBERT \ubc0f XLM-R \ubaa8\ub378\uc5d0 \ub300\ud574 \uc791\uc5c5 \uc131\ub2a5\uacfc \ub370\uc774\ud130 \uc591 \uac04\uc758 \ud53c\uc5b4\uc2a8 \ubc0f \uc2a4\ud53c\uc5b4\ub9cc \uc0c1\uad00 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  '\uc0ac\uc804 \uc801\uc751' \uc5f4\uc740 \uc5b4\ub311\ud130\ub97c \uc801\uc6a9\ud558\uae30 \uc804 \ubaa8\ub378\uc758 \uc0ac\uc804 \ud6c8\ub828 \ub370\uc774\ud130 \ud06c\uae30\uc640 \uc791\uc5c5 \uc131\ub2a5 \uac04\uc758 \uc0c1\uad00 \uad00\uacc4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. '\uc0ac\ud6c4 \uc801\uc751' \uc5f4\uc740 Seq_bn_inv \uc5b8\uc5b4 \uc5b4\ub311\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc801\uc751\ub41c \ubaa8\ub378\uc758 \uc0ac\uc804 \ud6c8\ub828 \ubc0f \uc801\uc751 \ub370\uc774\ud130 \ud06c\uae30\uc758 \ud569\uacfc \uc791\uc5c5 \uc131\ub2a5 \uac04\uc758 \uc0c1\uad00 \uad00\uacc4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ubcf8 \ubd84\uc11d\uc740 \uc0ac\uc804 \ud6c8\ub828 \ubc0f \uc801\uc751 \ub370\uc774\ud130 \ud06c\uae30\uac00 \uc791\uc5c5 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "7.4 \uc0ac\uc804 \ubc0f \uc0ac\ud6c4 \ud6c8\ub828 \ub370\uc774\ud130 \ud06c\uae30\uc758 \uc601\ud5a5"}, {"content": "| ISO | XLM-R |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n| | **XLM-R** |  |  |  |  |  |  |  |\n| **ISO** |  | **ConceptNet** |  |  | **Glot** |  |  |  |\n|  | **Base** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **FFT** |\n| th | 87.93 | 87.19 | 87.22 | 85.99 | 86.97 | 86.8 | **88.5** | 84.21 |\n| ro | 86.94 | 87.0 | 86.85 | **88.02** | 87.47 | 86.95 | 87.6 | **88.03** |\n| bg | 86.55 | 86.0 | **87.81** | 86.41 | 86.46 | 86.33 | 86.19 | 87.53 |\n| da | 86.04 | 84.94 | 83.7 | 84.26 | 86.47 | 84.88 | **86.41** | **87.06** |\n| el | **86.74** | 85.59 | 85.64 | 84.32 | 85.77 | 85.28 | 86.6 | **88.1** |\n| he | 85.02 | 84.9 | 83.8 | 84.79 | **86.62** | 84.19 | 83.36 | 84.67 |\n| sk | **87.18** | 85.53 | 84.81 | 85.2 | 85.46 | 86.52 | 86.03 | 85.59 |\n| sl | 85.47 | 86.24 | **86.95** | 86.28 | 84.94 | 86.67 | 85.28 | **88.12** |\n| lv | 86.25 | 87.83 | 86.93 | **88.97** | 85.22 | 86.38 | 87.41 | 87.52 |\n| ms | **88.12** | 87.11 | 85.82 | 85.81 | 87.94 | 85.21 | 87.94 | **89.49** |\n| ka | 84.08 | **85.37** | 83.79 | 83.18 | 83.92 | 85.0 | 83.95 | 82.27 |\n| bn | 80.29 | 81.11 | 80.85 | 82.09 | **83.56** | 82.59 | 83.38 | **84.95** |\n| az | 84.05 | 85.86 | 84.24 | 85.07 | 84.43 | 85.16 | **86.39** | 86.08 |\n| ur | **83.25** | 81.04 | 80.29 | 82.35 | 82.97 | 81.98 | 82.17 | **83.97** |\n| mk | 86.45 | 86.41 | 86.99 | 85.45 | 86.94 | 85.97 | **87.15** | **88.15** |\n| te | 83.58 | **83.64** | 84.26 | 83.13 | 82.43 | 84.13 | 83.43 | **85.65** |\n| ne | 84.14 | 83.98 | 83.92 | 83.77 | 82.65 | **84.71** | 82.85 | 84.2 |\n| si | 84.92 | 84.54 | 84.86 | 82.23 | 84.49 | 83.37 | **84.99** | 84.53 |\n| mr | 81.03 | 82.84 | 81.34 | 80.08 | 82.2 | 79.54 | **84.23** | 84.21 |\n| sw | 77.83 | 75.58 | 76.23 | 77.97 | 80.23 | 78.73 | **81.57** | **85.95** |\n| cy | 79.54 | 78.44 | 80.1 | 78.99 | 78.83 | 79.15 | **81.37** | **85.17** |\n| am | 77.5 | 78.4 | 77.93 | 77.91 | 80.67 | 77.52 | **81.51** | **84.22** |\n| uz | 81.93 | 78.73 | 78.43 | 76.97 | **83.35** | 81.13 | 80.68 | **86.37** |\n| ku | 13.49 | 14.09 | 15.76 | 17.28 | 68.57 | 46.29 | **73.97** | **81.72** |\n| ug | 79.56 | 79.11 | 78.67 | 78.86 | 81.29 | **82.23** | 80.14 | **84.95** |\n| jv | 81.35 | 79.32 | 82.23 | 81.43 | **83.59** | 81.84 | 81.74 | 81.2 |\n| Avg. | 81.14 | 80.82 | 80.71 | 80.64 | 83.63 | 82.31 | **84.06** | **85.61** |\n| mt<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">\u2021</span></sup> | 64.56 | 63.62 | 61.43 | 64.43 | 77.39 | 69.74 | **77.92** | **84.35** |\n| bo<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">\u2021</span></sup> | 10.69 | 9.89 | 9.73 | 11.74 | 17.65 | **17.85** | 16.93 | **20.41** |\n| yo<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">\u2021</span></sup> | 28.29 | 26.06 | 16.07 | 24.6 | 54.13 | 35.24 | **59.44** | **67.13** |\n| Avg. | 34.52 | 33.19 | 29.08 | 33.59 | 49.72 | 40.94 | **51.43** | **57.3** |\n| Total avg. | 76.48 | 76.05 | 75.54 | 75.93 | 80.24 | 78.17 | **80.79** | **82.77** |", "caption": "Table 18: F1 scores comparison for mBERT in ConceptNet and Glot for named entity recognition. All results are averaged over 3 independent runs with different random seeds.", "description": "\ud45c 18\uc740 mBERT \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec ConceptNet\uacfc Glot \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uac1c\uccb4\uba85 \uc778\uc2dd \uc791\uc5c5\uc5d0 \ub300\ud55c F1 \uc810\uc218 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uacb0\uacfc\ub294 \uc11c\ub85c \ub2e4\ub978 \ub09c\uc218 \uc2dc\ub4dc\ub97c \uc0ac\uc6a9\ud55c 3\ubc88\uc758 \ub3c5\ub9bd\uc801\uc778 \uc2e4\ud589\uc5d0 \ub300\ud55c \ud3c9\uade0\uac12\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc5b4\ub311\ud130 \uc544\ud0a4\ud14d\ucc98(Seq_bn, LORA, Seq_bn_inv)\uc640 \ub450 \uac00\uc9c0 \ub370\uc774\ud130 \uc18c\uc2a4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \uc81c\uc2dc\ud558\uba70, \uc5b4\ub311\ud130 \uae30\ubc18 \uc811\uadfc \ubc29\uc2dd\uc758 \ud6a8\uacfc\ub97c \ud3c9\uac00\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.  \ud2b9\ud788, \uac01 \uc5b4\ub311\ud130 \uc544\ud0a4\ud14d\ucc98\uac00 ConceptNet\uacfc Glot \ub370\uc774\ud130\uc5d0\uc11c \uac1c\uccb4\uba85 \uc778\uc2dd \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uacfc, \ub370\uc774\ud130\uc14b \uc720\ud615\uc5d0 \ub530\ub978 \uc131\ub2a5 \ucc28\uc774\ub97c \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "5.2 \uc9c0\uba85 \uc778\uc2dd"}, {"content": "| Model | Task | Pre-Adapt | Pre-Adapt | Post-Adapt (Glot) | Post-Adapt (Glot) | Post-Adapt (CN) | Post-Adapt (CN) |\n|---|---|---|---|---|---|---|---|\n|  |  | P (p-value) | S (p-value) | P (p-value) | S (p-value) | P (p-value) | S (p-value) |\n| mBERT | TC | 0.35 (_0.1_) | 0.53 (_0.008_) | 0.45 (_0.03_) | 0.32 (_0.13_) | 0.38 (_0.06_) | 0.55 (_0.006_) |\n| XLM-R | TC | 0.28 (_0.16_) | 0.82 (_\u00a10.005_) | 0.55 (_0.002_) | 0.75 (_\u00a10.005_) | 0.28 (_0.15_) | 0.83 (_\u00a10.005_) |", "caption": "Table 19: F1 scores for XLM-R across ConceptNet and Glot for named entity recognition. All results are averaged over 3 independent runs with different random seeds.", "description": "\ud45c 19\ub294 XLM-R \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec ConceptNet \ubc0f Glot \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uac1c\uccb4\uba85 \uc778\uc2dd \uc791\uc5c5\uc744 \uc218\ud589\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc138 \uac00\uc9c0 \ub2e4\ub978 \uc5b4\ub311\ud130(Seq_bn, LORA, Seq_bn_inv) \uc544\ud0a4\ud14d\ucc98\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac01 \uc5b8\uc5b4\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4.  \uac01 \uacb0\uacfc\ub294 \uc11c\ub85c \ub2e4\ub978 \ub79c\ub364 \uc2dc\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub3c5\ub9bd\uc801\uc73c\ub85c 3\ud68c \uc2e4\ud589\ud55c \ud3c9\uade0\uac12\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. ConceptNet\uacfc Glot \ub370\uc774\ud130\uc14b \ubaa8\ub450\uc5d0\uc11c \uc5b4\ub311\ud130 \uae30\ubc18 \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uc5b4\ub5a4 \ub370\uc774\ud130\uc14b\uacfc \uc5b4\ub311\ud130 \uc870\ud569\uc774 \uac1c\uccb4\uba85 \uc778\uc2dd \uc791\uc5c5\uc5d0 \uac00\uc7a5 \ud6a8\uacfc\uc801\uc778\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.2 Named Entity Recognition"}, {"content": "| ISO | mBERT |  |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---| \n| **ISO** | **mBERT** |  |  |  |  |  |  |  |  |\n|  |  | **ConceptNet** |  |  | **Glot** |  |  | **Fusion** |  |\n|  | **Base** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **Seq_bn** | **Seq_bn_inv** |\n| he | 84.46 | 84.1 | 84.24 | 84.59 | 83.57 | 84.22 | 83.89 | **84.84** | 84.53 |\n| el | 90.16 | 90.11 | 90.45 | 90.27 | 89.9 | **90.5** | 89.35 | 90.3 | 90.0 |\n| bg | 91.25 | 91.64 | 91.64 | 91.48 | 91.64 | 91.59 | 91.56 | **91.78** | 91.76 |\n| th | 67.34 | 65.65 | 66.79 | 66.68 | 67.22 | **67.8** | 66.95 | 67.36 | 67.57 |\n| ro | 91.61 | 91.88 | 91.85 | 91.89 | 91.74 | 91.65 | 91.79 | 91.69 | **92.17** |\n| bn | 95.46 | 96.07 | 95.82 | **96.49** | 96.42 | 96.03 | 96.3 | 95.86 | 96.1 |\n| te | 75.41 | 76.17 | 76.94 | 75.29 | 75.51 | 74.69 | 75.37 | 76.53 | **77.02** |\n| ka | **86.17** | 86.07 | 86.11 | 86.05 | 85.32 | 85.89 | 85.71 | 86.05 | 86.07 |\n| mk | 92.43 | 92.09 | 92.3 | 92.2 | **92.62** | 92.2 | 92.02 | 91.61 | 91.98 |\n| da | 89.76 | 90.08 | **90.33** | 89.74 | 90.02 | 89.72 | 88.99 | 89.41 | 89.48 |\n| sl | 92.61 | 92.85 | 92.82 | 92.78 | **92.93** | 92.62 | 92.77 | 92.71 | 92.56 |\n| az | 87.81 | 87.54 | 87.8 | **88.23** | 87.27 | 87.3 | 87.3 | 86.46 | 87.22 |\n| sk | 90.87 | 90.88 | 90.89 | 90.96 | 90.83 | **91.3** | 90.99 | **91.04** | 90.84 |\n| ms | 93.26 | 93.0 | 92.98 | 92.95 | 93.16 | **93.93** | 93.47 | 92.65 | 92.59 |\n| uz | 86.48 | 86.69 | 86.58 | 86.33 | 86.87 | 86.46 | 87.73 | 87.5 | **88.45** |\n| ur | 94.37 | 94.2 | 93.93 | 94.23 | 94.4 | 94.26 | 94.29 | 94.25 | **94.85** |\n| cy | 88.72 | 89.34 | 89.35 | 89.05 | 89.18 | 89.36 | **90.02** | 88.95 | 88.71 |\n| lv | 92.78 | 92.82 | 93.25 | 93.16 | 92.7 | 92.94 | 92.64 | **93.34** | 92.66 |\n| mr | **86.34** | 86.19 | 85.97 | 86.29 | 86.32 | 86.07 | 84.35 | 86.24 | 86.22 |\n| ne | 66.45 | 61.96 | 61.75 | 64.56 | **71.12** | 69.37 | 70.46 | 70.18 | 66.89 |\n| jv | 52.87 | 62.83 | 61.76 | **65.3** | 63.97 | 58.73 | 63.34 | 57.21 | 58.67 |\n| sw | 83.41 | 83.44 | 83.99 | 83.54 | 83.4 | 83.79 | **84.07** | 81.68 | 81.96 |\n| su | 52.62 | 55.88 | 53.72 | 57.53 | 49.48 | 50.79 | 51.6 | 57.12 | **57.74** |\n| yo | 79.0 | 83.02 | **83.87** | 83.1 | 81.48 | 79.58 | 79.74 | 79.81 | 78.54 |\n| Avg. | 83.82 | 84.35 | 84.38 | **84.7** | 84.46 | 84.2 | 84.36 | 84.36 | 84.36 |\n| mt\u2020 | 58.3 | 49.01 | 51.58 | 50.46 | 60.55 | 61.41 | **64.93** | 60.32 | 62.93 |\n| ku\u2020 | 52.34 | 60.41 | **59.92** | 59.39 | 59.9 | 52.93 | 51.51 | 52.33 | 52.4 |\n| ug\u2020 | 34.1 | 35.33 | 33.07 | 34.56 | 40.2 | 36.24 | 37.62 | 42.93 | **44.05** |\n| si\u2020 | 16.59 | 13.41 | 14.06 | 13.94 | 22.97 | 14.58 | 19.94 | 20.7 | **24.24** |\n| am\u2020 | 37.88 | 33.02 | 33.7 | 35.23 | 32.72 | 46.46 | **46.49** | 36.94 | 32.45 |\n| bo\u2020 | **56.04** | 56.02 | 55.57 | 55.29 | 53.92 | 55.45 | 53.38 | 52.03 | 53.53 |\n| Avg. | 42.54 | 41.2 | 41.32 | 41.48 | 45.04 | 44.51 | **45.64** | 44.21 | 44.93 |\n| Total avg. | 75.56 | 75.72 | 75.77 | 76.05 | 76.58 | 76.26 | **76.62** | 76.33 | 76.47 |", "caption": "Table 20: Pearson and Spearman Correlations for mBERT and XLM-R (Pre-Adapt and Post-Adapt) between task performance and data amounts. Post-Adapt is represented by the models adapted with the Seq_bn_inv language adapters and denote the correlation between the sum of the pre-training and adaptation data sizes and downstream task performance scores after the adaptation.", "description": "\uc774 \ud45c\ub294 mBERT\uc640 XLM-R \ubaa8\ub378\uc5d0 \ub300\ud574 \uc791\uc5c5 \uc131\ub2a5\uacfc \ub370\uc774\ud130 \uc591 \uac04\uc758 \ud53c\uc5b4\uc2a8\uacfc \uc2a4\ud53c\uc5b4\uba3c \uc0c1\uad00 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788, \uc0ac\uc804 \uc801\uc751(Pre-Adapt)\uacfc \uc0ac\ud6c4 \uc801\uc751(Post-Adapt) \ubaa8\ub450\uc5d0 \ub300\ud55c \uc0c1\uad00 \uad00\uacc4\uac00 \uc81c\uc2dc\ub429\ub2c8\ub2e4. \uc0ac\ud6c4 \uc801\uc751\uc758 \uacbd\uc6b0 Seq_bn_inv \uc5b8\uc5b4 \uc5b4\ub311\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \uc801\uc751\uc2dc\ucf30\uc73c\uba70, \uc0ac\uc804 \ud559\uc2b5 \ub370\uc774\ud130\uc640 \uc801\uc751 \ub370\uc774\ud130 \ud06c\uae30\uc758 \ud569\uacc4\uc640 \uc791\uc5c5 \uc131\ub2a5 \uc810\uc218 \uac04\uc758 \uc0c1\uad00 \uad00\uacc4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "7.4 \uc0ac\uc804 \ud559\uc2b5 \ubc0f \uc0ac\ud6c4 \ud559\uc2b5 \ub370\uc774\ud130 \ud06c\uae30\uc758 \uc601\ud5a5"}, {"content": "| ISO | XLM-R |  |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **ISO** | **XLM-R** |  |  |  |  |  |  |  |  |  |\n|  |  | **ConceptNet** |  |  | **Glot** |  |  | **Fusion** |  |  |\n|  | **Base** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **Seq_bn** | **Seq_bn_inv** |\n| th | 66.55 | 66.4 | **66.85** | 66.76 | 66.63 | 65.29 | 66.2 | 65.89 | 66.82 |\n| ro | 91.78 | 91.79 | 91.78 | 91.92 | 92.0 | 91.87 | **92.18** | 92.02 | 92.05 |\n| bg | 91.09 | 91.22 | 91.36 | **91.48** | 91.34 | 91.4 | 91.43 | 90.91 | 91.43 |\n| da | 89.58 | 89.57 | 89.54 | 89.45 | 89.44 | 89.85 | 89.72 | 89.85 | **89.89** |\n| el | 90.03 | 90.32 | 89.88 | 90.14 | 89.89 | 90.02 | 90.02 | 90.18 | **90.5** |\n| he | 85.56 | 85.48 | 85.45 | 84.99 | 84.92 | **85.69** | 85.28 | 85.35 | 85.4 |\n| sk | 91.36 | 91.19 | 91.21 | 91.26 | 91.32 | 91.45 | **91.49** | 91.4 | 91.24 |\n| sl | 92.28 | **92.58** | 92.16 | 92.41 | 92.36 | 92.05 | 92.33 | 92.21 | 92.12 |\n| lv | 92.64 | 92.73 | 92.65 | 92.95 | 92.84 | 92.88 | **93.1** | 92.99 | 92.93 |\n| ms | 92.0 | 92.36 | 91.65 | 92.28 | 92.4 | 92.06 | 91.9 | **92.67** | 91.82 |\n| ka | 86.96 | 86.77 | 86.88 | **87.73** | 87.31 | 87.66 | 87.37 | 86.59 | 87.33 |\n| bn | 95.87 | 95.66 | 95.9 | 96.06 | 96.07 | 96.13 | 96.09 | 95.57 | **96.23** |\n| az | 86.13 | 85.34 | 86.47 | 86.53 | 87.03 | 86.63 | **87.59** | 86.23 | 86.38 |\n| ur | 95.02 | 94.57 | **95.04** | 94.86 | 94.43 | 94.89 | 94.27 | 94.4 | 94.56 |\n| mk | 92.97 | 92.47 | **93.26** | 92.28 | 92.83 | 92.68 | 92.72 | 92.32 | 92.46 |\n| te | 74.67 | 73.64 | **76.07** | 74.27 | 75.18 | 74.38 | 74.82 | 72.92 | 73.91 |\n| ne | 55.47 | 53.0 | 60.02 | 60.0 | 59.08 | 54.99 | 56.61 | **67.84** | 67.34 |\n| si | 63.85 | 58.43 | 63.83 | 57.43 | 68.15 | 60.34 | 66.2 | 71.94 | **73.66** |\n| mr | 85.92 | 85.86 | 85.5 | 85.77 | 84.75 | 85.25 | **86.1** | 85.8 | 85.52 |\n| sw | 84.34 | 83.31 | 84.37 | 84.26 | **84.72** | 84.4 | 84.47 | 84.56 | 83.5 |\n| cy | 89.33 | 88.9 | 88.88 | 88.97 | 89.3 | **89.72** | 89.41 | 89.4 | 89.36 |\n| am | 51.22 | 49.9 | 49.29 | 48.18 | 52.57 | 47.17 | 51.67 | **55.0** | 52.55 |\n| uz | 89.64 | 88.66 | 87.51 | 87.89 | 88.64 | **89.97** | 86.86 | 89.05 | 87.64 |\n| ku | 35.34 | 39.53 | 42.99 | 43.83 | 40.41 | 31.43 | 29.4 | **58.02** | 56.93 |\n| ug | 42.36 | 52.63 | 50.67 | 51.98 | 49.88 | 50.5 | 52.63 | 53.12 | **58.5** |\n| jv | 42.99 | 45.64 | 44.7 | 50.87 | 46.51 | 44.7 | 47.96 | **63.53** | 58.81 |\n| su | 33.07 | 38.4 | 42.26 | 48.32 | 41.47 | 39.76 | 42.89 | **52.53** | 49.61 |\n| Avg. | 77.33 | 77.64 | 78.38 | 78.62 | 78.57 | 77.52 | 78.17 | **80.83** | 80.68 |\n| mt\u2021 | 46.31 | 32.69 | 40.11 | 32.13 | 48.03 | 41.54 | 53.57 | **64.31** | 57.57 |\n| bo\u2021 | 43.51 | 44.29 | 44.55 | 46.41 | 41.86 | 39.64 | 38.27 | **48.15** | 47.55 |\n| yo\u2021 | 73.54 | 71.2 | 73.46 | 74.59 | 73.3 | 74.87 | 75.09 | 73.04 | **75.8** |\n| Avg. | 54.45 | 49.39 | 52.71 | 51.04 | 54.4 | 52.01 | 55.64 | **61.83** | 60.31 |\n| Total avg. | 75.05 | 74.82 | 75.81 | 75.87 | 76.16 | 74.97 | 75.92 | **78.93** | 78.65 |", "caption": "Table 21: F1 scores comparison across different adapters for mBERT in ConceptNet and Glot for sentiment analysis. All results are averaged over 3 independent runs with different random seeds.", "description": "\ubcf8 \ud45c\ub294 mBERT \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec ConceptNet\uacfc Glot \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uac10\uc815 \ubd84\uc11d \uc791\uc5c5\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \uc5b4\ub311\ud130(Seq-bn, LORA, Seq-bn-inv)\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. ConceptNet\uacfc Glot \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uac01 \uc5b4\ub311\ud130\uc758 F1 \uc2a4\ucf54\uc5b4\ub97c \uc81c\uc2dc\ud558\uba70, \uac01 \uacb0\uacfc\ub294 \uc11c\ub85c \ub2e4\ub978 \ub09c\uc218 \uc2dc\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec 3\ubc88 \ub3c5\ub9bd\uc801\uc73c\ub85c \uc2e4\ud589\ud55c \uacb0\uacfc\uc758 \ud3c9\uade0\uac12\uc785\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uac01 \ub370\uc774\ud130\uc14b\uacfc \uc5b4\ub311\ud130 \uc870\ud569\uc5d0 \ub530\ub978 \uac10\uc815 \ubd84\uc11d \uc131\ub2a5\uc758 \ucc28\uc774\ub97c \ubcf4\ub2e4 \uc815\ud655\ud558\uac8c \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.2 \uac10\uc815 \ubd84\uc11d"}, {"content": "| Model | Task | Pre-Adapt | Pre-Adapt | Post-Adapt (Glot) | Post-Adapt (Glot) | Post-Adapt (CN) | Post-Adapt (CN) |\n|---|---|---|---|---|---|---|---| \n|  |  | P (p-value) | S (p-value) | P (p-value) | S (p-value) | P (p-value) | S (p-value) |\n| mBERT | NER | 0.32 (0.1) | 0.32 (0.1) | 0.42 (0.04) | 0.29 (0.2) | 0.20 (0.3) | 0.44 (0.03) |\n| XLM-R | NER | 0.31 (0.1) | 0.58 (0.002) | 0.31 (0.1) | 0.61 (\u00a10.005) | 0.32 (0.1) | 0.60 (\u00a10.005) |", "caption": "Table 22: F1 scores comparison across different adapters for XLM-R in ConceptNet and Glot for sentiment analysis. All results are averaged over 3 independent runs with different random seeds.", "description": "\ud45c 22\ub294 XLM-R \uc5b8\uc5b4 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec ConceptNet\uacfc Glot \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc138 \uac00\uc9c0 \ub2e4\ub978 \uc5b4\ub311\ud130(Seq-bn, LORA, Seq-bn-inv)\ub97c \uc801\uc6a9\ud55c \uac10\uc815 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc138 \uac00\uc9c0 \ub3c5\ub9bd\uc801\uc778 \uc2e4\ud589 \uacb0\uacfc\uc758 \ud3c9\uade0\uac12\uc744 \uc81c\uc2dc\ud558\uba70, \uac01 \uc5b8\uc5b4\uc5d0 \ub300\ud55c ConceptNet\uacfc Glot \ub370\uc774\ud130\uc14b\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc5b4\ub311\ud130 \uc544\ud0a4\ud14d\ucc98\uac00 \uc800\uc790\uc6d0 \uc5b8\uc5b4\uc758 \uac10\uc815 \ubd84\uc11d \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.2 \uac10\uc815 \ubd84\uc11d"}, {"content": "| ISO | mBERT |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n| **ISO** | **mBERT** |  |  |  |  |  |  |  |\n|  |  | **ConceptNet** |  |  | **Glot** |  |  |  |\n|  | **Base** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **FFT** |\n| he | 91.42 | 91.55 | 90.44 | 90.81 | 90.79 | 90.87 | **91.58** | 90.6 |\n| el | **86.35** | 86.27 | 86.05 | 86.22 | 84.88 | 84.95 | 84.52 | 86.38 |\n| bg | 88.82 | 89.41 | 89.17 | **89.54** | 88.76 | 88.65 | 89.2 | 89.99 |\n| th | 81.68 | 81.97 | 81.92 | 82.45 | 82.57 | 82.0 | **83.23** | 83.19 |\n| ro | 92.87 | 92.67 | 92.62 | 92.64 | 93.13 | **92.98** | 92.96 | 93.7 |\n| bn | 92.28 | 92.16 | 92.6 | 91.88 | 92.26 | 92.56 | **92.57** | 92.48 |\n| te | 83.49 | 83.29 | 84.17 | 85.01 | **85.55** | 84.45 | 85.26 | 88.41 |\n| ka | 78.12 | 78.1 | 76.68 | 76.05 | 80.03 | 80.23 | **81.24** | 86.97 |\n| mk | 62.47 | **69.01** | 66.4 | 62.07 | 67.54 | 65.06 | 65.21 | 68.98 |\n| da | 95.71 | 95.33 | 95.77 | 95.33 | 95.95 | **96.15** | 96.09 | 96.84 |\n| sl | 85.71 | 86.46 | 86.28 | 85.81 | 86.79 | 86.4 | **87.83** | 88.66 |\n| az | 79.42 | 79.59 | 79.72 | 80.03 | 79.62 | **80.15** | 80.13 | 81.44 |\n| sk | 91.11 | 88.86 | 89.9 | 89.73 | 90.87 | 91.16 | **92.18** | 91.08 |\n| ms | 91.5 | 92.03 | 91.87 | 91.99 | 92.06 | 91.7 | **92.57** | 93.83 |\n| uz | 86.84 | 85.67 | 86.76 | 85.85 | 86.52 | 86.36 | **86.85** | 88.33 |\n| ur | 82.43 | 81.89 | 82.01 | 82.13 | 82.69 | 82.66 | **82.72** | 83.81 |\n| cy | 87.28 | 86.99 | 87.82 | 86.15 | 87.71 | **87.76** | 87.42 | 88.53 |\n| lv | 75.41 | 75.66 | 73.99 | 74.71 | 76.32 | 75.41 | **76.65** | 79.24 |\n| mr | 88.7 | 88.76 | 89.0 | 88.67 | **89.43** | 89.13 | 88.97 | 90.43 |\n| ne | 59.51 | 51.46 | **67.17** | 55.31 | 56.77 | 59.35 | 63.19 | 63.47 |\n| jv | 75.38 | 74.24 | 74.75 | 73.94 | **76.16** | 75.7 | 75.43 | 75.44 |\n| sw | 57.71 | 54.25 | 57.24 | 52.9 | 65.05 | 62.21 | **69.64** | 54.6 |\n| su | 82.13 | 84.25 | 84.62 | 83.33 | 84.42 | **84.75** | 83.99 | 84.06 |\n| yo | 76.1 | 75.66 | 75.24 | 75.35 | 75.93 | 75.43 | **77.85** | 77.32 |\n| Avg. | 82.18 | 81.9 | 82.59 | 81.58 | 82.99 | 82.75 | **83.64** | 84.07 |\n| mt\u2020 | 65.24 | 65.68 | 62.82 | 66.88 | 68.79 | **73.87** | 65.34 | 74.11 |\n| ku\u2020 | 84.2 | 82.82 | 83.97 | 83.37 | 85.14 | 84.46 | **86.14** | 85.55 |\n| ug\u2020 | 70.94 | 68.35 | 72.67 | 72.19 | 76.91 | 71.35 | **80.4** | 76.63 |\n| si\u2020 | 64.97 | 64.89 | 65.01 | 64.67 | 65.42 | **66.02** | 65.62 | 66.26 |\n| am\u2020 | 61.45 | 62.02 | 60.87 | 61.45 | 60.3 | 61.62 | **63.81** | 59.48 |\n| bo\u2020 | 79.4 | 79.12 | 79.38 | 80.67 | **83.27** | 82.33 | 82.14 | 81.77 |\n| Avg. | 71.03 | 70.48 | 70.79 | 71.54 | 73.3 | 73.27 | **73.91** | 73.97 |\n| Total avg. | 79.95 | 79.61 | 80.23 | 79.57 | 81.05 | 80.86 | **81.69** | 82.05 |", "caption": "Table 23: Pearson and Spearman Correlations for mBERT and XLM-R (Pre-Adapt and Post-Adapt) between task performance and data amounts. Post-Adapt is represented by the models adapted with the Seq_bn_inv language adapters and denote the correlation between the sum of the pre-training and adaptation data sizes and downstream task performance scores after the adaptation.", "description": "\ubcf8 \ud45c\ub294 mBERT\uc640 XLM-R \ubaa8\ub378\uc5d0 \ub300\ud574 \uc791\uc5c5 \uc131\ub2a5\uacfc \ub370\uc774\ud130 \uc591 \uac04\uc758 \ud53c\uc5b4\uc2a8\uacfc \uc2a4\ud53c\uc5b4\uba3c \uc0c1\uad00 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Pre-Adapt \uc5f4\uc740 \uc5b4\ub311\ud130\ub97c \uc801\uc6a9\ud558\uae30 \uc804\uc758 \ubaa8\ub378\uc758 \uc0ac\uc804 \ud559\uc2b5 \ub370\uc774\ud130 \ud06c\uae30\uc640 \uc791\uc5c5 \uc131\ub2a5 \uac04\uc758 \uc0c1\uad00 \uad00\uacc4\ub97c \ub098\ud0c0\ub0b4\uace0, Post-Adapt \uc5f4\uc740 Seq_bn_inv \uc5b8\uc5b4 \uc5b4\ub311\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5b4\ub311\ud130\ub97c \uc801\uc6a9\ud55c \ud6c4 \uc0ac\uc804 \ud559\uc2b5 \ubc0f \uc801\uc751 \ub370\uc774\ud130 \ud06c\uae30\uc758 \ud569\uacfc \uc791\uc5c5 \uc131\ub2a5 \uac04\uc758 \uc0c1\uad00 \uad00\uacc4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc791\uc5c5(TC, SA, NER)\uc5d0 \ub300\ud55c \uc0c1\uad00 \uad00\uacc4\uac00  p-\uac12\uacfc \ud568\uaed8 \uc81c\uc2dc\ub429\ub2c8\ub2e4.", "section": "7.4 Impact of Pre- and Post-Training Data Size on MLM and Downstream Tasks"}, {"content": "| ISO | XLM-R |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n| **ISO** | **XLM-R** |  |  |  |  |  |  |  |\n|  | **ConceptNet** |  |  | **Glot** |  |  |  |  |\n|  | **Base** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **Seq_bn** | **LoRA** | **Seq_bn_inv** | **FFT** |\n| th | 88.18 | 88.26 | 88.43 | **88.46** | 88.11 | 88.31 | 88.13 | 86.39 |\n| ro | 94.37 | 94.84 | 95.03 | **95.04** | 94.74 | 94.67 | 95.03 | 94.55 |\n| bg | 91.36 | 90.66 | **91.43** | 91.41 | 91.26 | 90.93 | 90.65 | 90.79 |\n| da | 98.04 | 97.84 | **98.13** | 98.02 | 98.09 | 98.04 | 97.98 | 97.82 |\n| el | 88.82 | 88.92 | **88.98** | 88.73 | 88.19 | 88.25 | 88.61 | 88.75 |\n| he | 91.26 | 89.66 | **91.81** | 91.25 | 90.48 | 90.27 | 90.85 | 90.2 |\n| sk | **94.6** | 93.86 | 93.87 | 93.43 | 93.22 | 93.72 | 93.44 | 94.03 |\n| sl | 93.75 | 93.46 | **94.32** | 92.68 | 94.23 | 93.57 | 93.86 | 92.73 |\n| lv | 83.3 | **83.78** | 83.36 | 83.83 | 82.47 | 83.12 | 83.65 | 82.97 |\n| ms | 95.51 | 95.27 | **95.66** | 95.57 | 95.44 | 95.29 | 95.53 | 95.26 |\n| ka | **91.92** | 91.51 | 90.8 | 91.21 | **91.92** | 91.11 | 91.41 | 93.33 |\n| bn | 93.78 | 94.14 | 94.3 | **94.46** | 94.13 | 94.1 | 94.43 | 94.41 |\n| az | 84.05 | 84.05 | 84.05 | 83.98 | 84.32 | 84.2 | **84.74** | 85.19 |\n| ur | 85.6 | 85.99 | 85.67 | 85.85 | 85.89 | **86.7** | 86.25 | 87.27 |\n| mk | 70.96 | 69.22 | 67.05 | 69.45 | **73.9** | 70.74 | 72.31 | 71.68 |\n| te | 89.72 | 89.15 | 89.59 | 89.22 | 89.56 | 89.72 | **89.9** | 90.92 |\n| ne | 64.6 | **69.37** | 64.06 | 63.02 | 67.49 | 68.38 | 68.65 | 65.46 |\n| si | 92.49 | 92.59 | 92.18 | **93.21** | 92.49 | 91.78 | 91.96 | 92.85 |\n| mr | 91.17 | 91.8 | 91.9 | 91.8 | 91.87 | **92.36** | 91.8 | 92.43 |\n| sw | 70.08 | 65.37 | 77.11 | 75.3 | **79.52** | 77.24 | 74.45 | 83.84 |\n| cy | 90.83 | 91.01 | 90.57 | 90.65 | 91.12 | 90.88 | **91.36** | 91.01 |\n| am | 86.15 | 83.77 | 84.2 | 82.88 | 87.04 | **87.9** | 87.7 | 87.49 |\n| uz | 87.63 | 88.24 | 88.37 | 88.13 | **88.47** | 87.98 | 88.39 | 90.08 |\n| ku | 89.39 | 89.73 | 89.08 | 89.78 | 92.57 | 89.09 | **93.31** | 95.31 |\n| ug | 88.97 | 88.88 | 89.91 | 87.64 | 88.81 | **90.01** | 89.65 | 91.72 |\n| jv | 76.51 | 77.34 | 77.01 | 77.14 | 76.51 | 76.79 | **77.65** | 75.53 |\n| su | 88.15 | 82.66 | 85.17 | 84.41 | 89.69 | **90.34** | 89.69 | 89.03 |\n| Avg. | 87.45 | 87.09 | 87.48 | 87.28 | **88.2** | 87.98 | **88.2** | 88.56 |\n| mt\u2021 | 55.63 | 55.19 | 55.32 | 54.13 | **69.4** | 63.15 | 69.31 | 70.38 |\n| bo\u2021 | 51.81 | 47.33 | 51.07 | 49.34 | **52.92** | 50.9 | 50.69 | 55.19 |\n| yo\u2021 | 74.73 | 73.4 | 73.6 | 75.09 | 75.5 | 72.0 | **77.65** | 78.99 |\n| Avg. | 60.72 | 58.64 | 60.00 | 59.52 | **65.94** | 62.02 | 65.88 | 68.19 |\n| Total avg. | 84.78 | 84.24 | 84.73 | 84.50 | **85.98** | 85.38 | 85.97 | 86.52 |", "caption": "Table 24: F1 Scores for All Large-Scale Models on TC. The results are based on 3-shot prompting, as reported by Ji et\u00a0al. (2024). GPT-3.5 and GPT-4 results are zero-shot, obtained from Adelani et\u00a0al. (2024a).", "description": "\ud45c 24\ub294 \ub2e4\uc591\ud55c \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc8fc\uc81c \ubd84\ub958(TC) \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Ji et al.(2024)\uc758 \uc5f0\uad6c\uc5d0\uc11c \ubcf4\uace0\ub41c \ubc14\uc640 \uac19\uc774 3-shot \ud504\ub86c\ud504\ud305 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc5ec \uacb0\uacfc\ub97c \uc5bb\uc5c8\uc2b5\ub2c8\ub2e4. Adelani et al.(2024a)\uc758 \uc5f0\uad6c\uc5d0\uc11c \uc81c\uc2dc\ub41c GPT-3.5\uc640 GPT-4\uc758 \uacb0\uacfc\ub294 \uc81c\ub85c\uc0f7 \uc124\uc815\uc5d0\uc11c \uc5bb\uc740 \uacb0\uacfc\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub300\uaddc\ubaa8 \ubaa8\ub378\uc758 TC \uc791\uc5c5 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc22b\uc790\ub85c \uc81c\uc2dc\ud558\uc5ec \uc9c1\uad00\uc801\uc73c\ub85c \ube44\uad50\ud560 \uc218 \uc788\uac8c \ud569\ub2c8\ub2e4. \ud2b9\ud788 3-shot \ud504\ub86c\ud504\ud305\uacfc \uc81c\ub85c\uc0f7 \uc124\uc815\uc5d0\uc11c\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uba85\ud655\ud788 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5 Results: Small mLMs"}, {"content": "| Model | Task | Pre-Adapt P (p-value) | Pre-Adapt S (p-value) | Post-Adapt (Glot) P (p-value) | Post-Adapt (Glot) S (p-value) | Post-Adapt (CN) P (p-value) | Post-Adapt (CN) S (p-value) |\n|---|---|---|---|---|---|---|---| \n| mBERT | SA | 0.45 (_0.03_) | 0.50 (_0.01_) | 0.38 (_0.07_) | 0.41 (_0.05_) | 0.39 (_0.06_) | 0.52 (_0.009_) |\n| XLM-R | SA | 0.36 (_0.07_) | 0.47 (_0.01_) | 0.32 (_0.1_) | 0.33 (_0.1_) | 0.38 (_0.05_) | 0.52 (_0.005_) |", "caption": "Table 25: Three-shot NER results across eight overlapping languages from BUFFET Asai et\u00a0al. (2023). The scores for GPT-3.5 are only provided for two languages.", "description": "\ud45c 25\ub294 Asai \uc678\uc758 \uc5f0\uad6c(2023)\uc758 BUFFET \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c 8\uac1c\uc758 \uc911\ubcf5 \uc5b8\uc5b4\uc5d0 \ub300\ud55c 3-\uc0f7 NER \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. GPT-3.5\uc758 \uc810\uc218\ub294 \ub450 \uac1c\uc758 \uc5b8\uc5b4\uc5d0 \ub300\ud574\uc11c\ub9cc \uc81c\uacf5\ub429\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc758 \uba87\uba87 \uc0f7 \ud559\uc2b5 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \uc139\uc158\uc758 \uc77c\ubd80\uc785\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc22b\uc790\ub85c \uba85\ud655\ud558\uac8c \uc81c\uc2dc\ud558\uc5ec \uc800\uc790\ub4e4\uc758 \uc5f0\uad6c \uacb0\uacfc\ub97c \ub4b7\ubc1b\uce68\ud569\ub2c8\ub2e4.", "section": "5 Results: Small mLMs"}, {"content": "|           | GPT-3.5-turbo-0613 | GPT-4-0613 | LLaMAX2-7B-Alpaca | Llama-2-7b-chat-hf | Meta-Llama-3-8B | Meta-Llama-3.1-8B | Qwen1.5-7B | Qwen2-7B | bloom-7b1 | bloomz-7b1 | gemma-2-9b | gemma-7b | mala-500-10b-v1 | mala-500-10b-v2 | occiglot-7b-eu5 | xglm-7.5B | yayi-7b-llama2 |\n| :--------- | :----------------- | :---------- | :----------------- | :---------------- | :-------------- | :--------------- | :----------- | :--------- | :-------- | :--------- | :----------- | :-------- | :--------------- | :--------------- | :--------------- | :---------- | :--------------- |\n| am         | 24.14              | 38.74       | 7.64               | 5.41              | 38.03           | 40.43            | 13.57        | 23.68       | 7.32      | 8.27       | 41.19         | 43.02      | 5.71             | 9.03             | 6.85             | 7.86        | 3.59             |\n| az         | 52.17              | 44.27       | 30.81              | 20.54             | 73.78           | 71.97            | 51.86        | 65.86       | 10.08     | 16.68      | 57.95         | 68.79      | 5.71             | 5.71             | 31.37            | 26.56       | 17.55            |\n| bn         | 54.29              | 50.55       | 23.79              | 9.35              | 65.89           | 63.43            | 42.62        | 66.08       | 10.75     | 20.93      | 51.22         | 66.91      | 5.71             | 5.69             | 22.42            | 28.25       | 12.99            |\n| bo         | 2.90               | 1.94        | 3.69               | 4.63              | 40.80           | 48.83            | 10.15        | 12.41       | 6.44      | 10.56      | 12.12         | 20.23      | 5.71             | 3.63             | 11.65            | 7.06        | 6.61             |\n| bg         | 54.80              | 58.33       | 31.47              | 29.92             | 64.95           | 63.53            | 55.15        | 77.06       | 20.17     | 16.58      | 51.85         | 63.26      | 5.71             | 5.23             | 44.70            | 41.81       | 24.77            |\n| ku         | 38.74              | 38.10       | 19.71              | 7.67              | 68.26           | 65.47            | 21.71        | 33.20       | 10.26     | 8.63       | 33.49         | 44.59      | 5.71             | 6.86             | 14.07            | 9.31        | 7.81             |\n| cy         | 43.08              | 42.47       | 26.76              | 18.08             | 68.75           | 68.69            | 37.47        | 49.93       | 10.45     | 18.09      | 50.38         | 56.87      | 5.71             | 5.71             | 26.21            | 17.57       | 19.37            |\n| da         | 52.71              | 52.17       | 33.17              | 34.03             | 73.03           | 73.73            | 57.73        | 75.95       | 17.85     | 21.90      | 45.05         | 71.14      | 5.71             | 5.39             | 49.20            | 56.88       | 32.02            |\n| el         | 54.29              | 60.27       | 21.84              | 21.69             | 70.22           | 73.70            | 46.99        | 63.73       | 11.97     | 11.90      | 39.08         | 67.20      | 5.71             | 5.71             | 31.48            | 55.80       | 20.84            |\n| he         | 56.84              | 51.09       | 24.39              | 17.55             | 69.01           | 69.80            | 46.93        | 70.07       | 10.87     | 8.53       | 44.35         | 64.03      | 5.71             | 4.76             | 22.82            | 10.66       | 9.51             |\n| jv         | 21.05              | 21.05       | 28.49              | 21.31             | 66.73           | 69.39            | 49.99        | 50.76       | 17.90     | 25.19      | 59.48         | 57.33      | 5.71             | 2.20             | 34.05            | 44.85       | 19.65            |\n| ka         | 47.19              | 43.68       | 18.37              | 15.25             | 68.58           | 63.50            | 32.76        | 52.02       | 3.50      | 14.76      | 58.73         | 69.17      | 5.71             | 8.13             | 25.17            | 9.35        | 13.24            |\n| lv         | 54.29              | 53.76       | 31.62              | 23.85             | 69.79           | 70.63            | 55.05        | 67.69       | 12.70     | 17.38      | 45.97         | 69.24      | 8.21             | 3.13             | 34.20            | 23.25       | 23.91            |\n| mr         | 52.71              | 51.09       | 19.90              | 14.04             | 64.84           | 63.07            | 39.41        | 56.66       | 26.78     | 29.62      | 27.30         | 56.58      | 5.71             | 5.83             | 19.63            | 23.24       | 9.59             |\n| mk         | 52.71              | 60.75       | 28.98              | 26.75             | 66.66           | 68.33            | 55.99        | 75.87       | 12.91     | 16.69      | 55.62         | 64.88      | 3.97             | 3.49             | 40.23            | 40.43       | 22.65            |\n| mt         | 44.27              | 50.55       | 29.18              | 23.07             | 63.25           | 67.22            | 44.26        | 56.10       | 11.45     | 20.18      | 43.93         | 62.54      | 5.71             | 5.71             | 34.33            | 28.45       | 24.09            |\n| ne         | 55.83              | 52.71       | 21.49              | 18.42             | 62.32           | 62.69            | 42.96        | 54.99       | 10.12     | 19.45      | 15.71         | 62.31      | 5.62             | 4.07             | 25.79            | 31.47       | 18.61            |\n| ro         | 51.64              | 54.80       | 34.88              | 31.49             | 70.19           | 72.20            | 56.43        | 74.64       | 20.10     | 20.76      | 52.51         | 69.08      | 5.71             | 5.71             | 47.32            | 43.15       | 30.92            |\n| si         | 23.38              | 62.63       | 8.66               | 4.81              | 60.25           | 57.45            | 12.49        | 29.29       | 5.98      | 9.38       | 46.12         | 65.92      | 6.60             | 2.20             | 10.82            | 5.48        | 5.71             |\n| sk         | 52.17              | 52.71       | 28.65              | 29.75             | 70.57           | 72.77            | 55.40        | 74.63       | 20.27     | 17.58      | 35.52         | 68.94      | 5.71             | 8.49             | 43.66            | 39.12       | 27.70            |\n| sl         | 53.76              | 47.76       | 33.60              | 31.05             | 75.67           | 70.18            | 55.53        | 63.56       | 11.10     | 17.18      | 48.42         | 67.87      | 9.22             | 3.30             | 40.19            | 30.21       | 28.09            |\n| su         | 26.38              | 20.26       | 28.22              | 23.89             | 63.50           | 67.46            | 46.31        | 58.94       | 17.55     | 21.68      | 60.68         | 65.78      | 5.71             | 7.69             | 32.18            | 44.52       | 21.59            |\n| sw         | 55.83              | 46.62       | 28.24              | 14.01             | 68.95           | 68.37            | 40.51        | 51.05       | 12.91     | 22.41      | 48.61         | 58.78      | 5.71             | 6.70             | 29.03            | 45.91       | 11.88            |\n| te         | 57.84              | 50.00       | 5.92               | 5.78              | 64.72           | 62.36            | 27.29        | 55.69       | 16.89     | 20.13      | 47.24         | 68.93      | 5.71             | 5.17             | 12.91            | 49.59       | 4.73             |\n| th         | 53.24              | 49.45       | 16.94              | 20.88             | 77.50           | 75.40            | 46.57        | 67.38       | 6.25      | 16.62      | 45.24         | 58.64      | 5.71             | 7.82             | 35.27            | 50.01       | 21.98            |\n| ug         | 44.27              | 46.04       | 6.53               | 6.90              | 66.23           | 62.22            | 12.37        | 54.64       | 9.29      | 11.72      | 33.74         | 45.77      | 7.54             | 3.66             | 16.20            | 7.76        | 7.13             |\n| ur         | 53.24              | 65.79       | 22.87              | 15.07             | 67.80           | 67.53            | 39.13        | 61.90       | 23.50     | 23.33      | 29.04         | 56.48      | 5.71             | 6.61             | 29.62            | 41.90       | 12.23            |\n| uz         | 44.87              | 34.82       | 29.50              | 13.49             | 69.53           | 68.35            | 33.53        | 54.55       | 10.05     | 13.89      | 56.50         | 65.44      | 5.71             | 11.34            | 26.86            | 15.93       | 10.11            |\n| yo         | 22.61              | 16.22       | 18.17              | 11.26             | 50.05           | 46.74            | 25.36        | 30.44       | 14.08     | 21.90      | 35.16         | 37.11      | 8.75             | 7.65             | 18.71            | 16.97       | 10.23            |\n| ms         | 49.45              | 55.83       | 30.46              | 27.35             | 74.10           | 73.28            | 56.74        | 76.05       | 11.13     | 23.31      | 55.96         | 69.52      | 5.71             | 5.71             | 40.15            | 46.19       | 27.33            |\n| Total avg. | 45.02              | 45.82       | 23.13              | 18.24             | 65.80           | 65.62            | 40.41        | 56.83       | 13.02     | 17.51      | 44.27         | 60.21      | 6.04             | 5.74             | 28.57            | 29.98       | 16.88            |", "caption": "Table 26: F1 Scores for DeepSeek-R1 distilled models of various sizes for TC. The results are based on zero-shot prompting and were obtained in our evaluation.", "description": "\ud45c 26\uc740 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 DeepSeek-R1 \uc99d\ub958 \ubaa8\ub378\uc5d0 \ub300\ud55c \uc8fc\uc81c \ubd84\ub958(TC) \uc791\uc5c5\uc758 F1 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uacb0\uacfc\ub294 \uc81c\ub85c\uc0f7 \ud504\ub86c\ud504\ud305\uc744 \uae30\ubc18\uc73c\ub85c \ud558\uba70, \ubcf8 \uc5f0\uad6c\uc758 \ud3c9\uac00\uc5d0\uc11c \uc5bb\uc740 \uacb0\uacfc\uc785\ub2c8\ub2e4. \ud45c\ub294 \ubaa8\ub378 \ud06c\uae30\ubcc4\ub85c \ub2e4\uc591\ud55c \uc5b8\uc5b4\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ud06c\uae30\uc640 \uc5b8\uc5b4\uc5d0 \ub530\ub978 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uc800\uc790\uc6d0 \uc5b8\uc5b4\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \uc131\ub2a5\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "5. Small mLMs vs. SOTA LLMs"}, {"content": "|       | Bloom | Bloomz | mT0 | GPT-3.5-turbo-0301 |\n| :---- | ----: | ----: | ----: | ----: |\n| th    | 1.0   | 0.2   | 1.4   | -     |\n| el    | 19.7  | 13.0  | 12.8  | 69.3  |\n| ur    | 71.7  | 47.3  | 47.1  | -     |\n| te    | 5.3   | 3.8   | 3.3   | -     |\n| sw    | 58.8  | 26.8  | 24.3  | -     |\n| bg    | 29.6  | 19.7  | 14.7  | 72.0  |\n| mr    | 27.9  | 20.4  | 12.3  | -     |\n| bn    | 36.8  | 36.2  | 23.9  | -     |\n| Total avg. | 31.35 | 20.92 | 17.48 | 70.65 |", "caption": "Table 27: Comparison of F1 Scores for LLaMA-3 Baseline (fine-tuned with a task adapter) and LLaMA-3+Seq_bn_inv on TC, NER, and SA. All results are averaged over 3 independent runs with different random seeds.", "description": "\ud45c 27\uc740 \uacfc\uc81c \uc801\uc751\uae30\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubbf8\uc138 \uc870\uc815\ub41c LLaMA-3 \uae30\uc900 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec TC, NER \ubc0f SA\uc5d0 \ub300\ud55c LLaMA-3+Seq-bn_inv\uc758 F1 \uc810\uc218\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \ubaa8\ub4e0 \uacb0\uacfc\ub294 \uc11c\ub85c \ub2e4\ub978 \ub09c\uc218 \uc2dc\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec 3\ubc88\uc758 \ub3c5\ub9bd \uc2e4\ud589\uc744 \ud3c9\uade0\ub0b8 \uac83\uc785\ub2c8\ub2e4.", "section": "6 Results: Small mLMs vs. SOTA LLMs"}]