{"references": [{"fullname_first_author": "Neil Houlsby", "paper_title": "Parameter-efficient transfer learning for NLP", "publication_date": "2019-00-00", "reason": "This paper introduces adapter-based methods, which are the core of the proposed adaptation techniques."}, {"fullname_first_author": "Jonas Pfeiffer", "paper_title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer", "publication_date": "2020-00-00", "reason": "This paper is highly relevant due to its introduction of the adapter architecture used in the study for multilingual language model adaptation."}, {"fullname_first_author": "Alexis Conneau", "paper_title": "Unsupervised cross-lingual representation learning at scale", "publication_date": "2020-00-00", "reason": "This paper introduces XLM-R, one of the main multilingual language models investigated in the study."}, {"fullname_first_author": "Amir Hossein Kargaran", "paper_title": "GlotCC: An open broad-coverage CommonCrawl corpus and pipeline for minority languages", "publication_date": "2024-00-00", "reason": "This paper introduces the GlotCC corpus, which is the main dataset used for the adaptation of multilingual language models in this study."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-00-00", "reason": "This paper introduces mBERT, one of the multilingual language models used as a base model in the study"}]}