---
title: "IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations"
summary: "IDArb:  Decomposition under varied lights."
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Computer Vision", "3D Vision", "ğŸ¢ Chinese University of Hong Kong",]
showSummary: true
date: 2024-12-16
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2412.12083 {{< /keyword >}}
{{< keyword icon="writer" >}} Zhibing Li et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2024-12-17 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2412.12083" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2412.12083" target="_self" >}}
â†— Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/idarb-intrinsic-decomposition-for-arbitrary" target="_self" >}}
â†— Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

**Traditional methods for separating an object's true color and material from lighting effects in images (intrinsic decomposition) struggle with long processing times and inaccuracies.** Optimization-based methods require hours and often mix lighting with material, while learning-based methods, though faster, are inconsistent across different viewpoints.  Existing datasets for this task are also limited in scope and diversity, making it hard to train truly robust models. **Accurate intrinsic decomposition is crucial** for applications like relighting objects in images, editing materials, and even creating realistic 3D models. 



IDArb tackles these challenges using a **new AI model that can handle any number of images of an object under different lighting conditions.** It employs clever attention mechanisms to ensure **consistent results across all viewpoints and disentangles material from lighting.**  Itâ€™s also trained on a new, massive dataset, ARB-Objaverse, containing millions of images with diverse objects and lighting, resulting in **more accurate and robust intrinsic decomposition.**  This enables significantly better results in various applications like **relighting, material editing, and 3D reconstruction.**

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} IDArb achieves multi-view consistent intrinsic decomposition under varying illuminations. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} A new dataset, ARB-Objaverse, provides large-scale multi-view intrinsic data with diverse lighting. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} IDArb facilitates downstream tasks like relighting, material editing, and 3D reconstruction, outperforming existing methods. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
**IDArb presents a significant advancement in intrinsic image decomposition**, impacting researchers in computer vision and graphics. It offers a robust, efficient solution for **multi-view decomposition under varied lighting**, which is crucial for realistic 3D content creation.  The introduction of **ARB-Objaverse dataset** enables future research on robust intrinsic decomposition models. Its application in relighting, material editing, and 3D reconstruction opens **new possibilities for realistic content creation and editing.**

------
#### Visual Insights



![](https://arxiv.org/html/2412.12083/x1.png)

> ğŸ”¼ IDArbëŠ” ì œì•½ ì—†ëŠ” ì¡°ëª… ì¡°ê±´ì—ì„œ ë‹¤ì–‘í•œ ìˆ˜ì˜ ë·°ë¥¼ ì…ë ¥ë°›ì•„ ë‚´ì¬ ë¶„í•´ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. í•™ìŠµ ê¸°ë°˜ ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ë‹¤ì¤‘ ë·° ì¼ê´€ì„±ì„ ë‹¬ì„±í•˜ê³  ìµœì í™” ê¸°ë°˜ ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ í•™ìŠµëœ ì‚¬ì „ ì§€ì‹ì„ í†µí•´ ì¡°ëª… íš¨ê³¼ì—ì„œ ë‚´ì¬ ìš”ì†Œë¥¼ ë” ì˜ ë¶„ë¦¬í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ì¬ì¡°ëª… ë° ì¬ì§ˆ í¸ì§‘, ì‚¬ì§„ ì¸¡ëŸ‰ ìŠ¤í…Œë ˆì˜¤, 3D ì¬êµ¬ì„±ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 1: IDArb tackles intrinsic decomposition for an arbitrary number of views under unconstrained illumination. Our approach (a) achieves multi-view consistency compared to learning-based methods and (b) better disentangles intrinsic components from lighting effects via learnt priors compared to optimization-based methods. Our method could enhance a wide range of applications such as image relighting and material editing, photometric stereo, and 3D reconstruction.
> </details>





{{< table-caption >}}
| | Albedo | | Normal | Metallic | Roughness |
|---|---|---|---|---|---|---| 
| | SSIMâ†‘ | PSNRâ†‘ | Cosine Similarity â†‘ | MSE â†“ | MSE â†“ |
| IID | 0.901 | 27.35 | - | 0.192 | 0.131 |
| RGBâ†”X | 0.902 | 28.09 | 0.834 | 0.162 | 0.347 |
| IntrinsicAnything | 0.901 | 28.17 | - | - | - |
| GeoWizard | - | - | 0.871 | - | - |
| Ours(single) | **0.935** | **32.79** | **0.928** | **0.037** | **0.058** |
| Ours(multi) | **0.937** | **33.62** | **0.941** | **0.016** | **0.033** |{{< /table-caption >}}

> ğŸ”¼ IDArbê°€ ë‹¤ë¥¸ ê¸°ì¤€ ëª¨ë¸ë“¤ê³¼ ë¹„êµí•˜ì—¬ ëª¨ë“  ì§€í‘œ(ì•Œë² ë„, ë…¸ë©€, ë©”íƒˆë¦­, ëŸ¬í”„ë‹ˆìŠ¤)ì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì£¼ëŠ” ì •ëŸ‰ì  í‰ê°€ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ í‘œì…ë‹ˆë‹¤. IDArbëŠ” ë‹¨ì¼ ë·° ë° ë‹¤ì¤‘ ë·° ì„¤ì • ëª¨ë‘ì—ì„œ ë‹¤ë¥¸ ë°©ë²•ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 1: Quantitative evaluation of IDArb against baselines.Â IDArb consistently achieves the best results among all albedo, normal, metallic and roughness metrics.
> </details>





### In-depth insights


#### Intrinsic Decomp
**ë³¸ì§ˆì  ë¶„í•´(Intrinsic Decomp)**ëŠ” ì»´í“¨í„° ë¹„ì „ ë° ê·¸ë˜í”½ì—ì„œ ì´ë¯¸ì§€ì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œë¥¼ ì¶”ì¶œí•˜ëŠ” í•µì‹¬ ê³¼ì œì…ë‹ˆë‹¤. ì´ëŠ” 3D ì¥ë©´ ì´í•´, ì¬ì§ˆ í¸ì§‘, ì¬ì¡°ëª… ë“± ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì˜ ê¸°ë°˜ì´ ë©ë‹ˆë‹¤. ë³¸ì§ˆì  ë¶„í•´ëŠ” ì…ë ¥ ì´ë¯¸ì§€ì—ì„œ **ì•Œë² ë„, ë²•ì„ , ê¸ˆì†ì„±, ê±°ì¹ ê¸°**ì™€ ê°™ì€ ê³ ìœ  ì†ì„±ì„ ë¶„ë¦¬í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì†ì„±ì€ ê°ì²´ì˜ **ëª¨ì–‘, ì¬ì§ˆ, ì¡°ëª…ê³¼ ë¬´ê´€**í•˜ë©° ì¥ë©´ì˜ ì§„ì •í•œ ë³¸ì§ˆì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì „í†µì ì¸ ìµœì í™” ê¸°ë°˜ ë°©ë²•ì€ ê³„ì‚°ì ìœ¼ë¡œ **ë¹„ì‹¸ê³  ì¡°ëª…ê³¼ ì¬ì§ˆì˜ ëª¨í˜¸ì„±ì„ í•´ê²°í•˜ëŠ” ë° ì–´ë ¤ì›€**ì„ ê²ªìŠµë‹ˆë‹¤. ìµœê·¼ ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ ë°©ë²•ì€ **ë°ì´í„° ê¸°ë°˜ ì‚¬ì „ ì •ë³´ í™œìš©**ì„ í†µí•´ **ê³ í’ˆì§ˆ ë¶„í•´**ë¥¼ ë‹¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¨ì¼ ì´ë¯¸ì§€ ê¸°ë°˜ ë°©ë²•ì€ ì—¬ëŸ¬ ë·°ì—ì„œ **ì¼ê´€ì„± ì—†ëŠ” ê²°ê³¼**ë¥¼ ìƒì„±í•˜ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ì¤‘ ë·° ì¼ê´€ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ë³¸ì§ˆì  ë¶„í•´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ê³¼ì œë¡œ ë‚¨ì•„ ìˆìœ¼ë©°, ë·° ê°„ì˜ **ì •ë³´ ìœµí•© ë° ëª¨í˜¸ì„± í•´ê²°ì„ ìœ„í•œ íš¨ê³¼ì ì¸ ì „ëµ**ì´ í•„ìš”í•©ë‹ˆë‹¤.

#### Diffusion Model
**í™•ì‚° ëª¨ë¸**ì€ ë…¸ì´ì¦ˆ ì œê±°ë¥¼ í†µí•œ ì—­ í™•ì‚° í”„ë¡œì„¸ìŠ¤ë¡œ **ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±**ì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤. **Stable Diffusion**ê³¼ ê°™ì€ ìµœì‹  ëª¨ë¸ì€ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±ì—ì„œ ì£¼ëª©í•  ë§Œí•œ ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆìœ¼ë©°, **ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼**ì— ê±¸ì³ ìœ ë§í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **ë‚´ì¬ì  ë¶„í•´**ë¥¼ ìœ„í•´ **êµì°¨ ë„ë©”ì¸ ì–´í…ì…˜ ëª¨ë“ˆ**ì„ í™œìš©í•˜ì—¬ **ë‹¤ì–‘í•œ ì…ë ¥ ë·°ì™€ ì¡°ëª… ì¡°ê±´**ì„ ì²˜ë¦¬í•˜ëŠ” í™•ì‚° ê¸°ë°˜ ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ **ì‚¬ì‹¤ì ì¸ 3D ì½˜í…ì¸  ì œì‘**ì„ ìœ„í•œ **ë©€í‹°ë·° ì¼ê´€ì„±** ë° **ê³ ì£¼íŒŒ ë””í…Œì¼**ì„ ê°–ì¶˜ **ì •í™•í•œ ë‚´ì¬ì  êµ¬ì„± ìš”ì†Œ ì¶”ì •**ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

#### Multi-view Data
**ë©€í‹° ë·° ë°ì´í„°ëŠ” ë¬¼ì²´ë‚˜ ì¥ë©´ì— ëŒ€í•œ í’ë¶€í•˜ê³  ë‹¤ì–‘í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì—¬** ë‹¤ì–‘í•œ ì»´í“¨í„° ë¹„ì „ ë° ê·¸ë˜í”½ ì‘ì—…ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ê°ë„ì—ì„œ ìº¡ì²˜ëœ ì´ë¯¸ì§€ëŠ” ê°ì²´ì˜ **3ì°¨ì› í˜•ìƒ, ì¬ì§ˆ ì†ì„±, ì£¼ë³€ ì¡°ëª…ì„** ë³´ë‹¤ ì™„ë²½í•˜ê²Œ í‘œí˜„í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ëŠ” **ê¹Šì´ ì¶”ì •, 3D ì¬êµ¬ì„±, ë¬¼ì²´ ì¸ì‹ ë° ì¥ë©´ ì´í•´**ì™€ ê°™ì€ ì‘ì—…ì—ì„œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. **ë©€í‹° ë·° ë°ì´í„°ëŠ” ë°ì´í„°ì˜ ì–‘ê³¼ ë‹¤ì–‘ì„± ë•ë¶„ì—** í›ˆë ¨ëœ ëª¨ë¸ì˜ **ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼œ** ë³´ë‹¤ **ì •í™•í•˜ê³  ê°•ë ¥í•œ ì˜ˆì¸¡ì„** ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, **ë©€í‹° ë·° ì¼ê´€ì„±ì„** í†µí•´ ì—¬ëŸ¬ ì‹œì ì—ì„œ **ì˜ˆì¸¡ì˜ ì •í™•ì„±ê³¼ ì•ˆì •ì„±ì„ ë³´ì¥**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë©€í‹° ë·° ë°ì´í„°ì˜ ì£¼ìš” ê³¼ì œ ì¤‘ í•˜ë‚˜ëŠ” ì—¬ëŸ¬ ì‹œì ì—ì„œ ìº¡ì²˜ëœ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **êµì°¨ ë·° ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜**ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ê¸°ìˆ ì´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë©”ì»¤ë‹ˆì¦˜ì€ **ë‹¤ë¥¸ ë·° ê°„ì˜ ê´€ê³„ë¥¼** ëª¨ë¸ë§í•˜ê³  **ì „ì—­ ì •ë³´ êµí™˜ì„** ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ **ì¼ê´€ë˜ê³  ì •í™•í•œ** ë©€í‹° ë·° ì¬êµ¬ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤. ìš”ì•½í•˜ë©´, **ë©€í‹° ë·° ë°ì´í„°ëŠ”** ì»´í“¨í„° ë¹„ì „ ë° ê·¸ë˜í”½ ë¶„ì•¼ì˜ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ **ì¤‘ìš”í•œ ì—­í• ì„** í•˜ë©°, ë©€í‹° ë·° ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ê¸°ìˆ ì€ **ë”ìš± ê°•ë ¥í•˜ê³  ì‚¬ì‹¤ì ì¸ 3D ëª¨ë¸ ë° ì¥ë©´ í‘œí˜„ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ìš”**í•©ë‹ˆë‹¤.

#### Relighting App
**ì¬ì¡°ëª… ì•±**ì€ ì´ë¯¸ì§€ì˜ ê³ ìœ í•œ ì†ì„±(ì•Œë² ë„, í‘œë©´ ë²•ì„ , ê¸ˆì†ì„±, ê±°ì¹ ê¸°)ì„ ë¶„í•´í•˜ì—¬ ë‹¤ì–‘í•œ ì¡°ëª… ì¡°ê±´ì—ì„œ ì‚¬ì‹¤ì ì¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì•±ì€ **ì—­ë Œë”ë§** ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ ê¸°í•˜í•™ì  ë° ì¬ì§ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” ì¡°ëª…ì„ ìˆ˜ì •í•˜ê±°ë‚˜ í¸ì§‘í•˜ì—¬ ì›ë³¸ ì´ë¯¸ì§€ì˜ ëª¨ì–‘ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–´ë‘ìš´ ì´ë¯¸ì§€ë¥¼ ë°ê²Œ í•˜ê±°ë‚˜, ì¡°ëª…ì˜ ìƒ‰ìƒì„ ë³€ê²½í•˜ê±°ë‚˜, ê·¸ë¦¼ìë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ëŠ¥ì€ **ì‚¬ì§„ í¸ì§‘, ê²Œì„ ê°œë°œ, ì˜í™” ì œì‘, ê±´ì¶• ë””ìì¸**ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, **ê°€ìƒ í™˜ê²½ì—ì„œ ì‚¬ì‹¤ì ì¸ ì¡°ëª… íš¨ê³¼ë¥¼ ì‹œë®¬ë ˆì´ì…˜**í•˜ê±°ë‚˜, **ì œí’ˆì˜ ì™¸ê´€ì„ ë‹¤ì–‘í•œ ì¡°ëª… ì¡°ê±´ì—ì„œ ë¯¸ë¦¬ í™•ì¸**í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤. ì¬ì¡°ëª… ì•±ì€ ì‚¬ìš©ìì—ê²Œ **ì°½ì˜ì ì¸ í‘œí˜„**ì„ ìœ„í•œ ê°•ë ¥í•œ ë„êµ¬ë¥¼ ì œê³µí•˜ë©°, **ëª°ì…í˜• ê²½í—˜**ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì•±ì˜ ë°œì „ì€ **ì»´í“¨í„° ë¹„ì „ ë° ê·¸ë˜í”½ ê¸°ìˆ ì˜ ë°œì „**ê³¼ ë°€ì ‘í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆìœ¼ë©°, ì•ìœ¼ë¡œ ë”ìš± ì‚¬ì‹¤ì ì´ê³  ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

#### Dataset & Limits
**ARB-Objaverse ë°ì´í„°ì…‹ì€ ë‹¤ì–‘í•œ ì¡°ëª… ì¡°ê±´ì—ì„œ ë Œë”ë§ëœ ëŒ€ê·œëª¨ ê°ì²´ë“¤ì„ ì œê³µí•˜ì—¬ ê¸°ì¡´ ë°ì´í„°ì…‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•©ë‹ˆë‹¤.** 68kê°œì˜ 3D ëª¨ë¸ì„ Objaverseì—ì„œ ì„ íƒí•˜ê³ , ê° ê°ì²´ì— ëŒ€í•´ ë‹¤ì–‘í•œ ì¡°ëª…ìœ¼ë¡œ 7ê°œì˜ ì´ë¯¸ì§€ë¥¼ 12ê°œ ì‹œì ì—ì„œ ë Œë”ë§í•˜ì—¬ **5.7Mê°œì˜ RGB ì´ë¯¸ì§€ì™€ ì¡°ëª… ì¡°ê±´ì— ë”°ë¥¸ ë³¸ì§ˆì  ìš”ì†Œ**ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” **ë‹¤ì–‘í•œ ì¡°ëª…, ì‹œì , ê°ì²´ì˜ ì¡°í•©**ìœ¼ë¡œ í›ˆë ¨ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì„ í™•ë³´í•˜ê³ , **ì¡°ëª…ê³¼ ì¬ì§ˆì˜ ëª¨í˜¸ì„± ë¬¸ì œ**ë¥¼ ì™„í™”í•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤. **í•˜ì§€ë§Œ ì‹¤ì œ ë°ì´í„° ë¶€ì¡±ì€ ì—¬ì „íˆ í•œê³„**ë¡œ ë‚¨ì•„ìˆìœ¼ë©°, íŠ¹íˆ ë³µì¡í•œ ì¬ì§ˆ ë³€í™”ë¥¼ ê°€ì§„ ê°ì²´ì˜ ê²½ìš° ê³¼ë„í•˜ê²Œ ë‹¨ìˆœí™”ëœ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ **ì‹¤ì œ ë°ì´í„°ë¥¼ í†µí•©í•˜ëŠ” ë¹„ì§€ë„ í•™ìŠµ ê¸°ë²•** ë“± ì¶”ê°€ ì—°êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. **ë˜í•œ, í˜„ì¬ êµì°¨ ì‹œì  ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ O(NÂ²) ë³µì¡ë„ëŠ” ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë˜ëŠ” ë§ì€ ì‹œì ì—ì„œì˜ ì²˜ë¦¬ë¥¼ ì–´ë µê²Œ í•©ë‹ˆë‹¤.** í–¥í›„ ì—°êµ¬ì—ì„œëŠ” **íš¨ìœ¨ì ì¸ êµì°¨ ì‹œì  ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜** ê°œë°œì´ ì¤‘ìš”í•©ë‹ˆë‹¤.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2412.12083/x2.png)

> ğŸ”¼ IDArbëŠ” ë‹¤ì–‘í•œ ì¡°ëª… ì¡°ê±´ì—ì„œ ì´¬ì˜ëœ ì„ì˜ ê°œìˆ˜ì˜ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ë°›ì•„ intrinsic decompositionì„ ìˆ˜í–‰í•˜ëŠ” í™•ì‚° ê¸°ë°˜ ëª¨ë¸ì…ë‹ˆë‹¤. ê·¸ë¦¼ì€ IDArbì˜ ì „ì²´ì ì¸ êµ¬ì¡°ì™€ UNet ë‚´ë¶€ì˜ attention blockì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì…ë ¥ ì´ë¯¸ì§€ë“¤ì€ N_vê°œì˜ ì‹œì ê³¼ N_iê°œì˜ ì¡°ëª… ì¡°ê±´ì—ì„œ ìƒ˜í”Œë§ë˜ë©°, ê° ì´ë¯¸ì§€ì˜ latent vectorëŠ” ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆì™€ ì—°ê²°ë˜ì–´ denoisingì— ì‚¬ìš©ë©ë‹ˆë‹¤. Intrinsic componentëŠ” Albedo, Normal, Metallic&Roughnessì˜ ì„¸ ê°€ì§€ tripletìœ¼ë¡œ ë‚˜ë‰˜ë©°, ê°ê° íŠ¹ì • í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì•ˆë‚´í•©ë‹ˆë‹¤. UNet ë‚´ë¶€ì˜ attention blockì€ cross-component attentionê³¼ cross-view attention ëª¨ë“ˆì„ í†µí•´ componentì™€ ì‹œì  ê°„ì˜ ì •ë³´ êµí™˜ì„ ì´‰ì§„í•˜ì—¬, ì „ì—­ ì •ë³´ êµí™˜ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 2: Top: Overview of Â IDArb. Bottom: Illustration of the attention block within the UNet. Our training batch consists of Nğ‘Nitalic_N input images, sampled from Nvsubscriptğ‘ğ‘£N_{v}italic_N start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT viewpoints and Nisubscriptğ‘ğ‘–N_{i}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT illuminations. The latent vector for each image is concatenated with Gaussian noise for denoising. Intrinsic components are divided into three triplets (Dğ·Ditalic_D=3): Albedo, Normal and Metallic&Roughness. Specific text prompts are used to guide the model toward different intrinsic components. For attention block inside UNet, we introduce cross-component and cross-view attention module into it, where attention is applied across components and views, facilitating global information exchange.
> </details>



![](https://arxiv.org/html/2412.12083/x3.png)

> ğŸ”¼ ARB-Objaverse ë°ì´í„°ì…‹ì€ ë‹¤ì–‘í•œ ë¬¼ì²´ë“¤ì„ ì—¬ëŸ¬ ì¡°ëª… ì¡°ê±´ì—ì„œ ë Œë”ë§í•˜ì—¬ ì¡°ëª… ë³€í™”ì— ê°•ì¸í•œ í•™ìŠµ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê° ë¬¼ì²´ëŠ” albedo, normal, metallic, roughnessì™€ ê°™ì€ intrinsic ìš”ì†Œë“¤ê³¼ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤. ê·¸ë¦¼ì—ì„œ ABO, G-Objaverse, A12-Objaverse ë°ì´í„°ì…‹ê³¼ ë¹„êµí•˜ì—¬ ARB-Objaverseì˜ ë‹¤ì–‘í•œ ë¬¼ì²´ ë° ì¡°ëª… ì¡°ê±´ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 3: Overview of the Arb-Objaverse dataset. Our custom dataset features a diverse collection of objects rendered under various lighting conditions, accompanied by their intrinsic components.
> </details>



![](https://arxiv.org/html/2412.12083/x4.png)

> ğŸ”¼ (a) ì•Œë² ë„ ì¶”ì •. IDArbëŠ” í•™ìŠµ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ê³¼ ë‹¬ë¦¬ í•˜ì´ë¼ì´íŠ¸ì™€ ê·¸ë¦¼ìë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì œê±°í•˜ì—¬ ë” ì •í™•í•œ ì•Œë² ë„ ë§µì„ ìƒì„±í•©ë‹ˆë‹¤. ìµœì í™” ê¸°ë°˜ ë°©ë²•ê³¼ ë¹„êµí–ˆì„ ë•Œ, IDArbëŠ” ì¡°ëª… íš¨ê³¼ë¥¼ ì•Œë² ë„ì— ì‚½ì…í•˜ì§€ ì•Šê³  ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (a) Albedo estimation. Our method effectively removes highlights and shadows.
> </details>



![](https://arxiv.org/html/2412.12083/x5.png)

> ğŸ”¼ IDArbê°€ ë‹¤ë¥¸ ë°©ë²•ë“¤(RGBâ†’X, GeoWizard)ê³¼ ë¹„êµí•˜ì—¬, í‰ë©´ì„ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•˜ë©´ì„œë„ ë¬¼ì²´ì˜ í˜•íƒœë¥¼ ì˜ ë‚˜íƒ€ë‚´ëŠ” ë…¸ë©€ ë§µì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. RGBâ†’XëŠ” ë¬¼ì²´ì˜ í…ìŠ¤ì²˜ì— ì˜í•´ ê°„ì„­ì„ ë°›ëŠ” ëª¨ìŠµì„ ë³´ì´ë©°, GeoWizardëŠ” íë¦¿í•œ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (b) Normal estimation. Our method gives shape geometry while correctly predicting flat surface.
> </details>



![](https://arxiv.org/html/2412.12083/x6.png)

> ğŸ”¼ IDArbëŠ” í…ìŠ¤ì²˜ íŒ¨í„´ ë° ì¡°ëª…ì˜ ê°„ì„­ ì—†ì´ ì‹¤ì œì™€ ê°™ì€ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì—¬ ê¸ˆì†ì„± ì¶”ì •ì—ì„œ IID ë° RGBâ†”Xë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (c) Metallic estimation. Our method outperforms IID and RGBâ†”â†”\leftrightarrowâ†”X with plausible results free of interference from texture patterns and lighting.
> </details>



![](https://arxiv.org/html/2412.12083/x7.png)

> ğŸ”¼ IDArbê°€ í…ìŠ¤ì²˜ íŒ¨í„´ ë° ì¡°ëª…ì˜ ê°„ì„­ ì—†ì´ ê·¸ëŸ´ë“¯í•œ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì—¬ IIDì™€ RGBâ†”Xë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ìœ¼ë¡œ ê±°ì¹ ê¸°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (d) Roughness estimation. Our method outperforms IID and RGBâ†”â†”\leftrightarrowâ†”X with plausible results free of interference from texture patterns and lighting.
> </details>



![](https://arxiv.org/html/2412.12083/x8.png)

> ğŸ”¼ IDArb ëª¨ë¸ì€ í•©ì„± ë°ì´í„°ì—ì„œ ë‹¤ë¥¸ ë°©ë²•ë“¤ê³¼ ë¹„êµí•˜ì—¬ ìš°ìˆ˜í•œ ë‚´ì¬ì  ì¶”ì • ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê·¸ë¦¼ì€ albedo, normal, metallic, roughness ì¶”ì • ê²°ê³¼ë¥¼ IID, RGBâ†’X, IntrinsicAnything, GeoWizard ì™€ ê°™ì€ ê¸°ì¡´ ë°©ë²•ë“¤ê³¼ ë¹„êµí•˜ê³  ìˆìŠµë‹ˆë‹¤. IDArbëŠ” albedoì—ì„œ í•˜ì´ë¼ì´íŠ¸ì™€ ê·¸ë¦¼ìë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì œê±°í•˜ê³ , normalì—ì„œ ì •í™•í•œ ê¸°í•˜í•™ì  í˜•íƒœë¥¼ ì œê³µí•˜ë©°, metallicê³¼ roughnessì—ì„œ í…ìŠ¤ì²˜ íŒ¨í„´ ë° ì¡°ëª…ì˜ ê°„ì„­ì„ ì œê±°í•˜ì—¬ ì‚¬ì‹¤ì ì¸ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 4: Qualitative comparison on synthetic data. Â IDArb demonstrates superior intrinsic estimation compared to all other methods.
> </details>



![](https://arxiv.org/html/2412.12083/x9.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ ì‹¤ì œ ë°ì´í„°ì— ëŒ€í•œ IDArbì˜ ì •ì„±ì  ë¹„êµ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. IDArbì€ ì‹¤ì œ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ì˜ ì¼ë°˜í™”ë˜ì–´ ì •í™•í•˜ê³  ì„¤ë“ë ¥ ìˆëŠ” ë¶„í•´ëŠ¥ê³¼ ê³ ì£¼íŒŒ ë””í…Œì¼ì„ ì œê³µí•©ë‹ˆë‹¤. ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì…ë ¥ ì´ë¯¸ì§€, IntrinsicAnythingë¡œ ì˜ˆì¸¡í•œ ê²°ê³¼, IDArbìœ¼ë¡œ ì˜ˆì¸¡í•œ ì•Œë² ë„, ë…¸ë§, ë©”íƒˆë¦­, ëŸ¬í”„ë‹ˆìŠ¤ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. IDArbì€ IntrinsicAnythingë³´ë‹¤ ë” ë‚˜ì€ ë””í…Œì¼ê³¼ ì‚¬ì‹¤ì ì¸ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 5: Qualitative comparison on real-world data. Â IDArb generalizes well to real data, with accurate, convincing decompositions and high-frequency details.
> </details>



![](https://arxiv.org/html/2412.12083/x10.png)

> ğŸ”¼ (a) ì—¬ëŸ¬ ì…ë ¥ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ ìƒ˜í”Œì˜ ë‹¤ì¤‘ ë·° ì¼ê´€ì„± ì‹œê°ì  ë¹„êµì…ë‹ˆë‹¤. IDArbëŠ” í•™ìŠµ ê¸°ë°˜ ë°©ë²•(IntrinsicAnything)ê³¼ ë¹„êµí•˜ì—¬ ë‹¤ì¤‘ ë·° ì¼ê´€ì„±ì„ ë‹¬ì„±í•˜ê³  ìµœì í™” ê¸°ë°˜ ë°©ë²•ì„ í†µí•´ í•™ìŠµëœ ì‚¬ì „ì„ í†µí•´ ì¡°ëª… íš¨ê³¼ì—ì„œ ë‚´ì¬ì  êµ¬ì„± ìš”ì†Œë¥¼ ë” ì˜ ë¶„ë¦¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (a)
> </details>



![](https://arxiv.org/html/2412.12083/x11.png)

> ğŸ”¼ (b) ìµœì í™” ê¸°ë°˜ ë°©ë²•(NVDiffRecMC)ê³¼ í•™ìŠµ ê¸°ë°˜ ë°©ë²•(IntrinsicAnything)ì˜ ë‹¨ì ì„ ë³´ì—¬ì£¼ëŠ” ê·¸ë¦¼ì…ë‹ˆë‹¤. NVDiffRecMCëŠ” ì¡°ëª… íš¨ê³¼ê°€ ì¬ì§ˆì— ì˜ëª» ë°˜ì˜ë˜ì–´(ì˜ˆ: ê¸ˆì†ì„± ì˜¤ë¸Œì íŠ¸ì˜ ì–´ë‘ìš´ ìƒ‰ìƒ), IntrinsicAnythingëŠ” ë©€í‹° ë·° ì…ë ¥ì— ëŒ€í•´ ì¼ê´€ì„± ì—†ëŠ” ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ì— ë°˜í•´ IDArbëŠ” í•™ìŠµ ê¸°ë°˜ ë°©ì‹ìœ¼ë¡œ ë©€í‹° ë·° ì¼ê´€ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì¡°ëª… íš¨ê³¼ì™€ ì¬ì§ˆì„ ë” ì˜ ë¶„ë¦¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (b)
> </details>



![](https://arxiv.org/html/2412.12083/x12.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ êµì°¨ êµ¬ì„± ìš”ì†Œ ì£¼ì˜ ë° í›ˆë ¨ ì „ëµì— ëŒ€í•œ ì ˆì œ ì—°êµ¬ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. (a)ëŠ” êµì°¨ êµ¬ì„± ìš”ì†Œ ì£¼ì˜ê°€ ì—†ì„ ë•Œ ê¸ˆì† ë° ê±°ì¹ ê¸°ì™€ ê°™ì€ ë³¸ì§ˆì ì¸ êµ¬ì„± ìš”ì†Œì˜ ì˜ˆì¸¡ì´ ì €í•˜ë¨ì„ ë³´ì—¬ì£¼ë©°, ì´ëŠ” ì´ëŸ¬í•œ êµ¬ì„± ìš”ì†Œ ê°„ì˜ ìƒí˜¸ ì‘ìš©ì„ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. (b)ëŠ” ë‹¤ì¤‘ ë·° ì…ë ¥ê³¼ ë‹¨ì¼ ì´ë¯¸ì§€ ì…ë ¥ì„ ëª¨ë‘ ì‚¬ìš©í•œ í›ˆë ¨ ì „ëµì˜ íš¨ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ë‹¤ì¤‘ ë·° ì…ë ¥ë§Œ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í•˜ë©´ ë‹¨ì¼ ì´ë¯¸ì§€ ì…ë ¥ì— ëŒ€í•œ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ë°˜ë©´, ì œì•ˆëœ í›ˆë ¨ ì „ëµì€ ë‹¤ì–‘í•œ ì…ë ¥ ìœ í˜•ì— ëŒ€í•œ ê°•ë ¥í•œ ì¼ë°˜í™” ê¸°ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, ë†’ì€ ë…¸ì´ì¦ˆ ë ˆë²¨ë¡œ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì´ë™í•˜ë©´ ê¸ˆì† ë° ê±°ì¹ ê¸° êµ¬ì„± ìš”ì†Œì˜ ì˜ˆì¸¡ì´ í–¥ìƒë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 6: Ablative studies on (a) cross-component attention and (b) training strategy.
> </details>



![](https://arxiv.org/html/2412.12083/x13.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ ë‹¤ì–‘í•œ ìˆ˜ì˜ ë·°í¬ì¸íŠ¸ì™€ ì¡°ëª… ì¡°ê±´ì—ì„œ IDArb ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë·°í¬ì¸íŠ¸ ìˆ˜(#V)ì™€ ì¡°ëª… ì¡°ê±´ ìˆ˜(#L)ë¥¼ ë‹¤ì–‘í•˜ê²Œ ë³€ê²½í•˜ë©° ì‹¤í—˜í•œ ê²°ê³¼, ë·°í¬ì¸íŠ¸ì™€ ì¡°ëª… ì¡°ê±´ì˜ ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ì „ë°˜ì ì¸ ë¶„í•´ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ê¸ˆì†ì„± ë° ê±°ì¹ ê¸° ì˜ˆì¸¡ì˜ ê²½ìš°, ë‹¤ì¤‘ ì¡°ëª… ìº¡ì²˜ê°€ ì¡°ëª… íš¨ê³¼ë¡œ ì¸í•œ ëª¨í˜¸ì„±ì„ í•´ê²°í•˜ëŠ” ë° ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤. 8ê°œ ì´ìƒì˜ ë·°í¬ì¸íŠ¸ë¥¼ ì¶”ê°€í•˜ë©´ ì„±ëŠ¥ í–¥ìƒì´ ê°ì†Œí•˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤. xì¶•ì€ ë·°í¬ì¸íŠ¸ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ê³ , yì¶•ì€ ì•Œë² ë„, ë…¸ë©€, ë©”íƒˆë¦­, ëŸ¬í”„ë‹ˆìŠ¤ ê°ê°ì˜ ì„±ëŠ¥ ì§€í‘œ ê°’ì˜ ë³€í™”ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìƒ‰ìƒ ë³€í™”ë¥¼ í†µí•´ ë·°í¬ì¸íŠ¸ ìˆ˜ì™€ ì¡°ëª… ì¡°ê±´ ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 7: Effects of number of viewpoints and lighting conditions. We find increasing the number of viewpoints and the lighting conditions generally improves decomposition performance.
> </details>



![](https://arxiv.org/html/2412.12083/x14.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ ì‹¤ì œ í™˜ê²½ì—ì„œ ì´¬ì˜ëœ ì´ë¯¸ì§€(a)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ì¡°ëª… ì¡°ê±´ì—ì„œì˜ ë¦¬ë¼ì´íŒ… ê²°ê³¼(b)ì™€ ì¬ì§ˆ ì†ì„± ë³€ê²½ ê²°ê³¼(c)ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. IDArb ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ ì…ë ¥ ì´ë¯¸ì§€ì—ì„œ ì•Œë² ë„, ë…¸ë§, ë©”íƒˆë¦­, ëŸ¬í”„ë‹ˆìŠ¤ ë“±ì˜ ê³ ìœ  ìš”ì†Œë¥¼ ì¶”ì¶œí•˜ì—¬ ì¬ì§ˆ ë° ì¡°ëª… í¸ì§‘ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 8: Relighting and material editing results. From in-the-wild captures (a), our model allows for relighting under novel illumination (b) and material property modifications (c).
> </details>



![](https://arxiv.org/html/2412.12083/x15.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ ìµœì í™” ê¸°ë°˜ ì—­ë Œë”ë§ ê¸°ë²•ì¸ NVDiffRecMCì— ì €ìë“¤ì´ ì œì•ˆí•œ ë°©ë²•ì„ ì ìš©í•˜ì—¬ ì¬ì§ˆ ì¶”ì • ê²°ê³¼ë¥¼ í–¥ìƒì‹œí‚¨ ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì €ìë“¤ì˜ ë°©ë²•ì€ ê° í•™ìŠµ ì´ë¯¸ì§€ë¥¼ í•´ë‹¹í•˜ëŠ” ì¬ì§ˆ ìš”ì†Œë¡œ ë¶„í•´í•˜ê³ , ì´ë¥¼ pseudo-material labelë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§¤ ë°˜ë³µë§ˆë‹¤ NVDiffRecMCì—ì„œ ì˜ˆì¸¡í•œ ì¬ì§ˆ ìš”ì†Œì™€ ì €ìë“¤ì˜ ë°©ë²•ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê°’ ì‚¬ì´ì˜ L2 ì •ê·œí™” í•­ì„ ì¶”ê°€í•˜ì—¬ ë¬¼ë¦¬ì  íƒ€ë‹¹ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤. ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ì €ìë“¤ì˜ ë°©ë²•ì„ ì ìš©í•˜ë©´ NVDiffRecMCì—ì„œ ì¬êµ¬ì„±ëœ albedoì˜ ìƒ‰ìƒ ë³€í™” ë¬¸ì œê°€ í¬ê²Œ ì™„í™”ë˜ì–´, ë” ë‚˜ì€ í’ˆì§ˆì˜ ë Œë”ë§ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 9: Optimization-based inverse rendering results. Our method guides NVDiffecMC generate more plausible material results.
> </details>



![](https://arxiv.org/html/2412.12083/x16.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ OpenIllumination ë° NeRFactor ë°ì´í„°ì…‹ì—ì„œ 4ê°œì˜ OLAT(One-Light-At-a-Time) ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•œ ì‚¬ì§„ ì¸¡ëŸ‰ ìŠ¤í…Œë ˆì˜¤ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. OLAT ì¡°ê±´ì—ì„œëŠ” ê° ì´ë¯¸ì§€ê°€ ì£¼ë³€ ì¡°ê´‘ ì—†ì´ ë‹¨ì¼ ì  ê´‘ì›ìœ¼ë¡œ ì¡°ëª…ë˜ì–´ ê·¸ë¦¼ìê°€ ìƒê¹ë‹ˆë‹¤. ê·¸ë¦¼ì—ëŠ” ì…ë ¥ OLAT ì´ë¯¸ì§€, ì˜ˆì¸¡ëœ ì•Œë² ë„ ë° ë²•ì„  ë§µì´ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. IDArbì€ OLATì™€ ê°™ì€ ê¹Œë‹¤ë¡œìš´ ì¡°ê±´ì—ì„œë„ ì‹¤ì œ ë° í•©ì„± ë°ì´í„° ëª¨ë‘ì—ì„œ ì¢‹ì€ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 10: Photometric stereo results using 4 OLAT images in OpenIllumination and NeRFactor.
> </details>



![](https://arxiv.org/html/2412.12083/x17.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ ì‹¤ì œ ë°ì´í„°ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° í–‰ì€ ì…ë ¥ ì´ë¯¸ì§€ì™€ í•´ë‹¹ ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ ì•Œë² ë„, ë…¸ë©€, ë©”íƒˆë¦­, ëŸ¬í”„ë‹ˆìŠ¤ ë§µì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. IDArbì€ ë‹¤ì–‘í•œ ì‹¤ì œ ë¬¼ì²´ì— ëŒ€í•´ ì‚¬ì‹¤ì ì´ê³  ì„¸ë¶€ì ì¸ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŠ” IDArbì´ í•©ì„± ë°ì´í„°ë¡œ í›ˆë ¨ë˜ì—ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì‹¤ì œ ì´ë¯¸ì§€ì— ì˜ ì¼ë°˜í™”ë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 11: More results on real-world data.
> </details>



![](https://arxiv.org/html/2412.12083/x18.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ ì‹¤ì œ ë°ì´í„°ì— ëŒ€í•œ ì¶”ê°€ ê²°ê³¼ì™€ ì¬êµ¬ì„± ë° ì¬ì¡°ëª… ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì…ë ¥ ì´ë¯¸ì§€ì—ì„œ ì˜ˆì¸¡ëœ albedo, normal, metallic, roughnessë¥¼ ì‚¬ìš©í•˜ì—¬ ë Œë”ë§ëœ ì´ë¯¸ì§€(Recon)ì™€ ë‹¤ì–‘í•œ ì¡°ëª… ì¡°ê±´ì—ì„œ ì¬ì¡°ëª…ëœ ì´ë¯¸ì§€(Relit 1, 2, 3)ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜¤í† ë°”ì´, ìë™ì°¨, íŠ¸ëŸ¼í«, ë¹µê³¼ ì¼ ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ë¬¼ì²´ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì œì‹œí•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 12: More results on real-world data. We also provide the reconstructed and relighting images.
> </details>



![](https://arxiv.org/html/2412.12083/x19.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ ì—¬ëŸ¬ ì‹œì ì—ì„œ ì´¬ì˜ëœ ë°ì´í„°ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° í–‰ì€ ì„œë¡œ ë‹¤ë¥¸ ë‹¤ì¤‘ ì‹œì  ë°ì´í„°ì…‹ì„ ë‚˜íƒ€ë‚´ë©°, ì…ë ¥ ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì˜ˆì¸¡ëœ ì•Œë² ë„, ë…¸ë©€, ë©”íƒˆë¦­, ëŸ¬í”„ë‹ˆìŠ¤ ë§µì´ í‘œì‹œë©ë‹ˆë‹¤. ì²« ë²ˆì§¸ í–‰ì€ ë“œëŸ¼ ì„¸íŠ¸, ë‘ ë²ˆì§¸ í–‰ì€ ë‹¤ì–‘í•œ ìŒì‹ì´ ë‹´ê¸´ ì ‘ì‹œ, ì„¸ ë²ˆì§¸ í–‰ì€ ìƒŒë“œìœ„ì¹˜ì™€ í•«ë„ê·¸ê°€ ë‹´ê¸´ ì ‘ì‹œì…ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì„ í†µí•´ IDArb ëª¨ë¸ì´ ë‹¤ì–‘í•œ ë‹¤ì¤‘ ì‹œì  ë°ì´í„°ì—ì„œ ì¼ê´€ì„± ìˆëŠ” ë³¸ì§ˆì  ìš”ì†Œë¥¼ ì¶”ì¶œí•˜ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 13: More results on multi-view data.
> </details>



![](https://arxiv.org/html/2412.12083/x20.png)

> ğŸ”¼ NeRD ë°ì´í„°ì…‹(Boss ì™¸, 2021a)ì˜ ê° ì¥ë©´ì— ëŒ€í•´ 4ê°œì˜ ë·°ë¥¼ ì…ë ¥í•˜ì—¬ ê·¹ë‹¨ì ì¸ ì¡°ëª… ë³€í™”ê°€ ìˆëŠ” ë‹¤ì¤‘ ë·° ì´ë¯¸ì§€ì—ì„œ ë³¸ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ê° ë·°ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì¡°ëª… ì¡°ê±´ì—ì„œ ë Œë”ë§ë©ë‹ˆë‹¤. ì…ë ¥ ì´ë¯¸ì§€, ì•Œë² ë„, ë…¸ë©€, ë©”íƒˆë¦­, ëŸ¬í”„ë‹ˆìŠ¤ë¥¼ ì˜ˆì¸¡í•œ ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 14: Multiview images with extreme lighting variation. For each scene in NeRD datasetÂ (Boss etÂ al., 2021a), we input 4 views.
> </details>



![](https://arxiv.org/html/2412.12083/x21.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ IDArb ëª¨ë¸ì˜ ì‹¤íŒ¨ ì‚¬ë¡€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì²« ë²ˆì§¸ í–‰ì€ ì•¼ì™¸ ì¥ë©´ìœ¼ë¡œ, ëª¨ë¸ì´ ê°ì²´ ì¤‘ì‹¬ ë°ì´í„°ì— ëŒ€í•´ ì£¼ë¡œ í›ˆë ¨ë˜ì—ˆê¸° ë•Œë¬¸ì— ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ë‘ ë²ˆì§¸ í–‰ì€ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ì´ë¯¸ì§€ë¡œ, ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ í…ìŠ¤íŠ¸ êµ¬ì¡°ë¥¼ ë³µêµ¬í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì„¸ ë²ˆì§¸ í–‰ì€ ì „í™”ê¸° ì´ë¯¸ì§€ë¡œ, ëª¨ë¸ì´ ë¯¸ë¬˜í•œ ì¬ì§ˆ ë””í…Œì¼ì„ ë³´ì¡´í•˜ì§€ ëª»í•˜ê³  ì§€ë‚˜ì¹˜ê²Œ ë‹¨ìˆœí™”ëœ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œëŠ” í•©ì„± í›ˆë ¨ ë°ì´í„°ê°€ ì¢…ì¢… ë” ë‹¨ìˆœí•œ ì¬ì§ˆ ë³€í˜•ì„ í¬í•¨í•˜ê³  ìˆì–´ ëª¨ë¸ì´ ì„¸ë°€í•œ ì¬ì§ˆ ì†ì„±ì„ ê³¼ë„í•˜ê²Œ ë‹¨ìˆœí™”í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì—ì„œ ë¹„ë¡¯ë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 15: Failure cases.
> </details>



![](https://arxiv.org/html/2412.12083/x22.png)

> ğŸ”¼ Mip-NeRF 360 ë°ì´í„°ì…‹ì˜ ì•¼ì™¸ ì¥ë©´ì— ëŒ€í•œ IDArbì˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ì¥ë©´ì— ëŒ€í•´ 4ê°œì˜ ë·°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ê·¸ë¦¼ì—ëŠ” ì…ë ¥ ì´ë¯¸ì§€, ì˜ˆì¸¡ëœ ì•Œë² ë„, ë²•ì„ , ë©”íƒˆë¦­, ëŸ¬í”„ë‹ˆìŠ¤ ë§µì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. IDArbì€ ë‹¤ì–‘í•œ ì•¼ì™¸ ì¥ë©´ì—ì„œ ì¼ê´€ë˜ê³  ì •í™•í•œ ë‚´ì¬ì  ì´ë¯¸ì§€ ë¶„í•´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 16: Results on Mip-NeRF 360Â (Barron etÂ al., 2022) (Part 1, outdoor). We input 4 views for each scene.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| # OLAT Images | 2 | 2 | 4 | 4 | 8 | 8 |
|---|---|---|---|---|---|---| 
| Methods | Albedo\uparrow | Normal\uparrow | Albedo\uparrow | Normal\uparrow | Albedo\uparrow | Normal\uparrow |
| IID | 22.23 | - | 22.40 | - | 22.86 | - |
| RGB <->X | 21.29 | 0.71 | 22.08 | 0.77 | 23.29 | 0.81 |
| SDM-UniPS | 22.95 | 0.74 | 23.20 | 0.76 | 23.37 | 0.81 |
| Ours | **23.50** | **0.83** | **23.64** | **0.84** | **25.15** | **0.85** |{{< /table-caption >}}
> ğŸ”¼ NeRFactor ë°ì´í„°ì…‹ì—ì„œ Photometric Stereoì— ëŒ€í•œ ì •ëŸ‰ì  ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” í‘œì…ë‹ˆë‹¤. 2, 4, 8ê°œì˜ OLAT(One-Light-At-a-Time) ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í–ˆìœ¼ë©°, ì œì•ˆëœ ë°©ë²•(Ours)ì´ ë¹„êµëœ ëª¨ë“  ë°©ë²• ì¤‘ì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.  OLATì€ ê° ì´ë¯¸ì§€ê°€ ì£¼ë³€ê´‘ ì—†ì´ ë‹¨ì¼ ì  ê´‘ì›ìœ¼ë¡œë§Œ ë¹„ì¶°ì§€ëŠ” ê¹Œë‹¤ë¡œìš´ ì¡°ê±´ìœ¼ë¡œ, ê·¸ë¦¼ìë„ ê°•í•˜ê²Œ ë“œë¦¬ì›Œì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¡°ê±´ì—ì„œë„ ë³¸ ì—°êµ¬ì˜ ë°©ë²•ì€ ë‹¤ë¥¸ ë°©ë²•ë“¤ê³¼ ë¹„êµí•˜ì—¬ albedo ë° normal ì˜ˆì¸¡ ì •í™•ë„ê°€ ê°€ì¥ ë†’ì•˜ìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 2: Quantitative results for photometric stereo on NeRFactor. We evaluate performance using 2, 4, and 8 OLAT images, and achieve the best performance among all compared methods.
> </details>

{{< table-caption >}}
| | Nerfactor | | | Synthetic4Relight | | | |
|---|---|---|---|---|---|---|---|---| 
| | Albedo (raw) | Albedo (scaled) | Relighting | Albedo (raw) | Albedo (scaled) | Relighting | Roughness |
| NVDiffRecMC | 17.89 | 25.88 | 22.65 | 17.03 | 29.64 | 24.05 | 0.046 |
| NVDiffRecMC w/ Ours | **20.90** | **26.61** | **27.20** | **26.42** | **30.73** | **31.01** | **0.014** |{{< /table-caption >}}
> ğŸ”¼ IDArbë¥¼ pseudo labelë¡œ ì‚¬ìš©í•˜ì—¬ ìµœì í™” ê¸°ë°˜ ì—­ë Œë”ë§ ê¸°ë²•ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì‹¤í—˜ ê²°ê³¼ë¥¼ NeRFactor ë° Synthetic4Relight ë°ì´í„°ì…‹ì— ëŒ€í•´ ë‚˜íƒ€ë‚¸ í‘œì…ë‹ˆë‹¤. albedo, relighting, roughnessì— ëŒ€í•œ ì •ëŸ‰ì  í‰ê°€ ê²°ê³¼ë¥¼ IDArbë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²½ìš°ì™€ ë¹„êµí•˜ì—¬ ì œì‹œí•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 3: Ablation onÂ IDArb pseudo labels for optimization-based inverse rendering on NeRFactor and Synthetic4Relight datasets.
> </details>

{{< table-caption >}}
| # L | # V | 1 | 2 | 4 | 8 | 12 |
|---|---|---|---|---|---|---| 
| 1 | | 29.16 | 28.72 | 30.12 | 30.49 | 30.77 |
| 2 | | 29.96 | 30.26 | 30.96 | 31.13 | 31.26 |
| 3 | | 30.25 | 30.73 | 31.16 | 31.33 | 31.40 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ë‹¤ì–‘í•œ ìˆ˜ì˜ ë·°í¬ì¸íŠ¸(# V) ë° ì¡°ëª… ì¡°ê±´(# L)ì— ë”°ë¥¸ ì•Œë² ë„ ì„±ëŠ¥(PSNR, â†‘â†‘ëŠ” ê°’ì´ í´ìˆ˜ë¡ ì¢‹ìŒ)ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë·°í¬ì¸íŠ¸ ìˆ˜ì™€ ì¡°ëª… ì¡°ê±´ ìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì•Œë² ë„ ì¶”ì • ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 4: Albedo Performance â†‘â†‘\uparrowâ†‘ across different numbers of viewpoints (# V) and lightings (# L).
> </details>

{{< table-caption >}}
| # L | # V | 1 | 2 | 4 | 8 | 12 |
|---|---|---|---|---|---|---| 
| 1 |  | 0.909 | 0.910 | 0.925 | 0.930 | 0.932 |
| 2 |  | 0.922 | 0.927 | 0.930 | 0.933 | 0.934 |
| 3 |  | 0.926 | 0.931 | 0.931 | 0.934 | 0.935 |{{< /table-caption >}}
> ğŸ”¼ ë‹¤ì–‘í•œ ìˆ˜ì˜ ë·°í¬ì¸íŠ¸(# V)ì™€ ì¡°ëª… ì¡°ê±´(# L)ì— ë”°ë¥¸ ë²•ì„  ì˜ˆì¸¡ ì„±ëŠ¥(Cosine Similarity)ì„ ë³´ì—¬ì£¼ëŠ” í‘œì…ë‹ˆë‹¤. ë·°í¬ì¸íŠ¸ì™€ ì¡°ëª… ì¡°ê±´ ìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ë²•ì„  ì˜ˆì¸¡ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 5: Normal Performance â†‘â†‘\uparrowâ†‘ across different numbers of viewpoints (# V) and lightings (# L).
> </details>

{{< table-caption >}}
| # L | # V | 1 | 2 | 4 | 8 | 12 |
|---|---|---|---|---|---|---| 
| 1 |   | 0.105 | 0.116 | 0.068 | 0.059 | 0.050 |
| 2 |   | 0.061 | 0.068 | 0.047 | 0.044 | 0.042 |
| 3 |   | 0.061 | 0.056 | 0.048 | 0.045 | 0.040 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ë‹¤ì–‘í•œ ìˆ˜ì˜ ë·°í¬ì¸íŠ¸(# V)ì™€ ì¡°ëª… ì¡°ê±´(# L)ì— ëŒ€í•œ ê¸ˆì†ì„± ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ë·°í¬ì¸íŠ¸ ìˆ˜ì™€ ì¡°ëª… ì¡°ê±´ì´ ì¦ê°€í•¨ì— ë”°ë¼ ê¸ˆì†ì„± ì¶”ì • ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìˆ«ìê°€ ë‚®ì„ìˆ˜ë¡ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 6: Metallic Performance â†“â†“\downarrowâ†“ across different numbers of viewpoints (# V) and lightings (# L).
> </details>

{{< table-caption >}}
| # L | # V | 1 | 2 | 4 | 8 | 12 |
|---|---|---|---|---|---|---| 
| 1 | | 0.049 | 0.050 | 0.024 | 0.019 | 0.021 |
| 2 | | 0.043 | 0.026 | 0.019 | 0.016 | 0.015 |
| 3 | | 0.031 | 0.022 | 0.016 | 0.014 | 0.013 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ë‹¤ì–‘í•œ ìˆ˜ì˜ ë·°í¬ì¸íŠ¸(# V)ì™€ ì¡°ëª… ì¡°ê±´(# L)ì— ë”°ë¥¸ ê±°ì¹ ê¸° ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ë·°í¬ì¸íŠ¸ ìˆ˜ì™€ ì¡°ëª… ì¡°ê±´ì´ ì¦ê°€í•¨ì— ë”°ë¼ ê±°ì¹ ê¸° ì˜ˆì¸¡ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 7: Roughness Performance â†“â†“\downarrowâ†“ across different numbers of viewpoints (# V) and lightings (# L).
> </details>

{{< table-caption >}}
| | SSIMâ†‘ | PSNRâ†‘ | LPIPSâ†“ |
|---|---|---|---| 
| Ours | 0.876 | **27.98** | **0.117** |
| IntrinsicAnything | **0.896** | 25.66 | 0.150 |{{< /table-caption >}}
> ğŸ”¼ MIT-Intrinsic ë°ì´í„°ì…‹ì—ì„œ albedo ì˜ˆì¸¡ ì •í™•ë„ë¥¼ IntrinsicAnythingì™€ ë¹„êµí•œ í‘œì…ë‹ˆë‹¤. SSIM, PSNR, LPIPS ì²™ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€í–ˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 8: Quantitative comparisons on MIT-Intrinsic.
> </details>

{{< table-caption >}}
|                      | Normal Cosine Distanceâ†“ | Albedo SSIMâ†‘ | Albedo PSNRâ†‘ | Albedo LPIPSâ†“ | Re-rendering PSNR-Hâ†‘ | Re-rendering PSNR-Lâ†‘ | Re-rendering SSIMâ†‘ | Re-rendering LPIPSâ†“ |
|----------------------|-----------------------|-------------|-------------|------------|-------------------|-------------------|----------------|----------------|
| Ours(single)        | 0.041                 | 0.978      | 41.30      | 0.039       | 24.11              | 31.28              | 0.969           | 0.024           |
| Ours(multi)         | **0.029**            | **0.978**   | **41.46**   | **0.038**    | **24.36**           | **31.43**           | **0.970**        | **0.024**        |
| StableNormal        | **0.038**            |             |             |            |                   |                   |                |                |
| IntrinsicNeRF       |                       | **0.981**   | 39.31      | 0.048       |                   |                   |                |                |{{< /table-caption >}}
> ğŸ”¼ Stanford-ORB ë°ì´í„°ì…‹ì—ì„œì˜ ì •ëŸ‰ì  ë¹„êµ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” í‘œì…ë‹ˆë‹¤. ë‹¨ì¼ ì´ë¯¸ì§€ ì…ë ¥ê³¼ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì…ë ¥ì— ëŒ€í•œ ì €í¬ ëª¨ë¸(Ours)ì˜ ì„±ëŠ¥ì„ StableNormal ë° IntrinsicNeRFì™€ ë¹„êµí•©ë‹ˆë‹¤. ë…¸ë©€ ì¶”ì •, ì•Œë² ë„ ì¶”ì •, ê·¸ë¦¬ê³  ë¦¬ë Œë”ë§ ê²°ê³¼ì— ëŒ€í•œ í‰ê°€ ê²°ê³¼ë¥¼ í¬í•¨í•˜ë©°, ê° ë©”íŠ¸ë¦­ì— ëŒ€í•œ ìµœê³  ì„±ëŠ¥ì€ ë³¼ë“œì²´ë¡œ í‘œì‹œë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 9: Quantitative comparisons on Stanford-ORB.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}