{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model that significantly advanced the field of multimodal retrieval and is frequently used as a basis for many subsequent models."}, {"fullname_first_author": "Alberto Baldrati", "paper_title": "Zero-shot composed image retrieval with textual inversion", "publication_date": "2023-10-01", "reason": "This paper introduces a novel zero-shot approach to composed image retrieval that serves as a strong baseline and benchmark within the paper's experimental evaluations."}, {"fullname_first_author": "Ting Jiang", "paper_title": "E5-V: Universal embeddings with multimodal large language models", "publication_date": "2024-07-01", "reason": "This paper presents a competitive multimodal embedding model that uses LLaVA, which is directly compared against in the paper's experiments."}, {"fullname_first_author": "Ziyan Jiang", "paper_title": "VLM2Vec: Training vision-language models for massive multimodal embedding tasks", "publication_date": "2024-10-01", "reason": "This paper is highly relevant due to its introduction of a benchmark dataset and experimental setting (MMEB) that directly informs the experiments and results presented in this paper."}, {"fullname_first_author": "Junjie Zhou", "paper_title": "VISTA: Visualized text embedding for universal multi-modal retrieval", "publication_date": "2024-07-01", "reason": "This paper is highly relevant as it is a closely related work from the same research group, focusing on a similar topic and employing similar techniques for multimodal retrieval."}]}