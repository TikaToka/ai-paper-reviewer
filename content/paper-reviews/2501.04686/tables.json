[{"content": "| Model | #Params | MathVista ALL | MathVista GPS | MathVista MWP | MathVista FQA | MathVista TQA | MathVista VQA | MathVerse ALL | MathVerse TD | MathVerse TL | MathVerse TO | MathVerse VI | MathVerse VD | MathVerse VO |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| *Baselines* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Random | - | 17.9 | 21.6 | 3.8 | 18.2 | 19.6 | 26.3 | 12.4 | 12.4 | 12.4 | 12.4 | 12.4 | 12.4 | 12.4 |\n| Human | - | 60.3 | 48.4 | 73.0 | 59.7 | 63.2 | 55.9 | 64.9 | 71.2 | 70.9 | 41.7 | 61.4 | 68.3 | 66.7 |\n| *Closed-Source MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o (OpenAI, 2024) | - | 63.8 | - | - | - | - | - | - | - | - | - | - | - | - |\n| GPT-4V (OpenAI, 2023) | - | 49.9 | 50.5 | 57.5 | 43.1 | 65.2 | 38.0 | 54.4 | 63.1 | 56.6 | 60.3 | 51.4 | 32.8 | 50.3 |\n| Gemini-1.5-002-Flash (Team et al., 2023) | - | 58.4 | - | - | - | - | - | - | - | - | - | - | - | - |\n| Gemini-1.5-Pro (Team et al., 2023) | - | 63.9 | - | - | - | - | - | 35.3 | 39.8 | 34.7 | 44.5 | 32.0 | 36.8 | 33.3 |\n| Claude-3.5-Sonnet (Anthropic, 2024) | - | 67.7 | - | - | - | - | - | - | - | - | - | - | - |  |\n| Qwen-VL-Plus (Bai et al., 2023) | - | 43.3 | 35.5 | 31.2 | 54.6 | 48.1 | 51.4 | 21.3 | 26.0 | 21.2 | 25.2 | 18.5 | 19.1 | 21.8 |\n| *Open-Source General MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| mPLUG-Owl2-7B (Ye et al., 2023) | 7B | 22.2 | 23.6 | 10.2 | 22.7 | 27.2 | 27.9 | 10.3 | 11.6 | 11.4 | 13.8 | 11.1 | 9.4 | 8.0 |\n| MiniGPT4-7B (Zhu et al., 2023) | 7B | 23.1 | 26.0 | 13.4 | 18.6 | 30.4 | 30.2 | 12.2 | 12.3 | 12.9 | 13.4 | 12.5 | 14.8 | 8.7 |\n| LLaVA-1.5-13B (Liu et al., 2024b) | 13B | 27.7 | 22.7 | 18.9 | 23.8 | 43.0 | 30.2 | 12.7 | 17.1 | 12.0 | 22.6 | 12.6 | 12.7 | 9.0 |\n| SPHINX-V2-13B (Lin et al., 2023) | 13B | 36.7 | 16.4 | 23.1 | 54.6 | 41.8 | 43.0 | 16.1 | 20.8 | 14.1 | 14.0 | 16.4 | 15.6 | 16.2 |\n| LLaVA-NeXT-34B (Liu et al., 2024c) | 34B | 46.5 | - | - | - | - | - | 34.6 | 49.0 | 37.6 | 30.1 | 35.2 | 28.9 | 22.4 |\n| InternLM-XComposer2-VL (Dong et al., 2024a) | 7B | 57.6 | 63.0 | 73.7 | 55.0 | 56.3 | 39.7 | 25.9 | 36.9 | 28.3 | 42.5 | 20.1 | 24.4 | 19.8 |\n| Deepseek-VL (Lu et al., 2024) | 7B | 34.9 | 28.4 | 55.9 | 26.8 | 32.9 | 34.6 | 19.3 | 23.0 | 23.2 | 23.1 | 20.2 | 18.4 | 11.8 |\n| InternVL2-8B (Chen et al., 2024b) | 8B | 58.3 | 62.0 | 59.1 | 58.7 | 61.4 | 49.7 | 35.9 | 39.0 | 33.8 | 36.0 | 32.2 | 30.9 | 27.7 |\n| Qwen2-VL (Wang et al., 2024a) | 7B | 58.9 | 40.9 | 64.0 | 69.1 | 60.1 | 58.1 | 33.6 | 37.4 | 33.5 | 35.0 | 31.3 | 30.3 | 28.1 |\n| *Open-Source Math MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| G-LLaVA-7B (Gao et al., 2023) | 7B | 25.1 | 48.7 | 3.6 | 19.1 | 25.0 | 28.7 | 16.6 | 20.9 | 20.7 | 21.1 | 17.2 | 14.6 | 9.4 |\n| Math-LLaVA-13B (Shi et al., 2024) | 13B | 46.6 | 57.7 | 56.5 | 37.2 | 51.3 | 33.5 | 22.9 | 27.3 | 24.9 | 27.0 | 24.5 | 21.7 | 16.1 |\n| Math-PUMA-Qwen2-7B (Zhuang et al., 2024) | 7B | 47.9 | 48.1 | 68.3 | 46.5 | 46.2 | 30.2 | 33.6 | 42.1 | 35.0 | 39.8 | 33.4 | 31.6 | 26.0 |\n| Math-PUMA-DeepSeek-Math (Zhuang et al., 2024) | 7B | 44.7 | 39.9 | 67.7 | 42.8 | 42.4 | 31.3 | 31.8 | 43.4 | 35.4 | 47.5 | 33.6 | 31.6 | 14.7 |\n| MAVIS-7B (Zhang et al., 2024d) | 7B | - | 64.1 | - | - | - | - | 35.2 | 43.2 | 37.2 | - | 34.1 | 29.7 | 31.8 |\n| InfiMM-Math (Han et al., 2024) | 7B | - | - | - | - | - | - | 34.5 | 46.7 | 32.4 | - | 38.1 | 32.4 | 15.8 |\n| MultiMath-7B (Peng et al., 2024) | 7B | 50.0 | 66.8 | 61.8 | 40.1 | 50.0 | 33.0 | 27.7 | 34.8 | 30.8 | 35.3 | 28.1 | 25.9 | 15.0 |\n| URSA-7B | 7B | 59.8 | 79.3 | 75.3 | 44.6 | 63.9 | 40.2 | 45.7 | 55.3 | 48.3 | 51.8 | 46.4 | 43.9 | 28.6 |\n| \u0394 over SOTA *Open-Source Math MLLMs* | - | +9.8 | +12.5 | +7.0 | -1.9 | +12.6 | +6.7 | +10.5 | +8.6 | +11.1 | +4.3 | +8.3 | +11.5 | -3.2 |", "caption": "Table 1: Comparison with closed-source MLLMs and open-source MLLMs on MATHVISTA testmini and MATHVERSE testmini. The best is bold, and the runner-up is underline. The best results of Closed-source MLLMs are highlighted in green. The best and second-best results of Open-source MLLMs are highlighted in red and blue respectively.", "description": "\ud45c 1\uc740 MATHVISTA testmini \ubc0f MATHVERSE testmini\uc5d0 \ub300\ud55c closed-source \ubc0f open-source \ub2e4\uc911 \ubaa8\ub4dc \uc5b8\uc5b4 \ubaa8\ub378(MLLM)\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Closed-source MLLM\uc758 \uacbd\uc6b0 \ucd5c\uace0 \uc131\ub2a5\uc740 \ub179\uc0c9\uc73c\ub85c, open-source MLLM\uc758 \uacbd\uc6b0 \ucd5c\uace0 \ubc0f \ub450 \ubc88\uc9f8\ub85c \ub192\uc740 \uc131\ub2a5\uc740 \uac01\uac01 \ube68\uac04\uc0c9\uacfc \ud30c\ub780\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218\uc640 \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5(\uc608: \uae30\ud558 \ubb38\uc81c \ud574\uacb0, \uc218\ud559 \ub2e8\uc5b4 \ubb38\uc81c, \uc22b\uc790 \ucd94\ub860, \uc2dc\uac01\uc801 \uc9c8\ubb38 \uc751\ub2f5 \ub4f1)\uc5d0 \ub300\ud55c \uc131\ub2a5\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ucd5c\uace0 \uc131\ub2a5\uc740 \uad75\uac8c, \ub450 \ubc88\uc9f8\ub85c \ub192\uc740 \uc131\ub2a5\uc740 \ubc11\uc904\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4.", "section": "4. Experiment"}, {"content": "| Model | #Params | Strict AVG \u2191 | Strict IK \u2193 | Strict IG \u2193 | Strict CM \u2191 | Strict RM \u2193 | Loose AVG \u2191 | Loose IK \u2193 | Loose IG \u2193 | Loose CM \u2191 | Loose RM \u2193 |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n|  Qwen-VL-Max (Bai et al., 2023) | - | 10.5 | 65.1 | 7.6 | 6.7 | 75.5 | 25.5 | 65.1 | 7.6 | 21.7 | 20.3 |\n| Gemini-1.5-Pro (Team et al., 2023) | - | 26.4 | 42.9 | 11.2 | 20.8 | 54.8 | 46.0 | 42.9 | 11.2 | 40.4 | 12.0 |\n| GPT-4V (OpenAI, 2023) | - | 31.1 | 39.8 | 14.5 | 23.8 | 47.9 | 51.4 | 39.8 | 14.5 | 44.2 | 3.3 |\n| GPT-4o (OpenAI, 2024) | - | 42.9 | 31.2 | 15.2 | 35.2 | 34.2 | 60.6 | 31.2 | 15.2 | 53.0 | 1.1 |\n| _Closed-source MLLMs_ |  |  |  |  |  |  |  |  |  |  |  |\n| LLaVA-1.6 (Liu et al., 2024c) | 7B | 3.3 | 78.3 | 2.5 | 2.1 | 89.1 | 13.8 | 78.3 | 2.5 | 12.6 | 34.7 |\n| LLaVA-1.6 (Liu et al., 2024c) | 13B | 5.2 | 69.1 | 3.2 | 3.6 | 86.9 | 22.0 | 69.1 | 3.2 | 20.4 | 26.2 |\n| InternVL-Chat-V1.5 (Chen et al., 2024a) | 26B | 12.7 | 56.4 | 10.5 | 7.4 | 77.6 | 31.0 | 56.4 | 10.5 | 25.7 | 22.4 |\n| LLaVA-NeXT (Liu et al., 2024c) | 72B | 13.4 | 58.9 | 7.1 | 9.9 | 71.0 | 31.5 | 58.9 | 7.1 | 28.0 | 17.9 |\n| DeepSeek-VL (Lu et al., 2024) | 7B | 6.3 | 69.1 | 4.6 | 4.0 | 84.8 | 21.0 | 69.1 | 4.6 | 18.7 | 29.0 |\n| Phi3-Vision (Abdin et al., 2024) | 4.2B | 10.6 | 58.9 | 9.0 | 6.1 | 81.1 | 29.8 | 58.9 | 9.0 | 25.3 | 21.3 |\n| GLM-4V-9B (GLM et al., 2024) | 9B | 14.9 | 53.0 | 9.5 | 10.1 | 73.1 | 35.1 | 53.0 | 9.5 | 30.3 | 19.3 |\n| InternLM-XComposer2-VL (Dong et al., 2024a) | 7B | 12.7 | 56.4 | 10.5 | 7.4 | 77.6 | 31.0 | 56.4 | 10.5 | 25.7 | 22.4 |\n| InternVL2-8B (Chen et al., 2024b) | 8B | 26.6 | 45.5 | 13.5 | 19.8 | 51.6 | 44.9 | 45.5 | 13.5 | 38.1 | 7.0 |\n| Qwen2-VL (Wang et al., 2024a) | 7B | 25.6 | 47.1 | 14.7 | 18.3 | 52.2 | 43.0 | 47.1 | 14.7 | 35.6 | 7.0 |\n| _Open-source General MLLMs_ |  |  |  |  |  |  |  |  |  |  |  |\n| G-LLaVA (Gao et al., 2023) | 13B | 6.5 | 64.2 | 4.6 | 4.2 | 86.6 | 22.3 | 64.2 | 4.6 | 20.0 | 36.0 |\n| Math-LLaVA (Shi et al., 2024) | 13B | 11.1 | 62.1 | 3.6 | 9.3 | 72.8 | 31.3 | 62.1 | 3.6 | 29.5 | 13.9 |\n| Math-PUMA-Qwen2-7B (Zhuang et al., 2024) | 7B | 19.2 | 47.8 | 13.7 | 12.4 | 67.8 | 41.0 | 47.8 | 13.7 | 34.1 | 11.4 |\n| Math-PUMA-DeepSeek-Math-7B (Zhuang et al., 2024) | 7B | 15.6 | 56.0 | 7.2 | 12.0 | 67.4 | 35.8 | 56.0 | 7.2 | 32.2 | 12.4 |\n| InfiMM-Math (Han et al., 2024) | 7B | 20.6 | 48.8 | 12.2 | 15.2 | 61.7 | - | - | - | - | - |\n| URSA-7B | 7B | 32.2 | 37.5 | 10.7 | 26.9 | 48.2 | 53.5 | 37.5 | 10.7 | 48.2 | 7.0 |\n| _Open-source Math MLLMs_ |  |  |  |  |  |  |  |  |  |  |  |\n| \u0394 over SOTA Open-Source Math MLLMs | - | +11.6 | +10.3 | -7.1 | +11.7 | +13.5 | +12.5 | +10.3 | -7.1 | +14.1 | +4.4 |", "caption": "Table 2: The performance comparison with Closed-source MLLMs and Open-source MLLMs on four-dimensional metrics for WE-MATH testmini reasoning evaluation. The best results of Closed-source MLLMs are highlighted in green. The best and second-best results of Open-source MLLMs are highlighted in red and blue respectively.", "description": "\ubcf8 \ud45c\ub294 WE-MATH \ud14c\uc2a4\ud2b8 \uc790\ub8cc\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud3d0\uc1c4\ud615 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uacfc \uac1c\ubc29\ud615 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud3c9\uac00\ub294 WE-MATH\uc5d0\uc11c \uc815\uc758\ub41c \ub124 \uac00\uc9c0 \ucc28\uc6d0(\ud3c9\uade0 \uc815\ud655\ub3c4, \ubd88\uc644\uc804\ud55c \uc9c0\uc2dd, \ubd80\uc801\uc808\ud55c \uc77c\ubc18\ud654, \uc644\uc804\ud55c \uc219\ub2ec)\uc744 \uae30\ubc18\uc73c\ub85c \uc774\ub8e8\uc5b4\uc84c\uc2b5\ub2c8\ub2e4.  \ud3d0\uc1c4\ud615 LLM\uc758 \ucd5c\uace0 \uc131\ub2a5\uc740 \ub179\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5c8\uace0, \uac1c\ubc29\ud615 LLM\uc758 \ucd5c\uace0 \ubc0f \ucc28\uc21c\uc704 \uc131\ub2a5\uc740 \uac01\uac01 \ube68\uac04\uc0c9\uacfc \ud30c\ub780\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \uba85\ud655\ud788 \ud30c\uc545\ud558\uace0, \ub2e4\ucc28\uc6d0\uc801 \ud3c9\uac00 \uc9c0\ud45c\ub97c \ud65c\uc6a9\ud55c \ubaa8\ub378 \uc131\ub2a5 \ubd84\uc11d\uc758 \uc911\uc694\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experiment"}, {"content": "| Model | ALL | PG | SG | AG | AL | PT | GT | AR |\n|---|---|---|---|---|---|---|---|---|\n| *Closed-source MLLMs* |  |  |  |  |  |  |  |  |\n| GPT-4o | 63.7 | 56.8 | 52.0 | 61.0 | 76.9 | 51.8 | 58.1 | 61.5 |\n| Claude-3.5-Sonnet | 64.8 | 49.9 | 49.3 | 55.3 | 81.0 | 44.1 | 69.4 | 61.2 |\n| Geimini-1.5-Pro | 60.5 | 52.7 | 42.7 | 61.6 | 70.8 | 20.6 | 65.2 | 54.2 |\n| *Open-source MLLMs* |  |  |  |  |  |  |  |  |\n| Llava-v1.5-7B | 16.6 | 10.5 | 7.3 | 19.5 | 6.5 | 8.2 | 32.3 | 10.8 |\n| Llava-v1.6-34B | 27.1 | 21.4 | 25.3 | 27.6 | 14.9 | 7.6 | 32.7 | 23.1 |\n| Deepseek-VL-7B-chat | 21.5 | 16.0 | 13.3 | 26.5 | 12.9 | 4.7 | 32.3 | 12.7 |\n| InternVL2-8B | 39.7 | 33.9 | 37.3 | 32.5 | 46.9 | 15.9 | 42.1 | 37.3 |\n| Qwen2-VL | 42.1 | 40.3 | 38.7 | 39.9 | 37.1 | 8.2 | 44.8 | 39.2 |\n| URSA-7B | 44.7 | 48.1 | 38.0 | 33.7 | 66.9 | 24.7 | 39.2 | 38.5 |", "caption": "Table 3: Comparison with open-source MLLMs on DYNAMATH testmini dataset.", "description": "\ubcf8 \ud45c\ub294 DYNAMATH \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \uc624\ud508 \uc18c\uc2a4 \ub2e4\uc911 \ubaa8\ub4dc \uc5b8\uc5b4 \ubaa8\ub378(MLLM)\uacfc\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc804\uccb4 \uc815\ud655\ub3c4\uc640 \uae30\ud558\ud559(PG), \uc785\uccb4\uae30\ud558(SG), \ub300\uc218(AL), \ud37c\uc990(PT), \uadf8\ub798\ud504 \uc774\ub860(GT), \uc0b0\uc220(AR), \uacfc\ud559 \uadf8\ub9bc(SF), \ud1b5\uacc4(ST)\uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uc218\ud559\uc801 \ubb38\uc81c \uc720\ud615\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 URSA-7B \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810, \uadf8\ub9ac\uace0 \ub2e4\ub978 \uc624\ud508 \uc18c\uc2a4 \ubaa8\ub378\ub4e4\uacfc\uc758 \uc0c1\ub300\uc801 \uc131\ub2a5\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"content": "| Method | N=4 | N=8 | N=16 | N=32 | N=64 |\n|---|---|---|---|---|---| \n| *MathVista-GPS* |  |  |  |  |  |\n| Self-Consistency | 67.4 | 67.9 | 68.2 | 68.7 | 68.9 |\n| URSA-RM-7B | **68.8** | **69.7** | **70.4** | **70.7** | **70.8** |\n| *MathVerse* |  |  |  |  |  |\n| Self-Consistency | 29.1 | 29.7 | 30.1 | 30.2 | 30.2 |\n| URSA-RM-7B | **31.0** | **32.7** | **33.0** | **33.2** | **33.0** |", "caption": "Table 4: OOD performance when URSA-RM-7B works on Multimath-7B.", "description": "\ud45c 4\ub294 URSA-RM-7B \ubaa8\ub378\uc774 Multimath-7B \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc5bc\ub9c8\ub098 \uc798 \uc77c\ubc18\ud654(OOD, Out-of-Distribution)\ub418\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, MathVista\uc640 MathVerse \ub370\uc774\ud130\uc14b\uc758 GPS(Geometry Problem Solving) \uacfc\uc81c\uc5d0 \ub300\ud574 Self-Consistency \ubc29\ubc95\uacfc URSA-RM-7B\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec, URSA-RM-7B\uac00 \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \ud655\uc7a5(test-time scaling)\uc5d0 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\ub294 \uc11c\ub85c \ub2e4\ub978 \uc0d8\ud50c\ub9c1 \ud69f\uc218(N)\uc5d0 \ub530\ub978 \uc815\ud655\ub3c4(Accuracy)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"content": "| Method | N=4 | N=8 | N=16 | N=32 | N=64 |\n|---|---|---|---|---|---| \n| *MathVista-GPS* |  |  |  |  |  |\n| URSA-RM-7B | **82.6** | **84.0** | **85.0** | **86.4** | **86.2** |\n| URSA-RM-7B w/o \\mathcal{S}_{BEL} | 80.1 | 81.7 | 82.2 | 83.1 | 83.0 |\n| URSA-RM-7B w/o \\mathcal{S}_{MIE} | 81.8 | 83.3 | 84.1 | 85.6 | 85.6 |\n| *MathVerse* |  |  |  |  |  |\n| URSA-RM-7B | **53.2** | **54.2** | **54.7** | **55.0** | **54.8** |\n| URSA-RM-7B w/o \\mathcal{S}_{BEL} | 49.9 | 50.7 | 51.8 | 52.0 | 52.1 |\n| URSA-RM-7B w/o \\mathcal{S}_{MIE} | 52.8 | 53.7 | 53.8 | 53.9 | 53.8 |", "caption": "Table 5: Ablation study on URSA-RM-7B.", "description": "\ud45c 5\ub294 URSA-RM-7B \ubaa8\ub378\uc5d0 \ub300\ud55c ablation study \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  MathVista-GPS\uc640 MathVerse \ub450 \ub370\uc774\ud130\uc14b\uc5d0\uc11c  URSA-RM-7B \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc0d8\ud50c\ub9c1 \ud69f\uc218(N=4, 8, 16, 32, 64)\ub97c \ubcc0\uacbd\ud558\uba70 \uce21\uc815\ud558\uace0,  BinaryErrorLocating(SBEL)\uacfc Misinterpretation Insertion Engine(SMIE)\uc744 \uc81c\uac70\ud588\uc744 \ub54c\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uac01 \uc0d8\ud50c\ub9c1 \ud69f\uc218\uc5d0 \ub530\ub978 \uc815\ud655\ub3c4 \ubcc0\ud654\ub97c \ud1b5\ud574  SBEL\uacfc SMIE\uc758 \ud6a8\uacfc\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ud3c9\uac00\ud558\uace0 \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "5. Ablations"}, {"content": "| Hyperparameters & Cost | Stage 1 | Stage 2 | Stage 3 |\n|---|---|---|---| \n| Learning Rate | 1e-4 | 1e-5 | 5e-6 |\n| Epoch | 1 | 2 | 2 |\n| Warm-up Ratio | 0.02 | 0.02 | 0.02 |\n| Weight Decay | 0.02 | 0.01 | 0.02 |\n| Batch Size | 64 | 128 | 128 |\n| Trainable Parts | Aligner | Vision Encoder, Aligner, Base LLM | Base LLM |\n| Data Size | 960K | 1.0M | 1.1M |\n| Time Cost | ~3.5h | ~11h | ~12h |", "caption": "Table 6: Hyperparameter setting and training time cost.", "description": "\ud45c 6\uc740 URSA-7B \ubaa8\ub378 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815\uacfc \uac01 \ub2e8\uacc4\ubcc4 \ud559\uc2b5\uc5d0 \uc18c\uc694\ub41c \uc2dc\uac04\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc138 \ub2e8\uacc4(Stage 1, Stage 2, Stage 3)\uc758 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ud559\uc2b5\ub960, \uc5d0\ud3ec\ud06c \uc218, \uc6e8\uc774\ud2b8 \uac10\uc1e0, \ubc30\uce58 \ud06c\uae30, \ud559\uc2b5 \ub300\uc0c1 \ud30c\ub77c\ubbf8\ud130, \ub370\uc774\ud130 \ud06c\uae30, \ud559\uc2b5 \uc2dc\uac04 \ub4f1\uc758 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubaa8\ub378 \ud559\uc2b5 \uacfc\uc815\uc5d0 \ub300\ud55c \uc138\ubd80\uc801\uc778 \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\uc5ec, \uc5f0\uad6c\uc758 \uc7ac\ud604\uc131\uc744 \ub192\uc774\ub294 \ub370 \uae30\uc5ec\ud569\ub2c8\ub2e4.", "section": "3. Model Training Process"}, {"content": "| Model | #Params | ALL | ALG | ARI | GEO | LOG | NUM | SCI | STA |\n|---|---|---|---|---|---|---|---|---|---| \n| *Baselines* |  |  |  |  |  |  |  |  |  |\n| Random Choice | - | 17.9 | 25.8 | 13.8 | 22.7 | 13.4 | 8.8 | 15.8 | 14.3 |\n| Human Performance | - | 60.3 | 50.9 | 59.2 | 51.4 | 40.7 | 53.8 | 64.9 | 63.9 |\n| *Closed-source MLLMs* |  |  |  |  |  |  |  |  |  |\n| Qwen-VL-Plus [2023] | - | 43.3 | 39.1 | 32.0 | 39.3 | 18.9 | 26.4 | 59.0 | 56.1 |\n| GPT-4V [2023] | - | 49.9 | 53.0 | 49.0 | 51.0 | 21.6 | 20.1 | 63.1 | 55.8 |\n| *Open-source Genreral MLLMs* |  |  |  |  |  |  |  |  |  |\n| mPLUG-Owl2-7B [2023] | 7B | 22.2 | 23.6 | 19.2 | 23.9 | 13.5 | 12.7 | 26.3 | 21.4 |\n| LLaVA-1.5-13B [2024c] | 13B | 25.7 | 19.6 | 28.6 | 17.6 | 10.8 | 27.8 | 33.6 | 22.9 |\n| MiniGPT-v2 [2023] | 7B | 23.1 | 28.1 | 21.0 | 24.7 | 16.2 | 16.7 | 25.4 | 17.9 |\n| InternLM-XComposer2-VL-7B [2024a] | 7B | 47.8 | 32.0 | 51.6 | 30.5 | 13.5 | 43.8 | 37.7 | 62.8 |\n| SPHINX-MoE [2023] | 8x7B | 42.3 | 31.7 | 41.6 | 30.5 | 16.2 | 27.1 | 50.8 | 50.8 |\n| DeepSeek-VL [2024] | 7B | 34.9 | 29.2 | 38.8 | 27.2 | 18.9 | 43.1 | 35.3 | 33.2 |\n| InternVL2-8B [2024b] | 8B | 58.3 | 59.8 | 56.4 | 60.3 | 10.8 | 30.6 | 59.0 | 68.8 |\n| Qwen2-VL [2024a] | 7B | 58.9 | 44.1 | 57.5 | 43.1 | 24.3 | 41.7 | 66.4 | 75.1 |\n| *Open-source Math MLLMs* |  |  |  |  |  |  |  |  |  |\n| G-LLaVA [2024] | 7B | 25.1 | 36.0 | 19.4 | 37.6 | 15.2 | 17.7 | 21.0 | 15.1 |\n| Math-LLaVA [2024] | 7B | 46.6 | 51.5 | 40.7 | 56.2 | 23.3 | 34.7 | 47.7 | 42.3 |\n| Multimath-7B [2024] | 7B | 50.0 | 61.9 | 42.2 | 64.9 | 23.3 | 32.6 | 42.6 | 49.2 |\n| Math-PUMA-Qwen2-7B [2024] | 7B | 47.9 | 47.7 | 46.2 | 47.3 | 21.6 | 32.6 | 42.6 | 55.8 |\n| URSA-7B | 7B | 59.8 | 74.0 | 53.5 | 77.4 | 21.6 | 35.4 | 58.2 | 57.1 |\n| \u0394 over SOTA *Open-Source Math MLLMs* | - | +9.8 | +12.1 | +7.3 | +12.5 | -1.7 | +0.7 | +10.5 | +1.3 |", "caption": "Table 7: Comparison with close-source MLLMs open-source MLLMs on MATHVISTA testmini mathematics capabilities.", "description": "\ud45c 7\uc740 MATHVISTA \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b\uc744 \uae30\ubc18\uc73c\ub85c \ud3d0\uc1c4\ud615 \ubc0f \uac1c\ubc29\ud615 \ub2e4\uc911\ubaa8\ub4dc \uc5b8\uc5b4 \ubaa8\ub378(MLLM)\uc758 \uc218\ud559\uc801 \ub2a5\ub825\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  'Close-source MLLMs' \uc640 'Open-source General MLLMs', 'Open-source Math MLLMs' \uadf8\ub8f9\uc73c\ub85c \ub098\ub258\uc5b4 \uac01 \uadf8\ub8f9 \ub0b4 \uc5ec\ub7ec \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \uc218\ud559\uc801 \ud558\uc704 \uacfc\uc81c(ALL, ALG, ARI, GEO, LOG, NUM, SCI, STA)\ubcc4\ub85c \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  URSA-7B \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \uc5bc\ub9c8\ub098 \uc6b0\uc218\ud55c\uc9c0, \ud2b9\ud788 \uac1c\ubc29\ud615 \uc218\ud559 MLLM\ub4e4 \uc911 \ucd5c\uace0 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec \uc5bc\ub9c8\ub098 \uc55e\uc11c\ub294\uc9c0\ub97c \uba85\ud655\ud788 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"content": "| Model | #Params | S1 | S2 | S3 | Mem UCU | Mem AL | PF CPF | PF UPF | SF CSF | SF USF | TMF BTF | TMF CCF | PD Dir | PD Pos | PD RoM | PD CCP |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| *Closed-source MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o (OpenAI, 2024) | - | 72.8 | 58.1 | 43.6 | 86.6 | 39.1 | 77.4 | 71.6 | 84.5 | 62.3 | 58.7 | 69.4 | 93.1 | 72.7 | 47.5 | 73.3 |\n| GPT-4V (OpenAI, 2023) | - | 65.5 | 49.2 | 38.2 | 82.5 | 38.4 | 70.7 | 60.2 | 76.6 | 56.3 | 57.8 | 67.7 | 79.3 | 57.5 | 47.8 | 63.3 |\n| Gemini-1.5-Pro (Team et al., 2023) | - | 56.1 | 51.4 | 33.9 | 51.0 | 31.2 | 61.8 | 45.0 | 70.0 | 57.5 | 39.2 | 62.7 | 68.8 | 54.1 | 40.7 | 60.0 |\n| Qwen-VL-Max (Bai et al., 2023) | - | 40.8 | 30.3 | 20.6 | 19.4 | 25.3 | 39.8 | 41.4 | 43.6 | 48.0 | 43.8 | 43.4 | 41.4 | 35.1 | 40.7 | 26.7 |\n| *Open-source General MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| InternVL-Chat-V1.5 (Chen et al., 2024a) | 26B | 49.4 | 30.6 | 28.5 | 44.0 | 29.8 | 52.2 | 52.1 | 44.2 | 48.2 | 47.1 | 65.7 | 50.5 | 36.5 | 36.7 |\n| LLaVA-1.6 (Liu et al., 2024c) | 7B | 23.0 | 20.8 | 15.8 | 18.5 | 20.5 | 16.9 | 29.6 | 15.6 | 18.6 | 42.7 | 24.1 | 17.6 | 43.3 | 28.9 | 26.7 |\n| LLaVA-1.6 (Liu et al., 2024c) | 13B | 29.4 | 25.3 | 32.7 | 21.7 | 23.2 | 23.4 | 34.7 | 25.3 | 26.4 | 37.5 | 41.7 | 26.9 | 28.9 | 37.1 | 30.0 |\n| GLM-4V-9B (GLM et al., 2024) | 9B | 47.3 | 37.2 | 38.2 | 53.4 | 37.0 | 51.3 | 46.5 | 50.6 | 38.2 | 44.1 | 45.2 | 41.0 | 49.3 | 36.8 | 53.3 |\n| MiniCPM-LLaMA3-V2.5 (Yao et al., 2024) | 8B | 39.8 | 31.1 | 29.7 | 28.6 | 37.0 | 40.8 | 39.8 | 41.0 | 38.6 | 32.0 | 42.7 | 41.0 | 42.7 | 44.0 | 43.3 |\n| LongVA (Zhang et al., 2024c) | 7B | 43.5 | 30.6 | 28.5 | 24.5 | 39.8 | 45.1 | 40.8 | 51.9 | 42.5 | 45.6 | 44.6 | 44.5 | 40.7 | 47.5 | 20.0 |\n| InternLM-XComposer2-VL (Dong et al., 2024a) | 7B | 47.0 | 33.1 | 33.3 | 31.3 | 46.5 | 47.7 | 42.6 | 51.4 | 43.9 | 41.1 | 50.6 | 65.5 | 53.9 | 55.2 | 40.0 |\n| Phi3-Vision (Abdin et al., 2024) | 4.2B | 42.1 | 34.2 | 27.9 | 28.7 | 16.0 | 47.2 | 38.8 | 50.0 | 44.4 | 28.8 | 31.2 | 48.6 | 49.2 | 26.4 | 50.0 |\n| DeepSeek-VL (Lu et al., 2024) | 7B | 32.6 | 26.7 | 25.5 | 16.6 | 35.1 | 27.3 | 38.0 | 24.2 | 38.7 | 50.0 | 23.3 | 24.5 | 41.0 | 51.7 | 23.3 |\n| InternVL2-8B (Chen et al., 2024b) | 8B | 59.4 | 43.6 | 35.2 | 71.4 | 20.5 | 62.0 | 55.5 | 67.1 | 57.3 | 54.0 | 60.5 | 58.6 | 63.6 | 44.5 | 50.0 |\n| Qwen2-VL (Wang et al., 2024a) | 7B | 59.1 | 43.6 | 26.7 | 62.7 | 37.2 | 62.6 | 60.8 | 65.7 | 49.2 | 52.5 | 49.2 | 48.1 | 68.2 | 55.0 | 56.7 |\n| *Open-source Math MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| G-LLaVA (Gao et al., 2023) | 7B | 32.4 | 30.6 | 32.7 | 33.3 | 29.1 | 32.0 | 37.9 | 19.6 | 33.5 | 37.1 | 32.8 | 31.2 | 33.2 | 25.6 | 40.0 |\n| Math-LLaVA (Shi et al., 2024) | 13B | 38.7 | 34.2 | 34.6 | 30.3 | 17.9 | 39.2 | 40.4 | 37.1 | 37.7 | 53.0 | 51.3 | 30.8 | 30.8 | 40.9 | 46.7 |\n| Math-PUMA-Qwen2-7B (Zhuang et al., 2024) | 7B | 53.3 | 39.4 | 36.4 | 63.5 | 42.5 | 60.2 | 45.9 | 66.2 | 48.6 | 42.3 | 53.5 | 31.2 | 37.7 | 40.4 | 46.7 |\n| MAVIS w/o DPO (Zhang et al., 2024d) | 7B | 56.9 | 37.1 | 33.2 | - | - | - | - | - | - | - | - | - | - | - | - |\n| MAVIS (Zhang et al., 2024d) | 7B | 57.2 | 37.9 | 34.6 | - | - | - | - | - | - | - | - | - | - | - | - |\n| URSA-7B | 7B | 63.1 | 56.4 | 41.8 | 59.1 | 32.5 | 72.3 | 60.3 | 70.9 | 66.0 | 51.4 | 59.8 | 58.3 | 39.5 | 58.8 | 53.3 |\n| \u0394 over SOTA *Open-Source Math MLLMs* | - | +5.9 | +27.0 | +5.4 | -4.4 | -10.0 | +12.3 | +14.4 | +4.7 | +17.4 | -1.6 | +6.3 | +27.1 | +1.8 | +17.9 | +6.6 |", "caption": "Table 8: Accuracy comparison with close-source MLLMs and open-source MLLMs on WE-MATH testmini subset. First 3 columns show the overall performance on one-step, two-step and three-step problems. The other columns are used to demonstrate the performance in different problem strategies.", "description": "\ud45c 8\uc740 WE-MATH \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b\uc758 \ud558\uc704 \uc9d1\ud569\uc5d0 \ub300\ud574 \ud074\ub85c\uc988\uc18c\uc2a4 \ubc0f \uc624\ud508\uc18c\uc2a4 MLLM\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uc55e\uc758 \uc138 \uc5f4\uc740 \uac01\uac01 1\ub2e8\uacc4, 2\ub2e8\uacc4, 3\ub2e8\uacc4 \ubb38\uc81c\uc5d0 \ub300\ud55c \uc804\ubc18\uc801\uc778 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub098\uba38\uc9c0 \uc5f4\ub4e4\uc740 \uc11c\ub85c \ub2e4\ub978 \ubb38\uc81c \ud574\uacb0 \uc804\ub7b5\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud574 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ubb38\uc81c \uc720\ud615\uacfc \ub09c\uc774\ub3c4\uc5d0 \ub530\ub978 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc790\uc138\ud788 \ubd84\uc11d\ud558\uc5ec, \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ud30c\uc545\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "4.1. \uc2e4\ud5d8 \uc124\uc815"}, {"content": "| Model | #Params | ALL | Elementary School | High School | Undergraduate |\n|---|---|---|---|---|---| \n| _Closed-source MLLMs_ |  |  |  |  |  |\n| GPT-4o (OpenAI, 2024) | - | 63.7 | 68.6 | 61.8 | 36.8 |\n| Claude-3.5-Sonnet (Anthropic, 2024) | - | 64.8 | 66.7 | 62.6 | 33.3 |\n| Gemini-1.5-Pro (Team et al., 2023) | - | 60.5 | 62.9 | 59.2 | 37.1 |\n| _Open-sourced MLLMs_ |  |  |  |  |  |\n| Llava-v1.5-7B (Liu et al., 2024c) | 7B | 16.6 | 18.9 | 13.3 | 11.7 |\n| Llava-v1.6-34B (Liu et al., 2024c) | 34B | 27.1 | 35.9 | 23.8 | 16.6 |\n| Deepseek-VL-7B-Chat (Lu et al., 2024) | 7B | 21.5 | 28.3 | 19.0 | 16.0 |\n| InternVL2-8B (Chen et al., 2024b) | 8B | 39.7 | 51.1 | 37.4 | 19.6 |\n| Qwen2-VL (Wang et al., 2024a) | 7B | 42.1 | 47.6 | 42.2 | 24.4 |\n| URSA-7B | 7B | 44.7 | 53.5 | 44.3 | 41.8 |", "caption": "Table 9: Comparison with close-source MLLMs open-source MLLMs on DYNAMATH testmini based on knowledge level.", "description": "\ud45c 9\ub294 DYNAMATH \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b\uc744 \uae30\ubc18\uc73c\ub85c, \uc9c0\uc2dd \uc218\uc900(\ucd08\ub4f1, \uc911\ub4f1, \uace0\ub4f1)\ubcc4\ub85c \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  \uadfc\uc811 \ube44\uad50 \ub300\uc0c1\uc73c\ub85c\ub294 \ud074\ub85c\uc988\uc18c\uc2a4 \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378\uacfc \uc624\ud508\uc18c\uc2a4 \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \uc9c0\uc2dd \uc218\uc900\uc5d0\uc11c\uc758 \uc815\ud655\ub3c4\ub97c \uc885\ud569\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1. \uc2e4\ud5d8 \uc124\uc815"}, {"content": "| Model | Size | Accuracy |\n|---|---|---|\n| **_Baselines_** |  |  |\n| Random Choice | - | 17.1 |\n| Human | - | 92.3 |\n| UniMath (Liang et al., 2023) | - | 50.0 |\n| **_Closed-source MLLMs_** |  |  |\n| GPT-4V (OpenAI, 2023) | - | 45.2 |\n| **_Open-source MLLMs_** |  |  |\n| LLaVA-1.5 (Liu et al., 2024b) | 13B | 20.3 |\n| G-LLaVA (Gao et al., 2023) | 7B | 64.2 |\n| G-LLaVA (Gao et al., 2023) | 13B | 67.0 |\n| Math-PUMA-DeepSeek-Math-7B (Zhuang et al., 2024) | 7B | 61.8 |\n| Math-PUMA-Qwen2-7B (Zhuang et al., 2024) | 7B | 63.6 |\n| Multimath (Peng et al., 2024) | 7B | 67.7 |\n| MAVIS-7B w/o DPO (Zhang et al., 2024d) | 7B | 66.7 |\n| MAVIS-7B (Zhang et al., 2024d) | 7B | 68.3 |\n| URSA-7B | 7B | 73.5 |\n| \u0394 over SOTA _Open-Source MLLMs_ | - | +5.2 |", "caption": "Table 10: Performance comparison of different MLLMs on GeoQA.", "description": "\ud45c 10\uc740 GeoQA \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub2e4\uc591\ud55c \ub2e4\uc911 \ubaa8\ub2ec \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(MLLM)\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  GeoQA\ub294 \uae30\ud558\ud559 \ubb38\uc81c \ud574\uacb0 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \ubca4\uce58\ub9c8\ud06c\uc774\uba70, \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c MLLM\uc758 \uc815\ud655\ub3c4\uac00 \ubaa8\ub378 \ud06c\uae30\ubcc4\ub85c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c MLLM\uc758 \uae30\ud558\ud559\uc801 \ucd94\ub860 \ub2a5\ub825\uc744 \uc0c1\ub300\uc801\uc73c\ub85c \ube44\uad50\ud558\uace0, URSA-7B \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\ub978 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec \uadf8 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experiment"}]