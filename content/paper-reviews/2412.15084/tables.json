[{"content": "| Models | HumanEval | MBPP | GSM8K | MATH | MMLU | MMLU Pro | Avg. |\n|---|---|---|---|---|---|---|---|\n| DeepSeek-Coder-7B-Instruct-v1.5 | 64.10 | 64.60 | 72.60 | 34.10 | 49.50 | - | - |\n| DeepSeek-Coder-7B-Base + Two-Stage SFT (Ours) | 78.05 | 73.54 | 82.56 | 55.62 | 54.65 | 33.28 | 62.95 |\n| Llama3.1-8B-Instruct | 72.60 | 69.60 | 84.50 | 51.90 | 69.40 | 48.30 | 66.05 |\n| Llama3.1-8B-Base + Two-Stage SFT (Ours) | 81.10 | 74.71 | 90.45 | 64.42 | 68.31 | 43.27 | 70.38 |\n| Qwen2.5-1.5B-Instruct | 61.60 | 63.20 | 73.20 | 55.20 | 58.37 | 32.40 | 57.33 |\n| Qwen2.5-1.5B-Base + Two-Stage SFT (Ours) | 73.17 | 65.76 | 80.44 | 60.34 | 58.17 | 33.78 | 61.94 |\n| Qwen2.5-7B-Instruct | 84.80 | 79.20 | 91.60 | 75.50 | 74.51 | 56.30 | 76.99 |\n| Qwen2.5-7B-Base + Two-Stage SFT (Ours) | 85.37 | 74.32 | 93.10 | 76.40 | 74.68 | 54.50 | 76.40 |\n| Qwen2.5-72B-Instruct | 86.60 | 88.20 | 95.80 | 83.10 | 84.67 | 71.10 | 84.91 |\n| Qwen2.5-72B-Base + Two-Stage SFT (Ours) | 89.63 | 83.66 | 96.36 | 84.50 | 83.88 | 66.10 | 84.02 |", "caption": "Table 1: Results of our general SFT models. We apply our proposed two-stage training strategy to conduct SFT on various base models. These finetuned models are then compared against the corresponding instruct baselines that are built upon the same base models.", "description": "\ud45c 1\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 2\ub2e8\uacc4 \ud559\uc2b5 \uc804\ub7b5\uc744 \ub2e4\uc591\ud55c \uae30\ubcf8 \ubaa8\ub378\uc5d0 \uc801\uc6a9\ud558\uc5ec \uc218\ud589\ud55c \uc9c0\ub3c4 \ud559\uc2b5 \ubbf8\uc138 \uc870\uc815(SFT) \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc5ec\ub7ec \uae30\ubcf8 \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud574 \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\ub4e4\uc744 \ud574\ub2f9 \uae30\ubcf8 \ubaa8\ub378\ub4e4\uacfc \ub3d9\uc77c\ud55c \uae30\ubcf8 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub9cc\ub4e4\uc5b4\uc9c4 \uc9c0\uc2dc\uc5b4 \uae30\ubc18 \uae30\uc900 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. \uc989,  \uc81c\uc548\ub41c 2\ub2e8\uacc4 SFT \uc804\ub7b5\uc758 \ud6a8\uacfc\ub97c \ub2e4\uc591\ud55c \uaddc\ubaa8\uc758 \ubaa8\ub378\ub4e4\uc5d0\uc11c \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.", "section": "3. Supervised Fine-tuning"}, {"content": "| Models | HumanEval | MBPP | GSM8K | MATH | MMLU | MMLU Pro | Avg. |\n|---|---|---|---|---|---|---|---| \n| Llama3.1-8B-Base + Two-Stage SFT | **81.10** | **74.71** | **90.45** | **64.42** | **68.31** | **43.27** | **70.38** |\n| Llama3.1-8B-Base + Single-Stage SFT w/ all general SFT data | 78.66 | 69.26 | 87.79 | 56.80 | 67.62 | 42.64 | 67.13 |\n| Llama3.1-8B-Base + Single-Stage SFT w/ only stage-2 data | 73.78 | 67.32 | 88.17 | 55.84 | 67.48 | 42.85 | 65.91 |\n| Qwen2.5-7B-Base + Two-Stage SFT | **85.37** | 74.32 | **93.10** | **76.40** | **74.68** | **54.50** | **76.40** |\n| Qwen2.5-7B-Base + Single-Stage SFT w/ all general SFT data | 83.54 | **75.49** | 91.96 | 75.04 | 73.96 | 53.36 | 75.56 |\n| Qwen2.5-7B-Base + Single-Stage SFT w/ only stage-2 data | 83.54 | 73.15 | 92.27 | 75.12 | 74.26 | 53.06 | 75.23 |", "caption": "Table 2: Ablation studies of our general SFT models regarding the effectiveness of the two-stage training strategy.", "description": "\ud45c 2\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 2\ub2e8\uacc4 \uc77c\ubc18 SFT(Supervised Fine-Tuning) \uc804\ub7b5\uc758 \ud6a8\uacfc\ub97c \ud3c9\uac00\ud558\uae30 \uc704\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uae30\ubcf8 \ubaa8\ub378(Llama3.1-8B, Qwen2.5-7B)\uc5d0 \ub300\ud574 2\ub2e8\uacc4 SFT \uc804\ub7b5\uc744 \uc801\uc6a9\ud558\uace0, \ub2e8\uc77c \ub2e8\uacc4 SFT \uc804\ub7b5 (\uc804\uccb4 \uc77c\ubc18 SFT \ub370\uc774\ud130 \uc0ac\uc6a9, 2\ub2e8\uacc4 \ub370\uc774\ud130\ub9cc \uc0ac\uc6a9)\uacfc \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uac01 SFT \uc804\ub7b5\uc758 \uc131\ub2a5\uc744 HumanEval, MBPP, GSM8K, MATH, MMLU, MMLU Pro \ub4f1 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ud3c9\uac00\ud558\uc5ec 2\ub2e8\uacc4 \uc804\ub7b5\uc758 \ud6a8\uacfc\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "3. Supervised Fine-tuning"}, {"content": "| Models | GSM8K | MATH | Minerva Math | GaoKao 2023 En | Olympiad Bench | College Math | MMLU STEM | Avg. |\n|---|---|---|---|---|---|---|---|---|\n| GPT-4o (2024-0806) | 92.90 | 81.10 | 50.74 | 67.50 | 43.30 | 48.50 | **87.99** | 67.43 |\n| Claude-3.5 Sonnet (2024-1022) | 96.40 | 75.90 | 48.16 | 64.94 | 37.93 | 48.47 | 85.06 | 65.27 |\n| Llama3.1-70B-Instruct | 94.10 | 65.70 | 34.20 | 54.00 | 27.70 | 42.50 | 80.40 | 56.94 |\n| Llama3.1-405B-Instruct | **96.80** | 73.80 | 54.04 | 62.08 | 34.81 | 49.25 | 83.10 | 64.84 |\n| OpenMath2-Llama3.1-8B | 91.70 | 67.80 | 16.91 | 53.76 | 28.00 | 46.13 | 46.02 | 50.08 |\n| Qwen2.5-Math-1.5B-Instruct | 84.80 | 75.80 | 29.40 | 65.50 | 38.10 | 47.70 | 57.50 | 56.97 |\n| Qwen2.5-Math-7B-Instruct | 95.20 | 83.60 | 37.10 | 66.80 | 41.60 | 46.80 | 71.90 | 63.29 |\n| Qwen2.5-Math-72B-Instruct | 95.90 | 85.90 | 44.10 | 71.90 | **49.00** | 49.50 | 80.80 | 68.16 |\n| AceMath-1.5B-Instruct (Ours) | 86.95 | 76.84 | 41.54 | 64.42 | 33.78 | 54.36 | 62.04 | 59.99 |\n| AceMath-7B-Instruct (Ours) | 93.71 | 83.14 | 51.11 | 68.05 | 42.22 | 56.64 | 75.32 | 67.17 |\n| AceMath-72B-Instruct (Ours) | 96.44 | **86.10** | **56.99** | **72.21** | 48.44 | **57.24** | 85.44 | **71.84** |", "caption": "Table 3: Greedy decoding (pass@1) results of math instruct models on math benchmarks. Our AceMath-1.5B/7B/72B-Instruct models are built upon the Qwen2.5-Math-1.5B/7B/72B-base models. AceMath-72B-Instruct greatly surpasses the previous state-of-the-art math-instruct model, Qwen2.5-Math-72B-Instruct.", "description": "\ud45c 3\uc740 \ub2e4\uc591\ud55c \uc218\ud559 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc218\ud559 \uad00\ub828 \uc9c0\uc2dc \ubaa8\ub378\uc758 \ud0d0\uc695\uc801 \ub514\ucf54\ub529(pass@1) \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. AceMath-1.5B/7B/72B-Instruct \ubaa8\ub378\uc740 Qwen2.5-Math-1.5B/7B/72B \uae30\ubcf8 \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c \uad6c\ucd95\ub418\uc5c8\uc73c\uba70, \ud2b9\ud788 AceMath-72B-Instruct \ubaa8\ub378\uc740 \uc774\uc804 \ucd5c\ucca8\ub2e8 \uc218\ud559 \uc9c0\uc2dc \ubaa8\ub378\uc778 Qwen2.5-Math-72B-Instruct \ubaa8\ub378\uc744 \ud06c\uac8c \ub2a5\uac00\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \ubaa8\ub378\ub4e4(1.5B, 7B, 72B)\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \ubaa8\ub378 \ud06c\uae30\uac00 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc740 \uc5ec\ub7ec \uc218\ud559 \ubca4\uce58\ub9c8\ud06c(GSM8K, MATH, Minerva Math, Gaokao 2023 English, Olympiad Bench, College Math, MMLU STEM)\uc5d0\uc11c \ud3c9\uac00\ub418\uc5c8\uc73c\uba70, pass@1 \uc815\ud655\ub3c4\uac00 \uc81c\uc2dc\ub429\ub2c8\ub2e4.", "section": "3.6 AceMath-Instruct \uacb0\uacfc"}, {"content": "|Minerva|\n|---|---| \n|Math|", "caption": "Table 4: Ablation Studies on training data and strategies across various backbone models for training our AceMath-Instruct models. The ablation studies can be categorized into three parts: 1) evaluating the effectiveness of using either GPT-4o-mini responses or Qwen2.5-Math-72B-Instruct responses individually; 2) analyzing the effectiveness of different math-specific samples for math SFT; and 3) assessing the impact of conducting general SFT prior to math SFT.", "description": "\ud45c 4\ub294 AceMath-Instruct \ubaa8\ub378 \ud559\uc2b5\uc744 \uc704\ud55c \ub2e4\uc591\ud55c \uae30\ubcf8 \ubaa8\ub378\uc5d0 \ub300\ud55c \ud559\uc2b5 \ub370\uc774\ud130 \ubc0f \uc804\ub7b5\uc5d0 \ub300\ud55c \ucd94\uac00 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc138 \uac00\uc9c0 \uce21\uba74\uc73c\ub85c \ub098\ub258\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4. \uccab\uc9f8, GPT-40-mini \uc751\ub2f5\uacfc Qwen-2.5-Math-72B-Instruct \uc751\ub2f5\uc744 \uac1c\ubcc4\uc801\uc73c\ub85c \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \ud6a8\uacfc\ub97c \ud3c9\uac00\ud569\ub2c8\ub2e4. \ub458\uc9f8, \uc218\ud559 SFT\ub97c \uc704\ud55c \ub2e4\uc591\ud55c \uc218\ud559 \ud2b9\uc815 \uc0d8\ud50c\uc758 \ud6a8\uacfc\ub97c \ubd84\uc11d\ud569\ub2c8\ub2e4. \uc14b\uc9f8, \uc218\ud559 SFT \uc804\uc5d0 \uc77c\ubc18\uc801\uc778 SFT\ub97c \uc218\ud589\ud588\uc744 \ub54c\uc758 \uc601\ud5a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "3. Supervised Fine-tuning"}, {"content": "| GaoKao |\n|---|---| \n| 2023 En |", "caption": "Table 5: Ablation studies on the synthetic data, exploring the effects of removing all synthetic math SFT data and incorporating additional low-quality synthetic math SFT data. The backbone of AceMath-Instruct is Qwen2.5-7B-Base. Results are average across the seven math benchmark.", "description": "\uc774 \ud45c\ub294 AceMath-Instruct \ubaa8\ub378\uc5d0 \ub300\ud55c \uc5d0\uc774\ube14\ub808\uc774\uc158 \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Qwen2.5-7B-Base \ubaa8\ub378\uc744 \ubc31\ubcf8\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec, \ud569\uc131\ub41c \uc218\ud559 SFT \ub370\uc774\ud130\ub97c \uc81c\uac70\ud558\uac70\ub098 \uc800\ud488\uc9c8\uc758 \ud569\uc131 \ub370\uc774\ud130\ub97c \ucd94\uac00\ud588\uc744 \ub54c\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  7\uac1c\uc758 \uc218\ud559 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc810\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud569\uc131 \ub370\uc774\ud130\uc758 \ud488\uc9c8\uacfc \uc591\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\uae30 \uc704\ud55c \uc2e4\ud5d8 \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "3.6.4. ABLATION STUDIES ON SYNTHETIC DATA"}, {"content": "| Olympiad | Bench |\n|---|---|", "caption": "Table 6: Reward model evaluation on AceMath-RewardBench. The average results (rm@8) of reward models on math benchmarks, randomly sample 8 responses from 64 candidates with 100 random seeds. Response candidates are generated from a pool of 8 LLMs (Qwen{2/2.5}-Math-{7/72}B-Instruct, Llama-3.1-{8/70}B-Instruct, Mathtral-7B-v0.1, deepseek-math-7b-instruct).", "description": "\ud45c 6\uc740 AceMath-RewardBench\uc5d0 \ub300\ud55c reward model \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  7\uac1c\uc758 \uc218\ud559 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud574 \ud3c9\uac00\ud588\uc73c\uba70, \uac01 \ubb38\uc81c\ub9c8\ub2e4 8\uac1c\uc758 LLM (Qwen{2/2.5}-Math-{7/72}B-Instruct, Llama-3.1-{8/70}B-Instruct, Mathtral-7B-v0.1, deepseek-math-7b-instruct)\uc5d0\uc11c \uc0dd\uc131\ub41c 64\uac1c\uc758 \uc751\ub2f5 \ud6c4\ubcf4 \uc911 8\uac1c\ub97c \ubb34\uc791\uc704\ub85c \uc120\ud0dd\ud558\uc5ec rm@8 \uc9c0\ud45c\ub97c \uacc4\uc0b0\ud588\uc2b5\ub2c8\ub2e4. \uc774 \uacfc\uc815\uc744 100\ubc88 \ubc18\ubcf5\ud558\uc5ec \ud3c9\uade0 rm@8 \uc810\uc218\ub97c \uc0b0\ucd9c\ud588\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub378\uc758 \ud3c9\uade0 rm@8 \uc815\ud655\ub3c4\uc640 7\uac1c \ubca4\uce58\ub9c8\ud06c\ubcc4 \uc131\ub2a5\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.3 Reward Evaluation Benchmarks"}, {"content": "| College |\n|---|---| \n| Math |", "caption": "Table 7: The accuracy of reward models on RewardBench (MATH500)\u00a0(Lambert et\u00a0al., 2024) and RewardMATH\u00a0(Kim et\u00a0al., 2024). \u2020\u2020\\dagger\u2020: Results are copied from RewardMATH. Bold: top-1. Underline: top-2 accuracy.", "description": "\ud45c 7\uc740 Lambert et al.(2024)\uc758 RewardBench(MATH500)\uc640 Kim et al.(2024)\uc758 RewardMATH \ub450 \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub2e4\uc591\ud55c \ubcf4\uc0c1 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. RewardBench\ub294 MATH \ub370\uc774\ud130\uc14b\uc758 500\uac1c \ubb38\uc81c\ub97c \uc0ac\uc6a9\ud558\uba70, \ud558\ub098\uc758 \uc815\ub2f5\uacfc \ud558\ub098\uc758 \uc624\ub2f5 \ud6c4\ubcf4\ub97c \uc81c\uacf5\ud558\ub294 \ubc18\uba74, RewardMATH\ub294 \ud558\ub098\uc758 \uc815\ub2f5(GPT-4\uac00 \ub2e4\uc2dc \uc791\uc131)\uacfc 9\uac1c\uc758 \uc624\ub2f5 \ud6c4\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.  \ud45c\ub294 \uac01 \ubaa8\ub378\uc758 MATH500\uacfc RewardMATH\uc5d0\uc11c\uc758 \uc815\ud655\ub3c4(rm@8 \uc9c0\ud45c \uc0ac\uc6a9)\ub97c \ubcf4\uc5ec\uc8fc\uba70, \ucd5c\uace0 \uc815\ud655\ub3c4(top-1)\uc640 \ub450 \ubc88\uc9f8\ub85c \ub192\uc740 \uc815\ud655\ub3c4(top-2)\ub97c \uad75\uc740 \uae00\uc528\uc640 \ubc11\uc904\ub85c \ud45c\uc2dc\ud558\uc5ec \uc2dc\uac01\uc801 \uc774\ud574\ub97c \ub3d5\uc2b5\ub2c8\ub2e4.  RewardMATH\uc758 \uacb0\uacfc\ub294 \ud574\ub2f9 \ub17c\ubb38\uc5d0\uc11c \uac00\uc838\uc628 \uac83\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubcf4\uc0c1 \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \uc2a4\ud0c0\uc77c\uacfc \ubcf5\uc7a1\uc131\uc758 \uc218\ud559 \ubb38\uc81c\uc5d0 \uc5b4\ub5bb\uac8c \ubc18\uc751\ud558\ub294\uc9c0 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "4.3 Reward Evaluation Benchmarks"}, {"content": "| MMLU |\n|---|---| \n| STEM |", "caption": "Table 8: Ablation study of AceMath-7/72B-RM on AceMath-RewardBench (Backbone: AceMath-7/72B-Instruct; Data: reward score-sorted sampling; Loss: listwise Bradley-Terry.", "description": "\ud45c 8\uc740 AceMath-RewardBench\ub97c \uc0ac\uc6a9\ud558\uc5ec AceMath-7/72B-RM\uc5d0 \ub300\ud55c \ucd94\uac00 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\ubcf8 \ubaa8\ub378(Backbone)\uc740 AceMath-7/72B-Instruct\uc774\uba70, \ub370\uc774\ud130\ub294 \ubcf4\uc0c1 \uc810\uc218\ub97c \uae30\ubc18\uc73c\ub85c \uc815\ub82c\ub41c \uc0d8\ud50c\ub9c1(reward score-sorted sampling) \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud588\uace0, \uc190\uc2e4 \ud568\uc218(Loss)\ub294 listwise Bradley-Terry \ud568\uc218\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubcf4\uc0c1 \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc694\uc18c\ub4e4(\uc608: \uae30\ubcf8 \ubaa8\ub378, \ub370\uc774\ud130 \uc0d8\ud50c\ub9c1 \ubc29\ubc95, \uc190\uc2e4 \ud568\uc218)\uc744 \uac01\uac01 \ubcc0\ud654\uc2dc\ucf1c\uac00\uba70 \uc2e4\ud5d8\ud55c \uacb0\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uc5b4\ub5a4 \uc694\uc18c\uac00 \uac00\uc7a5 \ud070 \uc601\ud5a5\uc744 \uc8fc\ub294\uc9c0 \ud30c\uc545\ud558\uace0, \ubcf4\uc0c1 \ubaa8\ub378\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud560 \uc218 \uc788\ub294 \ucd5c\uc801\uc758 \uc124\uc815\uc744 \ucc3e\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "4. Reward Model Training"}, {"content": "| Models | GSM8K | MATH | Minerva | GaoKao | Olympiad | College | MMLU | Avg. |\n|---|---|---|---|---|---|---|---|---|\n| Backbone: Llama3.1-8B-Base |  |  |  |  |  |  |  |  |\n| AceMath-Instruct | 91.51 | 69.06 | 31.99 | 59.74 | 32.00 | 49.08 | 67.94 | 57.33 |\n| \u25b7 Only Qwen2.5-Math-72B-Instruct | 91.13 | 69.66 | 33.82 | 60.26 | 30.37 | 49.86 | 66.21 | 57.33 |\n| \u25b7 Only GPT-4o-mini | 90.83 | 68.12 | 36.03 | 60.26 | 31.70 | 48.05 | 66.50 | **57.36** |\n| \u25b7 Skipping general SFT | 91.81 | 68.70 | 31.99 | 59.48 | 31.11 | 48.40 | 62.76 | 56.32 |\n| Backbone: Qwen2.5-7B-Base |  |  |  |  |  |  |  |  |\n| AceMath-Instruct | 93.56 | 77.10 | 43.38 | 65.19 | 37.78 | 54.90 | 77.41 | **64.19** |\n| \u25b7 Only Qwen2.5-Math-72B-Instruct | 92.80 | 76.96 | 41.91 | 63.64 | 38.07 | 54.93 | 75.64 | 63.42 |\n| \u25b7 Only GPT-4o-mini | 91.66 | 74.14 | 43.75 | 64.42 | 39.26 | 52.27 | 76.03 | 63.08 |\n| \u25b7 Math SFT with all math samples | 93.40 | 77.12 | 42.28 | 65.19 | 37.78 | 54.05 | 75.33 | 63.59 |\n| \u25b7 Math SFT with only cross-checked samples | 92.72 | 76.76 | 41.54 | 65.97 | 36.74 | 54.33 | 76.78 | 63.55 |\n| \u25b7 Skipping general SFT | 93.03 | 77.52 | 40.44 | 62.86 | 37.19 | 54.58 | 75.77 | 63.06 |\n| Backbone: Qwen2.5-Math-72B-Base |  |  |  |  |  |  |  |  |\n| AceMath-Instruct | 96.44 | 86.10 | 56.99 | 72.21 | 48.44 | 57.24 | 85.44 | **71.84** |\n| \u25b7 Math SFT with all math samples | 96.29 | 86.06 | 55.15 | 70.13 | 46.67 | 57.49 | 84.96 | 70.96 |\n| \u25b7 Skipping general SFT | 95.75 | 85.52 | 56.25 | 71.43 | 45.33 | 56.71 | 84.42 | 70.77 |", "caption": "Table 9: Greedy decoding results of AceMath-Instruct on AIME 2024 and AMC 2023.", "description": "\ud45c 9\ub294 AceMath-Instruct \ubaa8\ub378\uc758 AIME 2024 \ubc0f AMC 2023 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc815\ub2f5\ub960\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Greedy decoding \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4.  AIME 2024\ub294 \uace0\ub09c\uc774\ub3c4 \uc218\ud559 \ubb38\uc81c\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uace0, AMC 2023\uc740 \uc0c1\ub300\uc801\uc73c\ub85c \uc26c\uc6b4 \ubb38\uc81c\ub4e4\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uae30 \ub54c\ubb38\uc5d0, \ub450 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ube44\uad50\ud568\uc73c\ub85c\uc368 AceMath-Instruct \ubaa8\ub378\uc758 \ub2e4\uc591\ud55c \ub09c\uc774\ub3c4\uc758 \ubb38\uc81c\uc5d0 \ub300\ud55c \ud574\uacb0 \ub2a5\ub825\uc744 \ud3c9\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.6 AceMath-Instruct \uacb0\uacfc"}, {"content": "|Minerva|\n|---|---| \n|Math|", "caption": "Table 10: Greedy decoding results of AceMath-Instruct across different backbone models.", "description": "\ud45c 10\uc740 \ub2e4\uc591\ud55c \ubc31\ubcf8 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec AceMath-Instruct \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  AceMath-Instruct \ubaa8\ub378\uc740 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 Qwen2.5, Llama 3.1, DeepSeek-Coder \ubc31\ubcf8 \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ub418\uc5c8\uc73c\uba70, GSM8K, MATH, Minerva Math, Gaokao 2023 English, Olympiad Bench, College Math, MMLU STEM \ub4f1 \ub2e4\uc591\ud55c \uc218\ud559 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc131\ub2a5\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \ubc31\ubcf8 \ubaa8\ub378\uc758 \uc885\ub958\uc640 \ud06c\uae30, \uadf8\ub9ac\uace0 AceMath-Instruct \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uc5b4\ub5a4 \ubc31\ubcf8 \ubaa8\ub378\uc774 AceMath-Instruct \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \uac00\uc7a5 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \ud655\uc778\ud558\uace0,  \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \ubaa8\ub378\uc5d0\uc11c AceMath-Instruct \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3.6 AceMath-Instruct \uacb0\uacfc"}]