[{"figure_path": "https://arxiv.org/html/2501.19399/x1.png", "caption": "Figure 1: \nComparison of Softmax and SSMax, illustrating the issue of attention fading and the effectiveness of SSMax in preventing it. As the input vector size increases, the maximum value of the output vector produced by Softmax decreases, demonstrating the problem of attention fading. In contrast, SSMax keeps the maximum value close to 1, regardless of the input size. The input vector consists of -2 for all elements except the last, which is set to +3. The scaling parameter s\ud835\udc60sitalic_s of SSMax is set to 0.43.", "description": "\uadf8\ub9bc 1\uc740 Softmax\uc640 SSMax\uc758 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc785\ub825 \ubca1\ud130\uc758 \ud06c\uae30\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c Softmax\uc758 \ucd9c\ub825 \ubca1\ud130 \ucd5c\ub300\uac12\uc740 \uac10\uc18c\ud558\ub294 \ubc18\uba74, SSMax\ub294 \uc785\ub825 \ud06c\uae30\uc5d0 \uad00\uacc4\uc5c6\uc774 \ucd5c\ub300\uac12\uc744 1\uc5d0 \uac00\uae5d\uac8c \uc720\uc9c0\ud569\ub2c8\ub2e4. \uc774\ub294 Softmax\uc5d0\uc11c \ubc1c\uc0dd\ud558\ub294 \uc5b4\ud150\uc158 \uac10\uc18c \ud604\uc0c1(attention fading)\uc744 SSMax\uac00 \ud6a8\uacfc\uc801\uc73c\ub85c \ubc29\uc9c0\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc785\ub825 \ubca1\ud130\ub294 \ub9c8\uc9c0\ub9c9 \uc694\uc18c\ub97c \uc81c\uc678\ud558\uace0\ub294 \ubaa8\ub4e0 \uc694\uc18c\uac00 -2\ub85c \uad6c\uc131\ub418\uba70, \ub9c8\uc9c0\ub9c9 \uc694\uc18c\ub294 +3\uc73c\ub85c \uc124\uc815\ub429\ub2c8\ub2e4. SSMax\uc758 \uc2a4\ucf00\uc77c\ub9c1 \ub9e4\uac1c\ubcc0\uc218 s\ub294 0.43\uc73c\ub85c \uc124\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "2. Scalable-Softmax (SSMax)"}, {"figure_path": "https://arxiv.org/html/2501.19399/extracted/6169187/fit.png", "caption": "Figure 2: \nRelationship between pnsubscript\ud835\udc5d\ud835\udc5bp_{n}italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and the input vector size n\ud835\udc5bnitalic_n. The red dots represent the learned values of pnsubscript\ud835\udc5d\ud835\udc5bp_{n}italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT after training, and the blue curve is a fitted logarithmic function of the form pn\u2248a1\u2062log\u2061n+a2subscript\ud835\udc5d\ud835\udc5bsubscript\ud835\udc4e1\ud835\udc5bsubscript\ud835\udc4e2p_{n}\\approx a_{1}\\log n+a_{2}italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT \u2248 italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT roman_log italic_n + italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. This result suggests that pnsubscript\ud835\udc5d\ud835\udc5bp_{n}italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT depends logarithmically on n\ud835\udc5bnitalic_n, motivating the reformulation of Softmax in Equation\u00a04.", "description": "\uadf8\ub9bc 2\ub294 \ud559\uc2b5\ub41c \ub9e4\uac1c\ubcc0\uc218 p<sub>n</sub>\uacfc \uc785\ub825 \ubca1\ud130 \ud06c\uae30 n \uc0ac\uc774\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube68\uac04 \uc810\uc740 \ud559\uc2b5 \ud6c4 p<sub>n</sub>\uc758 \ud559\uc2b5\ub41c \uac12\uc744 \ub098\ud0c0\ub0b4\uace0, \ud30c\ub780\uc0c9 \uace1\uc120\uc740 p<sub>n</sub> \u2248 a<sub>1</sub>logn + a<sub>2</sub> \ud615\ud0dc\uc758 \uc801\ud569\ub41c \ub85c\uadf8 \ud568\uc218\uc785\ub2c8\ub2e4. \uc774 \uacb0\uacfc\ub294 p<sub>n</sub>\uc774 n\uc5d0 \ub530\ub77c \ub85c\uadf8\uc801\uc73c\ub85c \uc758\uc874\ud55c\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud558\uba70, \uc774\ub294 \uc2dd (4)\uc5d0\uc11c Softmax\ub97c \uc7ac\uad6c\uc131\ud558\ub294 \ub3d9\uae30\ub97c \ubd80\uc5ec\ud569\ub2c8\ub2e4.  \uc989, \uc785\ub825 \ubca1\ud130\uc758 \ud06c\uae30\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c Softmax \ud568\uc218\uc758 \ucd5c\ub300\uac12\uc774 \uac10\uc18c\ud558\ub294 \ud604\uc0c1(attention fading)\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574 Softmax \ud568\uc218\ub97c \uc218\uc815\ud574\uc57c \ud568\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4.", "section": "2. Scalable-Softmax (SSMax)"}, {"figure_path": "https://arxiv.org/html/2501.19399/extracted/6169187/concept.png", "caption": "Figure 3: \nAn example illustrating the behavior of Softmax and SSMax for an input vector of size n\ud835\udc5bnitalic_n given by (0,1n\u22122,2n\u22122,\u2026,n\u22121n\u22122,1,zmax)01\ud835\udc5b22\ud835\udc5b2\u2026\ud835\udc5b1\ud835\udc5b21subscript\ud835\udc67max(0,\\frac{1}{n-2},\\frac{2}{n-2},\\dots,\\frac{n-1}{n-2},1,z_{\\mathrm{max}})( 0 , divide start_ARG 1 end_ARG start_ARG italic_n - 2 end_ARG , divide start_ARG 2 end_ARG start_ARG italic_n - 2 end_ARG , \u2026 , divide start_ARG italic_n - 1 end_ARG start_ARG italic_n - 2 end_ARG , 1 , italic_z start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ). The horizontal axis represents the value of zmaxsubscript\ud835\udc67maxz_{\\mathrm{max}}italic_z start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT, while the vertical axis represents its transformed value. The red and orange lines correspond to SSMax with different scaling parameters s\ud835\udc60sitalic_s, and the blue lines correspond to Softmax, with line styles indicating different input vector sizes. This figure demonstrates that, under Softmax, the value of zmaxsubscript\ud835\udc67maxz_{\\mathrm{max}}italic_z start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT required to focus attention increases indefinitely as n\ud835\udc5bnitalic_n grows. In contrast, SSMax ensures that attention is focused as long as zmaxsubscript\ud835\udc67maxz_{\\mathrm{max}}italic_z start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT exceeds the other values by approximately 1s1\ud835\udc60\\frac{1}{s}divide start_ARG 1 end_ARG start_ARG italic_s end_ARG, regardless of n\ud835\udc5bnitalic_n.", "description": "\uadf8\ub9bc 3\uc740 Softmax\uc640 SSMax\uc758 \ub3d9\uc791\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uc785\ub825 \ubca1\ud130\uc758 \ud06c\uae30 n\uc774 (0, 1/(n-2), 2/(n-2), ..., (n-1)/(n-2), 1, z_max)\ub85c \uc8fc\uc5b4\uc9c8 \ub54c, Softmax\uc640 SSMax\uac00 z_max \uac12\uc744 \uc5b4\ub5bb\uac8c \ubcc0\ud658\ud558\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\ub85c\ucd95\uc740 z_max\uc758 \uac12\uc744, \uc138\ub85c\ucd95\uc740 \ubcc0\ud658\ub41c \uac12\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ube68\uac04\uc0c9\uacfc \uc8fc\ud669\uc0c9 \uc120\uc740 \uc11c\ub85c \ub2e4\ub978 \uc2a4\ucf00\uc77c\ub9c1 \ub9e4\uac1c\ubcc0\uc218 s\ub97c \uc0ac\uc6a9\ud55c SSMax\ub97c, \ud30c\ub780\uc0c9 \uc120\uc740 Softmax\ub97c \ub098\ud0c0\ub0b4\uba70, \uc120\uc758 \uc2a4\ud0c0\uc77c\uc740 \uc11c\ub85c \ub2e4\ub978 \uc785\ub825 \ubca1\ud130 \ud06c\uae30\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 Softmax\uc5d0\uc11c\ub294 \uc8fc\uc758 \uc9d1\uc911\uc744 \uc704\ud574 \ud544\uc694\ud55c z_max \uac12\uc774 n\uc774 \ucee4\uc9d0\uc5d0 \ub530\ub77c \ubb34\ud55c\uc815 \uc99d\uac00\ud558\uc9c0\ub9cc, SSMax\uc5d0\uc11c\ub294 z_max \uac12\uc774 \ub2e4\ub978 \uac12\ubcf4\ub2e4 \uc57d 1/s\ub9cc\ud07c \ud06c\uba74 \uc8fc\uc758\uac00 \uc9d1\uc911\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, SSMax\ub294 \uc785\ub825 \ubca1\ud130 \ud06c\uae30\uc5d0 \uad00\uacc4\uc5c6\uc774 \uc8fc\uc758 \uc9d1\uc911\uc744 \uc720\uc9c0\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.", "section": "2. Scalable-Softmax (SSMax)"}, {"figure_path": "https://arxiv.org/html/2501.19399/extracted/6169187/train.png", "caption": "Figure 4: \nLearning curves comparing the standard Transformer (a) and SSMax variants (b)\u2013(d). All SSMax variants achieve consistently lower training loss compared to (a). Among them, the model with SSMax incorporating a bias parameter (d) exhibits the lowest loss throughout training. The results also indicate that removing the scaling parameter, as in (c), has little impact on the learning curve compared to (b).", "description": "\uadf8\ub9bc 4\ub294 \ud45c\uc900 Transformer \ubaa8\ub378\uacfc \uc5ec\ub7ec SSMax \ubcc0\ud615 \ubaa8\ub378\uc758 \ud559\uc2b5 \uace1\uc120\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4.  SSMax \ubcc0\ud615 \ubaa8\ub378\ub4e4\uc740 \ubaa8\ub450 \ud45c\uc900 Transformer \ubaa8\ub378\ubcf4\ub2e4 \ud6c8\ub828 \uc190\uc2e4\uc774 \uc9c0\uc18d\uc801\uc73c\ub85c \ub0ae\uc558\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, \ud3b8\ud5a5 \ub9e4\uac1c\ubcc0\uc218\ub97c \ud3ec\ud568\ud55c SSMax \ubaa8\ub378 (d)\uc774 \ud6c8\ub828 \uacfc\uc815 \uc804\uccb4\uc5d0\uc11c \uac00\uc7a5 \ub0ae\uc740 \uc190\uc2e4\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uc2a4\ucf00\uc77c\ub9c1 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc81c\uac70\ud55c SSMax \ubaa8\ub378 (c)\uc758 \uacbd\uc6b0 \ud559\uc2b5 \uace1\uc120\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc774 (b)\uc640 \ube44\uad50\ud588\uc744 \ub54c \uac70\uc758 \uc5c6\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.1 \ud559\uc2b5 \uace1\uc120 \ubd84\uc11d"}, {"figure_path": "https://arxiv.org/html/2501.19399/extracted/6169187/posloss.png", "caption": "Figure 5: \nPer-position test loss across context sizes up to 20,000. The x-axis represents context size, and the y-axis represents test loss. RoPE\u2019s \u03b8\ud835\udf03\\thetaitalic_\u03b8 was set to 50 times the training value, with no additional training after modification. The gray dotted line indicates the training sequence length of 1024. Results correspond to configurations (a)\u2013(f). SSMax models (b) and (c) demonstrate improved long-context generalization compared to (a), while (d) exhibits degraded performance due to the bias parameter. Model (e), where Softmax was replaced with SSMax post-training, struggles with shorter contexts, whereas (f), which switched to SSMax during the final phase of pretraining, achieves performance somewhat close to (b), though not entirely equivalent.", "description": "\uadf8\ub9bc 5\ub294 \ubb38\ub9e5 \ud06c\uae30\uac00 \ucd5c\ub300 20,000 \ud1a0\ud070\uae4c\uc9c0 \ud655\uc7a5\ub41c \uc0c1\ud669\uc5d0\uc11c\uc758 \uc704\uce58\ubcc4 \ud14c\uc2a4\ud2b8 \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. x\ucd95\uc740 \ubb38\ub9e5 \ud06c\uae30\ub97c, y\ucd95\uc740 \ud14c\uc2a4\ud2b8 \uc190\uc2e4\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ucd94\uac00\uc801\uc778 \ud6c8\ub828 \uc5c6\uc774 RoPE\uc758 \u03b8\uac12\uc744 \ud6c8\ub828 \uc2dc \uac12\uc758 50\ubc30\ub85c \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4. \ud68c\uc0c9 \uc810\uc120\uc740 1024 \ud1a0\ud070\uc758 \ud6c8\ub828 \uc2dc\ud000\uc2a4 \uae38\uc774\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uadf8\ub9bc\uc740 (a)~(f) \uad6c\uc131\uc744 \ub530\ub985\ub2c8\ub2e4. SSMax \ubaa8\ub378 (b)\uc640 (c)\ub294 (a)\uc640 \ube44\uad50\ud558\uc5ec \uc7a5\ubb38\ub9e5 \uc77c\ubc18\ud654 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\uc5c8\uc9c0\ub9cc, (d)\ub294 \ubc14\uc774\uc5b4\uc2a4 \ub9e4\uac1c\ubcc0\uc218\ub85c \uc778\ud574 \uc131\ub2a5\uc774 \uc800\ud558\ub418\uc5c8\uc2b5\ub2c8\ub2e4. Softmax\ub97c \ud6c8\ub828 \ud6c4 SSMax\ub85c \ubc14\uafbc \ubaa8\ub378 (e)\ub294 \uc9e7\uc740 \ubb38\ub9e5\uc5d0\uc11c \uc5b4\ub824\uc6c0\uc744 \uacaa\ub294 \ubc18\uba74, \ubbf8\uc138 \uc870\uc815 \ub9c8\uc9c0\ub9c9 \ub2e8\uacc4\uc5d0\uc11c SSMax\ub85c \uc804\ud658\ud55c (f)\ub294 (b)\uc640 \uac70\uc758 \ub3d9\ub4f1\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.", "section": "3.2. \ub354 \uae34 \ubb38\ub9e5\uc73c\ub85c\uc758 \uc77c\ubc18\ud654"}, {"figure_path": "https://arxiv.org/html/2501.19399/extracted/6169187/niah.png", "caption": "Figure 6: \nNeedle-In-A-Haystack test results. The horizontal axis represents context size, while the vertical axis denotes the depth at which the needle is embedded within the context. Colors indicate retrieval accuracy. RoPE\u2019s \u03b8\ud835\udf03\\thetaitalic_\u03b8 was set to 500,000, a 50-fold increase from the pretraining value. The standard Transformer (a) fails to retrieve key information beyond short context sizes, while the SSMax model (b) maintains high retrieval accuracy even at context sizes approximately 10 times longer than in training. Models (c) and (d) show lower retrieval accuracy than (b), demonstrating that removing the scaling parameter or introducing a bias parameter degrades retrieval performance. Models where Softmax was replaced with SSMax after pretraining (e) and during pretraining (f) show partial improvements over (a) but remain far below (b).", "description": "\uadf8\ub9bc 6\uc740 Needle-In-A-Haystack \ud14c\uc2a4\ud2b8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\ub85c\ucd95\uc740 \ubb38\ub9e5\uc758 \uae38\uc774\ub97c, \uc138\ub85c\ucd95\uc740 \ubb38\ub9e5 \ub0b4\uc5d0\uc11c \ubc14\ub298(\ubaa9\ud45c \uc815\ubcf4)\uc774 \uc0bd\uc785\ub41c \uae4a\uc774\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc0c9\uc0c1\uc740 \uac80\uc0c9 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. RoPE\uc758 \u03b8\ub294 \uc0ac\uc804 \ud6c8\ub828 \uac12\ubcf4\ub2e4 50\ubc30 \uc99d\uac00\ud55c 500,000\uc73c\ub85c \uc124\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud45c\uc900 Transformer(a)\ub294 \uc9e7\uc740 \ubb38\ub9e5\uc744 \ub118\uc5b4\uc11c\uba74 \uc8fc\uc694 \uc815\ubcf4 \uac80\uc0c9\uc5d0 \uc2e4\ud328\ud558\uc9c0\ub9cc, SSMax \ubaa8\ub378(b)\ub294 \ud6c8\ub828 \uc2dc \ubb38\ub9e5 \uae38\uc774\ubcf4\ub2e4 \uc57d 10\ubc30 \ub354 \uae34 \ubb38\ub9e5\uc5d0\uc11c\ub3c4 \ub192\uc740 \uac80\uc0c9 \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud569\ub2c8\ub2e4. \ubaa8\ub378 (c)\uc640 (d)\ub294 (b)\ubcf4\ub2e4 \uac80\uc0c9 \uc815\ud655\ub3c4\uac00 \ub0ae\uc740\ub370, \uc774\ub294 \uc2a4\ucf00\uc77c\ub9c1 \ub9e4\uac1c\ubcc0\uc218 \uc81c\uac70 \ub610\ub294 \ubc14\uc774\uc5b4\uc2a4 \ub9e4\uac1c\ubcc0\uc218 \ub3c4\uc785\uc774 \uac80\uc0c9 \uc131\ub2a5\uc744 \uc800\ud558\uc2dc\ud0a4\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \uc0ac\uc804 \ud6c8\ub828 \ud6c4 \ub610\ub294 \uc0ac\uc804 \ud6c8\ub828 \uc911\uc5d0 Softmax\ub97c SSMax\ub85c \ubc14\uafbc \ubaa8\ub378 (e)\uc640 (f)\ub294 (a)\ubcf4\ub2e4 \ubd80\ubd84\uc801\uc73c\ub85c \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\uc9c0\ub9cc (b)\uc5d0\ub294 \ud6e8\uc52c \ubabb \ubbf8\uce69\ub2c8\ub2e4.", "section": "3.3. Key Information Retrieval"}, {"figure_path": "https://arxiv.org/html/2501.19399/extracted/6169187/needle_score.png", "caption": "Figure 7: \nNeedle score distribution across attention layers and heads. The horizontal axis represents attention heads ranked by needle score (highest to lowest), while the vertical axis shows the corresponding needle score. Note that only the top 25 heads are shown for clarity, rather than all 144 heads. RoPE\u2019s \u03b8\ud835\udf03\\thetaitalic_\u03b8 was set to 500,000, a 50-fold increase from pretraining. The context size was 8000, with the needle sentence \u201cThe special magic Tokyo number is: 8106422.\u201d inserted at a depth of 50%. The results demonstrate that the standard Transformer (a) fails to allocate significant attention to key tokens, whereas SSMax (b) effectively concentrates attention on them. Models (c), (d), (e), and (f) allocate more attention than (a) but fail to match the focus achieved by (b). Inference results indicate that (a) failed retrieval entirely, (b) and (c) successfully retrieved the correct number, and (d), (e), and (f) retrieved only the first digit but failed to recall the full number.", "description": "\uadf8\ub9bc 7\uc740 \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uc5d0\uc11c \ud575\uc2ec \uc815\ubcf4 \uac80\uc0c9\uc5d0 \ub300\ud55c \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc8fc\uc758 \ud5e4\ub4dc\ubcc4 \ubc14\ub298 \uc810\uc218 \ubd84\ud3ec\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uac00\ub85c\ucd95\uc740 \ubc14\ub298 \uc810\uc218\uc5d0 \ub530\ub77c \uc21c\uc704\uac00 \ub9e4\uaca8\uc9c4(\ub192\uc740 \uc810\uc218\uc5d0\uc11c \ub0ae\uc740 \uc810\uc218 \uc21c\uc11c) \uc8fc\uc758 \ud5e4\ub4dc\ub97c \ub098\ud0c0\ub0b4\uace0, \uc138\ub85c\ucd95\uc740 \ud574\ub2f9 \ubc14\ub298 \uc810\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uba85\ud655\uc131\uc744 \uc704\ud574 \ubaa8\ub4e0 144\uac1c\uc758 \ud5e4\ub4dc \ub300\uc2e0 \uc0c1\uc704 25\uac1c\uc758 \ud5e4\ub4dc\ub9cc \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4. RoPE\uc758 \u03b8\ub294 \uc0ac\uc804 \ud6c8\ub828\ubcf4\ub2e4 50\ubc30 \uc99d\uac00\ud55c 500,000\uc73c\ub85c \uc124\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ubb38\ub9e5 \ud06c\uae30\ub294 8000\uc774\uc5c8\uace0, \"\ud2b9\ubcc4\ud55c \ub9c8\ubc95 \ub3c4\ucfc4 \ubc88\ud638\ub294: 8106422.\"\ub77c\ub294 \ubc14\ub298 \ubb38\uc7a5\uc774 50% \uae4a\uc774\uc5d0 \uc0bd\uc785\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uacb0\uacfc\ub294 \ud45c\uc900 \ud2b8\ub79c\uc2a4\ud3ec\uba38(a)\ub294 \ud575\uc2ec \ud1a0\ud070\uc5d0 \uc911\uc694\ud55c \uc8fc\uc758\ub97c \ud560\ub2f9\ud558\uc9c0 \ubabb\ud558\uc9c0\ub9cc, SSMax(b)\ub294 \ud6a8\uacfc\uc801\uc73c\ub85c \ud575\uc2ec \ud1a0\ud070\uc5d0 \uc8fc\uc758\ub97c \uc9d1\uc911\ud55c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378 (c), (d), (e) \ubc0f (f)\ub294 (a)\ubcf4\ub2e4 \ub354 \ub9ce\uc740 \uc8fc\uc758\ub97c \ud560\ub2f9\ud558\uc9c0\ub9cc (b)\uc5d0\uc11c \ub2ec\uc131\ud55c \ucd08\uc810\uc5d0\ub294 \ubbf8\uce58\uc9c0 \ubabb\ud569\ub2c8\ub2e4. \ucd94\ub860 \uacb0\uacfc\ub294 (a)\uac00 \uc804\uccb4 \uac80\uc0c9\uc5d0 \uc2e4\ud328\ud588\uace0, (b)\uc640 (c)\ub294 \uc62c\ubc14\ub978 \uc22b\uc790\ub97c \uc131\uacf5\uc801\uc73c\ub85c \uac80\uc0c9\ud588\uc73c\uba70, (d), (e) \ubc0f (f)\ub294 \uccab \ubc88\uc9f8 \uc790\ub9bf\uc218\ub9cc \uac80\uc0c9\ud558\uace0 \uc804\uccb4 \uc22b\uc790\ub294 \uac80\uc0c9\ud558\uc9c0 \ubabb\ud588\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "3.4. \ud575\uc2ec \uc815\ubcf4\uc5d0 \ub300\ud55c \uc8fc\uc758 \ud560\ub2f9"}, {"figure_path": "https://arxiv.org/html/2501.19399/extracted/6169187/top_needle_score.png", "caption": "Figure 8: \nTop needle score distribution across models. Each model was evaluated over 100 trials, and the highest needle score from each trial (corresponding to the leftmost value in Figure\u00a07) was recorded. The horizontal axis represents the rank of the top needle scores, sorted in descending order, while the vertical axis shows the corresponding score. Different markers indicate whether the retrieved number was fully correct (\u2219\u2219\\bullet\u2219), incorrect but with the first digit correct (\u25b2\u25b2\\blacktriangle\u25b2), or completely incorrect (\u00d7\\bm{\\times}bold_\u00d7). RoPE\u2019s \u03b8\ud835\udf03\\thetaitalic_\u03b8 was set to 500,000, a 50-fold increase from pretraining. Context size was fixed at 8000, with city names, numbers, and insertion depths randomly assigned. The results confirm that the standard Transformer (a) fails to focus attention on key tokens, whereas SSMax (b) exhibits strong concentration. Models (c), (d), (e), and (f) show partial improvements over (a) but fail to match (b)\u2019s level of attention focus.", "description": "\uadf8\ub9bc 8\uc740 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud55c \ucd5c\uace0 \ubc14\ub298 \uc810\uc218 \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc740 100\ubc88\uc758 \uc2dc\ud5d8\uc744 \uac70\ucce4\uace0, \uac01 \uc2dc\ud5d8\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \ubc14\ub298 \uc810\uc218(\uadf8\ub9bc 7\uc758 \uac00\uc7a5 \uc67c\ucabd \uac12\uc5d0 \ud574\ub2f9)\uac00 \uae30\ub85d\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uac00\ub85c\ucd95\uc740 \ucd5c\uace0 \ubc14\ub298 \uc810\uc218\uc758 \uc21c\uc704(\ub0b4\ub9bc\ucc28\uc21c)\ub97c \ub098\ud0c0\ub0b4\uace0, \uc138\ub85c\ucd95\uc740 \ud574\ub2f9 \uc810\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ub9c8\ucee4\ub294 \uac80\uc0c9\ub41c \uc22b\uc790\uac00 \uc644\uc804\ud788 \uc815\ud655\ud55c\uc9c0(\u2219), \uccab \ubc88\uc9f8 \uc790\ub9bf\uc218\ub9cc \uc815\ud655\ud55c\uc9c0(\u25b2), \uc644\uc804\ud788 \uc798\ubabb\ub41c\uc9c0(\u00d7)\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. RoPE\uc758 \u03b8\ub294 \uc0ac\uc804 \ud6c8\ub828 \uac12\ubcf4\ub2e4 50\ubc30 \ub192\uc740 500,000\uc73c\ub85c \uc124\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ubb38\ub9e5 \ud06c\uae30\ub294 8000\uc73c\ub85c \uace0\uc815\ub418\uc5c8\uace0, \ub3c4\uc2dc \uc774\ub984, \uc22b\uc790, \uc0bd\uc785 \uae4a\uc774\ub294 \ubb34\uc791\uc704\ub85c \ud560\ub2f9\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uacb0\uacfc\ub294 \ud45c\uc900 Transformer(a)\uac00 \uc8fc\uc694 \ud1a0\ud070\uc5d0 \ub300\ud55c \uc9d1\uc911\uc744 \uc2e4\ud328\ud55c \ubc18\uba74, SSMax(b)\ub294 \uac15\ub825\ud55c \uc9d1\uc911\uc744 \ubcf4\uc5ec\uc90c\uc744 \ud655\uc778\uc2dc\ucf1c\uc90d\ub2c8\ub2e4. \ubaa8\ub378 (c), (d), (e), (f)\ub294 (a)\ubcf4\ub2e4 \ubd80\ubd84\uc801\uc73c\ub85c \uac1c\uc120\ub418\uc5c8\uc9c0\ub9cc (b)\uc758 \uc9d1\uc911 \uc218\uc900\uc5d0\ub294 \ubbf8\uce58\uc9c0 \ubabb\ud588\uc2b5\ub2c8\ub2e4.", "section": "3.4. \uc8fc\uc694 \uc815\ubcf4\uc5d0 \ub300\ud55c \uc8fc\uc758 \ud560\ub2f9"}]