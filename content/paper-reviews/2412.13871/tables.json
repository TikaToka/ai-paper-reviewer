[{"content": "| Method | #Data | MaxRes. | #FLOPs. | Avg. | VQA<sup>D</sup> | Bench<sup><i>OCR</i></sup> | VQA<sup>C</sup> | VQA<sup>T</sup> | AI2D | SQA | MMMU<sup>v</sup> | GQA | SEED<sup>I</sup> | MMB | MME<sup>P</sup> | RWQA | Bench<sup><i>HR</i></sup> |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| Qwen-VL [10] | 1.45B | 448\u00d7448 | 4.0T | 56.9 | 62.6 | 48.8 | **66.3** | 61.5 | 57.7 | 68.2 | 35.9 | 57.5 | 65.4 | 61.8 | 74.4 | 49.3 | 30.5 |\n| MiniGPT-v2 [16] | 326M | 448\u00d7448 | 4.0T | - | - | 15.7 | - | - | - | - | - | 60.3 | - | - | - | - | - |\n| mPLUG-Owl2 [105] | 401M | 448\u00d7448 | 1.7T | - | - | - | - | 58.2 | - | 68.7 | - | 56.1 | 57.8 | 64.5 | 72.5 | - | - |\n| UReader [104] | 86M | 896\u00d71120 | 20.3T | - | 65.4 | - | 59.3 | 57.6 | - | - | - | - | - | - | - | - | - |\n| LLaVA-1.5 [63] | 1.22M | 336\u00d7336 | 8.0T | 49.0 | 21.8 | 31.8 | 17.8 | 45.5 | 55.5 | 66.8 | 37.0 | 62.0 | 65.8 | 66.5 | 75.3 | 54.8 | 36.1 |\n| SPHINX-2k [60] | 1.01B | 762\u00d7762 | 42.2T | - | - | - | - | 61.2 | - | <u>**70.6**</u> | - | 63.1 | <u>**71.6**</u> | 65.9 | 73.6 | - | - |\n| SPHINX-X [28] | 15.3M | 448\u00d7448 | 21.3T | - | 56.3 | - | 39.7 | 58.1 | 63.0 | 70.4 | - | 56.2 | 68.8 | 57.9 | 63.0 | - | - |\n| LLaVA-HR [73] | 1.22M | 1024\u00d71024 | 24.3T | - | - | - | - | 67.1 | - | 65.1 | - | 64.2 | 64.2 | - | <u>**77.7**</u> | - | - |\n| VILA [56] | 51M | 336\u00d7336 | 8.2T | - | - | - | - | 64.4 | - | 68.2 | - | 62.3 | 61.1 | <u>**68.9**</u> | 76.7 | - | - |\n| Honey-bee [15] | 52.5M | 336\u00d7336 | 2.6T | - | - | - | - | - | - | - | 35.3 | - | 64.5 | <u>**70.1**</u> | <u>**79.2**</u> | - | - |\n| Mini-Gemini [54] | 3.0M | 672\u00d7672 | 54.6T | 59.4 | 61.9 | 47.7 | 47.4 | 65.2 | <u>**68.2**</u> | 69.6 | 36.8 | <u>**64.5**</u> | 66.9 | 65.8 | 77.3 | 51.1 | <u>**50.1**</u> |\n| Monkey [55] | 1.40B | 896\u00d71344 | 28.0T | 59.2 | <u>**66.5**</u> | 51.4 | <u>**65.1**</u> | 67.6 | 62.6 | 69.4 | <u>**38.9**</u> | 60.7 | 64.3 | 59.8 | 73.6 | 51.6 | 38.0 |\n| LLaVA-Next [62] | 1.34M | 672\u00d7672 | 44.4T | 61.0 | 63.6 | <u>**53.2**</u> | 54.3 | 64.9 | 67.0 | 70.1 | 35.8 | 64.2 | <u>**70.2**</u> | 67.4 | 76.0 | <u>**57.8**</u> | 47.9 |\n| LLaVA-UHD v2 (ours) | 1.42M | 1008\u00d7672 | 17.5T | **63.2** | **68.1** | **53.9** | 64.5 | **67.6** | **70.5** | **71.3** | <u>**38.2**</u> | **65.4** | 70.0 | 68.2 | 74.7 | **58.2** | **51.5** |", "caption": "Table 1: Main performance on popular benchmarks. For a fair comparison, we only report the method using 7B level LLM (e.g.formulae-sequence\ud835\udc52\ud835\udc54e.g.italic_e . italic_g ., Vicuna-7B).\n#Data denotes the volume of overall data during MLLM pre-training and supervised fine-tuning. \u201cMaxRes.\u201d is the maximum accessible resolution of MLLM. \u201cAvg.\u201d: average results of 13 benchmarks. \u201cVQAD: DocVQA. \u201cBenchOCR\u201d: OCR-Bench. \u201cVQAC\u201d: ChartQA. \u201cVQAT\u201d: TextVQA. \u201cSQA\u201d: Science-QA. \u201cMMMUv\u201d: MMMU-val. \u201cSEEDI\u201d: SEED-Image. \u201cMMEP\u201d: perception sub-set of MME. \u201cRWQA\u201d: RealWorldQA. \u201cBenchHR\u201d: HR-Bench.", "description": "\ud45c 1\uc740 \uc5ec\ub7ec \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc8fc\uc694 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574 Vicuna-7B\uc640 \uac19\uc774 7B \uc218\uc900\uc758 LLM\uc744 \uc0ac\uc6a9\ud55c \ubc29\ubc95\ub9cc\uc744 \ubcf4\uace0\ud588\uc2b5\ub2c8\ub2e4. #Data\ub294 MLLM\uc758 \uc0ac\uc804 \ud6c8\ub828 \ubc0f \uc9c0\ub3c4 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc758 \uc591\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. MaxRes\ub294 MLLM\uc774 \uc811\uadfc\ud560 \uc218 \uc788\ub294 \ucd5c\ub300 \ud574\uc0c1\ub3c4\uc774\uace0, Avg\ub294 13\uac1c\uc758 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uade0 \uacb0\uacfc\uc785\ub2c8\ub2e4. \uc57d\uc5b4\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. VQAD: DocVQA, BenchOCR: OCR-Bench, VQAC: ChartQA, VQAT: TextVQA, SQA: Science-QA, MMMUv: MMMU-val, SEEDI: SEED-Image, MMEP: MME\uc758 \uc778\uc2dd \ud558\uc704 \uc9d1\ud569, RWQA: RealWorldQA, BenchHR: HR-Bench.", "section": "4 Experiment"}, {"content": "| Method | Average | VQA<sup><i>D</i></sup> | Bench<sup><i>OCR</i></sup> | VQA<sup><i>C</i></sup> | VQA<sup><i>T</i></sup> | AI2D | SQA | MMMU<sup><i>v</i></sup> | GQA | SEED<sup><i>I</i></sup> | MMB | MME<sup><i>P</i></sup> | RWQA | REC | Bench<sup><i>HR</i></sup> |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| LLaVA-UHD [31] | 58.0 | 56.7 | 40.9 | 56.3 | 62.2 | 55.4 | 70.7 | 37.0 | 63.8 | 65.6 | 64.8 | 70.0 | 54.4 | 68.3 | 45.6 |\n| + JBU module | 60.0 | 60.2 | 50.4 | 60.4 | 67.1 | 57.8 | 70.5 | 38.2 | 64.0 | 66.7 | 65.6 | 71.2 | 51.9 | 72.3 | 43.9 |\n| + HFP integration | 61.5 | 65.0 | 51.3 | 62.5 | 68.5 | 58.1 | 69.2 | 38.9 | 64.6 | 67.4 | 65.5 | 73.0 | 55.5 | 73.3 | 48.9 |\n| + Token organization | 61.7 | 66.0 | 50.1 | 62.8 | 66.8 | 59.4 | 69.8 | 37.6 | 64.0 | 67.4 | 66.1 | 73.6 | 56.9 | 74.0 | 49.0 |\n| \u0394 | +3.7 | +9.3 | +9.2 | +6.5 | +4.6 | +4.0 | -0.9 | +0.6 | +0.2 | +1.8 | +1.3 | +3.6 | +2.5 | +5.7 | +3.4 |", "caption": "Table 2: Ablation studies of modules in our proposed method. \u201cHFP\u201d is the abbreviation of high-resolution feature pyramid. \u201c\u0394\u0394\\Deltaroman_\u0394\u201d denotes the overall improvement compared to the baseline. REC reports the average accuracy of RefCOCO/g/+.", "description": "\ud45c 2\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uc5d0\uc11c \uac01 \ubaa8\ub4c8\uc758 \ud6a8\uacfc\ub97c \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \"HFP\"\ub294 \uace0\ud574\uc0c1\ub3c4 \ud2b9\uc9d5 \ud53c\ub77c\ubbf8\ub4dc\uc758 \uc57d\uc790\uc774\uba70,  \"\u0394\"\ub294 \uae30\uc900 \ubc29\ubc95 \ub300\ube44 \uc804\ubc18\uc801\uc778 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. REC\ub294 RefCOCO/g/+ \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc815\ud655\ub3c4\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uace0\ud574\uc0c1\ub3c4 \ud2b9\uc9d5 \ud53c\ub77c\ubbf8\ub4dc(HFP)\ub97c \uad6c\uc131\ud558\uace0 \ud1b5\ud569\ud558\ub294 \uacfc\uc815\uc5d0\uc11c \uac01 \ubaa8\ub4c8(JBU, HFP \ud1b5\ud569, \ud1a0\ud070 \uad6c\uc131)\uc774 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ubd84\uc11d\ud558\uc5ec, \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub4c8\uc774 \ucd94\uac00\ub428\uc5d0 \ub530\ub77c \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc73c\uba70, \ud2b9\ud788 DocVQA \ub370\uc774\ud130\uc14b\uc5d0\uc11c 9.3%\uc758 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 Experiment"}, {"content": "| Method | Average | MME<sup>P</sup> | GQA | AI2D | VQA<sup>C</sup> | VQA<sup>T</sup> | VQA<sup>D</sup> | Bench<sup>HR</sup> |\n|---|---|---|---|---|---|---|---|---|\n| LLaVA-UHD | 58.6 | 70.0 | 63.8 | 55.4 | 56.3 | 62.2 | 56.7 | 45.6 |\n| *w. ConvNext* | 59.7 | 68.2 | 62.7 | 55.6 | 61.8 | 63.5 | 61.8 | 44.0 |\n| *w. DeConv.* | 61.7 | 71.2 | 64.2 | 57.4 | 61.8 | 67.8 | 63.4 | 46.3 |\n| *w. Bilinear* | 62.0 | 72.0 | 64.5 | 57.8 | 62.2 | 67.6 | 63.7 | 46.5 |\n| *w. JBU module* | 63.0 | 73.0 | 64.6 | 58.3 | 62.5 | 68.5 | 65.0 | 48.9 |", "caption": "Table 3: Comparison of different methods for feature pyramid construction. \u201cConvNext\u201d means we replace the CILP-ViT with CLIP-ConvNext\u00a0[68] as visual encoder and directly use the feature maps from multiple stages as the final hierarchical feature pyramid.", "description": "\ud45c 3\uc740 \ub2e4\uc591\ud55c \ud2b9\uc9d5 \ud53c\ub77c\ubbf8\ub4dc \uad6c\uc131 \ubc29\ubc95\uc758 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  CLIP-ViT \ub300\uc2e0 CLIP-ConvNext [68]\ub97c \ube44\uc8fc\uc5bc \uc778\ucf54\ub354\ub85c \uc0ac\uc6a9\ud558\uace0 \uc5ec\ub7ec \ub2e8\uacc4\uc758 \ud2b9\uc9d5 \ub9f5\uc744 \ucd5c\uc885 \uacc4\uce35\uc801 \ud2b9\uc9d5 \ud53c\ub77c\ubbf8\ub4dc\ub85c \uc9c1\uc811 \uc0ac\uc6a9\ud55c \uacbd\uc6b0\ub97c  \"ConvNext\" \ub77c\uace0 \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc5ec\ub7ec \uac00\uc9c0 \ubc29\ubc95\uc73c\ub85c \uc0dd\uc131\ub41c \ud2b9\uc9d5 \ud53c\ub77c\ubbf8\ub4dc\uc758 \uc131\ub2a5\uacfc \ud6a8\uc728\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \ud2b9\ud788, \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \uc131\ub2a5\uacfc \uacc4\uc0b0 \ube44\uc6a9 \uce21\uba74\uc5d0\uc11c\uc758 \ube44\uad50\uac00 \uc774\ub8e8\uc5b4\uc9d1\ub2c8\ub2e4.", "section": "3 Method"}, {"content": "| Method | Period(h) | Latency(s) | Memory(G) | Efficiency Average | General MME<sup>P</sup> | GQA | AI2D | VQA<sup>C</sup> | VQA<sup>T</sup> | VQA<sup>D</sup> |\n|---|---|---|---|---|---|---|---|---|---|---|\n| _Pyramid_ | 62.4 | 1.26 | 60.3 | 62.4 | 69.0 | 60.8 | 57.3 | 60.7 | 67.5 | 58.9 |\n| _Fix [3\u00d73]_ | 26.9 | 0.62 | 41.7 | 64.6 | 73.8 | 63.9 | 58.8 | 60.9 | 66.2 | 63.8 |\n| _Selective_ | 27.7 | 0.54 | 39.4 | 65.3 | 73.0 | 64.6 | 58.3 | 62.5 | 68.5 | 65.0 |", "caption": "Table 4: Comparison of different choice of grid sizes on performance and efficiency. \u201cPyramid\u201d means the feature grids from different levels form a region-level feature pyramid, e.g.formulae-sequence\ud835\udc52\ud835\udc54e.g.italic_e . italic_g ., [2\u00d7\\times\u00d73] for level-0, [4\u00d7\\times\u00d76] for level-1, [8\u00d7\\times\u00d712] for leval-2. \u201cFix\u201d represents all feature maps are pooled into a 3\u00d7\\times\u00d73 feature grid. We measure the training period on 8\u00d7\\times\u00d7A100s, the latency on an A100 with a 1008\u00d7\\times\u00d7672 image, and the GPU memory on 8\u00d7\\times\u00d7A100s with 1 image per GPU in supervised fine-tune phase.", "description": "\ud45c 4\ub294 \ub2e4\uc591\ud55c \uadf8\ub9ac\ub4dc \ud06c\uae30 \uc120\ud0dd\uc5d0 \ub530\ub978 \uc131\ub2a5\uacfc \ud6a8\uc728\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  \"Pyramid\" \ubc29\uc2dd\uc740 \uc5ec\ub7ec \ub808\ubca8\uc758 \ud2b9\uc9d5 \ub9f5\uc744 \uc9c0\uc5ed \uc218\uc900\uc758 \ud2b9\uc9d5 \ud53c\ub77c\ubbf8\ub4dc\ub85c \uad6c\uc131\ud558\ub294 \ubc29\uc2dd\uc744 \uc758\ubbf8\ud558\uba70, \uc608\ub97c \ub4e4\uc5b4 \ub808\ubca8 0\uc740 2x3, \ub808\ubca8 1\uc740 4x6, \ub808\ubca8 2\ub294 8x12 \ud06c\uae30\uc758 \uadf8\ub9ac\ub4dc\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.  \"Fix\" \ubc29\uc2dd\uc740 \ubaa8\ub4e0 \ud2b9\uc9d5 \ub9f5\uc744 3x3 \uadf8\ub9ac\ub4dc\ub85c \ud1b5\ud569\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.  \ubcf8 \ud45c\uc5d0\uc11c\ub294 8\uac1c\uc758 A100 GPU\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5 \uc2dc\uac04\uc744 \uce21\uc815\ud558\uace0, A100 GPU 1\uac1c\ub97c \uc0ac\uc6a9\ud558\uc5ec 1008x672 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud55c \uc9c0\uc5f0 \uc2dc\uac04\uc744 \uce21\uc815\ud558\uba70, 8\uac1c\uc758 A100 GPU\ub97c \uc0ac\uc6a9\ud558\uace0 GPU\ub2f9 1\uac1c\uc758 \uc774\ubbf8\uc9c0\ub85c GPU \uba54\ubaa8\ub9ac\ub97c \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4.  \ubaa8\ub450 \uc9c0\ub3c4 \ud559\uc2b5 \ubbf8\uc138 \uc870\uc815 \ub2e8\uacc4\uc5d0\uc11c \uce21\uc815\ub41c \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "4 Experiment"}, {"content": "| Data | Size | Response formatting prompts |\n|---|---|---|\n| LLaVA [63] | 158K | \u2013 |\n| ShareGPT [90] | 40K | \u2013 |\n| VQAv2 [29] | 83K | Answer the question using a single word or phrase. |\n| GQA [38] | 72K |  |\n| OKVQA [75] | 9K |  |\n| OCRVQA [82] | 80K |  |\n| DocVQA [95] | 15K |  |\n| ChartQA [76] | 20K |  |\n| A-OKVQA [88] | 66K | Answer directly with the option\u2019s letter from the given choices. |\n| DVQA [41] | 20K | \u2013 |\n| TextCaps [92] | 22K | Provide a one-sentence caption for the provided image. |\n| ShareGPT4V [18] | 55K | \u2013 |\n| AI2D [43] | 3K | \u2013 |\n| LAION-GPT4V [3] | 11K | \u2013 |\n| SythDog-EN [46] | 40K | \u2013 |\n| LRV-Instruct [61] | 30K | \u2013 |\n| RefCOCO [42, 74] | 48K | Provide a short description for this region. _ (for Region Caption)_ |\n| VG [48] | 86K | Provide the bounding box coordinate of the region this sentence describes. _ (for Referring Expression Comprehension)_ |\n| Total | 858K |  |", "caption": "Table 5: Detailed composition of our 858k-mixed dataset.", "description": "\ud45c 5\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c 858,000\uac1c\uc758 \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b\uc758 \uc0c1\uc138 \uad6c\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub370\uc774\ud130\uc14b\uc740 \ub2e4\uc591\ud55c \ube44\uc804-\uc5b8\uc5b4 \uc791\uc5c5(VQA, OCR, \uc774\ubbf8\uc9c0 \ucea1\uc158 \uc0dd\uc131 \ub4f1)\uc744 \uc704\ud55c \uc5ec\ub7ec \uac1c\uc758 \uae30\uc874 \ub370\uc774\ud130\uc14b\ub4e4\uc744 \ud558\ub098\ub85c \ud569\uccd0 \ub9cc\ub4e0 \ud63c\ud569 \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\uc640 \ud568\uaed8 \ud574\ub2f9 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \uc751\ub2f5 \ud615\uc2dd(\uc608: \ud55c \ub2e8\uc5b4 \ub610\ub294 \uad6c\uc808\ub85c \ub2f5\ud558\uae30, \uac1d\uad00\uc2dd \ub2f5\ubcc0, \uc601\uc5ed \uc124\uba85 \uc81c\uacf5 \ub4f1)\uc774 \uba85\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc758 \uc885\ub958\uc640 \ud2b9\uc9d5\uc744 \uc790\uc138\ud788 \ubcf4\uc5ec\uc90c\uc73c\ub85c\uc368, \uc2e4\ud5d8 \uacb0\uacfc\uc758 \uc2e0\ub8b0\uc131\uacfc \uc77c\ubc18\ud654 \uac00\ub2a5\uc131\uc744 \ub192\uc774\ub294 \ub370 \uae30\uc5ec\ud569\ub2c8\ub2e4.", "section": "4.1 \uad6c\ud604 \uc138\ubd80 \uc815\ubcf4"}, {"content": "| Level | Period(h) | Memory(G) | Average | GQA | SQA | REC | VQA<sup>C</sup> | VQA<sup>T</sup> | ESTVQA | MME<sup>P</sup> |\n|---|---|---|---|---|---|---|---|---|---|---|\n| _0,2_ | 27.7 | 41.9 | 63.4 | 63.9 | 69.5 | 71.5 | 60.5 | 66.5 | 40.6 | 71.0 |\n| _0,1,2_ | 28.0 | 41.9 | 63.7 | 63.8 | 70.2 | 71.8 | 60.5 | 66.9 | 40.8 | 72.1 |\n| _0,1,2,3_ | 45.6 | 53.0 | 63.8 | 64.4 | 69.3 | 72.6 | 60.7 | 66.4 | 41.6 | 71.4 |\n| _0,1,2,3_ (w/o HS.) | 45.6 | 52.6 | 62.4 | 63.6 | 69.8 | 67.1 | 57.8 | 66.5 | 39.9 | 72.0 |", "caption": "Table 6: Comparison of different choices of feature level on performance and efficiency. \u201cHS.\u201d: hierarchical supervision. ESTVQA\u00a0[101] is a VQA benchmark focusing on scene text recognition.", "description": "\ud45c 6\uc740 \ub2e4\uc591\ud55c \uc218\uc900\uc758 \ud2b9\uc9d5(feature level)\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c \uc131\ub2a5\uacfc \ud6a8\uc728\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'HS'\ub294 \uacc4\uce35\uc801 \uac10\ub3c5(hierarchical supervision)\uc744 \uc758\ubbf8\ud558\uba70, ESTVQA [101]\ub294 \uc7a5\uba74 \ud14d\uc2a4\ud2b8 \uc778\uc2dd(scene text recognition)\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd98 VQA \ubca4\uce58\ub9c8\ud06c\uc785\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \ud2b9\uc9d5 \uc218\uc900\ubcc4\ub85c \ud559\uc2b5 \uc2dc\uac04, \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9, \uadf8\ub9ac\uace0 GQA, SQA, REC, VQAC, VQAT, ESTVQA, MMEP \ub4f1 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \uc131\ub2a5\uc774 \ub098\ud0c0\ub098 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.2 \uc2e4\ud5d8 \uc124\uc815"}]