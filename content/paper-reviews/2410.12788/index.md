---
title: "Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception"
summary: "Meta-Chunking: A novel text segmentation method using LLMs improves RAG efficiency by 1.32 on 2WikiMultihopQA, consuming only 45.8% of the time compared to similarity chunking."
categories: ["AI Generated"]
tags: ["ðŸ”– 24-10-16", "ðŸ¤— 24-10-22"]
showSummary: true
date: 2024-10-16
draft: false
---

### TL;DR


{{< lead >}}

This paper introduces Meta-Chunking, a smart way to divide text into meaningful chunks for better knowledge retrieval.  Instead of simply splitting sentences or paragraphs, it finds groups of sentences with strong logical connections within a paragraph. They created two methods using large language models (LLMs): Margin Sampling and Perplexity Chunking. The first decides whether to split based on probability differences.  The second identifies boundaries using perplexity (how surprising the text is). They also added a "dynamic merging" step to adjust chunk sizes as needed.  Experiments on many different datasets showed that Meta-Chunking significantly improved the accuracy of question-answering systems that use knowledge retrieval, especially when dealing with multiple-hop questions (questions that need information from multiple sources).  It was also much faster than other chunking methods.  The code is publicly available.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.12788" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}

#### Why does it matter?
This paper is crucial for researchers working on Retrieval Augmented Generation (RAG) systems and text segmentation. It introduces a novel "Meta-Chunking" approach that significantly improves RAG performance and efficiency, addressing a commonly overlooked aspect of text processing in RAG.  The findings open avenues for research on efficient LLM-based text segmentation and optimization of RAG pipelines, impacting various knowledge-intensive NLP tasks.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Meta-Chunking, a new text segmentation strategy, enhances logical coherence in text segmentation. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} Margin Sampling and Perplexity Chunking, two LLM-based Meta-Chunking methods, improve RAG performance and efficiency. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Combining Meta-Chunking with dynamic merging balances fine-grained and coarse-grained text chunking. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](figures/figures_3_0.png "ðŸ”¼ Figure 1: Overview of RAG pipeline, as well as examples based on rules, similarity, and PPL segmentation. The same background color represents being located in the same chunk.")

> The figure illustrates the RAG pipeline and compares three different text chunking methods based on rules, similarity, and perplexity.





![](charts/charts_8_0.png "ðŸ”¼ Figure 3: Performance of different methods on single-hop query in the CRUD QA dataset. ppl represents direct PPL Chunking, with a threshold of 0.5. comb. indicates PPL Chunking with dynamic combination, with a threshold of 0 when performing PPL Chunking. Precise chunk length results and performance of remaining multi-hop scenarios are included in Appendix A.3.")

> The chart compares the performance of various text chunking methods (rule-based, similarity-based, and two Meta-Chunking approaches) on a single-hop query task from the CRUD QA dataset, measured by BLEU scores, ROUGE-L, and BERTScore.





{{< table-caption caption="ðŸ”½ Table 1: Main experimental results are presented in five QA datasets. The first four datasets are sourced from LongBench. sent. indicates whether it is suitable to separate two sentences, while chunk signifies whether the latter sentence is appropriate to be merged with the preceding chunk. comb. refers to the process of first segmenting the text using PPL Chunking with a threshold of 0, followed by dynamic combination." >}}
<table id='2' style='font-size:14px'><tr><td>Dataset</td><td colspan="2">2WikiMultihopQA</td><td colspan="2">Qasper</td><td colspan="2">MultiFieldQA-en</td><td colspan="2">MultiFieldQA-zh</td><td colspan="4">MultiHop-RAG</td></tr><tr><td>Chunking Method</td><td>F1</td><td>Time</td><td>F1</td><td>Time</td><td>F1</td><td>Time</td><td>F1</td><td>Time</td><td>Hits@10</td><td>Hits@4</td><td>MAP@10</td><td>MRR@10</td></tr><tr><td colspan="13">Baselines with rule-based or similarity-based chunking</td></tr><tr><td>Original</td><td>11.89</td><td>0.21</td><td>9.45</td><td>0.13</td><td>29.89</td><td>0.16</td><td>22.45</td><td>0.06</td><td>0.6027</td><td>0.4523</td><td>0.1512</td><td>0.3507</td></tr><tr><td>Llama_index</td><td>11.74</td><td>8.12</td><td>10.15</td><td>5.81</td><td>28.30</td><td>6.25</td><td>21.85</td><td>5.53</td><td>0.7366</td><td>0.5437</td><td>0.1889</td><td>0.4068</td></tr><tr><td>Similarity Chunking</td><td>12.00</td><td>416.45</td><td>9.93</td><td>307.05</td><td>29.19</td><td>318.41</td><td>22.39</td><td>134.80</td><td>0.7232</td><td>0.5362</td><td>0.1841</td><td>0.3934</td></tr><tr><td colspan="13">Margin Sampling Chunking based on different models</td></tr><tr><td>Pythia-0.16B sent.</td><td>13.14</td><td>478.91</td><td>9.15</td><td>229.68</td><td>31.19</td><td>273.10</td><td>-</td><td>-</td><td>- 0.6993</td><td>0.5069</td><td>0.1793</td><td>0.3773</td></tr><tr><td>Pythia-0.41B sent.</td><td>11.86</td><td>926.29</td><td>9.76</td><td>498.46</td><td>29.30</td><td>545.15</td><td>-</td><td>-</td><td>0.7259</td><td>0.5596</td><td>0.1934</td><td>0.4235</td></tr><tr><td>Qwen2-0.5B sent.</td><td>11.74</td><td>788.30</td><td>9.67</td><td>599.97</td><td>31.28</td><td>648.76</td><td>23.35</td><td>480.35</td><td>0.7162</td><td>0.5246</td><td>0.1830</td><td>0.3913</td></tr><tr><td>Qwen2-1.5B sent.</td><td>11.18</td><td>1908.25</td><td>10.09</td><td>1401.30</td><td>32.19</td><td>1457.31</td><td>22.27</td><td>1081.64</td><td>0.7805</td><td>0.6089</td><td>0.2106</td><td>0.4661</td></tr><tr><td>Qwen2-7B sent.</td><td>13.22</td><td>7108.37</td><td>10.58</td><td>5207.87</td><td>32.32</td><td>5316.62</td><td>23.24</td><td>4212.00</td><td>0.6993</td><td>0.5197</td><td>0.1794</td><td>0.3835</td></tr><tr><td>Qwen2-1.5B, chunk</td><td>11.30</td><td>2189.29</td><td>9.49</td><td>1487.27</td><td>32.81</td><td>1614.01</td><td>22.08</td><td>1881.15</td><td>0.7109</td><td>0.5517</td><td>0.1970</td><td>0.4252</td></tr><tr><td>Qwen2-7B chunk</td><td>12.94</td><td>8781.82</td><td>11.37</td><td>5755.79</td><td>33.56</td><td>6287.31</td><td>24.24</td><td>5084.95</td><td>0.7175</td><td>0.5415</td><td>0.1903</td><td>0.4141</td></tr><tr><td colspan="13">Perplexity Chunking based on different models</td></tr><tr><td>Internlm2-1.8Bcomb.</td><td>12.37</td><td>355.53</td><td>10.02</td><td>200.69</td><td>30.81</td><td>251.06</td><td>22.53</td><td>161.15</td><td>0.7237</td><td>0.5499</td><td>0.1897</td><td>0.4121</td></tr><tr><td>Qwen2-1.5B comb.</td><td>13.32</td><td>190.93</td><td>9.82</td><td>122.44</td><td>31.30</td><td>136.96</td><td>22.57</td><td>107.94</td><td>0.7366</td><td>0.5570</td><td>0.1979</td><td>0.4300</td></tr><tr><td>Baichuan2-7B comb.</td><td>12.98</td><td>858.99</td><td>10.04</td><td>569.72</td><td>32.55</td><td>632.80</td><td>23.36</td><td>569.72</td><td>0.7206</td><td>0.5636</td><td>0.2048</td><td>0.4406</td></tr><tr><td>Qwen2-7B comb.</td><td>13.41</td><td>736.69</td><td>9.39</td><td>486.48</td><td>32.35</td><td>523.74</td><td>22.81</td><td>424.96</td><td>0.7215</td><td>0.5521</td><td>0.1967</td><td>0.4229</td></tr></table>{{< /table-caption >}}

> Table 1 presents the main experimental results of five question answering datasets, comparing different text chunking methods based on F1 score, time consumption, and MultiHop-RAG metrics.



### More visual insights

<details>
<summary>More on figures
</summary>


![](figures/figures_4_0.png "ðŸ”¼ Figure 2: Overview of the entire process of Meta-Chunking. Each circle represents a complete sentence, and the sentence lengths are not consistent. The vertical lines indicate where to segment. The two sides at the bottom of the figure reveal Margin Sampling Chunking and Perplexity Chunking. Circles with the same background color represent a meta-chunk, which is dynamically combined to make the final chunk length meet user needs.")

> The figure illustrates the Meta-Chunking process, showing how margin sampling and perplexity methods dynamically combine sentences into chunks of varying lengths to maintain logical coherence.


![](figures/figures_9_0.png "ðŸ”¼ Figure 2: Overview of the entire process of Meta-Chunking. Each circle represents a complete sentence, and the sentence lengths are not consistent. The vertical lines indicate where to segment. The two sides at the bottom of the figure reveal Margin Sampling Chunking and Perplexity Chunking. Circles with the same background color represent a meta-chunk, which is dynamically combined to make the final chunk length meet user needs.")

> The figure illustrates the process of Meta-Chunking, showing how sentences are grouped into meta-chunks and dynamically combined to achieve desired chunk sizes.


![](figures/figures_9_1.png "ðŸ”¼ Figure 3: Performance of different methods on single-hop query in the CRUD QA dataset. ppl represents direct PPL Chunking, with a threshold of 0.5. comb. indicates PPL Chunking with dynamic combination, with a threshold of 0 when performing PPL Chunking. Precise chunk length results and performance of remaining multi-hop scenarios are included in Appendix A.3.")

> The figure shows the performance comparison of different text chunking methods on a single-hop query task from the CRUD QA dataset, highlighting the effectiveness of PPL Chunking with dynamic combination.


</details>




<details>
<summary>More on tables
</summary>


{{< table-caption caption="ðŸ”½ Table 1: Main experimental results are presented in five QA datasets. The first four datasets are sourced from LongBench. sent. indicates whether it is suitable to separate two sentences, while chunk signifies whether the latter sentence is appropriate to be merged with the preceding chunk. comb. refers to the process of first segmenting the text using PPL Chunking with a threshold of 0, followed by dynamic combination." >}}
<br><table id='2' style='font-size:14px'><tr><td>Chunking Method</td><td>Overlap</td><td>BLEU-1</td><td>BLEU-2</td><td>BLEU-3</td><td>BLEU-4</td><td>BLEU-Avg</td><td>ROUGE-L</td><td>BERTScore</td></tr><tr><td colspan="9">Single-hop Query</td></tr><tr><td>Original</td><td>Fixed</td><td>0.3330</td><td>0.2641</td><td>0.2214</td><td>- 0.1881</td><td>- 0.2410</td><td>0.4060</td><td>0.8425</td></tr><tr><td>Llama_index</td><td>Dynamic</td><td>0.3326</td><td>0.2645</td><td>0.2214</td><td>0.1890</td><td>0.2413</td><td>0.4039</td><td>0.8439</td></tr><tr><td>Qwen2-1.5B, ppl</td><td>Dynamic</td><td>0.3592</td><td>0.2888</td><td>0.2435</td><td>0.2081</td><td>0.2644</td><td>0.4332</td><td>0.8555</td></tr><tr><td>Qwen2-7B ppl</td><td>Dynamic</td><td>0.3582</td><td>0.2898</td><td>0.2450</td><td>0.2097</td><td>0.2657</td><td>0.4308</td><td>0.8548</td></tr><tr><td>Baichuan2-7Bppi</td><td>Dynamic</td><td>0.3656</td><td>0.2952</td><td>0.2497</td><td>0.2143</td><td>0.2705</td><td>0.4393</td><td>0.8549</td></tr><tr><td colspan="9">Two-hop Query -</td></tr><tr><td>Original</td><td>Fixed</td><td>0.2251</td><td>- - 0.1300</td><td>- 0.0909</td><td>- 0.0689</td><td>- 0.1114</td><td>0.2579</td><td>0.8747</td></tr><tr><td>Llama_index</td><td>Dynamic</td><td>0.2223</td><td>0.1282</td><td>0.0896</td><td>0.0677</td><td>0.1099</td><td>0.2555</td><td>0.8732</td></tr><tr><td>Qwen2-1.5Bppl</td><td>Dynamic</td><td>0.2295</td><td>0.1331</td><td>0.0934</td><td>0.0709</td><td>0.1143</td><td>0.2609</td><td>0.8700</td></tr><tr><td>Qwen2-7B ppl</td><td>Dynamic</td><td>0.2312</td><td>0.1353</td><td>0.0949</td><td>0.0719</td><td>0.1162</td><td>0.2638</td><td>0.8751</td></tr><tr><td>Baichuan2-7Bppl</td><td>Dynamic</td><td>0.2336</td><td>0.1350</td><td>0.0940</td><td>0.0710</td><td>0.1154</td><td>0.2650</td><td>0.8754</td></tr><tr><td colspan="9">Three-hop Query -</td></tr><tr><td>Original</td><td>Fixed</td><td>0.2384</td><td>0.1268</td><td>0.0832</td><td>- 0.0602</td><td>- 0.1066</td><td>- 0.2546</td><td>0.8823</td></tr><tr><td>Llama_index</td><td>Dynamic</td><td>0.2331</td><td>0.1250</td><td>0.0825</td><td>0.0598</td><td>0.1049</td><td>0.2517</td><td>0.8796</td></tr><tr><td>Qwen2-1.5B, ppl</td><td>Dynamic</td><td>0.2453</td><td>0.1319</td><td>0.0881</td><td>0.0643</td><td>0.1114</td><td>0.2599</td><td>0.8808</td></tr><tr><td>Qwen2-7B ppl</td><td>Dynamic</td><td>0.2447</td><td>0.1330</td><td>0.0891</td><td>0.0651</td><td>0.1122</td><td>0.2618</td><td>0.8817</td></tr><tr><td>Baichuan2-7Bppi</td><td>Dynamic</td><td>0.2463</td><td>0.1324</td><td>0.0887</td><td>0.0651</td><td>0.1120</td><td>0.2596</td><td>0.8811</td></tr></table>{{< /table-caption >}}

> Table 1 presents the main experimental results of five QA datasets, comparing the performance of different chunking methods on F1 score and time consumption.


{{< table-caption caption="ðŸ”½ Table 1: Main experimental results are presented in five QA datasets. The first four datasets are sourced from LongBench. sent. indicates whether it is suitable to separate two sentences, while chunk signifies whether the latter sentence is appropriate to be merged with the preceding chunk. comb. refers to the process of first segmenting the text using PPL Chunking with a threshold of 0, followed by dynamic combination." >}}
<br><table id='4' style='font-size:14px'><tr><td rowspan="2">Dataset Chunking Method</td><td colspan="2">2WikiMultihopQA</td><td colspan="2">Qasper</td><td colspan="2">MultiFieldQA-en</td><td colspan="2">MultiFieldQA-zh</td><td colspan="2">MultiHop-RAG</td></tr><tr><td>Length</td><td>Threshold</td><td>Length</td><td>Threshold</td><td>Length</td><td>Threshold</td><td>Length</td><td>Threshold</td><td>Length</td><td>Threshold</td></tr><tr><td colspan="11">Baselines with rule-based or similarity-based chunking</td></tr><tr><td>Original</td><td>123</td><td>-</td><td>- - 121</td><td>-</td><td>113</td><td>-</td><td>178</td><td>-</td><td>78</td><td>- - -</td></tr><tr><td>Llama_index</td><td>122.61(215)</td><td>-</td><td>120.91(198)</td><td>-</td><td>112.59(208)</td><td>-</td><td>178.04(242)</td><td>-</td><td>79.68</td><td>-</td></tr><tr><td>Similarity Chunking</td><td>125.24</td><td>0.82</td><td>122.91</td><td>0.83</td><td>114.18</td><td>0.83</td><td>180.23</td><td>0.73</td><td>80.13</td><td>0.75</td></tr><tr><td colspan="11">LLMs Direct Chunking - - - - -</td></tr><tr><td>Qwen2-72B</td><td>122.13(128)</td><td>-</td><td>- 120.17(90)</td><td>- -</td><td>111.98(88)</td><td>-</td><td>178.05(190)</td><td>-</td><td>-</td><td>-</td></tr><tr><td colspan="11">Margin Sampling Chunking based on different models</td></tr><tr><td>Pythia-0.16B sent.</td><td>122.45(144)</td><td>0+comb.</td><td>- - 120.77(148)</td><td>0+comb.</td><td>111.89(133)</td><td>- 0+comb.</td><td>- - - - -</td><td>-</td><td>- 77.60(85)</td><td>0+comb.</td></tr><tr><td>Pythia-0.41B sent.</td><td>121.83(143)</td><td>0+comb.</td><td>120.75(148)</td><td>0+comb.</td><td>112.31(134)</td><td>0+comb.</td><td>-</td><td>-</td><td>77.96(83)</td><td>0+comb.</td></tr><tr><td>Qwen2-0.5B sent.</td><td>122.33(148)</td><td>0+comb.</td><td>120.07(147)</td><td>0+comb.</td><td>112.46(136)</td><td>0+comb.</td><td>178.09(180)</td><td>0+comb.</td><td>78.04(91)</td><td>0+comb.</td></tr><tr><td>Qwen2-1.5B sent.</td><td>121.60(151)</td><td>0+comb.</td><td>120.61(148)</td><td>0+comb.</td><td>111.60(136)</td><td>0+comb.</td><td>177.11(195)</td><td>0+comb.</td><td>78.20(95)</td><td>0+comb.</td></tr><tr><td>Qwen2-7B sent.</td><td>121.75(145)</td><td>0+comb.</td><td>120.47(145)</td><td>0+comb.</td><td>111.93(134)</td><td>0+comb.</td><td>177.47(195)</td><td>0+comb.</td><td>77.90(95)</td><td>0+comb.</td></tr><tr><td>Qwen2-1.5B chunk</td><td>121.99(148)</td><td>0+comb.</td><td>120.21(144)</td><td>0+comb.</td><td>111.52(134)</td><td>0+comb.</td><td>177.80(200)</td><td>0+comb.</td><td>78.16(97)</td><td>0+comb.</td></tr><tr><td>Qwen2-7B chunk</td><td>121.81(138)</td><td>0+comb.</td><td>120.01(141)</td><td>0+comb.</td><td>111.56(129)</td><td>0+comb.</td><td>178.00(188)</td><td>0+comb.</td><td>77.49(95)</td><td>0+comb.</td></tr><tr><td colspan="11">Perplexity Chunking based on different models</td></tr><tr><td>Internlm2-1.8Bcomb.</td><td>122.62(152)</td><td>0+comb.</td><td>- - 120.14(155)</td><td>0+comb.</td><td>111.98(138)</td><td>- - 0+comb.</td><td>178.00(158)</td><td>0+comb.</td><td>78.25(89)</td><td>0+comb.</td></tr><tr><td>Qwen2-1.5B comb.</td><td>122.48(152)</td><td>0+comb.</td><td>120.56(156)</td><td>0+comb.</td><td>111.35(138)</td><td>0+comb.</td><td>178.00(159)</td><td>0+comb.</td><td>78.19(89)</td><td>0+comb.</td></tr><tr><td>Baichuan2-7B, comb.</td><td>122.37(152)</td><td>0+comb.</td><td>120.66(155)</td><td>0+comb.</td><td>111.85(138)</td><td>0+comb.</td><td>178.00(159)</td><td>0+comb.</td><td>78.01(90)</td><td>0+comb.</td></tr><tr><td>Qwen2-7B comb.</td><td>122.26(152)</td><td>0+comb.</td><td>120.26(155)</td><td>0+comb.</td><td>111.47(137)</td><td>0+comb.</td><td>177.80(156)</td><td>0+comb.</td><td>78.11(89)</td><td>0+comb.</td></tr></table>{{< /table-caption >}}

> Table 1 presents the main experimental results of five QA datasets, comparing the performance of various chunking methods on F1 scores and processing time.


{{< table-caption caption="ðŸ”½ Table 1: Main experimental results are presented in five QA datasets. The first four datasets are sourced from LongBench. sent. indicates whether it is suitable to separate two sentences, while chunk signifies whether the latter sentence is appropriate to be merged with the preceding chunk. comb. refers to the process of first segmenting the text using PPL Chunking with a threshold of 0, followed by dynamic combination." >}}
<br><table id='2' style='font-size:20px'><tr><td>Chunking Method</td><td>Overlap Length</td><td>Chunk Length</td></tr><tr><td colspan="3">Chunking with Overlap</td></tr><tr><td>Original</td><td>50</td><td>218</td></tr><tr><td>Llama_index</td><td>48.78</td><td>217.03</td></tr><tr><td>Qwen2-1.5B ppl</td><td>49.97</td><td>212.79</td></tr><tr><td>Qwen2-7B ppl</td><td>50.41</td><td>217.53</td></tr><tr><td>Baichuan2-7Bppi</td><td>48.91</td><td>201.35</td></tr><tr><td colspan="3">Chunking without Overlap</td></tr><tr><td>Original</td><td>0</td><td>179</td></tr><tr><td>Llama_index</td><td>0</td><td>177.53</td></tr><tr><td>Qwen2-1.5B ppl</td><td>0</td><td>173.88</td></tr><tr><td>Qwen2-7B ppl</td><td>0</td><td>178.59</td></tr><tr><td>Baichuan2-7Bppi</td><td>0</td><td>162.56</td></tr><tr><td>Qwen2-1.5B comb.</td><td>0</td><td>177.95</td></tr><tr><td>Qwen2-7B comb.</td><td>0</td><td>178.09</td></tr><tr><td>Baichuan2-7Bcomb.</td><td>0</td><td>178.09</td></tr></table>{{< /table-caption >}}

> Table 1 presents the main experimental results of five question answering datasets, comparing different chunking methods and their performance in terms of F1 score and time consumption.


{{< table-caption caption="ðŸ”½ Table 1: Main experimental results are presented in five QA datasets. The first four datasets are sourced from LongBench. sent. indicates whether it is suitable to separate two sentences, while chunk signifies whether the latter sentence is appropriate to be merged with the preceding chunk. comb. refers to the process of first segmenting the text using PPL Chunking with a threshold of 0, followed by dynamic combination." >}}
<br><table id='2' style='font-size:14px'><tr><td>Chunking Method</td><td>BLEU-1</td><td>BLEU-2</td><td>BLEU-3</td><td>BLEU-4</td><td>BLEU-Avg</td><td>ROUGE-L</td><td>BERTScore</td></tr><tr><td colspan="8">Single-hop Query -</td></tr><tr><td>Original</td><td>0.3515</td><td>0.2788</td><td>0.2340</td><td>0.1997</td><td>0.2548</td><td>0.4213</td><td>0.8489</td></tr><tr><td>Llama_index</td><td>0.3620</td><td>0.2920</td><td>0.2480</td><td>0.2134</td><td>0.2682</td><td>0.4326</td><td>0.8521</td></tr><tr><td>Qwen2-1.5B ppl</td><td>0.3714</td><td>0.3013</td><td>0.2569</td><td>0.2223</td><td>0.2778</td><td>0.4426</td><td>0.8563</td></tr><tr><td>Qwen2-7B ppl</td><td>0.3661</td><td>0.2935</td><td>0.2481</td><td>0.2127</td><td>0.2691</td><td>0.4379</td><td>0.8558</td></tr><tr><td>Baichuan2-7Bppl</td><td>0.3725</td><td>0.3011</td><td>0.2558</td><td>0.2207</td><td>0.2772</td><td>0.4429</td><td>0.8562</td></tr><tr><td>Qwen2-1.5B comb.</td><td>0.3760</td><td>0.3034</td><td>0.2577</td><td>0.2224</td><td>0.2797</td><td>0.4443</td><td>0.8586</td></tr><tr><td>Qwen2-7B comb.</td><td>0.3724</td><td>0.3012</td><td>0.2561</td><td>0.2206</td><td>0.2774</td><td>0.4445</td><td>0.8584</td></tr><tr><td>Baichuan2-7Bcomb.</td><td>0.3812</td><td>0.3091</td><td>0.2622</td><td>0.2259</td><td>0.2840</td><td>0.4494</td><td>0.8603</td></tr><tr><td colspan="8">Two-hop Query</td></tr><tr><td>Original</td><td>0.2322</td><td>0.1324</td><td>0.0919</td><td>0.0695</td><td>0.1133</td><td>0.2613</td><td>0.8768</td></tr><tr><td>Llama_index</td><td>0.2315</td><td>0.1321</td><td>0.0923</td><td>0.0697</td><td>0.1133</td><td>0.2585</td><td>0.8762</td></tr><tr><td>Qwen2-1.5B ppl</td><td>0.2328</td><td>0.1326</td><td>0.0918</td><td>0.0694</td><td>0.1133</td><td>0.2611</td><td>0.8749</td></tr><tr><td>Qwen2-7B ppl</td><td>0.2310</td><td>0.1323</td><td>0.0916</td><td>0.0691</td><td>0.1124</td><td>0.2597</td><td>0.8752</td></tr><tr><td>Baichuan2-7B ppl</td><td>0.2350</td><td>0.1341</td><td>0.0924</td><td>0.0695</td><td>0.1141</td><td>0.2637</td><td>0.8772</td></tr><tr><td>Qwen2-1.5B comb.</td><td>0.2372</td><td>0.1363</td><td>0.0950</td><td>0.0722</td><td>0.1164</td><td>0.2658</td><td>0.8743</td></tr><tr><td>Qwen2-7B comb.</td><td>0.2364</td><td>0.1360</td><td>0.0945</td><td>0.0713</td><td>0.1161</td><td>0.2661</td><td>0.8761</td></tr><tr><td>Baichuan2-7Bcomb.</td><td>0.2325</td><td>0.1329</td><td>0.0917</td><td>0.0689</td><td>0.1133</td><td>0.2623</td><td>0.8754</td></tr><tr><td colspan="8">Three-hop Query</td></tr><tr><td>Original</td><td>0.2494</td><td>0.1317</td><td>0.0869</td><td>0.0636</td><td>0.1110</td><td>0.2595</td><td>0.8827</td></tr><tr><td>Llama_index</td><td>0.2464</td><td>0.1327</td><td>0.0883</td><td>0.0644</td><td>0.1120</td><td>0.2596</td><td>0.8840</td></tr><tr><td>Qwen2-1.5B ppl</td><td>0.2402</td><td>0.1260</td><td>0.0827</td><td>0.0596</td><td>0.1054</td><td>0.2531</td><td>0.8802</td></tr><tr><td>Qwen2-7B ppl</td><td>0.2415</td><td>0.1266</td><td>0.0828</td><td>0.0597</td><td>0.1058</td><td>0.2549</td><td>0.8816</td></tr><tr><td>Baichuan2-7Bppl</td><td>0.2460</td><td>0.1293</td><td>0.0851</td><td>0.0615</td><td>0.1084</td><td>0.2568</td><td>0.8828</td></tr><tr><td>Qwen2-1.5B comb.</td><td>0.2449</td><td>0.1294</td><td>0.0855</td><td>0.0624</td><td>0.1086</td><td>0.2566</td><td>0.8828</td></tr><tr><td>Qwen2-7B comb.</td><td>0.2408</td><td>0.1274</td><td>0.0837</td><td>0.0610</td><td>0.1068</td><td>0.2551</td><td>0.8825</td></tr><tr><td>Baichuan2-7Bcomb.</td><td>0.2494</td><td>0.1324</td><td>0.0870</td><td>0.0632</td><td>0.1111</td><td>0.2613</td><td>0.8832</td></tr></table>{{< /table-caption >}}

> Table 1 presents the main experimental results of five QA datasets, comparing the performance of various chunking methods based on different metrics, model sizes, and strategies.


{{< table-caption caption="ðŸ”½ Table 8: Settings of overlap length and chunk length for different chunking methods in the CUAD dataset. ppl represents direct PPL Chunking, with a threshold of 0." >}}
<br><table id='4' style='font-size:18px'><tr><td>Chunking Method</td><td>Overlap Length</td><td>Chunk Length</td></tr><tr><td>Original</td><td>0</td><td>98.00</td></tr><tr><td>Llama_index</td><td>0</td><td>98.49</td></tr><tr><td>Qwen2-1.5B ppl</td><td>0</td><td>97.70</td></tr><tr><td>Qwen2-7B ppl</td><td>0</td><td>96.08</td></tr><tr><td>Baichuan2-7Bppi</td><td>0</td><td>97.59</td></tr></table>{{< /table-caption >}}

> Table 8 presents the overlap length and chunk length for different chunking methods used in the CUAD dataset, showing the settings for original, Llama index, Qwen2-1.5B ppl, Qwen2-7B ppl, and Baichuan2-7B ppl methods.


{{< table-caption caption="ðŸ”½ Table 1: Main experimental results are presented in five QA datasets. The first four datasets are sourced from LongBench. sent. indicates whether it is suitable to separate two sentences, while chunk signifies whether the latter sentence is appropriate to be merged with the preceding chunk. comb. refers to the process of first segmenting the text using PPL Chunking with a threshold of 0, followed by dynamic combination." >}}
<br><table id='2' style='font-size:14px'><tr><td>Dataset</td><td colspan="2">HotpotQA</td><td colspan="2">MuSiQue</td><td colspan="2">NarrativeQA</td><td colspan="2">DuReader</td></tr><tr><td>Chunking Method</td><td>Length</td><td>Threshold</td><td>Length</td><td>Threshold</td><td>Length</td><td>Threshold</td><td>Length</td><td>Threshold</td></tr><tr><td>Original</td><td>87</td><td>-</td><td>90</td><td>-</td><td>71</td><td>-</td><td>262</td><td></td></tr><tr><td>Llama_index</td><td>86.73(154)</td><td>-</td><td>89.94(157)</td><td>-</td><td>70.35(139)</td><td>-</td><td>262.06(330)</td><td>-</td></tr><tr><td>Qwen2-1.5Bppi</td><td>86.72</td><td>0.5</td><td>89.51</td><td>0.5</td><td>70.28</td><td>1.34</td><td>261.41</td><td>0.5</td></tr><tr><td>Qwen2-1.5B comb.</td><td>86.80(98)</td><td>0+comb.</td><td>89.59(103)</td><td>0+comb.</td><td>70.32(82)</td><td>0+comb.</td><td>261.34(213)</td><td>0+comb.</td></tr><tr><td>Qwen2-1.5B comb.</td><td>86.52(96)</td><td>0.1+comb.</td><td>89.60(100)</td><td>0.1+comb.</td><td>70.47(82)</td><td>0.1+comb.</td><td>261.98(200)</td><td>0.1+comb.</td></tr><tr><td>Qwen2-1.5B comb.</td><td>86.58(92)</td><td>0.2+comb.</td><td>89.75(96)</td><td>0.2+comb.</td><td>70.17(81)</td><td>0.2+comb.</td><td>261.92(189)</td><td>0.2+comb.</td></tr><tr><td>Qwen2-1.5B comb.</td><td>86.77(85)</td><td>0.3+comb.</td><td>89.60(88)</td><td>0.3+comb.</td><td>70.19(79)</td><td>0.3+comb.</td><td>261.06(170)</td><td>0.3+comb.</td></tr><tr><td>Qwen2-1.5B comb.</td><td>86.81(70)</td><td>0.4+comb.</td><td>89.68(75)</td><td>0.4+comb.</td><td>70.66(78)</td><td>0.4+comb.</td><td>261.48(140)</td><td>0.4+comb.</td></tr></table>{{< /table-caption >}}

> Table 1 presents the main experimental results of five question answering datasets, comparing the performance of various chunking methods based on different metrics and model parameters.


{{< table-caption caption="ðŸ”½ Table 1: Main experimental results are presented in five QA datasets. The first four datasets are sourced from LongBench. sent. indicates whether it is suitable to separate two sentences, while chunk signifies whether the latter sentence is appropriate to be merged with the preceding chunk. comb. refers to the process of first segmenting the text using PPL Chunking with a threshold of 0, followed by dynamic combination." >}}
<br><table id='4' style='font-size:16px'><tr><td>Chunking Method</td><td>BLEU-1</td><td>BLEU-2</td><td>BLEU-3</td><td>BLEU-4</td><td>BLEU-Avg</td><td>ROUGE-L</td><td>BERTScore</td></tr><tr><td>Original</td><td>0.6845</td><td>0.4496</td><td>0.2997</td><td>0.1798</td><td>0.3513</td><td>0.4217</td><td>0.8043</td></tr><tr><td>Llama_index</td><td>0.6966</td><td>0.4573</td><td>0.3006</td><td>0.1730</td><td>0.3493</td><td>0.4137</td><td>0.8001</td></tr><tr><td>Qwen2-1.5B ppl</td><td>0.7098</td><td>0.4722</td><td>0.3180</td><td>0.1932</td><td>0.3677</td><td>0.4060</td><td>0.8006</td></tr><tr><td>Qwen2-7B ppl</td><td>0.7038</td><td>0.4670</td><td>0.3143</td><td>0.1911</td><td>0.3638</td><td>0.4070</td><td>0.8018</td></tr><tr><td>Baichuan2-7Bppl</td><td>0.7195</td><td>0.4738</td><td>0.3160</td><td>0.1884</td><td>0.3665</td><td>0.4111</td><td>0.8025</td></tr></table>{{< /table-caption >}}

> Table 1 presents the main experimental results of five question answering datasets, comparing different chunking methods based on F1 score and time.


{{< table-caption caption="ðŸ”½ Table 1: Main experimental results are presented in five QA datasets. The first four datasets are sourced from LongBench. sent. indicates whether it is suitable to separate two sentences, while chunk signifies whether the latter sentence is appropriate to be merged with the preceding chunk. comb. refers to the process of first segmenting the text using PPL Chunking with a threshold of 0, followed by dynamic combination." >}}
<table id='6' style='font-size:20px'><tr><td>Chunking Method</td><td>Dataset Threshold</td><td>HotpotQA F1</td><td>MuSiQue F1</td><td>NarrativeQA F1</td><td>DuReader ROUGE-L</td></tr><tr><td>Original</td><td>-</td><td>15.79</td><td>7.21</td><td>5.72</td><td>20.69</td></tr><tr><td>Llama_index</td><td>-</td><td>15.72</td><td>8.19</td><td>5.03</td><td>21.41</td></tr><tr><td>Qwen2-1.5B ppl</td><td>Multi</td><td>17.74</td><td>8.39</td><td>6.12</td><td>20.77</td></tr><tr><td>Qwen2-1.5B comb.</td><td>0</td><td>17.47</td><td>8.08</td><td>4.93</td><td>20.77</td></tr><tr><td>Qwen2-1.5B comb.</td><td>0.1</td><td>17.19</td><td>7.48</td><td>4.91</td><td>20.33</td></tr><tr><td>Qwen2-1.5B comb.</td><td>0.2</td><td>17.70</td><td>7.31</td><td>5.20</td><td>20.95</td></tr><tr><td>Qwen2-1.5B comb.</td><td>0.3</td><td>17.46</td><td>7.92</td><td>5.08</td><td>21.22</td></tr><tr><td>Qwen2-1.5Bcomb.</td><td>0.4</td><td>16.44</td><td>8.05</td><td>5.80</td><td>21.65</td></tr></table>{{< /table-caption >}}

> Table 1 presents the main experimental results of five question answering datasets, comparing the performance of various text chunking methods on F1 score and time consumption.


{{< table-caption caption="ðŸ”½ Table 1: Main experimental results are presented in five QA datasets. The first four datasets are sourced from LongBench. sent. indicates whether it is suitable to separate two sentences, while chunk signifies whether the latter sentence is appropriate to be merged with the preceding chunk. comb. refers to the process of first segmenting the text using PPL Chunking with a threshold of 0, followed by dynamic combination." >}}
<br><table id='2' style='font-size:14px'><tr><td>Chunking and Re-ranking</td><td>Chunk Length</td><td>Threshold</td></tr><tr><td>Original</td><td>78</td><td>-</td></tr><tr><td>Original and BgeRerank</td><td>78</td><td>-</td></tr><tr><td>Original and PPLRerank</td><td>78 ä¸€</td><td>- -</td></tr><tr><td>Qwen2-1.5B, ppl</td><td>77.60</td><td>0.5</td></tr><tr><td>Qwen2-1.5B ppl BgeRerank and</td><td>77.60</td><td>0.5</td></tr><tr><td>Qwen2-1.5B ppl and PPLRerank</td><td>77.60</td><td>0.5</td></tr></table>{{< /table-caption >}}

> Table 1 presents the F1 scores and processing times of various text chunking methods on five question answering datasets, comparing rule-based and similarity-based baselines with Margin Sampling Chunking and Perplexity Chunking strategies using different LLMs.


{{< table-caption caption="ðŸ”½ Table 1: Main experimental results are presented in five QA datasets. The first four datasets are sourced from LongBench. sent. indicates whether it is suitable to separate two sentences, while chunk signifies whether the latter sentence is appropriate to be merged with the preceding chunk. comb. refers to the process of first segmenting the text using PPL Chunking with a threshold of 0, followed by dynamic combination." >}}
<table id='4' style='font-size:16px'><tr><td>Chunking and Re-ranking</td><td>Hits@8</td><td>Hits@6</td><td>Hits@4</td><td>Hits@2</td><td>MAP@10</td><td>MRR@10</td></tr><tr><td>Original</td><td>0.5627</td><td>0.5180</td><td>0.4523</td><td>0.3499</td><td>0.1512</td><td>0.3507</td></tr><tr><td>Original and BgeRerank</td><td>0.5818</td><td>0.5406</td><td>0.4741</td><td>0.3379</td><td>0.1486</td><td>0.3391</td></tr><tr><td>Original and PPLRerank</td><td>0.5769</td><td>0.5521</td><td>0.5055</td><td>0.4102</td><td>0.1849</td><td>0.4147</td></tr><tr><td>Qwen2-1.5Bppt</td><td>0.6838</td><td>- 0.6244</td><td>- 0.5503</td><td>- 0.4151</td><td>- 0.1954</td><td>- - 0.4195</td></tr><tr><td>Qwen2-1.5B, ppl BgeRerank and</td><td>0.6927</td><td>0.6435</td><td>0.5721</td><td>0.4381</td><td>0.2075</td><td>0.4413</td></tr><tr><td>Qwen2-1.5B ppl and PPLRerank</td><td>0.7197</td><td>0.6931</td><td>0.6568</td><td>0.5721</td><td>0.2590</td><td>0.5558</td></tr></table>{{< /table-caption >}}

> Table 1 presents the main experimental results of five question answering datasets, comparing the performance of various text chunking methods (rule-based, similarity-based, margin sampling chunking, and perplexity chunking) on different LLMs.


</details>


### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/21.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}