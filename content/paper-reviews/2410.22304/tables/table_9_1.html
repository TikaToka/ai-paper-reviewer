<br><table id='3' style='font-size:16px'><tr><td>Learning rate</td><td>2e-4</td></tr><tr><td>Optimizer</td><td>Adam</td></tr><tr><td>Global batch size</td><td>16</td></tr><tr><td>Gradient clipping</td><td>1.0</td></tr><tr><td>gradient_accumulation_steps</td><td>2</td></tr><tr><td>warmup_ratio</td><td>0.1</td></tr><tr><td>lora_r</td><td>16</td></tr><tr><td>lora_alpha</td><td>16</td></tr><tr><td>lora_dropout</td><td>0.05</td></tr><tr><td>lora_target</td><td>all</td></tr><tr><td>Training epochs</td><td>3</td></tr></table>