[{"figure_path": "https://arxiv.org/html/2501.06186/x4.png", "caption": "Figure 1: \nComparison of the reasoning abilities of our model (LlamaV-o1) with closed-source Gemini-1.5-Flash and Claude-3.5-Sonnet on an example in pattern recognition task from our proposed VRC-Bench. While Claude-3.5-Sonnet concludes \"none of the options,\" its reasoning steps lack full alignment with the observed logic (highlighted in red). Gemini-1.5-Flash demonstrates weaker reasoning with less logical coherence details (highlighted in red). Our LlamaV-o1 provides better and more systematic reasoning, identifying that option D follows the established pattern, thereby showcasing its logical reasoning capability. Additional results are presented in Fig. 5.", "description": "\uadf8\ub9bc 1\uc740 \uc81c\uc548\ub41c VRC-Bench\uc758 \ud328\ud134 \uc778\uc2dd \uacfc\uc81c\uc5d0\uc11c LlamaV-01 \ubaa8\ub378\uacfc \ud074\ub85c\uc988\uc18c\uc2a4 Gemini-1.5-Flash \ubc0f Claude-3.5-Sonnet \ubaa8\ub378\uc758 \ucd94\ub860 \ub2a5\ub825\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. Claude-3.5-Sonnet\uc740 '\uc635\uc158 \uc5c6\uc74c'\uc774\ub77c\ub294 \uacb0\ub860\uc744 \ub0b4\ub838\uc9c0\ub9cc, \uadf8 \ucd94\ub860 \ub2e8\uacc4\ub294 \uad00\ucc30\ub41c \ub17c\ub9ac\uc640 \uc644\uc804\ud788 \uc77c\uce58\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4 (\ube68\uac04\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc). Gemini-1.5-Flash\ub294 \ub17c\ub9ac\uc801 \uc77c\uad00\uc131\uc774 \ubd80\uc871\ud55c \ucd94\ub860\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4 (\ube68\uac04\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc). \ubc18\uba74 LlamaV-01\uc740 \uc635\uc158 D\uac00 \ud655\ub9bd\ub41c \ud328\ud134\uc744 \ub530\ub978\ub2e4\ub294 \uc810\uc744 \ubc1d\ud600\ub0c4\uc73c\ub85c\uc368 \ub354 \ub098\uc740 \uccb4\uacc4\uc801\uc778 \ucd94\ub860\uc744 \uc81c\uacf5\ud558\uc5ec \ub17c\ub9ac\uc801 \ucd94\ub860 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc 5\uc5d0 \ucd94\uac00 \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.06186/x5.png", "caption": "Figure 2: \nThe proposed VRC-Bench examples show the diverse and challenging reasoning tasks our benchmark encompasses, spanning a wide range of modalities and contexts. Each example emphasizes step-by-step reasoning, starting from task comprehension and progressing to logical inference and answer generation. The tasks include mathematical reasoning using geometric principles, scientific classification based on molecular structures, visual interpretation of charts and diagrams, artistic identification from historical paintings, and medical diagnosis from tissue images.\nFor instance, one example demonstrates the calculation of an angle in a geometric diagram by leveraging linear pairs and perpendicular relationships. Another highlights scientific reasoning by identifying ethane as a compound based on its molecular composition. Visual perception tasks challenge the model to analyze pie charts for global energy reserves or recognize reflected shapes. Artistic and cultural tasks require identifying paintings and sports based on visual and contextual cues. Finally, tasks in medical imaging and advertisement recognition test the model\u2019s ability to classify tissue types or extract product names through careful observation.", "description": "\uadf8\ub9bc 2\ub294 \uc81c\uc548\ub41c VRC-Bench(Visual Reasoning Chain Benchmark)\uc758 \ub2e4\uc591\ud55c \uc608\uc2dc\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc608\uc2dc\ub294 \ub2e4\uc591\ud55c \ubaa8\ub2ec\ub9ac\ud2f0(\ud14d\uc2a4\ud2b8, \uc774\ubbf8\uc9c0, \ucc28\ud2b8 \ub4f1)\uc640 \ub9e5\ub77d\uc744 \uc0ac\uc6a9\ud558\ub294 \ub2e4\ub2e8\uacc4 \ucd94\ub860 \uacfc\uc81c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubb38\uc81c\ub294 \uacfc\uc81c \uc774\ud574\ubd80\ud130 \ub17c\ub9ac\uc801 \ucd94\ub860, \ucd5c\uc885 \ub2f5\ubcc0 \uc0dd\uc131\uae4c\uc9c0 \ub2e8\uacc4\ubcc4 \ucd94\ub860 \uacfc\uc815\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.  \uacfc\uc81c\uc758 \uc720\ud615\uc740 \uae30\ud558\ud559\uc801 \uc6d0\ub9ac\ub97c \uc774\uc6a9\ud55c \uc218\ud559\uc801 \ucd94\ub860, \ubd84\uc790 \uad6c\uc870\ub97c \uae30\ubc18\uc73c\ub85c \ud55c \uacfc\ud559\uc801 \ubd84\ub958, \ucc28\ud2b8\uc640 \ub2e4\uc774\uc5b4\uadf8\ub7a8\uc758 \uc2dc\uac01\uc801 \ud574\uc11d, \uc5ed\uc0ac\uc801 \uadf8\ub9bc\uc758 \uc608\uc220\uc801 \uc2dd\ubcc4, \uc870\uc9c1 \uc774\ubbf8\uc9c0\ub97c \ud1b5\ud55c \uc758\ud559\uc801 \uc9c4\ub2e8 \ub4f1 \ub2e4\uc591\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uae30\ud558\ud559\uc801 \ub2e4\uc774\uc5b4\uadf8\ub7a8\uc5d0\uc11c \uc120\ud615 \uc30d\uacfc \uc218\uc9c1 \uad00\uacc4\ub97c \uc774\uc6a9\ud558\uc5ec \uac01\ub3c4\ub97c \uacc4\uc0b0\ud558\ub294 \ubb38\uc81c, \ubd84\uc790 \uad6c\uc131\uc744 \ubc14\ud0d5\uc73c\ub85c \uc5d0\ud0c4\uc744 \ud654\ud569\ubb3c\ub85c \uc2dd\ubcc4\ud558\ub294 \ubb38\uc81c, \uc804 \uc138\uacc4 \ube44\uc7ac\uc0dd \uc5d0\ub108\uc9c0 \ub9e4\uc7a5\ub7c9\uc5d0 \ub300\ud55c \uc6d0\ud615 \ucc28\ud2b8 \ubd84\uc11d \ub610\ub294 \ubc18\uc0ac\ub41c \ubaa8\uc591 \uc778\uc2dd \ubb38\uc81c \ub4f1\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4.  \ub610\ud55c, \uc2dc\uac01\uc801 \ub2e8\uc11c\uc640 \ub9e5\ub77d\uc801 \ub2e8\uc11c\ub97c \uae30\ubc18\uc73c\ub85c \uadf8\ub9bc\uacfc \uc2a4\ud3ec\uce20\ub97c \uc2dd\ubcc4\ud558\ub294 \uc608\uc220 \ubc0f \ubb38\ud654\uc801 \uacfc\uc81c\uc640 \uc870\uc9c1 \uc720\ud615\uc744 \ubd84\ub958\ud558\uac70\ub098 \uad11\uace0\uc5d0\uc11c \uc81c\ud488 \uc774\ub984\uc744 \ucd94\ucd9c\ud558\ub294 \uc758\ub8cc \uc774\ubbf8\uc9d5 \ubc0f \uad11\uace0 \uc778\uc2dd \uacfc\uc81c\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc804\ubc18\uc801\uc73c\ub85c, \uc774 \uadf8\ub9bc\uc740 VRC-Bench\uc758 \uad11\ubc94\uc704\ud55c \ub09c\uc774\ub3c4\uc640 \ub2e4\uc591\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub300\ud45c\uc801\uc778 \uc608\uc2dc\ub4e4\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "3 Step-by-Step Visual Reasoning Benchmark: VRC-Bench"}, {"figure_path": "https://arxiv.org/html/2501.06186/x6.png", "caption": "Figure 3: \nThe figure illustrates our comprehensive benchmark structure and comparative performance of LMMs on the proposed ReasoningChain-Bench. (Left) The dataset spans multiple domains, including carefully selected samples for mathematical and logical reasoning (e.g., MathVista [38] with 231 samples and LogicVista with 158 samples), scientific reasoning (e.g., Science-QA [40] with 83 samples), and visual perception (e.g., Blink-IQ-Test [15] with 35 samples). Additionally, it includes specialized areas such as medical imaging (e.g., MMMU-Medical [72] with 29 samples), cultural and social understanding (e.g., ALM-Bench [57] with 104 samples), and document understanding through OCR (e.g., Doc-VQA [46] with 61 samples). By integrating tasks like chart and diagram comprehension (e.g., Chart-VQA [44] with 107 samples), our dataset not only covers a broad spectrum of real-world applications but also expand LMM\u2019s ability to reason, perceive, and interpret complex multimodal information.\n(Right) The bar chart compares various SoTA reasoning models on the VRC-Bench, highlighting both final answer accuracy and step-by-step reasoning scores. The models evaluated for complex reasoning tasks include GPT-4o, Gemini-2.0-Flash, Claude-3.5-Sonnet, and Llava-CoT. Our benchmark evaluates models not only on their ability to generate accurate final answers but also on the coherence and logical flow of their reasoning steps. Our approach, LlamaV-o1, outperforms GPT-4o-mini, Gemini-1.5-Flash and Llava-CoT in the VRC-Bench, achieving superior results in final answer accuracy across complex multimodal reasoning tasks.", "description": "\uadf8\ub9bc 3\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ub41c ReasoningChain-Bench\uc5d0 \ub300\ud55c LMM(\ub300\uaddc\ubaa8 \ub2e4\uc911 \ubaa8\ub4dc \ubaa8\ub378)\uc758 \uc885\ud569\uc801\uc778 \ubca4\uce58\ub9c8\ud06c \uad6c\uc870\uc640 \ube44\uad50 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd \uadf8\ub9bc\uc740 \uc218\ud559 \ubc0f \ub17c\ub9ac \ucd94\ub860(MathVista[38] 231\uac1c \uc0d8\ud50c, LogicVista 158\uac1c \uc0d8\ud50c), \uacfc\ud559\uc801 \ucd94\ub860(Science-QA[40] 83\uac1c \uc0d8\ud50c), \uc2dc\uac01\uc801 \uc778\uc9c0(Blink-IQ-Test[15] 35\uac1c \uc0d8\ud50c) \ub4f1 \ub2e4\uc591\ud55c \uc601\uc5ed\uc758 \uc0d8\ud50c\uc744 \ud3ec\ud568\ud558\ub294 \ub370\uc774\ud130\uc14b\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c \uc758\ub8cc \uc774\ubbf8\uc9d5(MMMU-Medical[72] 29\uac1c \uc0d8\ud50c), \ubb38\ud654 \ubc0f \uc0ac\ud68c\uc801 \uc774\ud574(ALM-Bench[57] 104\uac1c \uc0d8\ud50c), OCR\uc744 \ud1b5\ud55c \ubb38\uc11c \uc774\ud574(Doc-VQA[46] 61\uac1c \uc0d8\ud50c)\uc640 \uac19\uc740 \ud2b9\uc218 \ubd84\uc57c\ub3c4 \ud3ec\ud568\ud569\ub2c8\ub2e4. \ucc28\ud2b8 \ubc0f \ub2e4\uc774\uc5b4\uadf8\ub7a8 \uc774\ud574(Chart-VQA[44] 107\uac1c \uc0d8\ud50c)\uc640 \uac19\uc740 \uc791\uc5c5\uc744 \ud1b5\ud569\ud558\uc5ec \ub370\uc774\ud130\uc14b\uc740 \uad11\ubc94\uc704\ud55c \uc2e4\uc81c \uc751\uc6a9 \ud504\ub85c\uadf8\ub7a8\uc744 \ub2e4\ub8f0 \ubfd0\ub9cc \uc544\ub2c8\ub77c LMM\uc758 \ucd94\ub860, \uc778\uc9c0 \ubc0f \ubcf5\ud569 \ub2e4\uc911 \ubaa8\ub4dc \uc815\ubcf4 \ud574\uc11d \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4. \uc624\ub978\ucabd \ub9c9\ub300 \uadf8\ub798\ud504\ub294 VRC-Bench\uc5d0\uc11c \ub2e4\uc591\ud55c \ucd5c\ucca8\ub2e8 \ucd94\ub860 \ubaa8\ub378\uc758 \ube44\uad50\ub97c \ubcf4\uc5ec\uc8fc\uba70, \ucd5c\uc885 \ub2f5\ubcc0 \uc815\ud655\ub3c4\uc640 \ub2e8\uacc4\ubcc4 \ucd94\ub860 \uc810\uc218\ub97c \ubaa8\ub450 \uac15\uc870\ud569\ub2c8\ub2e4. \ubcf5\uc7a1\ud55c \ucd94\ub860 \uc791\uc5c5\uc5d0 \ub300\ud574 \ud3c9\uac00\ub41c \ubaa8\ub378\uc5d0\ub294 GPT-4o, Gemini-2.0-Flash, Claude-3.5-Sonnet \ubc0f Llava-CoT\uac00 \ud3ec\ud568\ub429\ub2c8\ub2e4. \uc81c\uc548\ub41c \ubca4\uce58\ub9c8\ud06c\ub294 \uc815\ud655\ud55c \ucd5c\uc885 \ub2f5\ubcc0 \uc0dd\uc131 \ub2a5\ub825\ubfd0\ub9cc \uc544\ub2c8\ub77c \ucd94\ub860 \ub2e8\uacc4\uc758 \uc77c\uad00\uc131\uacfc \ub17c\ub9ac\uc801 \ud750\ub984\ub3c4 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uc81c\uc548\ub41c \ubc29\ubc95\uc778 LlamaV-o1\uc740 VRC-Bench\uc5d0\uc11c GPT-4o-mini, Gemini-1.5-Flash \ubc0f Llava-CoT\ub97c \ub2a5\uac00\ud558\uba70 \ubcf5\uc7a1\ud55c \ub2e4\uc911 \ubaa8\ub4dc \ucd94\ub860 \uc791\uc5c5\uc5d0\uc11c \uc6b0\uc218\ud55c \ucd5c\uc885 \ub2f5\ubcc0 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud569\ub2c8\ub2e4.", "section": "3 Step-by-Step Visual Reasoning Benchmark: VRC-Bench"}, {"figure_path": "https://arxiv.org/html/2501.06186/x7.png", "caption": "Figure 4: \nThe comprehensive comparison of category-wise and overall performance scores achieved by various models on diverse reasoning tasks. The evaluation spans multiple domains, including Math & Logic Reasoning, Scientific Reasoning, Complex Visual Perception, Chart & Diagram Understanding, Medical Imaging, Social & Cultural Context, Visual Reasoning, and OCR & Document Understanding. The models assessed include GPT-4o, Claude-3.5-Sonnet, Gemini variants, LLAVA-CoT, and our proposed model.\nOur model demonstrates consistently superior performance in critical categories such as Math & Logic Reasoning, Chart & Diagram Understanding, and Medical Imaging, achieving a balanced improvement across both step-by-step reasoning (Step Scores) and final answer accuracy (Final Answer Scores). Compared to LLAVA-CoT, our approach excels in maintaining high accuracy across tasks while showcasing robustness and interpretability in multi-step reasoning challenges.", "description": "\uadf8\ub9bc 4\ub294 \ub2e4\uc591\ud55c \ucd94\ub860 \uacfc\uc81c\uc5d0\uc11c \uc5ec\ub7ec \ubaa8\ub378\uc758 \ubc94\uc8fc\ubcc4 \ubc0f \uc804\ubc18\uc801 \uc131\ub2a5 \uc810\uc218\ub97c \uc885\ud569\uc801\uc73c\ub85c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \ud3c9\uac00\ub294 \uc218\ud559 \ubc0f \ub17c\ub9ac \ucd94\ub860, \uacfc\ud559\uc801 \ucd94\ub860, \ubcf5\uc7a1\ud55c \uc2dc\uac01\uc801 \uc778\uc2dd, \ucc28\ud2b8 \ubc0f \ub2e4\uc774\uc5b4\uadf8\ub7a8 \uc774\ud574, \uc758\ub8cc \uc601\uc0c1, \uc0ac\ud68c \ubc0f \ubb38\ud654\uc801 \ub9e5\ub77d, \uc2dc\uac01\uc801 \ucd94\ub860 \ubc0f OCR \ubc0f \ubb38\uc11c \uc774\ud574\ub97c \ud3ec\ud568\ud55c \uc5ec\ub7ec \ub3c4\uba54\uc778\uc5d0 \uac78\uccd0 \uc788\uc2b5\ub2c8\ub2e4. \ud3c9\uac00\ub41c \ubaa8\ub378\uc5d0\ub294 GPT-4o, Claude-3.5-Sonnet, Gemini \ubcc0\ud615, LLAVA-CoT \ubc0f \uc81c\uc548\ub41c \ubaa8\ub378\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4. \uc81c\uc548\ub41c \ubaa8\ub378\uc740 \uc218\ud559 \ubc0f \ub17c\ub9ac \ucd94\ub860, \ucc28\ud2b8 \ubc0f \ub2e4\uc774\uc5b4\uadf8\ub7a8 \uc774\ud574, \uc758\ub8cc \uc601\uc0c1\uacfc \uac19\uc740 \uc911\uc694\ud55c \ubc94\uc8fc\uc5d0\uc11c \uc77c\uad00\ub418\uac8c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\uba70 \ub2e8\uacc4\ubcc4 \ucd94\ub860(\ub2e8\uacc4 \uc810\uc218)\uacfc \ucd5c\uc885 \ub2f5\ubcc0 \uc815\ud655\ub3c4(\ucd5c\uc885 \ub2f5\ubcc0 \uc810\uc218) \ubaa8\ub450\uc5d0\uc11c \uade0\ud615 \uc7a1\ud78c \uac1c\uc120\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4. LLAVA-CoT\uc640 \ube44\uad50\ud558\uc5ec \uc81c\uc548\ub41c \ubc29\ubc95\uc740 \uacfc\uc81c \uc804\ubc18\uc5d0 \uac78\uccd0 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud558\uba74\uc11c \ub2e4\ub2e8\uacc4 \ucd94\ub860 \uacfc\uc81c\uc5d0\uc11c \uacac\uace0\uc131\uacfc \ud574\uc11d \uac00\ub2a5\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5 \uacb0\uacfc \ubc0f \ub17c\uc758"}, {"figure_path": "https://arxiv.org/html/2501.06186/x8.png", "caption": "Figure 5: \nQualitative comparison between Llava-CoT and the proposed LlamaV-o1 on examples from the VRC-Bench. First row: the example shows visual reasoning capabilities on an example chart. Here, Llava-CoT makes mistakes (highlighted in red) for both the intermediate steps and the final answer. In Comparison, our LlamaV-o1 provides an accurate description of the steps as well as the final answer. Second row: While both Llava-CoT and our LlamaV-o1 provide accurate step descriptions on an example real-world VQA, Llava-CoT fails to infer the final answer. Last row: Llava-CoT fails to accurately answer for the counting task, while also missing the intermediate counting steps. In contrast, our LlamaV-o1 model performs better in intermediate reasoning steps while also providing the accurate final answer.", "description": "\uadf8\ub9bc 5\ub294 \uc81c\uc548\ub41c VRC-Bench\uc758 \uc608\uc2dc\ub97c \uc0ac\uc6a9\ud558\uc5ec Llava-CoT\uc640 \uc81c\uc548\ub41c LlamaV-01\uc758 \uc131\ub2a5\uc744 \uc815\uc131\uc801\uc73c\ub85c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uccab \ubc88\uc9f8 \uc904\uc740 \ucc28\ud2b8\ub97c \uc774\uc6a9\ud55c \uc2dc\uac01\uc801 \ucd94\ub860 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Llava-CoT\ub294 \uc911\uac04 \ub2e8\uacc4\uc640 \ucd5c\uc885 \ub2f5\ubcc0 \ubaa8\ub450\uc5d0\uc11c \uc2e4\uc218\ub97c \uc800\uc9c0\ub974\ub294 \ubc18\uba74, LlamaV-01\uc740 \uc815\ud655\ud55c \ub2e8\uacc4 \uc124\uba85\uacfc \ucd5c\uc885 \ub2f5\ubcc0\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \ub450 \ubc88\uc9f8 \uc904\uc740 \uc2e4\uc81c \uc2dc\uac01\uc801 \uc9c8\uc758\uc751\ub2f5(VQA) \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub450 \ubaa8\ub378 \ubaa8\ub450 \uc815\ud655\ud55c \ub2e8\uacc4\uc801 \uc124\uba85\uc744 \uc81c\uacf5\ud558\uc9c0\ub9cc, Llava-CoT\ub294 \ucd5c\uc885 \ub2f5\ubcc0\uc744 \uc720\ucd94\ud558\ub294 \ub370 \uc2e4\ud328\ud569\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9 \uc904\uc740 \uacc4\uc0b0 \ubb38\uc81c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Llava-CoT\ub294 \uc815\ud655\ud55c \ub2f5\uc744 \uc81c\uc2dc\ud558\uc9c0 \ubabb\ud558\uace0 \uc911\uac04 \ub2e8\uacc4\ub3c4 \ub204\ub77d\ud55c \ubc18\uba74, LlamaV-01\uc740 \uc911\uac04 \ub2e8\uacc4\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uace0 \uc815\ud655\ud55c \ucd5c\uc885 \ub2f5\ubcc0\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4 Proposed Step-by-Step Visual Reasoning Model: LlamaV-01"}]