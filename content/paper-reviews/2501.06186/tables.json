[{"content": "| ![webpage_logo.png](https://arxiv.org/html/2501.06186/extracted/6124330/assets/webpage_logo.png) | **LlamaV-o1 Project:** | [https://mbzuai-oryx.github.io/LlamaV-o1/](https://mbzuai-oryx.github.io/LlamaV-o1/) |\n| ![x2.png](https://arxiv.org/html/2501.06186/x2.png) | **LlamaV-o1 Model:** | [https://huggingface.co/omkarthawakar/LlamaV-o1](https://huggingface.co/omkarthawakar/LlamaV-o1) |\n| ![x3.png](https://arxiv.org/html/2501.06186/x3.png) | **LlamaV-o1 Code:** | [https://github.com/mbzuai-oryx/LlamaV-o1](https://github.com/mbzuai-oryx/LlamaV-o1) |\n| ![x2.png](https://arxiv.org/html/2501.06186/x2.png) | **VRC-Bench** | [https://huggingface.co/datasets/omkarthawakar/VRC-Bench](https://huggingface.co/datasets/omkarthawakar/VRC-Bench) |", "caption": "Table 1: \nAn overview of comprehensive set of attributes considered in our evaluation to assess the quality of reasoning in LMMs. These attributes focus on critical aspects such as faithfulness, informativeness, and logical coherence of reasoning steps. Key measures include ensuring alignment of reasoning steps with the source (Faithfulness-Step and Token), completeness of information (Informativeness-Step), and identifying issues like hallucinations, redundancy, or missing steps. Additional metrics, such as Semantic Coverage and Reasoning Alignment, evaluate the logical and semantic integrity of the response. Together, these metrics provide a robust framework for evaluating the accuracy, completeness, and reliability of LLM-generated reasoning.", "description": "\uc774 \ud45c\ub294 \ub300\uaddc\ubaa8 \ub2e4\uc911 \ubaa8\ub4dc \uc5b8\uc5b4 \ubaa8\ub378(LMM)\uc758 \ucd94\ub860 \ud488\uc9c8\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \uace0\ub824\ub41c \ud3ec\uad04\uc801\uc778 \uc18d\uc131\ub4e4\uc744 \uac1c\uad04\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc18d\uc131\ub4e4\uc740 \ucda9\uc2e4\uc131, \uc815\ubcf4\uc131, \ub17c\ub9ac\uc801 \uc77c\uad00\uc131\uacfc \uac19\uc740 \uc911\uc694\ud55c \uce21\uba74\uc5d0 \uc911\uc810\uc744 \ub461\ub2c8\ub2e4. \uc8fc\uc694 \uce21\uc815 \uc9c0\ud45c\uc5d0\ub294 \uc6d0\ubcf8\uacfc \ucd94\ub860 \ub2e8\uacc4\uc758 \uc815\ub82c \ud655\uc778(\ucda9\uc2e4\uc131-\ub2e8\uacc4 \ubc0f \ud1a0\ud070), \uc815\ubcf4\uc758 \uc644\uc804\uc131(\uc815\ubcf4\uc131-\ub2e8\uacc4), \ud658\uac01, \ubc18\ubcf5 \ub610\ub294 \ub204\ub77d\ub41c \ub2e8\uacc4\uc640 \uac19\uc740 \ubb38\uc81c \uc2dd\ubcc4\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4. \uc758\ubbf8 \ubc94\uc704 \ubc0f \ucd94\ub860 \uc815\ub82c\uacfc \uac19\uc740 \ucd94\uac00\uc801\uc778 \uc9c0\ud45c\ub294 \uc751\ub2f5\uc758 \ub17c\ub9ac\uc801 \ubc0f \uc758\ubbf8\uc801 \ubb34\uacb0\uc131\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uc885\ud569\uc801\uc73c\ub85c, \uc774\ub7ec\ud55c \uc9c0\ud45c\ub4e4\uc740 LLM\uc774 \uc0dd\uc131\ud55c \ucd94\ub860\uc758 \uc815\ud655\uc131, \uc644\uc804\uc131 \ubc0f \uc2e0\ub8b0\uc131\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \uac15\ub825\ud55c \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3 Step-by-Step Visual Reasoning Benchmark: VRC-Bench"}, {"content": "| Metric | Definition |\n|---|---| \n| `Faithfulness-Step` | Measures how well the reasoning steps in the LMM response align with the source reasoning steps. |\n| `Faithfulness-Token` | Extends Faithfulness-Step to token-level granularity, checking if the content within each step is accurate. |\n| `Informativeness-Step` | Measures how well the reasoning steps extract all relevant information from the context. |\n| `Repetition-Token` | Identifies repeated or unnecessarily paraphrased reasoning steps. |\n| `Hallucination` | Detects irrelevant or fabricated reasoning steps not aligned with the source. |\n| `Redundancy` | Identifies redundant reasoning steps that do not add value. |\n| `Semantic Coverage-Step` | Measures how well the response covers the essential semantic elements of the source reasoning steps. |\n| `Reasoning Alignment` | Overall alignment between the hypothesis and reference reasoning chain. |\n| `Commonsense` | Checks for missing commonsense reasoning are required to solve the problem. |\n| `Missing Step` | Identifies if any necessary reasoning steps are missing. |", "caption": "Table 2: Comparison of models based on Final Answer accuracy and Reasoning Steps performance on the proposed VRC-Bench. The best results in each case (closed-source and open-source) are in bold. Our LlamaV-o1 achieves superior performance compared to its open-source counterpart (Llava-CoT) while also being competitive against the closed-source models.", "description": "\ud45c 2\ub294 \uc81c\uc548\ub41c VRC-Bench(Visual Reasoning Chain Benchmark)\ub97c \uae30\ubc18\uc73c\ub85c \ucd5c\uc885 \ub2f5\ubcc0 \uc815\ud655\ub3c4\uc640 \ucd94\ub860 \ub2e8\uacc4 \uc131\ub2a5 \uce21\uba74\uc5d0\uc11c \uc5ec\ub7ec \ubaa8\ub378\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  '\ucd5c\uc885 \ub2f5\ubcc0 \uc815\ud655\ub3c4'\ub294 \ubaa8\ub378\uc774 \uc81c\uc2dc\ud55c \ucd5c\uc885 \ub2f5\ubcc0\uc774 \uc2e4\uc81c \uc815\ub2f5\uacfc \uc5bc\ub9c8\ub098 \uc77c\uce58\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 \uc9c0\ud45c\uc774\uba70, '\ucd94\ub860 \ub2e8\uacc4 \uc131\ub2a5'\uc740 \ubaa8\ub378\uc774 \ubb38\uc81c \ud574\uacb0 \uacfc\uc815\uc744 \uc5bc\ub9c8\ub098 \ub17c\ub9ac\uc801\uc774\uace0 \uccb4\uacc4\uc801\uc73c\ub85c \uc124\uba85\ud558\ub294\uc9c0\ub97c \ud3c9\uac00\ud558\ub294 \uc9c0\ud45c\uc785\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uc5ec\ub7ec \uc885\ub958\uc758 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uacfc \ub2e4\uc911 \ubaa8\ub2ec \ubaa8\ub378(LMM)\ub4e4\uc758 \uc131\ub2a5\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \ubaa8\ub378\uc758 \ucd5c\uc885 \ub2f5\ubcc0 \uc815\ud655\ub3c4\uc640 \ucd94\ub860 \ub2e8\uacc4 \uc131\ub2a5 \uc810\uc218\uac00 \ube44\uad50\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ub41c LlamaV-01 \ubaa8\ub378\uc740 \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378 \uc911\uc5d0\uc11c\ub294 \uac00\uc7a5 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\uba70, \uc0c1\uc6a9 \ubaa8\ub378\ub4e4\uacfc\ub3c4 \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0a9\ub2c8\ub2e4. \ud45c\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uc810\uc218\ub97c \uae30\ub85d\ud55c \ubaa8\ub378\uc740 \uad75\uc740 \uae00\uc528\uccb4\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5 Experiments"}, {"content": "| Model | Close-Source | Close-Source | Close-Source | Close-Source | Close-Source | Close-Source | Open-Source | Open-Source | Open-Source | Open-Source | Open-Source |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|  | GPT-4o | Claude-3.5 | Gemini-2.0 | Gemini-1.5 | Gemini-1.5 | GPT-4o | Llama-3.2 | Mulberry | Llava-CoT | LlamaV-o1 |\n|  | [2] | Sonnet [1] | Flash | Pro [52] | Flash [52] | mini [48] | Vision [47] | [68] | [66] | (Ours) |\n| Final Answer | 59.28 | 61.35 | 61.16 | 61.35 | 54.99 | 56.39 | 48.40 | 51.90 | 54.09 | 56.49 |\n| Steps | 76.68 | 72.12 | 74.08 | 72.12 | 71.86 | 74.05 | 58.37 | 63.86 | 66.21 | 68.93 |", "caption": "Table 3: Performance comparison on six benchmark datasets (MMStar\u00a0[9], MMBench\u00a0[35], MMVet\u00a0[71], MathVista\u00a0[39], AI2D\u00a0[29], and Hallusion\u00a0[21]) along with their average scores. The comparison includes both close-source and open-source models. The best performing close-source model is GPT-4o with an average score of 71.8%. Among open-source models, our proposed LlamaV-o1 achieves the best performance with an average score of 67.33% outperforming the recent Llava-CoT by 3.8%.", "description": "\ud45c 3\uc740 MMStar, MMBench, MMVet, MathVista, AI2D, Hallusion \ub4f1 \uc5ec\uc12f \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ube44\uad50 \ub300\uc0c1\uc5d0\ub294 \ub3c5\uc810 \ubaa8\ub378(closed-source model)\uacfc \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378(open-source model) \ubaa8\ub450 \ud3ec\ud568\ub429\ub2c8\ub2e4.  \ud3c9\uade0 \uc810\uc218 \uae30\uc900\uc73c\ub85c, GPT-4o\uac00 \ub3c5\uc810 \ubaa8\ub378 \uc911 \ucd5c\uace0 \uc131\ub2a5(71.8%)\uc744 \ub2ec\uc131\ud588\uace0, \uc81c\uc548\ub41c LlamaV-01 \ubaa8\ub378\uc740 \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378 \uc911 \ucd5c\uace0 \uc131\ub2a5(67.33%)\uc744 \uae30\ub85d\ud558\uc5ec \ucd5c\uadfc\uc758 Llava-CoT \ubaa8\ub378\ubcf4\ub2e4 3.8% \uc55e\uc130\uc2b5\ub2c8\ub2e4.", "section": "5.2 \uacb0\uacfc \ubc0f \ub17c\uc758"}, {"content": "| Model | MMStar | MMBench | MMVet | MathVista | AI2D | Hallusion | Average |\n|---|---|---|---|---|---|---|---| \n| **Close-Source** |  |  |  |  |  |  |  |\n| GPT-4o-0806 [2] | 66.0 | 82.4 | 80.8 | 62.7 | 84.7 | 54.2 | 71.8 |\n| Claude3.5-Sonnet-0620 [1] | 64.2 | 75.4 | 68.7 | 61.6 | 80.2 | 49.9 | 66.7 |\n| Gemini-1.5-Pro [52] | 56.4 | 71.5 | 71.3 | 57.7 | 79.1 | 45.6 | 63.6 |\n| GPT-4o-mini-0718 [48] | 54.9 | 76.9 | 74.6 | 52.4 | 77.8 | 46.1 | 63.8 |\n| **Open-Source** |  |  |  |  |  |  |  |\n| InternVL2-8B [10] | 62.50 | 77.40 | 56.90 | 58.30 | 83.60 | 45.00 | 64.00 |\n| Ovis1.5-Gemma2-9B [41] | 58.70 | 76.30 | 50.90 | 65.60 | 84.50 | 48.20 | 64.00 |\n| MiniCPM-V2.6-8B [70] | 57.10 | 75.70 | 56.30 | 60.60 | 82.10 | 48.10 | 63.30 |\n| Llama-3.2-90B-Vision-Inst [47] | 51.10 | 76.80 | 74.10 | 58.30 | 69.50 | 44.10 | 62.30 |\n| VILA-1.5-40B [36] | 53.20 | 75.30 | 44.40 | 49.50 | 77.80 | 40.90 | 56.90 |\n| Mulberry-7B [68] | 61.30 | 75.34 | 43.90 | 57.49 | 78.95 | 54.10 | 62.78 |\n| Llava-CoT [66] | 57.60 | 75.00 | 60.30 | 54.80 | 85.70 | 47.80 | 63.50 |\n| **Our Models** |  |  |  |  |  |  |  |\n| Llama-3.2-11B-Vision-Inst [47] (baseline) | 49.80 | 65.80 | 57.60 | 48.60 | 77.30 | 40.30 | 56.90 |\n| **LlamaV-o1 (Ours)** | 59.53 | 79.89 | 65.40 | 54.40 | 81.24 | 63.51 | 67.33 |", "caption": "Table 4: \nImpact of our proposed contributions on multimodal reasoning tasks across six benchmarks: MMStar, MMBench, MMVet, MathVista, AI2D, and Hallusion. Starting with Curriculum Learning combined with Multi-Step CoT reasoning (2nd row), the model achieves a 9.14% absolute gain compared to base model Llama-3.2-11B-Vision-Inst\u00a0[47], demonstrating its ability to handle complex multi-step reasoning effectively. This baseline approach leverages structured training to improve performance across diverse tasks, including logical reasoning and visual understanding. By incorporating Beam Search, the model\u2019s performance further improves (3rd row). This enhancement is particularly noticeable in benchmarks such as MMVet (65.40% vs. 61.88%), MathVista (54.40% vs. 53.20%), and AI2D (81.24% vs. 80.18%), showcasing the model\u2019s ability to generalize better with more accurate reasoning.\nOur final approach that combines curriculum learning with optimized inference strategies achieves an overall average gain of 10.43%, compared to the baseline.", "description": "\ud45c 4\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \uae30\uc5ec\uac00 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c(MMStar, MMBench, MMVet, MathVista, AI2D, Hallusion)\uc5d0\uc11c \ub2e4\uc911 \ubaa8\ub4dc \ucd94\ub860 \uc791\uc5c5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uba3c\uc800 \ucee4\ub9ac\ud058\ub7fc \ud559\uc2b5\uacfc \ub2e4\ub2e8\uacc4 CoT \ucd94\ub860\uc744 \uacb0\ud569\ud55c \uae30\uc900 \ubaa8\ub378(Llama-3.2-11B-Vision-Inst [47])\uacfc \ube44\uad50\ud558\uc5ec 9.14%\uc758 \uc808\ub300\uc801\uc778 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ubcf5\uc7a1\ud55c \ub2e4\ub2e8\uacc4 \ucd94\ub860\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ucc98\ub9ac\ud558\ub294 \ubaa8\ub378\uc758 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uae30\uc900 \ubc29\ubc95\uc740 \ub17c\ub9ac\uc801 \ucd94\ub860\uacfc \uc2dc\uac01\uc801 \uc774\ud574\ub97c \ud3ec\ud568\ud55c \ub2e4\uc591\ud55c \uc791\uc5c5\uc5d0\uc11c \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uae30 \uc704\ud574 \uad6c\uc870\ud654\ub41c \ud6c8\ub828\uc744 \ud65c\uc6a9\ud569\ub2c8\ub2e4.  \ube54 \uc11c\uce58\ub97c \ud1b5\ud569\ud558\uc5ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub354\uc6b1 \ud5a5\uc0c1\uc2dc\ucf30\uc2b5\ub2c8\ub2e4 (3\ubc88\uc9f8 \ud589). \uc774\ub7ec\ud55c \ud5a5\uc0c1\uc740 MMVet(65.40% \ub300 61.88%), MathVista(54.40% \ub300 53.20%), AI2D(81.24% \ub300 80.18%)\uc640 \uac19\uc740 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ud2b9\ud788 \ub450\ub4dc\ub7ec\uc9c0\uac8c \ub098\ud0c0\ub0ac\uc73c\uba70, \ub354\uc6b1 \uc815\ud655\ud55c \ucd94\ub860\uc73c\ub85c \uc77c\ubc18\ud654 \ub2a5\ub825\uc774 \ud5a5\uc0c1\ub418\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ucee4\ub9ac\ud058\ub7fc \ud559\uc2b5\uacfc \ucd5c\uc801\ud654\ub41c \ucd94\ub860 \uc804\ub7b5\uc744 \uacb0\ud569\ud55c \ucd5c\uc885 \ubc29\ubc95\uc740 \uae30\uc900 \ubc29\ubc95\uacfc \ube44\uad50\ud558\uc5ec 10.43%\uc758 \uc804\ubc18\uc801\uc778 \ud3c9\uade0 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.", "section": "4 Proposed Step-by-Step Visual Reasoning Model: LlamaV-01"}, {"content": "| Model | MMStar | MMBench | MMVet | MathVista | AI2D | Hallusion | Average |\n|---|---|---|---|---|---|---|---| \n| Llama-3.2-11B-Vision-Inst (_baseline_) | 49.80 | 65.80 | 57.60 | 48.60 | 77.30 | 40.30 | 56.90 |\n| + Curriculum with Multi-Step CoT Reasoning | 58.13 | 79.55 | 61.88 | 53.20 | 80.18 | 63.31 | 66.04 |\n| + Beam Search | 59.53 | 79.89 | 65.40 | 54.40 | 81.24 | 63.51 | 67.33 |", "caption": "Table 5: \nComparison of inference scaling techniques on the MMVet benchmark, evaluated using a single NVIDIA A100 GPU.\nLeft: Llava-CoT with stage-level beam search shows improved MMVet scores with more beams but suffers from quadratic scaling, significantly increasing inference time.\nRight: Performance of our approach utilizing Beam Search achieving higher MMVet scores with much lower inference time, due to its linear scaling efficiency.\nFor instance, our method scores 65.40 with four beams in 6.1 GPU hours, compared to Llava-CoT\u2019s 62.9 score requiring 46.1 GPU hours. This demonstrates the efficiency and practicality of our approach for real-world applications.", "description": "\ud45c 5\ub294 \ub2e8\uc77c NVIDIA A100 GPU\ub97c \uc0ac\uc6a9\ud558\uc5ec MMVet \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ucd94\ub860 \ud655\uc7a5 \uae30\ubc95\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc740 \ub2e8\uacc4\ubcc4 \ube54 \uac80\uc0c9\uc744 \uc0ac\uc6a9\ud558\ub294 Llava-CoT\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube54 \uac1c\uc218\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c MMVet \uc810\uc218\uac00 \ud5a5\uc0c1\ub418\uc9c0\ub9cc, \uc774\ucc28\uc801 \ud655\uc7a5\uc73c\ub85c \uc778\ud574 \ucd94\ub860 \uc2dc\uac04\uc774 \ud06c\uac8c \uc99d\uac00\ud569\ub2c8\ub2e4. \uc624\ub978\ucabd\uc740 \ube54 \uac80\uc0c9\uc744 \uc0ac\uc6a9\ud558\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc120\ud615 \ud655\uc7a5 \ud6a8\uc728 \ub355\ubd84\uc5d0 \ud6e8\uc52c \uc9e7\uc740 \ucd94\ub860 \uc2dc\uac04\uc73c\ub85c \ub354 \ub192\uc740 MMVet \uc810\uc218\ub97c \ub2ec\uc131\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uc81c\uc548\ub41c \ubc29\ubc95\uc740 6.1 GPU \uc2dc\uac04\uc5d0 4\uac1c\uc758 \ube54\uc73c\ub85c 65.40\uc810\uc744 \ubc1b\uc558\uc9c0\ub9cc, Llava-CoT\ub294 46.1 GPU \uc2dc\uac04\uc5d0 62.9\uc810\uc744 \ubc1b\uc558\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc2e4\uc81c \uc751\uc6a9 \ud504\ub85c\uadf8\ub7a8\uc5d0 \ub300\ud55c \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \ud6a8\uc728\uc131\uacfc \uc2e4\uc6a9\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2.3 Optimizing Inference Efficiency: Beam Search"}]