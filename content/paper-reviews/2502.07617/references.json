{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a highly influential model that leverages contrastive learning on a massive dataset to learn powerful visual representations from natural language supervision, which is foundational to many vision-language models."}, {"fullname_first_author": "X. Chen", "paper_title": "Pali: A jointly-scaled multilingual language-image model", "publication_date": "2022-09-01", "reason": "This paper introduces PaLI, a significant step forward in vision-language models, demonstrating the benefits of joint scaling of multilingual data and model size for improved performance."}, {"fullname_first_author": "J. Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-01", "reason": "This paper establishes scaling laws for neural language models, providing a theoretical framework for understanding the relationship between model size, dataset size, and performance, which directly applies to VLMs."}, {"fullname_first_author": "I. Alabdulmohsin", "paper_title": "Revisiting neural scaling laws in language and vision", "publication_date": "2022-12-01", "reason": "This paper extends scaling laws to vision-language models, empirically demonstrating the importance of data size in improving model performance and providing justification for this study's large-scale investigation."}, {"fullname_first_author": "J. Hestness", "paper_title": "Deep learning scaling is predictable, empirically", "publication_date": "2017-12-01", "reason": "This paper presents early work on scaling laws in deep learning, showing a predictable relationship between model size, dataset size, and compute, which is a foundational concept for understanding scaling behavior in VLMs."}]}