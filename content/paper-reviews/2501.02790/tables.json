[{"content": "| Action |  | AlpacaEval 2.0 |  | Arena-Hard |  | MT-Bench |\n|---|---|---|---|---|---|---|---|\n|  |  | LC(%) | WR(%) | # char |  | WR% | # token |  | GPT-4o |\n|---|---|---|---|---|---|---|---|---|---|\n| Phi3-mini Instruct |  | 18.89 | 14.41 | 1473 |  | 25.1 | 490 |  | 7.33 |\n| Bandit (Sequence) |  | 27.05 | 29.07 | 2164 |  | 31.3 | 623 |  | 7.46 |\n| Sentence |  | 25.56 | 32.92 | 2626 |  | 32.8 | 671 |  | 7.51 |\n| Token |  | 27.82 | 26.46 | 1940 |  | 27.2 | 533 |  | 7.58 |\n| Segment (Ours) |  | 31.05 | 34.53 | 2257 |  | 34.0 | 593 |  | 7.65 |\n| Bandit as Segment |  | 14.39 | 6.46 | 691 |  | 11.1 | 308 |  | 6.61 |\n| Segment as Bandit |  | 27.15 | 28.20 | 2079 |  | 30.9 | 620 |  | 7.38 |", "caption": "Table 1: \nPerformance comparison among different action definitions on PPO-trained LM policy, with the backbone model being Phi3-mini Instruct.\n# {char, token} measures the average response length in the benchmark tests.\nHighest value of each column is in bold.", "description": "\ubcf8 \ud45c\ub294 \ubc31\ubcf8 \ubaa8\ub378\ub85c Phi3-mini Instruct\ub97c \uc0ac\uc6a9\ud558\uc5ec PPO\ub85c \ud559\uc2b5\ub41c \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \ud589\ub3d9 \uc815\uc758\uc5d0 \ub530\ub77c \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud589\ub3d9 \uc815\uc758(\ub2e8\uc5b4, \ubb38\uc7a5, \uad6c\uc808, \uc804\uccb4 \uc2dc\ud000\uc2a4)\uc5d0 \ub530\ub978 \uc138 \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c(AlpacaEval 2.0, Arena-Hard, MT-Bench)\uc5d0\uc11c\uc758 \uc131\ub2a5(LC, WR, \ubb38\uc790/\ud1a0\ud070 \uc218)\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. \uac01 \uc5f4\uc758 \ucd5c\uace0\uac12\uc740 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ubcf8 \ud45c\ub294 \ub2e4\uc591\ud55c \ud589\ub3d9 \ub2e8\uc704\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c  PPO \uae30\ubc18\uc758 \uc5b8\uc5b4 \ubaa8\ub378 \ud559\uc2b5 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc911\uc810\uc744 \ub461\ub2c8\ub2e4.", "section": "4.2 \uc8fc\uc694 \uc2e4\ud5d8\uc801 \ube44\uad50"}, {"content": "| Action | Definition | AlpacaEval 2.0 LC (%) | AlpacaEval 2.0 WR (%) | AlpacaEval 2.0 # char |  | Arena-Hard WR (%) | Arena-Hard # token |  | MT-Bench GPT-4o | \n|---|---|---|---|---|---|---|---|---|---|\n| Phi3.1-mini SFT |  | 14.93 | 10.19 | 1271 |  | 14.5 | 476 |  | 7.00 | \n| Bandit (Sequence) |  | 19.39 | 14.78 | 1542 |  | 19.5 | 524 |  | 7.26 | \n| Token |  | 22.48 | 19.25 | 1687 |  | 23.2 | 525 |  | 7.43 | \n| Segment (**Ours**) |  | **26.19** | **23.85** | **1795** |  | **28.5** | **585** |  | **7.49** | ", "caption": "Table 2: \nPerformance comparison among different action definitions on PPO-trained LM policy, with the backbone model being the 3.8B SFT checkpoint of Phi3.1-mini Instruct. Table format follows Table\u00a01.", "description": "\ud45c 2\ub294 \uae30\ubcf8 \ubaa8\ub378\ub85c Phi3.1-mini Instruct\uc758 3.8B SFT \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uc0ac\uc6a9\ud558\uc5ec PPO\ub85c \ud559\uc2b5\ub41c \uc5b8\uc5b4 \ubaa8\ub378 \uc815\ucc45\uc5d0\uc11c \uc11c\ub85c \ub2e4\ub978 \ud589\ub3d9 \uc815\uc758 \uac04\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\uc758 \ud615\uc2dd\uc740 \ud45c 1\uacfc \ub3d9\uc77c\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud589\ub3d9 \uc815\uc758(\uc804\uccb4 \uc2dc\ud000\uc2a4, \ubb38\uc7a5, \ud1a0\ud070, \uc138\uadf8\uba3c\ud2b8)\uc5d0 \ub530\ub978 PPO \ud559\uc2b5\ub41c LM \uc815\ucc45\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uc81c\uc548\ub41c \uc138\uadf8\uba3c\ud2b8 \uc218\uc900 \ubc29\ubc95\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  AlpacaEval 2.0, Arena-Hard, MT-Bench \uc138 \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \uc81c\uc2dc\ud558\uace0 \uac01 \ubca4\uce58\ub9c8\ud06c\uc758 \ud3c9\uade0 \uc751\ub2f5 \uae38\uc774\ub97c \uce21\uc815\ud569\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8"}, {"content": "| Action | Definition | AlpacaEval 2.0 LC (%) | AlpacaEval 2.0 WR (%) | AlpacaEval 2.0 # char |  | Arena-Hard WR (%) | Arena-Hard # token |  | MT-Bench GPT-4o |\n|---|---|---|---|---|---|---|---|---|---|\n| Llama-3-8B-SFT |  | 16.31 | 9.50 | 1221 |  | 10.4 | 469 |  | 6.82 |\n| Bandit (Sequence) |  | 21.20 | 20.99 | 2218 |  | 18.7 | 513 |  | 7.11 |\n| Token |  | 23.84 | 20.87 | 1744 |  | 26.0 | 622 |  | 7.13 |\n| Segment (Ours) |  | **25.11** | **28.57** | **2264** |  | **30.4** | 616 |  | **7.15** |", "caption": "Table 3: \nPerformance comparison among different action definitions on PPO-trained LM policy, with the backbone model being the 8B SFT checkpoint of Llama-3 released by RLHFlow. Table format follows Table\u00a01.", "description": "\ubcf8 \ud45c\ub294 RLHFlow\uc5d0\uc11c \uacf5\uac1c\ud55c Llama-3\uc758 8B SFT \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \ubc31\ubcf8 \ubaa8\ub378\ub85c \uc0ac\uc6a9\ud558\uc5ec PPO\ub85c \ud559\uc2b5\ub41c \uc5b8\uc5b4 \ubaa8\ub378 \uc815\ucc45\uc5d0\uc11c \uc11c\ub85c \ub2e4\ub978 \ud589\ub3d9 \uc815\uc758 \uac04\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\uc5d0\ub294  AlpacaEval 2.0, Arena-Hard, MT-Bench \uc138 \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758  \uc815\ud655\ub3c4(LC, WR)\uc640 \uc751\ub2f5 \uae38\uc774(# char, # token)\uac00 \ud3ec\ud568\ub429\ub2c8\ub2e4.  \ub2e4\ub978 \ud589\ub3d9 \uc815\uc758(Bandit, Sentence, Token)\uc640 \ube44\uad50\ud558\uc5ec \ubcf8 \uc5f0\uad6c\uc5d0\uc11c \uc81c\uc548\ud55c Segment \uae30\ubc18 \ubc29\uc2dd\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4. \ud45c\uc758 \ud615\uc2dd\uc740 \ud45c 1\uacfc \ub3d9\uc77c\ud569\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8"}, {"content": "| Reward                     | AlpacaEval 2.0          |             |                | Arena-Hard                 |             |                | MT-Bench   |\n| :-------------------------- | :----------------------- | :---------- | :------------ | :-------------------------- | :---------- | :------------ | :---------- |\n|                             | LC (%)                    | WR (%)      | # char       | WR (%)                    | # token    |                | GPT-4o     |\n| No Reward Normalization     | 19.64                    | 24.10      | 2446         | 29.9                     | 625        |                | 7.25       |\n| Global Statistics of All    | 17.34                    | 22.11      | 2420         | 31.3                     | 639        |                | 7.14       |\n| Statistics of the Last Rewards | 20.30                    | 24.72      | 2551         | 29.2                     | 641        |                | 7.10       |\n| Regression-based (Section 2.3) | **31.05**                | **34.53**  | 2257         | **34.0**                  | 593        |                | **7.65**   |", "caption": "Table 4: \nComparison of different constructions of segment-level reward normalizers, on performance of the resulted PPO-trained LM policies.\nHighest numeric of each metric is in bold.", "description": "\ubcf8 \ud45c\ub294 \ub2e4\uc591\ud55c \ubc29\ubc95\uc73c\ub85c \uad6c\uc131\ub41c \uc138\uadf8\uba3c\ud2b8 \uc218\uc900\uc758 \ubcf4\uc0c1 \uc815\uaddc\ud654\uae30\ub97c \uc0ac\uc6a9\ud558\uc5ec PPO\ub85c \ud559\uc2b5\ub41c \uc5b8\uc5b4 \ubaa8\ub378 \uc815\ucc45\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, \ubcf4\uc0c1 \uc815\uaddc\ud654\ub97c \ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0, \uc804\uccb4 \ubcf4\uc0c1\uc758 \uc804\uc5ed \ud1b5\uacc4\ub7c9\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0, \ub9c8\uc9c0\ub9c9 \ubcf4\uc0c1\uc758 \ud1b5\uacc4\ub7c9\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0, \uadf8\ub9ac\uace0 \ud68c\uadc0 \uae30\ubc18 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0\uc758 \ub124 \uac00\uc9c0 \ubc29\ubc95\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. \uac01 \uc9c0\ud45c(AlpacaEval 2.0\uc758 LC, WR, \ubb38\uc790 \uc218, Arena-Hard\uc758 WR, \ud1a0\ud070 \uc218, MT-Bench\uc758 GPT-40 \uc810\uc218)\uc5d0 \ub300\ud55c \ucd5c\uace0\uc810\uc740 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.3 Ablation Study"}, {"content": "| Interpolation Strategy | AlpacaEval 2.0 |  | Arena-Hard |  | MT-Bench |\n|---|---|---|---|---|---|---|\n|  | LC (%) | WR (%) | # char |  | WR (%) | # token | GPT-4o |\n| No Interpolation | 25.98 | 34.53 | 2666 |  | 39.1 | 675 |  |\n| Repeat Segment Reward | 26.34 | 23.48 | 1795 |  | 23.0 | 512 |  |\n| Even Split (Section 2.3) | **31.05** | **34.53** | 2257 |  | 34.0 | 593 | **7.65** |", "caption": "Table 5: \nComparison of different within-segment reward interpolation strategies. Shown are the results of the resulted PPO-trained LM policies on AlpacaEval 2.0 and Arena-Hard.\nHighest numeric of each metric is in bold.", "description": "\ud45c 5\ub294 \uc138\uadf8\uba3c\ud2b8 \uc218\uc900\uc758 \ubcf4\uc0c1 \ubcf4\uac04 \uc804\ub7b5\ub4e4\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. PPO\ub85c \ud559\uc2b5\ub41c \uc5b8\uc5b4 \ubaa8\ub378 \uc815\ucc45\uc758 AlpacaEval 2.0 \ubc0f Arena-Hard \uc131\ub2a5 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc9c0\ud45c\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uc218\uce58\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc138\uadf8\uba3c\ud2b8 \ub0b4 \ubcf4\uc0c1 \ubcf4\uac04 \ubc29\ubc95\uc774  PPO \uae30\ubc18 \uc5b8\uc5b4 \ubaa8\ub378 \ud559\uc2b5 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\uae30 \uc704\ud574 \ub2e4\uc591\ud55c \ubcf4\uc0c1 \ubcf4\uac04 \uae30\ubc95\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "4.3 Ablation Study"}, {"content": "| Action Definition | ARC | TruthfulQA | Winograd | HellaSwag | MMLU | GSM8K | Average |\n|---|---|---|---|---|---|---|---| \n| Phi-Instruct | **64.76** | 54.44 | 74.51 | 79.03 | 70.41 | 81.6 | 70.79 |\n| Bandit (Sequence) | **64.76** | **55.11** | 74.35 | 79.32 | 70.42 | 77.8 | 70.29 |\n| Sentence | 63.40 | 53.99 | 72.93 | 79.34 | 70.42 | 84.1 | 70.70 |\n| Token | 62.71 | 53.94 | 71.43 | **79.46** | **70.55** | **87.3** | 70.90 |\n| Segment (**Ours**) | 62.71 | 54.74 | 72.06 | 79.23 | 70.42 | 86.7 | **70.98** |\n| Bandit as Segment | 64.16 | 54.62 | 74.66 | 78.95 | **70.55** | 81.0 | 70.66 |\n| Segment as Bandit | 64.33 | 54.81 | **74.74** | 79.23 | 70.39 | 78.6 | 70.35 |", "caption": "Table 6: \nEvaluation results of downstream tasks on the HuggingFace OpenLLM Leaderboard (Beeching et\u00a0al., 2023), comparing LM policies in\nTable\u00a01.", "description": "\ubcf8 \ud45c\ub294 HuggingFace OpenLLM \ub9ac\ub354\ubcf4\ub4dc(Beeching et al., 2023)\uc758 \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c 1\uc758 LM \uc815\ucc45\ub4e4\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.  \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \ud589\ub3d9 \uc815\uc758(action definition)\uc5d0 \ub530\ub77c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec  \uac01 \uc815\ucc45\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c\ub294  Phi-Instruct \uae30\ubcf8 \ubaa8\ub378\uacfc Bandit(Sequence), Sentence, Token, Segment(\ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548), Bandit as Segment, Segment as Bandit \ub4f1 6\uac00\uc9c0 \ubc29\ubc95\uc758  ARC, TruthfulQA, Winograd, HellaSwag, MMLU, GSM8K  \ub4f1 \ub2e4\uc591\ud55c \uc9c0\ud45c\uc5d0 \ub300\ud55c \ud3c9\uac00 \uc810\uc218\ub97c \ube44\uad50\ud558\uc5ec \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uc81c\uc548\ub41c Segment \ubc29\ubc95\uc758 \ud6a8\uacfc\uc131\uc744 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.2 \uc8fc\uc694 \uc2e4\ud5d8\uc801 \ube44\uad50"}]