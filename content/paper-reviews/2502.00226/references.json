{"references": [{"fullname_first_author": "Zhaojian Yu", "paper_title": "HumanEval pro and mbpp pro: Evaluating large language models on self-invoking code generation", "publication_date": "2024-12-21", "reason": "This paper is a benchmark for evaluating large language models' code generation capabilities, providing a basis of comparison for the current work."}, {"fullname_first_author": "Carlos E. Jimenez", "paper_title": "SWE-bench: Can language models resolve real-world github issues?", "publication_date": "2023-10-06", "reason": "This paper introduces a benchmark using real-world GitHub issues to evaluate large language models' ability to solve real-world problems, influencing the design and scope of the current benchmark."}, {"fullname_first_author": "John Yang", "paper_title": "SWE-bench multimodal: Do AI systems generalize to visual software domains?", "publication_date": "2024-10-03", "reason": "This paper expands the SWE-bench benchmark to include multimodal tasks, demonstrating the trend towards more comprehensive and real-world evaluations of LLMs, which the current work aims to further advance."}, {"fullname_first_author": "Jia Li", "paper_title": "DevEval: A manually-annotated code generation benchmark aligned with real-world code repositories", "publication_date": "2024-05-19", "reason": "This paper provides a benchmark for multi-file coding tasks, which directly informs the design choices for the complexity and structure of tasks in the current benchmark."}, {"fullname_first_author": "Baizhou Huang", "paper_title": "Enhancing large language models in coding through multi-perspective self-consistency", "publication_date": "2024-09-17", "reason": "This paper highlights the importance of self-consistency in code generation, which is a key focus of the current work's evaluation metrics and methodology."}]}