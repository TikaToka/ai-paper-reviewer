{"references": [{"fullname_first_author": "Gabriel Alon", "paper_title": "Detecting language model attacks with perplexity", "publication_date": "2023-08-14", "reason": "This paper proposes a defense mechanism against jailbreaking attacks by detecting abnormal inputs through perplexity filtering, a key concern addressed in the main paper."}, {"fullname_first_author": "Moustafa Farid Alzantot", "paper_title": "Generating natural language adversarial examples", "publication_date": "2018-04-07", "reason": "This paper is foundational to the concept of adversarial attacks on NLP models, providing a basis for understanding the methods explored in the main paper for jailbreaking."}, {"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-05-14", "reason": "This paper introduces the concept of in-context learning, a technique relevant to the main paper's approach to jailbreaking through prompt engineering."}, {"fullname_first_author": "Andy Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-15", "reason": "This paper introduces the concept of universal adversarial attacks, which is directly relevant to the main paper's goal of developing a universal jailbreaking method."}, {"fullname_first_author": "Daniel M. Ziegler", "paper_title": "Fine-tuning language models from human preferences", "publication_date": "2019-09-08", "reason": "This paper discusses safety alignment techniques in LLMs, which is crucial to understanding the challenges and motivations behind jailbreaking research, a central theme of the main paper."}]}