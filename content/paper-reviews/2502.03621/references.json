{"references": [{"fullname_first_author": "Omer Bar-Tal", "paper_title": "Lumiere: A Space-Time Diffusion Model for Video Generation", "publication_date": "2024-01-12", "reason": "This paper introduces a novel space-time diffusion model for video generation, which is directly relevant to the DynVFX method's core technology."}, {"fullname_first_author": "Wenyi Hong", "paper_title": "Cogvideo: Large-scale pretraining for text-to-video generation via transformers", "publication_date": "2022-05-15", "reason": "This paper introduces the CogVideo model, a large-scale pretrained text-to-video generation model that forms the foundation of DynVFX's approach."}, {"fullname_first_author": "Zhuoyi Yang", "paper_title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer", "publication_date": "2024-08-06", "reason": "This paper introduces CogVideoX, a significant improvement on the original CogVideo model, directly used by DynVFX for generating high-quality videos."}, {"fullname_first_author": "Shelly Sheynin", "paper_title": "Emu Edit: Precise Image Editing via Recognition and Generation Tasks", "publication_date": "2023-00-00", "reason": "This paper provides a foundation for instruction-based image editing, a concept that is conceptually related to DynVFX's video editing approach."}, {"fullname_first_author": "Yoad Tewel", "paper_title": "Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models", "publication_date": "2024-11-07", "reason": "This paper presents a training-free method for object insertion in images, which shares a similar goal with DynVFX but operates in the image domain."}]}