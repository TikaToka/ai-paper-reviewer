<table id='5' style='font-size:14px'><tr><td>Hyper-parameters</td><td>MPTrj training</td><td>OMat training</td><td>MPTrj Fine-tuning</td><td>MPTrj+sAlex Fine-tuning</td></tr><tr><td>Optimizer</td><td>AdamW</td><td>AdamW</td><td>AdamW</td><td>AdamW</td></tr><tr><td>Learning rate scheduling</td><td>Cosine</td><td>Cosine</td><td>Cosine</td><td>Cosine</td></tr><tr><td>Warmup epochs</td><td>0.1</td><td>0.01</td><td>0.1</td><td>0.1</td></tr><tr><td>Warmup factor</td><td>0.2</td><td>0.2</td><td>0.2</td><td>0.2</td></tr><tr><td>Maximum learning rate</td><td>2 x 10-4</td><td>2 X 10-4</td><td>2 x 10-4</td><td>2 x 10-4</td></tr><tr><td>Minimum learning rate factor</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td>Batch size</td><td>512</td><td>512</td><td>256</td><td>256</td></tr><tr><td>Number of epochs</td><td>150</td><td>2</td><td>32</td><td>8</td></tr><tr><td>Gradient clipping norm threshold</td><td>100</td><td>100</td><td>100</td><td>100</td></tr><tr><td>Model EMA decay</td><td>0.999</td><td>0.999</td><td>0.999</td><td>0.999</td></tr><tr><td>Weight decay</td><td>1 x 10-3</td><td>1 X 10-3</td><td>1 x 10-3</td><td>1 x 10-3</td></tr><tr><td>Dropout rate</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td></tr><tr><td>Stochastic depth</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td></tr><tr><td>Energy loss coefficient</td><td>20</td><td>20</td><td>20</td><td>20</td></tr><tr><td>Force loss coefficient</td><td>20</td><td>20</td><td>10</td><td>10</td></tr><tr><td>Stress loss coefficient</td><td>5</td><td>5</td><td>1</td><td>1</td></tr><tr><td>DeNS settings</td><td></td><td></td><td></td><td></td></tr><tr><td>Probability of optimizing DeNS</td><td>0.5</td><td>0.25</td><td></td><td>-</td></tr><tr><td>Standard deviation of Gaussian noise</td><td>0.1</td><td>0.1</td><td></td><td></td></tr><tr><td>DeNS loss coefficient</td><td>10</td><td>10</td><td>-</td><td>-</td></tr></table>