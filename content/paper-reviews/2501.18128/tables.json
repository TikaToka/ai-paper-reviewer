[{"content": "| Name | Year | #Sources | Train Set | Val Set | Test Set |\n|---|---|---|---|---|---| \n| CNN/DM [21, 22, 25] | 2015/2016 | 2 | 287,113 | 13,368 | 11,490 |\n| Newsroom [23] | 2018 | 38 | 995,041 | 108,837 | 108,862 |\n| XSum [24, 26] | 2018 | 1 | 204,045 | 11,332 | 11,334 |", "caption": "Table 1: General information about the popular datasets in the field of News Summarization.", "description": "\ubcf8 \ud45c\ub294 \ub274\uc2a4 \uc694\uc57d \ubd84\uc57c\uc5d0\uc11c \ub110\ub9ac \uc0ac\uc6a9\ub418\ub294 \uc138 \uac00\uc9c0 \ub370\uc774\ud130\uc14b(CNN/Daily Mail, Newsroom, XSum)\uc5d0 \ub300\ud55c \uc77c\ubc18\uc801\uc778 \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc758 \ucd9c\ucc98, \uc791\uc131 \uc5f0\ub3c4, \ub274\uc2a4 \uae30\uc0ac \uc218(\ud6c8\ub828, \uac80\uc99d, \ud14c\uc2a4\ud2b8 \uc138\ud2b8\ub85c \uad6c\ubd84), \uadf8\ub9ac\uace0 \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ucd94\uac00 \uc815\ubcf4\ub97c \ub2f4\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \uaddc\ubaa8\uc640 \ud2b9\uc131\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "3. Experimental setup"}, {"content": "| Model Name | Creator | #Parameters | Context Window | Public |\n|---|---|---|---|---|\n| Gemini-1.5-Pro-0409 [28] | Google | - | 128K | \u2717 |\n| Gemma-2B [29] | Google | 2B | 8K | \u2713 |\n| Gemma-7B [29] | Google | 7B | 8K | \u2713 |\n| GPT-3.5-Turbo-0613 [30] | OpenAI | - | 4K | \u2717 |\n| GPT-4-0125-preview [31] | OpenAI | - | 8K | \u2717 |\n| Llama-2-7b-hf [32] | Meta | 7B | 4K | \u2713 |\n| Meta-Llama-3-8B [33] | Meta | 8B | 8K | \u2713 |\n| Meta-Llama-3-8B-Instruct [33] | Meta | 8B | 8K | \u2713 |\n| Mistral-7B-v0.1 [34] | Mistral AI | 7B | 4K | \u2713 |\n| Mistral-7B-Instruct-v0.1 [34] | Mistral AI | 7B | 4K | \u2713 |\n| Phi-3-Mini-4K-Instruct [35] | Microsoft | 3.8B | 4K | \u2713 |\n| Qwen1.5-0.5B [36] | Alibaba Cloud | 620M | 32K | \u2713 |\n| Qwen1.5-1.8B [36] | Alibaba Cloud | 1.8B | 32K | \u2713 |\n| Qwen1.5-4B [36] | Alibaba Cloud | 4B | 32K | \u2713 |\n| Qwen1.5-7B [36] | Alibaba Cloud | 7B | 32K | \u2713 |\n| SOLAR-10.7B-v1.0 [37] | Upstage | 10.7B | 4K | \u2713 |\n| SOLAR-10.7B-Instruct-v1.0 [37] | Upstage | 10.7B | 4K | \u2713 |\n| Yi-6B [38] | 01.AI | 6B | 4K | \u2713 |\n| Yi-9B [38] | 01.AI | 9B | 4K | \u2713 |\n| Zephyr-7B-Beta [39] | Hugging Face | 7B | 4K | \u2713 |", "caption": "Table 2: List of the selected language models.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c 20\uac1c\uc758 \uc5b8\uc5b4 \ubaa8\ub378 \ubaa9\ub85d\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc774\ub984, \ub9e4\uac1c\ubcc0\uc218 \uc218, \uacf5\uac1c \uc5ec\ubd80 \ubc0f \ucd5c\ub300 \ucee8\ud14d\uc2a4\ud2b8 \ucc3d \ud06c\uae30\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378 \ud06c\uae30, \uacf5\uac1c \uc5ec\ubd80 \ubc0f \ucee8\ud14d\uc2a4\ud2b8 \ucc3d \ud06c\uae30\ub294 \ubaa8\ub378 \uc120\ud0dd\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc911\uc694\ud55c \uc694\uc18c\uc785\ub2c8\ub2e4.", "section": "3. Experimental setup"}, {"content": "| Model Name | ROUGE-L | BERTScore | METEOR | Relevance | Faithfulness | Coherence | Relevance | Faithfulness | Coherence |\n|---|---|---|---|---|---|---|---|---|---| \n| Gemini-1.5-Pro | 0.189 | 0.866 | 0.3358 | 4.6 | 5.0 | 4.8 | 5.0 | 5.0 | 5.0 |\n| GPT-3.5-Turbo | 0.2077 | 0.8764 | 0.3613 | 4.4 | 4.8 | 4.8 | 5.0 | 5.0 | 4.8 |\n| GPT-4 | 0.1643 | 0.8674 | 0.3399 | 4.8 | 5.0 | 4.8 | 4.8 | 5.0 | 5.0 |\n| Gemma-2B | 0.1795 | 0.8542 | 0.1987 | 2.0 | 2.4 | 3.0 | 3.6 | 3.4 | 3.0 |\n| Gemma-7B | 0.1927 | 0.8578 | 0.2242 | 3.6 | 4.0 | 3.4 | 4.0 | 5.0 | 4.6 |\n| Llama-2-hf | 0.1653 | 0.8399 | 0.2151 | 3.8 | 4.0 | 4.0 | 3.0 | 3.6 | 3.2 |\n| Llama-3 | 0.1828 | 0.8584 | 0.2556 | 3.2 | 4.0 | 3.4 | 4.8 | 4.8 | 4.2 |\n| Llama-3-Instruct | 0.1675 | 0.8495 | 0.301 | 3.8 | 4.0 | 4.2 | 4.8 | 4.6 | 4.0 |\n| Mistral-v0.1 | 0.1698 | 0.8534 | 0.2773 | 3.2 | 2.6 | 3.2 | 4.0 | 3.6 | 3.6 |\n| Mistral-Instruct-v0.1 | 0.1344 | 0.8381 | 0.1587 | 3.4 | 2.6 | 2.6 | 3.0 | 3.2 | 3.4 |\n| Phi-3-Mini-Instruct | 0.1593 | 0.8523 | 0.2604 | 3.8 | 2.6 | 3.0 | 3.8 | 3.8 | 3.4 |\n| Qwen1.5-0.5B | 0.1608 | 0.8534 | 0.279 | 2.8 | 1.8 | 2.2 | 3.2 | 2.6 | 2.8 |\n| Qwen1.5-1.8B | 0.1617 | 0.8502 | 0.268 | 3.4 | 3.2 | 3.4 | 3.4 | 3.2 | 3.2 |\n| Qwen1.5-4B | 0.1450 | 0.83 | 0.2503 | 3.4 | 3.0 | 3.2 | 3.6 | 4.0 | 3.8 |\n| Qwen1.5-7B | 0.1735 | 0.8575 | 0.2823 | 4.2 | 4.4 | 4.0 | 4.4 | 4.2 | 4.2 |\n| SOLAR-v1.0 | 0.1534 | 0.855 | 0.2522 | 3.4 | 3.8 | 4.0 | 4.0 | 3.4 | 3.4 |\n| SOLAR-Instruct-v1.0 | 0.1692 | 0.8594 | 0.2887 | 3.6 | 3.2 | 3.6 | 4.6 | 4.6 | 4.6 |\n| Yi-6B | 0.1919 | 0.8539 | 0.2409 | 3.6 | 3.4 | 4.2 | 3.6 | 4.0 | 4.2 |\n| Yi-9B | 0.2112 | 0.8649 | 0.2515 | 3.2 | 3.4 | 3.8 | 4.4 | 4.4 | 4.6 |\n| Zephyr-Beta | 0.1633 | 0.8573 | 0.2894 | 3.6 | 2.8 | 3.8 | 4.0 | 3.6 | 3.8 |", "caption": "Table 3: Evaluation results for zero-shot LMs on CNN/DM dataset. The highest values in the automatic evaluation metrics are emphasized in bold.", "description": "\ud45c 3\uc740 CNN/DM \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc81c\ub85c\uc0f7 \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ub41c \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc790\ub3d9 \ud3c9\uac00 \uc9c0\ud45c(ROUGE-L, BERTScore, METEOR)\uc640 \uc778\uac04 \ud3c9\uac00(\uad00\ub828\uc131, \uc0ac\uc2e4 \uc815\ud655\uc131, \uc77c\uad00\uc131), \uadf8\ub9ac\uace0 LLM\uc744 \uc774\uc6a9\ud55c \ud3c9\uac00 \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc790\ub3d9 \ud3c9\uac00 \uc9c0\ud45c\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uac12\ub4e4\uc740 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \ubaa8\ub378 \uc131\ub2a5\uc744 \uba85\ud655\ud788 \ud30c\uc545\ud560 \uc218 \uc788\ub3c4\ub85d \ud588\uc2b5\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc81c\ub85c\uc0f7 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \uce21\uba74\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \ub274\uc2a4 \uc694\uc57d \uc791\uc5c5\uc5d0 \ub300\ud55c \uac01 \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \uacb0\uacfc \ubc0f \ub17c\uc758"}, {"content": "| Model Name | ROUGE-L | BERTScore | METEOR | Relevance | Faithfulness | Coherence | Relevance | Faithfulness | Coherence |\n|---|---|---|---|---|---|---|---|---|---| \n| Gemini-1.5-Pro | 0.1704 | 0.8676 | 0.2658 | 4.4 | 5.0 | 4.8 | 4.8 | 5.0 | 5.0 |\n| GPT-3.5-Turbo | 0.1987 | 0.8714 | 0.2915 | 4.2 | 4.8 | 4.8 | 4.6 | 5.0 | 5.0 |\n| GPT-4 | 0.1684 | 0.8649 | 0.2704 | 4.6 | 4.8 | 4.8 | 4.6 | 5.0 | 5.0 |\n| Gemma-2B | 0.1538 | 0.8507 | 0.1978 | 3.2 | 3.6 | 3.8 | 3.0 | 4.6 | 3.8 |\n| Gemma-7B | 0.1572 | 0.814 | 0.226 | 2.6 | 3.0 | 3.0 | 3.8 | 4.2 | 4.4 |\n| Llama-2-hf | 0.1155 | 0.6605 | 0.1557 | 3.8 | 3.2 | 4.2 | 3.4 | 4.2 | 3.8 |\n| Llama-3 | 0.1152 | 0.6791 | 0.1853 | 2.4 | 2.4 | 3.0 | 4.2 | 5.0 | 4.8 |\n| Llama-3-Instruct | 0.101 | 0.8086 | 0.1405 | 1.2 | 1.4 | 2.6 | 4.6 | 5.0 | 5.0 |\n| Mistral-v0.1 | 0.1297 | 0.8449 | 0.2231 | 3.2 | 2.6 | 2.6 | 4.0 | 4.6 | 4.0 |\n| Mistral-Instruct-v0.1 | 0.1497 | 0.8504 | 0.1797 | 2.4 | 3.0 | 3.0 | 3.6 | 4.2 | 3.8 |\n| Phi-3-Mini-Instruct | 0.1294 | 0.8476 | 0.2092 | 3.4 | 3.2 | 3.6 | 4.0 | 4.0 | 4.8 |\n| Qwen1.5-0.5B | 0.1342 | 0.8531 | 0.2259 | 3.6 | 2.2 | 3.2 | 3.8 | 4.0 | 4.0 |\n| Qwen1.5-1.8B | 0.1375 | 0.8538 | 0.2299 | 3.6 | 2.8 | 3.8 | 3.8 | 4.6 | 4.8 |\n| Qwen1.5-4B | 0.1315 | 0.85 | 0.2268 | 3.4 | 3.0 | 3.2 | 4.0 | 4.6 | 4.8 |\n| Qwen1.5-7B | 0.1481 | 0.8569 | 0.2376 | 3.8 | 3.0 | 4.6 | 4.2 | 5.0 | 4.8 |\n| SOLAR-v1.0 | 0.1079 | 0.8168 | 0.1571 | 3.2 | 1.8 | 3.0 | 3.6 | 4.4 | 4.4 |\n| SOLAR-Instruct-v1.0 | 0.1415 | 0.8536 | 0.222 | 3.6 | 2.4 | 3.6 | 4.0 | 5.0 | 5.0 |\n| Yi-6B | 0.1986 | 0.8573 | 0.2098 | 2.6 | 3.4 | 3.4 | 2.8 | 3.2 | 3.2 |\n| Yi-9B | 0.2022 | 0.8626 | 0.1872 | 2.4 | 3.0 | 3.4 | 2.2 | 2.8 | 4.0 |\n| Zephyr-Beta | 0.1188 | 0.8478 | 0.2216 | 4.2 | 4.0 | 4.4 | 4.2 | 4.8 | 5.0 |", "caption": "Table 4: Evaluation results for zero-shot LMs on Newsroom dataset. The highest values in the automatic evaluation metrics are emphasized in bold.", "description": "\ud45c 4\ub294 \ub274\uc2a4\ub8f8 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc81c\ub85c\uc0f7 \ubc29\uc2dd\uc73c\ub85c \ud3c9\uac00\ud55c \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc790\ub3d9 \ud3c9\uac00 \uc9c0\ud45c(ROUGE-L, BERTScore, METEOR) \uc810\uc218\uc640 \uc778\uac04 \ud3c9\uac00\uc790 \ubc0f LLM \ud310\uc815 \uc810\uc218(\uad00\ub828\uc131, \uc0ac\uc2e4 \uc815\ud655\uc131, \uc77c\uad00\uc131)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc790\ub3d9 \ud3c9\uac00 \uc9c0\ud45c\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \ubaa8\ub378 \uc131\ub2a5 \ube44\uad50\ub97c \uc6a9\uc774\ud558\uac8c \ud569\ub2c8\ub2e4.  \uac01 \uc9c0\ud45c\ub294 \ub274\uc2a4 \uc694\uc57d \uc791\uc5c5\uc5d0\uc11c \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788, \uc81c\ub85c\uc0f7 \uc124\uc815\uc5d0\uc11c \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \ub2a5\ub825\uacfc \ub2e4\uc591\ud55c \ub274\uc2a4 \uc2a4\ud0c0\uc77c\uc744 \ucc98\ub9ac\ud558\ub294 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc911\uc810\uc744 \ub461\ub2c8\ub2e4.", "section": "4.1.2 Zero-shot Learning on Newsroom dataset"}, {"content": "| Model Name | ROUGE-L | BERTScore | METEOR | Relevance | Faithfulness | Coherence | Relevance | Faithfulness | Coherence |\n|---|---|---|---|---|---|---|---|---|---| \n| Gemini-1.5-Pro | 0.2197 | 0.8869 | 0.2923 | 4.4 | 4.8 | 4.8 | 4.4 | 5.0 | 4.8 |\n| GPT-3.5-Turbo | 0.1934 | 0.8791 | 0.2617 | 4.4 | 4.6 | 4.4 | 4.6 | 5.0 | 5.0 |\n| GPT-4 | 0.1644 | 0.8718 | 0.2588 | 4.0 | 4.6 | 4.8 | 4.8 | 5.0 | 5.0 |\n| Gemma-2B | 0.1645 | 0.8694 | 0.1924 | 3.0 | 2.4 | 3.8 | 3.8 | 5.0 | 4.8 |\n| Gemma-7B | 0.1198 | 0.5835 | 0.1529 | 4.0 | 4.0 | 4.6 | 4.4 | 4.8 | 4.8 |\n| Llama-2-hf | 0.1096 | 0.6522 | 0.1356 | 1.0 | 1.0 | 1.4 | 3.2 | 4.0 | 4.0 |\n| Llama-3 | 0.1422 | 0.7916 | 0.2144 | 3.4 | 2.8 | 3.8 | 4.0 | 4.8 | 4.8 |\n| Llama-3-Instruct | 0.1042 | 0.8126 | 0.1498 | 1.6 | 1.6 | 2.8 | 4.0 | 4.8 | 4.8 |\n| Mistral-v0.1 | 0.1281 | 0.8553 | 0.2175 | 4.2 | 3.6 | 4.2 | 3.8 | 3.4 | 4.2 |\n| Mistral-Instruct-v0.1 | 0.159 | 0.8564 | 0.1993 | 3.0 | 3.0 | 3.4 | 3.4 | 4.0 | 3.8 |\n| Phi-3-Mini-Instruct | 0.1231 | 0.8523 | 0.1947 | 4.0 | 3.0 | 4.2 | 4.2 | 4.4 | 4.6 |\n| Qwen1.5-0.5B | 0.1387 | 0.8608 | 0.2028 | 3.4 | 2.6 | 3.6 | 3.8 | 3.8 | 4.4 |\n| Qwen1.5-1.8B | 0.1322 | 0.8585 | 0.2098 | 3.8 | 3.2 | 3.6 | 4.4 | 5.0 | 5.0 |\n| Qwen1.5-4B | 0.1492 | 0.8635 | 0.2281 | 4.0 | 3.8 | 4.4 | 4.2 | 4.8 | 4.8 |\n| Qwen1.5-7B | 0.1629 | 0.8669 | 0.2065 | 4.2 | 3.8 | 4.8 | 4.2 | 4.8 | 4.6 |\n| SOLAR-v1.0 | 0.1435 | 0.8580 | 0.2006 | 4.0 | 3.8 | 4.2 | 4.6 | 5.0 | 5.0 |\n| SOLAR-Instruct-v1.0 | 0.1433 | 0.8597 | 0.2118 | 4.2 | 3.6 | 3.8 | 4.0 | 5.0 | 4.6 |\n| Yi-6B | 0.2222 | 0.8809 | 0.2325 | 2.8 | 2.6 | 3.2 | 3.8 | 4.8 | 4.6 |\n| Yi-9B | 0.2534 | 0.8884 | 0.2649 | 2.2 | 1.8 | 2.4 | 3.6 | 4.6 | 4.8 |\n| Zephyr-Beta | 0.1349 | 0.8572 | 0.2374 | 4.0 | 4.4 | 4.2 | 4.8 | 5.0 | 5.0 |", "caption": "Table 5: Evaluation results for zero-shot LMs on XSum dataset. The highest values in the automatic evaluation metrics are emphasized in bold.", "description": "\ud45c 5\ub294 XSum \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud55c \uc81c\ub85c\uc0f7(Zero-shot) \ud559\uc2b5 \ubc29\uc2dd\uc758 \uc5ec\ub7ec \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc790\ub3d9 \ud3c9\uac00 \uc9c0\ud45c(ROUGE-L, BERTScore, METEOR) \uc810\uc218\uc640 \uc0ac\ub78c \ud3c9\uac00 \uc810\uc218(Relevance, Faithfulness, Coherence), \uadf8\ub9ac\uace0 LLM \uae30\ubc18 \ud3c9\uac00 \uc810\uc218\uac00 \ubaa8\ub378\ubcc4\ub85c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc790\ub3d9 \ud3c9\uac00 \uc9c0\ud45c\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uc810\uc218\ub97c \uae30\ub85d\ud55c \uac12\ub4e4\uc740 \uad75\uc740 \uae00\uc528\ub85c \ud45c\uc2dc\ub418\uc5b4 \ubaa8\ub378 \uc131\ub2a5\uc744 \ud55c\ub208\uc5d0 \ud30c\uc545\ud560 \uc218 \uc788\ub3c4\ub85d \ub3d5\uc2b5\ub2c8\ub2e4. \uac01 \uc9c0\ud45c\ub294 \uc694\uc57d\ubb38\uc758 \uad00\ub828\uc131, \uc0ac\uc2e4 \uc815\ud655\uc131, \uc77c\uad00\uc131 \ub4f1\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "4.1. Zero-shot Learning Results"}, {"content": "| Model Name | ROUGE-L | BERTScore | METEOR | Relevance | Faithfulness | Coherence | Relevance | Faithfulness | Coherence |\n|---|---|---|---|---|---|---|---|---|---| \n| Gemini-1.5-Pro | 0.2343 | 0.8811 | 0.3485 | 4.4 | 5.0 | 4.8 | 5.0 | 5.0 | 5.0 |\n| GPT-3.5-Turbo | 0.2377 | 0.8806 | 0.3525 | 4.2 | 4.8 | 4.8 | 5.0 | 5.0 | 4.8 |\n| GPT-4 | 0.1947 | 0.8736 | 0.3484 | 4.8 | 5.0 | 4.6 | 4.8 | 5.0 | 4.8 |\n| Gemma-2B | 0.1653 | 0.8525 | 0.1946 | 3.6 | 4.4 | 4.0 | 2.4 | 2.6 | 3.0 |\n| Gemma-7B | 0.0558 | 0.2573 | 0.0596 | 3.4 | 4.2 | 4.2 | 3.6 | 3.8 | 3.6 |\n| Llama-2-hf | 0.1518 | 0.79 | 0.179 | 3.4 | 3.4 | 3.4 | 3.0 | 2.6 | 2.4 |\n| Llama-3 | 0.1694 | 0.7939 | 0.2252 | 3.8 | 3.8 | 4.2 | 3.8 | 4.6 | 4.4 |\n| Llama-3-Instruct | 0.1612 | 0.8104 | 0.2432 | 4.0 | 4.0 | 4.0 | 4.0 | 4.0 | 4.0 |\n| Mistral-v0.1 | 0.0522 | 0.2773 | 0.0673 | 3.0 | 3.2 | 3.2 | 3.0 | 2.6 | 2.8 |\n| Mistral-Instruct-v0.1 | 0.1368 | 0.6916 | 0.1577 | 3.4 | 3.6 | 3.8 | 2.2 | 3.0 | 2.8 |\n| Phi-3-Mini-Instruct | 0.1541 | 0.8436 | 0.2359 | 2.6 | 2.4 | 2.6 | 3.4 | 3.6 | 3.2 |\n| Qwen1.5-0.5B | 0.1654 | 0.8521 | 0.2723 | 3.0 | 2.0 | 2.8 | 3.4 | 3.2 | 3.0 |\n| Qwen1.5-1.8B | 0.1769 | 0.8536 | 0.2517 | 3.8 | 3.6 | 3.2 | 2.8 | 2.4 | 2.6 |\n| Qwen1.5-4B | 0.1741 | 0.8548 | 0.277 | 3.6 | 3.6 | 3.6 | 3.0 | 3.2 | 3.2 |\n| Qwen1.5-7B | 0.1714 | 0.8537 | 0.2556 | 4.4 | 4.2 | 4.2 | 3.8 | 3.8 | 3.4 |\n| SOLAR-v1.0 | 0.1546 | 0.8389 | 0.1994 | 3.8 | 3.6 | 3.6 | 4.0 | 4.0 | 3.6 |\n| SOLAR-Instruct-v1.0 | 0.1682 | 0.8594 | 0.285 | 4.2 | 4.0 | 4.2 | 4.0 | 3.8 | 3.8 |\n| Yi-6B | 0.1883 | 0.8624 | 0.1857 | 4.0 | 3.6 | 3.8 | 3.4 | 3.8 | 3.4 |\n| Yi-9B | 0.1804 | 0.8574 | 0.1565 | 4.0 | 3.4 | 4.0 | 3.2 | 3.2 | 4.0 |\n| Zephyr-Beta | 0.1624 | 0.8435 | 0.2641 | 4.0 | 3.6 | 3.8 | 3.6 | 3.6 | 3.8 |", "caption": "Table 6: Evaluation results for three-shot LMs on CNN/DM dataset. The highest values in the automatic evaluation metrics are emphasized in bold.", "description": "\ud45c 6\uc740 CNN/DM \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc138 \uac00\uc9c0 \uc608\uc2dc\ub97c \uc0ac\uc6a9\ud55c \uc5b8\uc5b4 \ubaa8\ub378\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc790\ub3d9 \ud3c9\uac00 \uc9c0\ud45c(ROUGE-L, BERTScore, METEOR)\uc640 \uc0ac\ub78c \ud3c9\uac00(\uad00\ub828\uc131, \uc0ac\uc2e4 \uc815\ud655\uc131, \uc77c\uad00\uc131), \uadf8\ub9ac\uace0 LLM\uc744 \uc774\uc6a9\ud55c \ud3c9\uac00 \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uc9c0\ud45c\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \ubaa8\ub378 \uc131\ub2a5 \ube44\uad50\ub97c \uc6a9\uc774\ud558\uac8c \ud569\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uc81c\ub85c\uc0f7 \ud559\uc2b5\uacfc\uc758 \ube44\uad50\ub97c \ud1b5\ud574 \uba87\uba87 \uc0f7 \ud559\uc2b5 \uc804\ub7b5\uc758 \ud6a8\uacfc\uc131\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "4.2.1. \uc138 \uac00\uc9c0 \uc608\uc2dc \ud559\uc2b5\uc744 \uc0ac\uc6a9\ud55c CNN/DM \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uacb0\uacfc"}, {"content": "| Model Name | ROUGE-L | BERTScore | METEOR | Relevance | Faithfulness | Coherence | Relevance | Faithfulness | Coherence |\n|---|---|---|---|---|---|---|---|---|---| \n| Gemini-1.5-Pro | 0.1772 | 0.8695 | 0.2632 | 4.4 | 4.6 | 5.0 | 4.4 | 5.0 | 5.0 |\n| GPT-3.5-Turbo | 0.2144 | 0.8744 | 0.2892 | 4.4 | 4.8 | 4.8 | 4.0 | 5.0 | 5.0 |\n| GPT-4 | 0.1917 | 0.8707 | 0.2621 | 4.2 | 4.8 | 4.8 | 4.8 | 5.0 | 5.0 |\n| Gemma-2B | 0.122 | 0.8383 | 0.1623 | 2.8 | 2.4 | 3.4 | 3.2 | 4.2 | 4.4 |\n| Gemma-7B | 0.0176 | 0.0943 | 0.0207 | 4.0 | 3.6 | 4.4 | 3.4 | 4.6 | 4.6 |\n| Llama-2-hf | 0.1407 | 0.8042 | 0.1728 | 2.4 | 2.6 | 3.4 | 3.0 | 3.6 | 3.2 |\n| Llama-3 | 0.1118 | 0.5872 | 0.1457 | 4.2 | 3.8 | 4.4 | 4.0 | 4.6 | 4.8 |\n| Llama-3-Instruct | 0.1062 | 0.7969 | 0.1583 | 2.0 | 2.2 | 2.2 | 4.4 | 5.0 | 5.0 |\n| Mistral-v0.1 | 0.0543 | 0.3082 | 0.0734 | 4.0 | 4.2 | 4.2 | 3.4 | 3.2 | 3.6 |\n| Mistral-Instruct-v0.1 | 0.1645 | 0.7966 | 0.1708 | 3.2 | 3.8 | 3.8 | 2.6 | 3.6 | 3.2 |\n| Phi-3-Mini-Instruct | 0.1342 | 0.8417 | 0.1936 | 3.6 | 3.8 | 3.4 | 3.8 | 4.0 | 4.6 |\n| Qwen1.5-0.5B | 0.1302 | 0.8525 | 0.2208 | 3.4 | 3.2 | 2.4 | 3.6 | 3.4 | 4.6 |\n| Qwen1.5-1.8B | 0.1388 | 0.8544 | 0.2249 | 3.4 | 2.6 | 3.6 | 3.4 | 3.2 | 4.4 |\n| Qwen1.5-4B | 0.1367 | 0.8542 | 0.231 | 4.2 | 3.6 | 4.0 | 4.2 | 3.6 | 4.8 |\n| Qwen1.5-7B | 0.1379 | 0.8544 | 0.2258 | 4.0 | 4.4 | 4.6 | 4.2 | 4.2 | 4.8 |\n| SOLAR-v1.0 | 0.1791 | 0.8572 | 0.1814 | 4.2 | 3.8 | 4.2 | 3.2 | 3.8 | 4.0 |\n| SOLAR-Instruct-v1.0 | 0.1777 | 0.8631 | 0.2358 | 4.0 | 3.6 | 4.2 | 3.4 | 3.8 | 4.8 |\n| Yi-6B | 0.1728 | 0.8511 | 0.1919 | 2.8 | 3.0 | 3.0 | 2.8 | 3.8 | 4.0 |\n| Yi-9B | 0.1905 | 0.8608 | 0.1798 | 2.0 | 2.2 | 2.8 | 3.4 | 4.2 | 4.8 |\n| Zephyr-Beta | 0.1279 | 0.8175 | 0.2064 | 4.0 | 3.6 | 4.4 | 4.2 | 3.8 | 4.8 |", "caption": "Table 7: Evaluation results for three-shot LMs on Newsroom dataset. The highest values in the automatic evaluation metrics are emphasized in bold.", "description": "\ud45c 7\uc740 \ub274\uc2a4\ub8f8 \ub370\uc774\ud130\uc14b\uc5d0\uc11c 3-\uc0f7 \ud559\uc2b5\uc744 \uc0ac\uc6a9\ud55c \uc5ec\ub7ec \uc5b8\uc5b4 \ubaa8\ub378\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc790\ub3d9 \ud3c9\uac00 \uc9c0\ud45c(ROUGE-L, BERTScore, METEOR)\uc640 \uc778\uac04 \ud3c9\uac00(\uad00\ub828\uc131, \uc0ac\uc2e4 \uc815\ud655\uc131, \uc77c\uad00\uc131), \uadf8\ub9ac\uace0 LLM \ud310\uc815 \ud3c9\uac00 \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uc9c0\ud45c\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \ubaa8\ub378 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4. \ub274\uc2a4 \uae30\uc0ac \uc694\uc57d \uc791\uc5c5\uc5d0\uc11c \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ud30c\uc545\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4.2. Few-shot Learning Results"}, {"content": "| Model Name | ROUGE-L | BERTScore | METEOR | Relevance | Faithfulness | Coherence | Relevance | Faithfulness | Coherence |\n|---|---|---|---|---|---|---|---|---|---| \n| Gemini-1.5-Pro | 0.2429 | 0.8929 | 0.3005 | 4.4 | 4.6 | 4.8 | 4.8 | 5.0 | 5.0 |\n| GPT-3.5-Turbo | 0.2159 | 0.8856 | 0.2807 | 4.2 | 4.4 | 4.8 | 4.8 | 5.0 | 5.0 |\n| GPT-4 | 0.1868 | 0.8773 | 0.27 | 4.2 | 4.6 | 4.8 | 4.6 | 5.0 | 5.0 |\n| Gemma-2B | 0.1464 | 0.8497 | 0.1674 | 3.6 | 4.0 | 4.0 | 3.2 | 5.0 | 4.6 |\n| Gemma-7B | 0.0123 | 0.0426 | 0.0136 | 3.4 | 3.0 | 4.2 | 3.6 | 4.2 | 4.4 |\n| Llama-2-hf | 0.1693 | 0.8275 | 0.192 | 3.4 | 3.6 | 3.6 | 3.8 | 4.0 | 4.6 |\n| Llama-3 | 0.1474 | 0.6694 | 0.1817 | 3.6 | 4.2 | 4.0 | 3.8 | 4.4 | 4.6 |\n| Llama-3-Instruct | 0.1082 | 0.8298 | 0.1787 | 2.4 | 2.4 | 2.6 | 4.8 | 5.0 | 5.0 |\n| Mistral-v0.1 | 0.0226 | 0.127 | 0.0311 | 4.0 | 4.4 | 4.2 | 3.8 | 3.8 | 4.4 |\n| Mistral-Instruct-v0.1 | 0.1744 | 0.8177 | 0.2038 | 3.6 | 3.8 | 4.6 | 4.2 | 4.4 | 4.6 |\n| Phi-3-Mini-Instruct | 0.1406 | 0.8594 | 0.2038 | 4.0 | 3.8 | 4.6 | 4.2 | 5.0 | 4.8 |\n| Qwen1.5-0.5B | 0.132 | 0.859 | 0.2061 | 3.6 | 3.2 | 4.2 | 3.8 | 4.2 | 4.6 |\n| Qwen1.5-1.8B | 0.1447 | 0.8635 | 0.2075 | 3.6 | 3.0 | 4.0 | 4.0 | 3.8 | 4.4 |\n| Qwen1.5-4B | 0.1546 | 0.867 | 0.2407 | 4.0 | 3.8 | 4.4 | 4.0 | 4.8 | 5.0 |\n| Qwen1.5-7B | 0.1755 | 0.8717 | 0.2231 | 4.2 | 4.0 | 4.8 | 4.0 | 5.0 | 4.8 |\n| SOLAR-v1.0 | 0.2334 | 0.8723 | 0.2666 | 4.2 | 4.0 | 4.2 | 4.2 | 5.0 | 5.0 |\n| SOLAR-Instruct-v1.0 | 0.1599 | 0.8663 | 0.228 | 3.8 | 3.8 | 4.2 | 3.8 | 5.0 | 4.6 |\n| Yi-6B | 0.2322 | 0.8812 | 0.2515 | 2.6 | 3.2 | 3.8 | 3.8 | 5.0 | 4.8 |\n| Yi-9B | 0.2623 | 0.8915 | 0.2808 | 3.0 | 3.2 | 3.0 | 3.4 | 4.2 | 4.0 |\n| Zephyr-Beta | 0.1578 | 0.8463 | 0.2397 | 4.2 | 4.2 | 4.6 | 4.4 | 4.0 | 4.8 |", "caption": "Table 8: Evaluation results for three-shot LMs on XSum dataset. The highest values in the automatic evaluation metrics are emphasized in bold.", "description": "\ud45c 8\uc740 XSum \ub370\uc774\ud130\uc14b\uc5d0\uc11c 3-shot \ud559\uc2b5\uc744 \uc801\uc6a9\ud55c \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc790\ub3d9 \ud3c9\uac00 \uc9c0\ud45c(ROUGE-L, BERTScore, METEOR)\uc640 \ub354\ubd88\uc5b4, \uc0ac\ub78c \ud3c9\uac00\uc790\uc640 LLM(Large Language Model) \ud310\uc815\uc790\uc758 \ud3c9\uac00 \uacb0\uacfc(\uad00\ub828\uc131, \uc0ac\uc2e4 \uc815\ud655\uc131, \uc77c\uad00\uc131)\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.  \uac01 \uc9c0\ud45c\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uc810\uc218\ub97c \uae30\ub85d\ud55c \ubaa8\ub378\uc740 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc81c\ub85c\uc0f7 \ud559\uc2b5\uacfc \ube44\uad50\ud558\uc5ec \uba87\uba87 \uc0f7 \ud559\uc2b5\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "4.2.3. Three-shot Learning on XSum dataset"}, {"content": "| Role Adoption | Task Specification | Multi-Step Breakdown | Clear Instructions | Providing Input | Length Constraint |\n|---|---|---|---|---|---| \n| \u25a0 | \u25a0 | \u25a0 | \u25a0 | \u25a0 | \u25a0 |", "caption": "Table 9: Designed Prompt for zero-shot experiments on XSum dataset", "description": "\uc774 \ud45c\ub294 XSum \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc81c\ub85c\uc0f7 \ud559\uc2b5 \ud658\uacbd\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ud504\ub86c\ud504\ud2b8\uc758 \ub514\uc790\uc778\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud504\ub86c\ud504\ud2b8\ub294 \ub274\uc2a4 \uae30\uc0ac\ub97c \ud55c \ubb38\uc7a5\uc73c\ub85c \uc694\uc57d\ud558\ub294 \uc791\uc5c5\uc744 \ubaa8\ub378\uc5d0\uac8c \uc9c0\uc2dc\ud558\ub294\ub370,  \uae30\uc790\ub4e4\uc774 \uae34 \uae30\uc0ac\ub97c \uac04\uacb0\ud558\uac8c \uc694\uc57d\ud558\ub294 \ubc29\uc2dd\uc744 \ubaa8\ubc29\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ud504\ub86c\ud504\ud2b8\uc758 \uc8fc\uc694 \uad6c\uc131 \uc694\uc18c(\uc5ed\ud560 \ubd80\uc5ec, \uba85\ud655\ud55c \uc9c0\uce68, \uc791\uc5c5 \uba85\uc138, \uc785\ub825 \uc81c\uacf5, \ub2e4\ub2e8\uacc4 \ubd84\uc11d, \uae38\uc774 \uc81c\ud55c)\ub97c \ub2e4\uc591\ud55c \uc0c9\uc0c1\uc73c\ub85c \uad6c\ubd84\ud558\uc5ec \uac01 \uc694\uc18c\uac00 \uc5b4\ub5a4 \ubc29\uc2dd\uc73c\ub85c \ud504\ub86c\ud504\ud2b8 \uae30\uc220\uc5d0 \uae30\uc5ec\ud558\ub294\uc9c0 \uba85\ud655\ud788 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Experimental setup"}]