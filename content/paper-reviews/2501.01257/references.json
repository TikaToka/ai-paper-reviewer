{"references": [{"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring coding challenge competence with apps", "publication_date": "2021-05-00", "reason": "This paper is foundational for the CODEELO benchmark because it introduced a similar benchmark, APPS, although CODEELO improves on it in several crucial aspects."}, {"fullname_first_author": "Yujia Li", "paper_title": "Competition-level code generation with alphacode", "publication_date": "2022-00-00", "reason": "This paper is highly relevant because it demonstrates the state-of-the-art in code generation using LLMs, which CODEELO aims to benchmark."}, {"fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-00", "reason": "This is a highly influential paper that explores the use of LLMs for program synthesis, which is directly relevant to the goal of CODEELO."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-00", "reason": "This paper is important as it provides a comprehensive evaluation framework for LLMs trained on code, which CODEELO builds upon."}, {"fullname_first_author": "Naman Jain", "paper_title": "LiveCodeBench: Holistic and contamination free evaluation of large language models for code", "publication_date": "2024-03-00", "reason": "This paper is highly relevant as it introduced another benchmark for code generation, LiveCodeBench, providing a comparison point for CODEELO."}]}