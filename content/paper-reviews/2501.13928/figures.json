[{"figure_path": "https://arxiv.org/html/2501.13928/x2.png", "caption": "Figure 1: Fast3R is a method towards 3D reconstructing 1000+ unordered, unposed images in a single forward pass.", "description": "\uc774 \uadf8\ub9bc\uc740 Fast3R\uc774\ub77c\ub294 \uc0c8\ub85c\uc6b4 3D \uc7ac\uad6c\uc131 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Fast3R\uc740 1,000\uac1c \uc774\uc0c1\uc758 \ubb34\uc9c8\uc11c\ud558\uace0 \uc790\uc138\uac00 \uc815\ud574\uc9c0\uc9c0 \uc54a\uc740 \uc774\ubbf8\uc9c0\ub4e4\uc744 \ub2e8\uc77c \uc804\ubc29 \ud328\uc2a4(single forward pass)\ub85c \ucc98\ub9ac\ud558\uc5ec 3D \ubaa8\ub378\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uae30\uc874\uc758 \uc30d\ubc29\ud5a5(pairwise) \uc811\uadfc \ubc29\uc2dd\uacfc \ub2ec\ub9ac, Fast3R\uc740 \ubaa8\ub4e0 \uc774\ubbf8\uc9c0\ub97c \ub3d9\uc2dc\uc5d0 \ucc98\ub9ac\ud558\uc5ec \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \ub192\uc774\uace0 \uc815\ud655\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c\ub294 \uc785\ub825 \uc774\ubbf8\uc9c0\ub4e4, Fast3R\uc758 \ucc98\ub9ac \uacfc\uc815, \uadf8\ub9ac\uace0 \uc0dd\uc131\ub41c 3D \ubaa8\ub378\uacfc \uce74\uba54\ub77c \uc704\uce58\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 Fast3R\uc758 \ud575\uc2ec\uc801\uc778 \ud2b9\uc9d5\uacfc \uc131\ub2a5\uc744 \ud55c\ub208\uc5d0 \ubcf4\uc5ec\uc8fc\ub294 \uc2dc\uac01\uc801 \uc124\uba85\uc785\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.13928/x3.png", "caption": "Figure 2: Model architecture of Fast3R.\nBuilt upon a novel Transformer-based architecture which supports bidirectional information flow, Fast3R is able to process dense input views simultaneously.", "description": "\uadf8\ub9bc 2\ub294 Fast3R\uc758 \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc591\ubc29\ud5a5 \uc815\ubcf4 \ud750\ub984\uc744 \uc9c0\uc6d0\ud558\ub294 \uc0c8\ub85c\uc6b4 Transformer \uae30\ubc18 \uc544\ud0a4\ud14d\ucc98\ub97c \uae30\ubc18\uc73c\ub85c Fast3R\uc740 \ub2e4\uc218\uc758 \uc785\ub825 \ubdf0\ub97c \ub3d9\uc2dc\uc5d0 \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc989, \uc5ec\ub7ec \uac1c\uc758 \uc774\ubbf8\uc9c0\ub97c \uc21c\ucc28\uc801\uc73c\ub85c \ucc98\ub9ac\ud558\ub294 \uae30\uc874 \ubc29\uc2dd\uacfc \ub2ec\ub9ac Fast3R\uc740 \uc5ec\ub7ec \uc774\ubbf8\uc9c0\ub97c \ubcd1\ub82c\uc801\uc73c\ub85c \ucc98\ub9ac\ud558\uc5ec 3D \uc7ac\uad6c\uc131 \uc18d\ub3c4\ub97c \ub192\uc774\uace0 \uc815\ud655\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\ub294 \uc774\ubbf8\uc9c0 \uc778\ucf54\ub354, \ud4e8\uc804 \ud2b8\ub79c\uc2a4\ud3ec\uba38, \ud3ec\uc778\ud2b8\ub9f5 \ub514\ucf54\ub354\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 \uad6c\uc131 \uc694\uc18c\uac00 \ub098\uc640 \uc788\uc73c\uba70, \uac01 \uc694\uc18c\uc758 \uae30\ub2a5\uacfc \uc0c1\ud638 \uc791\uc6a9\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \ud4e8\uc804 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub294 \ubaa8\ub4e0 \uc774\ubbf8\uc9c0 \ud328\uce58 \uac04\uc758 \uc804\uc5ed\uc801 \uc0c1\ud638 \uc791\uc6a9\uc744 \uac00\ub2a5\ud558\uac8c \ud558\uc5ec,  \uc804\uccb4\uc801\uc778 \ub9e5\ub77d \uc815\ubcf4\ub97c \ud65c\uc6a9\ud55c \ubcf4\ub2e4 \uc815\ud655\ud55c 3D \uc7ac\uad6c\uc131\uc744 \uac00\ub2a5\ud558\uac8c \ud569\ub2c8\ub2e4.", "section": "3. Model"}, {"figure_path": "https://arxiv.org/html/2501.13928/x4.png", "caption": "Figure 3: Qualitative examples of Fast3R\u2019s output. The text on the yellow sign says \u201cCaution, cleaning in progress\u201d and is legible if zoomed in.", "description": "\uadf8\ub9bc 3\uc740 Fast3R\uc758 \ucd9c\ub825 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc815\uc131\uc801 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uc774\ubbf8\uc9c0\ub294 \ub2e4\uc591\ud55c \uac01\ub3c4\uc5d0\uc11c \ucd2c\uc601\ub41c \uc5ec\ub7ec \uc7a5\uba74\ub4e4\uc744 \ubcf4\uc5ec\uc8fc\uba70, Fast3R\uc774 \uc774\ub7ec\ud55c \uc5ec\ub7ec \uc774\ubbf8\uc9c0\ub4e4\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc815\ud655\ud558\uace0 \uc0c1\uc138\ud55c 3D \ubaa8\ub378\uc744 \uc7ac\uad6c\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 \uadf8\ub9bc\uc758 \ub178\ub780\uc0c9 \ud45c\uc9c0\ud310\uc5d0 \uc4f0\uc778 \"Caution, cleaning in progress\" \ubb38\uad6c\ub294 \ud655\ub300 \uc2dc \uc77d\uc744 \uc218 \uc788\uc744 \uc815\ub3c4\ub85c \uc120\uba85\ud558\uac8c \uc7ac\uad6c\uc131\ub418\uc5b4 Fast3R\uc758 \ub192\uc740 \ud574\uc0c1\ub3c4\uc640 \uc815\ud655\ub3c4\ub97c \ub2e4\uc2dc \ud55c\ubc88 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc2dc\uac01\uc5d0\uc11c \ucd2c\uc601\ub41c \uc774\ubbf8\uc9c0\ub4e4\uc744 \uae30\ubc18\uc73c\ub85c 3D \ubaa8\ub378\uc744 \uc0dd\uc131\ud558\ub294 Fast3R\uc758 \uc131\ub2a5\uc744 \uc9c1\uad00\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \ub300\ud45c\uc801\uc778 \uc608\uc2dc\uc785\ub2c8\ub2e4.", "section": "3. Model"}, {"figure_path": "https://arxiv.org/html/2501.13928/x5.png", "caption": "Figure 4: Pose accuracy with more views: Fast3R improves with the context from more views. Fast3R saturates the orientation portion of the benchmark, even using 3-5 views.", "description": "\uadf8\ub9bc 4\ub294 Fast3R\uc774 \ub354 \ub9ce\uc740 \ubdf0(\uc774\ubbf8\uc9c0)\ub97c \uc0ac\uc6a9\ud560\uc218\ub85d \uc815\ud655\ub3c4\uac00 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 \ubc29\ud5a5(Orientation) \ubd80\ubd84\uc5d0\uc11c\ub294 3~5\uac1c\uc758 \ubdf0\ub9cc \uc0ac\uc6a9\ud574\ub3c4 \uc131\ub2a5\uc774 \ud3ec\ud654\uc0c1\ud0dc\uc5d0 \ub3c4\ub2ec\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, Fast3R\uc740 \uc81c\ud55c\ub41c \uc218\uc758 \ubdf0\ub9cc\uc73c\ub85c\ub3c4 \uac1d\uccb4\uc758 \ubc29\ud5a5\uc744 \uc815\ud655\ud558\uac8c \ucd94\uc815\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  \uc774\uac83\uc740 Fast3R\uc774 \uc5ec\ub7ec \ubdf0\ub4e4 \uac04\uc758 \uc0c1\ud638\uc791\uc6a9\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud65c\uc6a9\ud558\uc5ec \uc815\ud655\ub3c4\ub97c \ub192\uc77c \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.13928/extracted/6149191/fig/4d_reconstruction.png", "caption": "Figure 5: DTU reconstruction quality vs. test number of views. Accuracy and Completion (lower is better) get better as we inference with more views.", "description": "\uadf8\ub9bc 5\ub294 DTU \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \uc218\uc758 \ud14c\uc2a4\ud2b8 \ubdf0\ub97c \uc0ac\uc6a9\ud558\uc5ec 3D \uc7ac\uad6c\uc131\uc758 \uc815\ud655\ub3c4\uc640 \uc644\uc131\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc815\ud655\ub3c4\ub294 RRA@5(Relative Rotation Accuracy at 5 degrees) \uc9c0\ud45c\ub85c, \uc644\uc131\ub3c4\ub294 Completion \uc9c0\ud45c\ub85c \uce21\uc815\ub429\ub2c8\ub2e4. \ub450 \uc9c0\ud45c \ubaa8\ub450 \uac12\uc774 \ub0ae\uc744\uc218\ub85d \uc88b\uc2b5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ud14c\uc2a4\ud2b8 \uc2dc \uc0ac\uc6a9\ub41c \ubdf0\uc758 \uc218\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\uc640 \uc644\uc131\ub3c4\uac00 \ud5a5\uc0c1\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, \ub354 \ub9ce\uc740 \ubdf0\ub97c \uc0ac\uc6a9\ud560\uc218\ub85d \ub354 \uc815\ud655\ud558\uace0 \uc644\uc131\ub3c4 \ub192\uc740 3D \uc7ac\uad6c\uc131 \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4.3 3D Reconstruction"}, {"figure_path": "https://arxiv.org/html/2501.13928/x6.png", "caption": "Figure 6: Qualitative 4D reconstruction results on unseen dynamic scenes in DAVIS. Results are obtained with one forward pass. The tracks are visualized using ground-truth track annotations from TAP-Vid-DAVIS [11].", "description": "\uadf8\ub9bc 6\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c Fast3R \ubaa8\ub378\uc744 DAVIS \ub370\uc774\ud130\uc14b\uc758 \ub3d9\uc801\uc778 \uc2dc\ud000\uc2a4\uc5d0 \uc801\uc6a9\ud558\uc5ec \uc5bb\uc740 4D \uc7ac\uad6c\uc131 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud55c \ubc88\uc758 \uc21c\uc804\ud30c(forward pass)\ub9cc\uc73c\ub85c \uacb0\uacfc\ub97c \uc5bb\uc5c8\uc73c\uba70, TAP-Vid-DAVIS [11] \ub370\uc774\ud130\uc14b\uc758 ground-truth track annotations\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2dc\uac01\ud654\ud588\uc2b5\ub2c8\ub2e4.  \uc774\ub294 Fast3R\uc774 \uc815\uc801\uc778 \uc7a5\uba74\ubfd0 \uc544\ub2c8\ub77c \ub3d9\uc801\uc778 \uc7a5\uba74\uc5d0\uc11c\ub3c4 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc815\uc131\uc801(qualitative) \ud3c9\uac00\uc785\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \uc5ec\ub7ec \uc2dc\uc810\uc5d0\uc11c\uc758 3D \uc7ac\uad6c\uc131 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uc5b4 \uc2dc\uac04\uc5d0 \ub530\ub978 \ubcc0\ud654\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ud655\uc778\ud560 \uc218 \uc788\uac8c \ud574\uc90d\ub2c8\ub2e4.", "section": "4.3 3D Reconstruction"}, {"figure_path": "https://arxiv.org/html/2501.13928/x7.png", "caption": "Figure 7: Increasing # views during training: camera pose estimation on CO3D. Estimates of both orientation (RRA@5) and translation (RTA@5) improve with more views.", "description": "\uadf8\ub9bc 7\uc740 CO3D \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uce74\uba54\ub77c \uc790\uc138 \ucd94\uc815\uc744 \uc704\ud55c \ud6c8\ub828 \uc911\uc5d0 \uc0ac\uc6a9\ub41c \ubdf0\uc758 \uc218\ub97c \ub298\ub838\uc744 \ub54c\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 \ud6c8\ub828\uc5d0 \uc0ac\uc6a9\ub41c \ubdf0\uc758 \uac1c\uc218\ub97c \ub098\ud0c0\ub0b4\uace0, y\ucd95\uc740 RRA@5(\ubc29\ud5a5 \uc624\ub958)\uc640 RTA@5(\ubcc0\ud658 \uc624\ub958)\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uadf8\ub798\ud504\ub294 \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \uc0ac\uc6a9\ub41c \ubdf0\uc758 \uc218\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ubc29\ud5a5 \ubc0f \ubcc0\ud658 \ucd94\uc815\uc758 \uc815\ud655\ub3c4\uac00 \ud5a5\uc0c1\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, \ub354 \ub9ce\uc740 \ubdf0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0a4\uba74 \uce74\uba54\ub77c\uc758 \ubc29\ud5a5\uacfc \uc704\uce58\ub97c \ub354 \uc815\ud655\ud558\uac8c \ucd94\uc815\ud560 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.13928/x8.png", "caption": "Figure 8: Increasing # views during training: reconstruction on 7scenes and NRGBD. Accuracy and Completion (lower is better) get better as we train with more views. Normal Consistency (high is better) also gets better as we train with more views.", "description": "\uadf8\ub9bc 8\uc740 Fast3R \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\ub294 \ub370 \uc0ac\uc6a9\ub41c \uc774\ubbf8\uc9c0\uc758 \uc218\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c 7-Scenes \ubc0f NRGBD \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 3D \uc7ac\uad6c\uc131 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc815\ud655\ub3c4(Accuracy)\uc640 \uc644\uc131\ub3c4(Completion)\ub294 \uac12\uc774 \ub0ae\uc744\uc218\ub85d \uc88b\uc73c\uba70, \ud6c8\ub828\uc5d0 \uc0ac\uc6a9\ub41c \uc774\ubbf8\uc9c0 \uc218\uac00 \ub9ce\uc544\uc9c8\uc218\ub85d \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c, Normal Consistency\ub294 \uac12\uc774 \ub192\uc744\uc218\ub85d \uc88b\uc73c\uba70, \ub9c8\ucc2c\uac00\uc9c0\ub85c \ud6c8\ub828 \uc774\ubbf8\uc9c0 \uc218 \uc99d\uac00\uc5d0 \ub530\ub77c \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ub354 \ub9ce\uc740 \ubdf0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ud558\uba74 \ubaa8\ub378\uc774 \ub354 \ub098\uc740 \ub9e5\ub77d \uc815\ubcf4\ub97c \ud559\uc2b5\ud558\uace0, \uadf8 \uacb0\uacfc 3D \uc7ac\uad6c\uc131 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub428\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.13928/x9.png", "caption": "Figure 9: Effect of sampling image index PE during training. If we train the model without sampling index embeddings, regression loss spikes (orange) when testing with more views than seen at training (top). Our embedding strategy performs comparably even with 6\u00d76\\times6 \u00d7 the number of views during training.", "description": "\uadf8\ub9bc 9\ub294 \ud559\uc2b5 \uc911 \uc774\ubbf8\uc9c0 \uc778\ub371\uc2a4 \uc704\uce58 \uc784\ubca0\ub529\uc744 \uc0d8\ud50c\ub9c1\ud558\ub294 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc778\ub371\uc2a4 \uc784\ubca0\ub529 \uc5c6\uc774 \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a4\uba74, \ud559\uc2b5 \uc2dc \ubcf4\uc558\ub358 \uac83\ubcf4\ub2e4 \ub354 \ub9ce\uc740 \ubdf0(6\ubc30)\ub85c \ud14c\uc2a4\ud2b8\ud560 \ub54c \ud68c\uadc0 \uc190\uc2e4\uc774 \uae09\uc99d\ud569\ub2c8\ub2e4(\uc624\ub80c\uc9c0\uc0c9). \ud558\uc9c0\ub9cc \uc81c\uc548\ub41c \uc784\ubca0\ub529 \uc804\ub7b5\uc744 \uc0ac\uc6a9\ud558\uba74 \ud559\uc2b5 \uc2dc \ubdf0\uc758 \uac1c\uc218\ubcf4\ub2e4 \ud6e8\uc52c \ub9ce\uc740 \ubdf0\ub97c \uc0ac\uc6a9\ud55c \ud14c\uc2a4\ud2b8\uc5d0\uc11c\ub3c4 \uc131\ub2a5\uc774 \ube44\uc2b7\ud558\uac8c \uc720\uc9c0\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.1. \ubdf0 \uc218 \ud655\uc7a5"}, {"figure_path": "https://arxiv.org/html/2501.13928/x10.png", "caption": "Figure 10: Visualizations of results on NRGBD scenes. Fast3R learns the regularity of indoor rooms (square-like shapes) and demonstrates \u201cloop closure\u201d capabilities.", "description": "\uadf8\ub9bc 10\uc740 Fast3R\uc774 NRGBD \uc2e4\ub0b4 \uc7a5\uba74 \ub370\uc774\ud130\uc14b\uc5d0\uc11c 3D \uc7ac\uad6c\uc131 \uacb0\uacfc\ub97c \uc2dc\uac01\ud654\ud55c \uac83\uc785\ub2c8\ub2e4. Fast3R\uc740 \uc2e4\ub0b4 \uacf5\uac04\uc758 \uaddc\uce59\uc801\uc778 \uad6c\uc870(\uc815\uc0ac\uac01\ud615 \ubaa8\uc591\uc758 \ubc29 \ub4f1)\ub97c \ud559\uc2b5\ud558\uace0, '\ub8e8\ud504 \ud074\ub85c\uc800'(\uc774\uc804\uc5d0 \ubc29\ubb38\ud588\ub358 \uc7a5\uc18c\ub97c \ub2e4\uc2dc \uc778\uc2dd\ud558\ub294 \ub2a5\ub825) \uae30\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ubbf8\uc9c0\ub294 Fast3R\uc774 \ub2e4\uc591\ud55c \uac01\ub3c4\uc5d0\uc11c \ucd2c\uc601\ub41c \uc5ec\ub7ec \uc774\ubbf8\uc9c0\ub97c \uae30\ubc18\uc73c\ub85c \uc2e4\ub0b4 \uacf5\uac04\uc758 3D \ubaa8\ub378\uc744 \uc815\ud655\ud558\uac8c \uc7ac\uad6c\uc131\ud558\uace0, \uacf5\uac04\uc801\uc778 \uc5f0\uc18d\uc131\uc744 \uc720\uc9c0\ud558\uba70, \uac19\uc740 \uc704\uce58\ub97c \uc5ec\ub7ec \ubc88 \ubc29\ubb38\ud588\uc744 \ub54c\ub3c4 \uc77c\uad00\ub418\uac8c \uc778\uc2dd\ud558\ub294 \ub2a5\ub825\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 Fast3R\uc758 \uac15\ub825\ud55c 3D \uc7ac\uad6c\uc131 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc911\uc694\ud55c \uc99d\uac70\uc785\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2501.13928/x11.png", "caption": "Figure 11: Model scaling effect. Increasing the size of the Fusion Transformer leads to better camera pose estimation (\u2191\u2191\\uparrow\u2191) and 3D reconstruction (\u2193\u2193\\downarrow\u2193). All models are trained for 60k steps (equivalent to 60 epochs; the main paper uses 100 epochs).", "description": "\uadf8\ub9bc 11\uc740 \ubaa8\ub378 \ud06c\uae30\uac00 Fusion Transformer\uc758 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378 \ud06c\uae30\uac00 \ucee4\uc9c8\uc218\ub85d \uce74\uba54\ub77c \uc790\uc138 \ucd94\uc815\uc758 \uc815\ud655\ub3c4\ub294 \ub192\uc544\uc9c0\uace0(\u2191\u2191), 3D \uc7ac\uad6c\uc131\uc758 \uc815\ud655\ub3c4\ub294 \ud5a5\uc0c1\ub429\ub2c8\ub2e4(\u2193\u2193). \ubaa8\ub4e0 \ubaa8\ub378\uc740 60,000\ubc88\uc758 step\uc73c\ub85c \ud559\uc2b5\ub418\uc5c8\uc73c\uba70, \uc774\ub294 60 epoch\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4(\ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 100 epoch\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4).  \uc774 \uc2e4\ud5d8\uc740 \ubaa8\ub378 \ud06c\uae30\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uce74\uba54\ub77c \uc790\uc138 \ucd94\uc815 \ubc0f 3D \uc7ac\uad6c\uc131 \uc791\uc5c5 \ubaa8\ub450\uc5d0\uc11c \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2501.13928/extracted/6149191/artifacts_for_suppl/gaussian_splat_bundle_adjustment/gaussian_vis_co3d.png", "caption": "Figure 12: Data scaling effect. More training data leads to better camera pose estimation (\u2191\u2191\\uparrow\u2191) and 3D reconstruction (\u2193\u2193\\downarrow\u2193). All models are trained for 60k steps (equivalent to 60 epochs; the main paper uses 100 epochs).", "description": "\uadf8\ub9bc 12\ub294 \ub370\uc774\ud130 \ud06c\uae30\uac00 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud6c8\ub828 \ub370\uc774\ud130\uc758 \uc591\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uce74\uba54\ub77c \uc790\uc138 \ucd94\uc815 \ubc0f 3D \uc7ac\uad6c\uc131 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \ubaa8\ub378\uc740 60,000\ubc88\uc758 \ud559\uc2b5 \ub2e8\uacc4(60 \uc5d0\ud3ed\uc5d0 \ud574\ub2f9, \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 100 \uc5d0\ud3ed \uc0ac\uc6a9)\ub97c \uac70\ucce4\uc2b5\ub2c8\ub2e4.  \uc774 \uadf8\ub798\ud504\ub294 \ud6c8\ub828 \ub370\uc774\ud130\uc758 \uc591\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ubaa8\ub378\uc758 \uce74\uba54\ub77c \uc790\uc138 \ucd94\uc815 \ubc0f 3D \uc7ac\uad6c\uc131 \uc815\ud655\ub3c4\uac00 \uc5b4\ub5bb\uac8c \uac1c\uc120\ub418\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.13928/extracted/6149191/artifacts_for_suppl/gaussian_splat_bundle_adjustment/pose_vis_tnt.png", "caption": "Figure 13: Visualization of Gaussians from unseen poses. The frames are ordered temporally along the direction of the arrows. The middle frames show poses very different from those used for reconstruction, as is evidenced by the large areas with no Gausisans. The scene is fit from 7 images from CO3D.", "description": "\uadf8\ub9bc 13\uc740 Fast3R\uc774 \uc7ac\uad6c\uc131\uc5d0 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\uc740 \uce74\uba54\ub77c \uc790\uc138\uc5d0\uc11c \uc0dd\uc131\ud55c \uac00\uc6b0\uc2dc\uc548\uc758 \uc2dc\uac01\ud654\uc785\ub2c8\ub2e4. \ud654\uc0b4\ud45c \ubc29\ud5a5\uc73c\ub85c \uc2dc\uac04 \uc21c\uc11c\ub300\ub85c \uc815\ub82c\ub41c \ud504\ub808\uc784\ub4e4\uc740 \uac00\uc6b0\uc2dc\uc548\uc774 \uc5c6\ub294 \ub113\uc740 \uc601\uc5ed\uc73c\ub85c \ubcf4\uc544 \uc7ac\uad6c\uc131\uc5d0 \uc0ac\uc6a9\ub41c \uac83\uacfc \ub9e4\uc6b0 \ub2e4\ub978 \uc790\uc138\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uc7a5\uba74\uc740 CO3D\uc758 7\uac1c \uc774\ubbf8\uc9c0\uc5d0\uc11c \uc5bb\uc740 \uac83\uc785\ub2c8\ub2e4.  \ubcf8\ub798 \ucea1\uc158\uc774 \uac04\ub7b5\ud558\uc5ec, \uc774\ud574\ub97c \ub3d5\uae30 \uc704\ud574 \ubcf4\ub2e4 \uc790\uc138\ud55c \uc124\uba85\uc744 \ucd94\uac00\ud588\uc2b5\ub2c8\ub2e4.", "section": "E. More Visualizations"}, {"figure_path": "https://arxiv.org/html/2501.13928/x12.png", "caption": "Figure 14: Bundle adjustment further improves pose. Left: reconstruction from Fast3R. Middle: Original poses pre-GS-BA. Right: Poses after GS-BA.", "description": "\uadf8\ub9bc 14\ub294 \ubc88\ub4e4 \uc870\uc815\uc774 \uc790\uc138 \ucd94\uc815\uc744 \uac1c\uc120\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc5d0\ub294 Fast3R\uc758 \uc7ac\uad6c\uc131 \uacb0\uacfc\uac00, \uac00\uc6b4\ub370\uc5d0\ub294 GS-BA(Gaussian Splatting \uae30\ubc18 \ubc88\ub4e4 \uc870\uc815)\ub97c \uc801\uc6a9\ud558\uae30 \uc804\uc758 \uc6d0\ubcf8 \uc790\uc138\uac00, \uc624\ub978\ucabd\uc5d0\ub294 GS-BA\ub97c \uc801\uc6a9\ud55c \ud6c4\uc758 \uc790\uc138\uac00 \uac01\uac01 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. GS-BA\ub97c \uc801\uc6a9\ud558\uba74 Fast3R\uc758 \uc7ac\uad6c\uc131 \uacb0\uacfc\ub97c \ub354\uc6b1 \uac1c\uc120\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc6d0\ubcf8 \uc790\uc138\uc640 GS-BA \uc801\uc6a9 \ud6c4\uc758 \uc790\uc138\ub97c \ube44\uad50\ud568\uc73c\ub85c\uc368 \ubc88\ub4e4 \uc870\uc815\uc758 \ud6a8\uacfc\ub97c \uba85\ud655\ud788 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}]