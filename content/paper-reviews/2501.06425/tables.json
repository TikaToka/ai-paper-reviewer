[{"content": "Method|KV Cache|# Parameters|# Query Heads|# KV Heads\nMHA|2hd<sub>h</sub>|4d<sub>model</sub><sup>2</sup>|h|h\nMQA|2d<sub>h</sub>|(2+2/h)d<sub>model</sub><sup>2</sup>|h|1\nGQA|2gd<sub>h</sub>|(2+2g/h)d<sub>model</sub><sup>2</sup>|h|g\nMLA|d<sub>c</sub>+d<sub>h</sub><sup>R</sup>|d<sub>c</sub>'(d<sub>model</sub>+hd<sub>h</sub>+hd<sub>h</sub><sup>R</sup>)+d<sub>model</sub>d<sub>h</sub><sup>R</sup>+d<sub>c</sub>(d<sub>model</sub>+2hd<sub>h</sub>)|h|h\nTPA|(R<sub>K</sub>+R<sub>V</sub>)(h+d<sub>h</sub>)|d<sub>model</sub>(R<sub>Q</sub>+R<sub>K</sub>+R<sub>V</sub>)(h+d<sub>h</sub>)+d<sub>model</sub>hd<sub>h</sub>|h|h\nTPA (KVonly)|(R<sub>K</sub>+R<sub>V</sub>)(h+d<sub>h</sub>)|d<sub>model</sub>(R<sub>K</sub>+R<sub>V</sub>)(h+d<sub>h</sub>)+2d<sub>model</sub>hd<sub>h</sub>|h|h\nTPA (Non-contextual A)|(R<sub>K</sub>+R<sub>V</sub>)d<sub>h</sub>|(R<sub>Q</sub>+R<sub>K</sub>+R<sub>V</sub>)(d<sub>model</sub>d<sub>h</sub>+h)+d<sub>model</sub>hd<sub>h</sub>|h|h\nTPA (Non-contextual B)|(R<sub>K</sub>+R<sub>V</sub>)h|(R<sub>Q</sub>+R<sub>K</sub>+R<sub>V</sub>)(d<sub>model</sub>h+d<sub>h</sub>)+d<sub>model</sub>hd<sub>h</sub>|h|h", "caption": "Table 1: \nComparison of different attention mechanisms. Here, RQsubscript\ud835\udc45\ud835\udc44R_{Q}italic_R start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT, RKsubscript\ud835\udc45\ud835\udc3eR_{K}italic_R start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT, and RVsubscript\ud835\udc45\ud835\udc49R_{V}italic_R start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT denote the ranks for queries, keys, and values in TPA, respectively. Variants of TPA, such as TPA (KVonly), TPA (Non-contextual A), and TPA (Non-contextual B), are detailed in Section\u00a03.5. For MLA, dhRsuperscriptsubscript\ud835\udc51\u210e\ud835\udc45d_{h}^{R}italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT and dhsubscript\ud835\udc51\u210ed_{h}italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT are the dimensions for RoPE and non-RoPE parts; dc\u2032superscriptsubscript\ud835\udc51\ud835\udc50\u2032d_{c}^{\\prime}italic_d start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT and dcsubscript\ud835\udc51\ud835\udc50d_{c}italic_d start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT are the dimensions of compressed vectors for query and key-value, respectively.", "description": "\ud45c 1\uc740 \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\ub4e4\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  TPA(Tensor Product Attention)\uc758 \ucffc\ub9ac, \ud0a4, \ubc38\ub958\uc5d0 \ub300\ud55c \ub7ad\ud06c(RQ, RK, RV)\ub97c \ubcf4\uc5ec\uc8fc\uace0, TPA\uc758 \ubcc0\ud615\ub4e4(TPA(KVonly), TPA(Non-contextual A), TPA(Non-contextual B))\uacfc MLA(Multi-head Latent Attention)\uc640\uc758 \ube44\uad50\ub97c \ud1b5\ud574 \ub9e4\uac1c\ubcc0\uc218 \uc218, KV \uce90\uc2dc \ud06c\uae30, \ucffc\ub9ac \ud5e4\ub4dc \uc218, KV \ud5e4\ub4dc \uc218 \ub4f1\uc744 \uc0c1\uc138\ud788 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud2b9\ud788 MLA\uc758 \uacbd\uc6b0 RoPE(Rotary Position Embedding) \uc0ac\uc6a9 \uc5ec\ubd80\uc5d0 \ub530\ub978 \ucc28\uc6d0(dhR, dh)\uacfc \ucffc\ub9ac \ubc0f \ud0a4-\ubc38\ub958 \ubca1\ud130 \uc555\ucd95 \ucc28\uc6d0(dc\u2032, dc)\uc744 \uad6c\ubd84\ud558\uc5ec \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "3 Tensor Product Attention"}, {"content": "| Method | ARC-E | ARC-C | BoolQ | HellaSw. | OBQA | PIQA | W.G. | MMLU | SciQ | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| MHA | 56.52 | 29.27 | 58.84 | 44.06 | 35.00 | 68.44 | 51.07 | 25.35 | 76.40 | 49.44 |\n| MQA | 55.68 | 28.24 | 60.86 | 44.17 | 35.20 | 68.66 | 52.72 | 25.14 | 72.90 | 49.29 |\n| GQA | 54.88 | 29.61 | 56.36 | 43.77 | 35.20 | 68.82 | 52.57 | 25.41 | 74.80 | 49.05 |\n| MLA | 55.30 | 29.27 | 58.96 | 41.92 | 35.40 | 67.25 | 51.78 | 25.20 | 75.60 | 48.96 |\n| **TPA-KVonly** | 57.11 | 30.03 | **61.25** | 44.83 | 34.60 | 69.04 | **54.54** | 23.35 | 74.60 | 49.93 |\n| **TPA** | **59.30** | **31.91** | 60.98 | **45.57** | 34.60 | **69.48** | 53.91 | 24.93 | **77.20** | **50.88** |", "caption": "Table 2: The evaluation results of medium models with different attention mechanisms pretrained using the FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande.", "description": "\ud45c 2\ub294 FineWeb-Edu 100B \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc804 \ud6c8\ub828\ub41c \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uac16\ub294 \uc911\uac04 \ud06c\uae30 \ubaa8\ub378(353M \ub9e4\uac1c\ubcc0\uc218)\uc758 0-shot \uc131\ub2a5 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  lm-evaluation-harness \ub3c4\uad6c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ub418\uc5c8\uc73c\uba70, \uac01 \uc5f4\uc758 \ucd5c\uace0 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. HellaSwag\uc640 WinoGrande\ub294 \uac01\uac01 HellaSw.\uc640 W.G.\ub85c \uc57d\uc5b4 \ucc98\ub9ac\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud45c\ub294 \ub2e4\uc591\ud55c \uc790\uc5f0\uc5b4 \ucc98\ub9ac \uacfc\uc81c(ARC-E, ARC-C, BoolQ, HellaSwag, OBQA, PIQA, WinoGrande, MMLU, SciQ)\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\uc5b4, \uac01 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \uc0c1\ub300\uc801 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.", "section": "4.1 Language Modeling Tasks"}, {"content": "| Method | ARC-E | ARC-C | BoolQ | HellaSw. | OBQA | PIQA | W.G. | MMLU | SciQ | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| MHA | 64.44 | 32.85 | 59.05 | 44.18 | 33.20 | 68.72 | 50.12 | 26.01 | 87.40 | 49.44 |\n| MQA | 64.27 | 32.94 | 57.71 | 44.36 | 31.80 | 68.01 | 51.70 | 25.99 | 86.00 | 49.29 |\n| GQA | 61.70 | 32.17 | 52.81 | 43.99 | 33.80 | 68.50 | 53.35 | 24.44 | 86.40 | 50.80 |\n| MLA | 62.75 | 30.80 | 59.17 | 42.02 | 34.80 | 67.08 | 52.41 | 26.11 | 84.80 | 51.10 |\n| **TPA-KVonly** | 65.99 | 33.70 | 57.49 | 44.47 | 34.20 | 69.53 | 53.28 | 24.23 | 86.50 | 49.93 |\n| **TPA** | 66.54 | 34.47 | 58.96 | 45.35 | 33.00 | 69.21 | 53.99 | 24.51 | 91.30 | 53.04 |", "caption": "Table 3: The evaluation results of medium models with different attention mechanisms pre-trained using the FineWeb-Edu 100B dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande.", "description": "\ud45c 3\uc740 FineWeb-Edu 100B \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc804 \ud6c8\ub828\ub41c \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uac00\uc9c4 \uc911\uac04 \ud06c\uae30 \ubaa8\ub378\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  lm-evaluation-harness\ub97c \uc0ac\uc6a9\ud558\uc5ec 2-shot \uc124\uc815\uc73c\ub85c \ud3c9\uac00\ub418\uc5c8\uc73c\uba70, \uac01 \uc5f4\uc758 \ucd5c\uace0 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  HellaSwag\uc640 WinoGrande\uc758 \uc57d\uc5b4\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0 \uac78\uccd0 \ube44\uad50\ud558\uc5ec, \uc5b4\ub5a4 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc774 \uc8fc\uc5b4\uc9c4 \uc791\uc5c5\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubc1c\ud718\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 Language Modeling Tasks"}, {"content": "| Method | ARC-E | ARC-C | BoolQ | HellaSw. | OBQA | PIQA | W.G. | MMLU | SciQ | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| MHA | 59.93 | 33.62 | 61.93 | 50.63 | 36.00 | 71.06 | 55.41 | 22.87 | **81.20** | 52.52 |\n| MQA | 60.73 | 33.62 | 57.34 | 50.09 | 37.00 | 69.97 | 55.49 | 25.30 | 79.60 | 52.13 |\n| GQA | 61.66 | 34.30 | 58.72 | 49.85 | **38.40** | 71.16 | 53.75 | 25.23 | 77.60 | 52.30 |\n| MLA | 60.73 | 31.57 | **61.74** | 48.96 | 35.40 | 69.59 | 55.09 | **26.39** | 76.70 | 51.80 |\n| **TPA-KVonly** | **63.26** | 34.13 | **61.96** | 50.66 | 37.20 | **72.09** | 55.25 | 26.06 | 81.10 | **53.52** |\n| **TPA** | 63.22 | **35.58** | 60.03 | **51.26** | 36.80 | 71.44 | **55.56** | 24.77 | 79.60 | 53.10 |", "caption": "Table 4: The evaluation results of large models with different attention mechanisms pre-trained using the FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande.", "description": "\ud45c 4\ub294 FineWeb-Edu 100B \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc804 \ud6c8\ub828\ub41c \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uac16\ub294 \ub300\uaddc\ubaa8 \ubaa8\ub378(\ub9e4\uac1c\ubcc0\uc218 7\uc5b5 7300\ub9cc \uac1c)\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  0-shot \uc124\uc815(lm-evaluation-harness \uc0ac\uc6a9)\uc5d0\uc11c \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 ARC-E, ARC-C, BoolQ, HellaSwag, OBQA, PIQA, WinoGrande, MMLU, SciQ \ub4f1 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud574 \ud3c9\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4. \uac01 \uc5f4\uc758 \ucd5c\uace0 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, HellaSwag\ub294 HellaSwag\uc758 \uc57d\uc790\uc774\uace0, W.G.\ub294 WinoGrande\uc758 \uc57d\uc790\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998(MHA, MQA, GQA, MLA, TPA, TPA-KVonly)\uc758 \uc0c1\ub300\uc801 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec TPA \uae30\ubc18 \ubaa8\ub378\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 Language Modeling Tasks"}, {"content": "| Method | ARC-E | ARC-C | BoolQ | HellaSwag | OBQA | PIQA | WG | MMLU | SciQ | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| MHA | 67.85 | 36.35 | 59.82 | 50.22 | 35.00 | 70.67 | 53.35 | 23.92 | 91.10 | 54.25 |\n| MQA | 68.86 | 36.09 | 53.79 | 50.50 | 37.00 | 70.89 | 54.70 | 25.01 | 88.00 | 53.87 |\n| GQA | 69.15 | 36.09 | 58.84 | 50.29 | 36.20 | 70.73 | 54.22 | 26.08 | 90.00 | 54.62 |\n| MLA | 68.56 | 35.41 | 60.12 | 49.18 | 38.00 | 69.21 | 55.25 | 25.29 | 88.20 | 54.36 |\n| **TPA-KVonly** | **71.34** | **37.71** | 59.76 | 51.10 | 36.00 | **71.49** | 54.62 | 25.83 | 90.10 | **55.33** |\n| **TPA** | 70.41 | **37.71** | 60.06 | **51.30** | 34.00 | 71.06 | 54.54 | 25.79 | 90.30 | 55.02 |", "caption": "Table 5: The evaluation results of large models with different attention mechanisms pre-trained using the FineWeb-Edu 100B dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSwag = HellaSwag, WG = WinoGrande.", "description": "\ud45c 5\ub294 FineWeb-Edu 100B \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc804 \ud6c8\ub828\ub41c \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uac00\uc9c4 \ub300\uaddc\ubaa8 \ubaa8\ub378(\ub9e4\uac1c\ubcc0\uc218 7\uc5b5 7300\ub9cc \uac1c)\uc758 \uc131\ub2a5 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  lm-evaluation-harness\ub97c \uc0ac\uc6a9\ud558\uc5ec 2-shot \uc124\uc815\uc5d0\uc11c \ud3c9\uac00\ub418\uc5c8\uc73c\uba70, \uac01 \uc5f4\uc758 \ucd5c\uace0 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  HellaSwag\uc640 WinoGrande\ub294 \uc57d\uc5b4\ub85c \ud45c\uae30\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5(ARC-E, ARC-C, BoolQ, HellaSwag, OBQA, PIQA, WinoGrande, MMLU, SciQ)\uc5d0 \ub300\ud55c \uac01 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\uc5b4, \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \uc0c1\ub300\uc801 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4. \ud2b9\ud788, TPA(Tensor Product Attention)\uc640 TPA-KVonly\uac00 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc5d0 \ube44\ud574 \uc804\ubc18\uc801\uc73c\ub85c \ub192\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.1 Language Modeling Tasks"}, {"content": "| Model Size | #Param | Devices | Micro BS. | GAS. | #Layer | dmodel |\n|---|---|---|---|---|---|---|\n| Small | 124M | 4 \u00d7 A100 GPUs | 24 | 5 | 12 | 768 |\n| Medium | 353M | 8 \u00d7 A100 GPUs | 20 | 3 | 24 | 1024 |\n| Large | 772M | 8 \u00d7 A100 GPUs | 15 | 4 | 36 | 1280 |", "caption": "Table 6: The architecture hyper-parameters and training devices of models. Abbreviations: BS. = Batch Size, GAS. = Gradient Accumulation Steps.", "description": "\ud45c 6\uc740 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc138 \uac00\uc9c0 \ud06c\uae30(\uc791\uc74c, \uc911\uac04, \ud07c)\uc758 \uc5b8\uc5b4 \ubaa8\ub378\uc5d0 \ub300\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\uc640 \ud559\uc2b5 \ud658\uacbd \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \ud30c\ub77c\ubbf8\ud130 \uc218, \uc0ac\uc6a9\ub41c GPU \uc218, \ubc30\uce58 \ud06c\uae30, \uadf8\ub798\ub514\uc5b8\ud2b8 \ub204\uc801 \ub2e8\uacc4, \ub808\uc774\uc5b4 \uc218, \uc784\ubca0\ub529 \ucc28\uc6d0 \ub4f1\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\uc640 \ud559\uc2b5 \uc124\uc815\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\uc5ec, \uc2e4\ud5d8 \uacb0\uacfc\uc758 \uc7ac\ud604\uc131\uc744 \ub192\uc774\uace0 \uc11c\ub85c \ub2e4\ub978 \ubaa8\ub378 \uac04\uc758 \ube44\uad50\ub97c \uc6a9\uc774\ud558\uac8c \ud569\ub2c8\ub2e4. \uc57d\uc5b4 BS\ub294 \ubc30\uce58 \ud06c\uae30(Batch Size), GAS\ub294 \uadf8\ub798\ub514\uc5b8\ud2b8 \ub204\uc801 \ub2e8\uacc4(Gradient Accumulation Steps)\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "B.1 Experimental Settings"}, {"content": "| Model Size | Small | Medium | Large |\n|---|---|---|---|\n| h (MHA) | 12 | 16 | 20 |\n| h (MQA) | 23 | 31 | 39 |\n| h (GQA) | 22 | 30 | 38 |\n| n<sub>h</sub> (MLA) | 12 | 23 | 34 |\n| h (TPA-KVonly) | 22 | 29 | 37 |\n| h (TPA) | 34 | 47 | 61 |\n| d<sub>c</sub> (MLA) | 256 | 512 | 512 |\n| d<sub>c</sub>' (MLA) | 512 | 1024 | 1024 |", "caption": "Table 7: The architecture hyper-parameters for different models.", "description": "\ud45c 7\uc740 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \uc5b8\uc5b4 \ubaa8\ub378\uc5d0 \ub300\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ud06c\uae30(\uc791\uc740 \ubaa8\ub378, \uc911\uac04 \ubaa8\ub378, \ud070 \ubaa8\ub378)\ubcc4\ub85c \ud5e4\ub4dc \uc218(MHA, MQA, GQA, MLA, TPA-KVONLY, TPA), \uc7a0\uc7ac \ucc28\uc6d0(MLA), \uadf8\ub9ac\uace0 \ubaa8\ub378\uc758 \ub2e4\ub978 \uc8fc\uc694 \uad6c\uc870\uc801 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ubaa8\ub378 \uad6c\uc870\uc640 \uc131\ub2a5 \uac04\uc758 \uad00\uacc4\ub97c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3.6 \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98"}, {"content": "| Method | ARC-E | ARC-C | BoolQ | HellaSw. | OBQA | PIQA | W.G. | MMLU | SciQ | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| MHA | 50.63 | 26.96 | 59.39 | 36.18 | 32.00 | 64.96 | **51.85** | 23.40 | 70.30 | 46.19 |\n| MQA | 49.62 | 25.34 | 55.72 | 35.94 | 31.40 | 64.85 | 51.30 | 23.37 | 68.70 | 45.14 |\n| GQA | 48.70 | 25.68 | 56.15 | 35.58 | 31.40 | 64.91 | 51.62 | 23.12 | 68.20 | 45.04 |\n| MLA | 49.66 | 26.45 | **61.22** | 33.94 | 32.40 | 62.73 | 50.43 | 23.29 | 71.50 | 45.74 |\n| **TPA-KVonly** | 51.05 | 26.54 | 57.25 | **36.77** | 32.60 | **65.02** | 50.91 | 23.64 | 69.70 | 45.94 |\n| **TPA** | **51.26** | **27.39** | 57.00 | 36.68 | **32.80** | 64.47 | 49.72 | **24.61** | **72.00** | **46.21** |", "caption": "Table 8: The evaluation results of small models with different attention mechanisms pre-trained using FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande.", "description": "\ud45c 8\uc740 FineWeb-Edu 100B \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc804 \ud6c8\ub828\ub41c \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uac00\uc9c4 \uc18c\uaddc\ubaa8 \ubaa8\ub378\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  lm-evaluation-harness\ub97c \uc0ac\uc6a9\ud558\uc5ec 0-shot \ud3c9\uac00\ub97c \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \uac01 \uc5f4\uc5d0\uc11c \uac00\uc7a5 \uc88b\uc740 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uc57d\uc5b4\ub294 HellaSw.\ub294 HellaSwag, W.G.\ub294 WinoGrande\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc790\uc5f0\uc5b4 \ucc98\ub9ac \uc791\uc5c5(ARC-E, ARC-C, BoolQ, HellaSwag, OBQA, PIQA, WinoGrande, MMLU, SciQ)\uc5d0 \ub300\ud55c \uac01 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\uc5b4, \uc5b4\ub5a4 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc774 \ud2b9\uc815 \uc791\uc5c5\uc5d0 \ub354 \uc801\ud569\ud55c\uc9c0 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "4.1 Language Modeling Tasks"}, {"content": "| Method | ARC-E | ARC-C | BoolQ | HellaSw. | OBQA | PIQA | W.G. | MMLU | SciQ | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| MHA | **57.66** | **28.24** | **57.28** | 36.43 | 29.60 | 64.09 | 51.14 | **26.57** | **82.00** | **48.11** |\n| MQA | 53.79 | 26.35 | 44.95 | 34.18 | 28.80 | 62.79 | 52.01 | 25.91 | 78.10 | 45.21 |\n| GQA | 55.01 | 25.94 | 55.72 | 35.68 | **31.80** | **65.29** | 51.93 | 25.27 | 77.80 | 47.16 |\n| MLA | 52.78 | 26.19 | 57.25 | 33.19 | 29.60 | 63.98 | 50.43 | 24.90 | 76.00 | 46.04 |\n| **TPA-KVonly** | 54.25 | 27.90 | 57.06 | 36.36 | **31.80** | 64.31 | **53.59** | 26.18 | 79.20 | 47.85 |\n| **TPA** | 57.53 | 28.07 | 56.33 | **36.49** | **31.80** | 64.36 | 51.14 | 25.92 | 79.70 | 47.93 |", "caption": "Table 9: The evaluation results of small models with different attention mechanisms on FineWeb-Edu 100B dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande.", "description": "\ud45c 9\ub294 FineWeb-Edu 100B \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc804 \ud6c8\ub828\ub41c \uc18c\uaddc\ubaa8 \ubaa8\ub378(\ub9e4\uac1c\ubcc0\uc218 1\uc5b5 2400\ub9cc \uac1c)\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  lm-evaluation-harness\ub97c \uc0ac\uc6a9\ud558\uc5ec 2-shot \uc124\uc815\uc5d0\uc11c \ud3c9\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4. \uac01 \uc5f4\uc758 \ucd5c\uace0 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uc57d\uc5b4 HellaSw.\ub294 HellaSwag\ub97c, W.G.\ub294 WinoGrande\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c(ARC-E, ARC-C, BoolQ, HellaSwag, OBQA, PIQA, WinoGrande, MMLU, SciQ)\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub2e4\uc591\ud55c \uc9c0\ud45c\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.1 \uc5b8\uc5b4 \ubaa8\ub378\ub9c1 \uc791\uc5c5"}, {"content": "| Method | ARC-E | ARC-C | BoolQ | HellaSw. | OBQA | PIQA | W.G. | MMLU | SciQ | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| MHA | **59.51** | 29.52 | 59.60 | 45.68 | 34.20 | 68.82 | 53.43 | 23.33 | 76.90 | 50.11 |\n| MQA | 57.62 | **31.91** | 59.45 | 45.69 | 35.40 | 69.31 | 53.51 | **26.47** | 74.60 | 50.44 |\n| GQA | 28.67 | 31.48 | 58.29 | 45.45 | 35.20 | 68.50 | **54.46** | 24.58 | 76.50 | 47.01 |\n| MLA | 57.49 | 29.44 | **59.97** | 44.09 | 25.77 | 68.66 | 53.04 | 25.77 | 76.40 | 48.96 |\n| **TPA-KVonly** | 58.01 | 30.12 | 58.01 | 45.95 | 35.60 | 69.10 | 53.12 | 25.39 | 75.10 | 50.04 |\n| **TPA** | 58.38 | 31.57 | 59.39 | **46.83** | **37.00** | **70.02** | 54.06 | 25.52 | **79.90** | **51.41** |", "caption": "Table 10: The evaluation results of medium models (learning rate=6\u00d710\u221246superscript1046\\times 10^{-4}6 \u00d7 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT) with different attention mechanisms pre-trained using FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande.", "description": "\ud45c 10\uc740 FineWeb-Edu 100B \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc804 \ud6c8\ub828\ub41c \uc911\uac04 \ud06c\uae30 \ubaa8\ub378(\ud559\uc2b5\ub960 6 \u00d7 10\u207b\u2074)\uc758 \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  lm-evaluation-harness\ub97c \uc0ac\uc6a9\ud558\uc5ec 0-shot \ud3c9\uac00\ub97c \uc218\ud589\ud588\uc73c\uba70, \uac01 \uc5f4\uc758 \ucd5c\uace0 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  HellaSwag\ub294 HellaSwag\uc758 \uc57d\uc790\uc774\uace0, W.G.\ub294 WinoGrande\uc758 \uc57d\uc790\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc790\uc5f0\uc5b4 \ucc98\ub9ac \uc791\uc5c5(ARC-E, ARC-C, BoolQ, HellaSwag, OBQA, PIQA, WinoGrande, MMLU, SciQ)\uc5d0\uc11c \uac01 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uc5b4\ub5a4 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc774 \ud2b9\uc815 \uc791\uc5c5\uc5d0 \ub354 \ud6a8\uacfc\uc801\uc778\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 Language Modeling Tasks"}, {"content": "| Method | ARC-E | ARC-C | BoolQ | HellaSw. | OBQA | PIQA | W.G. | MMLU | SciQ | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| MHA | 64.73 | 32.42 | 58.29 | 45.89 | 34.20 | 68.50 | 53.20 | 25.86 | 88.00 | 52.34 |\n| MQA | 64.98 | 33.62 | 55.02 | 45.81 | 34.00 | 69.59 | 53.43 | 24.30 | 85.20 | 51.77 |\n| GQA | 65.24 | 33.19 | 56.54 | 45.41 | 34.80 | 69.04 | 55.72 | 24.73 | 87.90 | 52.51 |\n| MLA | 63.80 | 31.06 | 58.50 | 44.19 | 35.40 | 68.44 | 51.62 | 25.22 | 88.50 | 51.86 |\n| **TPA-KVonly** | 64.69 | 32.34 | **59.48** | 46.23 | **35.40** | **70.08** | 54.06 | 25.64 | 86.30 | 52.69 |\n| **TPA** | **67.97** | **34.56** | 57.22 | **46.87** | 34.60 | 69.91 | 52.01 | 25.07 | **89.90** | **53.12** |", "caption": "Table 11: The evaluation results of medium models (learning rate 6\u00d710\u221246superscript1046\\times 10^{-4}6 \u00d7 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT) with different attention mechanisms pre-trained using FineWeb-Edu 100B dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande.", "description": "\ud45c 11\uc740 FineWeb-Edu 100B \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc804 \ud6c8\ub828\ub41c \uc911\uac04 \ud06c\uae30 \ubaa8\ub378(\ud559\uc2b5\ub960 6 \u00d7 10\u207b\u2074)\uc5d0 \ub300\ud574 \uc11c\ub85c \ub2e4\ub978 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  lm-evaluation-harness\ub97c \uc0ac\uc6a9\ud558\uc5ec 2-shot \uc124\uc815\uc73c\ub85c \ud3c9\uac00\ub418\uc5c8\uc73c\uba70, \uac01 \uc5f4\uc758 \ucd5c\uace0 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  HellaSw.\ub294 HellaSwag\ub97c, W.G.\ub294 WinoGrande\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud45c\ub294 \ub2e4\uc591\ud55c \uc790\uc5f0\uc5b4 \ucc98\ub9ac \uc791\uc5c5\uc5d0 \ub300\ud55c \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998 \uc120\ud0dd\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 Language Modeling Tasks"}]