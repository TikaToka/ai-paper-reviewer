[{"figure_path": "https://arxiv.org/html/2501.06425/x1.png", "caption": "Figure 1: Tensor Product Attention (TPA) in the Tensor ProducT ATTenTion Transformer (T6). Different from multi-head attention, in each layer, firstly the hidden state goes through different linear layers to get the latent factor matrices \ud835\udc00\ud835\udc00\\mathbf{A}bold_A\u2019s and \ud835\udc01\ud835\udc01\\mathbf{B}bold_B\u2019s for query, key, and value. We additionally apply RoPE to \ud835\udc01Qsubscript\ud835\udc01\ud835\udc44\\mathbf{B}_{Q}bold_B start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT and \ud835\udc01Ksubscript\ud835\udc01\ud835\udc3e\\mathbf{B}_{K}bold_B start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT for query and key. Then the multi-head query, key, and value vectors are attained by the tensor product of \ud835\udc00(\u22c5)subscript\ud835\udc00\u22c5\\mathbf{A}_{(\\cdot)}bold_A start_POSTSUBSCRIPT ( \u22c5 ) end_POSTSUBSCRIPT and \ud835\udc01(\u22c5)subscript\ud835\udc01\u22c5\\mathbf{B}_{(\\cdot)}bold_B start_POSTSUBSCRIPT ( \u22c5 ) end_POSTSUBSCRIPT. Finally, the output of TPA is produced by scaled dot-product attention followed by linear projection of concatenated results of multiple heads.", "description": "\uadf8\ub9bc 1\uc740 Tensor Product Attention Transformer (T6)\uc5d0\uc11c Tensor Product Attention (TPA) \uba54\ucee4\ub2c8\uc998\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc778 \uba40\ud2f0-\ud5e4\ub4dc \uc5b4\ud150\uc158\uacfc \ub2ec\ub9ac, TPA\ub294 \uac01 \ub808\uc774\uc5b4\uc5d0\uc11c \uc740\ub2c9 \uc0c1\ud0dc\ub97c \uba3c\uc800 \uc5ec\ub7ec \uc120\ud615 \ub808\uc774\uc5b4\ub97c \ud1b5\uacfc\uc2dc\ucf1c \ucffc\ub9ac, \ud0a4, \uac12\uc5d0 \ub300\ud55c \uc7a0\uc7ac \uc694\uc778 \ud589\ub82c A\uc640 B\ub97c \uc5bb\uc2b5\ub2c8\ub2e4.  \ucffc\ub9ac\uc640 \ud0a4\uc5d0\ub294 \ucd94\uac00\uc801\uc73c\ub85c RoPE(Rotary Position Embedding)\uac00 B_Q\uc640 B_K\uc5d0 \uc801\uc6a9\ub429\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c, A\uc640 B\uc758 \ud150\uc11c \uacf1\uc744 \ud1b5\ud574 \uba40\ud2f0-\ud5e4\ub4dc \ucffc\ub9ac, \ud0a4, \uac12 \ubca1\ud130\uac00 \uc0dd\uc131\ub429\ub2c8\ub2e4.  \ucd5c\uc885\uc801\uc73c\ub85c TPA\uc758 \ucd9c\ub825\uc740 \uc2a4\ucf00\uc77c\ub4dc \ub2f7-\ud504\ub85c\ub355\ud2b8 \uc5b4\ud150\uc158\uacfc \uc5ec\ub7ec \ud5e4\ub4dc\uc758 \uc5f0\uacb0\ub41c \uacb0\uacfc\uc5d0 \ub300\ud55c \uc120\ud615 \ud22c\uc601\uc744 \ud1b5\ud574 \uc0dd\uc131\ub429\ub2c8\ub2e4.", "section": "3 Tensor Product Attention"}, {"figure_path": "https://arxiv.org/html/2501.06425/x2.png", "caption": "(a) Training Loss", "description": "\uadf8\ub9bc 2(a)\ub294 FineWeb-Edu-100B \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uc801\uc6a9\ud55c \ub300\uaddc\ubaa8(773M) \ubaa8\ub378\uc758 \ud6c8\ub828 \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 \ud6c8\ub828 \ud1a0\ud070 \uc218 (\ub2e8\uc704: 10\uc5b5)\uc774\uace0 y\ucd95\uc740 \ud6c8\ub828 \uc190\uc2e4 \uac12\uc785\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998(MHA, MQA, GQA, MLA, TPA-KVonly, TPA)\uc758 \ud6c8\ub828 \uc190\uc2e4 \uace1\uc120\uc744 \ube44\uad50\ud558\uc5ec \uac01 \uba54\ucee4\ub2c8\uc998\uc758 \ud6c8\ub828 \ud6a8\uc728\uc131\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  TPA \uae30\ubc18 \ubaa8\ub378\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \ud6c8\ub828 \uc190\uc2e4\uc774 \ub0ae\uc740 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2 \ubc30\uacbd"}, {"figure_path": "https://arxiv.org/html/2501.06425/x3.png", "caption": "(b) Validation Loss", "description": "\uadf8\ub9bc 2(b)\ub294 FineWeb-Edu-100B \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud558\ub294 \ub300\uaddc\ubaa8(773M) \ubaa8\ub378\uc758 \uac80\uc99d \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud6c8\ub828 \ud1a0\ud070 \uc218\uc5d0 \ub530\ub978 \uac80\uc99d \uc190\uc2e4\uc758 \ubcc0\ud654\ub97c \ub098\ud0c0\ub0b4\uba70, TPA \ubc0f TPA-KVonly \ubaa8\ub378\uc774 \ub2e4\ub978 \uae30\uc900 \ubaa8\ub378(MHA, MQA, GQA, MLA)\uc5d0 \ube44\ud574 \ud6c8\ub828 \ud6c4\ubc18\ubd80\uae4c\uc9c0 \uc9c0\uc18d\uc801\uc73c\ub85c \ub0ae\uc740 \uac80\uc99d \uc190\uc2e4\uc744 \uc720\uc9c0\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 TPA \uae30\ubc18 \ubaa8\ub378\uc758 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2 \ubc30\uacbd"}, {"figure_path": "https://arxiv.org/html/2501.06425/x4.png", "caption": "Figure 2: Training loss and validation loss of pretraining large-size (773M) models with different attention mechanisms on the FineWeb-Edu-100B dataset.", "description": "\uadf8\ub9bc 2\ub294 FineWeb-Edu-100B \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc804 \ud6c8\ub828\ub41c \ub300\uaddc\ubaa8(7\uc5b5 7300\ub9cc \ub9e4\uac1c\ubcc0\uc218) \ubaa8\ub378\uc5d0\uc11c \uc11c\ub85c \ub2e4\ub978 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc5d0 \ub530\ub978 \ud6c8\ub828 \uc190\uc2e4 \ubc0f \uac80\uc99d \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998(MHA, MQA, GQA, MLA, TPA-KVonly, TPA)\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uac01 \uba54\ucee4\ub2c8\uc998\uc758 \ud6c8\ub828 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud6c8\ub828 \uc190\uc2e4\uacfc \uac80\uc99d \uc190\uc2e4\uc758 \ucd94\uc774\ub97c \ud1b5\ud574 \uc5b4\ub5a4 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc774 \ub354 \ube60\ub974\uac8c \uc218\ub834\ud558\uace0, \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2 Background"}, {"figure_path": "https://arxiv.org/html/2501.06425/x5.png", "caption": "(a) Training Loss", "description": "\uadf8\ub9bc 2(a)\ub294 FineWeb-Edu-100B \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc11c\ub85c \ub2e4\ub978 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uac00\uc9c4 \ub300\uaddc\ubaa8(773M) \ubaa8\ub378\uc758 \ud6c8\ub828 \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 \ud6c8\ub828 \ud1a0\ud070 \uc218 (B \ub2e8\uc704)\uc774\uace0, y\ucd95\uc740 \ud6c8\ub828 \uc190\uc2e4 \uac12\uc785\ub2c8\ub2e4.  \uc5ec\ub7ec \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998 (MHA, MQA, GQA, MLA, TPA-KVonly, TPA)\uc758 \ud6c8\ub828 \uc190\uc2e4 \uace1\uc120\uc774 \ud45c\uc2dc\ub418\uc5b4 \uac01 \uba54\ucee4\ub2c8\uc998\uc758 \ud6c8\ub828 \ud6a8\uc728\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  TPA \uae30\ubc18 \ubaa8\ub378\uc774 \ub2e4\ub978 \uae30\uc900 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \ud6c8\ub828 \uc190\uc2e4\uc774 \ub354 \ub0ae\uc740 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2 Background"}, {"figure_path": "https://arxiv.org/html/2501.06425/x6.png", "caption": "(b) Validation Loss", "description": "\uadf8\ub9bc 2(b)\ub294 FineWeb-Edu-100B \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub41c \ud06c\uae30\uac00 \ud070(773M) \ubaa8\ub378\uc758 \uac80\uc99d \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998(MHA, MQA, GQA, MLA, TPA-KVonly, TPA)\uc744 \uc0ac\uc6a9\ud55c \ubaa8\ub378\uc758 \uac80\uc99d \uc190\uc2e4 \uace1\uc120\uc744 \ube44\uad50\ud558\uc5ec TPA \uae30\ubc18 \ubaa8\ub378\uc774 \ub2e4\ub978 \uae30\uc900 \ubaa8\ub378\ubcf4\ub2e4 \ud6e8\uc52c \ub0ae\uc740 \uac80\uc99d \uc190\uc2e4\uc744 \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, TPA\uc640 TPA-KVonly \ubaa8\ub378\uc740 \ud6c8\ub828 \uacfc\uc815\uc758 \ub300\ubd80\ubd84\uc5d0\uc11c MHA \uae30\uc900 \ubaa8\ub378\ubcf4\ub2e4 \ub0ae\uc740 \uac80\uc99d \uc190\uc2e4\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4.", "section": "2 \ubc30\uacbd"}, {"figure_path": "https://arxiv.org/html/2501.06425/x7.png", "caption": "Figure 3: The training loss and validation loss of medium-size (353M) models with different attention mechanisms on the FineWeb-Edu 100B dataset.", "description": "\uadf8\ub9bc 3\uc740 FineWeb-Edu 100B \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uc801\uc6a9\ud55c \uc911\uac04 \ud06c\uae30(353M) \ubaa8\ub378\uc758 \ud559\uc2b5 \uc190\uc2e4 \ubc0f \uac80\uc99d \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 \ud559\uc2b5 \ud1a0\ud070 \uc218(\ub2e8\uc704: 10\uc5b5)\uc774\uace0, y\ucd95\uc740 \uc190\uc2e4 \uac12\uc785\ub2c8\ub2e4.  \uac01 \uc120\uc740 \uc11c\ub85c \ub2e4\ub978 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998(MHA, MQA, GQA, MLA, TPA-KVonly, TPA)\uc744 \uc0ac\uc6a9\ud55c \ubaa8\ub378\uc758 \uc190\uc2e4\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \uadf8\ub798\ud504\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \ud559\uc2b5 \ubc0f \uc131\ub2a5\uc744 \ube44\uad50\ud558\uace0, TPA \uae30\ubc18 \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2 \ubc30\uacbd"}, {"figure_path": "https://arxiv.org/html/2501.06425/x8.png", "caption": "(a) Validation Perplexity of Medium Models", "description": "\uadf8\ub9bc 4 (a)\ub294 \uc911\uac04 \ud06c\uae30(353M) \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uac80\uc99d \ud37c\ud50c\ub809\uc11c\ud2f0(perplexity)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998(MHA, MQA, GQA, MLA, TPA-KVonly, TPA)\uc744 \uc0ac\uc6a9\ud558\uc5ec FineWeb-Edu-100B \ub370\uc774\ud130\uc14b\uc73c\ub85c \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \ud6c8\ub828 \ud1a0\ud070 \uc218\uc5d0 \ub530\ub978 \ud37c\ud50c\ub809\uc11c\ud2f0 \ubcc0\ud654\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \uadf8\ub798\ud504\ub97c \ud1b5\ud574 \uac01 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uace0, \ud2b9\ud788 TPA \ubc0f TPA-KVonly\uac00 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc5d0 \ube44\ud574 \ub354 \ub0ae\uc740 \ud37c\ud50c\ub809\uc11c\ud2f0\ub97c \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989,  TPA \uae30\ubc18 \ubaa8\ub378\uc774 \ub354 \ub098\uc740 \uc77c\ubc18\ud654 \uc131\ub2a5\uc744 \uac00\uc9d0\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2501.06425/x9.png", "caption": "(b) Validation Perplexity of Large Models", "description": "\uc774 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc758 \ud070 \ubaa8\ub378(773M \ub9e4\uac1c\ubcc0\uc218)\uc5d0 \ub300\ud55c \uac80\uc99d \ud37c\ud50c\ub809\uc11c\ud2f0(perplexity)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998(MHA, MQA, GQA, MLA, TPA-KVonly, TPA)\uc744 \uc0ac\uc6a9\ud55c \ubaa8\ub378\uc758 \ud559\uc2b5 \uc9c4\ud589\uc5d0 \ub530\ub978 \uac80\uc99d \ud37c\ud50c\ub809\uc11c\ud2f0 \ubcc0\ud654\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uadf8\ub798\ud504\uc758 x\ucd95\uc740 \ud559\uc2b5 \ud1a0\ud070 \uc218(B \ub2e8\uc704), y\ucd95\uc740 \uac80\uc99d \ud37c\ud50c\ub809\uc11c\ud2f0 \uac12\uc785\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uac01 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \uc131\ub2a5\uacfc \ud6a8\uc728\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, TPA\uc640 TPA-KVonly\uac00 \ub2e4\ub978 \uba54\ucee4\ub2c8\uc998\ub4e4\ubcf4\ub2e4 \ub0ae\uc740 \ud37c\ud50c\ub809\uc11c\ud2f0\ub97c \ub2ec\uc131\ud558\uc5ec \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2 \ubc30\uacbd"}, {"figure_path": "https://arxiv.org/html/2501.06425/x10.png", "caption": "Figure 4: The validation perplexity of medium-size (353M) models and large-size (773M) models with different attention mechanisms on the FineWeb-Edu 100B dataset.", "description": "\uadf8\ub9bc 4\ub294 FineWeb-Edu 100B \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud55c \uc911\uac04 \ud06c\uae30(353M) \ubc0f \ub300\uaddc\ubaa8(773M) \ubaa8\ub378\uc758 \uac80\uc99d \ud37c\ud50c\ub809\uc11c\ud2f0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 \ud559\uc2b5 \ud1a0\ud070 \uc218 (\ub2e8\uc704: 10\uc5b5)\uc774\uace0 y\ucd95\uc740 \uac80\uc99d \ud37c\ud50c\ub809\uc11c\ud2f0\uc785\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998(MHA, MQA, GQA, MLA, TPA-KVonly, TPA)\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec TPA \uae30\ubc18 \ubaa8\ub378\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \ub0ae\uc740 \ud37c\ud50c\ub809\uc11c\ud2f0\ub97c \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 TPA\uac00 \uba54\ubaa8\ub9ac \ud6a8\uc728\uc131\uc744 \uc720\uc9c0\ud558\uba74\uc11c \ubaa8\ub378 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8"}]