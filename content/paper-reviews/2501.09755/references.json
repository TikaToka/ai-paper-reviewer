{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is fundamental to many modern large language models and is a key component of ViTok."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-06-01", "reason": "This paper introduced the Vision Transformer (ViT), which is the basis for the ViTok architecture and is critical for its scalability and performance."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-06-01", "reason": "This paper showed how to successfully use Transformers for high-resolution image generation, which is directly relevant to ViTok's downstream applications."}, {"fullname_first_author": "Diederik P. Kingma", "paper_title": "Auto-encoding variational Bayes", "publication_date": "2013-05-01", "reason": "This foundational paper introduced the Variational Autoencoder (VAE), which is the theoretical basis for the autoencoding approach used in ViTok."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-01", "reason": "The Llama architecture, used in ViTok, provides significant improvements in efficiency and scalability for large language models, which directly benefit ViTok's performance."}]}