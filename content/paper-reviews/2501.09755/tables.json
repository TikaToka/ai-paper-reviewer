[{"content": "| Model | Hidden Dimension | Blocks | Heads | Parameters (M) | GFLOPs |\n|---|---|---|---|---|---| \n| Small (S) | 768 | 6 | 12 | 43.3 | 11.6 |\n| Base (B) | 768 | 12 | 12 | 85.8 | 23.1 |\n| Large (L) | 1152 | 24 | 16 | 383.7 | 101.8 |", "caption": "Table 1: Model Sizes and FLOPs for ViTok. We describe ViTok variants by specifying the encoder and decoder sizes separately, along with the tubelet sizes. For example, ViTok S-B/4x16 refers to a model with an encoder of size Small (S) and a decoder of size Base (B), using tubelet size q=4\ud835\udc5e4q=4italic_q = 4 and p=16\ud835\udc5d16p=16italic_p = 16. We modified the traditional Small (S) model by increasing its hidden dimension from 384 to 768 and reducing the number of blocks from 12 to 6 to increase flops and parameters slightly. Additionally, for the Large (L) model, we increased the hidden dimension to 1152 from 1024 to ensure divisibility by 3 for 3D RoPE integration.", "description": "\ud45c 1\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c ViTok \ubaa8\ub378\uc758 \ud06c\uae30\uc640 FLOPs(Floating Point Operations)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. ViTok \ubaa8\ub378\uc740 \uc778\ucf54\ub354\uc640 \ub514\ucf54\ub354\uc758 \ud06c\uae30, \uadf8\ub9ac\uace0 tubelet \ud06c\uae30(q\uc640 p)\ub97c \uc870\ud569\ud558\uc5ec \ub2e4\uc591\ud55c \ubcc0\ud615\uc744 \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, ViTok S-B/4x16\uc740 \uc791\uc740(Small) \uc778\ucf54\ub354\uc640 \uae30\ubcf8(Base) \ub514\ucf54\ub354\ub97c \uc0ac\uc6a9\ud558\uace0, tubelet \ud06c\uae30\ub294 q=4, p=16\uc778 \ubaa8\ub378\uc785\ub2c8\ub2e4. \ub17c\ubb38\uc5d0\uc11c\ub294 \uae30\uc874\uc758 \uc791\uc740(Small) \ubaa8\ub378\uc744 \uc218\uc815\ud558\uc5ec \ud788\ub4e0 \ucc28\uc6d0\uc744 384\uc5d0\uc11c 768\ub85c \ub298\ub9ac\uace0, \ube14\ub85d \uc218\ub97c 12\uc5d0\uc11c 6\uc73c\ub85c \uc904\uc5ec FLOPs\uc640 \ud30c\ub77c\ubbf8\ud130 \uc218\ub97c \uc57d\uac04 \uc99d\uac00\uc2dc\ucf30\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \ud070(Large) \ubaa8\ub378\uc758 \uacbd\uc6b0 3D RoPE(Rotary Position Embedding) \ud1b5\ud569\uc744 \uc704\ud574 \ud788\ub4e0 \ucc28\uc6d0\uc744 1024\uc5d0\uc11c 1152\ub85c \ub298\ub838\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub378\uc758 \ud788\ub4e0 \ucc28\uc6d0, \ube14\ub85d \uc218, \ud5e4\ub4dc \uc218, \ud30c\ub77c\ubbf8\ud130 \uc218(\ubc31\ub9cc \ub2e8\uc704), \uadf8\ub9ac\uace0 FLOPs\uac00 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2 Background"}, {"content": "| Ground Truth | 16384 | 8192 | 4096 | 2048 | 1024 |\n|---|---|---|---|---|---|---|\n| ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/0_truth.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/0_cw64.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/0_cw32.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/0_cw16.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/0_cw8.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/0_cw4.png) |\n| ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/2_truth.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/2_cw64.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/2_cw32.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/2_cw16.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/2_cw8.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/2_cw4.png) |\n| ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/3_truth.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/3_cw64.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/3_cw32.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/3_cw16.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/3_cw8.png) | ![Refer to caption](https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/3_cw4.png) |", "caption": "Table 2: Precision comparison for E\ud835\udc38Eitalic_E. We train ViTok S-B/16 with full float32 precision and bfloat16 autocasting on 256p images in same fashion as Figure\u00a02. The performance is close indicating that E\ud835\udc38Eitalic_E isn\u2019t effected by changing precision.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \uc815\ubc00\ub3c4(float32 \ubc0f bfloat16)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub41c ViTok S-B/16 \ubaa8\ub378\uc758 256p \uc774\ubbf8\uc9c0 \uc7ac\uad6c\uc131 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4. \uadf8\ub9bc 2\uc640 \ub3d9\uc77c\ud55c \ubc29\uc2dd\uc73c\ub85c \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec, E \uac12(\uc7a0\uc7ac \uacf5\uac04\uc758 \ucd1d \ubd80\ub3d9\uc18c\uc218\uc810 \uc218)\uc774 \uc7ac\uad6c\uc131 \uc131\ub2a5\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \ubc18\uba74, \uc815\ubc00\ub3c4 \ubcc0\ud654\ub294 \uc601\ud5a5\uc744 \ubbf8\uce58\uc9c0 \uc54a\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \ub192\uc740 \uc815\ubc00\ub3c4\ub97c \uc0ac\uc6a9\ud574\ub3c4 E \uac12\uc758 \ubcc0\ud654 \uc5c6\uc774 \uc131\ub2a5 \ud5a5\uc0c1\uc740 \uc81c\ud55c\uc801\uc784\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "3.1 E as the Main Bottleneck in Image Reconstruction"}, {"content": "| Precision | rFID | rIS | rSSIM | rPSNR |\n|---|---|---|---|---|\n| BFloat16 | 1.63 | 194 | 0.79 | 26.1 |\n| Float32 | 1.62 | 194 | 0.80 | 26.1 |", "caption": "Table 3: 256p image reconstruction comparison. We assess the reconstruction performance of ViTok on the 256p ImageNet-1K and COCO-2017 validation sets, benchmarking them against CNN-based tokenizers with an equivalent compression ratio (\u00d716absent16\\times 16\u00d7 16 spatial compression). Our ViTok S-B/16 tokenizer achieves state-of-the-art (SOTA) rFID scores on both ImageNet-1K and COCO datasets, outperforming other CNN-based continuous tokenizers while utilizing significantly fewer FLOPs. Furthermore, ViTok maintains competitive performance in SSIM and PSNR metrics compared to prior methods. When scaling decoder size to Large, ViTok improves all its reconstruction numbers.", "description": "\ud45c 3\uc740 256\ud53d\uc140 \ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0 \uc7ac\uad6c\uc131 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  ImageNet-1K\uc640 COCO-2017 \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c ViTok\uc758 \uc7ac\uad6c\uc131 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uace0, \ub3d9\ub4f1\ud55c \uc555\ucd95\ub960(16\ubc30 \uacf5\uac04 \uc555\ucd95)\uc744 \uac00\uc9c4 CNN \uae30\ubc18 \ud1a0\ud06c\ub098\uc774\uc800\uc640 \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.  ViTok S-B/16 \ud1a0\ud06c\ub098\uc774\uc800\ub294 ImageNet-1K\uc640 COCO \ub370\uc774\ud130 \uc138\ud2b8 \ubaa8\ub450\uc5d0\uc11c \ucd5c\ucca8\ub2e8(SOTA) rFID \uc810\uc218\ub97c \ub2ec\uc131\ud558\uc5ec \ub2e4\ub978 CNN \uae30\ubc18 \uc5f0\uc18d \ud1a0\ud06c\ub098\uc774\uc800\ubcf4\ub2e4 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc600\uc73c\uba70, FLOP(\ubd80\ub3d9 \uc18c\uc218\uc810 \uc5f0\uc0b0) \uc0ac\uc6a9\ub7c9\uc740 \ud6e8\uc52c \uc801\uc5c8\uc2b5\ub2c8\ub2e4.  \ub610\ud55c ViTok\uc740 \uae30\uc874 \ubc29\uc2dd\ub4e4\uacfc \ube44\uad50\ud558\uc5ec SSIM \ubc0f PSNR \uc9c0\ud45c\uc5d0\uc11c\ub3c4 \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \uc720\uc9c0\ud588\uc2b5\ub2c8\ub2e4. \ub514\ucf54\ub354 \ud06c\uae30\ub97c Large\ub85c \ud655\uc7a5\ud558\uba74 ViTok\uc758 \ubaa8\ub4e0 \uc7ac\uad6c\uc131 \uc218\uce58\uac00 \ud5a5\uc0c1\ub429\ub2c8\ub2e4.", "section": "4.1 \uc774\ubbf8\uc9c0 \uc7ac\uad6c\uc131 \ubc0f \uc0dd\uc131"}, {"content": "| Name | Params (M) | GFLOPs | ImageNet rFID \u2193 | ImageNet PSNR \u2191 | ImageNet SSIM \u2191 | COCO rFID \u2193 | COCO PSNR \u2191 | COCO SSIM \u2191 |\n|---|---|---|---|---|---|---|---|---|\n| SD-VAE | 59.3 | 162.2 | 0.78 | 25.08 | 0.705 | 4.63 | 24.82 | 0.720 |\n| SDXL-VAE | - | - | 0.68 | 26.04 | **0.834** | 4.07 | 25.76 | **0.845** |\n| OAI | - | - | 0.81 | 24.43 | 0.786 | 4.59 | 24.19 | 0.800 |\n| Cosmos-CI | - | - | 2.02 | **31.74** | 0.700 | 5.6 | **31.74** | 0.703 |\n| ViTok S-B/16 | 129.0 | 34.8 | 0.50 | 24.36 | 0.747 | 3.94 | 24.45 | 0.759 |\n| ViTok S-L/16 | 426.8 | 113.4 | **0.46** | 24.74 | 0.758 | **3.87** | 24.82 | 0.771 |", "caption": "Table 4: 512p image reconstruction comparison. We assess the reconstruction performance of our top-performing tokenizers on the 512p ImageNet-1K and COCO-2017 validation sets, benchmarking them against a CNN-based tokenizer with an equivalent compression ratio (\u00d716absent16\\times 16\u00d7 16 spatial compression). Our ViTok S-B/16 tokenizer maintains state-of-the-art (SOTA) results across all metrics, while maintaining computational significantly reducing flops.", "description": "\ud45c 4\ub294 512p \ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0 \uc7ac\uad6c\uc131 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4. \ucd5c\uace0 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 ViTok \ud1a0\ud06c\ub098\uc774\uc800\ub97c ImageNet-1K \ubc0f COCO-2017 \uac80\uc99d \uc138\ud2b8\uc5d0 \ub300\ud574 \ud3c9\uac00\ud558\uace0, \uacf5\uac04 \uc555\ucd95\ub960\uc774 \ub3d9\uc77c\ud55c CNN \uae30\ubc18 \ud1a0\ud06c\ub098\uc774\uc800\uc640 \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4. ViTok S-B/16 \ud1a0\ud06c\ub098\uc774\uc800\ub294 \ubaa8\ub4e0 \uc9c0\ud45c\uc5d0\uc11c \ucd5c\ucca8\ub2e8(SOTA) \uacb0\uacfc\ub97c \uc720\uc9c0\ud558\uba74\uc11c \uacc4\uc0b0 \ube44\uc6a9\uc744 \ud06c\uac8c \uc904\uc600\uc2b5\ub2c8\ub2e4.", "section": "4.1 \uc774\ubbf8\uc9c0 \uc7ac\uad6c\uc131 \ubc0f \uc0dd\uc131"}, {"content": "| Name | Params(M) | GFLOPs | ImageNet rFID\u2193 | ImageNet PSNR\u2191 | ImageNet SSIM\u2191 | COCO rFID\u2193 | COCO PSNR\u2191 | COCO SSIM\u2191 |\n|---|---|---|---|---|---|---|---|---|\n| SD-VAE | 59.3 | 653.8 | 0.19 | - | - | - | - | - |\n| ViTok S-B/16 | 129.0 | 160.8 | **0.18** | 26.72 | 0.803 | **2.00** | 26.14 | 0.790 |", "caption": "Table 5: 128p Video Reconstruction. We evaluate S-B/4x8, S-B/8x8, and S-B/4x16 on video reconstruction for 16\u00d7\\times\u00d7128\u00d7\\times\u00d7128 video on UCF-101 11k train set. ViTok S-B/4x8 achieves SOTA performance in rFVD and various compression statistics. ViTok S-B/8x8 and ViTok S-B/4x16 also provide competitive reconstruction numbers for the compression rate performed. ViTok also reduces the total FLOPs required from prior transformer based methods.", "description": "\ud45c 5\ub294 128p \ube44\ub514\uc624 \uc7ac\uad6c\uc131 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  UCF-101 \ub370\uc774\ud130\uc14b\uc758 11,000\uac1c \uc601\uc0c1\uc744 \uc0ac\uc6a9\ud558\uc5ec ViTok\uc758 \uc138 \uac00\uc9c0 \ubcc0\ud615 \ubaa8\ub378 (S-B/4x8, S-B/8x8, S-B/4x16)\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc740 \ube44\ub514\uc624 \ud504\ub808\uc784 \ud06c\uae30 16x128x128\uc5d0 \ub300\ud574 \uc11c\ub85c \ub2e4\ub978 \uc555\ucd95\ub960\uc744 \uc801\uc6a9\ud588\uc2b5\ub2c8\ub2e4.  ViTok S-B/4x8 \ubaa8\ub378\uc740 rFVD(Fr\u00e9chet Video Distance) \ubc0f \ub2e4\uc591\ud55c \uc555\ucd95 \ud1b5\uacc4 \uc9c0\ud45c\uc5d0\uc11c \ucd5c\ucca8\ub2e8(SOTA) \uc131\ub2a5\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.  S-B/8x8\uacfc S-B/4x16 \ubaa8\ub378 \ub610\ud55c \uacbd\uc7c1\ub825 \uc788\ub294 \uc7ac\uad6c\uc131 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4.  ViTok\uc740 \uae30\uc874\uc758 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uae30\ubc18 \ubc29\ubc95\ub4e4\uc5d0 \ube44\ud574 \ud544\uc694\ud55c FLOPs(\uc5f0\uc0b0\ub7c9)\uc744 \uc904\uc600\uc2b5\ub2c8\ub2e4.", "section": "4.2 \ube44\ub514\uc624 \uc7ac\uad6c\uc131 \ubc0f \uc0dd\uc131"}, {"content": "| Method | Params(M) | GFLOPs | # Tokens | rFID\u2193 | rFVD\u2193 | PSNR\u2191 | SSIM\u2191 |\n|---|---|---|---|---|---|---|---| \n| TATS | 32 | Unk | 2048 | - | 162 | - | - |\n| MAGViT | 158 | Unk | 1280 | - | 25 | 22.0 | .701 |\n| MAGViTv2 | 158 | Unk | 1280 | - | 16.12 | - | - |\n| LARP-L-Long | 174 | 505.3 | 1024 | - | 20 | - | - |\n| ViTok S-B/4x8 | 129 | 160.8 | 1024 | 2.13 | 8.04 | 30.11 | 0.923 |\n| ViTok S-B/8x8 | 129 | 73.2 | 512 | 2.78 | 20.05 | 28.55 | 0.898 |\n| ViTok S-B/4x16 | 129 | 34.8 | 256 | 4.46 | 53.98 | 26.26 | 0.850 |", "caption": "Table 6: Class Conditional Image Generation Results. We evaluate our tokenizers on class-conditional generation at resolutions of 256p and 512p on the ImageNet-1K dataset compared to prior methods. ViTok performance is competitive with prior continuous diffusion geneation methods like SD-VAE + DiT for both 256p and 512p generation.", "description": "\ud45c 6\uc740 ImageNet-1K \ub370\uc774\ud130\uc14b\uc5d0\uc11c 256p \ubc0f 512p \ud574\uc0c1\ub3c4\ub85c \uc218\ud589\ub41c \uc870\uac74\ubd80 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uc791\uc5c5\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\uc874\uc758 \uc5f0\uc18d \ud655\uc0b0 \uc0dd\uc131 \ubaa8\ub378(\uc608: SD-VAE + DiT)\uacfc \ube44\uad50\ud558\uc5ec ViTok\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4.  \uacb0\uacfc\ub294 256p\uc640 512p \ud574\uc0c1\ub3c4 \ubaa8\ub450\uc5d0\uc11c ViTok\uc758 \uc131\ub2a5\uc774 \uae30\uc874 \ubaa8\ub378\ub4e4\uacfc \uacbd\uc7c1\ub825\uc774 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 \uc774\ubbf8\uc9c0 \uc7ac\uad6c\uc131 \ubc0f \uc0dd\uc131"}, {"content": "| Tokenizer | Generator | Params (M) | 256p Generation gFID\u2193 | 256p Generation gIS\u2191 | 512p Generation gFID\u2193 | 512p Generation gIS\u2191 |\n|---|---|---|---|---|---|---|\n| SD-VAE | LDM-4 | 400 | 3.60 | 247.7 | - | - |\n| SD-VAE | DiT-XL/2 | 675 | 2.27 | 278.24 | 3.04 | 240.82 |\n| Taming-VQGAN | Taming-Transformer | 1400 | 15.78 | - | - | - |\n| TikTok-B | MaskGIT-ViT | 177 | 2.48 | - | 2.49 | - |\n| ViTok S-B/16 | DiT-XL | 675 | 2.45 | 284.39 | 3.41 | 251.46 |", "caption": "Table 7: 128p class conditional video generation. We evaluate our tokenizers on class-conditional generation 16\u00d7\\times\u00d7128\u00d7\\times\u00d7128 on the UCF-101 dataset compared to prior methods. ViTok S-B/4x8 achieves SOTA performance when used with a comparable compression ratio with prior methods, though even our more aggressive tokenizer variant ViTok S-B/8x8 achieves SOTA results compared to prior methods.", "description": "\ud45c 7\uc740 128p \ud574\uc0c1\ub3c4\uc758 \ube44\ub514\uc624\uc5d0 \ub300\ud55c \ud074\ub798\uc2a4 \uc870\uac74\ubd80 \uc0dd\uc131 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  UCF-101 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \ud1a0\ud06c\ub098\uc774\uc800(ViTok \ud3ec\ud568)\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.  \uc555\ucd95\ub960\uc774 \ube44\uc2b7\ud55c \ub2e4\ub978 \ubc29\ubc95\ub4e4\uacfc \ube44\uad50\ud588\uc744 \ub54c, ViTok S-B/4x8\uc740 \ucd5c\ucca8\ub2e8(SOTA) \uc131\ub2a5\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.  \ub354 \uacf5\uaca9\uc801\uc778 \ud1a0\ud06c\ub098\uc774\uc800 \ubcc0\ud615\uc778 ViTok S-B/8x8 \ub610\ud55c \uae30\uc874 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4.", "section": "4.2 \ube44\ub514\uc624 \uc7ac\uad6c\uc131 \ubc0f \uc0dd\uc131"}]