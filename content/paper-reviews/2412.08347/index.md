---
title: "SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs"
summary: "Smaller language models reason better with fine-tuned training recipes."
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Natural Language Processing", "Large Language Models", "ğŸ¢ Saudi Data & Artificial Intelligence Authority",]
showSummary: true
date: 2024-12-11
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2412.08347 {{< /keyword >}}
{{< keyword icon="writer" >}} Sultan Alrashed et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2024-12-16 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2412.08347" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2412.08347" target="_self" >}}
â†— Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/smoltulu-higher-learning-rate-to-batch-size" target="_self" >}}
â†— Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

**Large language models (LLMs) excel, but smaller models are crucial for broader access**.  Existing post-training techniques, effective on LLMs, remain underexplored on smaller scales, hindering efficient model deployment in resource-limited settings. It also raises a problem on the lack of understanding in scaling these techniques into SLMs, particularly on various optimization strategies.  This research tackles efficient post-training for smaller language models. **Existing training strategies for large language models (LLMs) might not suit smaller ones.**

This paper explores how training dynamics, specifically the learning rate to batch size ratio, impact smaller model performance.  By adapting AllenAI's Tulu 3 pipeline to a 1.7B parameter model, the research demonstrates that **optimizing this ratio is crucial**, especially for complex reasoning tasks. **Higher ratios boosted reasoning**, while lower ones benefited pattern recognition. This careful tuning yielded state-of-the-art results for smaller models, demonstrating that efficient model adaptation can bridge the gap between smaller and larger language models.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Reasoning tasks in small language models (SLMs) benefit from higher learning rate to batch size ratios, while pattern recognition tasks favor lower ratios. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} Careful optimization is crucial for maximizing SLM performance, sometimes surpassing larger models on specific tasks. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} The Tulu 3 training pipeline, adapted for smaller models, proves effective, democratizing access to powerful language models and showing potential for even greater efficiency with careful tuning of training parameters such as learning rate and batch size {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
**Smaller language models (SLMs) are crucial for democratizing access to AI but often underperform larger models**. This research demonstrates how careful tuning, especially of the learning rate to batch size ratio, can **significantly enhance SLM capabilities**, opening new avenues for efficient model deployment. The study's insights into optimization dynamics and task-specific tuning are valuable for researchers exploring efficient deep learning and contribute to the growing field of SLM optimization, pushing the boundaries of what's possible with smaller, more accessible models.

------
#### Visual Insights



![](https://arxiv.org/html/2412.08347/extracted/6061800/assets/arc_contour.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ SmolLM2-135M ëª¨ë¸ì˜ ì§€ë„ ë¯¸ì„¸ ì¡°ì •(Supervised Fine-tuning) ê³¼ì •ì—ì„œ í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸°ê°€ ARC ì ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë“±ê³ ì„  ë¶„ì„ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ìƒ‰ìƒ ìŠ¤ì¼€ì¼ì€ ê° ì§€í‘œì— ëŒ€í•œ ì ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, ê²€ì€ìƒ‰ì¼ìˆ˜ë¡ ì„±ëŠ¥ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì„ í†µí•´ í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸° ë¹„ìœ¨ì˜ ìµœì ê°’ì´ ì‘ì—…ì— ë”°ë¼ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (a) Effect of learning rate and batch size on ARC score.
> </details>





{{< table-caption >}}
| Benchmark | Contamination |
|---|---| 
| cais/mmlu | 1.34% |
| openai/openai_humaneval | 0.00% |
| openai/gsm8k | 0.08% |
| ucinlp/drop | 0.20% |
| lighteval/MATH | 0.06% |
| google/IFEval | 0.00% |
| akariasai/PopQA | 7.21% |
| tatsu-lab/alpaca_eval | 1.37% |
| lukaemon/bbh | 0.02% |
| truthfulqa/truthful_qa | 1.47% |
| allenai/wildguardmix | 0.06% |
| allenai/wildjailbreak | 0.00% |
| TIGER-Lab/MMLU-Pro | 0.93% |
| Idavidrein/gpqa | 0.00% |
| lighteval/agi_eval_en | 0.00% |
| bigcode/bigcodebench | 0.00% |
| deepmind/math_dataset | 0.00% |{{< /table-caption >}}

> ğŸ”¼ ì´ í‘œëŠ” SFT ë°ì´í„°ì…‹(allenai/tulu-3-sft-mixture)ì— ì‚¬ìš©ëœ ë²¤ì¹˜ë§ˆí¬ë“¤ì˜ ì˜¤ì—¼ë¥ ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì˜¤ì—¼ë¥ ì´ë€, í›ˆë ¨ ë°ì´í„°ì…‹ì— í‰ê°€ ë°ì´í„°ì…‹ì˜ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆëŠ” ë¹„ìœ¨ì„ ì˜ë¯¸í•˜ë©°, ì´ëŠ” ëª¨ë¸ í‰ê°€ì˜ ì‹ ë¢°ë„ë¥¼ ë–¨ì–´ëœ¨ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í‘œì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ëŒ€ë¶€ë¶„ì˜ ë²¤ì¹˜ë§ˆí¬ëŠ” 1.5% ë¯¸ë§Œì˜ ë‚®ì€ ì˜¤ì—¼ë¥ ì„ ë³´ì´ê³  ìˆìœ¼ë©°, GSM8K, IFEval, AGI Evalê³¼ ê°™ì€ ì£¼ìš” í‰ê°€ ë²¤ì¹˜ë§ˆí¬ëŠ” ì˜¤ì—¼ë¥ ì´ ê±°ì˜ 0ì— ê°€ê¹ìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 1: Contamination of benchmarks in the SFT dataset used allenai/tulu-3-sft-mixture
> </details>





### In-depth insights


#### LR/BS Ratios in SLMs
**í•™ìŠµë¥ (LR) ëŒ€ ë°°ì¹˜ í¬ê¸°(BS) ë¹„ìœ¨ì€ ì†Œê·œëª¨ ì–¸ì–´ ëª¨ë¸(SLM)ì˜ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.** ë³¸ ì—°êµ¬ëŠ” ì¶”ë¡ ê³¼ íŒ¨í„´ ì¸ì‹ ì‘ì—…ì—ì„œ LR/BS ë¹„ìœ¨ì˜ íš¨ê³¼ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì¶”ë¡  ì‘ì—…ì˜ ê²½ìš°, ë” ë†’ì€ LR/BS ë¹„ìœ¨ì´ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìœ¼ë©°, ì´ëŠ” ë” ì¦ì€ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ì™€ ì¼ì¹˜í•©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ íŒ¨í„´ ì¸ì‹ ì‘ì—…ì€ ë” ë‚®ì€ ë¹„ìœ¨ì—ì„œ ìµœì ì˜ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì°¨ì´ëŠ” ëª¨ë¸ ìš©ëŸ‰ì˜ ì œì•½ê³¼ ìµœì í™” ì „ëµì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. **í¥ë¯¸ë¡­ê²Œë„, ë” í° ëª¨ë¸ì—ì„œëŠ” LR/BS ë¹„ìœ¨ì˜ ì˜í–¥ì´ ì‘ì—… ìœ í˜•ì— ë”°ë¼ ëœ ëšœë ·í•´ì§€ëŠ” ê²½í–¥ì´ ìˆì—ˆìŠµë‹ˆë‹¤.** ì´ëŸ¬í•œ ê´€ì°°ì€ ëª¨ë¸ ìš©ëŸ‰ì´ ì¦ê°€í•¨ì— ë”°ë¼ ìµœì í™”ì˜ ìœ ì—°ì„±ì´ í–¥ìƒë¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. **SLM êµìœ¡ì„ ìœ„í•œ ìµœì ì˜ LR/BS ë¹„ìœ¨ì„ ê²°ì •í•˜ëŠ” ë° ìˆì–´ ëª¨ë¸ í¬ê¸°ì™€ ì‘ì—… ìœ í˜• ê°„ì˜ ë³µì¡í•œ ìƒí˜¸ ì‘ìš©ì— ëŒ€í•œ ì¶”ê°€ ì¡°ì‚¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.**

#### SmolTulu Optimization
**SmolTulu ìµœì í™”**ëŠ” ì‘ì€ ì–¸ì–´ ëª¨ë¸ì˜ íš¨ìœ¨ì ì¸ ë¯¸ì„¸ ì¡°ì •ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ì£¼ìš” ëª©í‘œëŠ” **í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸° ë¹„ìœ¨ì„ ì¡°ì •í•˜ì—¬** ì¶”ë¡  ë° íŒ¨í„´ ì¸ì‹ ì‘ì—… ëª¨ë‘ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—°êµ¬ì— ë”°ë¥´ë©´ ë” ë†’ì€ ë¹„ìœ¨ì€ GSM8Kì™€ ê°™ì€ **ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬**ì— ìœ ìµí•œ ë°˜ë©´ ë‚®ì€ ë¹„ìœ¨ì€ HellaSwag ë° IFEvalê³¼ ê°™ì€ **íŒ¨í„´ ì¸ì‹**ì—ì„œ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œê²¬ì€ ëª¨ë¸ í¬ê¸°ì™€ ì‘ì—… ìœ í˜•ì— ë”°ë¼ **ìµœì ì˜ ë¹„ìœ¨ì´ ë‹¤ë¦„**ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. SmolTuluëŠ” ë˜í•œ **Direct Preference Optimization(DPO)**ë¥¼ í™œìš©í•˜ì—¬ ë³´ìƒ ëª¨ë¸ ì—†ì´ ì •ì±…ì„ ì§ì ‘ ìµœì í™”í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê³„ì‚° íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¤ê³  ë” ì‘ì€ ëª¨ë¸ì— ì í•©í•©ë‹ˆë‹¤. ë˜í•œ ì—°êµ¬ëŠ” **ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒì„ ì‚¬ìš©í•œ ê°•í™” í•™ìŠµ(RLVR)**ì˜ ì ì¬ë ¥ì„ íƒêµ¬í•˜ì§€ë§Œ ê³„ì‚° ì œì•½ìœ¼ë¡œ ì¸í•´ ì² ì €í•œ íƒìƒ‰ì´ ì œí•œë©ë‹ˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ SmolTulu ìµœì í™”ëŠ” **ì‘ì€ ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ íš¨ìœ¨ì ì´ê³  íš¨ê³¼ì ì¸ í›ˆë ¨ ì „ëµì„** í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.

#### Task-Specific Dynamics
**ì‘ì—…ë³„ ë™ì  íŠ¹ì„±**ì€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ëª¨ë¸ ìµœì í™”ì˜ ë³µì¡ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì¶”ë¡ ê³¼ íŒ¨í„´ ì¸ì‹ì€ ì„œë¡œ ë‹¤ë¥¸ ìµœì í™” ì „ëµì´ í•„ìš”í•¨ì´ ë¶„ëª…í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ GSM8Kì™€ ê°™ì€ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ëŠ” ë†’ì€ í•™ìŠµë¥  ëŒ€ ë°°ì¹˜ í¬ê¸° ë¹„ìœ¨ì—ì„œ ì´ì ì„ ì–»ëŠ” ë°˜ë©´ HellaSwag ë° IFEvalê³¼ ê°™ì€ íŒ¨í„´ ì¸ì‹ ì‘ì—…ì€ ë‚®ì€ ë¹„ìœ¨ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì°¨ì´ëŠ” ì‘ì—… ìœ í˜•ì— ë”°ë¼ **ëª¨ë¸ ìš©ëŸ‰ í• ë‹¹** ë°©ì‹ì´ ë‹¤ë¦„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. í¥ë¯¸ë¡­ê²Œë„ ì´ëŸ¬í•œ ë™ì  íŠ¹ì„±ì€ ëª¨ë¸ ê·œëª¨ì— ë”°ë¼ ë³€í•©ë‹ˆë‹¤. ì†Œê·œëª¨ ëª¨ë¸ì˜ ê²½ìš° ì´ëŸ¬í•œ ì°¨ì´ëŠ” ë”ìš± ë‘ë“œëŸ¬ì§€ì§€ë§Œ, ëŒ€ê·œëª¨ ëª¨ë¸ì—ì„œëŠ” ì´ëŸ¬í•œ ê²½ê³„ê°€ ëª¨í˜¸í•´ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ê´€ì°°ì€ ì‘ì—…ì˜ ë³µì¡ì„±, ëª¨ë¸ í¬ê¸° ë° ìµœì í™” ì „ëµ ê°„ì˜ **ë³µì¡í•œ ìƒí˜¸ ì‘ìš©**ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³µì¡ì„±ì„ ì™„ì „íˆ ì´í•´í•˜ë ¤ë©´ ì¶”ê°€ ì—°êµ¬ê°€ í•„ìš”í•˜ì§€ë§Œ, ì´ëŸ¬í•œ ì´ˆê¸° ê²°ê³¼ëŠ” ë” íš¨ìœ¨ì ì´ê³  ì‘ì—…ë³„ ëª¨ë¸ ìµœì í™”ë¥¼ ìœ„í•œ **ë§ì¶¤í˜• ì „ëµ** ê°œë°œì˜ ì¤‘ìš”ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

#### Scaling Laws in SFT/DPO
**SFT(Supervised Fine-tuning)**ì™€ **DPO(Direct Preference Optimization)**ì—ì„œ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì€ ëª¨ë¸ í¬ê¸°, ë°ì´í„°ì…‹ í¬ê¸°, í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸° ë“± ë‹¤ì–‘í•œ ìš”ì†Œê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ í¬ê¸°ê°€ ì¦ê°€í• ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ê²½í–¥ì´ ìˆì§€ë§Œ, ìµœì ì˜ í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸°ëŠ” ì‘ì—… ë° ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤. ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ ì´í•´í•˜ë©´ **ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ìµœìƒì˜ ì„±ëŠ¥ì„ ë‹¬ì„±**í•˜ê¸° ìœ„í•œ ì ì ˆí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. íŠ¹íˆ ì‘ì€ ëª¨ë¸ì˜ ê²½ìš°, ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ ì‹ ì¤‘í•˜ê²Œ ì¡°ì •í•˜ì—¬ **ëŒ€ê·œëª¨ ëª¨ë¸ê³¼ì˜ ì„±ëŠ¥ ê²©ì°¨ë¥¼ ì¤„ì´ëŠ” ê²ƒ**ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë²•ì¹™ì€ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ê³¼ ìµœì í™” ê³¼ì •ì—ë„ ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ, SFT ë° DPOì—ì„œ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ íƒêµ¬í•˜ëŠ” ê²ƒì€ íš¨ìœ¨ì ì´ê³  íš¨ê³¼ì ì¸ ëª¨ë¸ í•™ìŠµì— í•„ìˆ˜ì ì…ë‹ˆë‹¤.

#### RLVR Challenges
**RLVR(Reinforcement Learning with Verifiable Rewards)ì€ ì–¸ì–´ ëª¨ë¸ í•™ìŠµì— ìœ ë§í•œ ì ‘ê·¼ ë°©ì‹ì´ì§€ë§Œ, íŠ¹íˆ ì‘ì€ ëª¨ë¸ì— ì ìš©í•  ë•Œ ëª‡ ê°€ì§€ ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤.** ì²«ì§¸, ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒ ì‹ í˜¸ëŠ” ë³¸ì§ˆì ìœ¼ë¡œ sparseí•©ë‹ˆë‹¤. ëª¨ë“  ì¶œë ¥ì— ëŒ€í•´ ëª…í™•í•œ ì˜³ê³  ê·¸ë¦„ì´ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆë¯€ë¡œ ëª¨ë¸ì´ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘˜ì§¸, ì‘ì€ ëª¨ë¸ì€ í° ëª¨ë¸ë³´ë‹¤ ìµœì í™”í•˜ê¸° ê¹Œë‹¤ë¡œìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. **í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸°ì˜ ê´€ê³„ëŠ” ëª¨ë¸ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤**, ì ì ˆí•œ ê· í˜•ì„ ì°¾ê¸°ê°€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, **ê³„ì‚° ë¦¬ì†ŒìŠ¤ì˜ ì œì•½**ì€ ì² ì €í•œ ì‹¤í—˜ì„ ì–´ë µê²Œ ë§Œë“¤ê³  ìµœì ì˜ hyperparameter ì„¤ì •ì„ ì°¾ëŠ” ê²ƒì„ ë°©í•´í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œì—ë„ ë¶ˆêµ¬í•˜ê³ , **RLVRì€ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì— í° ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆê¸°ì— ì¶”ê°€ ì—°êµ¬ê°€ í•„ìš”**í•©ë‹ˆë‹¤.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2412.08347/extracted/6061800/assets/gsm8k_contour.png)

> ğŸ”¼ SmolLM2-135M ëª¨ë¸ì˜ ì§€ë„ ë¯¸ì„¸ ì¡°ì • ì¤‘ í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸°ê°€ GSM8K ì ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë³´ì—¬ì£¼ëŠ” ë“±ê³ ì„  ë¶„ì„ì…ë‹ˆë‹¤. ìƒ‰ìƒ ì²™ë„ëŠ” ê° ì§€í‘œì˜ ì ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©° ê²€ì€ìƒ‰ì¼ìˆ˜ë¡ ì„±ëŠ¥ì´ ë” ë†’ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ íŒ¨í„´ì€ í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸°ì˜ ìµœì  ë¹„ìœ¨ì´ ì‘ì—…ì— ë”°ë¼ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. GSM8Kì™€ ê°™ì€ ì¶”ë¡  ì‘ì—…ì€ í•™ìŠµë¥  ëŒ€ ë°°ì¹˜ í¬ê¸° ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (b) Effect of learning rate and batch size on GSM8K score.
> </details>



![](https://arxiv.org/html/2412.08347/extracted/6061800/assets/hellaswag_contour.png)

> ğŸ”¼ SmolLM2-135M ëª¨ë¸ì˜ ì§€ë„ ë¯¸ì„¸ ì¡°ì • ì¤‘ í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸°ê°€ HellaSwag ì ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë“±ê³ ì„  ë¶„ì„ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ê·¸ë¦¼ì…ë‹ˆë‹¤. ìƒ‰ìƒ ì²™ë„ëŠ” ê° ì§€í‘œì˜ ì ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, ê²€ì€ìƒ‰ì¼ìˆ˜ë¡ ì„±ëŠ¥ì´ ë†’ìŠµë‹ˆë‹¤. ì´ íŒ¨í„´ì€ í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸° ê°„ì˜ ì‘ì—…ë³„ ìµœì  ë¹„ìœ¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. HellaSwagì—ì„œ í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸° ë¹„ìœ¨ì´ ë‚®ì„ ë•Œ ìµœì ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (c) Effect of learning rate and batch size on HellaSwag score.
> </details>



![](https://arxiv.org/html/2412.08347/extracted/6061800/assets/ifeval_contour.png)

> ğŸ”¼ SmolLM2-135M ëª¨ë¸ì˜ ì§€ë„ ë¯¸ì„¸ ì¡°ì • ì¤‘ í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸°ê°€ IFEval ì ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë“±ê³ ì„  ë¶„ì„ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ê·¸ë¦¼ì…ë‹ˆë‹¤. ìƒ‰ìƒ ì²™ë„ëŠ” ê° ì§€í‘œì˜ ì ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, ê²€ì€ìƒ‰ì¼ìˆ˜ë¡ ì„±ëŠ¥ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸°ì˜ ë¹„ìœ¨ì´ ì‘ì—…ì— ë”°ë¼ ìµœì ì˜ ê°’ì„ ê°€ì§ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ IFEvalì˜ ê²½ìš°, ë‚®ì€ í•™ìŠµë¥  ëŒ€ ë°°ì¹˜ í¬ê¸° ë¹„ìœ¨ì—ì„œ ìµœì ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¶”ë¡  ì‘ì—…ê³¼ íŒ¨í„´ ì¸ì‹ ì‘ì—…ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ìµœì í™” ì „ëµì´ í•„ìš”í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> (d) Effect of learning rate and batch size on IFEval score.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Hyperparameter | SmolTulu | SmolTulu | Tulu 3 | Tulu 3 |
|---|---|---|---|---| 
|  | **SFT-1130** | **SFT-1207** | **SFT 8b** | **SFT 70b** |
| Learning Rate (LR) | 9.0e-5 | 3.1e-6 | 5.0e-6 | 2.0e-6 |
| Batch Size (BS) | 8 | 32 | 128 | 128 |
| LR/BS x 10^6 | 11.25 | 0.097 | 0.039 | 0.016 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ì§€ë„ ë¯¸ì„¸ ì¡°ì •(SFT) ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ë‹¤ì–‘í•œ í¬ê¸°ì˜ ëª¨ë¸(SmolTulu, Tulu 3)ì— ëŒ€í•œ í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸° ë° í•™ìŠµë¥  ëŒ€ ë°°ì¹˜ í¬ê¸° ë¹„ìœ¨ì„ ë¹„êµí•©ë‹ˆë‹¤. SmolTulu ëª¨ë¸ì€ ë” í° í•™ìŠµë¥  ëŒ€ ë°°ì¹˜ í¬ê¸° ë¹„ìœ¨ì„ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´ Tulu 3 ëª¨ë¸ì€ ë” ì‘ì€ ë¹„ìœ¨ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¹„ìœ¨ì€ ëª¨ë¸ í¬ê¸° ë° ì‘ì—… ìœ í˜•ì— ë”°ë¼ ìµœì ì˜ í•™ìŠµ ì—­í•™ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 2: SFT hyperparameter selection
> </details>

{{< table-caption >}}
| Metric | SmolTulu<br>SFT-1130 | SmolTulu<br>SFT-1207 | SmolLM2<br>1.7B-Instruct |
|---|---|---|---| 
| ARC (Average) | 51.0 | **55.6** | 51.7 |
| BBH (3-shot) | **34.7** | 34.0 | 32.2 |
| GSM8K (5-shot) | **49.0** | 42.8 | 48.2 |
| HellaSwag | 61.5 | **67.5** | 66.1 |
| IFEval (Average) | **61.0** | 47.8 | 56.7 |
| MMLU-Pro (MCF) | 17.6 | 17.9 | **19.3** |
| PIQA | 72.7 | **76.9** | 74.4 |{{< /table-caption >}}
> ğŸ”¼ SFT ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ: SmolTulu SFT-1130, SmolTulu SFT-1207, SmolLM2 1.7B-Instruct ëª¨ë¸ì˜ ARC, BBH, GSM8K, HellaSwag, IFEval, MMLU-Pro, PIQA ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ ë¹„êµ
> <details>
> <summary>read the caption</summary>
> Table 3: Performance comparison of SFT models
> </details>

{{< table-caption >}}
| Benchmark | Contamination |
|---|---| 
| cais/mmlu | 0.69% |
| openai/openai_humaneval | 0.00% |
| openai/gsm8k | 0.00% |
| ucinlp/drop | 0.07% |
| lighteval/MATH | 0.02% |
| google/IFEval | 0.00% |
| akariasai/PopQA | 2.72% |
| tatsu-lab/alpaca_eval | 1.24% |
| lukaemon/bbh | 0.00% |
| truthfulqa/truthful_qa | 0.61% |
| allenai/wildguardmix | 0.06% |
| allenai/wildjailbreak | 0.00% |
| TIGER-Lab/MMLU-Pro | 0.36% |
| Idavidrein/gpqa | 0.00% |
| lighteval/agi_eval_en | 0.00% |
| bigcode/bigcodebench | 0.00% |
| deepmind/math_dataset | 0.00% |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸(llama-3.1-tulu-3-8b-preference-mixture)ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë° ì‚¬ìš©ëœ DPO ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë²¤ì¹˜ë§ˆí¬ì˜ ì˜¤ì—¼ ë¹„ìœ¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ë²¤ì¹˜ë§ˆí¬ëŠ” 1% ë¯¸ë§Œì˜ ë‚®ì€ ì˜¤ì—¼ë¥ ì„ ë³´ì´ë©°, GSM8K, IFEval, BBHì™€ ê°™ì€ í•µì‹¬ ë²¤ì¹˜ë§ˆí¬ëŠ” ì˜¤ì—¼ì´ ì „í˜€ ì—†ìŠµë‹ˆë‹¤. PopQAì—ì„œ ê°€ì¥ ë†’ì€ ì˜¤ì—¼ë¥ ì¸ 2.72%ê°€ ê´€ì°°ë˜ì—ˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 4: Contamination of benchmarks in the DPO dataset used allenai/llama-3.1-tulu-3-8b-preference-mixture
> </details>

{{< table-caption >}}
| Hyperparameter | SmolTulu<br>DPO-1130 | SmolTulu<br>DPO-1207 | Tulu 3<br>DPO 8b | Tulu 3<br>DPO 70b |
|---|---|---|---|---| 
| Learning Rate (LR) | $8.0 \times 10^{-7}$ | $5 \times 10^{-7}$ | $5.0 \times 10^{-7}$ | $2.0 \times 10^{-7}$ |
| Batch Size (BS) | 12 | 32 | 128 | 128 |
| $\frac{LR}{BS} \times 10^{7}$ | 0.667 | 0.156 | 0.039 | 0.016 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” SmolTulu, Tulu 3 ëª¨ë¸ì˜ DPO ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°, ê·¸ë¦¬ê³  ê·¸ ë¹„ìœ¨ì´ ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 5: DPO hyperparameter selection
> </details>

{{< table-caption >}}
| Metric | SmolTulu<br>DPO-1130 | SmolTulu<br>DPO-1207 | SmolLM2<br>1.7B-Instruct |
|---|---|---|---| 
| ARC (Average) | 51.5 | **57.1** | 51.7 |
| BBH (3-shot) | **33.8** | **34.2** | 32.2 |
| GSM8K (5-shot) | **51.6** | 44.7 | 48.2 |
| HellaSwag | 61.1 | 64.2 | **66.1** |
| IFEval (Average) | **67.7** | 56.6 | 56.7 |
| MMLU-Pro (MCF) | 17.4 | 19.1 | **19.3** |
| PIQA | 72.2 | **76.4** | 74.4 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” Direct Preference Optimization(DPO) ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ ë¹„êµë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. SmolTulu DPO-1130ê³¼ SmolTulu DPO-1207 ë‘ ê°€ì§€ DPO ëª¨ë¸ì˜ ì„±ëŠ¥ì„  SmolLM2 1.7B-Instruct ëª¨ë¸ê³¼ ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë¹„êµí•˜ê³  ìˆìŠµë‹ˆë‹¤. SmolTulu DPO-1130ì€ IFEvalê³¼ GSM8Kì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ë°˜ë©´ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ ARCì™€ PIQAì—ì„œ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 6: Performance comparison of DPO models
> </details>

{{< table-caption >}}
| Hyperparameter | SmolTulu | SmolTulu | Tulu 3 |
|---|---|---|---| 
|  | **RM-1130** | **RM-1207** | **DPO 8b** |
| Learning Rate (LR) | 4.0 Ã— 10â»âµ | 7.5 Ã— 10â»â· | 5.0 Ã— 10â»â· |
| Batch Size (BS) | 4 | 8 | 128 |
| LR/BS Ã— 10â· | 100 | 0.938 | 0.039 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ë³´ìƒ ëª¨ë¸(Reward Model, RM) í•™ìŠµì— ì‚¬ìš©ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. SmolTulu RM-1130, SmolTulu RM-1207, ê·¸ë¦¬ê³  Tulu 3 DPO 8b ëª¨ë¸ì˜ í•™ìŠµë¥ (Learning Rate), ë°°ì¹˜ í¬ê¸°(Batch Size), ê·¸ë¦¬ê³  í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸°ì˜ ë¹„ìœ¨(LR/BS)ì´ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. SmolTulu ëª¨ë¸ë“¤ì€ Tulu 3 ëª¨ë¸ì— ë¹„í•´ ë” ë†’ì€ LR/BS ë¹„ìœ¨ì„ ì‚¬ìš©í•œ ê²ƒì´ íŠ¹ì§•ì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 7: Reward model hyperparameter selection
> </details>

{{< table-caption >}}
| Metric | SmolTulu<br>RM-1130 | SmolTulu<br>RM-1207 | Tulu 3<br>8b RM |
|---|---|---|---| 
| RB Chat | *94.13* | 83.52 | **96.27** |
| RB Chat Hard | 43.64 | *44.74* | **55.92** |
| RB Safety | *75.54* | 64.59 | **84.05** |
| RB Reasoning | *68.01* | 54.71 | **76.50** |
| RB Average | *72.43* | 58.59 | **81.34** |
| UFB | *73.17* | 61.66 | **77.34** |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ë³´ìƒ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•œ í‘œì…ë‹ˆë‹¤. UFBëŠ” allenai/ultrafeedback_binarized_cleanedì˜ test_prefs ë¶„í• ì´ê³  RBëŠ” RewardBenchì…ë‹ˆë‹¤. SmolTulu RM-1130ì€ í‘œì¤€ ì±„íŒ… í‰ê°€ì—ì„œ 94.13%, ì•ˆì „ í‰ê°€ì—ì„œ 75.54%ë¥¼ ë‹¬ì„±í•˜ëŠ” ë“± ë‹¤ì–‘í•œ ì§€í‘œì—ì„œ RewardBenchì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê°•ë ¥í•œ ìƒëŒ€ì  ì„±ëŠ¥ íŒ¨í„´ì€ ë‹¤ë¥¸ ì§€í‘œì—ë„ ì ìš©ë˜ë©°, SmolTulu RM-1130ì€ UltraFeedback ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì„ í˜¸ë„ì—ì„œ 73.17%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ì—¬ ë§¤ê°œë³€ìˆ˜ì˜ ì•½ 21%ë§Œ ì‚¬ìš©í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  Tulu 3ì˜ 77.34%ì— ë¶ˆê³¼ 4.17% í¬ì¸íŠ¸ ì°¨ì´ë¡œ ë’¤ì²˜ì¡ŒìŠµë‹ˆë‹¤. (Shallue et al., 2019)ì˜ í”„ë ˆì„ì›Œí¬ì— ë”°ë¥´ë©´, ì´ëŸ¬í•œ ê²°ê³¼ëŠ” íŠ¹íˆ ì ì ˆí•˜ê²Œ ì¡°  ëœ ìµœì í™” ì „ëµì„ ì‚¬ìš©í•  ë•Œ ë³´ìƒ ëª¨ë¸ë§ì´ ì´ì „ì— ê°€ì •í–ˆë˜ ê²ƒë³´ë‹¤ ë” ì‘ì€ ì•„í‚¤í…ì²˜ë¡œ ë” ìš°ì•„í•˜ê²Œ í™•ì¥ë  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. RM-1130ê³¼ RM-1207(RBì—ì„œ 72.43% ëŒ€ 58.59%) ê°„ì˜ ìƒë‹¹í•œ ì„±ëŠ¥ ê²©ì°¨ëŠ” ì†Œê·œëª¨ ëª¨ë¸ì—ì„œ í•™ìŠµë¥  ëŒ€ ë°°ì¹˜ í¬ê¸° ë¹„ìœ¨ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•œ ì´ì „ ê²°ê³¼ë¥¼ ê°•í™”í•©ë‹ˆë‹¤. RM-1130ì—ì„œ ì‚¬ìš©ëœ ë” ë†’ì€ ë¹„ìœ¨ì€ íŠ¹íˆ ì„ í˜¸ë„ ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ì‘ì—…ì—ì„œ ë³´ìƒ ëª¨ë¸ë§ì— ì¤‘ìš”í•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë” í° ì˜ˆì‹œë‹¹ ì—…ë°ì´íŠ¸ì™€ ë” ë¹ˆë²ˆí•œ ê·¸ë¼ë°ì´ì…˜ ê³„ì‚°ì˜ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ê´€ê³„ì˜ ì •í™•í•œ íŠ¹ì„±ì„ í™•ë¦½í•˜ë ¤ë©´ ë” ê´‘ë²”ìœ„í•œ ì ˆì œ ì—°êµ¬ê°€ í•„ìš”í•˜ë©°, ì´ëŠ” ë” í° ê³„ì‚° ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•œ í–¥í›„ ì‘ì—…ìœ¼ë¡œ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 8: Performance comparison of reward models, where UFB is the test_prefs split of allenai/ultrafeedback_binarized_cleaned and RB is RewardBench.
> </details>

{{< table-caption >}}
| Metric | SmolTulu<br>DPO-1130 | SmolTulu<br>DPO-1207 | SmolTulu<br>SFT-1130 | SmolTulu<br>SFT-1207 | SmolLM2<br>1.7B-Instruct | Llama-3.2<br>1B-Instruct | Qwen2.5<br>1.5B-Instruct |
|---|---|---|---|---|---|---|---|
| ARC (Average) | 51.5 | **57.1** | 51.0 | 55.6 | 51.7 | 41.6 | 46.2 |
| BBH (3-shot) | 33.8 | 34.2 | 34.7 | 34.0 | 32.2 | 27.6 | **35.3** |
| GSM8K (5-shot) | **51.6** | 44.7 | 49.0 | 42.8 | 48.2 | 26.8 | 42.8 |
| HellaSwag | 61.1 | 64.2 | 61.5 | **67.5** | 66.1 | 56.1 | 60.9 |
| IFEval (Average) | **67.7** | 56.6 | 61.0 | 47.8 | 56.7 | 53.5 | 47.4 |
| MMLU-Pro (MCF) | 17.4 | 19.1 | 17.6 | 17.9 | 19.3 | 12.7 | **24.2** |
| PIQA | 72.2 | 76.4 | 72.7 | **76.9** | 74.4 | 72.3 | 73.2 |{{< /table-caption >}}
> ğŸ”¼ ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ê³¼ SmolTuluì˜ ì„±ëŠ¥ì„ ë¹„êµí•œ í‘œì…ë‹ˆë‹¤. SmolTulu DPO-1130, SmolTulu DPO-1207, SmolTulu SFT-1130, SmolTulu SFT-1207, SmolLM2 1.7B-Instruct, Llama-3.2 1B-Instruct, Qwen2.5 1.5B-Instruct ëª¨ë¸ë“¤ì˜ ARC, BBH, GSM8K, HellaSwag, IFEval, MMLU-Pro, PIQA ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ SmolTuluì˜ ì„±ëŠ¥ ìš°ìœ„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 9: A comparison against a wider selection of models
> </details>

{{< table-caption >}}
| Language | Presence (%) |
|---|---| 
| English | 83.13 |
| Hindi | 3.79 |
| Swahili | 2.02 |
| Russian | 2.00 |
| Spanish | 1.15 |
| Arabic | 0.98 |
| Chinese | 0.94 |
| Turkish | 0.87 |
| Urdu | 0.78 |
| Portuguese | 0.77 |
| Vietnamese | 0.64 |
| Japanese | 0.63 |
| French | 0.66 |
| Bulgarian | 0.33 |
| Italian | 0.32 |
| Dutch | 0.31 |
| Polish | 0.25 |
| German | 0.23 |
| Thai | 0.10 |
| Greek | 0.09 |{{< /table-caption >}}
> ğŸ”¼ SFT ë°ì´í„°ì…‹ì— ì‚¬ìš©ëœ allenai/tulu-3-sft-mixtureì˜ ì–¸ì–´ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚¸ í‘œì…ë‹ˆë‹¤. ë°ì´í„°ì…‹ì—ì„œ ì˜ì–´ê°€ 83.13%ë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ì—ˆê³ , ê·¸ ë’¤ë¥¼ íŒë””ì–´(3.79%), ìŠ¤ì™€íë¦¬ì–´(2.02%), ëŸ¬ì‹œì•„ì–´(2.00%) ë“±ì´ ì°¨ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 10: Language distribution in SFT dataset.
> </details>

{{< table-caption >}}
| Language | Presence (%) |
|---|---| 
| English | 86.24 |
| Hindi | 2.23 |
| Russian | 2.03 |
| French | 1.42 |
| Spanish | 1.40 |
| Chinese | 1.37 |
| Urdu | 0.68 |
| Swahili | 0.65 |
| German | 0.58 |
| Japanese | 0.57 |
| Portuguese | 0.54 |
| Arabic | 0.51 |
| Turkish | 0.42 |
| Vietnamese | 0.33 |
| Italian | 0.32 |
| Polish | 0.22 |
| Dutch | 0.18 |
| Bulgarian | 0.18 |
| Thai | 0.10 |
| Greek | 0.04 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” DPO(Direct Preference Optimization) ë° RM(Reward Modeling) ë°ì´í„°ì…‹ì—ì„œ ê° ì–¸ì–´ê°€ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í‘œì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ì˜ì–´ê°€ ê°€ì¥ í° ë¹„ì¤‘ì„ ì°¨ì§€í•˜ê³  ìˆìœ¼ë©°, ê·¸ ì™¸ ë‹¤ì–‘í•œ ì–¸ì–´ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 11: Language distribution in DPO / RM dataset.
> </details>

{{< table-caption >}}
| Language | Presence (%) |
|---|---| 
| English | 94.80 |
| French | 1.29 |
| Spanish | 1.04 |
| Chinese | 0.66 |
| German | 0.55 |
| Russian | 0.48 |
| Japanese | 0.40 |
| Hindi | 0.23 |
| Polish | 0.10 |
| Portuguese | 0.10 |
| Dutch | 0.08 |
| Urdu | 0.07 |
| Bulgarian | 0.07 |
| Italian | 0.05 |
| Turkish | 0.03 |
| Arabic | 0.03 |
| Vietnamese | 0.02 |
| Swahili | 0.00 |{{< /table-caption >}}
> ğŸ”¼ RLVR ë°ì´í„°ì…‹ì˜ ì–¸ì–´ ë¶„í¬ë¥¼ ë³´ì—¬ì£¼ëŠ” í‘œì…ë‹ˆë‹¤. ì£¼ë¡œ ì˜ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, í”„ë‘ìŠ¤ì–´, ìŠ¤í˜ì¸ì–´, ì¤‘êµ­ì–´, ë…ì¼ì–´, ëŸ¬ì‹œì•„ì–´, ì¼ë³¸ì–´ ë“± ë‹¤ì–‘í•œ ì–¸ì–´ê°€ ì†ŒëŸ‰ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 12: Language distribution in RLVR dataset.
> </details>

{{< table-caption >}}
| Benchmark | Contamination |
|---|---| 
| cais/mmlu | 0.65% |
| openai/openai_humaneval | 0.00% |
| openai/gsm8k | 0.00% |
| ucinlp/drop | 0.00% |
| lighteval/MATH | 0.24% |
| google/IFEval | 0.00% |
| akariasai/PopQA | 0.45% |
| tatsu-lab/alpaca_eval | 0.12% |
| lukaemon/bbh | 0.00% |
| truthfulqa/truthful_qa | 0.12% |
| allenai/wildguardmix | 0.00% |
| allenai/wildjailbreak | 0.00% |
| TIGER-Lab/MMLU-Pro | 0.66% |
| Idavidrein/gpqa | 0.00% |
| lighteval/agi_eval_en | 0.00% |
| bigcode/bigcodebench | 0.00% |
| deepmind/math_dataset | 0.00% |{{< /table-caption >}}
> ğŸ”¼ RLVR ë°ì´í„°ì…‹(allenai/RLVR-GSM-MATH-IF-Mixed-Constraints)ì˜ ë²¤ì¹˜ë§ˆí¬ë³„ ì˜¤ì—¼ë„ë¥¼ ë‚˜íƒ€ë‚¸ í‘œì…ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì˜¤ì—¼ë„ëŠ” 1% ë¯¸ë§Œìœ¼ë¡œ ë‚®ê²Œ ë‚˜íƒ€ë‚¬ìœ¼ë©°, GSM8K, IFEval, BBHì™€ ê°™ì€ ì¤‘ìš” ë²¤ì¹˜ë§ˆí¬ëŠ” ì˜¤ì—¼ë„ 0%ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 13: Contamination of benchmarks in the RLVR dataset allenai/RLVR-GSM-MATH-IF-Mixed-Constraints
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}