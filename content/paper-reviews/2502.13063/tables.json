[{"content": "| Model | Metric | Pythia-160M | Pythia-410M | Pythia-1.4B | Llama-3.2-1B | Llama-3.2-3B | Llama-3.1-8B |\n|---|---|---|---|---|---|---|---| \n| PG-19 | Max, tokens | 80 | 96 | 160 | 512 | 1024 | 1568 |\n|  | Gain, tokens | 70.9 \u00b1 11.0 | 81.3 \u00b1 12.0 | 158.0 \u00b1 29.1 | 426.2 \u00b1 79.2 | 720.3 \u00b1 80.2 | 1094.1 \u00b1 127.6 |\n|  | Information Gain | 396.4 \u00b1 46.0 | 431.4 \u00b1 51.6 | 792.8 \u00b1 143.4 | 2119.9 \u00b1 364.8 | 3292.2 \u00b1 320.0 | 4865.7 \u00b1 546.6 |\n| Fanfics | Max, tokens | 80 | 96 | 192 | 512 | 1024 | 1568 |\n|  | Gain, tokens | 70.9 \u00b1 10.5 | 81.2 \u00b1 11.6 | 152.9 \u00b1 28.0 | 449.6 \u00b1 83.7 | 734.1 \u00b1 85.0 | 1071.8 \u00b1 168.6 |\n|  | Information Gain | 378.1 \u00b1 45.9 | 429.8 \u00b1 46.2 | 776.9 \u00b1 132.5 | 2213.8 \u00b1 365.8 | 3354.5 \u00b1 344.9 | 4768.9 \u00b1 622.6 |\n| Random | Max, tokens | 65 | 72 | 139 | 316 | 460 | 792 |\n|  | Gain, tokens | 61.3 \u00b1 6.6 | 76.9 \u00b1 8.7 | 144.4 \u00b1 17.5 | 294.9 \u00b1 64.8 | 456.9 \u00b1 72.1 | 623.2 \u00b1 97.3 |\n|  | Information Gain | 500.8 \u00b1 38.9 | 630.4 \u00b1 65.2 | 1108.2 \u00b1 136.2 | 2265.2 \u00b1 498.7 | 3382.6 \u00b1 585.2 | 4541.2 \u00b1 758.6 |", "caption": "Table 1: \nCompression capacity across different text sources and models.\nWe report Decoding Capacity (in Tokens) (\"Max, tokens\" in the Table), Token Gain, and Information Gain for texts from PG-19, fanfics, random.\nNotably, Information Gain remains similar across all text sources for each model (except random for Pythia).\nFor PG-19 and fanfics, LMs leverage their ability to predict natural language, so the Decoding Capacity (in Tokens) generally exceeds the Token Gain. Furthermore, we find no evidence that the models benefit from potentially having PG-19 in their pre-training data, as their performance on PG-19 is not significantly better than on fanfics published after October 2024.\nIn contrast, random text offers no predictable structure, making these two metrics nearly identical. This allows us to distinguish how many tokens model can predict by itself compared to decoding from trainable input vector.\nLarger models consistently show greater compression capacity across all metrics.", "description": "\ud45c 1\uc740 \ub2e4\uc591\ud55c \ud14d\uc2a4\ud2b8 \uc18c\uc2a4\uc640 \ubaa8\ub378\uc5d0 \ub300\ud55c \uc555\ucd95 \uc6a9\ub7c9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \"Max tokens\" \uc5f4\uc740 \ubaa8\ub378\uc774 \uc131\uacf5\uc801\uc73c\ub85c \ub514\ucf54\ub529\ud560 \uc218 \uc788\ub294 \ucd5c\ub300 \ud1a0\ud070 \uc218\ub97c \ub098\ud0c0\ub0b4\uace0, Token Gain\uc740 \uba54\ubaa8\ub9ac \ubca1\ud130\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c \ucd94\uac00\uc801\uc73c\ub85c \uc815\ud655\ud558\uac8c \uc608\uce21\ub41c \ud1a0\ud070 \uc218\ub97c \ub098\ud0c0\ub0b4\uba70, Information Gain\uc740 \uba54\ubaa8\ub9ac \ubca1\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac10\uc18c\ub41c \ubd88\ud655\uc2e4\uc131(\ud06c\ub85c\uc2a4 \uc5d4\ud2b8\ub85c\ud53c)\uc744 \uce21\uc815\ud569\ub2c8\ub2e4. PG-19\uc640 \ud32c\ud53d\uc158 \ub370\uc774\ud130\uc14b\uc758 \uacbd\uc6b0, \uc5b8\uc5b4 \ubaa8\ub378\uc740 \uc790\uc5f0\uc5b4 \uc608\uce21 \ub2a5\ub825\uc744 \ud65c\uc6a9\ud558\uc5ec \ub514\ucf54\ub529 \uc6a9\ub7c9\uc774 Token Gain\ubcf4\ub2e4 \ub354 \ud070 \uacbd\ud5a5\uc744 \ubcf4\uc774\uba70, 2024\ub144 10\uc6d4 \uc774\ud6c4\uc5d0 \uac8c\uc2dc\ub41c \ud32c\ud53d\uc158 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc131\ub2a5\uc774 PG-19 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc131\ub2a5\uacfc \uc720\uc0ac\ud558\ub2e4\ub294 \uc810\uc744 \ud1b5\ud574 \ubaa8\ub378\uc774 \uc0ac\uc804 \ud559\uc2b5 \ub370\uc774\ud130\uc5d0 \uc758\uc874\ud558\uc9c0 \uc54a\uc74c\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubc18\uba74 \ubb34\uc791\uc704 \ud14d\uc2a4\ud2b8\uc758 \uacbd\uc6b0 \uc608\uce21 \uac00\ub2a5\ud55c \uad6c\uc870\uac00 \uc5c6\uc5b4 \ub450 \uc9c0\ud45c\uac00 \uac70\uc758 \ub3d9\uc77c\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378 \uc790\uccb4\uc758 \uc608\uce21 \ub2a5\ub825\uacfc \ud559\uc2b5 \uac00\ub2a5\ud55c \uc785\ub825 \ubca1\ud130\ub85c\ubd80\ud130\uc758 \ub514\ucf54\ub529\uc744 \uad6c\ubd84\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud070 \ubaa8\ub378\uc77c\uc218\ub85d \ubaa8\ub4e0 \uc9c0\ud45c\uc5d0\uc11c \ub354 \ub192\uc740 \uc555\ucd95 \uc6a9\ub7c9\uc744 \ubcf4\uc785\ub2c8\ub2e4.", "section": "4 Experiments and Results"}, {"content": "| Model Name | Link to HuggingFace | Params (B) | Input Hidden Size | Vocabulary Size |\n|---|---|---|---|---|\n| Pythia-160M | [EleutherAI/pythia-160m](https://huggingface.co/EleutherAI/pythia-160m) | 0.16 | 768 | 50304 |\n| Pythia-410M | [EleutherAI/pythia-410m](https://huggingface.co/EleutherAI/pythia-410m) | 0.41 | 1024 | 50304 |\n| Pythia-1.4B | [EleutherAI/pythia-1.4b](https://huggingface.co/EleutherAI/pythia-1.4b) | 1.4 | 2048 | 50304 |\n| Pythia-2.8B | [EleutherAI/pythia-2.8b](https://huggingface.co/EleutherAI/pythia-2.8b) | 2.8 | 2560 | 50304 |\n| OPT-1.3B | [facebook/opt-1.3b](https://huggingface.co/facebook/opt-1.3b) | 1.3 | 2048 | 50272 |\n| OLMo-1B | [allenai/OLMo-1B-0724-hf](https://huggingface.co/allenai/OLMo-1B-0724-hf) | 1.0 | 2048 | 50304 |\n| Sheared-LLaMA-1.3B | [princeton-nlp/Sheared-LLaMA-1.3B](https://huggingface.co/princeton-nlp/Sheared-LLaMA-1.3B) | 1.3 | 2048 | 32000 |\n| Llama-3.2-1B | [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) | 1.0 | 2048 | 128256 |\n| Llama-3.2-3B | [meta-llama/Llama-3.2-3B](https://huggingface.co/meta-llama/Llama-3.2-3B) | 3.0 | 4096 | 128256 |\n| Llama-3.1-8B | [meta-llama/Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | 8.0 | 4096 | 128256 |", "caption": "Table 2: List of used language models and their parameters.", "description": "\ud45c 2\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc758 \ubaa9\ub85d\uacfc \uac01 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc774\ub984, Hugging Face \ub9c1\ud06c, \ub9e4\uac1c\ubcc0\uc218 \ud06c\uae30(B \ub2e8\uc704), \uc785\ub825 \ubc0f \uc740\ub2c9 \uc0c1\ud0dc \ud06c\uae30, \uc5b4\ud718 \ud06c\uae30\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc2e4\ud5d8\uc5d0 \uc0ac\uc6a9\ub41c \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \ud06c\uae30\uc640 \ud2b9\uc131\uc5d0 \ub300\ud55c \uac1c\uc694\ub97c \uc81c\uacf5\ud558\uc5ec \ub3c5\uc790\ub4e4\uc774 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "A \ubaa8\ub378 \ubc0f \ud559\uc2b5 \uc138\ubd80 \uc815\ubcf4"}]