[{"figure_path": "https://arxiv.org/html/2502.13063/x1.png", "caption": "Figure 1: \nHow many tokens fit into a single input vector? We estimate maximum number of tokens that can be decoded from a single input vector across various language models.", "description": "\uadf8\ub9bc 1\uc740 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\uc5d0\uc11c \ub2e8\uc77c \uc785\ub825 \ubca1\ud130\ub85c\ubd80\ud130 \ub514\ucf54\ub529\ub420 \uc218 \uc788\ub294 \ucd5c\ub300 \ud1a0\ud070 \uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc785\ub825 \ubca1\ud130 \ud06c\uae30\uc5d0 \ub530\ub77c \ub514\ucf54\ub529 \uac00\ub2a5\ud55c \ud1a0\ud070 \uc218\uac00 \ub2e4\ub974\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4.  \uc608\ub97c \ub4e4\uc5b4, \uc791\uc740 \ubaa8\ub378(\uc608: pythia-160m)\uc740 \uba87 \uac1c\uc758 \ud1a0\ud070\ub9cc \ucc98\ub9ac\ud560 \uc218 \uc788\uc9c0\ub9cc, \ub354 \ud070 \ubaa8\ub378(\uc608: Llama-3.2-8B)\uc740 \ud6e8\uc52c \ub354 \ub9ce\uc740 \ud1a0\ud070\uc744 \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc758 \ud06c\uae30\uac00 \ucee4\uc9d0\uc5d0 \ub530\ub77c \ub2e8\uc77c \ubca1\ud130\uc5d0 \ub354 \ub9ce\uc740 \uc815\ubcf4\ub97c \ub2f4\uc744 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ud558\ub294 \uc555\ucd95 \ubc29\ubc95\uc758 \ud6a8\uc728\uc131\uacfc \uac00\ub2a5\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc911\uc694\ud55c \uadfc\uac70\uc785\ub2c8\ub2e4.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13063/extracted/6214616/imgs/compression_schema.drawio.png", "caption": "Figure 2: Compressing text into a [mem] vector. The pre-trained LLM is frozen, and we only finetune one or multiple [mem] vectors to decode the sequence of tokens [t1,t2,\u2026,tN]subscript\ud835\udc611subscript\ud835\udc612\u2026subscript\ud835\udc61\ud835\udc41[t_{1},t_{2},\\ldots,t_{N}][ italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_t start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]. [mem] vectors are trained for each text separately.", "description": "\uadf8\ub9bc 2\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ud14d\uc2a4\ud2b8 \uc555\ucd95 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uba3c\uc800, \ubbf8\ub9ac \ud6c8\ub828\ub41c \ud070 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ubaa8\ub378\uc740 \uace0\uc815\ub418\uc5b4 \uc788\uc73c\uba70, \uc0c8\ub85c\uc6b4 \ub9e4\uac1c\ubcc0\uc218\ub294 \ucd94\uac00\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.  \uc785\ub825 \ud14d\uc2a4\ud2b8\ub294  [t1, t2, ..., tN] \ud1a0\ud070 \uc2dc\ud000\uc2a4\ub85c \ud45c\ud604\ub429\ub2c8\ub2e4.  \uc774 \ud14d\uc2a4\ud2b8\ub97c \uc555\ucd95\ud558\uae30 \uc704\ud574 \ud558\ub098 \ub610\ub294 \uc5ec\ub7ec \uac1c\uc758 '\uba54\ubaa8\ub9ac(mem)' \ubca1\ud130\uac00 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc774 \uba54\ubaa8\ub9ac \ubca1\ud130\ub294 \uac01 \ud14d\uc2a4\ud2b8\uc5d0 \ub300\ud574 \ubcc4\ub3c4\ub85c \ud559\uc2b5\ub429\ub2c8\ub2e4. \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uba54\ubaa8\ub9ac \ubca1\ud130\ub294 LLM\uc758 \uc785\ub825\uc73c\ub85c \ucd94\uac00\ub418\uace0, LLM\uc740 \ub2e4\uc74c \ud1a0\ud070\uc744 \uc608\uce21\ud558\uae30 \uc704\ud574 [m1, ..., mk, t1, t2, ..., ti] \uc2dc\ud000\uc2a4\ub97c \ucc98\ub9ac\ud569\ub2c8\ub2e4. \ubaa9\ud45c\ub294 \uba54\ubaa8\ub9ac \ubca1\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc6d0\ub798 \ud1a0\ud070 \uc2dc\ud000\uc2a4\ub97c \uc815\ud655\ud558\uac8c \ubcf5\uc6d0\ud558\ub294 \uac83\uc785\ub2c8\ub2e4.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.13063/x2.png", "caption": "Figure 3: \nInformation gain of text compression to [mem] vector doesn\u2019t depend on language understanding capabilities of models. Compression results for various language models show the relationship between the cross-entropy (CE) of the original and decompressed texts. If the text CE falls below a model-specific threshold (red line), the text is losslessly compressed. This value is a input vector capacity in terms of entropy (Information Gain, CHsubscript\ud835\udc36\ud835\udc3bC_{H}italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT).\nFor texts that are not perfectly compressed, the compression process reduces their CE to a consistent, model-specific value (bias of the black dashed line).\nLarger models (e.g., Llama-3.1-8B) can handle longer texts before reaching the compression threshold, due to their greater capacity compared to smaller models (e.g., Pythia-160M). This behavior holds for both natural texts (PG-19) and unnatural random texts consisting of random word sequences.", "description": "\uadf8\ub9bc 3\uc740 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc555\ucd95 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4.  \uac00\ub85c\ucd95\uc740 \uc6d0\ubcf8 \ud14d\uc2a4\ud2b8\uc758 \uad50\ucc28 \uc5d4\ud2b8\ub85c\ud53c(CE)\ub97c, \uc138\ub85c\ucd95\uc740 \uc555\ucd95\ub41c \ud14d\uc2a4\ud2b8\uc758 \uad50\ucc28 \uc5d4\ud2b8\ub85c\ud53c\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ube68\uac04\uc0c9 \uc120\uc740 \ubaa8\ub378\ubcc4 \uc784\uacc4\uac12\uc73c\ub85c, \uc774 \uac12\ubcf4\ub2e4 \ub0ae\uc73c\uba74 \ud14d\uc2a4\ud2b8\uac00 \uc190\uc2e4 \uc5c6\uc774 \uc555\ucd95\ub418\uc5c8\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774 \uc784\uacc4\uac12\uc740 \uc5d4\ud2b8\ub85c\ud53c \uad00\uc810\uc5d0\uc11c \uc785\ub825 \ubca1\ud130\uc758 \uc6a9\ub7c9(\uc815\ubcf4 \uc774\ub4dd, C<sub>H</sub>)\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc644\ubcbd\ud558\uac8c \uc555\ucd95\ub418\uc9c0 \uc54a\uc740 \ud14d\uc2a4\ud2b8\uc758 \uacbd\uc6b0, \uc555\ucd95 \uacfc\uc815\uc740 \uc77c\uad00\ub41c \ubaa8\ub378\ubcc4 \uac12(\uac80\uc740\uc0c9 \uc810\uc120\uc758 \ud3b8\ud5a5)\uc73c\ub85c CE\ub97c \uc904\uc785\ub2c8\ub2e4.  Llama-3.1-8B\uc640 \uac19\uc774 \ud070 \ubaa8\ub378\uc740 \uc791\uc740 \ubaa8\ub378(\uc608: Pythia-160M)\uc5d0 \ube44\ud574 \ub354 \ud070 \uc6a9\ub7c9\uc73c\ub85c \uc778\ud574 \uc555\ucd95 \uc784\uacc4\uac12\uc5d0 \ub3c4\ub2ec\ud558\uae30 \uc804\uc5d0 \ub354 \uae34 \ud14d\uc2a4\ud2b8\ub97c \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub7ec\ud55c \uacbd\ud5a5\uc740 PG-19\uc640 \uac19\uc740 \uc790\uc5f0\uc5b4 \ud14d\uc2a4\ud2b8\uc640 \ubb34\uc791\uc704 \ub2e8\uc5b4 \uc2dc\ud000\uc2a4\ub85c \uad6c\uc131\ub41c \ube44\uc790\uc5f0\uc5b4 \ud14d\uc2a4\ud2b8 \ubaa8\ub450\uc5d0\uc11c \ub098\ud0c0\ub0a9\ub2c8\ub2e4.", "section": "4 Experiments and Results"}, {"figure_path": "https://arxiv.org/html/2502.13063/x3.png", "caption": "Figure 4: \nCompression scales linearly with the number of trainable [mem] vectors.\nDashed lines represent ideal linear scaling, and shaded regions indicate \u00b11plus-or-minus1\\pm 1\u00b1 1 std.\nPythia-160m reaches its maximum input context length of 2048 tokens and can successfully encode texts of up to 2016 tokens into 32 [mem] input vectors. Llama-3.2-1B can perfectly decode texts of 7168 tokens from just 16 input vectors.", "description": "\uadf8\ub9bc 4\ub294 \ud559\uc2b5 \uac00\ub2a5\ud55c [mem] \ubca1\ud130\uc758 \uac1c\uc218\uc5d0 \ub530\ub77c \uc555\ucd95 \uc131\ub2a5\uc774 \uc120\ud615\uc801\uc73c\ub85c \uc99d\uac00\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc810\uc120\uc740 \uc774\uc0c1\uc801\uc778 \uc120\ud615\uc801 \ube44\ub840 \uad00\uacc4\ub97c \ub098\ud0c0\ub0b4\uace0, \uc74c\uc601 \uc601\uc5ed\uc740 \ud45c\uc900 \ud3b8\ucc28 \u00b11\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. Pythia-160m \ubaa8\ub378\uc740 \ucd5c\ub300 2048 \ud1a0\ud070\uc758 \uc785\ub825 \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774\ub97c \uac00\uc9c0\uba70, \ucd5c\ub300 2016 \ud1a0\ud070\uc758 \ud14d\uc2a4\ud2b8\ub97c 32\uac1c\uc758 [mem] \uc785\ub825 \ubca1\ud130\ub85c \uc131\uacf5\uc801\uc73c\ub85c \uc778\ucf54\ub529\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Llama-3.2-1B \ubaa8\ub378\uc740 \ub2e8 16\uac1c\uc758 \uc785\ub825 \ubca1\ud130\ub9cc\uc73c\ub85c\ub3c4 7168 \ud1a0\ud070\uc758 \ud14d\uc2a4\ud2b8\ub97c \uc644\ubcbd\ud558\uac8c \ub514\ucf54\ub529\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.4 Scaling Compression with More Trainable Vectors"}, {"figure_path": "https://arxiv.org/html/2502.13063/x4.png", "caption": "Figure 5: \nOnly fraction of learned input embedding information capacity can be utilized. Top. Maximum token capacity (see\u00a0Eq.\u00a01) against gain in correctly decoded tokens shows differences in utilization of learned memory embedding for studied models.\nBottom. Capacity utilization for natural and random texts.", "description": "\uadf8\ub9bc 5\ub294 \ud559\uc2b5\ub41c \uc785\ub825 \uc784\ubca0\ub529\uc758 \uc815\ubcf4 \uc6a9\ub7c9 \ud65c\uc6a9\uc5d0 \ub300\ud55c \ub0b4\uc6a9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0c1\ub2e8 \uadf8\ub798\ud504\ub294 \ucd5c\ub300 \ud1a0\ud070 \uc6a9\ub7c9(\uc2dd 1 \ucc38\uc870)\uacfc \uc815\ud655\ud558\uac8c \ub514\ucf54\ub529\ub41c \ud1a0\ud070\uc758 \uc99d\uac00\ub7c9\uc744 \ube44\uad50\ud558\uc5ec \ud559\uc2b5\ub41c \uba54\ubaa8\ub9ac \uc784\ubca0\ub529\uc758 \ud65c\uc6a9\ub3c4 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud558\ub2e8 \uadf8\ub798\ud504\ub294 \uc790\uc5f0\uc5b4 \ud14d\uc2a4\ud2b8\uc640 \ubb34\uc791\uc704 \ud14d\uc2a4\ud2b8\uc5d0 \ub300\ud55c \uc6a9\ub7c9 \ud65c\uc6a9\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.5 \uc784\ubca0\ub529 \uc6a9\ub7c9 \ud65c\uc6a9"}, {"figure_path": "https://arxiv.org/html/2502.13063/x5.png", "caption": "Figure 6: \nInformation gain of text compression to [mem] vector doesn\u2019t depend on language understanding capabilities of models. Compression results for various language models show the relationship between the cross-entropy (CE) of the original and decompressed texts. If the text CE falls below a model-specific threshold (red line), the text is losslessly compressed. This value is a input vector capacity in terms of entropy (Information Gain, CHsubscript\ud835\udc36\ud835\udc3bC_{H}italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT). For texts that are not perfectly compressed, the compression process reduces their CE to a consistent, model-specific value (bias of the black dashed line).\nLarger models (e.g., Llama-3.1-8B) can handle longer texts before reaching the compression threshold, due to their greater capacity compared to smaller models (e.g., Pythia-160M). This behavior holds for both natural texts (PG-19) and unnatural random texts consisting of random word sequences.", "description": "\uadf8\ub9bc 6\uc740 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc555\ucd95 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. \uac00\ub85c\ucd95\uc740 \uc6d0\ubcf8 \ud14d\uc2a4\ud2b8\uc758 \uad50\ucc28 \uc5d4\ud2b8\ub85c\ud53c(CE), \uc138\ub85c\ucd95\uc740 \uc555\ucd95\ub41c \ud14d\uc2a4\ud2b8\uc758 \uad50\ucc28 \uc5d4\ud2b8\ub85c\ud53c\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ubaa8\ub378\ubcc4 \uc784\uacc4\uac12(\ube68\uac04\uc0c9 \uc120) \uc544\ub798\uc5d0 \uc788\ub294 \ud14d\uc2a4\ud2b8\ub294 \uc190\uc2e4 \uc5c6\uc774 \uc555\ucd95\ub418\uc5c8\uc74c\uc744 \uc758\ubbf8\ud558\uba70, \uc774\ub294 \uc785\ub825 \ubca1\ud130\uc758 \uc5d4\ud2b8\ub85c\ud53c \uc6a9\ub7c9(\uc815\ubcf4 \uc774\ub4dd, C<sub>H</sub>)\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4. \uc784\uacc4\uac12\ubcf4\ub2e4 \ub192\uc740 \ud14d\uc2a4\ud2b8\ub294 \uc77c\uad00\ub41c \ubaa8\ub378\ubcc4 \uac12(\uac80\uc740\uc0c9 \uc810\uc120\uc758 \ud3b8\ud5a5)\uc73c\ub85c CE\uac00 \uac10\uc18c\ud569\ub2c8\ub2e4. Llama-3.1-8B \uc640 \uac19\uc774 \ud070 \ubaa8\ub378\uc740 \uc791\uc740 \ubaa8\ub378(\uc608: Pythia-160M)\ubcf4\ub2e4 \ub354 \uae34 \ud14d\uc2a4\ud2b8\ub97c \uc555\ucd95\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacbd\ud5a5\uc740 PG-19\uc640 \uac19\uc740 \uc790\uc5f0\uc5b4 \ud14d\uc2a4\ud2b8\uc640 \uc784\uc758\uc758 \ub2e8\uc5b4 \uc2dc\ud000\uc2a4\ub85c \uad6c\uc131\ub41c \ube44\uc790\uc5f0\uc5b4 \ud14d\uc2a4\ud2b8 \ubaa8\ub450\uc5d0\uc11c \ub098\ud0c0\ub0a9\ub2c8\ub2e4.", "section": "4 Experiments and Results"}, {"figure_path": "https://arxiv.org/html/2502.13063/x6.png", "caption": "Figure 7: Intra/inter-sample embeddings cosine similarity. Empirical\nprobability densities of cosine similarity between intra-sample and inter-sample\nembeddings. Intra-sample similarities are measured between\nof the same sequence of tokens, while inter-sample between different\nones. Measured on GovReport Huang et\u00a0al. (2021) and Sheared-Llama-1.3B Xia et\u00a0al. (2024).", "description": "\uadf8\ub9bc 7\uc740 \ub3d9\uc77c\ud55c \ubb38\uc7a5\uc5d0 \ub300\ud55c \uc784\ubca0\ub529 \uac04\uc758 \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4\uc640 \uc11c\ub85c \ub2e4\ub978 \ubb38\uc7a5\uc5d0 \ub300\ud55c \uc784\ubca0\ub529 \uac04\uc758 \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4\uc758 \uacbd\ud5d8\uc801 \ud655\ub960 \ubc00\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc790\uae30 \uc720\uc0ac\ub3c4\ub294 \ub3d9\uc77c\ud55c \ud1a0\ud070 \uc2dc\ud000\uc2a4 \uc0ac\uc774\uc5d0\uc11c \uce21\uc815\ub418\uace0, \ud0c0 \uc720\uc0ac\ub3c4\ub294 \uc11c\ub85c \ub2e4\ub978 \uc2dc\ud000\uc2a4 \uc0ac\uc774\uc5d0\uc11c \uce21\uc815\ub429\ub2c8\ub2e4. GovReport(Huang et al., 2021)\uc640 Sheared-LLaMA-1.3B (Xia et al., 2024)\uc5d0\uc11c \uce21\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "D \uc555\ucd95\ub41c \ubca1\ud130 \uad6c\uc870 \uc774\ud574"}, {"figure_path": "https://arxiv.org/html/2502.13063/x7.png", "caption": "Figure 8: Intra-sample Interpolation Accuracies. Interpolation lines\nare provided for all pairs between 32 embeddings of the same input sequence.\nAll interpolation lines are printed with high transparency\nto show denser regions. Grey lines depict minimums and maximums of the\naccuracy for a given interpolation parameter \u03b8\ud835\udf03\\thetaitalic_\u03b8.", "description": " \uadf8\ub9bc 8\uc740 \ub3d9\uc77c\ud55c \uc785\ub825 \uc2dc\ud000\uc2a4\uc5d0 \ub300\ud55c 32\uac1c\uc758 \uc784\ubca0\ub529 \uc30d \uc0ac\uc774\uc758 \ubaa8\ub4e0 \uc120\ud615 \ubcf4\uac04\uc120\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \ubcf4\uac04\uc120\uc740 \ubc00\ub3c4\uac00 \ub192\uc740 \uc601\uc5ed\uc744 \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud574 \ub192\uc740 \ud22c\uba85\ub3c4\ub85c \uadf8\ub824\uc9d1\ub2c8\ub2e4. \ud68c\uc0c9 \uc120\uc740 \uc8fc\uc5b4\uc9c4 \ubcf4\uac04 \ub9e4\uac1c\ubcc0\uc218 \u03b8\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\uc758 \ucd5c\uc18c\uac12\uacfc \ucd5c\ub300\uac12\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub3d9\uc77c\ud55c \uc785\ub825 \uc2dc\ud000\uc2a4\uc5d0 \ub300\ud574 \uc5ec\ub7ec \uac1c\uc758 \uc784\ubca0\ub529\uc774 \uc0dd\uc131\ub420 \uc218 \uc788\uc73c\uba70, \uc774\ub7ec\ud55c \uc784\ubca0\ub529 \uac04\uc758 \ubcf4\uac04\uc774 \ud56d\uc0c1 \uc644\ubcbd\ud55c \uc7ac\uad6c\uc131\uc744 \ubcf4\uc7a5\ud558\uc9c0 \uc54a\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, \uc784\ubca0\ub529 \uacf5\uac04\uc774 \uc5f0\uc18d\uc801\uc774\uc9c0 \uc54a\uace0 \ubd88\uc5f0\uc18d\uc801\uc778 \ubd80\ubd84\uc774 \uc788\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "D. \uc555\ucd95\ub41c \ubca1\ud130\uc758 \uad6c\uc870 \uc774\ud574"}]