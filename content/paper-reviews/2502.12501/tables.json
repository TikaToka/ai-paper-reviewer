[{"content": "| Model | Reward | HelpSteer2 | MTBench | Judge | EvalBias | Avg. |\n|---|---|---|---|---|---|---|\n| **GPT-4o** |  |  |  |  |  |  |\n| _Vanilla_ | 85.2 | 66.1 | 82.1 | 66.3 | 68.5 | 73.6 |\n| _LongPrompt_ | 86.9 | 67.3 | 81.8 | 63.5 | 70.5 | 74.0 |\n| _EvalPlan_ | 88.7 | 65.5 | 81.4 | 62.9 | 74.4 | 74.6 |\n| _16-Criteria_ | 87.3 | 69.1 | 82.8 | 66.6 | 73.7 | 75.9 |\n| _Maj@16_ | 87.9 | 68.9 | 82.4 | 68.6 | 75.5 | 76.7 |\n| _Agg@16_ | 88.1 | 68.7 | 82.6 | 67.2 | 77.9 | 76.9 |\n| CCE-_random@16_ | 91.2 | 69.5 | 83.1 | 68.9 | 80.1 | 78.6 |\n| **CCE**_@16_ | **91.8** | **70.6** | **83.6** | **70.4** | **85.0** | **80.3** |\n| **Qwen 2.5 7B-Instruct** |  |  |  |  |  |  |\n| _Vanilla_ | 78.2 | 60.7 | 76.1 | 58.3 | 57.4 | 66.1 |\n| **CCE**_@16_ | **80.4** | **64.2** | **76.7** | **64.0** | **79.4** | **72.9** |\n| **Qwen 2.5 32B-Instruct** |  |  |  |  |  |  |\n| _Vanilla_ | 87.4 | **72.3** | 79.0 | 68.9 | 71.1 | 75.7 |\n| **CCE**_@16_ | **90.8** | 72.1 | **82.1** | **70.6** | **80.5** | **79.2** |\n| **Qwen 2.5 72B-Instruct** |  |  |  |  |  |  |\n| _Vanilla_ | 85.2 | **69.5** | 79.5 | 68.3 | 68.5 | 74.0 |\n| **CCE**_@16_ | **93.7** | 68.5 | **88.9** | **75.7** | **85.9** | **82.7** |\n| **Llama 3.3 70B-Instruct** |  |  |  |  |  |  |\n| _Vanilla_ | 86.4 | 70.4 | 81.1 | 67.1 | 70.6 | 75.1 |\n| **CCE**_@16_ | **91.7** | **71.3** | **83.5** | **69.7** | **79.2** | **79.1** |", "caption": "Table 1: Accuracy of LLM-as-a-Judge on pair-wise comparison benchmarks. CCE can consistently enhance the LLM-as-a-Judge\u2019s performance across 5 benchmarks, especially considerably outperforming other scaling inference strategies, like maj@16. The highest values are bolded. Here, CCE-random refers to replacing the \u201cCriticizing Selection+++Outcome-Removal Processing\u201d with \u201cRandom Selection\u201d.", "description": "\ud45c 1\uc740 \uc30d\ubc29 \ube44\uad50 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c LLM-as-a-Judge\uc758 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c LLM\uacfc \ubca4\uce58\ub9c8\ud06c\uc5d0 \uac78\uccd0 CCE(Crowd-based Comparative Evaluation)\uac00 LLM-as-a-Judge\uc758 \uc131\ub2a5\uc744 \uc9c0\uc18d\uc801\uc73c\ub85c \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, maj@16\uacfc \uac19\uc740 \ub2e4\ub978 \uc2a4\ucf00\uc77c\ub9c1 \ucd94\ub860 \uc804\ub7b5\uc5d0 \ube44\ud574 \uc0c1\ub2f9\ud788 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. \uac00\uc7a5 \ub192\uc740 \uac12\uc740 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, CCE-random\uc740 \"\ube44\ud310\uc801 \uc120\ud0dd \ubc0f \uacb0\uacfc \uc81c\uac70 \ucc98\ub9ac\"\ub97c \"\ubb34\uc791\uc704 \uc120\ud0dd\"\uc73c\ub85c \ub300\uccb4\ud55c \uacbd\uc6b0\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ub2e4\uc12f \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c(REWARDBENCH, HELPSTEER2, MTBENCH HUMAN, JUDGEBENCH, EVALBIAS)\uc5d0\uc11c\uc758 \ud3c9\uade0 \uc815\ud655\ub3c4 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 \uc2e4\ud5d8 \uc124\uc815"}, {"content": "| Model | # of Training Samples | RewardBench | HelpSteer2 | MTBench Human | JudgeBench | EvalBias | Avg. | \n|---|---|---|---|---|---|---|---| \n| **JudgeLM-7B** (Zhu et al., 2023a) | 100,000 | **46.4** | **60.1** | 64.1 | 32.6 | **42.4** | **49.1** | \n| **PandaLM-7B** (Wang et al., 2024c) | 300,000 | 45.7 | 57.6 | **75.0** | 36.0 | 27.0 | 48.3 | \n| **Auto-J-13B** (Li et al., 2024b) | 4,396 | **47.5** | **65.1** | **75.2** | **50.9** | 16.5 | **51.0** | \n| **Prometheus-7B** (Kim et al., 2024a) | 100,000 | 34.6 | 30.8 | 52.8 | 9.3 | 11.7 | 27.8 | \n| **Prometheus-2-7B** (Kim et al., 2024b) | 300,000 | 43.7 | 37.6 | 55.0 | **39.4** | **39.8** | 43.1 | \n| **Llama-3.1-8B-Tuned** |  |  |  |  |  |  |  | \n| *Synthetic Judgment from Vanilla* | 10,000 | 66.8 | 56.0 | 71.6 | **60.1** | 34.2 | 57.7 | \n| *Synthetic Judgment from Vanilla* | 30,000 | **72.5** | **58.6** | **73.9** | 50.4 | **46.2** | 60.3 | \n| *Synthetic Judgment from CCE* | 10,000 | 69.7 | **58.6** | 72.7 | **66.4** | 38.7 | **61.2** | \n| *Synthetic Judgment from CCE* | 30,000 | **70.0** | **60.1** | **74.3** | 50.3 | **50.7** | **61.1** | \n| **Qwen 2.5-7B-Tuned** |  |  |  |  |  |  |  | \n| *Synthetic Judgment from Vanilla* | 10,000 | 68.1 | 55.6 | 70.7 | **50.2** | 38.4 | 56.6 | \n| *Synthetic Judgment from Vanilla* | 30,000 | 71.4 | 56.2 | 75.1 | 48.2 | 54.7 | 61.1 | \n| *Synthetic Judgment from CCE* | 10,000 | 68.8 | 56.7 | 71.3 | 49.8 | 40.2 | 57.4 | \n| *Synthetic Judgment from CCE* | 30,000 | **73.3** | **59.5** | **74.9** | 50.1 | **57.1** | **63.0** | \n| *Mix Synthetic Judgment from CCE&Vanilla* | 60,000 | **74.1** | **60.7** | **76.6** | **61.6** | **60.6** | **66.7** |", "caption": "Table 2: Accuracy of Trained small LLM-as-a-Judge on pair-wise comparison benchmarks. Under the same preference pairs data, the model trained with judgments synthesized using CCE achieves more reliable evaluation results. The highest values are bolded, and the second highest is underlined.", "description": "\ud45c 2\ub294 \uc18c\uaddc\ubaa8 LLM-as-a-Judge \ubaa8\ub378\uc744 \uc30d\ubc29\ud5a5 \ube44\uad50 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ud3c9\uac00\ud55c \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub3d9\uc77c\ud55c \uc120\ud638\ub3c4 \ub370\uc774\ud130 \uc30d\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c, CCE\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ub41c \ud310\ub2e8\uc73c\ub85c \ud559\uc2b5\ub41c \ubaa8\ub378\uc774 \ub354\uc6b1 \uc2e0\ub8b0\ud560 \uc218 \uc788\ub294 \ud3c9\uac00 \uacb0\uacfc\ub97c \ub3c4\ucd9c\ud569\ub2c8\ub2e4. \uac00\uc7a5 \ub192\uc740 \uac12\uc740 \uad75\uac8c \ud45c\uc2dc\ud558\uace0, \ub450 \ubc88\uc9f8\ub85c \ub192\uc740 \uac12\uc740 \ubc11\uc904\uc744 \ucce4\uc2b5\ub2c8\ub2e4.", "section": "3.5 Extensive Application\u2013Crowd Rejection Sampling in SFT"}, {"content": "| Rejection Sampling Method | MTBench | AlpacaEval-2 |\n|---|---|---|\n| **Llama 3.1 8B Base** |  |  |\n| **Instructions from LIMA # 1K** |  |  |\n| *Random Sampling* | 4.33 | 2.89/3.29 |\n| *Vanilla Rejection Sampling* | 4.28 | **2.91/3.29** |\n| *Crowd Rejection Sampling* | **4.53** | **3.02/3.31** |\n| **Instructions from Tulu 3 # 10K** |  |  |\n| *Random Sampling* | 7.51 | 12.81/12.45 |\n| *Vanilla Rejection Sampling* | **7.56** | **19.92/17.17** |\n| *Crowd Rejection Sampling* | **7.63** | **22.23/19.74** |\n| **Qwen 2.5 7B Base** |  |  |\n| **Instructions from LIMA # 1K** |  |  |\n| *Random Sampling* | **8.06** | **14.52/9.40** |\n| *Vanilla Rejection Sampling* | 7.91 | 14.40/9.44 |\n| *Crowd Rejection Sampling* | **8.63** | **14.86/9.59** |\n| **Instructions from Tulu 3 # 10K** |  |  |\n| *Random Sampling* | 8.36 | 21.39/13.68 |\n| *Vanilla Rejection Sampling* | **8.46** | **22.71/16.44** |\n| *Crowd Rejection Sampling* | **8.41** | **23.78/17.56** |", "caption": "Table 3: SFT Rejection Sampling Performance on the Instruction-Following Benchmark.\nThe model fine-tuned with responses sampled using CCE demonstrates improved generative performance.", "description": "\ubcf8 \ud45c\ub294 \uc9c0\uc2dc\uc0ac\ud56d \ub530\ub974\uae30 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 SFT(Supervised Fine-Tuning) \uac70\ubd80 \uc0d8\ud50c\ub9c1 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. CCE(Crowd-based Comparative Evaluation) \ubc29\uc2dd\uc73c\ub85c \uc0d8\ud50c\ub9c1\ub41c \uc751\ub2f5\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\uc774 \ud5a5\uc0c1\ub41c \uc0dd\uc131 \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c\ub294, LIMA\uc640 TULU3-SFT \ub450 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc138 \uac00\uc9c0 \uc0d8\ud50c\ub9c1 \ubc29\ubc95(\ubb34\uc791\uc704 \uc0d8\ud50c\ub9c1, \uc77c\ubc18\uc801\uc778 \uac70\ubd80 \uc0d8\ud50c\ub9c1, CCE \uae30\ubc18 \uac70\ubd80 \uc0d8\ud50c\ub9c1)\uc744 \ube44\uad50\ud558\uc5ec, CCE \uae30\ubc18 \uac70\ubd80 \uc0d8\ud50c\ub9c1\uc774 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \uc81c\uacf5\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ubbf8\uc138\uc870\uc815\ub41c \ubaa8\ub378\uc758 \uc0dd\uc131 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\uc5ec \ud655\uc778\ub429\ub2c8\ub2e4.", "section": "3.5 Extensive Application\u2013Crowd Rejection Sampling in SFT"}, {"content": "| Strategy | # of Selection Samples | RewardBench | HelpSteer2 | MTBench Human | JudgeBench | EvalBias | Avg. | \n|---|---|---|---|---|---|---|---| \n| _Random-Selection_ | 8 | 91.0 | 69.9 | 82.6 | 68.7 | 78.4 | 78.1 | \n| _Praising-Selection_ | 8 | 86.6 | 64.2 | 81.5 | 67.1 | 77.7 | 75.4 | \n| _Criticizing-Selection_ | 8 | 91.2 | 69.2 | 83.0 | 68.9 | 79.1 | 78.3 | \n| _Balanced-Selection_ | 8 | 90.7 | 68.6 | 82.8 | 67.4 | 78.7 | 77.6 | \n| _Outcome-Removal Random-Selection_ | 8 | 91.5 | 69.9 | 83.0 | 69.4 | 79.5 | 78.7 | \n| _Outcome-Removal Criticizing-Selection (Sota)_ | 8 | 91.5 | 70.1 | 83.2 | 69.5 | 79.9 | 78.8 | \n| _Random-Selection_ | 16 | 91.2 | 69.5 | 83.1 | 68.9 | 80.1 | 78.6 | \n| _Praising-Selection_ | 16 | 87.0 | 68.4 | 82.0 | 67.1 | 77.9 | 76.5 | \n| _Criticizing-Selection_ | 16 | 90.8 | 69.7 | 83.0 | 69.6 | 82.9 | 79.2 | \n| _Balanced-Selection_ | 16 | 90.6 | 69.3 | 82.9 | 68.0 | 79.6 | 78.1 | \n| _Outcome-Removal Random-Selection_ | 16 | 91.7 | 69.7 | 83.2 | 70.0 | 81.5 | 79.2 | \n| _Outcome-Removal Criticizing-Selection(Sota)_ | 16 | 91.8 | 70.6 | 83.6 | 70.4 | 85.0 | 80.3 | ", "caption": "Table 4: Accuracy of LLM-as-a-Judge on pair-wise comparison benchmarks. CCE can consistently enhance the LLM-as-a-Judge\u2019s performance during the test-time inference phase, especially considerably outperforming maj@16 and 16-Criteria; under the same preference pairs data, the model trained with judgments synthesized using CCE achieves more reliable evaluation results.", "description": "\ud45c 4\ub294 \uc30d\ubc29 \ube44\uad50 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c LLM-as-a-Judge\uc758 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uc778 CCE(Crowd-based Comparative Evaluation)\uac00 LLM-as-a-Judge\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4. \ud2b9\ud788, CCE\ub294 \uae30\uc874\uc758 \ub2e4\uc218\uacb0 \ud22c\ud45c \ubc29\uc2dd(maj@16)\uc774\ub098 \uae30\uc900 \ud655\uc7a5 \ubc29\uc2dd(16-Criteria)\ubcf4\ub2e4 \ud6e8\uc52c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. \ub3d9\uc77c\ud55c \uc120\ud638\ub3c4 \uc30d \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c, CCE\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ub41c \ud310\ub2e8\uc73c\ub85c \ud559\uc2b5\ub41c \ubaa8\ub378\uc774 \ub354\uc6b1 \uc2e0\ub8b0\ud560 \uc218 \uc788\ub294 \ud3c9\uac00 \uacb0\uacfc\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.  \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \ucd94\ub860 \ub2e8\uacc4\uc5d0\uc11c CCE\uac00 LLM-as-a-Judge\uc758 \uc131\ub2a5\uc744 \uc77c\uad00\ub418\uac8c \ud5a5\uc0c1\uc2dc\ud0a4\uace0, \ud2b9\ud788 maj@16 \ubc0f 16-Criteria\ubcf4\ub2e4 \ud6e8\uc52c \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.  \uac19\uc740 \uc120\ud638\ub3c4 \ub370\uc774\ud130 \uc30d\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c, CCE\ub85c \ud569\uc131\ub41c \ud310\ub2e8\uc73c\ub85c \ud6c8\ub828\ub41c \ubaa8\ub378\uc774 \ub354\uc6b1 \uc2e0\ub8b0\ud560 \uc218 \uc788\ub294 \ud3c9\uac00 \uacb0\uacfc\ub97c \ub3c4\ucd9c\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 \uc2e4\ud5d8 \uc124\uc815"}, {"content": "| Instruction | Response A | Response B | Vanilla LLM-as-a-Judge | CCE |\n|---|---|---|---|---|\n| Classify this tweet from the categories of positive, negative, or neutral. Tweet: I\u2019m so thankful for all the wonderful people in my life | <p style=\"color:#FF0000;\"><b>Response A</b></p>  The sentiment of this social media comment is positive. The use of the word \"grateful\" implies appreciation and thankfulness, while \"amazing experiences\" suggest that these experiences have been very fulfilling or enjoyable. | <p style=\"color:#32CD32;\"><b>Response B</b></p> Positive | Assistant A\u2019s response is more detailed than Assistant B\u2019s. <p style=\"color:#000000;background-color:#FBE5D6;\">It highlights specific words and phrases in the tweet, such as \u201cthankful\u201d and \u201cwonderful people,\u201d to justify the positive sentiment classification. This explanation adds depth and clarity to the response, making it more helpful and informative.</p> Assistant B\u2019s response is less informative and helpful.  - Final Verdict: <p style=\"color:#FF0000;\">[[A]]</p> | <p style=\"font-weight:bold;font-variant:small-caps;\">CCE</p> <p style=\"font-style:italic;font-weight:bold;\">AC Judgment</p> <p style=\"text-decoration:underline;text-decoration-style:wavy;\">However, Assistant A makes a mistake by referencing words not present in the tweet, such as \"grateful\" and \"amazing experiences,\"</p> Assistant C also classifies the tweet as positive and provides a detailed explanation. Assistant A <p style=\"color:#000000;background-color:#E2F0D9;\">, but it inaccurately references words not present in the tweet, such as \"grateful\" and \"amazing experiences.\" This detracts from the accuracy of the response and could potentially confuse the user.</p> Assistant B <p style=\"color:#000000;background-color:#E2F0D9;\">is concise and correctly classifies the tweet as positive. However, it lacks any explanation or reasoning, which limits its helpfulness and depth.</p> In comparing the two, <p style=\"color:#000000;background-color:#E2F0D9;\">Given the importance of accuracy and explanation in sentiment analysis,</p> - Final Verdict: <p style=\"color:#00FF00;\">[[B]]</p> |", "caption": "Table 5: A pairwise comparison case evaluated by different methods. Preference refers to right result and Preference refers to wrong result. We emphasize the noisy evaluation elements in \norange, while highlighting the useful elements of the evaluation in \nlimongreen.", "description": "\ud45c 5\ub294 \uc11c\ub85c \ub2e4\ub978 \ud3c9\uac00 \ubc29\ubc95\uc73c\ub85c \ud3c9\uac00\ub41c \uc9dd \ube44\uad50 \uc0ac\ub840\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'Preference'\ub294 \uc815\ub2f5\uc744, 'Preference'\ub294 \uc624\ub2f5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc624\ub80c\uc9c0\uc0c9\uc740 \ud3c9\uac00\uc5d0\uc11c \ub178\uc774\uc988\uac00 \ub9ce\uc740 \uc694\uc18c\ub97c, \uc5f0\ub450\uc0c9\uc740 \uc720\uc6a9\ud55c \uc694\uc18c\ub97c \uac15\uc870\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc778\uac04 \ud3c9\uac00\uc790\uc640 \uc720\uc0ac\ud558\uac8c LLM \uae30\ubc18 \ud310\ub2e8\uc774 \uc7a1\uc74c\uc774 \ub9ce\uace0 \ubd88\uc644\uc804\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub300\ud45c\uc801\uc778 \uc0ac\ub840\ub97c \uc81c\uc2dc\ud558\uba70, \uc81c\uc548\ub41c Crowd Comparative Evaluation (CCE) \ubc29\ubc95\uc774 \uc774\ub7ec\ud55c \ubb38\uc81c\uc810\uc744 \uc5b4\ub5bb\uac8c \ud574\uacb0\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  CCE\ub294 \ub2e4\uc591\ud55c \uad00\uc810\uc744 \uc81c\uacf5\ud558\ub294 \ucd94\uac00\uc801\uc778 \uc751\ub2f5\uc744 \ud65c\uc6a9\ud558\uc5ec, \ub354 \ud3ec\uad04\uc801\uc774\uace0 \uc815\ud655\ud55c \ud3c9\uac00\ub97c \uac00\ub2a5\ud558\uac8c \ud569\ub2c8\ub2e4.", "section": "4.3 \ubd84\uc11d \uc2e4\ud5d8"}, {"content": "| Benchmarks | Size | Focus |\n|---|---|---|\n| *RewardBench* | 2,985 | It covers multiple scenarios, including Chat, Chat-Hard, Safety, and Reasoning. |\n| *HelpSteeer2* | 519 | It provides multiple fine-grained dimensions for evaluation, like Helpfulness, Coherence, Correctness, Complexity, Verbosity. |\n| *MTBench Human* | 2,665 | It provides multi-turn conversation for evaluation, and we filter the samples whose outcome is \u201cTie\u201d. |\n| *JudgeBench* | 350 | It focuses on challenging response pairs spanning knowledge, reasoning, math, and coding |\n| *EvalBias* | 1,000 | It tests the robustness of judges on various scenarios containing evaluation biases. |", "caption": "Table 6: The brief description of Preference Benchmarks for testing.", "description": "\ud45c 6\uc740 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub2e4\uc12f \uac00\uc9c0 \uc120\ud638\ub3c4 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uac04\ub7b5\ud55c \uc124\uba85\uc785\ub2c8\ub2e4. \uac01 \ubca4\uce58\ub9c8\ud06c\ub294 \ud2b9\uc815\ud55c \ud3c9\uac00 \ubaa9\ud45c\ub97c \uac00\uc9c0\uace0 \uc788\uc73c\uba70, \ub2e4\uc591\ud55c \uc2dc\ub098\ub9ac\uc624(\ub300\ud654, \uc548\uc804, \ucd94\ub860 \ub4f1)\uc640 \uc138\ubd80 \ud3c9\uac00 \uae30\uc900(\uc720\uc6a9\uc131, \uc77c\uad00\uc131, \uc815\ud655\uc131, \ubcf5\uc7a1\uc131, \uc790\uc138\ud568 \ub4f1)\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4. \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 \uc774 \ub2e4\uc12f \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec LLM-as-a-Judge\uc758 \uc131\ub2a5\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "B Testing Preference Benchmark"}]