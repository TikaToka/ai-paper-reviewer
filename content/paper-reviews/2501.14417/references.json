{"references": [{"fullname_first_author": "Amey Agrawal", "paper_title": "Sarathi: Efficient LLM inference by piggybacking decodes with chunked prefills", "publication_date": "2023-08-16", "reason": "This paper proposes a novel technique to improve LLM inference efficiency by combining decodes with chunked prefills, a concept directly relevant to DEEPFLOW's optimization strategies."}, {"fullname_first_author": "Cunchen Hu", "paper_title": "Memserve: Context caching for disaggregated LLM serving with elastic memory pool", "publication_date": "2024-06-17", "reason": "Memserve directly addresses the challenges of context caching and disaggregated serving in LLMs, which are central design considerations in DEEPFLOW."}, {"fullname_first_author": "Cunchen Hu", "paper_title": "Inference without interference: Disaggregate LLM inference for mixed downstream workloads", "publication_date": "2024-01-11", "reason": "This paper explores disaggregated LLM inference, a key technique also employed by DEEPFLOW to enhance efficiency and scalability."}, {"fullname_first_author": "Jiangfei Duan", "paper_title": "Muxserve: Flexible spatial-temporal multiplexing for multiple LLM serving", "publication_date": "2024-07-21", "reason": "Muxserve tackles the challenge of efficiently serving multiple LLMs concurrently, a problem that DEEPFLOW also aims to solve with its serverless design."}, {"fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient memory management for large language model serving with page-dattention", "publication_date": "2023-MM-DD", "reason": "This paper presents efficient memory management techniques for large language models, which is crucial for the performance of DEEPFLOW's serving engine."}]}