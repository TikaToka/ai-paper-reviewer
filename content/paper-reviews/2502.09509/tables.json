[{"content": "| Image | SD-VAE | SD-VAE +Ours | SDXL-VAE | SDXL-VAE +Ours |\n|---|---|---|---|---|\n|  |  |  |  |  |\n|  |  |  |  |  |\n|  |  |  |  |  |\n|  |  |  |  |  |", "caption": "Table 1: Comparison of Autoencoders with and without EQ-VAE.\nWe evaluate reconstruction quality, equivariance errors (defined in LABEL:sec:appendix_metrics), and generative performance for continuous (SD-VAE, SDXL-VAE, SD3-VAE) and discrete (VQ-GAN) autoencoders, with and without EQ-VAE.\nGenerative FID (gFID) is measured using DiT-B for continuous VAEs and MaskGIT for VQ-GAN.\nOur approach reduces reconstruction rFID and equivariance errors while enhancing generative performance (gFID). For additional reconstruction metrics see\nLABEL:tab:appendix_recon.", "description": "\ud45c 1\uc740 EQ-VAE\ub97c \uc801\uc6a9\ud588\uc744 \ub54c\uc640 \uc801\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\uc758 \ub2e4\uc591\ud55c \uc624\ud1a0\uc778\ucf54\ub354\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc5f0\uc18d\uc801\uc778(SD-VAE, SDXL-VAE, SD3-VAE) \ubc0f \uc774\uc0b0\uc801\uc778(VQ-GAN) \uc624\ud1a0\uc778\ucf54\ub354 \ubaa8\ub450\uc5d0 \ub300\ud574 \uc7ac\uad6c\uc131 \ud488\uc9c8, \ub4f1\ubcc0\ub7c9 \uc624\ub958 \ubc0f \uc0dd\uc131 \uc131\ub2a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  \uc0dd\uc131 FID(gFID)\ub294 \uc5f0\uc18d VAE\uc758 \uacbd\uc6b0 DiT-B\ub97c \uc0ac\uc6a9\ud558\uace0, VQ-GAN\uc758 \uacbd\uc6b0 MaskGIT\ub97c \uc0ac\uc6a9\ud558\uc5ec \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4. EQ-VAE\ub294 \uc7ac\uad6c\uc131 rFID\uc640 \ub4f1\ubcc0\ub7c9 \uc624\ub958\ub97c \uc904\uc774\uba74\uc11c \uc0dd\uc131 \uc131\ub2a5(gFID)\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ud6a8\uacfc\ub97c \ubcf4\uc785\ub2c8\ub2e4. \ucd94\uac00\uc801\uc778 \uc7ac\uad6c\uc131 \uc9c0\ud45c\ub294 \ubd80\ub85d\uc758 \ud45c\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"content": "| Input Image **x** | SD-VAE | SD-VAE | Ours |\n|---|---|---|---|\n| $\n\\mathcal{D}(\n\\mathcal{E}(\ntau\n\\circ\n\\mathbf{x})\n)$ | $\n\\mathcal{D}(\ntau\n\\circ\n\\mathcal{E}(\n\\mathbf{x})\n)$ | $\n\\mathcal{D}(\ntau\n\\circ\n\\mathcal{E}(\n\\mathbf{x})\n)$ |  |\n|  |  |  |  |\n|  |  |  |  |", "caption": "Table 2: gFID Comparisons. gFID scores on ImageNet 256\u00d7256256256256\\times 256256 \u00d7 256 for DiT, SiT, and REPA trained with either SD-VAE-FT-EMA or our EQ-VAE. No classifier-free guidance (CFG) is used.\nEQ-VAE consistently enhances both generative performance and training efficiency across all generative models.", "description": "\ud45c 2\ub294 DiT, SiT, REPA\uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uc0dd\uc131 \ubaa8\ub378\uc5d0 \ub300\ud574 SD-VAE-FT-EMA \ub610\ub294 \uc81c\uc548\ub41c EQ-VAE\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ud588\uc744 \ub54c\uc758 gFID \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. ImageNet 256x256 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud588\uc73c\uba70, classifier-free guidance\ub294 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c EQ-VAE\ub294 \ubaa8\ub4e0 \uc0dd\uc131 \ubaa8\ub378\uc5d0\uc11c \uc0dd\uc131 \uc131\ub2a5\uacfc \ud559\uc2b5 \ud6a8\uc728\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2 Equivariance-regularized VAEs"}, {"content": "|                       | Autoencoder | rFID\u2193 | gFID\u2193 | Equiv. Error |             |\n| :-------------------- | :----------- | :---- | :---- | :------------- | :------------ |\n|                       |             |       |       |       |             |\n| SD-VAE                | 0.90         | 43.8  | 0.93  | 0.80          |             |\n| + EQ-VAE (ours)       | 0.82         | 34.1  | 0.49  | 0.15          |             |\n|                       |             |       |       |               |             |\n| SDXL-VAE              | 0.67         | 46.0  | 1.25  | 0.97          |             |\n| + EQ-VAE (ours)       | 0.65         | 35.9  | 0.65  | 0.35          |             |\n|                       |             |       |       |               |             |\n| SD3-VAE               | 0.20         | 58.9  | 0.51  | 0.16          |             |\n| + EQ-VAE (ours)       | 0.19         | 54.0  | 0.37  | 0.11          |             |\n|                       |             |       |       |               |             |\n| SD-VAE-16             | 0.87         | 64.1  | 0.95  | 0.85          |             |\n| + EQ-VAE (ours)       | 0.82         | 49.7  | 0.39  | 0.17          |             |\n|                       |             |       |       |               |             |\n| VQ-GAN                | 7.94         | 6.8   | 1.35  | 1.22          |             |\n| + EQ-VAE (ours)       | 7.54         | 5.9   | 0.64  | 0.55          |             |", "caption": "Table 3: Boosting Masked Generative Modeling.\nComparison of gFID and IS on ImageNet 256\u00d7256256256256\\times 256256 \u00d7 256 for MaskGIT\u00a0(chang2022maskgit) and its open-source PyTorch reproduction\u2020\u00a0(besnier2023pytorch), trained with either VQ-GAN or our EQ-VAE. EQ-VAE accelerates training by more than \u00d72absent2\\times 2\u00d7 2 (130 vs. 300 epochs), highlighting EQ-VAE can be effectively applied to vector-quantized autoencoders.", "description": "\uc774 \ud45c\ub294 \ub9c8\uc2a4\ud06c\uac00 \uc801\uc6a9\ub41c \uc0dd\uc131 \ubaa8\ub378\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. ImageNet 256x256 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec MaskGIT \ubaa8\ub378\uacfc \uadf8 \ud30c\uc774\ud1a0\uce58 \uc624\ud508\uc18c\uc2a4 \uc7ac\ud604 \ubaa8\ub378\uc744 VQ-GAN\uacfc EQ-VAE\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\uc2dc\ud0a8 \uacb0\uacfc\ub97c gFID\uc640 IS \uc9c0\ud45c\ub85c \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. EQ-VAE\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c \ud559\uc2b5 \uc18d\ub3c4\uac00 2\ubc30 \uc774\uc0c1 \ube68\ub77c\uc84c\uace0(130 epoch vs 300 epoch), \ubca1\ud130 \uc591\uc790\ud654 \uc624\ud1a0\uc778\ucf54\ub354\uc5d0\ub3c4 \ud6a8\uacfc\uc801\uc73c\ub85c \uc801\uc6a9\ub420 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Model | #Params | Iter. | gFID\u2193 |\n|---|---|---|---| \n| DiT-B/2 | 130M | 400K | 43.5 |\n| w/ EQ-VAE (ours) | 130M | 400K | 34.1 |\n| SiT-B/2 | 130M | 400K | 33.0 |\n| w/ EQ-VAE (ours) | 130M | 400K | 31.2 |\n| DiT-XL/2 | 675M | 400K | 19.5 |\n| w/ EQ-VAE (ours) | 675M | 400K | 14.5 |\n| SiT-XL/2 | 675M | 400K | 17.2 |\n| w/ EQ-VAE (ours) | 675M | 400K | 16.1 |\n| DiT-XL/2 | 675M | 7M | 9.6 |\n| w/ EQ-VAE (ours) | 675M | 1.5M | 8.8 |\n| SiT-XL/2+REPA | 675M | 4M | 5.9 |\n| w/ EQ-VAE (ours) | 675M | 1M | 5.9 |", "caption": "Table 4: Comparison on ImageNet 256\u00d7\\times\u00d7256 with CFG.\n\u2020\u2020\\dagger\u2020 indicates that the used autoencoder is the original SD-VAE (instead of SD-VAE-FT-EMA). REPA uses SiT-XL/2.\u00a0* denotes that guidance interval\u00a0(Kynkaanniemi2024) is applied.", "description": "\ud45c 4\ub294 CFG(Classifier-free guidance)\ub97c \uc0ac\uc6a9\ud558\uc5ec ImageNet 256x256 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ud6c8\ub828\ub41c \uc5ec\ub7ec \ud655\uc0b0 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 DiT-XL/2, SiT-XL/2, REPA \ubaa8\ub378\uacfc \uc774\ub4e4 \ubaa8\ub378\uc5d0 EQ-VAE \uae30\ubc95\uc744 \uc801\uc6a9\ud55c \uacbd\uc6b0\uc758 \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ud2b9\ud788, \u2020 \uae30\ud638\ub294 \uc6d0\ubcf8 SD-VAE \uc624\ud1a0\uc778\ucf54\ub354\ub97c \uc0ac\uc6a9\ud588\uc74c\uc744, * \uae30\ud638\ub294 Kynkaanniemi \ub4f1(2024)\uc758 \uac00\uc774\ub4dc \uac04\uaca9(guidance interval) \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  REPA \ubaa8\ub378\uc758 \uacbd\uc6b0 SiT-XL/2 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 EQ-VAE \uae30\ubc95\uc774 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc5d0 \uc801\uc6a9\ub420 \ub54c \uc131\ub2a5 \ud5a5\uc0c1 \ubc0f \ud6c8\ub828 \uc18d\ub3c4 \ud5a5\uc0c1 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc8fc\uc694 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc694\uc57d\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Model | Epoch | gFID\u2193 | IS\u2191 |\n|---|---|---|---|\n| MaskGIT | 300 | 6.19 | 182.1 |\n| MaskGIT\u2020 | 300 | 6.80 | 214.0 |\n| w/ EQ-VAE (ours) | 130 | 6.80 | 188.1 |\n| w/ EQ-VAE (ours) | 300 | 5.91 | 228.8 |", "caption": "Table 5: Spatial Transformation Ablation in EQ-VAE.\nWe measure gFID, rFID, and intrinsic dimension (ID) for latents regularized via rotations, isotropic scaling, anisotropic scaling, and combinations. Combining transformations lowers ID and enhances generative performance, though anisotropic scaling can slightly degrade reconstruction.", "description": "\ud45c 5\ub294 EQ-VAE\uc5d0\uc11c \uacf5\uac04 \ubcc0\ud658\uc758 \uc601\ud5a5\uc744 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud68c\uc804, \ub4f1\ubc29\uc131 \uc2a4\ucf00\uc77c\ub9c1, \uc774\ubc29\uc131 \uc2a4\ucf00\uc77c\ub9c1, \uadf8\ub9ac\uace0 \uc774\ub4e4\uc758 \uc870\ud569\uc744 \ud1b5\ud574 \uc7a0\uc7ac \uacf5\uac04\uc744 \uaddc\uc81c\ud588\uc744 \ub54c\uc758 gFID, rFID, \uace0\uc720 \ucc28\uc6d0(ID)\uc744 \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4. \ubcc0\ud658\uc744 \uc870\ud569\ud558\uba74 \uace0\uc720 \ucc28\uc6d0\uc774 \uac10\uc18c\ud558\uace0 \uc0dd\uc131 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\ub294 \ubc18\uba74, \uc774\ubc29\uc131 \uc2a4\ucf00\uc77c\ub9c1\uc740 \uc7ac\uad6c\uc131 \uc131\ub2a5\uc744 \uc57d\uac04 \uc800\ud558\uc2dc\ud0ac \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \ub2e4\uc591\ud55c \uacf5\uac04 \ubcc0\ud658 \uae30\ubc95\uc744 \uc801\uc6a9\ud558\uc5ec \uc0dd\uc131 \ubaa8\ub378\uc758 \uc131\ub2a5\uacfc \uc7a0\uc7ac \uacf5\uac04\uc758 \ubcf5\uc7a1\ub3c4 \uc0ac\uc774\uc758 \uad00\uacc4\ub97c \ubd84\uc11d\ud558\uace0, \ucd5c\uc801\uc758 \uc7a0\uc7ac \uacf5\uac04 \uaddc\uc81c \uc804\ub7b5\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "3.3 EQ-VAE: Regularization via equivariance constraints"}]