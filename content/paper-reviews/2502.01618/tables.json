[{"content": "| Model | Method | MATH500 | AIME 2024 |\n|---|---|---|---| \n| **Closed-Source LLMs** |  |  |  |\n| GPT-4o | - | 76.2 | 13.3 |\n| o1-preview | - | **87.0** | **40.0** |\n| Claude3.5-Sonnet | - | 78.3 | 16.0 |\n| **Open-Source LLMs** |  |  |  |\n| Llama-3.1-70B-Instruct | - | 65.7 | 16.6 |\n| Qwen2.5-Math-72B-Instruct | - | 82.0 | 30.0 |\n| **Open-Source SLMs** |  |  |  |\n| Llama-3.2-1B-Instruct | Pass@1 | 26.8 | 0.0 |\n|  | BoN | 46.6 | 3.3 |\n|  | WBoN | 47.8 | 3.3 |\n|  | DVTS | 52.8 | 6.6 |\n|  | Ours - PF | **59.6** | **10.0** |\n| Llama-3.1-8B-Instruct | Pass@1 | 49.9 | 6.6 |\n|  | BoN | 58.6 | 10.0 |\n|  | WBoN | 59.0 | 10.0 |\n|  | DVTS | 65.7 | 13.3 |\n|  | Ours - PF | **74.4** | **16.6** |\n| **Open-Source Math SLMs** |  |  |  |\n| Qwen2.5-Math-1.5B-Instruct | Pass@1 | 70.0 | 10.0 |\n|  | BoN | 82.6 | 13.3 |\n|  | WBoN | 82.8 | 13.3 |\n|  | DVTS | 83.4 | 16.6 |\n|  | Ours - PF | **85.4** | **23.3** |\n| Qwen2.5-Math-7B-Instruct | Pass@1 | 79.6 | 16.6 |\n|  | BoN | 83.0 | 20.0 |\n|  | WBoN | 84.6 | 20.0 |\n|  | DVTS | 85.4 | 20.0 |\n|  | Ours - PF | **87.0** | **23.3** |", "caption": "Table 1: Results of various LLMs on MATH500 and AIME 2024 where bold indicates the best in each category and italic indicates the overall best. The table highlights the performance of Inference Scaling methods, where Qwen2.5-Math-PRM-7B was used as the Reward Model.\nEach inference scaling methods were run with a computational budget of 64 model generations. Notably, the Qwen2.5-Math-7B model, when scaled with inference-time compute, achieves performance on par with o1-preview in MATH500, further showcasing the power of inference-time scaling for competitive performance with smaller models.", "description": "\ud45c 1\uc740 MATH500 \ubc0f AIME 2024 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\ub4e4\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uae30\ubcf8 \uc131\ub2a5(Pass@1)\uacfc  Qwen2.5-Math-PRM-7B \ubcf4\uc0c1 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud55c \uc138 \uac00\uc9c0 \ucd94\ub860 \uc2dc\uac04 \ud655\uc7a5 \uae30\ubc95(BoN, WBoN, DVTS) \ubc0f \uc81c\uc548\ub41c \ud655\ub960\uc801 \ucd94\ub860 \uae30\ubc18 \ubc29\ubc95(PF)\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.  \uad75\uc740 \uae00\uc528\ub294 \uac01 \ubc94\uc8fc\uc5d0\uc11c \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744, \uae30\uc6b8\uc784\uaf34\uc740 \uc804\uccb4\uc801\uc73c\ub85c \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ubaa8\ub4e0 \ucd94\ub860 \uc2dc\uac04 \ud655\uc7a5 \uae30\ubc95\uc740 64\ubc88\uc758 \ubaa8\ub378 \uc0dd\uc131\uc774\ub77c\ub294 \uacc4\uc0b0 \ube44\uc6a9 \uc81c\ud55c \ud558\uc5d0 \uc2e4\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, Qwen2.5-Math-7B \ubaa8\ub378\uc740 \ucd94\ub860 \uc2dc\uac04 \ud655\uc7a5\uc744 \ud1b5\ud574 MATH500 \ub370\uc774\ud130\uc14b\uc5d0\uc11c o1-preview \ubaa8\ub378\uacfc \ub3d9\ub4f1\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\uc5ec, \ub354 \uc791\uc740 \ubaa8\ub378\uc5d0\uc11c \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\uae30 \uc704\ud55c \ucd94\ub860 \uc2dc\uac04 \ud655\uc7a5\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.2. Main results"}]