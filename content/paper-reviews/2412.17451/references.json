{"references": [{"fullname_first_author": "Hunter Lightman", "paper_title": "Let's verify step by step", "publication_date": "2023-05-20", "reason": "This paper introduces a process reward model (PRM) which is adapted for multimodal reasoning and improves the self-evolving training."}, {"fullname_first_author": "Avi Singh", "paper_title": "Beyond human data: Scaling self-training for problem-solving with language models", "publication_date": "2023-12-06", "reason": "This paper is highly influential as it provides a framework for self-evolving training that is adapted to the multimodal setting and greatly improves performance."}, {"fullname_first_author": "Caglar Gulcehre", "paper_title": "Reinforced self-training (ReST) for language modeling", "publication_date": "2023-08-08", "reason": "This paper provides an iterative self-training method, ReST, that is particularly important due to its effectiveness and generalizability."}, {"fullname_first_author": "Eric Zelikman", "paper_title": "Star: Bootstrapping reasoning with reasoning", "publication_date": "2022-12-01", "reason": "This paper is highly relevant as it introduces an earlier self-evolving training framework, STaR, which is further expanded upon in this work."}, {"fullname_first_author": "Zhihong Shao", "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models", "publication_date": "2024-02-03", "reason": "This paper focuses on mathematical reasoning, a key area of multimodal reasoning, and provides a relevant self-evolving method."}]}