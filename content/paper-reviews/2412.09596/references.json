{"references": [{"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-31", "reason": "This paper introduces Flamingo, a visual language model that is foundational to the architecture of InternLM-XComposer2.5-OmniLive."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-31", "reason": "This paper is highly influential in the field of large language models and is referenced to highlight the advancements in few-shot learning capabilities."}, {"fullname_first_author": "Joya Chen", "paper_title": "VideoLLM-Online: Online video large language model for streaming video", "publication_date": "2024-12-31", "reason": "This paper introduces VideoLLM-Online, a key reference due to its focus on online video understanding and streaming video, which is a core functionality of InternLM-XComposer2.5-OmniLive."}, {"fullname_first_author": "Yue Fan", "paper_title": "VideoAgent: A memory-augmented multimodal agent for video understanding", "publication_date": "2024-12-31", "reason": "VideoAgent is highlighted for its approach to multimodal understanding, especially in video, which is relevant to the design of InternLM-XComposer2.5-OmniLive."}, {"fullname_first_author": "Chaoyou Fu", "paper_title": "VITA: Towards open-source interactive omni multimodal llm", "publication_date": "2024-12-31", "reason": "The VITA model is mentioned for its work on creating systems capable of continuous interaction across multiple modalities, a challenge directly addressed by InternLM-XComposer2.5-OmniLive."}]}