[{"content": "| Stage | Task | Dataset | Data Num |\n|---|---|---|---| \n| Pretrain | ASR | GigaSpeech [11] | 8,282,987 |\n| SFT | ASR | WenetSpeech [140] | 17,821,017 |\n|  |  | LibriSpeech [87] | 281,241 |\n|  |  | VCTK [111] | 44,070 |\n|  |  | AISHELL-1 [8] | 120,098 |\n|  |  | AISHELL-4 [39] | 102,254 |\n|  |  | MD-RAMC [129] | 219,325 |\n|  |  | ASCEND [76] | 12,314 |\n|  |  | KeSpeech [106] | 888,428 |\n|  |  | DASR [27] | 190,732 |\n|  |  | CommonVoice [2] | 2,813,852 |\n|  | CLS | FSD50K [35] | 40,966 |\n|  |  | AudioSet [53] | 18,683 |\n|  |  | Silence | 475 |", "caption": "Table 1: Overview of datasets used in pretraining and supervised fine-tuning (SFT) for the Audio Translation Module. The pretraining stage focuses solely on the automatic speech recognition (ASR) task, utilizing the GigaSpeech and WenetSpeech datasets. The SFT stage includes both ASR and audio classification (CLS) tasks, leveraging diverse datasets. For CommonVoice, we only use the English and Chinese splits. Additionally, 475 self-constructed \u201cSilence\u201d samples are used for CLS tasks.", "description": "\ud45c 1\uc740 \ub17c\ubb38\uc758 \uc624\ub514\uc624 \ubc88\uc5ed \ubaa8\ub4c8\uc5d0 \ub300\ud55c \uc0ac\uc804 \ud6c8\ub828 \ubc0f \uc9c0\ub3c4 \ud559\uc2b5 \ubbf8\uc138 \uc870\uc815\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0ac\uc804 \ud6c8\ub828 \ub2e8\uacc4\ub294 GigaSpeech \ubc0f WenetSpeech \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc790\ub3d9 \uc74c\uc131 \uc778\uc2dd(ASR) \uc791\uc5c5\uc5d0\ub9cc \uc911\uc810\uc744 \ub461\ub2c8\ub2e4. \uc9c0\ub3c4 \ud559\uc2b5 \ubbf8\uc138 \uc870\uc815 \ub2e8\uacc4\ub294 \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b\uc744 \ud65c\uc6a9\ud558\uc5ec ASR \ubc0f \uc624\ub514\uc624 \ubd84\ub958(CLS) \uc791\uc5c5\uc744 \ubaa8\ub450 \ud3ec\ud568\ud569\ub2c8\ub2e4. Common Voice\uc758 \uacbd\uc6b0 \uc601\uc5b4 \ubc0f \uc911\uad6d\uc5b4 \ubd84\ud560\ub9cc \uc0ac\uc6a9\ud558\uba70, CLS \uc791\uc5c5\uc5d0\ub294 475\uac1c\uc758 \uc790\uccb4 \uc81c\uc791\ub41c \"\uce68\ubb35\" \uc0d8\ud50c\uc774 \ucd94\uac00\uc801\uc73c\ub85c \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \ubcf8 \ud45c\ub294 \ub370\uc774\ud130\uc14b\uc758 \uc774\ub984, \uc791\uc5c5 \uc885\ub958, \ub370\uc774\ud130\uc14b \ud06c\uae30 \ub4f1\uc758 \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\uc5ec \uc624\ub514\uc624 \ubc88\uc5ed \ubaa8\ub4c8\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud55c \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b\uc758 \uc5ed\ud560\uc744 \uba85\ud655\ud788 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. \ubc29\ubc95"}, {"content": "| Model | Dataset |\n|---|---| \n| Memory Module | ShareGPT4Video [15], Ego4D [41], ActivityNet [32], Semantics Implicit QA, Reference Implicit QA |\n| IXC2.5 | ShareGPT4Video [15], ActivityNet [32], FunQA [122], TrafficQA [125], VideoChat2-IT [61], LLaVA-Video [152] |", "caption": "Table 2: Video Datasets used in IXC2.5-OL.", "description": "\ud45c 2\ub294 \ub17c\ubb38\uc758 IXC2.5-OL \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ube44\ub514\uc624 \ub370\uc774\ud130\uc14b \ubaa9\ub85d\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b\uc740 \ub2e4\uc591\ud55c \uc885\ub958\uc758 \ube44\ub514\uc624\uc640 \uc9c8\uc758\uc751\ub2f5 \uc30d(question-answer pairs)\uc744 \ud3ec\ud568\ud558\uba70, \uc2dc\uc2a4\ud15c\uc758 \ub2e4\uc591\ud55c \ubaa8\ub4c8(Streaming Perception Module, Multi-modal Long Memory Module, Reasoning Module) \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud2b9\ud788, 'Semantics Implicit Question' \ubc0f 'Reference Implicit Question'\uacfc \uac19\uc774 \uac04\uc811\uc801\uc778 \uc9c8\ubb38\uc744 \ud3ec\ud568\ud558\ub294 \ub370\uc774\ud130\uc14b\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc2dc\uc2a4\ud15c\uc758 \uacac\uace0\uc131 \ubc0f \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \uae30\uc5ec\ud588\uc2b5\ub2c8\ub2e4.", "section": "3. Method"}, {"content": "| Method |  | LLM |  | Wenetspeech (CN) |  | Librispeech (ENG) |\n|---|---|---|---|---|---|---|\n|  |  |  |  | Test_Net \u2193 | Test_Meeting \u2193 |  | Dev_clean \u2193 | Dev_other \u2193 | Test_clean \u2193 | Test_other \u2193 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Qwen2-Audio [26] |  | Qwen2-7B [128] |  | 7.8 | 8.4 |  | 1.3 | 3.4 | 1.6 | 3.6 |\n| Mini-Omni [123] |  | Qwen2-0.5B [128] |  | - | - |  | 4.5 | 9.7 | 4.6 | 9.2 |\n| VITA [38] |  | Mixtral-8x7B [47] |  | 12.2 | 16.5 |  | 7.6 | 16.6 | 8.1 | 18.4 |\n| IXC2.5-OL |  | Qwen2-1.5B [128] |  | 9.0 | 9.2 |  | 2.5 | 5.7 | 2.6 | 5.8 |", "caption": "Table 3: Evaluation results on ASR tasks: \u201dCN\u201d refers to Chinese speech, while \u201dENG\u201d refers to English speech. The performance is measured using WER \u2193\u2193\\downarrow\u2193 (Word Error Rate).", "description": "\ud45c 3\uc740 \uc790\ub3d9 \uc74c\uc131 \uc778\uc2dd(ASR) \uc791\uc5c5\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \"CN\"\uc740 \uc911\uad6d\uc5b4 \uc74c\uc131\uc744, \"ENG\"\ub294 \uc601\uc5b4 \uc74c\uc131\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc131\ub2a5\uc740 WER(\ub2e8\uc5b4 \uc624\ub958\uc728)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uce21\uc815\ub429\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378(Qwen2-Audio, Qwen2-7B, Mini-Omni, VITA, IXC2.5-OL)\uc758 \uc911\uad6d\uc5b4 \ubc0f \uc601\uc5b4 \uc74c\uc131 \uc778\uc2dd \uc131\ub2a5\uc744 WER \uc218\uce58\ub97c \ud1b5\ud574 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uac01 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  WER \uac12\uc774 \ub0ae\uc744\uc218\ub85d \ub354 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Method | Params | Topic Rea. | Anomaly Recog. | Needle QA | Ego Rea. | Plot QA | Action Or. | Action Co. | M-Avg |\n|---|---|---|---|---|---|---|---|---|---| \n| <br> *Closed-source APIs*. <br> |  |  |  |  |  |  |  |  |  |\n| Claude-3-Opus | - | 67.2 | 43.5 | 21.6 | 40.2 | 47.8 | 18.2 | 16.7 | 36.5 |\n| Qwen-VL-Max | - | 67.4 | 63.5 | 40.3 | 40.9 | 43.3 | 25.0 | 14.8 | 42.2 |\n| GPT-4 Turbo | - | 79.5 | 68.0 | 45.9 | 47.4 | 60.6 | 26.5 | 16.1 | 49.2 |\n| GPT-4o | - | 87.4 | 74.5 | 64.8 | 57.1 | 65.1 | 56.7 | 46.3 | 64.6 |\n| <br> *Open-source models*. <br> |  |  |  |  |  |  |  |  |  |\n| MovieChat [99] | 7B | 29.5 | 25.0 | 24.2 | 24.7 | 25.8 | 28.6 | 22.8 | 25.8 |\n| LLaMA-VID [65] | 7B | 50.8 | 34.5 | 30.1 | 32.7 | 32.5 | 23.9 | 27.8 | 33.2 |\n| LLaVA-1.6 [71] | 7B | 60.6 | 41.0 | 43.1 | 38.4 | 41.0 | 25.5 | 25.7 | 39.3 |\n| ShareGPT4Video [15] | 7B | 75.8 | 51.5 | 47.6 | 43.2 | 48.4 | 34.0 | 23.3 | 46.4 |\n| VideoLlaMA2 [23] | 7B | 74.6 | 64.5 | 49.9 | 43.8 | 45.1 | 34.0 | 27.4 | 48.5 |\n| LongVA [149] | 7B | 83.3 | 58.5 | 69.3 | 50.0 | 67.2 | 38.6 | 27.2 | 56.3 |\n| IXC2.5 [148] | 7B | - | - | - | - | - | - | - | 58.8 |\n| InternVL2 [22] | 8B | - | - | - | - | - | - | - | 64.0 |\n| LLaVA-OneVision [57] | 7B | - | - | - | - | - | - | - | 64.7 |\n| Video-XL [97] | 7B | - | - | - | - | - | - | - | 64.9 |\n| IXC2.5-OL | 7B | 84.1 | 68.5 | 76.6 | 60.8 | 75.1 | 57.1 | 41.3 | 66.2 |", "caption": "Table 4: Evaluation results on MLVU benchmark. IXC2.5-OL has demonstrated excellent performance, surpassing both open-source models and closed-source APIs, achieving SOTA at the 7B model scale.", "description": "\ud45c 4\ub294 MLVU \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. IXC2.5-OL \ubaa8\ub378\uc740 70\uc5b5 \uac1c\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \uac00\uc9c4 \ubaa8\ub378 \uc911\uc5d0\uc11c \ucd5c\uace0 \uc131\ub2a5(SOTA)\uc744 \ub2ec\uc131\ud588\uc73c\uba70, \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378\uacfc \uc0c1\uc6a9 API\ub97c \ubaa8\ub450 \ub2a5\uac00\ud558\ub294 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ube44\ub514\uc624 \uc774\ud574 \uc791\uc5c5(\uc8fc\uc81c \ucd94\ub860, \uc774\uc0c1 \uac10\uc9c0, \uc9c8\ubb38 \uc751\ub2f5 \ub4f1)\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec IXC2.5-OL\uc758 \uc6b0\uc218\uc131\uc744 \uc785\uc99d\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\uc640 \uc804\uccb4 \ud3c9\uade0 \uc131\ub2a5\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Method | Params | Short | Medium | Long | Overall |\n|---|---|---|---|---|---| \n| <em class=\"ltx_emph ltx_font_italic\">Closed-source APIs.</em> |  |  |  |  |  |\n| GPT-4V | - | 70.5 | 55.8 | 53.5 | 59.9 |\n| Claude 3.5 Sonnet | - | 71.0 | 57.4 | 51.2 | 60.0 |\n| GPT-4o mini | - | 72.5 | 63.1 | 58.6 | 64.8 |\n| GPT-4o | - | 80.0 | 70.3 | 65.3 | 71.9 |\n| Gemini 1.5 Pro | - | 81.7 | 74.3 | 67.4 | 75.0 |\n| <em class=\"ltx_emph ltx_font_italic\">Open-source models.</em> |  |  |  |  |  |\n| ShareGPT4Video [15] | 7B | 48.3 | 36.3 | 35.0 | 39.9 |\n| VideoLlaMA2 [23] | 7B | - | - | - | 47.9 |\n| LongVA [149] | 7B | 61.1 | 50.4 | 46.2 | 52.6 |\n| Video-XL [97] | 7B | 64.0 | 53.2 | 49.2 | 55.5 |\n| VITA [38] | 8x7B | 65.9 | 52.9 | 48.6 | 55.8 |\n| IXC2.5 [148] | 7B | - | - | - | 55.8 |\n| InternVL2 [22] | 8B | - | - | - | 56.3 |\n| LLaVA-OneVision [57] | 7B | - | - | - | 58.2 |\n| mPLUG-Owl3 [131] | 7B | 70.0 | 57.7 | 50.1 | 59.3 |\n| MiniCPM-V 2.6 [130] | 8B | - | - | - | 60.9 |\n| IXC2.5-OL | 7B | 72.7 | 58.2 | 50.8 | 60.6 |", "caption": "Table 5: Evaluation results on Video-MME benchmark. IXC2.5-OL demonstrates performance close to that of the open-source SOTA.", "description": "\ud45c 5\ub294 Video-MME \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Video-MME\ub294 \ub2e4\uc591\ud55c \ube44\ub514\uc624 \uc774\ud574 \uc791\uc5c5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \uace0\uc548\ub41c \uc885\ud569\uc801\uc778 \ubca4\uce58\ub9c8\ud06c\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 IXC2.5-OL \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc624\ud508\uc18c\uc2a4 \ucd5c\ucca8\ub2e8(SOTA) \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc8fc\uba70, IXC2.5-OL\uc774 \uc624\ud508\uc18c\uc2a4 SOTA \ubaa8\ub378\ub4e4\uacfc \uac70\uc758 \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c\ub294 \uc5ec\ub7ec \ube44\ub514\uc624 \uc774\ud574 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4(\uc608: \uc8fc\uc81c \ucd94\ub860, \uc774\uc0c1 \uac10\uc9c0, \uc9c8\ubb38 \uc751\ub2f5 \ub4f1)\ub97c \uc218\uce58\ub85c \uc81c\uc2dc\ud558\uc5ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc790\uc138\ud788 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "4. Video Benchmarks"}, {"content": "| Method | Params | OP | CR | CS | ATP | EU | TR | PR | SU | ACP | CT | Overall |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Human | - | 89.47 | 92.00 | 93.60 | 91.47 | 95.65 | 92.52 | 88.00 | 88.75 | 89.74 | 91.30 | 91.46 |\n| _Closed-source APIs._ |  |  |  |  |  |  |  |  |  |  |  |  |\n| Claude 3.5 Sonnet | - | 80.49 | 77.34 | 82.02 | 81.73 | 72.33 | 75.39 | 61.11 | 61.79 | 69.32 | 43.09 | 72.44 |\n| GPT-4o | - | 77.11 | 80.47 | 83.91 | 76.47 | 70.19 | 83.80 | 66.67 | 62.19 | 69.12 | 49.22 | 73.28 |\n| Gemini 1.5 Pro | - | 79.02 | 80.47 | 83.54 | 79.67 | 80.00 | 84.74 | 77.78 | 64.23 | 71.95 | 48.70 | 75.69 |\n| _Open-source models._ |  |  |  |  |  |  |  |  |  |  |  |  |\n| VideoLLM-online [12] | 8B | 39.07 | 40.06 | 34.49 | 31.05 | 45.96 | 32.40 | 31.48 | 34.16 | 42.49 | 27.89 | 35.99 |\n| VideoLLaMA2 [23] | 7B | 55.86 | 55.47 | 57.41 | 58.17 | 52.80 | 43.61 | 39.21 | 42.68 | 45.61 | 35.23 | 49.52 |\n| VILA-1.5 [68] | 8B | 53.68 | 49.22 | 70.98 | 56.86 | 53.42 | 53.89 | 54.63 | 48.78 | 50.14 | 17.62 | 52.32 |\n| LongVA [149] | 7B | 70.03 | 63.28 | 61.20 | 70.92 | 62.73 | 59.50 | 61.11 | 53.66 | 54.67 | 34.72 | 59.96 |\n| InternVL2 [22] | 8B | 68.12 | 60.94 | 69.40 | 77.12 | 67.70 | 62.93 | 59.26 | 53.25 | 54.96 | 56.48 | 63.72 |\n| Kangaroo [72] | 7B | 71.12 | 84.38 | 70.66 | 73.20 | 67.08 | 61.68 | 56.48 | 55.69 | 62.04 | 38.86 | 64.60 |\n| MiniCPM-V 2.6 [130] | 8B | 71.93 | 71.09 | 77.92 | 75.82 | 64.60 | 65.73 | 70.37 | 56.10 | 62.32 | 53.37 | 67.44 |\n| Qwen2-VL [113] | 7B | 75.20 | 82.81 | 73.19 | 77.45 | 68.32 | 71.03 | 72.22 | 61.19 | 69.04 | 46.11 | 69.04 |\n| LLaVA-OneVision [57] | 7B | 80.38 | 74.22 | 76.03 | 80.72 | 72.67 | 71.65 | 67.59 | 65.45 | 65.72 | 45.08 | 71.12 |\n| IXC2.5-OL | 7B | 82.83 | 73.77 | 78.66 | 82.95 | 72.50 | 76.01 | 61.11 | 60.67 | 71.59 | 58.85 | 73.79 |", "caption": "Table 6: Evaluation results on StreamingBench for Real-Time Visual Understanding. Metrics include Object Perception (OP), Causal Reasoning (CR), Clips Summarization (CS), Attribute Perception (ATP), Event Understanding (EU), Text-Rich Understanding (TR), Prospective Reasoning (PR), Spatial Understanding (SU), Action Perception (ACP), and Counting (CT). IXC2.5-OL excels among all open-source models, and falling just short of the closed-source API, Gemini 1.5 Pro.", "description": "\ud45c 6\uc740 \uc2e4\uc2dc\uac04 \uc2dc\uac01\uc801 \uc774\ud574\ub97c \uc704\ud55c StreamingBench\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uce21\uc815 \uc9c0\ud45c\ub294 \uac1d\uccb4 \uc778\uc2dd(OP), \uc778\uacfc \ucd94\ub860(CR), \ud074\ub9bd \uc694\uc57d(CS), \uc18d\uc131 \uc778\uc2dd(ATP), \uc0ac\uac74 \uc774\ud574(EU), \ud48d\ubd80\ud55c \ud14d\uc2a4\ud2b8 \uc774\ud574(TR), \uc804\ub9dd\uc801 \ucd94\ub860(PR), \uacf5\uac04\uc801 \uc774\ud574(SU), \ub3d9\uc791 \uc778\uc2dd(ACP), \uacc4\uc0b0(CT)\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4. IXC2.5-OL\uc740 \ubaa8\ub4e0 \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378 \uc911\uc5d0\uc11c \uac00\uc7a5 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc774\uba70, \ud074\ub85c\uc988\ub4dc \uc18c\uc2a4 API\uc778 Gemini 1.5 Pro\uc5d0 \uadfc\uc811\ud55c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4.  \ud45c\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uc2e4\uc2dc\uac04 \uc2dc\uac01\uc801 \uc774\ud574 \ub2a5\ub825\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uac01 \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ud30c\uc545\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4. \ud2b9\ud788,  \uac1d\uccb4 \uc778\uc2dd, \uc0ac\uac74 \uc774\ud574, \ub3d9\uc791 \uc778\uc2dd\uacfc \uac19\uc740 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc774\ud574 \uacfc\uc81c\uc5d0\uc11c \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Method | Params | CP | FP-S | FP-C | HL | Mean | LR | AR | RR | CSR | TP | Mean | Overall |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| *Closed-source APIs*. |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Claude 3.5 Sonnet | - | 1.57 | 1.39 | 1.07 | 1.40 | 1.38 | 1.13 | 1.70 | 1.48 | 1.54 | 1.04 | 1.35 | 1.38 |\n| Gemini 1.0 Pro | - | 1.61 | 1.56 | 1.30 | 0.65 | 1.50 | 1.15 | 1.57 | 1.55 | 1.36 | 1.33 | 1.39 | 1.48 |\n| Gemini 1.5 Pro | - | 1.99 | 2.04 | 1.70 | 1.90 | 1.98 | 1.98 | 2.02 | 1.92 | 1.78 | 1.63 | 1.86 | 1.94 |\n| GPT-4V | - | 1.83 | 1.65 | 1.40 | 1.76 | 1.66 | 1.45 | 1.91 | 1.86 | 1.83 | 1.53 | 1.69 | 1.68 |\n| GPT-4o | - | 2.23 | 2.24 | 2.01 | 1.90 | 2.19 | 2.11 | 2.12 | 2.17 | 1.94 | 1.97 | 2.08 | 2.15 |\n| *Open-source models*. |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| MovieLLM [101] | 7B | 0.95 | 0.82 | 0.70 | 0.15 | 0.81 | 0.52 | 1.12 | 1.22 | 0.54 | 1.05 | 0.97 | 0.87 |\n| LLaVA-OneVision [57] | 72B | 1.22 | 1.07 | 0.90 | 0.21 | 1.03 | 0.76 | 0.96 | 0.55 | 0.81 | 0.48 | 0.70 | 0.94 |\n| PLLaVA [126] | 7B | 1.08 | 1.06 | 0.86 | 0.52 | 1.02 | 0.64 | 1.25 | 1.17 | 0.98 | 1.01 | 1.03 | 1.03 |\n| ShareGPT4Video [15] | 7B | 1.20 | 1.05 | 1.00 | 0.32 | 1.04 | 0.89 | 1.06 | 1.19 | 1.01 | 0.99 | 1.03 | 1.05 |\n| VideoStreaming [89] | 7B | 1.38 | 1.13 | 0.8 | 0.32 | 1.13 | 0.77 | 1.27 | 1.11 | 1.01 | 1.10 | 1.09 | 1.12 |\n| LLaVA-NeXT-Video [151] | 7B | 1.35 | 1.15 | 0.97 | 0.58 | 1.14 | 0.64 | 1.38 | 1.30 | 1.27 | 1.03 | 1.13 | 1.14 |\n| VILA1.5 [68] | 13B | 1.51 | 1.45 | 1.26 | 0.24 | 1.39 | 0.80 | 1.52 | 1.30 | 1.40 | 1.28 | 1.28 | 1.36 |\n| InternVL2 [22] | 8B | 1.41 | 1.37 | 1.15 | 0.19 | 1.30 | 0.90 | 1.34 | 1.38 | 1.14 | 1.00 | 1.16 | 1.26 |\n| Qwen2-VL [113] | 7B | 1.63 | 1.51 | 1.19 | 0.55 | 1.46 | 1.16 | 1.56 | 1.49 | 1.37 | 1.21 | 1.35 | 1.44 |\n| IXC2.5-OL | 7B | 1.53 | 1.61 | 1.20 | 0.15 | 1.49 | 0.93 | 1.44 | 1.57 | 1.30 | 1.08 | 1.25 | 1.42 |", "caption": "Table 7: Evaluation results on MMBench-Video. Tasks include Coarse Perception (CP), Single-Instance Finegrained Perception (FP-S), Cross-Instance Finegrained Perception (FP-C), Hallucination (HL), Logic Reasoning (LR), Attribute Reasoning (AR), Relation Reasoning (RR), Commonsense Reasoning (CSR), and Temporal Reasoning (TP).", "description": "\ud45c 7\uc740 MMBench-Video \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MMBench-Video\ub294 \ube44\ub514\uc624 \uc774\ud574\ub97c \uc704\ud55c \ub2e4\uc591\ud55c \uacfc\uc81c\ub97c \ud3ec\ud568\ud558\ub294 \ubca4\uce58\ub9c8\ud06c\uc785\ub2c8\ub2e4. \uc774 \ud45c\uc5d0\ub294 \ucd1d 9\uac00\uc9c0 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc131\ub2a5\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uacfc\uc81c\ub294 \ube44\ub514\uc624 \uc774\ud574\uc758 \ud2b9\uc815 \uce21\uba74\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, Coarse Perception(CP)\uc740 \ube44\ub514\uc624\uc758 \uc804\ubc18\uc801\uc778 \ub0b4\uc6a9 \uc774\ud574\ub97c \uce21\uc815\ud558\uace0, Single-Instance Finegrained Perception(FP-S)\uacfc Cross-Instance Finegrained Perception(FP-C)\ub294 \uc138\ubd80\uc801\uc778 \uac1d\uccb4 \uc778\uc2dd \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. Hallucination(HL)\uc740 \uc798\ubabb\ub41c \uc815\ubcf4\ub97c \uc0dd\uc131\ud558\ub294 \uacbd\ud5a5\uc744 \ud3c9\uac00\ud558\uace0, Logic Reasoning(LR), Attribute Reasoning(AR), Relation Reasoning(RR), Commonsense Reasoning(CSR), Temporal Reasoning(TP)\uc740 \uac01\uac01 \ub17c\ub9ac\uc801 \ucd94\ub860, \uc18d\uc131 \ucd94\ub860, \uad00\uacc4 \ucd94\ub860, \uc0c1\uc2dd\uc801 \ucd94\ub860, \uc2dc\uac04\uc801 \ucd94\ub860 \ub2a5\ub825\uc744 \uce21\uc815\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \ubaa8\ub378\uc758 \ud30c\ub77c\ubbf8\ud130 \uc218\uc640 \uac01 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc131\ub2a5 \uc810\uc218\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uc5ec\ub7ec \ubaa8\ub378\uc758 \ube44\ub514\uc624 \uc774\ud574 \ub2a5\ub825\uc744 \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Method | Params | AS | AP | AA | FA | UA | OE | OI | OS | MD | AL | ST | AC | MC | MA | SC | FP | CO | EN | ER | CI | Avg |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| GPT-4V | - | 55.5 | 63.5 | 72.0 | 46.5 | 73.5 | 18.5 | 59.0 | 29.5 | 12.0 | 40.5 | 83.5 | 39.0 | 12.0 | 22.5 | 45.0 | 47.5 | 52.0 | 31.0 | 59.0 | 11.0 | 43.5 |\n| GPT-4o | - | 61.5 | 56.5 | 72.0 | 54.0 | 82.0 | 62.5 | 66.5 | 44.0 | 36.5 | 33.5 | 93.0 | 54.5 | 33.5 | 54.5 | 53.5 | 74.5 | 71.5 | 32.5 | 71.0 | 42.5 | 57.5 |\n| _Closed-source APIs._ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| VideoLLaMA [144] | 7B | 27.5 | 25.5 | 51.0 | 29.0 | 39.0 | 48.0 | 40.5 | 38.0 | 22.5 | 22.5 | 43.0 | 34.0 | 22.5 | 32.5 | 45.5 | 32.5 | 40.0 | 30.0 | 21.0 | 37.0 | 34.1 |\n| VideoChat [60] | 7B | 33.5 | 26.5 | 56.0 | 33.5 | 40.5 | 53.0 | 40.5 | 30.0 | 25.5 | 27.0 | 48.5 | 35.0 | 20.5 | 42.5 | 46.0 | 26.5 | 41.0 | 23.5 | 23.5 | 36.0 | 35.5 |\n| MiniCPM-V 2.6 [130] | 7B | 38.0 | 43.0 | 63.0 | 35.5 | 67.5 | 55.5 | 46.0 | 35.5 | 25.5 | 33.0 | 77.5 | 48.0 | 37.0 | 54.0 | 42.5 | 40.0 | 31.0 | 38.0 | 43.0 | 40.5 | 44.7 |\n| VideoChat2 [62] | 7B | 66.0 | 47.5 | 83.5 | 49.5 | 60.0 | 58.0 | 71.5 | 42.5 | 23.0 | 23.0 | 88.5 | 39.0 | 42.0 | 58.5 | 44.0 | 49.0 | 36.5 | 35.0 | 40.5 | 65.5 | 51.1 |\n| Qwen2-VL [113] | 7B | 51.0 | 58.0 | 77.5 | 47.0 | 64.0 | 63.0 | 65.5 | 40.0 | 25.5 | 35.5 | 77.0 | 43.5 | 47.0 | 62.0 | 42.0 | 61.5 | 49.5 | 41.5 | 47.5 | 41.5 | 52.0 |\n| PLLaVA [126] | 34B | 65.0 | 53.0 | 83.5 | 45.0 | 77.5 | 70.0 | 64.5 | 38.5 | 37.5 | 49.0 | 89.5 | 41.5 | 43.5 | 70.0 | 53.0 | 52.5 | 65.0 | 39.5 | 60.5 | 58.0 | 57.8 |\n| LLaVA-OneVision [57] | 72B | 63.0 | 58.0 | 84.5 | 46.5 | 85.5 | 64.0 | 73.5 | 41.5 | 37.0 | 69.0 | 95.0 | 47.5 | 47.5 | 75.5 | 53.5 | 52.0 | 70.5 | 34.0 | 64.0 | 54.5 | 60.8 |\n| InternVL2 [22] | 8B | 75.0 | 62.0 | 83.5 | 40.5 | 69.5 | 96.0 | 72.0 | 29.5 | 58.0 | 53.0 | 88.5 | 39.5 | 83.0 | 97.0 | 51.0 | 78.5 | 65.0 | 33.0 | 48.0 | 67.0 | 64.5 |\n| _Open-source models._ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| IXC2.5-OL | 7B | 84.5 | 81.0 | 75.0 | 46.0 | 81.0 | 92.0 | 79.5 | 36.5 | 83.0 | 47.0 | 90.0 | 60.5 | 75.0 | 93.0 | 58.0 | 60.5 | 74.0 | 42.0 | 53.0 | 62.0 | 68.7 |", "caption": "Table 8: Evaluatation results on MVBench. Tasks include Action Sequence (AS), Action Prediction (AP), Action Antonym (AA), Fine-grained Action (FA), Unexpected Action (UA), Object Existence (OE), Object Interaction (OI), Object Shuffle (OS), Moving Direction (MD), Action Localization (AL), Scene Transition (ST), Action Count (AC), Moving Count (MC), Moving Attribute (MA), State Change (SC), Fine-grained Pose (FP), Character Order (CO), Egocentric Navigation (EN), Episodic Reasoning (ER), and Counterfactual Inference (CI).", "description": "\ud45c 8\uc740 MVBench\ub77c\ub294 \ube44\ub514\uc624 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MVBench\ub294 \ub2e4\uc591\ud55c \ube44\ub514\uc624 \uc774\ud574 \uc791\uc5c5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \uc124\uacc4\ub418\uc5c8\uc73c\uba70,  \uc561\uc158 \uc21c\uc11c(AS), \uc561\uc158 \uc608\uce21(AP), \uc561\uc158 \ubc18\uc758\uc5b4(AA), \uc138\ubd84\ud654\ub41c \uc561\uc158(FA), \uc608\uc0c1\uce58 \ubabb\ud55c \uc561\uc158(UA), \uac1d\uccb4 \uc874\uc7ac(OE), \uac1d\uccb4 \uc0c1\ud638\uc791\uc6a9(OI), \uac1d\uccb4 \uc11e\uae30(OS), \uc774\ub3d9 \ubc29\ud5a5(MD), \uc561\uc158 \uc9c0\uc5ed\ud654(AL), \uc7a5\uba74 \uc804\ud658(ST), \uc561\uc158 \uac1c\uc218(AC), \uc774\ub3d9 \uac1c\uc218(MC), \uc774\ub3d9 \uc18d\uc131(MA), \uc0c1\ud0dc \ubcc0\ud654(SC), \uc138\ubd84\ud654\ub41c \ud3ec\uc988(FP), \uce90\ub9ad\ud130 \uc21c\uc11c(CO), \uc2dc\uc810 \ud0d0\uc0c9(EN), \uc5d0\ud53c\uc18c\ub4dc \ucd94\ub860(ER), \ubc18\uc0ac\uc2e4\uc801 \ucd94\ub860(CI) \ub4f1 20\uac00\uc9c0 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}]