{"references": [{"fullname_first_author": "Akari Asai", "paper_title": "Task-aware retrieval with instructions", "publication_date": "2023-00-00", "reason": "This paper is highly relevant because it explores task-aware retrieval, a technique directly applicable to improving the efficiency and effectiveness of RAG systems by focusing on instruction fine-tuning for better retriever performance."}, {"fullname_first_author": "Ryan C. Barron", "paper_title": "Domain-specific retrieval-augmented generation using vector stores, knowledge graphs, and tensor factorization", "publication_date": "2024-00-00", "reason": "This paper is cited for its work on domain-specific RAG, which is the core focus of the current research, making it a key reference for understanding and addressing domain-specific challenges in RAG applications."}, {"fullname_first_author": "Patrice Bechard", "paper_title": "Reducing hallucination in structured outputs via retrieval-augmented generation", "publication_date": "2024-00-00", "reason": "This is a highly relevant reference as it is a previous work by the same authors on the topic of RAG, and is directly relevant to the problem of hallucination in structured outputs."}, {"fullname_first_author": "Tianqi Chen", "paper_title": "Training deep nets with sublinear memory cost", "publication_date": "2016-04-00", "reason": "This paper presents a memory-efficient training technique essential for handling large models which are important for efficient RAG implementation and training."}, {"fullname_first_author": "Wenqi Fan", "paper_title": "A survey on RAG meeting LLMs: Towards retrieval-augmented large language models", "publication_date": "2024-00-00", "reason": "This survey paper provides a comprehensive overview of RAG and LLMs, which is the theoretical foundation of the current work, making it a key reference for understanding the broader context of RAG research."}]}