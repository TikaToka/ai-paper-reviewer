[{"content": "| Model | Datasets | Data Recipe | Training Recipe | Model Weights |\n|---|---|---|---|---|\n| OpenAI\u2019s o-series | \u2717 | \u2717 | \u2717 | \u2717 |\n| Google\u2019s Gemini 2.0 Flash Thinking | \u2717 | \u2717 | \u2717 | \u2717 |\n| Qwen\u2019s QwQ | \u2717 | \u2717 | \u2717 | \u2713 |\n| DeepSeek R1 | \u2717 | \u2717 | P | \u2713 |\n| **Typhoon T1** | \u2713 | \u2713 | \u2713 | \u2713 |", "caption": "Table 1: A comparison of openness among popular reasoning models, focusing on dataset availability, data processing transparency, training methodology, and model accessibility. P denotes partial details. Typhoon T1 is the only model providing full openness across all categories, including its data recipe.", "description": "\ud45c 1\uc740 \uc778\uae30 \uc788\ub294 \ucd94\ub860 \ubaa8\ub378\ub4e4\uc758 \uac1c\ubc29\uc131\uc744 \ub370\uc774\ud130\uc14b \uac00\uc6a9\uc131, \ub370\uc774\ud130 \ucc98\ub9ac \ud22c\uba85\uc131, \ud6c8\ub828 \ubc29\ubc95 \ubc0f \ubaa8\ub378 \uc811\uadfc\uc131\uc774\ub77c\ub294 \ub124 \uac00\uc9c0 \uce21\uba74\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  'P'\ub294 \ubd80\ubd84\uc801\uc778 \uc815\ubcf4\ub9cc \uacf5\uac1c\ub418\uc5c8\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. Typhoon T1 \ubaa8\ub378\uc740 \ub370\uc774\ud130 \ub808\uc2dc\ud53c\ub97c \ud3ec\ud568\ud55c \ubaa8\ub4e0 \ubc94\uc8fc\uc5d0\uc11c \uc644\ubcbd\ud55c \uac1c\ubc29\uc131\uc744 \uc81c\uacf5\ud558\ub294 \uc720\uc77c\ud55c \ubaa8\ub378\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \ub2e4\ub978 \ubaa8\ub378\ub4e4\uacfc \ub2ec\ub9ac Typhoon T1\uc740 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b, \ub370\uc774\ud130 \uc804\ucc98\ub9ac \uacfc\uc815, \ubaa8\ub378 \ud6c8\ub828 \uacfc\uc815, \uadf8\ub9ac\uace0 \ubaa8\ub378 \uc790\uccb4\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \uc815\ubcf4\ub97c \uacf5\uac1c\ud558\uc5ec, \uc5f0\uad6c\uc758 \uc7ac\ud604\uc131\uacfc \ud22c\uba85\uc131\uc744 \ub192\uc600\uc2b5\ub2c8\ub2e4.", "section": "1 INTRODUCTION"}, {"content": "| Model | GSM8K | HumanEval+ | IFEval | GPQA | MMLU Pro | ThaiExam |\n|---|---|---|---|---|---|---|\n| **Typhoon 2** |  |  |  |  |  |  |\n| \u2003Zero-Shot | 57.32 | 63.51 | **69.32** | 25.00 | **26.61** | 32.69 |\n| \u2003Zero-Shot CoT | 53.83 | 0.00 | 68.95 | 25.45 | 23.36 | **33.27** |\n| \u2003SFT | 20.62 | 46.24 | 17.74 | 16.74 | 13.96 | 15.65 |\n| **Typhoon T** |  |  |  |  |  |  |\n| \u2003Unstructured | 59.82 | 67.88 | 34.01 | 24.78 | 20.44 | 21.36 |\n| \u2003Semi-structured | 57.24 | **72.87** | 55.27 | **27.68** | 19.46 | 21.92 |\n| \u2003Structured | **62.02** | 69.76 | 53.60 | **27.23** | 23.56 | 22.84 |", "caption": "Table 2: Performance of models on each benchmark (higher is better). Bold indicates the best score in each column. Underlined scores denote improvements over the baseline, Typhoon 2 3B Instruct. We apply this convention across all results tables. Typhoon 2 refers to Typhoon 2 3B Instruct, and Typhoon T refers to its variant trained on a long-thinking dataset. SFT refers to supervised fine-tuning on the original datasets. All reasoning models show improvement over SFT on the original dataset.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub192\uc740 \uc810\uc218\uc77c\uc218\ub85d \uc131\ub2a5\uc774 \uc88b\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uad75\uc740 \uae00\uc528\ub294 \uac01 \uc5f4\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uc810\uc218\ub97c \ub098\ud0c0\ub0b4\uace0, \ubc11\uc904 \uce5c \uc810\uc218\ub294 \uae30\uc900 \ubaa8\ub378\uc778 Typhoon 2 3B Instruct\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\uc5c8\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. Typhoon 2\ub294 Typhoon 2 3B Instruct\ub97c, Typhoon T\ub294 \uc7a5\uae30 \uc0ac\uace0 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud6c8\ub828\ub41c \ubcc0\ud615 \ubaa8\ub378\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. SFT\ub294 \uc6d0\ubcf8 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc9c0\ub3c4 \ud559\uc2b5 \ubbf8\uc138 \uc870\uc815\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ubaa8\ub4e0 \ucd94\ub860 \ubaa8\ub378\uc740 \uc6d0\ubcf8 \ub370\uc774\ud130\uc14b\uc5d0\uc11c SFT\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "3 \uc2e4\ud5d8"}, {"content": "| Model | GSM8K | HumanEval+ | IFEval | GPQA | MMLU Pro | ThaiExam |\n|---|---|---|---|---|---|---|\n| **Typhoon T1-EN** | **62.09** | **70.60** | 49.54 | **30.80** | **27.39** | 21.71 |\n| + 1.5k, CSFT | 41.39 | 65.79 | 33.83 | 23.66 | 4.30 | 21.20 |\n| + 1.5k | 60.12 | 67.90 | **51.76** | 29.91 | 19.32 | **23.56** |\n| + 1k | 61.94 | 66.77 | **50.09** | 24.55 | 23.48 | 21.57 |\n| + 0.5k | 60.88 | 68.24 | **49.72** | 25.45 | 23.05 | **22.62** |", "caption": "Table 3: Performance of model variants on various benchmarks, evaluating the impact of additional training data. CSFT refers to continual SFT. Adding 1.5k samples improves IFEval and ThaiExam scores, while CSFT significantly reduces overall performance.", "description": "\ud45c 3\uc740 \ucd94\uac00\uc801\uc778 \ud6c8\ub828 \ub370\uc774\ud130\uc758 \uc601\ud5a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ubaa8\ub378 \ubcc0\ud615\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. CSFT\ub294 \uc9c0\uc18d\uc801\uc778 SFT(Supervised Fine-Tuning)\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4. 1.5k\uac1c\uc758 \uc0d8\ud50c\uc744 \ucd94\uac00\ud558\uba74 IFEval\uacfc ThaiExam \uc810\uc218\uac00 \ud5a5\uc0c1\ub418\uc9c0\ub9cc, CSFT\ub294 \uc804\ubc18\uc801\uc778 \uc131\ub2a5\uc744 \ud06c\uac8c \uc800\ud558\uc2dc\ud0b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ucd94\uac00 \ub370\uc774\ud130\uac00 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uacfc \uc9c0\uc18d\uc801\uc778 \ubbf8\uc138 \uc870\uc815\uc758 \uc704\ud5d8\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.4 THAI-TRANSLATED DATA IMPROVES THAI PERFORMANCE AT THE COST OF OTHERS"}, {"content": "| Model | GSM8K | HumanEval+ | IFEval | GPQA | MMLU Pro | ThaiExam |\n|---|---|---|---|---|---|---|\n| **Typhoon T1** | 60.12 | 67.90 | **51.76** | **29.91** | **19.32** | 23.56 |\n| + EN | 46.17 | 0.00 | 48.98 | 26.56 | 16.55 | **25.31** |\n| + TH | 48.29 | 0.00 | 44.73 | 25.67 | 16.05 | 24.66 |", "caption": "Table 4: EN denotes forced reasoning in English, and TH denotes forced reasoning in Thai. Constraining Typhoon T1 to reason in a specific language degrades overall accuracy. English reasoning is slightly more effective than Thai reasoning across most benchmarks. However, allowing the model to choose its own thinking language yields the best performance.", "description": "\ud45c 4\ub294 Typhoon T1 \ubaa8\ub378\uc774 \uc601\uc5b4 \ub610\ub294 \ud0dc\uad6d\uc5b4\ub85c \ucd94\ub860\ud558\ub3c4\ub85d \uc81c\uc57d\ud588\uc744 \ub54c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\uc815 \uc5b8\uc5b4\ub85c \ucd94\ub860\ud558\ub3c4\ub85d \uc81c\ud55c\ud558\uba74 \uc804\ubc18\uc801\uc778 \uc815\ud655\ub3c4\uac00 \uc800\ud558\ub429\ub2c8\ub2e4. \ub300\ubd80\ubd84\uc758 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc601\uc5b4 \ucd94\ub860\uc774 \ud0dc\uad6d\uc5b4 \ucd94\ub860\ubcf4\ub2e4 \uc57d\uac04 \ub354 \ud6a8\uacfc\uc801\uc785\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \ubaa8\ub378\uc774 \uc790\uccb4 \uc0ac\uace0 \uc5b8\uc5b4\ub97c \uc120\ud0dd\ud558\ub3c4\ub85d \ud558\uba74 \ucd5c\uc0c1\uc758 \uc131\ub2a5\uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.5 \ucd94\ub860 \ubaa8\ub378\uc774 \uc790\uccb4 \ucd94\ub860 \uc5b8\uc5b4 \uc120\ud0dd"}, {"content": "| Domain/Dataset | #Records |\n|---|---| \n| _Mathematics_ | _21,941_ |\n| MATH (Hendrycks et al., 2021) | 7,500 |\n| Tulu 3 SFT Personas Math Grade (Lambert et al., 2025) | 7,497 |\n| PRM800K Phase 2 (Lightman et al., 2024) | 5,809 |\n| PRM800K Phase 1 (Lightman et al., 2024) | 808 |\n| O1 Journey (Qin et al., 2024) | 327 |\n| _Instruction Following_ | _13,188_ |\n| No Robots (Rajani et al., 2023) | 9,500 |\n| UltraFeedback (Cui et al., 2024) | 3,688 |\n| _Coding_ | _10,814_ |\n| Evol codealpaca v1 (Luo et al., 2023) | 5,564 |\n| Tulu 3 SFT Personas Code (Lambert et al., 2025) | 5,250 |\n| _Safety_ | _5,300_ |\n| HelpSteer (Wang et al., 2023c) | 5,300 |\n| _Finance_ | _4,434_ |\n| Wealth Alpaca (Bharti, 2023) | 4,434 |\n| **Total** | **55,677** |", "caption": "Table 5: Data mixture of the training set.", "description": "\ud45c 5\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc758 \ub370\uc774\ud130 \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc218\ud559, \uc9c0\uc2dc\uc0ac\ud56d \ub530\ub974\uae30, \ucf54\ub529, \uc548\uc804, \uae08\uc735 \ub4f1 \ub2e4\uc12f \uac00\uc9c0 \ub3c4\uba54\uc778\uc5d0\uc11c \uc218\uc9d1\ub41c \uc5ec\ub7ec \uac1c\ubc29\ud615 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud588\uc73c\uba70, \uac01 \ub3c4\uba54\uc778\ubcc4 \ub370\uc774\ud130\uc14b\uc758 \uc885\ub958\uc640 \uac1c\uc218\ub97c \uc0c1\uc138\ud788 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\ub294 \ub2e4\uc591\ud558\uc9c0\ub9cc, \ucd1d 55,677\uac1c\uc758 \ub808\ucf54\ub4dc\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ubcf8 \ud45c\ub294 \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc758 \uad6c\uc131\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub418\ub294 \uc138\ubd80 \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "2.2 \ub370\uc774\ud130 \uc900\ube44"}, {"content": "| Model | GSM8K | GPQA | MMLU Pro | ThaiExam |\n|---|---|---|---|---|\n| **Typhoon 2** |  |  |  |  |\n| Zero-shot | 104.61 | 384.78 | 130.41 | 21.90 |\n| Zero-shot CoT | 741.97 | 1238.54 | 1697.96 | 149.19 |\n| SFT | 72.22 | 479.55 | 91.25 | 587.95 |\n| **Typhoon T** |  |  |  |  |\n| Unstructured | 169.03 | 478.53 | 491.33 | 829.21 |\n| Semi-structured | 170.20 | 795.38 | 487.39 | 900.90 |\n| Structured | 102.96 | 466.21 | 293.23 | 995.04 |", "caption": "Table 6: Average number of output tokens generated by each model on the benchmarks.", "description": "\ud45c 6\uc740 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uac01 \ubaa8\ub378\uc774 \uc0dd\uc131\ud55c \ud3c9\uade0 \ucd9c\ub825 \ud1a0\ud070 \uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uace0 \ud3c9\uade0 \ucd9c\ub825 \uae38\uc774\uac00 \ubaa8\ub378\uc758 \uc131\ub2a5\uc774\ub098 \ud2b9\uc815 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.  \uc81c\ub85c\uc0f7, \uc81c\ub85c\uc0f7 CoT, SFT, \uadf8\ub9ac\uace0 \uc138 \uac00\uc9c0 \uc720\ud615\uc758 Typhoon T \ubaa8\ub378(\uad6c\uc870\ud654\ub418\uc9c0 \uc54a\uc74c, \ubc18\uad6c\uc870\ud654\ub428, \uad6c\uc870\ud654\ub428)\uc758 \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3 \uc2e4\ud5d8"}, {"content": "| Dataset Size | GSM8K | HumanEval+ | IFEval | GPQA | MMLU Pro | ThaiExam |\n|---|---|---|---|---|---|---|\n| 100% | 62.02 | 69.76 | **53.60** | 27.23 | 23.56 | **22.84** |\n| 75% | **62.09** | **70.60** | 49.54 | **30.80** | 27.39 | 21.71 |\n| 50% | 61.87 | 64.59 | 48.80 | **29.46** | **27.63** | 20.36 |\n| 25% | **62.09** | 66.93 | 50.46 | **29.69** | **30.05** | 20.54 |\n| 10% | 60.20 | 65.51 | 50.65 | **29.24** | **29.03** | 21.07 |\n| 5% | 60.88 | 64.62 | 47.13 | **30.13** | **29.65** | 19.91 |", "caption": "Table 7: Performance at different dataset sizes. Smaller dataset sizes can sometimes outperform the 100% baseline, particularly in GPQA and MMLU Pro.", "description": "\ud45c 7\uc740 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 GPQA \ubc0f MMLU Pro\uc640 \uac19\uc740 \ud2b9\uc815 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub354 \uc791\uc740 \ub370\uc774\ud130\uc14b\uc774 100% \ud06c\uae30\uc758 \ub370\uc774\ud130\uc14b\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub354 \uc88b\uc740 \uacbd\uc6b0\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uacfc\uc801\ud569(overfitting)\uc758 \uc704\ud5d8\uc131\uc744 \uace0\ub824\ud558\uc5ec \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \uc131\ub2a5(generalization)\uacfc \uacfc\uc801\ud569(overfitting) \uac04\uc758 \uade0\ud615\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \uc911\uc694\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4.  \ub370\uc774\ud130\uc14b \ud06c\uae30 \ubcc0\ud654\uc5d0 \ub530\ub978 \ubaa8\ub378 \uc131\ub2a5 \ubcc0\ud654\ub97c \uc790\uc138\ud788 \ubd84\uc11d\ud558\uc5ec \ucd5c\uc801\uc758 \ub370\uc774\ud130\uc14b \ud06c\uae30\ub97c \uacb0\uc815\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3.2 \ub370\uc774\ud130 \uc591\uc758 \uade0\ud615: \ucd5c\uc801\uc758 \ucd94\ub860 \ubaa8\ub378 \uc131\ub2a5\uc744 \uc704\ud55c \ud575\uc2ec \uc694\uc18c"}, {"content": "| Model | GSM8K | HumanEval+ | IFEval | GPQA | MMLU Pro | ThaiExam |\n|---|---|---|---|---|---|---|\n| **Typhoon T1-EN** | **62.09** | **70.60** | 49.54 | **30.80** | 27.39 | 21.71 |\n|  - IF | 59.59 | 69.57 | 46.58 | 29.02 | 26.34 | **22.64** |\n|  - Math | 59.51 | 69.47 | **53.60** | *25.45* | **28.52** | 20.88 |\n|  - Code | 56.94 | *64.24* | 41.96 | 27.68 | **27.65** | 19.57 |\n|  - Safety | *56.71* | 64.35 | *41.59* | 30.13 | **29.38** | *17.19* |\n|  - Finance | 61.94 | 67.06 | **50.65** | 27.90 | *20.45* | 18.68 |", "caption": "Table 8: Leave-one-out experiment results, assessing the impact of removing specific domains from training. red values highlight the largest performance drop in each column. The \u201c-\u201d symbol denotes the removal of the corresponding domain from training. Excluding mathematical reasoning strongly improves IFEval performance, while safety removal boosts MMLU Pro.", "description": "\ud45c 8\uc740 \ud2b9\uc815 \ub3c4\uba54\uc778\uc744 \uc81c\uc678\ud558\uace0 \ud6c8\ub828\uc2dc\ud0a8 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube68\uac04\uc0c9 \uac12\uc740 \uac01 \uc5f4\uc5d0\uc11c \uac00\uc7a5 \ud070 \uc131\ub2a5 \uc800\ud558\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. '-' \uae30\ud638\ub294 \ud6c8\ub828\uc5d0\uc11c \ud2b9\uc815 \ub3c4\uba54\uc778\uc774 \uc81c\uac70\ub418\uc5c8\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc218\ud559\uc801 \ucd94\ub860\uc744 \uc81c\uc678\ud558\uba74 IFEval \uc131\ub2a5\uc774 \ud06c\uac8c \ud5a5\uc0c1\ub418\ub294 \ubc18\uba74, \uc548\uc804 \ub3c4\uba54\uc778\uc744 \uc81c\uac70\ud558\uba74 MMLU Pro \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub429\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ub3c4\uba54\uc778\uc758 \ud6c8\ub828 \ub370\uc774\ud130\uac00 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud55c \uac83\uc785\ub2c8\ub2e4. \ud2b9\uc815 \ub3c4\uba54\uc778\uc758 \ub370\uc774\ud130\ub97c \uc81c\uc678\ud588\uc744 \ub54c, \ub2e4\ub978 \ub3c4\uba54\uc778\uc758 \uc131\ub2a5\uc5d0 \uc5b4\ub5a4 \uc601\ud5a5\uc744 \uc8fc\ub294\uc9c0 \uc790\uc138\ud788 \uc0b4\ud3b4\ubd05\ub2c8\ub2e4.  \uc218\ud559, \uc548\uc804, \ucf54\ub529, \uae08\uc735, \uc9c0\uc2dc\uc0ac\ud56d \ub530\ub974\uae30 \ub4f1 \ub2e4\uc12f \uac1c\uc758 \ub3c4\uba54\uc778 \ub370\uc774\ud130\uc14b\uc744 \uac01\uac01 \uc81c\uc678\ud55c \uacbd\uc6b0\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc73c\ub85c,  \uac01 \ub3c4\uba54\uc778 \ub370\uc774\ud130\uac00 \ubaa8\ub378\uc758 \uc804\ubc18\uc801\uc778 \ucd94\ub860 \ub2a5\ub825\uc5d0 \uc5bc\ub9c8\ub098 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud558\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.3 \uc548\uc804 \ub3c4\uba54\uc778 \uc81c\uc678 \uc2dc \ubaa8\ub378 \ud6c8\ub828\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5"}]