{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of large language models (LLMs), introducing the concept of few-shot learning and significantly impacting subsequent research on LLMs' reasoning abilities."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduced the chain-of-thought prompting technique, a crucial method for eliciting step-by-step reasoning in LLMs and greatly advancing the development of large reasoning models."}, {"fullname_first_author": "Hunter Lightman", "paper_title": "Let's verify step by step", "publication_date": "2023-05-01", "reason": "This paper introduced the concept of process reward models (PRMs), a significant step in improving LLMs' reasoning capabilities by providing step-wise rewards and greatly enhancing the automated data construction for LLM reasoning."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-12-01", "reason": "This paper introduced direct preference optimization (DPO), an efficient and effective method to align LLMs with human preferences through pairwise preference comparisons, which reduces reliance on explicit reward models."}, {"fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-04-01", "reason": "This paper introduced a crucial reinforcement learning framework (RLHF) for training helpful and harmless LLMs, which has been widely adopted and significantly improved LLMs' reasoning and alignment with human values."}]}