<table id='0' style='font-size:14px'><tr><td>Datasets</td><td colspan="3">PTB-XL Super</td><td>PTB-XL Report</td><td colspan="3">CODE-15%</td><td>ECG-QA</td></tr><tr><td>Metric</td><td>AUC</td><td>F1</td><td>HL</td><td>Report Score</td><td>AUC</td><td>F1</td><td>HL</td><td>Accuracy</td></tr><tr><td>Random</td><td>50.3</td><td>33.2</td><td>50.1</td><td>0</td><td>48.8</td><td>15.0</td><td>32.1</td><td>16.2</td></tr><tr><td colspan="9">Domain-specific Methods</td></tr><tr><td>METS</td><td>-</td><td>65.7†</td><td>-</td><td>N/A</td><td>-</td><td>-</td><td>-</td><td>N/A</td></tr><tr><td>MERL</td><td>74.2t</td><td>-</td><td>-</td><td>N/A</td><td>-</td><td>-</td><td>、</td><td>N/A</td></tr><tr><td>ST-MEM</td><td>71.4†</td><td>-</td><td>-</td><td>N/A</td><td>-</td><td>-</td><td>-</td><td>N/A</td></tr><tr><td>ECG-GPT</td><td>69.5*</td><td>53.9*</td><td>20.1*</td><td>47.8*</td><td>68.9*</td><td>40.1*</td><td>17.4*</td><td>N/A</td></tr><tr><td colspan="9">Proprietary MLLMs</td></tr><tr><td>GPT-4o</td><td>55.6</td><td>28.3</td><td>26.2</td><td>50.2</td><td>59.9</td><td>24.9</td><td>15.7</td><td>35.2</td></tr><tr><td>GPT-4o mini</td><td>52.0</td><td>20.4</td><td>31.7</td><td>37.1</td><td>57.5</td><td>22.0</td><td>15.1</td><td>14.9</td></tr><tr><td>Gemini 1.5 Pro</td><td>50.7</td><td>15.3</td><td>27.9</td><td>35.9</td><td>56.7</td><td>20.0</td><td>15.9</td><td>33.2</td></tr><tr><td>Claude 3.5 Sonnet</td><td>54.0</td><td>27.5</td><td>29.6</td><td>43.7</td><td>58.3</td><td>20.3</td><td>17.8</td><td>34.2</td></tr><tr><td colspan="9">Open-source MLLMs</td></tr><tr><td>LLaVA-Med</td><td>50.0</td><td>12.3</td><td>28.1</td><td>24.3</td><td>69.2</td><td>27.0</td><td>33.4</td><td>29.5</td></tr><tr><td>LLaVA-1.5-7B</td><td>50.0</td><td>12.3</td><td>28.1</td><td>27.2</td><td>63.9</td><td>19.2</td><td>25.3</td><td>25.2</td></tr><tr><td>LLaVA-1.5-13B</td><td>50.0</td><td>35.2</td><td>48.4</td><td>20.7</td><td>53.9</td><td>13.1</td><td>13.6</td><td>21.2</td></tr><tr><td>LLaVA-1.6- Vicuna-7B</td><td>50.0</td><td>15.8</td><td>29.4</td><td>16.5</td><td>50.1</td><td>1.0</td><td>13.6</td><td>13.3</td></tr><tr><td>LLaVA-1.6- Vicuna-13B</td><td>50.0</td><td>20.1</td><td>38.3</td><td>5.9</td><td>53.0</td><td>3.6</td><td>16.6</td><td>22.0</td></tr><tr><td>LLaVA-1.6-34B</td><td>50.2</td><td>19.9</td><td>36.0</td><td>17.0</td><td>57.2</td><td>12.8</td><td>16.6</td><td>22.4</td></tr><tr><td>LLaVA-One Vision-7B</td><td>49.8</td><td>11.4</td><td>34.5</td><td>30.0</td><td>58.7</td><td>17.0</td><td>20.6</td><td>20.4</td></tr><tr><td>LLaVA-OneVision-72B</td><td>50.6</td><td>29.6</td><td>50.4</td><td>40.6</td><td>52.3</td><td>7.0</td><td>13.1</td><td>25.0</td></tr><tr><td>Deepseek-VL-Chat-7B</td><td>50.9</td><td>15.7</td><td>27.9</td><td>15.6</td><td>63.7</td><td>27.5</td><td>22.4</td><td>21.1</td></tr><tr><td>Idefics2-8B</td><td>50.7</td><td>21.9</td><td>31.2</td><td>10.6</td><td>49.0</td><td>17.9</td><td>47.9</td><td>26.1</td></tr><tr><td>Mantis-8B-siglip-Llama3</td><td>50.6</td><td>20.4</td><td>30.0</td><td>16.0</td><td>57.5</td><td>17.9</td><td>15.7</td><td>23.8</td></tr><tr><td>MiniCPM-V-2.6</td><td>49.0</td><td>37.7</td><td>63.8</td><td>15.4</td><td>56.6</td><td>25.3</td><td>22.0</td><td>20.8</td></tr><tr><td>Phi-3- Vision-128k-Instruct</td><td>50.0</td><td>29.6</td><td>48.4</td><td>20.2</td><td>69.6</td><td>22.6</td><td>38.8</td><td>28.4</td></tr><tr><td>Qwen2-VL-7B</td><td>51.3</td><td>22.4</td><td>30.8</td><td>43.0</td><td>60.7</td><td>24.8</td><td>20.5</td><td>20.4</td></tr><tr><td>Qwen2-VL-72B</td><td>54.0</td><td>28.3</td><td>30.2</td><td>48.9</td><td>60.6</td><td>23.6</td><td>16.1</td><td>23.7</td></tr><tr><td>Intern VL2-8B</td><td>50.6</td><td>14.3</td><td>27.8</td><td>38.1</td><td>55.8</td><td>16.1</td><td>17.7</td><td>22.3</td></tr><tr><td>Intern VL2-40B</td><td>51.2</td><td>18.7</td><td>34.6</td><td>41.8</td><td>56.7</td><td>16.2</td><td>17.4</td><td>18.2</td></tr><tr><td>Intern VL2-Llama3-76B</td><td>50.4</td><td>9.4</td><td>35.6</td><td>41.4</td><td>59.0</td><td>20.2</td><td>20.5</td><td>21.8</td></tr><tr><td>PULSE-7B (Ours)</td><td>82.4</td><td>74.8</td><td>11.0</td><td>61.3</td><td>90.7</td><td>85.4</td><td>5.0</td><td>73.8</td></tr><tr><td>△ over best proprietary MLLM</td><td>+27</td><td>+47</td><td>+15</td><td>+11</td><td>+30</td><td>+61</td><td>+10</td><td>+39</td></tr><tr><td>△ over best open-source MLLM</td><td>+28</td><td>+37</td><td>+17</td><td>+12</td><td>+21</td><td>+58</td><td>+8</td><td>+44</td></tr></table>