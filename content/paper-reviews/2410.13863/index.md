---
title: "Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens"
summary: "FLUID, a 10.5B parameter autoregressive model using continuous tokens and random order generation, achieves state-of-the-art text-to-image generation, demonstrating that careful model design can unloc..."
categories: ["AI Generated"]
tags: ["ðŸ”– 24-10-17", ]
showSummary: true
date: 2024-10-17
draft: false
---

{{< keyword >}} 2410.13863 {{< /keyword >}}

### TL;DR


{{< lead >}}

This research paper investigates why scaling autoregressive models for image generation hasn't been as successful as with language models.  The authors explore two key factors: using continuous vs. discrete tokens and employing random vs. fixed order token generation. Experiments show that continuous tokens produce significantly better image quality, and random order generation is superior for capturing the global structure of the generated image, outperforming fixed-order generation in benchmark evaluations.  Based on these insights, they developed FLUID, a 10.5B parameter autoregressive model. FLUID uses continuous tokens and random order generation and sets a new state-of-the-art in zero-shot FID scores on MS-COCO, as well as an improved GenEval score. The study concludes that the scaling gap between vision and language models can be bridged with careful design choices, and that the findings encourage future research in this area.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.13863" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}
<br><br>
{{< button href="https://huggingface.co/papers/2410.13863" target="_self" >}}
{{< icon "hf-logo" >}} &nbsp; on Hugging Face
{{< /button >}}

#### Why does it matter?
This paper is crucial for researchers in computer vision and large language models. It challenges the common belief that scaling autoregressive models in vision is less effective than in language by demonstrating that using continuous tokens and random generation orders significantly improves performance.  The findings provide valuable insights for future research in bridging the scaling gap between vision and language models and offer a new approach to developing high-quality text-to-image generation models.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Using continuous tokens instead of discrete tokens significantly enhances image generation quality in autoregressive models. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} Random-order generation outperforms traditional raster-order generation in terms of image quality and alignment with text prompts. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Scaling autoregressive models effectively in vision is possible, but requires design choices that account for the unique characteristics of visual data. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](https://ai-paper-reviewer.com/2410.13863/figures_1_0.png)

> ðŸ”¼ Figure 1 displays example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>





![](https://ai-paper-reviewer.com/2410.13863/charts_4_0.png)

> ðŸ”¼ The chart compares and contrasts raster-order and random-order autoregressive models in terms of their token prediction mechanisms and attention strategies.
> <details>
> <summary>read the caption</summary>
> Figure 2: Autoregressive models with different orders. (a) A raster-order autoregressive model predicts one next token based on the known ones, implemented using a GPT-like transformer with causal attention. (b) A random-order autoregressive model predicts one or multiple tokens simultaneously given a random order, implemented using a BERT-like transformer with bidirectional attention.
> </details>





{{< table-caption >}}
<br><table id='2' style='font-size:14px'><tr><td></td><td rowspan="2">#params</td><td rowspan="2">MS-COCO FID-30Kâ†“</td><td colspan="7">GenEval</td></tr><tr><td></td><td>Single Obj.</td><td>Two Obj.</td><td>Counting</td><td>Colors</td><td>Position</td><td>Color Attri.</td><td>Overall</td></tr><tr><td colspan="10">diffusion model</td></tr><tr><td>LDM</td><td>1.4B</td><td>12.64</td><td>0.92</td><td>0.29</td><td>0.23</td><td>0.70</td><td>0.02</td><td>0.05</td><td>0.37</td></tr><tr><td>DALL-E 2</td><td>4.2B</td><td>10.39</td><td>0.94</td><td>0.66</td><td>0.49</td><td>0.77</td><td>0.10</td><td>0.19</td><td>0.52</td></tr><tr><td>DALL-E 3</td><td>-</td><td>-</td><td>0.96</td><td>0.87</td><td>0.47</td><td>0.83</td><td>0.43</td><td>0.45</td><td>0.67</td></tr><tr><td>Imagen</td><td>3B</td><td>7.27</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SD3</td><td>8B</td><td>-</td><td>0.98</td><td>0.84</td><td>0.66</td><td>0.74</td><td>0.40</td><td>0.43</td><td>0.68</td></tr><tr><td>Transfusion</td><td>7.3B</td><td>6.78</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.63</td></tr><tr><td>RAPHAEL</td><td>3B</td><td>6.61</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td colspan="10">autoregressive model</td></tr><tr><td>CM3Leon+</td><td>7B</td><td>10.82</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Show-o</td><td>1.3B</td><td>9.24</td><td>0.95</td><td>0.52</td><td>0.49</td><td>0.82</td><td>0.11</td><td>0.28</td><td>0.53</td></tr><tr><td>Muse</td><td>3B</td><td>7.88</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Parti</td><td>20B</td><td>7.23</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Fluid (our work)</td><td>369M</td><td>7.23</td><td>0.96</td><td>0.64</td><td>0.53</td><td>0.78</td><td>0.33</td><td>0.46</td><td>0.62</td></tr><tr><td></td><td>665M</td><td>6.84</td><td>0.96</td><td>0.73</td><td>0.51</td><td>0.77</td><td>0.42</td><td>0.51</td><td>0.65</td></tr><tr><td></td><td>1.1B</td><td>6.59</td><td>0.96</td><td>0.77</td><td>0.61</td><td>0.78</td><td>0.34</td><td>0.53</td><td>0.67</td></tr><tr><td></td><td>3.1B</td><td>6.41</td><td>0.98</td><td>0.83</td><td>0.60</td><td>0.82</td><td>0.41</td><td>0.53</td><td>0.70</td></tr><tr><td></td><td>10.5B</td><td>6.16</td><td>0.96</td><td>0.83</td><td>0.63</td><td>0.80</td><td>0.39</td><td>0.51</td><td>0.69</td></tr></table>{{< /table-caption >}}

> ðŸ”¼ Table 1 compares the performance of Fluid against other state-of-the-art text-to-image generation models using FID-30K and GenEval benchmark metrics.
> <details>
> <summary>read the caption</summary>
> Table 1: System-level comparison. Fluid achieves leading results on both MS-COCO zero-shot FID-30K and GenEval benchmark (Ghosh et al., 2024). â€ : CM3Leon result is reported without retrieval.
> </details>



### More visual insights

<details>
<summary>More on figures
</summary>


![](https://ai-paper-reviewer.com/2410.13863/figures_5_0.png)

> ðŸ”¼ This figure illustrates the framework for text-to-image generation using a transformer-based model with either discrete or continuous image tokens.
> <details>
> <summary>read the caption</summary>
> Figure 3: Our text-to-image generation framework. A pre-trained image tokenizer converts the image into either discrete or continuous tokens. The text is embedded using a pre-trained T5 encoder, followed by a trainable text aligner. The transformer then takes cross-attention from the text embeddings to predict the missing tokens (only random order model is shown here).
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_5_1.png)

> ðŸ”¼ The figure shows the reconstruction quality of discrete and continuous image tokenizers, demonstrating the superior performance of the continuous tokenizer.
> <details>
> <summary>read the caption</summary>
> Figure 4: Reconstruction quality of the tokenizers. Image resolution is 256x256. The discrete tokenizer is significantly worse than the continuous tokenizer.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_9_0.png)

> ðŸ”¼ Figure 1 presents a series of images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_9_1.png)

> ðŸ”¼ The figure displays various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_9_2.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_9_3.png)

> ðŸ”¼ Figure 1 presents example image samples generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce high-quality and diverse images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_9_4.png)

> ðŸ”¼ Figure 1 shows various example images generated by the Fluid 10.5B autoregressive model, highlighting its ability to produce high-quality and diverse outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_9_5.png)

> ðŸ”¼ Figure 1 presents several images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_0.png)

> ðŸ”¼ Figure 1 shows example images generated by the Fluid 10.5B autoregressive model, highlighting its ability to generate high-quality and diverse images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_1.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce high-quality and diverse outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_2.png)

> ðŸ”¼ The figure displays several example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_3.png)

> ðŸ”¼ Figure 1 shows example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate high-quality and diverse images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_4.png)

> ðŸ”¼ The figure shows various example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_5.png)

> ðŸ”¼ Figure 1 presents example image samples generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate high-quality images with diverse content.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_6.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_7.png)

> ðŸ”¼ Figure 1 presents several example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_8.png)

> ðŸ”¼ The figure displays a collection of images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_9.png)

> ðŸ”¼ Figure 1 shows several example images generated by the Fluid 10.5B autoregressive model, highlighting its ability to generate high-quality and diverse images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_10.png)

> ðŸ”¼ Figure 1 presents several example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_11.png)

> ðŸ”¼ Figure 1 shows several example images generated by the Fluid 10.5B autoregressive model, highlighting its ability to produce high-quality and diverse outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_12.png)

> ðŸ”¼ Figure 1 shows example images generated by the Fluid 10.5B autoregressive model, highlighting its ability to generate high-quality and diverse images from text descriptions.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_13.png)

> ðŸ”¼ Figure 1 shows example image samples generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_14.png)

> ðŸ”¼ Figure 1 presents image samples generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate high-quality and diverse images using continuous tokens.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_17_15.png)

> ðŸ”¼ Figure 1 presents several example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce high-quality and diverse outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_0.png)

> ðŸ”¼ Figure 1 shows several example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_1.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_2.png)

> ðŸ”¼ Figure 1 shows several example images generated by the Fluid 10.5B autoregressive model, highlighting the model's ability to generate diverse and high-quality images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_3.png)

> ðŸ”¼ Figure 1 shows various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_4.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_5.png)

> ðŸ”¼ The figure displays various image samples generated by the Fluid 10.5B autoregressive model, showcasing its capability to generate diverse and high-quality images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_6.png)

> ðŸ”¼ Figure 1 presents several example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce high-quality and diverse outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_7.png)

> ðŸ”¼ The figure displays various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to create diverse and visually appealing outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_8.png)

> ðŸ”¼ Figure 1 displays example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_9.png)

> ðŸ”¼ The figure displays various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_10.png)

> ðŸ”¼ Figure 1 shows various image samples generated by the Fluid 10.5B autoregressive model, highlighting its ability to produce diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_11.png)

> ðŸ”¼ Figure 1 shows various image samples generated by the Fluid 10.5B autoregressive model, highlighting its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_12.png)

> ðŸ”¼ Figure 1 shows various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_13.png)

> ðŸ”¼ Figure 1 shows example images generated by the Fluid 10.5B autoregressive model, highlighting its ability to generate high-quality and diverse images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_14.png)

> ðŸ”¼ Figure 1 presents sample images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_18_15.png)

> ðŸ”¼ The figure displays several example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce high-quality and diverse outputs from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_0.png)

> ðŸ”¼ The figure displays various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce high-quality and diverse outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_1.png)

> ðŸ”¼ Figure 1 shows various example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_2.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_3.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_4.png)

> ðŸ”¼ The figure displays various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to create diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_5.png)

> ðŸ”¼ Figure 1 presents sample images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce high-quality and diverse images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_6.png)

> ðŸ”¼ Figure 1 presents various example images generated by the Fluid 10.5B autoregressive model, showcasing its capacity to produce high-quality and diverse outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_7.png)

> ðŸ”¼ The figure displays various image samples generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_8.png)

> ðŸ”¼ The figure displays several example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate high-quality and diverse images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_9.png)

> ðŸ”¼ The figure displays various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_10.png)

> ðŸ”¼ Figure 1 shows several example images generated by the Fluid 10.5B autoregressive model, highlighting the model's ability to generate high-quality and diverse images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_11.png)

> ðŸ”¼ The figure shows example images generated by the Fluid 10.5B autoregressive model, highlighting its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_12.png)

> ðŸ”¼ The figure displays various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to create diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_13.png)

> ðŸ”¼ Figure 1 presents image samples generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images using continuous tokens.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_14.png)

> ðŸ”¼ Figure 1 presents image samples generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images using continuous tokens.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_19_15.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_0.png)

> ðŸ”¼ The figure shows several example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_1.png)

> ðŸ”¼ Figure 1 presents sample images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate high-quality and diverse images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_2.png)

> ðŸ”¼ Figure 1 shows image samples generated by the Fluid 10.5B autoregressive model, highlighting its ability to generate high-quality images using continuous tokens.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_3.png)

> ðŸ”¼ The figure illustrates the text-to-image generation framework, showing the image tokenizer, text encoder, transformer, and output head components.
> <details>
> <summary>read the caption</summary>
> Figure 3: Our text-to-image generation framework. A pre-trained image tokenizer converts the image into either discrete or continuous tokens. The text is embedded using a pre-trained T5 encoder, followed by a trainable text aligner. The transformer then takes cross-attention from the text embeddings to predict the missing tokens (only random order model is shown here).
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_4.png)

> ðŸ”¼ Figure 1 shows example images generated by the Fluid 10.5B autoregressive model, highlighting its ability to produce high-quality and diverse samples.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_5.png)

> ðŸ”¼ The figure displays various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_6.png)

> ðŸ”¼ Figure 1 shows several example images generated by the Fluid 10.5B autoregressive model, highlighting its ability to generate high-quality and diverse images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_7.png)

> ðŸ”¼ The figure shows various example images generated by the Fluid 10.5B autoregressive model, highlighting its ability to generate diverse and high-quality images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_8.png)

> ðŸ”¼ Figure 1 presents image samples generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_9.png)

> ðŸ”¼ Figure 1 shows various example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_10.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_11.png)

> ðŸ”¼ The figure displays various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce high-quality and diverse visual content from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_12.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality outputs using continuous tokens.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_13.png)

> ðŸ”¼ Figure 1 presents several images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality outputs from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_20_14.png)

> ðŸ”¼ Figure 1 presents image samples generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_21_0.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_21_1.png)

> ðŸ”¼ Figure 1 presents several example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_21_2.png)

> ðŸ”¼ The figure displays various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to create diverse and high-quality outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_21_3.png)

> ðŸ”¼ The figure shows image samples generated by the Fluid 10.5B autoregressive model, highlighting its ability to generate diverse and high-quality images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_21_4.png)

> ðŸ”¼ The figure shows example images generated by the Fluid 10.5B autoregressive model, highlighting the model's ability to generate high-quality and diverse images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_21_5.png)

> ðŸ”¼ Figure 1 shows several images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate high-quality and diverse images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_21_6.png)

> ðŸ”¼ Figure 1 displays various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_21_7.png)

> ðŸ”¼ Figure 1 shows various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_22_0.png)

> ðŸ”¼ The figure displays various images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality outputs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_22_1.png)

> ðŸ”¼ Figure 1 presents several example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_22_2.png)

> ðŸ”¼ Figure 1 shows example images generated by the Fluid 10.5B autoregressive model, highlighting its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_22_3.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to produce diverse and high-quality images from text descriptions.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_22_4.png)

> ðŸ”¼ The figure shows image samples generated by the Fluid 10.5B autoregressive model, highlighting the model's ability to generate high-quality images with diverse styles and content.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_22_5.png)

> ðŸ”¼ Figure 1 shows example images generated by the Fluid 10.5B autoregressive model, highlighting the model's ability to generate high-quality and diverse images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



![](https://ai-paper-reviewer.com/2410.13863/figures_22_6.png)

> ðŸ”¼ Figure 1 presents example images generated by the Fluid 10.5B autoregressive model, showcasing its ability to generate diverse and high-quality images from text prompts.
> <details>
> <summary>read the caption</summary>
> Figure 1: Samples from our Fluid 10.5B autoregressive model with continuous tokens.
> </details>



</details>



<details>
<summary>More on charts
</summary>


![](https://ai-paper-reviewer.com/2410.13863/charts_7_0.png)

> ðŸ”¼ The chart displays the scaling behavior of validation loss across four different autoregressive image generation model variants with respect to model size.
> <details>
> <summary>read the caption</summary>
> Figure 5: Validation loss scales as a power-law with model size. The validation loss is evaluated on 30K images randomly sampled from the MS-COCO 2014 training set. The x and y axes are in log-scale. The change in y is relatively small for each plot, making the log-scale alike linear-scale.
> </details>


![](https://ai-paper-reviewer.com/2410.13863/charts_7_1.png)

> ðŸ”¼ The chart displays the scaling behavior of four different autoregressive image generation model variants in terms of FID and GenEval scores, showing that random-order models using continuous tokens perform best.
> <details>
> <summary>read the caption</summary>
> Figure 6: Random-order models using continuous tokens (orange) achieve the best performance on evaluation metrics. FID (lower is better) is evaluated on 30K images randomly sampled from the MS-COCO 2014 training set, while the GenEval overall score (higher is better) is assessed using the 553 prompts provided by the official benchmark, with four images generated for each prompt. Among all models, random-order models on continuous tokens consistently show an improvement in evaluation metrics as model size increases and achieve the best FID and GenEval scores.
> </details>


![](https://ai-paper-reviewer.com/2410.13863/charts_8_0.png)

> ðŸ”¼ The chart displays the scaling behavior of validation loss, FID, and GenEval scores as functions of training steps and compute for different model sizes of Fluid, showing consistent improvements in both validation loss and evaluation performance with increased training steps and compute.
> <details>
> <summary>read the caption</summary>
> Figure 7: Validation losses and evaluation performance scale with increasing training steps and computes. We use random-order models with continuous tokens. Results for other autoregressive variants are included in the appendix. The training compute is computed as model GFLOPs Ã— batch size Ã— training steps Ã— 3, where the factor of 3 accounts for the backward pass being approximately twice as compute-intensive as the forward pass.
> </details>


![](https://ai-paper-reviewer.com/2410.13863/charts_8_1.png)

> ðŸ”¼ The chart displays the strong correlation between validation loss and FID/GenEval scores for Fluid models of varying sizes, indicating a near-linear relationship.
> <details>
> <summary>read the caption</summary>
> Figure 8: Validation loss and evaluation metrics are highly correlated. We use random-order models with continuous tokens. The Pearson correlation coefficients for FID and GenEval scores are 0.917 and -0.931, respectively. We also observe that the linear correlation slightly weakens and becomes less pronounced for the 3.1B model.
> </details>


![](https://ai-paper-reviewer.com/2410.13863/charts_15_0.png)

> ðŸ”¼ The chart displays the scaling behavior of validation loss across different model sizes, showing a consistent linear relationship in log space.
> <details>
> <summary>read the caption</summary>
> Figure 5: Validation loss scales as a power-law with model size. The validation loss is evaluated on 30K images randomly sampled from the MS-COCO 2014 training set. The x and y axes are in log-scale. The change in y is relatively small for each plot, making the log-scale alike linear-scale.
> </details>


![](https://ai-paper-reviewer.com/2410.13863/charts_15_1.png)

> ðŸ”¼ The chart shows that validation loss consistently scales with model size across four autoregressive image generation model variants, exhibiting a linear relationship in log space.
> <details>
> <summary>read the caption</summary>
> Figure 5: Validation loss scales as a power-law with model size. The validation loss is evaluated on 30K images randomly sampled from the MS-COCO 2014 training set. The x and y axes are in log-scale. The change in y is relatively small for each plot, making the log-scale alike linear-scale.
> </details>


![](https://ai-paper-reviewer.com/2410.13863/charts_15_2.png)

> ðŸ”¼ The chart displays the scaling behavior of validation loss across four different autoregressive image generation models with varying model sizes.
> <details>
> <summary>read the caption</summary>
> Figure 5: Validation loss scales as a power-law with model size. The validation loss is evaluated on 30K images randomly sampled from the MS-COCO 2014 training set. The x and y axes are in log-scale. The change in y is relatively small for each plot, making the log-scale alike linear-scale.
> </details>


![](https://ai-paper-reviewer.com/2410.13863/charts_16_0.png)

> ðŸ”¼ The chart visualizes the scaling behavior of four autoregressive image generation model variants across different metrics (single object, two objects, counting, colors, position, color attribution) as model size increases.
> <details>
> <summary>read the caption</summary>
> Figure 13: Validation loss and FID w.r.t. training FLOPs for raster-order models with discrete tokens.
> </details>


</details>



<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<br><table id='7' style='font-size:14px'><tr><td>166M</td><td>12</td><td>768</td><td>12</td><td>0.047</td></tr><tr><td>369M</td><td>16</td><td>1024</td><td>16</td><td>0.078</td></tr><tr><td>665M</td><td>20</td><td>1280</td><td>16</td><td>0.110</td></tr><tr><td>1.1B</td><td>24</td><td>1536</td><td>16</td><td>0.180</td></tr><tr><td>3.1B</td><td>32</td><td>2304</td><td>24</td><td>0.483</td></tr><tr><td>10.5B</td><td>34</td><td>4096</td><td>64</td><td>1.571</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ Table 1 compares the performance of Fluid against other state-of-the-art text-to-image generation models using FID-30K and GenEval metrics.
> <details>
> <summary>read the caption</summary>
> Table 1: System-level comparison. Fluid achieves leading results on both MS-COCO zero-shot FID-30K and GenEval benchmark (Ghosh et al., 2024). â€ : CM3Leon result is reported without retrieval.
> </details>

{{< table-caption >}}
<br><table id='3' style='font-size:18px'><tr><td>#layers</td><td>FID</td></tr><tr><td>0</td><td>9.38</td></tr><tr><td>3</td><td>8.61</td></tr><tr><td>6</td><td>8.42</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ The table shows the results of an ablation study on the number of layers in the text aligner, showing that increasing the number of layers improves the FID score.
> <details>
> <summary>read the caption</summary>
> Table 3: Ablation on the number of layers in the text aligner.
> </details>

{{< table-caption >}}
<table id='22' style='font-size:16px'><tr><td>Model variants</td><td>w</td><td>T</td></tr><tr><td>random order, continuous token</td><td>5</td><td>0.975</td></tr><tr><td>raster order, continuous token</td><td>4.5</td><td>0.975</td></tr><tr><td>random order, discrete token</td><td>1.6</td><td>1.05</td></tr><tr><td>raster order, discrete token</td><td>2.5</td><td>0.95</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ Table 1 compares the performance of Fluid with other leading text-to-image generation models on MS-COCO zero-shot FID-30K and the GenEval benchmark.
> <details>
> <summary>read the caption</summary>
> Table 1: System-level comparison. Fluid achieves leading results on both MS-COCO zero-shot FID-30K and GenEval benchmark (Ghosh et al., 2024). â€ : CM3Leon result is reported without retrieval.
> </details>

</details>


### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2410.13863/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/21.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13863/22.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}