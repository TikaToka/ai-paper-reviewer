{"references": [{"fullname_first_author": "Achiam, J.", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report describing GPT-4, a large language model which is used as a benchmark model in the experiments."}, {"fullname_first_author": "Chen, M.", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-03", "reason": "This paper introduces HumanEval, a benchmark dataset for evaluating LLMs' code generation capabilities, which is used as the basis for creating the HE+Fix dataset in the current paper."}, {"fullname_first_author": "Austin, J.", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-07", "reason": "This paper presents MBPP, another benchmark dataset for evaluating LLMs, used to create the MBPP+Fix dataset in the current work."}, {"fullname_first_author": "Liu, J.", "paper_title": "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation", "publication_date": "2024-00-00", "reason": "This paper provides a more rigorous evaluation of LLMs' code generation capabilities and introduces extended evaluation suites for HumanEval and MBPP, which are used in the current paper's experiments."}, {"fullname_first_author": "Wang, X.", "paper_title": "Self-consistency improves chain of thought reasoning in language models", "publication_date": "2022-00-00", "reason": "This paper introduces the self-consistency method for improving the reliability of LLM outputs, a technique used in the current paper's UTDEBUG pipeline."}]}