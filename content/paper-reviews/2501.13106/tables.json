[{"content": "| Task | Dataset | Amount |\n|---|---|---|\n| Scene Image | VL3-Syn7M-short, LLaVA-Pretrain-558k [liu2023improvedllava](https://arxiv.org/html/2501.13106/liu2023improvedllava.png), Objects365-Recap [Objects365](https://arxiv.org/html/2501.13106/Objects365.png), SA-1B-Recap [kirillov2023segment](https://arxiv.org/html/2501.13106/kirillov2023segment.png) | 11.84M |\n| Scene Text Image | BLIP3-OCR-Recap [Xue2024xGenMMA](https://arxiv.org/html/2501.13106/Xue2024xGenMMA.png) | 0.93M |\n| Document | pdfa-eng-wds [pdfa](https://arxiv.org/html/2501.13106/pdfa.png), idl-wds [idlwds](https://arxiv.org/html/2501.13106/idlwds.png) | 2.80M |", "caption": "Table 1: Data mixture in vision encoder adaptation stage.", "description": "\ud45c 1\uc740 Vision Encoder Adaptation \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc758 \uc885\ub958\uc640 \uc591\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Scene Image, Scene Text Image, Document \uc138 \uac00\uc9c0 \uc720\ud615\uc758 \uc774\ubbf8\uc9c0 \ub370\uc774\ud130\uac00 \uc0ac\uc6a9\ub418\uc5c8\uc73c\uba70, \uac01 \uc720\ud615\ubcc4\ub85c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uacfc \ub370\uc774\ud130 \uc218\ub7c9\uc774 \uc790\uc138\ud788 \uba85\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 VideoLLaMA3 \ubaa8\ub378 \ud559\uc2b5\uc758 \uccab \ubc88\uc9f8 \ub2e8\uacc4\uc778 Vision Encoder Adaptation \ub2e8\uacc4\uc5d0\uc11c \uc5b4\ub5a4 \uc885\ub958\uc758 \ub370\uc774\ud130\ub97c \uc5bc\ub9c8\ub098 \uc0ac\uc6a9\ud588\ub294\uc9c0 \ubcf4\uc5ec\uc90c\uc73c\ub85c\uc368 \ubaa8\ub378 \ud559\uc2b5 \uacfc\uc815\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ub3d5\uc2b5\ub2c8\ub2e4.", "section": "3.2.1 Vision Encoder Adaptation"}, {"content": "| Task | Dataset | Amount |\n|---|---|---|\n| **Scene Image** | VL3-Syn7M-detailed, Objects365-Recap [Objects365, ], SA-1B-Recap [kirillov2023segment, ], COCO2017-Recap [lin2014microsoft, ], ShareGPT4o [Chen2024HowFA, ], TextCaps [sidorov2020textcaps, ], ShareGPT4V [chen2023sharegpt4v, ], DenseFusion [li2024densefusion, ], LLaVA-ReCap (LCS-558K) [li2024llavaonevision, ] | 12.56M |\n| **Scene Text Image** | Laion-OCR [schuhmann2022laion, ], COCO-Text [veit2016coco, ], TextOCR [singh2021textocr, ], BLIP3-OCR-Recap [Xue2024xGenMMA, ], LSVT [Sun2019ICDAR2C, ], ReCTS [liu2019icdar, ] | 4.69M |\n| **Document** | SynthDoG-EN [kim2022ocr, ], SynthDoG-ZH [kim2022ocr, ], UReader-TR [Ye2023UReaderUO, ], FUNSD [funsd, ], DUDE [van2023icdar, ], Vary-600k [wei2023vary, ], pdfa-eng-wds [pdfa, ], idl-wds [idlwds, ] | 2.68M |\n| **Chart** | Chart-to-Text [2022Chart, ] | 0.04M |\n| **Fine-grained** | Osprey-724K [yuan2024osprey, ], MDVP-Data [lin2024draw, ], ADE20K-Recap [zhou2019semantic, ], Object365 [Objects365, ], Flickr-30K [young2014image, ], GranD [rasheed2024glamm, ] | 1.00M |\n| **Text-only** | Evol-Instruct-143K [chen2024allava, ], Infinity-Instruct-code [InfinityInstruct2024, ], Infinity-Instruct-commonsense [InfinityInstruct2024 ], Infinity-Instruct-math [InfinityInstruct2024, ] | 6.25M |", "caption": "Table 2: Data mixture in vision-language alignment stage.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 Vision-Language Alignment \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130 \ubbf9\uc2a4\uccd0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc885\ub958 (Scene Image, Scene Text Image, Document, Chart, Fine-grained, Text-only) \uc640 \ud574\ub2f9 \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30 (Amount) \ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc720\ud615\uc758 \uc774\ubbf8\uc9c0 \ubc0f \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \ub2e4\uc911 \ubaa8\ub4dc \uc774\ud574 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b \uad6c\uc131\uc744 \uc790\uc138\ud788 \uc124\uba85\ud569\ub2c8\ub2e4.", "section": "3.2.2 Vision-Language Alignment"}, {"content": "| Task | Dataset | Amount |\n|---|---|---|\n| **Image & Text Data** |  |  |\n| General | LLaVA-SFT-665K [li2024llava], LLaVA-OV-SI [li2024llavaonevision], Cambrian-cleaned [tong2024cambrian], Pixmo (docs, cap, points, cap-qa, ask-model-anything) [molmo2024] | 9.87M |\n| Document | DocVQA [mathew2021docvqadatasetvqadocument], Docmatix [lauren\u00e7on2024building] | 1.31M |\n| Chart/Figure | ChartQA [masry2022chartqa], MMC_Instruction [liu2023mmc], DVQA [kafle2018dvqa], LRV_Instruction [liu2023aligning], ChartGemma [masry2024chartgemmavisualinstructiontuningchart], InfoVQA [mathew2022infographicvqa], PlotQA [methani2020plotqa] | 1.00M |\n| OCR | MultiUI [liu2024harnessingwebpageuistextrich], in-house data | 0.83M |\n| Grounding | RefCoco [kazemzadeh2014referitgame], VCR [zellers2019vcr], in-house data | 0.50M |\n| Multi-Image | Demon-Full [li2024fine], Contrastive_Caption [jiang2024mantisinterleavedmultiimageinstruction] | 0.41M |\n| Text-only | Magpie [xu2024magpie], Magpie-Pro [xu2024magpie], Synthia [Synthia-70B-v1.2], Infinity-Instruct-subjective [InfinityInstruct2024], NuminaMath [li2024numinamath] | 2.21M |\n| **Video Data** |  |  |\n| General | LLaVA-Video-178K [zhang2024video], ShareGPT4o-Video [chen2024sharegpt4video], FineVideo [Farr\u00e92024FineVideo], CinePile [rawal2024cinepile], ShareGemini-k400 [sharegemini], ShareGemini-WebVID [sharegemini], VCG-Human [Maaz2024VideoGPT+], VCG-Plus [Maaz2024VideoGPT+], VideoLLaMA2 in-house data, Temporal Grounding in-house data | 2.92M |", "caption": "Table 3: Data mixture in massive multi-task fine-tuning stage.", "description": "\ud45c 3\uc740 VideoLLaMA3 \ubaa8\ub378\uc758 \ub2e4\uc911 \uc791\uc5c5 \ubbf8\uc138 \uc870\uc815 \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130 \ubbf9\uc2a4\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5(\uc77c\ubc18 \uc774\ubbf8\uc9c0 \ubc0f \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130, \ubb38\uc11c, \ucc28\ud2b8/\uadf8\ub9bc, OCR, \uadf8\ub77c\uc6b4\ub529, \ub2e4\uc911 \uc774\ubbf8\uc9c0, \ud14d\uc2a4\ud2b8 \uc804\uc6a9)\uc5d0 \ub300\ud55c \ub370\uc774\ud130\uc14b\uacfc \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc591\uc744 \uc790\uc138\ud788 \uc124\uba85\ud569\ub2c8\ub2e4.  \uac01 \ud558\uc704 \uc791\uc5c5\uc740 \uc2dc\uac01\uc801 \uc774\ud574\uc758 \ud2b9\uc815 \uce21\uba74\uc744 \ubaa9\ud45c\ub85c \ud558\uc5ec \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \uc2dc\uac01\uc801 \uc815\ubcf4\ub97c \ucc98\ub9ac\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.  \uc5ec\uae30\uc5d0\ub294 \uc2dc\uac01\uc801 \ub370\uc774\ud130\ubfd0 \uc544\ub2c8\ub77c \ubaa8\ub378\uc758 \uc5b8\uc5b4\uc801 \uc774\ud574 \ub2a5\ub825\uc744 \uac15\ud654\ud558\uae30 \uc704\ud55c \uc0c1\ub2f9\ub7c9\uc758 \ud14d\uc2a4\ud2b8 \uc804\uc6a9 \ub370\uc774\ud130\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.2.3 \ub2e4\uc911 \uc791\uc5c5 \ubbf8\uc138 \uc870\uc815"}, {"content": "| Task | Dataset | Amount |\n|---|---|---|\n| General Video | LLaVA-Video-178K [zhang2024video], ShareGPT4o-Video [chen2024sharegpt4video], FineVideo [Farr\u00e92024FineVideo], CinePile [rawal2024cinepile], ShareGemini-k400 [sharegemini], ShareGemini-WebVID [sharegemini], VCG-Human [Maaz2024VideoGPT+], VCG-Plus [Maaz2024VideoGPT+], VideoRefer [yuan2024videorefer], VideoLLaMA2 in-house data, In-house synthetic data | 3.03M |\n| Streaming Video | ActivityNet [krishna2017dense], YouCook2 [zhou2018towards], Ego4D-narration [grauman2022ego4d], Ego4D-livechat [chen2024videollm] | 36.2K |\n| Temporal Grounding | ActivityNet [krishna2017dense], YouCook2 [zhou2018towards], ViTT [huang2020multimodal], QuerYD [oncescu2021queryd], HiREST [zala2023hierarchical], Charades-STA [gao2017tall], Moment-10M [qian2024momentor], COIN [tang2019coin] | 0.21M |\n| Image-only | LLaVA-SFT-665K [li2024llava], LLaVA-OV-SI [li2024llavaonevision] | 0.88M |\n| Text-only | Magpie [xu2024magpie], Tulu 3 [lambert2024tulu3] | 1.56M |", "caption": "Table 4: Data mixture in video-centric fine-tuning stage.", "description": "\ud45c 4\ub294 VideoLLaMA3\uc758 \ube44\ub514\uc624 \uc911\uc2ec \ubbf8\uc138 \uc870\uc815 \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130 \ubbf9\uc2a4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ube44\ub514\uc624 \ub370\uc774\ud130 (\uc77c\ubc18 \ube44\ub514\uc624, \uc2a4\ud2b8\ub9ac\ubc0d \ube44\ub514\uc624, \uc2dc\uac04\uc801 \uadfc\uac70 \ub370\uc774\ud130, \uc774\ubbf8\uc9c0 \uc804\uc6a9 \ub370\uc774\ud130, \ud14d\uc2a4\ud2b8 \uc804\uc6a9 \ub370\uc774\ud130)\uc640 \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc591\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130 \uc720\ud615\uc740 \ud2b9\uc815 \ube44\ub514\uc624 \uc774\ud574 \uce21\uba74\uc744 \uac15\ud654\ud558\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uc77c\ubc18 \ube44\ub514\uc624 \ub370\uc774\ud130\ub294 \ub2e4\uc591\ud55c \ube44\ub514\uc624 \uc2dc\ub098\ub9ac\uc624\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \uc774\ud574\ub3c4\ub97c \ub192\uc774\uace0, \uc2a4\ud2b8\ub9ac\ubc0d \ube44\ub514\uc624 \ub370\uc774\ud130\ub294 \uc2e4\uc2dc\uac04 \ube44\ub514\uc624 \ucc98\ub9ac \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \uae30\uc5ec\ud558\uba70, \uc2dc\uac04\uc801 \uadfc\uac70 \ub370\uc774\ud130\ub294 \ube44\ub514\uc624 \ud504\ub808\uc784 \uac04\uc758 \uad00\uacc4 \ud30c\uc545 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.  \uacb0\ub860\uc801\uc73c\ub85c, \uc774 \ud45c\ub294 VideoLLaMA3\uc758 \ube44\ub514\uc624 \uc774\ud574 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uae30 \uc704\ud574 \ub2e4\uc591\ud558\uace0 \ud48d\ubd80\ud55c \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud588\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.2.4 Video-centric Fine-tuning"}, {"content": "| Benchmark | Model (2B) | SmoltVLM | InternVL 2.5 | Qwen2-VL | VideoLLaMA-3 |\n|---|---|---|---|---|---| \n| **Benchmark** | **Model** |  |  |  |  |\n|---|---|---|---|---|---| \n|  | **SmolVLM** <br>  ![https://arxiv.org/html/2501.13106/figures/icons/huggingface.png](https://arxiv.org/html/2501.13106/figures/icons/huggingface.png) | 65.3* | 79.2 | 73.5 | 79.8 |\n|  | **2B** |  |  |  |  |\n|---|---|---|---|---|---| \n|  | **InternVL 2.5** <br> ![https://arxiv.org/html/2501.13106/figures/icons/opengvlab.jpeg](https://arxiv.org/html/2501.13106/figures/icons/opengvlab.jpeg) | 81.6 | 88.7 | 90.1 | 91.9 |\n|  | **2B** |  |  |  |  |\n|---|---|---|---|---|---| \n|  | **Qwen2-VL** <br> ![https://arxiv.org/html/2501.13106/figures/icons/qwen.png](https://arxiv.org/html/2501.13106/figures/icons/qwen.png) | - | 60.9 | 65.5 | 69.4 |\n|  | **2B** |  |  |  |  |\n|---|---|---|---|---|---| \n|  | **VideoLLaMA-3** | 622* | 804 | 767* | 779 |\n|---|---|---|---|---|---| \n| *Document/Chart/Scene Text Understanding* |  |  |  |  |  |\n|---|---|---|---|---|---| \n| ChartQA | 65.3* | 79.2 | 73.5 | 79.8 |\n| DocVQA<sub>test</sub> | 81.6 | 88.7 | 90.1 | 91.9 |\n| InfoVQA<sub>test</sub> | - | 60.9 | 65.5 | 69.4 |\n| OCRBench | 622* | 804 | 767* | 779 |\n| *Math* |  |  |  |  |  |\n|---|---|---|---|---|---| \n| MathVista<sub>testmini</sub> | 44.6 | 51.3 | 43.0 | 59.2 |\n| MathVision<sub>test</sub> | 6.5* | 14.7 | 12.4 | 15.5 |\n| *Multi Image* |  |  |  |  |  |\n|---|---|---|---|---|---| \n| MMMU-Pro | 17.1* | 23.7 | 26.0 | 28.6 |\n| MMMU<sub>val</sub> | 38.8 | 43.6 | 41.1 | 45.3 |\n| BLINK<sub>test</sub> | 42.3* | 44.0 | 43.1* | 44.2 |\n| *Knowledge/General QA* |  |  |  |  |  |\n|---|---|---|---|---|---| \n| RealWorldQA | 48.8* | 60.1 | 62.9 | 67.3 |\n| AI2D | 62.1* | 74.9 | 69.9 | 78.2 |\n| GQA | 49.2* | 59.5* | 59.8* | 62.7 |\n| MME | 1600* | 2005* | 1872 | 1901 |", "caption": "Table 5: Evaluation results of 2B models on image benchmarks. \u2217 denotes the reproduced results. The best results are in bold and the second best ones are underlined.", "description": "\ud45c 5\ub294 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c 20\uc5b5 \ub9e4\uac1c\ubcc0\uc218 \ubaa8\ub378\uc758 \uc774\ubbf8\uc9c0 \ubca4\uce58\ub9c8\ud06c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0 \uc774\ud574 \uc791\uc5c5(\ubb38\uc11c/\ucc28\ud2b8/\uc7a5\uba74 \ud14d\uc2a4\ud2b8 \uc774\ud574, \uc218\ud559\uc801 \ucd94\ub860, \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc774\ud574, \uc77c\ubc18\uc801\uc778 \uc9c0\uc2dd QA)\uc5d0 \ub300\ud55c \uc5ec\ub7ec \uae30\uc900 \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \uc815\ud655\ub3c4(%)\ub85c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  * \ud45c\uc2dc\ub294 \uc7ac\ud604\ub41c \uacb0\uacfc\uc784\uc744 \ub098\ud0c0\ub0b4\uba70, \ucd5c\uace0 \uc131\ub2a5\uc740 \uad75\uac8c, \ub450 \ubc88\uc9f8\ub85c \ub192\uc740 \uc131\ub2a5\uc740 \ubc11\uc904\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 VideoLLaMA3 \ubaa8\ub378\uc758 \uc774\ubbf8\uc9c0 \uc774\ud574 \ub2a5\ub825\uc744 \ub2e4\ub978 \ucd5c\ucca8\ub2e8 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc8fc\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "4.1 \uc774\ubbf8\uc9c0 \uae30\ubc18 \ud3c9\uac00"}, {"content": "| Model | Size | Document/Chart/Scene Text Understanding | Math | Multi Image | Knowledge/General QA |\n|---|---|---|---|---|---| \n| Molmo-7B-D 7B <img src=\"https://arxiv.org/html/2501.13106/figures/icons/ai2.png\" width=\"27\" height=\"9\"> | 84.1 | 92.2 | 51.6 | - | 70.7 | - |\n| InternVL2.5 8B <img src=\"https://arxiv.org/html/2501.13106/figures/icons/opengvlab.jpeg\" width=\"12\" height=\"12\"> | 84.8 | 93.0 | 64.4 | 34.3 | 70.1 | 2344 |\n| LLaVA-OneVision 7B <img src=\"https://arxiv.org/html/2501.13106/figures/icons/bytedance.jpg\" width=\"14\" height=\"12\"> | 80.0 | 87.5 | 63.2 | -24.1\u2020 | 66.3 | 1998 |\n| NVILA 8B <img src=\"https://arxiv.org/html/2501.13106/figures/icons/nvidia.jpg\" width=\"16\" height=\"12\"> | 86.1 | 93.7 | 65.4 | -29.5* | 68.6 | 2219 |\n| Qwen2-VL 7B <img src=\"https://arxiv.org/html/2501.13106/figures/icons/qwen.png\" width=\"11\" height=\"12\"> | 83.0 | 94.5 | 58.2 | -31.4* | 70.1 | 2327 |\n| VideoLLaMA3 7B | 86.3 | 94.9 | 67.1 | 33.6 | 72.7 | 2102 |", "caption": "Table 6: Evaluation results of 7B models on image benchmarks. \u2217 denotes the reproduced results. \u2020 denotes the results retrieved from the official leaderboard. The best results are in bold and the second best ones are underlined.", "description": "\ud45c 6\uc740 \ub17c\ubb38\uc5d0\uc11c \uc5b8\uae09\ub41c 7B \ubaa8\ub378\uc758 \uc774\ubbf8\uc9c0 \ubca4\uce58\ub9c8\ud06c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0 \uc774\ud574 \uc791\uc5c5(\ubb38\uc11c/\ucc28\ud2b8/\uc7a5\uba74 \ud14d\uc2a4\ud2b8 \uc774\ud574, \uc218\ud559\uc801 \ucd94\ub860, \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc774\ud574, \uc77c\ubc18\uc801\uc778 \uc9c0\uc2dd \uc9c8\ubb38 \ub2f5\ubcc0)\uc5d0 \ub300\ud55c \uc5ec\ub7ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \uc815\ub7c9\uc801\uc73c\ub85c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \ucd5c\uace0 \uc131\ub2a5\uc740 \uad75\uac8c \ud45c\uc2dc\ub418\uc5c8\uace0, \ub450 \ubc88\uc9f8\ub85c \ub192\uc740 \uc131\ub2a5\uc740 \ubc11\uc904\uc774 \uadf8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4.  \uc77c\ubd80 \uacb0\uacfc\ub294 \uc5f0\uad6c\uc790\ub4e4\uc774 \uc7ac\ud604\ud55c \uacb0\uacfc(*)\uc774\uace0, \uc77c\ubd80\ub294 \uacf5\uc2dd \ub9ac\ub354\ubcf4\ub4dc(\u2020)\uc5d0\uc11c \uac00\uc838\uc628 \uacb0\uacfc\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0 \uc774\ud574 \ub2a5\ub825\uc744 \uac00\uc9c4 7B \ud30c\ub77c\ubbf8\ud130 \ubaa8\ub378 \uac04\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \ud1b5\ud574 VideoLLaMA3 \ubaa8\ub378\uc758 \uac15\uc810\uc744 \ubcf4\ub2e4 \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc8fc\uace0\uc790 \ud569\ub2c8\ub2e4.", "section": "4.1 \uc774\ubbf8\uc9c0 \uae30\ubc18 \ud3c9\uac00"}, {"content": "| Benchmark | Model | Apollo | InternVL2.5 | Qwen2-VL | VideoLLaMA3 |\n|---|---|---|---|---|---|---|\n| | **2B** | **2B** | **2B** | 2B |\n|---|---|---|---|---|---|---|\n|  | <img src=\"https://arxiv.org/html/2501.13106/figures/icons/meta.png\" width=\"12\" height=\"12\"> **Apollo** | <img src=\"https://arxiv.org/html/2501.13106/figures/icons/opengvlab.jpeg\" width=\"12\" height=\"12\"> **InternVL2.5** | <img src=\"https://arxiv.org/html/2501.13106/figures/icons/qwen.png\" width=\"12\" height=\"12\"> **Qwen2-VL** | **VideoLLaMA3** |\n|---|---|---|---|---|---|---|\n| *General Video Understanding* |  |  |  |  |\n|---|---|---|---|---|---|---|\n| VideoMME _w/o sub_ | 53.0 | 51.9 | 55.6 | **59.6** |\n| VideoMME _w/ sub_ | 54.6 | 54.1 | **60.4** | **63.4** |\n|---|---|---|---|---|---|---|\n| MMVU<sub>val</sub> | - | 33.6<sup>\u2217</sup> | 36.5<sup>\u2020</sup> | **39.9** |\n| MVBench | - | **68.8** | 63.2 | **65.5** |\n| EgoSchema<sub>test</sub> | - | 58.1<sup>\u2217</sup> | 54.9 | **58.5** |\n| PerceptionTest<sub>test</sub> | 61.0 | **66.3**<sup>\u2217</sup> | 53.9 | **68.0** |\n| ActivityNet-QA | - | **54.1**<sup>\u2217</sup> | 53.3<sup>\u2217</sup> | **58.2** |\n|---|---|---|---|---|---|---|\n| *Long Video Understanding* |  |  |  |  |\n|---|---|---|---|---|---|---|\n| MLVU<sub>dev</sub> | **63.3** | 58.9<sup>\u2217</sup> | 62.7<sup>\u2217</sup> | **65.4** |\n| LongVideoBench<sub>val</sub> | - | **52.0** | 48.7<sup>\u2217</sup> | **57.1** |\n| LVBench | - | 37.3<sup>\u2217</sup> | **38.0**<sup>\u2217</sup> | **40.4** |\n|---|---|---|---|---|---|---|\n| *Temporal Reasoning* |  |  |  |  |\n|---|---|---|---|---|---|---|\n| TempCompass | 60.8 | 57.7<sup>\u2217</sup> | **62.2**<sup>\u2217</sup> | **63.4** |\n| NextQA | - | 75.6<sup>\u2217</sup> | **77.2**<sup>\u2217</sup> | **81.1** |\n| Charades-STA | - | - | - | **55.5** |", "caption": "Table 7: Evaluation results of 2B models on video benchmarks. * denotes the reproduced results. \u2020 denotes the results retrieved from the official leaderboard. The best results are in bold and the second best ones are underlined.", "description": "\ud45c 7\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c VideoLLaMA3 \ubaa8\ub378\uc758 \ube44\ub514\uc624 \uc774\ud574 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \uc218\ud589\ub41c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ube44\ub514\uc624 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c VideoLLaMA3(2B \ubaa8\ub378)\uc758 \uc131\ub2a5\uc744 \uae30\uc874 \ucd5c\ucca8\ub2e8 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50 \ubd84\uc11d\ud55c \uac83\uc785\ub2c8\ub2e4.  * \ud45c\uc2dc\ub294 \uc7ac\ud604\ub41c \uacb0\uacfc\uc774\uba70 \u2020 \ud45c\uc2dc\ub294 \uacf5\uc2dd \ub9ac\ub354\ubcf4\ub4dc\uc5d0\uc11c \uac00\uc838\uc628 \uacb0\uacfc\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud45c\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uace0, \ub450 \ubc88\uc9f8\ub85c \ub192\uc740 \uc810\uc218\ub294 \ubc11\uc904\uc774 \uadf8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uc77c\ubc18\uc801\uc778 \ube44\ub514\uc624 \uc774\ud574, \uc7a5\uae30 \ube44\ub514\uc624 \uc774\ud574, \uadf8\ub9ac\uace0 \uc2dc\uac04\uc801 \ucd94\ub860 \uc138 \uac00\uc9c0 \uc8fc\uc694 \ucc28\uc6d0\uc5d0 \uac78\uce5c \uc131\ub2a5\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ucc28\uc6d0\uc740 \uc5ec\ub7ec \uac1c\uc758 \ud558\uc704 \ubca4\uce58\ub9c8\ud06c\ub85c \ub098\ub204\uc5b4\uc838 \uc788\uc73c\uba70, VideoLLaMA3\uac00 \uac01 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub2ec\uc131\ud55c \uc131\ub2a5 \uc218\uce58\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.1 \ube44\ub514\uc624 \uae30\ubc18 \ud3c9\uac00"}, {"content": "| Model | 7B | 8B | 7B | 8B | 7B | 2.1-7B | 3-7B |\n|---|---|---|---|---|---|---|---| \n| Qwen2-VL [https://arxiv.org/html/2501.13106/figures/icons/qwen.png](https://arxiv.org/html/2501.13106/figures/icons/qwen.png) |  |  |  |  |  |  |  |\n| InternVL2.5 [https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/opengvlab.jpeg](https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/opengvlab.jpeg) |  |  |  |  |  |  |  |\n| LLaVA-Video [https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/bytedance.jpg](https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/bytedance.jpg) |  |  |  |  |  |  |  |\n| NVILA [https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/nvidia.jpg](https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/nvidia.jpg) |  |  |  |  |  |  |  |\n| Apollo [https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/meta.png](https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/meta.png) |  |  |  |  |  |  |  |\n| VideoLLaMA |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---| \n| *General Video Understanding* |  |  |  |  |  |  |  |\n| VideoMME _w/o sub_ | 63.3 | 64.2 | 63.3 | 64.2 | 61.3 | 54.9 | **66.2** |\n| VideoMME _w/ sub_ | 69.0 | 66.9 | 69.7 | **70.0** | 63.3 | 56.4 | **70.3** |\n| MMVU<sub>val</sub> | -42.1<sup>\u2020</sup> | -41.1<sup>\u2020</sup> | -42.4<sup>\u2217</sup> | 43.7<sup>\u2217</sup> | - | -39.5<sup>\u2020</sup> | **44.1** |\n| MVBench | 67.0 | **72.0** | 58.6 | 68.1 | - | 57.3 | **69.7** |\n| EgoSchema<sub>test</sub> | **66.7** | 66.2<sup>\u2217</sup> | 57.3 | 54.3<sup>\u2217</sup> | - | 53.1 | 63.3 |\n| PerceptionTest<sub>test</sub> | 62.3 | 68.9<sup>\u2217</sup> | **67.9**<sup>\u2217</sup> | 65.4<sup>\u2217</sup> | - | 54.9 | **72.8** |\n| ActivityNet-QA | 57.4<sup>\u2217</sup> | 58.9<sup>\u2217</sup> | 56.5 | **60.9** | - | 53.0 | **61.3** |\n|---|---|---|---|---|---|---|---| \n| *Long Video Understanding* |  |  |  |  |  |  |  |\n| MLVU<sub>dev</sub> | 69.8<sup>\u2217</sup> | 69.0<sup>\u2217</sup> | 70.8<sup>\u2217</sup> | 70.6<sup>\u2217</sup> | **70.9** | 57.4 | **73.0** |\n| LongVideoBench<sub>val</sub> | 55.6<sup>\u2020</sup> | **60.0** | 58.2 | 57.7 | 58.5 | - | **59.8** |\n| LVBench | 44.2<sup>\u2217</sup> | 41.5<sup>\u2217</sup> | 40.3<sup>\u2217</sup> | 42.6<sup>\u2217</sup> | - | 36.3 | **43.7** |\n|---|---|---|---|---|---|---|---| \n| *Temporal Reasoning* |  |  |  |  |  |  |  |\n| TempCompass | 67.9<sup>\u2020</sup> | **68.3**<sup>\u2217</sup> | 65.4 | **69.7**<sup>\u2217</sup> | 64.9 | 56.8 | 68.1 |\n| NextQA | 81.2<sup>\u2217</sup> | **85.0**<sup>\u2217</sup> | 83.2 | 82.2 | - | 75.6 | **84.5** |\n| Charades-STA | - | - | - | - | - | - | **60.7** |", "caption": "Table 8: Evaluation results of 7B models on video benchmarks. * denotes the reproduced results. \u2020 denotes the results retrieved from the official leaderboard. The best results are in bold and the second best ones are underlined.", "description": "\ud45c 8\uc740 \ub17c\ubb38\uc5d0\uc11c \ub2e4\ub8e8\ub294 7B \ubaa8\ub378\uc758 \ube44\ub514\uc624 \ubca4\uce58\ub9c8\ud06c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uc5ec\ub7ec \ube44\ub514\uc624 \uc774\ud574 \ubca4\uce58\ub9c8\ud06c(\uc77c\ubc18\uc801\uc778 \ube44\ub514\uc624 \uc774\ud574, \uc7a5\uae30 \ube44\ub514\uc624 \uc774\ud574, \uc2dc\uac04\uc801 \ucd94\ub860)\uc5d0\uc11c \ub2e4\uc591\ud55c 7B \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\uac00 \uc218\uce58\ub85c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  * \ud45c\uc2dc\ub294 \uc7ac\ud604\ub41c \uacb0\uacfc\ub97c, \u2020 \ud45c\uc2dc\ub294 \uacf5\uc2dd \ub9ac\ub354\ubcf4\ub4dc\uc5d0\uc11c \uac00\uc838\uc628 \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uac00\uc7a5 \uc88b\uc740 \uacb0\uacfc\ub294 \uad75\uc740 \uae00\uc528\uccb4\ub85c, \ub450 \ubc88\uc9f8\ub85c \uc88b\uc740 \uacb0\uacfc\ub294 \ubc11\uc904\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ubca4\uce58\ub9c8\ud06c\ub294 \ubaa8\ub378\uc758 \ube44\ub514\uc624 \uc774\ud574 \ub2a5\ub825\uc758 \ud2b9\uc815 \uce21\uba74(\uc608: \uc7a5\uae30 \ube44\ub514\uc624 \ucc98\ub9ac, \uc2dc\uac04\uc801 \uad00\uacc4 \ud30c\uc545, \uc77c\ubc18\uc801\uc778 \ube44\ub514\uc624 \uc9c8\ubb38 \uc751\ub2f5)\uc744 \ud3c9\uac00\ud558\ub3c4\ub85d \uace0\uc548\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 VideoLLaMA3 \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \ube44\ub514\uc624 \uc774\ud574 \uc791\uc5c5\uc5d0\uc11c \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90c\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc8fc\uc694 \uadfc\uac70 \uc911 \ud558\ub098\uc785\ub2c8\ub2e4.", "section": "4.2 \ube44\ub514\uc624 \uae30\ubc18 \ud3c9\uac00"}, {"content": "| Model | GQA | AI2D | ChartQA | DocVQA<sub>val</sub> | MME |\n|---|---|---|---|---|---| \n| clip-vit-large-patch14-336 [radford2021learning] | 61.50 | 56.28 | 18.32 | 24.86 | 1668.41 |\n| dfn5B-clip-vit-h-14-378 [fang2023data] | 62.70 | 56.87 | 16.40 | 23.09 | 1665.35 |\n| siglip-so400m-patch14-384 [Zhai2023SigmoidLF] | 62.92 | 57.12 | 22.44 | 31.32 | 1667.92 |", "caption": "Table 9: Ablation Study on Vision Encoders.", "description": "\ubcf8 \ud45c\ub294 \ube44\uc804 \uc778\ucf54\ub354\uc5d0 \ub300\ud55c ablation study \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc138 \uac00\uc9c0 \uc0ac\uc804 \ud6c8\ub828\ub41c \ube44\uc804 \uc778\ucf54\ub354 (CLIP, DFN, SigLIP)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uace0, \uac01 \uc778\ucf54\ub354\uc758 \uc7a5\ub2e8\uc810 \ubc0f \uc801\ud569\ud55c \uc0ac\uc6a9 \uc2dc\ub098\ub9ac\uc624\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4. \ud2b9\ud788, SigLIP \uc778\ucf54\ub354\uac00 \ud14d\uc2a4\ud2b8\ub97c \ud3ec\ud568\ud558\ub294 \uc138\ubd80\uc801\uc778 \uc2dc\uac01\uc801 \uc774\ud574 \uc791\uc5c5\uc5d0\uc11c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ud655\uc778\ud558\uace0, \ubcf8 \ub17c\ubb38\uc5d0\uc11c SigLIP \uc778\ucf54\ub354\ub97c \uae30\ubcf8 \ube44\uc804 \uc778\ucf54\ub354\ub85c \ucc44\ud0dd\ud55c \uc774\uc720\ub97c \uc124\uba85\ud569\ub2c8\ub2e4.", "section": "4.4 Ablation Study"}]