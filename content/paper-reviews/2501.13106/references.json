{"references": [{"fullname_first_author": "Bo Li", "paper_title": "LLaVA-OneVision: Easy visual task transfer", "publication_date": "2024-08-08", "reason": "This paper introduces the LLaVA-OneVision model, which serves as a foundational model for VideoLLaMA3, and its vision-centric training paradigm is a key component of VideoLLaMA3's design."}, {"fullname_first_author": "Zesen Cheng", "paper_title": "VideoLLaMA 2: Advancing spatial-temporal modeling and audio understanding in video-LLMs", "publication_date": "2024-06-07", "reason": "VideoLLaMA2 is a direct predecessor to VideoLLaMA3, and many of its core design principles and training techniques are carried over to VideoLLaMA3."}, {"fullname_first_author": "Zhe Chen", "paper_title": "InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "publication_date": "2023-12-14", "reason": "InternVL's vision encoder and training techniques are adapted and incorporated into VideoLLaMA3, providing the foundation for its image and video understanding capabilities."}, {"fullname_first_author": "Yuanhan Zhang", "paper_title": "Video instruction tuning with synthetic data", "publication_date": "2024-10-02", "reason": "This paper introduces a novel video instruction tuning approach and dataset that's used in the VideoLLaMA3 training pipeline, improving its video understanding capabilities significantly."}, {"fullname_first_author": "Peng Wang", "paper_title": "Qwen2-VL: Enhancing vision-language model's perception of the world at any resolution", "publication_date": "2024-09-12", "reason": "Qwen2-VL's large language model architecture and multimodal capabilities are leveraged in VideoLLaMA3, which heavily relies on its strong language processing abilities for instruction following and response generation."}]}