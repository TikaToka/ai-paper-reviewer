---
title: "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding"
summary: "VideoLLaMA3: ê³ í’ˆì§ˆ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„° ê¸°ë°˜ ë¹„ì „ ì¤‘ì‹¬ í›ˆë ¨ìœ¼ë¡œ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì´í•´ ì„±ëŠ¥ í˜ì‹ !"
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Multimodal Learning", "Vision-Language Models", "ğŸ¢ DAMO Academy, Alibaba Group",]
showSummary: true
date: 2025-01-22
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2501.13106 {{< /keyword >}}
{{< keyword icon="writer" >}} Boqiang Zhang et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2025-01-23 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2501.13106" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2501.13106" target="_self" >}}
â†— Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/videollama-3-frontier-multimodal-foundation" target="_self" >}}
â†— Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

ìµœê·¼ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë°œì „ìœ¼ë¡œ ë©€í‹°ëª¨ë‹¬ í•™ìŠµì´ ì£¼ëª©ë°›ê³  ìˆì§€ë§Œ, **ê³ í’ˆì§ˆì˜ ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ë°ì´í„° í™•ë³´ì˜ ì–´ë ¤ì›€**ê³¼ **ë¹„ë””ì˜¤ ë°ì´í„°ì˜ ë³µì¡ì„±**ìœ¼ë¡œ ì¸í•´ ì˜ìƒ ì´í•´ ë¶„ì•¼ì˜ ë°œì „ì€ ë”ë ìŠµë‹ˆë‹¤.  ê¸°ì¡´ì˜ ë©€í‹°ëª¨ë‹¬ LLMì€ ë¹„ë””ì˜¤ ë°ì´í„°ì˜ ì‹œê°„ì  ë³µì¡ì„±ì„ ì²˜ë¦¬í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì—ˆê³ , ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ë‚®ì€ í’ˆì§ˆë¡œ ì¸í•´ ì„±ëŠ¥ í–¥ìƒì— ì œì•½ì´ ìˆì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ê¸°ì¡´ ëª¨ë¸ë“¤ì€ ê³ ì •ëœ í¬ê¸°ì˜ ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ ë‹¤ì–‘í•œ í•´ìƒë„ì˜ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **ë¹„ì „ ì¤‘ì‹¬ì  ì ‘ê·¼ ë°©ì‹**ì„ ì±„íƒí•œ VideoLLaMA3 ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ **ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹**ì„ êµ¬ì¶•í•˜ê³ , **ë³€í™”í•˜ëŠ” í•´ìƒë„ì˜ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤**ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ì„ ê°œë°œí•˜ì—¬ ê¸°ì¡´ì˜ í•œê³„ë¥¼ ê·¹ë³µí–ˆìŠµë‹ˆë‹¤. ë˜í•œ, VideoLLaMA3ëŠ” ë‹¤ì–‘í•œ í•˜ìœ„ ì‘ì—…(ì˜ˆ: ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì§ˆë¬¸ ë‹µë³€, ë¹„ë””ì˜¤ ìº¡ì…˜ ìƒì„±)ì— ëŒ€í•œ **ë©€í‹° íƒœìŠ¤í¬ ë¯¸ì„¸ ì¡°ì •**ì„ í†µí•´ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ì´í•´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.  ì‹¤í—˜ ê²°ê³¼, VideoLLaMA3ëŠ” ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ **ìµœì²¨ë‹¨ ì„±ëŠ¥**ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} ë¹„ì „ ì¤‘ì‹¬ì  í›ˆë ¨ íŒ¨ëŸ¬ë‹¤ì„ ë° í”„ë ˆì„ì›Œí¬ ì„¤ê³„ë¥¼ í†µí•´ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ì´í•´ ì„±ëŠ¥ í–¥ìƒ {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} ë‹¤ì–‘í•œ í•´ìƒë„ì˜ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” Any-resolution Vision Tokenization (AVT) ë° Differential Frame Pruner (DiffFP) ê¸°ìˆ  ì œì•ˆ {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì´í•´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ ë‹¬ì„± {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
ë³¸ ë…¼ë¬¸ì€ **ì˜ìƒ ì´í•´ ë¶„ì•¼ì˜ íšê¸°ì ì¸ ë°œì „**ì„ ì œì‹œí•˜ë©°, ê³ í’ˆì§ˆ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í™œìš©í•œ ë¹„ì „ ì¤‘ì‹¬ì  ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ì´í•´ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ëŠ” **ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ì‘ì—…ì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥**ì„ ë‹¬ì„±í•˜ì—¬, ê´€ë ¨ ì—°êµ¬ ë¶„ì•¼ì— í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. íŠ¹íˆ, **ë¹„ë””ì˜¤ ì´í•´ì— ëŒ€í•œ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹**ì„ ì œì‹œí•˜ê³ , ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì²˜ë¦¬ë¥¼ ìœ„í•œ íš¨ìœ¨ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•¨ìœ¼ë¡œì¨, í–¥í›„ **ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° ë¶„ì„ ê¸°ìˆ ** ë°œì „ì— ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

------
#### Visual Insights



![](https://arxiv.org/html/2501.13106/x1.png)

> ğŸ”¼ ê·¸ë¦¼ 1ì€ VideoLLaMA3ì˜ ì„±ëŠ¥ì„ ì´ì „ì˜ ê³ ê¸‰ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ë‹¤ì¤‘ ëª¨ë“œ ì–¸ì–´ ëª¨ë¸(MLLM)ê³¼ ë‹¤ì–‘í•œ ëŒ€í‘œì ì¸ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë¹„êµí•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. VideoLLaMA3ëŠ” ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ VideoLLaMA3ëŠ” VideoMME, PerceptionTest, MLVUì™€ ê°™ì€ ë¹„ë””ì˜¤ ì´í•´ ëŠ¥ë ¥ ë¿ë§Œ ì•„ë‹ˆë¼ DocVQAì™€ ê°™ì€ ë¬¸ì„œ ì´í•´ ëŠ¥ë ¥ê³¼ MathVistaì™€ ê°™ì€ ë‹¤ì¤‘ ëª¨ë“œ ìˆ˜í•™ì  ì¶”ë¡  ëŠ¥ë ¥ë„ ë›°ì–´ë‚¬ìŠµë‹ˆë‹¤. LLaVA-OneVisionì€ ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ì—ë§Œ ì‚¬ìš©ë˜ì—ˆê³ , LLaVA-VideoëŠ” ë¹„ë””ì˜¤ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ì—ë§Œ ì‚¬ìš©ë˜ì—ˆë‹¤ëŠ” ì ì— ìœ ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 1: Performance Comparison of VideoLLaMA3 with the previous advanced image/video MLLM on various representative benchmarks. As shown in the figure, VideoLLaMA3 has achieved very competitive results on various benchmarks. Specifically, VideoLLaMA3 not only demonstrates strong video understanding capabilitiesÂ (VideoMME, PerceptionTest, MLVU) but also maintains excellent document comprehension abilitiesÂ (DocVQA) and multimodal mathematical reasoning skillsÂ (MathVista). Note that LLaVA-OneVision is only used for evaluating image benchmarks, while LLaVA-Video is only used for evaluating video benchmarks.
> </details>





{{< table-caption >}}
| Task | Dataset | Amount |
|---|---|---|
| Scene Image | VL3-Syn7M-short, LLaVA-Pretrain-558k [liu2023improvedllava](https://arxiv.org/html/2501.13106/liu2023improvedllava.png), Objects365-Recap [Objects365](https://arxiv.org/html/2501.13106/Objects365.png), SA-1B-Recap [kirillov2023segment](https://arxiv.org/html/2501.13106/kirillov2023segment.png) | 11.84M |
| Scene Text Image | BLIP3-OCR-Recap [Xue2024xGenMMA](https://arxiv.org/html/2501.13106/Xue2024xGenMMA.png) | 0.93M |
| Document | pdfa-eng-wds [pdfa](https://arxiv.org/html/2501.13106/pdfa.png), idl-wds [idlwds](https://arxiv.org/html/2501.13106/idlwds.png) | 2.80M |{{< /table-caption >}}

> ğŸ”¼ í‘œ 1ì€ Vision Encoder Adaptation ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„°ì˜ ì¢…ë¥˜ì™€ ì–‘ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  Scene Image, Scene Text Image, Document ì„¸ ê°€ì§€ ìœ í˜•ì˜ ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì‚¬ìš©ë˜ì—ˆìœ¼ë©°, ê° ìœ í˜•ë³„ë¡œ ì‚¬ìš©ëœ ë°ì´í„°ì…‹ê³¼ ë°ì´í„° ìˆ˜ëŸ‰ì´ ìì„¸íˆ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ í‘œëŠ” VideoLLaMA3 ëª¨ë¸ í•™ìŠµì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ì¸ Vision Encoder Adaptation ë‹¨ê³„ì—ì„œ ì–´ë–¤ ì¢…ë¥˜ì˜ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì‚¬ìš©í–ˆëŠ”ì§€ ë³´ì—¬ì¤Œìœ¼ë¡œì¨ ëª¨ë¸ í•™ìŠµ ê³¼ì •ì— ëŒ€í•œ ì´í•´ë¥¼ ë•ìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 1: Data mixture in vision encoder adaptation stage.
> </details>





### In-depth insights


#### Vision-centric Training
ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ ë¹„ì „ ì¤‘ì‹¬ í•™ìŠµì€ **ê³ í’ˆì§ˆ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì´í•´ì— ë§¤ìš° ì¤‘ìš”**í•˜ë‹¤ëŠ” í†µì°°ë ¥ì— ê¸°ë°˜í•©ë‹ˆë‹¤.  ëŒ€ê·œëª¨ ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•˜ëŠ” ëŒ€ì‹  **ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ êµ¬ì¶•**ì— ì§‘ì¤‘í•©ë‹ˆë‹¤. ì´ëŠ” ë¹„ë””ì˜¤ë¥¼ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ë¡œ ê°„ì£¼í•˜ì—¬ **ì´ë¯¸ì§€ ì´í•´ ëŠ¥ë ¥ í–¥ìƒì„ í†µí•´ ë¹„ë””ì˜¤ ì´í•´ ëŠ¥ë ¥ í–¥ìƒ**ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê°€ì •ì— ê·¼ê±°í•©ë‹ˆë‹¤.  ì´ëŸ¬í•œ ì „ëµì€ ë¹„ë””ì˜¤ ë°ì´í„°ë³´ë‹¤ ìˆ˜ì§‘ ë° ì£¼ì„ ì²˜ë¦¬ê°€ ìš©ì´í•œ ì´ë¯¸ì§€ ë°ì´í„°ì˜ ì¥ì ì„ í™œìš©í•©ë‹ˆë‹¤.  **ë¹„ì „ ì¸ì½”ë” ì ì‘, ë¹„ì „-ì–¸ì–´ ì •ë ¬, ë‹¤ì¤‘ ì‘ì—… ë¯¸ì„¸ ì¡°ì •, ë¹„ë””ì˜¤ ì¤‘ì‹¬ ë¯¸ì„¸ ì¡°ì •** ë“± ë„¤ ë‹¨ê³„ì˜ í›ˆë ¨ ê³¼ì •ì„ í†µí•´ ëª¨ë¸ì˜ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì´í•´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.  ê° ë‹¨ê³„ëŠ” íŠ¹ì • ëª©í‘œë¥¼ ë‹¬ì„±í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìœ¼ë©°, **ì´ë¯¸ì§€ ì´í•´ ëŠ¥ë ¥ì„ ë¨¼ì € ê°•í™”í•œ í›„ ë¹„ë””ì˜¤ ì´í•´ ëŠ¥ë ¥ í–¥ìƒ**ì— ì§‘ì¤‘í•˜ëŠ” ìˆœì°¨ì ì¸ í•™ìŠµ ë°©ì‹ì„ ì±„íƒí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¹„ì „ ì¤‘ì‹¬ì  ì ‘ê·¼ ë°©ì‹ì€ ì‹¤í—˜ ê²°ê³¼ë¥¼ í†µí•´ íš¨ê³¼ê°€ ì…ì¦ë˜ì—ˆìœ¼ë©°, ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì´í•´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

#### Multimodal Adaptation
ë‹¤ëª¨ ì•„ì¹´ë°ë¯¸ì—ì„œ ë°œí‘œí•œ ì—°êµ¬ëŠ” **ë¹„ì „ ì¤‘ì‹¬ì  ì ‘ê·¼ ë°©ì‹**ì„ ê°•ì¡°í•˜ë©°, ì˜ìƒ ì´í•´ë¥¼ ìœ„í•œ ê¸°ë°˜ ëª¨ë¸ë¡œì„œ VideoLLaMA3ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.  ì´ëŠ” ë‹¨ìˆœíˆ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ê²°í•©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ê³ í’ˆì§ˆ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°**ë¥¼ í™œìš©í•˜ì—¬ ë¹„ë””ì˜¤ ì´í•´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì „ëµì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  **ì´ë¯¸ì§€ ì´í•´ ëŠ¥ë ¥ í–¥ìƒì„ í†µí•´ ë¹„ë””ì˜¤ ì´í•´ ëŠ¥ë ¥ì„ ê°„ì ‘ì ìœ¼ë¡œ í–¥ìƒ**ì‹œí‚¤ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.  ì¦‰, ëŒ€ìš©ëŸ‰ì˜ ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ êµ¬ì¶•í•˜ëŠ” ëŒ€ì‹ , ê³ í’ˆì§ˆ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•´ ë¹„ì „ ì¸ì½”ë”ë¥¼ ê°•í™”í•˜ê³ , ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ í•´ìƒë„ì˜ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ì…ë ¥ì— ëŒ€í•œ ì ì‘ë ¥ì„ ë†’ì…ë‹ˆë‹¤.  ì´ëŠ” **ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì´ë¯¸ì§€ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬**í•˜ëŠ” ë¹„ì „ ì¤‘ì‹¬ì  í”„ë ˆì„ì›Œí¬ ì„¤ê³„ë¡œ ì´ì–´ì§€ë©°, **ë¹„ë””ì˜¤ í† í°ì˜ ì••ì¶•**ì„ í†µí•´ ë¹„ë””ì˜¤ ì²˜ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.  ê²°ë¡ ì ìœ¼ë¡œ VideoLLaMA3ì˜ í•µì‹¬ì€ **ê³ í’ˆì§ˆ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„° ê¸°ë°˜ì˜ ë¹„ì „ ì¤‘ì‹¬ì  í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ê³¼ í”„ë ˆì„ì›Œí¬ ì„¤ê³„**ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì‹œê°ì  ë°ì´í„°ì— íš¨ê³¼ì ìœ¼ë¡œ ì ì‘í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë“œ ì ì‘ ëŠ¥ë ¥ì„ êµ¬ì¶•í•˜ëŠ” ë° ìˆìŠµë‹ˆë‹¤.

#### High-Quality Data
ë³¸ ë…¼ë¬¸ì—ì„œ ê°•ì¡°í•˜ëŠ” ê³ í’ˆì§ˆ ë°ì´í„°ëŠ” ë‹¨ìˆœíˆ ì–‘ì´ ë§ì€ ë°ì´í„°ê°€ ì•„ë‹Œ, **ì •í™•í•˜ê³  ì¼ê´€ëœ ì£¼ì„**ì´ í¬í•¨ëœ ë°ì´í„°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.  ì´ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢Œìš°í•˜ëŠ” í•µì‹¬ ìš”ì†Œì´ë©°, íŠ¹íˆ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì˜ ì„±ëŠ¥ í–¥ìƒì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.  **ë¹„ì „ ì¤‘ì‹¬ì  ì ‘ê·¼**ì„ í†µí•´ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ êµ¬ì¶•ì— ì§‘ì¤‘í•¨ìœ¼ë¡œì¨ ë¹„êµì  ì‰½ê²Œ í™•ë³´í•˜ê³  í’ˆì§ˆ ê´€ë¦¬ê°€ ìš©ì´í•œ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ í™•ë³´í•˜ê³ ì í•©ë‹ˆë‹¤.  **ì˜ìƒ ë°ì´í„°ì˜ ê²½ìš°, í’ˆì§ˆì´ ë‚®ê³  ì£¼ì„ ì‘ì—…ì´ ì–´ë ¤ìš´ ì ì„ ê³ ë ¤í•˜ì—¬ ì´ë¯¸ì§€ ì´í•´ ëŠ¥ë ¥ í–¥ìƒì„ í†µí•´ ì˜ìƒ ì´í•´ ëŠ¥ë ¥ì„ ê°œì„ í•˜ëŠ” ì „ëµ**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.  ë”°ë¼ì„œ, ê³ í’ˆì§ˆ ë°ì´í„° í™•ë³´ë¥¼ ìœ„í•œ ë…¸ë ¥ì€ ë‹¨ìˆœíˆ ë°ì´í„° ì–‘ì˜ ì¦ê°€ê°€ ì•„ë‹Œ, **ë°ì´í„°ì˜ ì§ˆì  í–¥ìƒê³¼ íš¨ìœ¨ì ì¸ ë°ì´í„° í™œìš© ì „ëµ**ì— ì§‘ì¤‘ë˜ì–´ì•¼ í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ì´ëŠ” ê¶ê·¹ì ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒê³¼ ë‹¤ì–‘í•œ ì‹¤ì œ ì‘ìš© ë¶„ì•¼ì—ì„œì˜ íš¨ê³¼ì ì¸ í™œìš©ìœ¼ë¡œ ì´ì–´ì§ˆ ê²ƒì…ë‹ˆë‹¤.

#### Future Research
ë³¸ ë…¼ë¬¸ì˜ "ë¯¸ë˜ ì—°êµ¬" ë¶€ë¶„ì€ **ë¹„ë””ì˜¤ ë°ì´í„°ì˜ ì§ˆê³¼ ë‹¤ì–‘ì„± ê°œì„ **, **ì‹¤ì‹œê°„ ì¶”ë¡  ìµœì í™”**, **ë‹¤ì¤‘ ëª¨ë‹¬ í™•ì¥**, **ê³ ê¸‰ ì‚¬í›„ í•™ìŠµ ê¸°ë²•** ë“± ë„¤ ê°€ì§€ ì£¼ìš” ë°©í–¥ìœ¼ë¡œ ë¯¸ë˜ ì—°êµ¬ì˜ í•„ìš”ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.  **ê³ í’ˆì§ˆ ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ êµ¬ì¶•**ì„ í†µí•´ ëª¨ë¸ì˜ ì‹œê°„ì  ì´í•´ì™€ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œì¼œì•¼ í•˜ë©°, **ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë¸ ê²½ëŸ‰í™” ë° ìµœì í™”**ê°€ í•„ìˆ˜ì ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ë˜í•œ, **ì˜¤ë””ì˜¤ë‚˜ ìŒì„± ë°ì´í„°ì™€ ê°™ì€ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹° í†µí•©**ì„ í†µí•´ ë³´ë‹¤ í¬ê´„ì ì¸ ë‹¤ì¤‘ ëª¨ë‹¬ ì´í•´ ëŠ¥ë ¥ì„ í™•ë³´í•˜ê³ , **ê°•í™” í•™ìŠµ ê¸°ë²• ë“± ê³ ê¸‰ ì‚¬í›„ í•™ìŠµ ê¸°ë²•** ë„ì…ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ ë”ìš± ê°œì„ í•´ì•¼ í•œë‹¤ëŠ” ì ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¨ìˆœíˆ ê¸°ìˆ ì  ê°œì„ ì„ ë„˜ì–´ì„œ **ì‹¤ì œ ì‘ìš© í™˜ê²½ì—ì„œì˜ ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•œ ì‹¤ìš©ì ì¸ ì—°êµ¬ ë°©í–¥**ì„ ì œì‹œí•˜ê³ , í–¥í›„ ë‹¤ì¤‘ ëª¨ë‹¬ AI ë°œì „ì— ëŒ€í•œ ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.

#### Model Limitations
ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ ëª¨ë¸ì˜ í•œê³„ëŠ” í¬ê²Œ **ë°ì´í„° í’ˆì§ˆ ë° ë‹¤ì–‘ì„±**, **ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥**, ê·¸ë¦¬ê³  **ìƒˆë¡œìš´ ëª¨ë‹¬ë¦¬í‹° ì¼ë°˜í™”** ì„¸ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  **ë°ì´í„° í’ˆì§ˆ** ì¸¡ë©´ì—ì„œëŠ” ê³ í’ˆì§ˆ ì˜ìƒ-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ í™•ë³´ì˜ ì–´ë ¤ì›€ì´ ì§€ì ë©ë‹ˆë‹¤. íŠ¹íˆ ì˜ìƒ ë°ì´í„°ëŠ” ì´ë¯¸ì§€ ë°ì´í„°ë³´ë‹¤ ì£¼ì„ ì‘ì—…ì´ ì–´ë µê³  ë¹„ìš©ì´ ë§ì´ ë“¤ê¸° ë•Œë¬¸ì— ì–‘ì§ˆì˜ ë°ì´í„° í™•ë³´ê°€ ì–´ë µìŠµë‹ˆë‹¤. ë˜í•œ, **ë°ì´í„° ë‹¤ì–‘ì„±** ì¸¡ë©´ì—ì„œë„ ë‹¤ì–‘í•œ ì˜ìƒ ìœ í˜•ê³¼ ì¥ë¥´ë¥¼ ì¶©ë¶„íˆ í¬í•¨í•˜ì§€ ëª»í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì— ì œí•œì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. **ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥** ì¸¡ë©´ì—ì„œ ê³ í•´ìƒë„ ì˜ìƒ ë° ê¸´ ì˜ìƒ ì²˜ë¦¬ ì‹œ ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¤ê¸° ë•Œë¬¸ì— ì‹¤ì‹œê°„ ì²˜ë¦¬ì— ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ììœ¨ ì£¼í–‰ì´ë‚˜ ì‹¤ì‹œê°„ ì˜ìƒ ë¶„ì„ ë“± ì‹¤ì‹œê°„ ì²˜ë¦¬ê°€ ìš”êµ¬ë˜ëŠ” ë¶„ì•¼ì—ëŠ” ì ìš©í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ **ìƒˆë¡œìš´ ëª¨ë‹¬ë¦¬í‹° ì¼ë°˜í™”** ëŠ¥ë ¥ì— ìˆì–´ì„œ, í˜„ì¬ ëª¨ë¸ì€ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ëª¨ë‹¬ë¦¬í‹°ì— ì§‘ì¤‘ë˜ì–´ ìˆê³  ì˜¤ë””ì˜¤ë‚˜ ìŒì„± ë°ì´í„°ì™€ ê°™ì€ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ì— ëŒ€í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì€ ì œí•œì ì…ë‹ˆë‹¤. í–¥í›„ ì—°êµ¬ì—ì„œëŠ” ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ë…¸ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤. íŠ¹íˆ ë‹¤ì–‘í•˜ê³  ê³ í’ˆì§ˆì˜ ì˜ìƒ-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ êµ¬ì¶•, ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•©í•˜ëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„ê°€ ì¤‘ìš”í•œ ì—°êµ¬ ë°©í–¥ì´ ë  ê²ƒì…ë‹ˆë‹¤.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2501.13106/x2.png)

> ğŸ”¼ ê·¸ë¦¼ 2ëŠ” VideoLLaMA3ì˜ í›ˆë ¨ ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. VideoLLaMA3ì˜ í›ˆë ¨ì€ í¬ê²Œ ë„¤ ë‹¨ê³„ë¡œ ë‚˜ë‰˜ëŠ”ë°, ê° ë‹¨ê³„ëŠ” íŠ¹ì • ëª©í‘œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. 1ë‹¨ê³„ì¸ Vision Encoder Adaptationì—ì„œëŠ” ë‹¤ì–‘í•œ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë¹„ì „ ì¸ì½”ë”ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤. 2ë‹¨ê³„ì¸ Vision-Language AlignmentëŠ” ëŒ€ê·œëª¨ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ì „ ì¸ì½”ë”ì™€ ì–¸ì–´ ëª¨ë¸ì„ ë™ì‹œì— ì¡°ì •í•˜ì—¬ ì‹œê°ì  ë° ì–¸ì–´ì  ì´í•´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. 3ë‹¨ê³„ì¸ Multi-task Fine-tuningì€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë° ë¹„ë””ì˜¤ ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ 4ë‹¨ê³„ì¸ Video-centric Fine-tuningì€ ë¹„ë””ì˜¤ ì´í•´ ëŠ¥ë ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë¹„ë””ì˜¤ ì¤‘ì‹¬ì˜ ë¯¸ì„¸ ì¡°ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì˜ ì¢…ë¥˜ì™€ ì–‘ë„ ê·¸ë¦¼ì— ìì„¸íˆ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 2: Training paradigm of VideoLLaMA3. The training of VideoLLaMA3 has four stages: (1) Vision Encoder Adaptation, (2) Vision-Language Alignment, (3) Multi-task Fine-tuning, and (4) Video-centric Fine-tuning.
> </details>



![](https://arxiv.org/html/2501.13106/x3.png)

> ğŸ”¼ ê·¸ë¦¼ 3ì€ VideoLLaMA3ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë‘ ê°€ì§€ í•µì‹¬ ê¸°ìˆ ì´ ìˆìŠµë‹ˆë‹¤. ì²«ì§¸, ì„ì˜ í•´ìƒë„ ë¹„ì „ í† í°í™”(AVT)ëŠ” ëª¨ë“  í•´ìƒë„ì˜ ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ë¥¼ 1ì°¨ì› í† í° ì‹œí€€ìŠ¤ ì§‘í•©ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë‹¤ì–‘í•œ ì–‘ì˜ ì…ë ¥ ì´ë¯¸ì§€ì™€ ì„œë¡œ ë‹¤ë¥¸ í•´ìƒë„ì˜ ë¹„ë””ì˜¤ë¥¼ ì§€ì›í•¨ìœ¼ë¡œì¨ ë³´ë‹¤ ìœ ì—°í•œ ë¹„ì „ ì…ë ¥ì„ ì§€ì›í•©ë‹ˆë‹¤. ë‘˜ì§¸, ì°¨ë³„ì  í”„ë ˆì„ ê°€ì§€ì¹˜ê¸°(DiffFP)ëŠ” ë¹„ë””ì˜¤ ì••ì¶•ê¸° ì—­í• ì„ í•˜ë©° ì¸ì ‘í•œ í”„ë ˆì„ ê°„ì˜ ì°¨ì´ê°€ ìµœì†Œì¸ ë¹„ë””ì˜¤ ì½˜í…ì¸ ë¥¼ ì œê±°í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ íŠ¹íˆ ê¸´ ë¹„ë””ì˜¤ì˜ ê²½ìš° ë¹„ë””ì˜¤ ì²˜ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 3: The overall pipeline of our VideoLLaMA3. There are two key technical points: â¶ Any-resolution Vision TokenizationÂ (AVT): AVT converts images or videos of any resolution into a set of 1-D token sequences, enabling compatibility with varying amounts of input images and videos of different resolutions, thereby supporting more flexible vision input; â· Differential Frame PrunerÂ (DiffFP): Serving as a video compressor, DiffFP eliminates video content with minimal differences between adjacent frames. This approach enhances video processing efficiency, particularly for long-form videos.
> </details>



![](https://arxiv.org/html/2501.13106/x4.png)

> ğŸ”¼ ê·¸ë¦¼ 4ëŠ” VideoLLaMA3 ëª¨ë¸ì˜ DiffFP(Differential Frame Pruner) ë™ì‘ ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. DiffFPëŠ” ë¹„ë””ì˜¤ í† í°ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.  êµ¬ì²´ì ìœ¼ë¡œ, DiffFPëŠ” ì´ì „ í”„ë ˆì„ê³¼ì˜ í”½ì…€ ê³µê°„ì—ì„œì˜ íŒ¨ì¹˜ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹„ë””ì˜¤ í† í°ì„ ì œê±°í•©ë‹ˆë‹¤.  ì¦‰, ì´ì „ í”„ë ˆì„ê³¼ ë§¤ìš° ìœ ì‚¬í•œ íŒ¨ì¹˜(ì˜ì—­)ëŠ” ì¤‘ë³µ ì •ë³´ë¡œ ê°„ì£¼ë˜ì–´ ì œê±°ë˜ê³ , ì´ë¥¼ í†µí•´ ë¹„ë””ì˜¤ì˜ íš¨ìœ¨ì ì¸ í‘œí˜„ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ê·¸ë¦¼ì—ì„œëŠ” í”„ë ˆì„ ê°„ ì°¨ì´ ê³„ì‚°, íŒ¨ì¹˜ ì œê±° ê³¼ì •ì´ ìˆœì°¨ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ ê±°ì³ ë¹„ë””ì˜¤ í† í°ì˜ ìˆ˜ë¥¼ ì¤„ì„ìœ¼ë¡œì¨, íŠ¹íˆ ê¸´ ë¹„ë””ì˜¤ì˜ ì²˜ë¦¬ íš¨ìœ¨ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 4: The calculation flow of our DiffFP. We prune video tokens based on patch similarities in pixel space, removing patches with smaller distances to the previous frame.
> </details>



![](https://arxiv.org/html/2501.13106/x5.png)

> ğŸ”¼ ê·¸ë¦¼ 5ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ìœ í˜•ì— ëŒ€í•œ ë°ì´í„° í˜•ì‹ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ì˜ ê²½ìš° ê° ì´ë¯¸ì§€ì˜ í† í°ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ '\n'ì„ ì‚¬ìš©í•˜ê³ , ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ì˜ ê²½ìš° ê° í”„ë ˆì„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ 'Time: xxs'ë¥¼, ì„œë¡œ ë‹¤ë¥¸ í”„ë ˆì„ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ','ë¥¼, ì„œë¡œ ë‹¤ë¥¸ ë¹„ë””ì˜¤ì˜ í† í°ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ '\n'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ì˜ ê²½ìš° ë¹„ë””ì˜¤ì™€ í…ìŠ¤íŠ¸ê°€ ì„ì—¬ì„œ êµ¬ì„±ë©ë‹ˆë‹¤.  ì´ ê·¸ë¦¼ì€ ë‹¤ì–‘í•œ í˜•íƒœì˜ ì‹œê°ì  ë°ì´í„° (ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤)ë¥¼ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ë° ì¤‘ì ì„ ë‘ê³  ìˆìŠµë‹ˆë‹¤.  ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ëŠ” ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ í† í°ë“¤ì„ '\n'ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜íƒ€ë‚´ê³ , ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ëŠ” ê° í”„ë ˆì„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ 'Time: xxs'ë¡œ í‘œì‹œí•˜ë©° í”„ë ˆì„ë“¤ ê°„ì—ëŠ” ','ë¡œ êµ¬ë¶„í•˜ê³ , ë‹¤ë¥¸ ë¹„ë””ì˜¤ì˜ í† í°ë“¤ì€ ë‹¤ì‹œ '\n'ìœ¼ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ëŠ” ë¹„ë””ì˜¤ì™€ í…ìŠ¤íŠ¸ í† í°ì´ ë²ˆê°ˆì•„ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 5: Data formats for different data types. â¶ For image sequence, we use '\n' to separate image tokens from different image; â· For video sequence, we use 'Time: xxs' to indicate timestamps of each frame, ',' to separate different frames, and '\n' to separate tokens from different videos; â¸ For streaming video sequence, videos and texts are organized in an interleaved format.
> </details>



![](https://arxiv.org/html/2501.13106/x6.png)

> ğŸ”¼ ê·¸ë¦¼ 6ì€ VideoLLaMA3 ëª¨ë¸ì´ ì°¨íŠ¸ ì´ë¯¸ì§€ë¥¼ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ì‚¬ë¡€ ì—°êµ¬ì…ë‹ˆë‹¤.  ë‘ ê°€ì§€ ì°¨íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ê³¼ VideoLLaMA3ì˜ ì‘ë‹µì´ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ ì‚¬ë¡€ëŠ” ì£¼ì‹ ì°¨íŠ¸ì— ëŒ€í•œ ë¶„ì„ê³¼ íˆ¬ì ì œì•ˆì„ ë³´ì—¬ì£¼ê³ , ë‘ ë²ˆì§¸ ì‚¬ë¡€ëŠ” ì—¬ëŸ¬ MLLM ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ VideoLLaMA3ê°€ ì°¨íŠ¸ ì´ë¯¸ì§€ ë‚´ì˜ ë°ì´í„°ì™€ íŒ¨í„´ì„ ì •í™•í•˜ê²Œ íŒŒì•…í•˜ê³  ë¶„ì„í•˜ì—¬, ìœ ì˜ë¯¸í•œ í†µì°°ë ¥ì„ ì œê³µí•˜ëŠ” ëŠ¥ë ¥ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 6: Case study of chart images understanding.
> </details>



![](https://arxiv.org/html/2501.13106/x7.png)

> ğŸ”¼ ê·¸ë¦¼ 7ì€ VideoLLaMA3 ëª¨ë¸ì´ OCR ë° ë¬¸ì„œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ì‚¬ë¡€ ì—°êµ¬ì…ë‹ˆë‹¤.  ì²« ë²ˆì§¸ ì˜ˆì‹œëŠ” ë””ìì¸ í¬ìŠ¤í„°ì— ìˆëŠ” í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³ , í¬ìŠ¤í„°ì˜ ê°œì„ ì ì„ ì œì•ˆí•˜ëŠ” VideoLLaMA3ì˜ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ì˜ˆì‹œì—ì„œëŠ” VideoLLaMA3ê°€ ë¬¸ì„œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” VideoLLaMA3ê°€ ì´ë¯¸ì§€ ë‚´ì˜ ë°€ì§‘ëœ ì •ë³´ë¥¼ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì´ ë›°ì–´ë‚¨ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ VideoLLaMA3ì˜ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì´í•´ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ëŒ€í‘œì ì¸ ì˜ˆì‹œë“¤ì„ ì œì‹œí•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 7: Case study of OCR and document images.
> </details>



![](https://arxiv.org/html/2501.13106/x8.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì´í•´ì— ëŒ€í•œ ì‚¬ë¡€ ì—°êµ¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì„¸ ê°œì˜ ë‹¤ë¥¸ ì˜ˆì‹œê°€ ìˆëŠ”ë°, ê° ì˜ˆì‹œëŠ” ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì´í•´ ì‘ì—…ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì²« ë²ˆì§¸ ì˜ˆì‹œëŠ” ë‘ ì¢…ë¥˜ì˜ ìƒˆë¥¼ êµ¬ë³„í•˜ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ê³ , ë‘ ë²ˆì§¸ ì˜ˆì‹œëŠ” ê¸´ ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ë©°, ì„¸ ë²ˆì§¸ ì˜ˆì‹œëŠ” ë§Œí™”ì—ì„œ ì¤„ê±°ë¦¬ë¥¼ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ VideoLLaMA3 ëª¨ë¸ì´ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì´ë¯¸ì§€ ì´í•´ ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 8: Case study of multi-image understanding.
> </details>



![](https://arxiv.org/html/2501.13106/x9.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ VideoLLaMA3 ëª¨ë¸ì´ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ì‚¬ë¡€ ì—°êµ¬ ì„¸ ê°€ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì²« ë²ˆì§¸ ì˜ˆì‹œëŠ” ë†êµ¬ ê²½ê¸° ì¤‘ ììœ íˆ¬ ìƒí™©ì„ ì„¤ëª…í•˜ê³ , ë‘ ë²ˆì§¸ ì˜ˆì‹œëŠ” ëª¨ë‚˜ë¦¬ìì˜ ì—­ì‚¬ì  ì˜í–¥ê³¼ ì˜ë¯¸ë¥¼ ë…¼í•˜ë©°, ì„¸ ë²ˆì§¸ ì˜ˆì‹œëŠ” ìš°ì£¼ì„  ì•ˆì— ê°•ì•„ì§€ ìš°ì£¼ë¹„í–‰ì‚¬ê°€ íƒ‘ìŠ¹í•˜ì—¬ ìš°ì£¼ì™€ ì§€êµ¬ë¥¼ ì—¬í–‰í•˜ëŠ” ì˜ìƒì„ ìì„¸í•˜ê²Œ ë¬˜ì‚¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.  ì´ ê·¸ë¦¼ì€ VideoLLAMA3 ëª¨ë¸ì˜ ë›°ì–´ë‚œ ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ê³¼ ê´‘ë²”ìœ„í•œ ì§€ì‹ ê¸°ë°˜ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 9: Case study of images with general knowledge.
> </details>



![](https://arxiv.org/html/2501.13106/x10.png)

> ğŸ”¼ ê·¸ë¦¼ 10ì€ ë¹„ë””ì˜¤ ì´í•´ì— ëŒ€í•œ ì‚¬ë¡€ ì—°êµ¬ë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê°ê°ì˜ ì‚¬ë¡€ëŠ” ë¹„ë””ì˜¤ì— ë‚˜íƒ€ë‚œ ê°ì²´ì˜ ì¢…ë¥˜ì™€ ìœ„ì¹˜, í‚¤ë³´ë“œì—ì„œ ì‚¬ë¼ì§€ëŠ” ë§ˆì§€ë§‰ í‚¤, ë¹„ë””ì˜¤ì˜ íŠ¹ì´í•œ ì , ë¹„ë””ì˜¤ì˜ ì„¸ë¶€ ë‚´ìš©, ê·¸ë¦¬ê³  ê²½ê¸°ì˜ ìŠ¹ìë¥¼ ì§ˆë¬¸í•˜ê³  VideoLLaMA 3 ëª¨ë¸ì´ ì´ì— ëŒ€í•œ ë‹µë³€ì„ ì œì‹œí•˜ëŠ” í˜•íƒœë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ VideoLLaMA 3 ëª¨ë¸ì´ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 10: Case study of video understanding.
> </details>



![](https://arxiv.org/html/2501.13106/x11.png)

> ğŸ”¼ ê·¸ë¦¼ 11ì€ VideoLLaMA3 ëª¨ë¸ì˜ ì¥ê¸° ë¹„ë””ì˜¤ ì´í•´, ì‹œê°„ì  ê¸°ë°˜, ê·¸ë¦¬ê³  ë¹„ë””ì˜¤-ì´ë¯¸ì§€ ê²°í•© ì´í•´ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ì‚¬ë¡€ ì—°êµ¬ì…ë‹ˆë‹¤.  ì™¼ìª½ ìœ„ ê·¸ë¦¼ì€ ì¥ì‹œê°„ì— ê±¸ì³ ë‹¤ì–‘í•œ ëŸ¬ì‹œì•„ í’ê²½ì„ ë³´ì—¬ì£¼ëŠ” ë¹„ë””ì˜¤ì˜ ì—¬ëŸ¬ ì¥ë©´ì„ ìº¡ì³í•œ ìŠ¤í‹¸ ì´ë¯¸ì§€ë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì™¼ìª½ ì•„ë˜ ê·¸ë¦¼ì€ ì½œë¼ë¥¼ ì»µì— ë”°ë¥´ëŠ” ë™ì‘ì„ ë‹´ì€ ë¹„ë””ì˜¤ì˜ í”„ë ˆì„ë“¤ì„ ì—°ì†ì ìœ¼ë¡œ ë³´ì—¬ì£¼ê³ , íŠ¹ì • ì‹œê°„ëŒ€(ì‹œì‘ ë° ì¢…ë£Œ ì‹œê°„)ë¥¼ íŠ¹ì •í•˜ëŠ” ì‹œê°„ì  ê¸°ë°˜ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.  ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì€ ê³ ì–‘ì´ì™€ ë³‘ì•„ë¦¬ê°€ ì„œë¡œ ê»´ì•ˆê³  ìˆëŠ” ë¹„ë””ì˜¤ì™€ ë°¤ê±°ë¦¬ë¥¼ ê±·ëŠ” ì—¬ì„±ì˜ ì‚¬ì§„ì„ ë³´ì—¬ì£¼ëŠ” ë°, ë¹„ë””ì˜¤ì™€ ì´ë¯¸ì§€ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ë¹„ë””ì˜¤-ì´ë¯¸ì§€ ê²°í•© ì´í•´ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì „ì²´ì ìœ¼ë¡œ, ê·¸ë¦¼ì€ VideoLLaMA3ê°€ ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ì´í•´ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 11: Case study of long video understanding, temporal grounding, and video-image joint understanding.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Task | Dataset | Amount |
|---|---|---|
| **Scene Image** | VL3-Syn7M-detailed, Objects365-Recap [Objects365, ], SA-1B-Recap [kirillov2023segment, ], COCO2017-Recap [lin2014microsoft, ], ShareGPT4o [Chen2024HowFA, ], TextCaps [sidorov2020textcaps, ], ShareGPT4V [chen2023sharegpt4v, ], DenseFusion [li2024densefusion, ], LLaVA-ReCap (LCS-558K) [li2024llavaonevision, ] | 12.56M |
| **Scene Text Image** | Laion-OCR [schuhmann2022laion, ], COCO-Text [veit2016coco, ], TextOCR [singh2021textocr, ], BLIP3-OCR-Recap [Xue2024xGenMMA, ], LSVT [Sun2019ICDAR2C, ], ReCTS [liu2019icdar, ] | 4.69M |
| **Document** | SynthDoG-EN [kim2022ocr, ], SynthDoG-ZH [kim2022ocr, ], UReader-TR [Ye2023UReaderUO, ], FUNSD [funsd, ], DUDE [van2023icdar, ], Vary-600k [wei2023vary, ], pdfa-eng-wds [pdfa, ], idl-wds [idlwds, ] | 2.68M |
| **Chart** | Chart-to-Text [2022Chart, ] | 0.04M |
| **Fine-grained** | Osprey-724K [yuan2024osprey, ], MDVP-Data [lin2024draw, ], ADE20K-Recap [zhou2019semantic, ], Object365 [Objects365, ], Flickr-30K [young2014image, ], GranD [rasheed2024glamm, ] | 1.00M |
| **Text-only** | Evol-Instruct-143K [chen2024allava, ], Infinity-Instruct-code [InfinityInstruct2024, ], Infinity-Instruct-commonsense [InfinityInstruct2024 ], Infinity-Instruct-math [InfinityInstruct2024, ] | 6.25M |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ë…¼ë¬¸ì˜ Vision-Language Alignment ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„° ë¯¹ìŠ¤ì³ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ê° ë°ì´í„°ì…‹ì˜ ì¢…ë¥˜ (Scene Image, Scene Text Image, Document, Chart, Fine-grained, Text-only) ì™€ í•´ë‹¹ ë°ì´í„°ì…‹ì˜ í¬ê¸° (Amount) ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ë‹¤ì¤‘ ëª¨ë“œ ì´í•´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì‚¬ìš©ëœ ë°ì´í„°ì…‹ êµ¬ì„±ì„ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 2: Data mixture in vision-language alignment stage.
> </details>

{{< table-caption >}}
| Task | Dataset | Amount |
|---|---|---|
| **Image & Text Data** |  |  |
| General | LLaVA-SFT-665K [li2024llava], LLaVA-OV-SI [li2024llavaonevision], Cambrian-cleaned [tong2024cambrian], Pixmo (docs, cap, points, cap-qa, ask-model-anything) [molmo2024] | 9.87M |
| Document | DocVQA [mathew2021docvqadatasetvqadocument], Docmatix [laurenÃ§on2024building] | 1.31M |
| Chart/Figure | ChartQA [masry2022chartqa], MMC_Instruction [liu2023mmc], DVQA [kafle2018dvqa], LRV_Instruction [liu2023aligning], ChartGemma [masry2024chartgemmavisualinstructiontuningchart], InfoVQA [mathew2022infographicvqa], PlotQA [methani2020plotqa] | 1.00M |
| OCR | MultiUI [liu2024harnessingwebpageuistextrich], in-house data | 0.83M |
| Grounding | RefCoco [kazemzadeh2014referitgame], VCR [zellers2019vcr], in-house data | 0.50M |
| Multi-Image | Demon-Full [li2024fine], Contrastive_Caption [jiang2024mantisinterleavedmultiimageinstruction] | 0.41M |
| Text-only | Magpie [xu2024magpie], Magpie-Pro [xu2024magpie], Synthia [Synthia-70B-v1.2], Infinity-Instruct-subjective [InfinityInstruct2024], NuminaMath [li2024numinamath] | 2.21M |
| **Video Data** |  |  |
| General | LLaVA-Video-178K [zhang2024video], ShareGPT4o-Video [chen2024sharegpt4video], FineVideo [FarrÃ©2024FineVideo], CinePile [rawal2024cinepile], ShareGemini-k400 [sharegemini], ShareGemini-WebVID [sharegemini], VCG-Human [Maaz2024VideoGPT+], VCG-Plus [Maaz2024VideoGPT+], VideoLLaMA2 in-house data, Temporal Grounding in-house data | 2.92M |{{< /table-caption >}}
> ğŸ”¼ í‘œ 3ì€ VideoLLaMA3 ëª¨ë¸ì˜ ë‹¤ì¤‘ ì‘ì—… ë¯¸ì„¸ ì¡°ì • ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„° ë¯¹ìŠ¤ì²˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì´ í‘œëŠ” ë‹¤ì–‘í•œ í•˜ìœ„ ì‘ì—…(ì¼ë°˜ ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ë°ì´í„°, ë¬¸ì„œ, ì°¨íŠ¸/ê·¸ë¦¼, OCR, ê·¸ë¼ìš´ë”©, ë‹¤ì¤‘ ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ì „ìš©)ì— ëŒ€í•œ ë°ì´í„°ì…‹ê³¼ ê° ë°ì´í„°ì…‹ì˜ ì–‘ì„ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.  ê° í•˜ìœ„ ì‘ì—…ì€ ì‹œê°ì  ì´í•´ì˜ íŠ¹ì • ì¸¡ë©´ì„ ëª©í‘œë¡œ í•˜ì—¬ ëª¨ë¸ì´ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì‹œê°ì  ì •ë³´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.  ì—¬ê¸°ì—ëŠ” ì‹œê°ì  ë°ì´í„°ë¿ ì•„ë‹ˆë¼ ëª¨ë¸ì˜ ì–¸ì–´ì  ì´í•´ ëŠ¥ë ¥ì„ ê°•í™”í•˜ê¸° ìœ„í•œ ìƒë‹¹ëŸ‰ì˜ í…ìŠ¤íŠ¸ ì „ìš© ë°ì´í„°ë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 3: Data mixture in massive multi-task fine-tuning stage.
> </details>

{{< table-caption >}}
| Task | Dataset | Amount |
|---|---|---|
| General Video | LLaVA-Video-178K [zhang2024video], ShareGPT4o-Video [chen2024sharegpt4video], FineVideo [FarrÃ©2024FineVideo], CinePile [rawal2024cinepile], ShareGemini-k400 [sharegemini], ShareGemini-WebVID [sharegemini], VCG-Human [Maaz2024VideoGPT+], VCG-Plus [Maaz2024VideoGPT+], VideoRefer [yuan2024videorefer], VideoLLaMA2 in-house data, In-house synthetic data | 3.03M |
| Streaming Video | ActivityNet [krishna2017dense], YouCook2 [zhou2018towards], Ego4D-narration [grauman2022ego4d], Ego4D-livechat [chen2024videollm] | 36.2K |
| Temporal Grounding | ActivityNet [krishna2017dense], YouCook2 [zhou2018towards], ViTT [huang2020multimodal], QuerYD [oncescu2021queryd], HiREST [zala2023hierarchical], Charades-STA [gao2017tall], Moment-10M [qian2024momentor], COIN [tang2019coin] | 0.21M |
| Image-only | LLaVA-SFT-665K [li2024llava], LLaVA-OV-SI [li2024llavaonevision] | 0.88M |
| Text-only | Magpie [xu2024magpie], Tulu 3 [lambert2024tulu3] | 1.56M |{{< /table-caption >}}
> ğŸ”¼ í‘œ 4ëŠ” VideoLLaMA3ì˜ ë¹„ë””ì˜¤ ì¤‘ì‹¬ ë¯¸ì„¸ ì¡°ì • ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„° ë¯¹ìŠ¤ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì´ í‘œëŠ” ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë¹„ë””ì˜¤ ë°ì´í„° (ì¼ë°˜ ë¹„ë””ì˜¤, ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤, ì‹œê°„ì  ê·¼ê±° ë°ì´í„°, ì´ë¯¸ì§€ ì „ìš© ë°ì´í„°, í…ìŠ¤íŠ¸ ì „ìš© ë°ì´í„°)ì™€ ê° ë°ì´í„°ì…‹ì˜ ì–‘ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ê° ë°ì´í„° ìœ í˜•ì€ íŠ¹ì • ë¹„ë””ì˜¤ ì´í•´ ì¸¡ë©´ì„ ê°•í™”í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¼ë°˜ ë¹„ë””ì˜¤ ë°ì´í„°ëŠ” ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ ëª¨ë¸ì˜ ì´í•´ë„ë¥¼ ë†’ì´ê³ , ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ë°ì´í„°ëŠ” ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•˜ë©°, ì‹œê°„ì  ê·¼ê±° ë°ì´í„°ëŠ” ë¹„ë””ì˜¤ í”„ë ˆì„ ê°„ì˜ ê´€ê³„ íŒŒì•… ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.  ê²°ë¡ ì ìœ¼ë¡œ, ì´ í‘œëŠ” VideoLLaMA3ì˜ ë¹„ë””ì˜¤ ì´í•´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë‹¤ì–‘í•˜ê³  í’ë¶€í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í–ˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 4: Data mixture in video-centric fine-tuning stage.
> </details>

{{< table-caption >}}
| Benchmark | Model (2B) | SmoltVLM | InternVL 2.5 | Qwen2-VL | VideoLLaMA-3 |
|---|---|---|---|---|---| 
| **Benchmark** | **Model** |  |  |  |  |
|---|---|---|---|---|---| 
|  | **SmolVLM** <br>  ![https://arxiv.org/html/2501.13106/figures/icons/huggingface.png](https://arxiv.org/html/2501.13106/figures/icons/huggingface.png) | 65.3* | 79.2 | 73.5 | 79.8 |
|  | **2B** |  |  |  |  |
|---|---|---|---|---|---| 
|  | **InternVL 2.5** <br> ![https://arxiv.org/html/2501.13106/figures/icons/opengvlab.jpeg](https://arxiv.org/html/2501.13106/figures/icons/opengvlab.jpeg) | 81.6 | 88.7 | 90.1 | 91.9 |
|  | **2B** |  |  |  |  |
|---|---|---|---|---|---| 
|  | **Qwen2-VL** <br> ![https://arxiv.org/html/2501.13106/figures/icons/qwen.png](https://arxiv.org/html/2501.13106/figures/icons/qwen.png) | - | 60.9 | 65.5 | 69.4 |
|  | **2B** |  |  |  |  |
|---|---|---|---|---|---| 
|  | **VideoLLaMA-3** | 622* | 804 | 767* | 779 |
|---|---|---|---|---|---| 
| *Document/Chart/Scene Text Understanding* |  |  |  |  |  |
|---|---|---|---|---|---| 
| ChartQA | 65.3* | 79.2 | 73.5 | 79.8 |
| DocVQA<sub>test</sub> | 81.6 | 88.7 | 90.1 | 91.9 |
| InfoVQA<sub>test</sub> | - | 60.9 | 65.5 | 69.4 |
| OCRBench | 622* | 804 | 767* | 779 |
| *Math* |  |  |  |  |  |
|---|---|---|---|---|---| 
| MathVista<sub>testmini</sub> | 44.6 | 51.3 | 43.0 | 59.2 |
| MathVision<sub>test</sub> | 6.5* | 14.7 | 12.4 | 15.5 |
| *Multi Image* |  |  |  |  |  |
|---|---|---|---|---|---| 
| MMMU-Pro | 17.1* | 23.7 | 26.0 | 28.6 |
| MMMU<sub>val</sub> | 38.8 | 43.6 | 41.1 | 45.3 |
| BLINK<sub>test</sub> | 42.3* | 44.0 | 43.1* | 44.2 |
| *Knowledge/General QA* |  |  |  |  |  |
|---|---|---|---|---|---| 
| RealWorldQA | 48.8* | 60.1 | 62.9 | 67.3 |
| AI2D | 62.1* | 74.9 | 69.9 | 78.2 |
| GQA | 49.2* | 59.5* | 59.8* | 62.7 |
| MME | 1600* | 2005* | 1872 | 1901 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 5ëŠ” ë…¼ë¬¸ì—ì„œ ì œì‹œëœ 20ì–µ ë§¤ê°œë³€ìˆ˜ ëª¨ë¸ì˜ ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  í‘œì—ëŠ” ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì´í•´ ì‘ì—…(ë¬¸ì„œ/ì°¨íŠ¸/ì¥ë©´ í…ìŠ¤íŠ¸ ì´í•´, ìˆ˜í•™ì  ì¶”ë¡ , ë‹¤ì¤‘ ì´ë¯¸ì§€ ì´í•´, ì¼ë°˜ì ì¸ ì§€ì‹ QA)ì— ëŒ€í•œ ì—¬ëŸ¬ ê¸°ì¤€ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì •í™•ë„(%)ë¡œ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.  * í‘œì‹œëŠ” ì¬í˜„ëœ ê²°ê³¼ì„ì„ ë‚˜íƒ€ë‚´ë©°, ìµœê³  ì„±ëŠ¥ì€ êµµê²Œ, ë‘ ë²ˆì§¸ë¡œ ë†’ì€ ì„±ëŠ¥ì€ ë°‘ì¤„ë¡œ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ í‘œëŠ” VideoLLaMA3 ëª¨ë¸ì˜ ì´ë¯¸ì§€ ì´í•´ ëŠ¥ë ¥ì„ ë‹¤ë¥¸ ìµœì²¨ë‹¨ ëª¨ë¸ë“¤ê³¼ ë¹„êµí•˜ì—¬ ë³´ì—¬ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 5: Evaluation results of 2B models on image benchmarks. âˆ— denotes the reproduced results. The best results are in bold and the second best ones are underlined.
> </details>

{{< table-caption >}}
| Model | Size | Document/Chart/Scene Text Understanding | Math | Multi Image | Knowledge/General QA |
|---|---|---|---|---|---| 
| Molmo-7B-D 7B <img src="https://arxiv.org/html/2501.13106/figures/icons/ai2.png" width="27" height="9"> | 84.1 | 92.2 | 51.6 | - | 70.7 | - |
| InternVL2.5 8B <img src="https://arxiv.org/html/2501.13106/figures/icons/opengvlab.jpeg" width="12" height="12"> | 84.8 | 93.0 | 64.4 | 34.3 | 70.1 | 2344 |
| LLaVA-OneVision 7B <img src="https://arxiv.org/html/2501.13106/figures/icons/bytedance.jpg" width="14" height="12"> | 80.0 | 87.5 | 63.2 | -24.1â€  | 66.3 | 1998 |
| NVILA 8B <img src="https://arxiv.org/html/2501.13106/figures/icons/nvidia.jpg" width="16" height="12"> | 86.1 | 93.7 | 65.4 | -29.5* | 68.6 | 2219 |
| Qwen2-VL 7B <img src="https://arxiv.org/html/2501.13106/figures/icons/qwen.png" width="11" height="12"> | 83.0 | 94.5 | 58.2 | -31.4* | 70.1 | 2327 |
| VideoLLaMA3 7B | 86.3 | 94.9 | 67.1 | 33.6 | 72.7 | 2102 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 6ì€ ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ 7B ëª¨ë¸ì˜ ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  í‘œì—ëŠ” ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì´í•´ ì‘ì—…(ë¬¸ì„œ/ì°¨íŠ¸/ì¥ë©´ í…ìŠ¤íŠ¸ ì´í•´, ìˆ˜í•™ì  ì¶”ë¡ , ë‹¤ì¤‘ ì´ë¯¸ì§€ ì´í•´, ì¼ë°˜ì ì¸ ì§€ì‹ ì§ˆë¬¸ ë‹µë³€)ì— ëŒ€í•œ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì •ëŸ‰ì ìœ¼ë¡œ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.  ê° ì‘ì—…ì— ëŒ€í•œ ìµœê³  ì„±ëŠ¥ì€ êµµê²Œ í‘œì‹œë˜ì—ˆê³ , ë‘ ë²ˆì§¸ë¡œ ë†’ì€ ì„±ëŠ¥ì€ ë°‘ì¤„ì´ ê·¸ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.  ì¼ë¶€ ê²°ê³¼ëŠ” ì—°êµ¬ìë“¤ì´ ì¬í˜„í•œ ê²°ê³¼(*)ì´ê³ , ì¼ë¶€ëŠ” ê³µì‹ ë¦¬ë”ë³´ë“œ(â€ )ì—ì„œ ê°€ì ¸ì˜¨ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ í‘œëŠ” ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì´í•´ ëŠ¥ë ¥ì„ ê°€ì§„ 7B íŒŒë¼ë¯¸í„° ëª¨ë¸ ê°„ì˜ ì„±ëŠ¥ ë¹„êµë¥¼ í†µí•´ VideoLLaMA3 ëª¨ë¸ì˜ ê°•ì ì„ ë³´ë‹¤ ëª…í™•í•˜ê²Œ ë³´ì—¬ì£¼ê³ ì í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 6: Evaluation results of 7B models on image benchmarks. âˆ— denotes the reproduced results. â€  denotes the results retrieved from the official leaderboard. The best results are in bold and the second best ones are underlined.
> </details>

{{< table-caption >}}
| Benchmark | Model | Apollo | InternVL2.5 | Qwen2-VL | VideoLLaMA3 |
|---|---|---|---|---|---|---|
| | **2B** | **2B** | **2B** | 2B |
|---|---|---|---|---|---|---|
|  | <img src="https://arxiv.org/html/2501.13106/figures/icons/meta.png" width="12" height="12"> **Apollo** | <img src="https://arxiv.org/html/2501.13106/figures/icons/opengvlab.jpeg" width="12" height="12"> **InternVL2.5** | <img src="https://arxiv.org/html/2501.13106/figures/icons/qwen.png" width="12" height="12"> **Qwen2-VL** | **VideoLLaMA3** |
|---|---|---|---|---|---|---|
| *General Video Understanding* |  |  |  |  |
|---|---|---|---|---|---|---|
| VideoMME _w/o sub_ | 53.0 | 51.9 | 55.6 | **59.6** |
| VideoMME _w/ sub_ | 54.6 | 54.1 | **60.4** | **63.4** |
|---|---|---|---|---|---|---|
| MMVU<sub>val</sub> | - | 33.6<sup>âˆ—</sup> | 36.5<sup>â€ </sup> | **39.9** |
| MVBench | - | **68.8** | 63.2 | **65.5** |
| EgoSchema<sub>test</sub> | - | 58.1<sup>âˆ—</sup> | 54.9 | **58.5** |
| PerceptionTest<sub>test</sub> | 61.0 | **66.3**<sup>âˆ—</sup> | 53.9 | **68.0** |
| ActivityNet-QA | - | **54.1**<sup>âˆ—</sup> | 53.3<sup>âˆ—</sup> | **58.2** |
|---|---|---|---|---|---|---|
| *Long Video Understanding* |  |  |  |  |
|---|---|---|---|---|---|---|
| MLVU<sub>dev</sub> | **63.3** | 58.9<sup>âˆ—</sup> | 62.7<sup>âˆ—</sup> | **65.4** |
| LongVideoBench<sub>val</sub> | - | **52.0** | 48.7<sup>âˆ—</sup> | **57.1** |
| LVBench | - | 37.3<sup>âˆ—</sup> | **38.0**<sup>âˆ—</sup> | **40.4** |
|---|---|---|---|---|---|---|
| *Temporal Reasoning* |  |  |  |  |
|---|---|---|---|---|---|---|
| TempCompass | 60.8 | 57.7<sup>âˆ—</sup> | **62.2**<sup>âˆ—</sup> | **63.4** |
| NextQA | - | 75.6<sup>âˆ—</sup> | **77.2**<sup>âˆ—</sup> | **81.1** |
| Charades-STA | - | - | - | **55.5** |{{< /table-caption >}}
> ğŸ”¼ í‘œ 7ì€ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ VideoLLaMA3 ëª¨ë¸ì˜ ë¹„ë””ì˜¤ ì´í•´ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ìˆ˜í–‰ëœ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ í‘œëŠ” ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ VideoLLaMA3(2B ëª¨ë¸)ì˜ ì„±ëŠ¥ì„ ê¸°ì¡´ ìµœì²¨ë‹¨ ëª¨ë¸ë“¤ê³¼ ë¹„êµ ë¶„ì„í•œ ê²ƒì…ë‹ˆë‹¤.  * í‘œì‹œëŠ” ì¬í˜„ëœ ê²°ê³¼ì´ë©° â€  í‘œì‹œëŠ” ê³µì‹ ë¦¬ë”ë³´ë“œì—ì„œ ê°€ì ¸ì˜¨ ê²°ê³¼ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  í‘œì—ì„œ ê°€ì¥ ë†’ì€ ì ìˆ˜ëŠ” êµµê²Œ í‘œì‹œë˜ê³ , ë‘ ë²ˆì§¸ë¡œ ë†’ì€ ì ìˆ˜ëŠ” ë°‘ì¤„ì´ ê·¸ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.  í‘œì—ëŠ” ì¼ë°˜ì ì¸ ë¹„ë””ì˜¤ ì´í•´, ì¥ê¸° ë¹„ë””ì˜¤ ì´í•´, ê·¸ë¦¬ê³  ì‹œê°„ì  ì¶”ë¡  ì„¸ ê°€ì§€ ì£¼ìš” ì°¨ì›ì— ê±¸ì¹œ ì„±ëŠ¥ì´ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê° ì°¨ì›ì€ ì—¬ëŸ¬ ê°œì˜ í•˜ìœ„ ë²¤ì¹˜ë§ˆí¬ë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆìœ¼ë©°, VideoLLaMA3ê°€ ê° ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë‹¬ì„±í•œ ì„±ëŠ¥ ìˆ˜ì¹˜ê°€ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 7: Evaluation results of 2B models on video benchmarks. * denotes the reproduced results. â€  denotes the results retrieved from the official leaderboard. The best results are in bold and the second best ones are underlined.
> </details>

{{< table-caption >}}
| Model | 7B | 8B | 7B | 8B | 7B | 2.1-7B | 3-7B |
|---|---|---|---|---|---|---|---| 
| Qwen2-VL [https://arxiv.org/html/2501.13106/figures/icons/qwen.png](https://arxiv.org/html/2501.13106/figures/icons/qwen.png) |  |  |  |  |  |  |  |
| InternVL2.5 [https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/opengvlab.jpeg](https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/opengvlab.jpeg) |  |  |  |  |  |  |  |
| LLaVA-Video [https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/bytedance.jpg](https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/bytedance.jpg) |  |  |  |  |  |  |  |
| NVILA [https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/nvidia.jpg](https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/nvidia.jpg) |  |  |  |  |  |  |  |
| Apollo [https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/meta.png](https://arxiv.org/html/2501.13106/extracted/6151431/figures/icons/meta.png) |  |  |  |  |  |  |  |
| VideoLLaMA |  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|---| 
| *General Video Understanding* |  |  |  |  |  |  |  |
| VideoMME _w/o sub_ | 63.3 | 64.2 | 63.3 | 64.2 | 61.3 | 54.9 | **66.2** |
| VideoMME _w/ sub_ | 69.0 | 66.9 | 69.7 | **70.0** | 63.3 | 56.4 | **70.3** |
| MMVU<sub>val</sub> | -42.1<sup>â€ </sup> | -41.1<sup>â€ </sup> | -42.4<sup>âˆ—</sup> | 43.7<sup>âˆ—</sup> | - | -39.5<sup>â€ </sup> | **44.1** |
| MVBench | 67.0 | **72.0** | 58.6 | 68.1 | - | 57.3 | **69.7** |
| EgoSchema<sub>test</sub> | **66.7** | 66.2<sup>âˆ—</sup> | 57.3 | 54.3<sup>âˆ—</sup> | - | 53.1 | 63.3 |
| PerceptionTest<sub>test</sub> | 62.3 | 68.9<sup>âˆ—</sup> | **67.9**<sup>âˆ—</sup> | 65.4<sup>âˆ—</sup> | - | 54.9 | **72.8** |
| ActivityNet-QA | 57.4<sup>âˆ—</sup> | 58.9<sup>âˆ—</sup> | 56.5 | **60.9** | - | 53.0 | **61.3** |
|---|---|---|---|---|---|---|---| 
| *Long Video Understanding* |  |  |  |  |  |  |  |
| MLVU<sub>dev</sub> | 69.8<sup>âˆ—</sup> | 69.0<sup>âˆ—</sup> | 70.8<sup>âˆ—</sup> | 70.6<sup>âˆ—</sup> | **70.9** | 57.4 | **73.0** |
| LongVideoBench<sub>val</sub> | 55.6<sup>â€ </sup> | **60.0** | 58.2 | 57.7 | 58.5 | - | **59.8** |
| LVBench | 44.2<sup>âˆ—</sup> | 41.5<sup>âˆ—</sup> | 40.3<sup>âˆ—</sup> | 42.6<sup>âˆ—</sup> | - | 36.3 | **43.7** |
|---|---|---|---|---|---|---|---| 
| *Temporal Reasoning* |  |  |  |  |  |  |  |
| TempCompass | 67.9<sup>â€ </sup> | **68.3**<sup>âˆ—</sup> | 65.4 | **69.7**<sup>âˆ—</sup> | 64.9 | 56.8 | 68.1 |
| NextQA | 81.2<sup>âˆ—</sup> | **85.0**<sup>âˆ—</sup> | 83.2 | 82.2 | - | 75.6 | **84.5** |
| Charades-STA | - | - | - | - | - | - | **60.7** |{{< /table-caption >}}
> ğŸ”¼ í‘œ 8ì€ ë…¼ë¬¸ì—ì„œ ë‹¤ë£¨ëŠ” 7B ëª¨ë¸ì˜ ë¹„ë””ì˜¤ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  í‘œì—ëŠ” ì—¬ëŸ¬ ë¹„ë””ì˜¤ ì´í•´ ë²¤ì¹˜ë§ˆí¬(ì¼ë°˜ì ì¸ ë¹„ë””ì˜¤ ì´í•´, ì¥ê¸° ë¹„ë””ì˜¤ ì´í•´, ì‹œê°„ì  ì¶”ë¡ )ì—ì„œ ë‹¤ì–‘í•œ 7B ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•œ ê²°ê³¼ê°€ ìˆ˜ì¹˜ë¡œ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.  * í‘œì‹œëŠ” ì¬í˜„ëœ ê²°ê³¼ë¥¼, â€  í‘œì‹œëŠ” ê³µì‹ ë¦¬ë”ë³´ë“œì—ì„œ ê°€ì ¸ì˜¨ ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  ê°€ì¥ ì¢‹ì€ ê²°ê³¼ëŠ” êµµì€ ê¸€ì”¨ì²´ë¡œ, ë‘ ë²ˆì§¸ë¡œ ì¢‹ì€ ê²°ê³¼ëŠ” ë°‘ì¤„ë¡œ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.  ê° ë²¤ì¹˜ë§ˆí¬ëŠ” ëª¨ë¸ì˜ ë¹„ë””ì˜¤ ì´í•´ ëŠ¥ë ¥ì˜ íŠ¹ì • ì¸¡ë©´(ì˜ˆ: ì¥ê¸° ë¹„ë””ì˜¤ ì²˜ë¦¬, ì‹œê°„ì  ê´€ê³„ íŒŒì•…, ì¼ë°˜ì ì¸ ë¹„ë””ì˜¤ ì§ˆë¬¸ ì‘ë‹µ)ì„ í‰ê°€í•˜ë„ë¡ ê³ ì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì´ í‘œëŠ” VideoLLaMA3 ëª¨ë¸ì´ ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ì´í•´ ì‘ì—…ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œì„ ë³´ì—¬ì£¼ëŠ” ì£¼ìš” ê·¼ê±° ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 8: Evaluation results of 7B models on video benchmarks. * denotes the reproduced results. â€  denotes the results retrieved from the official leaderboard. The best results are in bold and the second best ones are underlined.
> </details>

{{< table-caption >}}
| Model | GQA | AI2D | ChartQA | DocVQA<sub>val</sub> | MME |
|---|---|---|---|---|---| 
| clip-vit-large-patch14-336 [radford2021learning] | 61.50 | 56.28 | 18.32 | 24.86 | 1668.41 |
| dfn5B-clip-vit-h-14-378 [fang2023data] | 62.70 | 56.87 | 16.40 | 23.09 | 1665.35 |
| siglip-so400m-patch14-384 [Zhai2023SigmoidLF] | 62.92 | 57.12 | 22.44 | 31.32 | 1667.92 |{{< /table-caption >}}
> ğŸ”¼ ë³¸ í‘œëŠ” ë¹„ì „ ì¸ì½”ë”ì— ëŒ€í•œ ablation study ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì„¸ ê°€ì§€ ì‚¬ì „ í›ˆë ¨ëœ ë¹„ì „ ì¸ì½”ë” (CLIP, DFN, SigLIP)ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•˜ê³ , ê° ì¸ì½”ë”ì˜ ì¥ë‹¨ì  ë° ì í•©í•œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. íŠ¹íˆ, SigLIP ì¸ì½”ë”ê°€ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ì„¸ë¶€ì ì¸ ì‹œê°ì  ì´í•´ ì‘ì—…ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ í™•ì¸í•˜ê³ , ë³¸ ë…¼ë¬¸ì—ì„œ SigLIP ì¸ì½”ë”ë¥¼ ê¸°ë³¸ ë¹„ì „ ì¸ì½”ë”ë¡œ ì±„íƒí•œ ì´ìœ ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 9: Ablation Study on Vision Encoders.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}