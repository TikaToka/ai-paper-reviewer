[{"content": "| Indices |  | Operations |  |\n|---|---|---|---|\n| <math>i</math> | Any indices | <math>\\cdot</math> (or omitted) | Matrix multiplication |\n| <math>s</math> | Index of current token | <math>\\odot</math> | Hadamard multiplication |\n| <math>t</math> | Index of chunk | Vectors and Matrices |  |\n| Constants |  | <math>\\mathbf{x}</math>, <math>\\mathbf{o}</math> <math>\\in\\mathbb{R}^{1\\times d}</math> | Input and output vectors |\n| <math>d</math> | Hidden dimension | <math>\\mathbf{q}</math>, <math>\\mathbf{k}</math>, <math>\\mathbf{v}</math> <math>\\in\\mathbb{R}^{1\\times d}</math> | Query, key, value vectors |\n| <math>W</math> | World size | <math>\\mathbf{X}</math>, <math>\\mathbf{O}</math> <math>\\in\\mathbb{R}^{N\\times d}</math> | Input and output matrices |\n| <math>N</math> | Sequence length | <math>\\mathbf{Q}</math>, <math>\\mathbf{K}</math>, <math>\\mathbf{V}</math> <math>\\in\\mathbb{R}^{N\\times d}</math> | Query, key, value matrices |\n| <math>T</math> | Total number of chunks | <math>\\mathbf{M}</math> <math>\\in\\mathbb{R}^{d\\times d}</math> | Memory state matrix |\n| <math>C</math> | Chunk length | <math>\\mathbf{W}_{Q}</math>, <math>\\mathbf{W}_{K}</math>, <math>\\mathbf{W}_{V}</math> <math>\\in\\mathbb{R}^{d\\times d}</math> | Weight matrices |", "caption": "Table 1: Notations. Indices, operations, constants, vectors and matrices used in the paper.", "description": "\ubcf8 \ub17c\ubb38\uc5d0 \uc0ac\uc6a9\ub41c \ud45c\uae30\ubc95\uc5d0 \ub300\ud55c \uc124\uba85\uc785\ub2c8\ub2e4. \uc0c9\uc778, \uc5f0\uc0b0, \uc0c1\uc218, \ubca1\ud130, \uadf8\ub9ac\uace0 \ud589\ub82c\uc744 \ud3ec\ud568\ud558\uc5ec \ud45c\uc5d0 \uc0ac\uc6a9\ub41c \ubaa8\ub4e0 \uae30\ud638\uc640 \uc57d\uc5b4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uae30\ud638\uc758 \uc758\ubbf8\uc640 \uc218\ud559\uc801 \ud45c\ud604\uc744 \uba85\ud655\ud788 \uc774\ud574\ud558\ub294\ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "2. Preliminary"}, {"content": "| Model | SP Method | Attention Module | Pure Model Thpt | Pure Model Loss | Hybrid Model Thpt | Hybrid Model Loss |\n|---|---|---|---|---|---|---|\n| Llama3 | Ring Attention | Standard Attention | 16549.5 | 2.759 | \\ | \\ |\n| Linear-Llama3 | LASP-2(H) | Basic Linear Attention | 17834.3 | 2.892 | 17394.7 | 2.824 |\n|  |  | Lightning Attention | 17926.1 | 2.862 | 17384.2 | 2.758 |\n|  |  | Retention | 17859.6 | 2.867 | 17352.5 | 2.759 |\n|  |  | GLA | 17785.3 | 2.845 | 17273.2 | 2.754 |\n|  |  | Based | 17946.1 | 2.754 | 17462.5 | 2.751 |\n|  |  | Rebased | 17896.2 | 2.845 | 17284.5 | 2.787 |", "caption": "Table 2: Convergence Performance Results. All experiments used 8 A100 GPUs, sequence length of 16K, and batch size of 8, trained on 50B tokens from the SlimPajama corpus.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud55c Linear-Llama3 \ubaa8\ub378\uc758 \uc218\ub834 \uc131\ub2a5 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub4e0 \uc2e4\ud5d8\uc740 SlimPajama \ub9d0\ubb49\uce58\uc5d0\uc11c 500\uc5b5 \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud558\uc5ec 8\uac1c\uc758 A100 GPU, \uc2dc\ud000\uc2a4 \uae38\uc774 16K, \ubc30\uce58 \ud06c\uae30 8\ub85c \uc9c4\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ubaa8\ub378 \uc885\ub958, \uc0ac\uc6a9\ub41c \uc2dc\ud000\uc2a4 \ubcd1\ub82c \ucc98\ub9ac(SP) \ubc29\ubc95, \uc5b4\ud150\uc158 \ubaa8\ub4c8, \ucc98\ub9ac\ub7c9(\ud1a0\ud070/\ucd08), \uc190\uc2e4 \uac12\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub294 \ub2e4\uc591\ud55c \uc124\uc815\uc5d0\uc11c LASP-2\uc758 \ud6a8\uc728\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Model | Training Loss | Validation Loss |\n|---|---|---|\n| RoBERTa Baseline (Ring Attention) | 1.815 | 1.957 |\n| RoBERTa with Basic Linear Attention (LASP-2) | 1.813 | 1.957 |", "caption": "Table 3: Convergence Performance on Bidirectional Language Modeling Task. Both training and validation loss values are reported.", "description": "\ud45c 3\uc740 \uc591\ubc29\ud5a5 \uc5b8\uc5b4 \ubaa8\ub378\ub9c1 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc218\ub834 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud6c8\ub828 \ubc0f \uac80\uc99d \uc190\uc2e4 \uac12\uc774 \ubaa8\ub450 \ubcf4\uace0\ub429\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \ubaa8\ub4c8(\uae30\ubcf8 \uc120\ud615 \uc5b4\ud150\uc158 \ud3ec\ud568)\uc744 \uc0ac\uc6a9\ud55c ROBERTa \ubaa8\ub378\uc758 \ud6c8\ub828 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788, LASP-2\ub97c \uae30\ubcf8 \uc120\ud615 \uc5b4\ud150\uc158 \ubaa8\ub4c8\uacfc \ud568\uaed8 \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \uc131\ub2a5\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Linear Sequence Modeling Module | 0 Hybrid (Pure Linear Model) | 1/8 Hybrid | 1/4 Hybrid | 1/2 Hybrid |\n|---|---|---|---|---|\n| Basic Linear Attention | 2.892 | 2.826 | 2.824 | 2.775 |\n| Lightning Attention | 2.848 | 2.756 | 2.750 | 2.742 |\n| Retention | 2.855 | 2.757 | 2.758 | 2.748 |\n| GLA | 2.845 | 2.751 | 2.754 | 2.753 |", "caption": "Table 4: Ablation Study on Hybrid Ratio in Hybrid Models. Loss values are reported in the Table. Note that pure linear models use LASP-2, while hybrid models use LASP-2H.", "description": "\ud45c 4\ub294 \uc21c\uc218 \uc120\ud615 \ubaa8\ub378\uacfc \ud558\uc774\ube0c\ub9ac\ub4dc \ubaa8\ub378\uc758 \uc190\uc2e4 \uac12\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc21c\uc218 \uc120\ud615 \ubaa8\ub378\uc740 LASP-2\ub97c, \ud558\uc774\ube0c\ub9ac\ub4dc \ubaa8\ub378\uc740 LASP-2H\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud558\uc774\ube0c\ub9ac\ub4dc \ubaa8\ub378\uc758 \uc120\ud615 \ub808\uc774\uc5b4\uc640 \ud45c\uc900 \uc5b4\ud150\uc158 \ub808\uc774\uc5b4\uc758 \ube44\uc728\uc744 \ub2ec\ub9ac\ud558\uc5ec \uc2e4\ud5d8\uc744 \uc9c4\ud589\ud558\uc600\uc73c\uba70, \uac01 \ubaa8\ub378\uc758 \uc190\uc2e4 \uac12\uc744 \ube44\uad50\ud568\uc73c\ub85c\uc368 \ud558\uc774\ube0c\ub9ac\ub4dc \ube44\uc728\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ud558\uc774\ube0c\ub9ac\ub4dc \ube44\uc728\uc774 0, 1/8, 1/4, 1/2\uc778 \ubaa8\ub378\uc758 \uc190\uc2e4 \uac12\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Split Size of Gathering | 2048 | 512 | 128 | 32 |\n|---|---|---|---|---|\n| Number of Splits | 1 | 4 | 16 | 64 |\n| Throughput | 486183 | 486166 | 486169 | 486158 |", "caption": "Table 5: Throughput Results (tokens/sec) on Varying Split Sizes of Gathering. Linear-Llama3-1B model (with 16 heads and hidden dimension of 2048) is used.", "description": "\uc774 \ud45c\ub294 Linear-Llama3-1B \ubaa8\ub378(\ud5e4\ub4dc 16\uac1c, \uc740\ub2c9 \ucc28\uc6d0 2048)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc218\uc9d1\ub41c \uba54\ubaa8\ub9ac \uc0c1\ud0dc\uc758 \ubd84\ud560 \ud06c\uae30\uc5d0 \ub530\ub978 \ucc98\ub9ac\ub7c9(\ud1a0\ud070/\ucd08)\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ubd84\ud560 \ud06c\uae30(2048, 512, 128, 64, 32, 16)\uc5d0\uc11c\uc758 \ucc98\ub9ac\ub7c9\uc744 \ube44\uad50\ud558\uc5ec \uba54\ubaa8\ub9ac \uc0c1\ud0dc \uc218\uc9d1\uc758 \ud6a8\uc728\uc131\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \ubd84\ud560 \ud06c\uae30\uac00 \uc791\uc744\uc218\ub85d \ubd84\ud560 \uc218\uac00 \ub9ce\uc544\uc9c0\uc9c0\ub9cc \ucc98\ub9ac\ub7c9\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc740 \ubbf8\ubbf8\ud558\uba70, \ub2e8\uc21c\ud788 \uc218\uc9d1 \uc5f0\uc0b0\uc758 \ud65c\uc6a9\ub9cc\uc73c\ub85c \ucc98\ub9ac\ub7c9 \ud5a5\uc0c1\uc774 \uc774\ub8e8\uc5b4\uc9c0\ub294 \uac83\uc774 \uc544\ub2d8\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.3. Scalability"}, {"content": "| Sequence Length | Number of GPUs | Throughput | Memory Usage Per GPU |\n|---|---|---|---| \n| **2K** | 16 | 1254 | 25.6 |\n|  | 32 | 1209 | 25.6 |\n|  | 64 | 1285 | 25.6 |\n|  | 128 | 1205 | 25.6 |\n| **4K** | 16 | 2478 | 25.6 |\n|  | 32 | 2446 | 25.6 |\n|  | 64 | 2327 | 25.6 |\n|  | 128 | 2344 | 25.6 |\n| **8K** | 16 | 4835 | 25.6 |\n|  | 32 | 4784 | 25.6 |\n|  | 64 | 4693 | 25.6 |\n|  | 128 | 4678 | 25.6 |\n| **16K** | 16 | 9530 | 25.6 |\n|  | 32 | 9494 | 25.6 |\n|  | 64 | 9305 | 25.6 |\n|  | 128 | 9313 | 25.6 |\n| **32K** | 16 | 18105 | 28.7 |\n|  | 32 | 17755 | 25.6 |\n|  | 64 | 17835 | 25.6 |\n|  | 128 | 17807 | 25.6 |\n| **64K** | 16 | 35507 | 33.8 |\n|  | 32 | 34240 | 28.7 |\n|  | 64 | 34118 | 25.6 |\n|  | 128 | 33344 | 25.6 |\n| **128K** | 16 | 68406 | 40.2 |\n|  | 32 | 68545 | 33.8 |\n|  | 64 | 67344 | 28.7 |\n|  | 128 | 66811 | 25.6 |\n| **256K** | 16 | 135635 | 57.8 |\n|  | 32 | 132605 | 40.2 |\n|  | 64 | 130215 | 33.8 |\n|  | 128 | 131550 | 28.7 |\n| **512K** | 16 | OOM | OOM |\n|  | 32 | 250586 | 57.8 |\n|  | 64 | 245353 | 40.2 |\n|  | 128 | 233442 | 33.8 |\n| **1024K** | 16 | OOM | OOM |\n|  | 32 | OOM | OOM |\n|  | 64 | 442221 | 57.8 |\n|  | 128 | 416465 | 40.2 |\n| **2048K** | 16 | OOM | OOM |\n|  | 32 | OOM | OOM |\n|  | 64 | OOM | OOM |\n|  | 128 | 769030 | 57.8 |\n| **4096K** | 16 | OOM | OOM |\n|  | 32 | OOM | OOM |\n|  | 64 | OOM | OOM |\n|  | 128 | OOM | OOM |", "caption": "Table 6: Quantitative Scalability Results of LASP-2 on Throughput (tokens/sec) and Memory Usage Per GPU (GB). Experiments are performed on Linear-Llama3-1B, scaling sequence length from 2K to 4096K.", "description": "\ud45c 6\uc740 LASP-2\uc758 \ucc98\ub9ac\ub7c9(\ud1a0\ud070/\ucd08) \ubc0f GPU\ub2f9 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9(GB)\uc5d0 \ub300\ud55c \uc815\ub7c9\uc801 \ud655\uc7a5\uc131 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Linear-Llama3-1B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2dc\ud000\uc2a4 \uae38\uc774\ub97c 2K\uc5d0\uc11c 4096K\uae4c\uc9c0 \ud655\uc7a5\ud558\uba74\uc11c \uc2e4\ud5d8\uc744 \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \ud45c\ub294 \ub2e4\uc591\ud55c \uc2dc\ud000\uc2a4 \uae38\uc774\uc640 GPU \uc218\uc5d0 \ub530\ub978 \ucc98\ub9ac\ub7c9\uacfc GPU\ub2f9 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788,  \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc740 GPU\uc758 \uc218\uc5d0 \ub530\ub77c \uc120\ud615\uc801\uc73c\ub85c \uc99d\uac00\ud558\uc9c0 \uc54a\uace0, \uc2dc\ud000\uc2a4 \uae38\uc774\uac00 \uae38\uc5b4\uc9d0\uc5d0 \ub530\ub77c GPU\ub2f9 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc774 \uae09\uaca9\ud788 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc2dc\ud000\uc2a4 \ubcd1\ub82c \ucc98\ub9ac \uc54c\uace0\ub9ac\uc998\uc758 \uba54\ubaa8\ub9ac \ud6a8\uc728\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.3. Scalability"}]