{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-05", "reason": "This paper introduces CLIP, a foundational vision-language model that has significantly influenced the field of multimodal learning and is frequently cited as a baseline in subsequent research."}, {"fullname_first_author": "Marah I Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "publication_date": "2024-04-14", "reason": "This paper introduces Phi-3, a powerful multimodal large language model (MLLM) that is used in the paper's methodology, enabling the generation of high-quality synthetic data."}, {"fullname_first_author": "Ting Jiang", "paper_title": "E5-V: universal embeddings with multimodal large language models", "publication_date": "2024-07-12", "reason": "This paper proposes E5-V, a strong multimodal embedding model, which serves as a direct comparison and competitor to the mmE5 model proposed in the current paper."}, {"fullname_first_author": "Ziyan Jiang", "paper_title": "VLM2Vec: Training vision-language models for massive multimodal embedding tasks", "publication_date": "2024-10-05", "reason": "This paper introduces VLM2Vec, a model that is benchmarked against the authors' model and whose approach to multimodal data generation is discussed in detail."}, {"fullname_first_author": "Junjie Zhou", "paper_title": "MegaPairs: Massive data synthesis for universal multimodal retrieval", "publication_date": "2024-12-14", "reason": "This paper discusses MegaPairs, a method for multimodal data synthesis, which is directly compared and contrasted with the proposed method in this paper."}]}