[{"content": "| Method | # Languages | Task | Modality Combinations | w/ MLLM | One Pass | Self-evaluation |\n|---|---|---|---|---|---|---|\n| MagicLens | 1 (English) | Retrieval | IT\u2192I | \u00d7 | \u221a | \u00d7 |\n| MegaPairs | 1 (English) | Retrieval | IT\u2192I | \u221a | \u00d7 | \u00d7 |\n| GME | 1 (English) | Retrieval | T\u2192IT, IT\u2192IT | \u00d7 | \u00d7 | \u00d7 |\n| mmE5 (Ours) | 93 (English, Spanish, etc.) | Classification, VQA, Retrieval | IT\u2192I, T\u2192IT, IT\u2192IT, I\u2192I, I\u2192T, IT\u2192T, T\u2192I | \u221a | \u221a | \u221a |", "caption": "Table 1: Comparison of the synthetic datasets in our work with those from previous methods.\nOur synthetic datasets incorporate 93 languages, two additional tasks, and more modality combinations.\n\u201cIT\u2192\u2192\\rightarrow\u2192T\u201d denotes a modality combination, where \u201cIT\u201d denotes images and texts on the query side and \u201cT\u201d denotes texts on the target side.\nThe entire data synthesis process is executed within a single pass of an MLLM, thereby avoiding potential information loss and ensuring robust cross-modal alignment.\nWe also employ real images and self-evaluation to maintain fidelity.", "description": "\ud45c 1\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ubc29\ubc95\uacfc \uae30\uc874 \ubc29\ubc95\ub4e4\uc758 \ud569\uc131 \ub370\uc774\ud130\uc14b\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. \ubcf8 \ub17c\ubb38\uc758 \ud569\uc131 \ub370\uc774\ud130\uc14b\uc740 93\uac1c \uc5b8\uc5b4, 2\uac00\uc9c0 \ucd94\uac00 \uacfc\uc81c, \uadf8\ub9ac\uace0 \ub354 \ub9ce\uc740 \ubaa8\ub2ec \uc870\ud569\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4.  'IT\u2192T'\ub294 \uc9c8\uc758 \uce21\uba74\uc5d0 \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8\uac00 \uc788\uace0, \ud0c0\uac9f \uce21\uba74\uc5d0 \ud14d\uc2a4\ud2b8\uac00 \uc788\ub294 \ubaa8\ub2ec \uc870\ud569\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ub370\uc774\ud130 \ud569\uc131 \uacfc\uc815 \uc804\uccb4\uac00 \ub300\uaddc\ubaa8 \ub2e4\uc911 \ubaa8\ub2ec \uc5b8\uc5b4 \ubaa8\ub378(MLLM)\uc758 \ub2e8\uc77c \ud328\uc2a4 \ub0b4\uc5d0\uc11c \uc218\ud589\ub418\uc5b4 \uc815\ubcf4 \uc190\uc2e4\uc744 \ubc29\uc9c0\ud558\uace0 \uac15\ub825\ud55c \uad50\ucc28 \ubaa8\ub2ec \uc815\ub82c\uc744 \ubcf4\uc7a5\ud569\ub2c8\ub2e4. \ub610\ud55c, \uc2e4\uc81c \uc774\ubbf8\uc9c0\uc640 \uc790\uccb4 \ud3c9\uac00\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucda9\uc2e4\ub3c4\ub97c \uc720\uc9c0\ud569\ub2c8\ub2e4.", "section": "3 Methodology: mmE5"}, {"content": "| 93 (English, Spanish, etc.) |", "caption": "Table 2: Results on MMEB benchmark, consisting of 36 tasks across four types: classification (Class.), VQA, retrieval (Retr.), and visual grounding (Ground.).\n\u2020 UniIR, MM-EMBED, and GME are not strictly zero-shot models.\nUniIR and MM-EMBED are trained on the MBEIR dataset\u00a0Wei et\u00a0al. (2024), which includes 10 retrieval datasets included in the MMEB.\nSimilarly, GME is trained on the UMRB dataset\u00a0Zhang et\u00a0al. (2024b), which shares 14 datasets with the MMEB.\nFor VLM2Vec, we use the LLaVA-based version with high-resolution images reported in its original paper.\nThe second-best performances are underlined and the best performances are in bold.", "description": "\ud45c 2\ub294 MMEB \ubca4\uce58\ub9c8\ud06c\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MMEB \ubca4\uce58\ub9c8\ud06c\ub294 \ubd84\ub958, VQA, \uac80\uc0c9, \uc2dc\uac01\uc801 \uae30\ubc18 \uc124\uc815 \ub4f1 \ub124 \uac00\uc9c0 \uc720\ud615\uc5d0 \uac78\uccd0 \ucd1d 36\uac00\uc9c0 \uacfc\uc81c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4.  UniIR, MM-EMBED, GME \ubaa8\ub378\uc740 \uc5c4\ubc00\ud788 \ub530\uc9c0\uba74 \uc81c\ub85c\uc0f7 \ubaa8\ub378\uc774 \uc544\ub2c8\uba70, UniIR\uacfc MM-EMBED\ub294 MBEIR \ub370\uc774\ud130\uc14b (Wei et al., 2024)\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ub418\uc5c8\uace0, \uc774 \ub370\uc774\ud130\uc14b\uc5d0\ub294 MMEB\uc5d0 \ud3ec\ud568\ub41c 10\uac1c\uc758 \uac80\uc0c9 \ub370\uc774\ud130\uc14b\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ub9c8\ucc2c\uac00\uc9c0\ub85c GME\ub294 UMRB \ub370\uc774\ud130\uc14b (Zhang et al., 2024b)\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ub418\uc5c8\uc73c\uba70, \uc774 \ub370\uc774\ud130\uc14b\uc740 MMEB\uc640 14\uac1c\uc758 \ub370\uc774\ud130\uc14b\uc744 \uacf5\uc720\ud569\ub2c8\ub2e4. VLM2Vec\uc758 \uacbd\uc6b0 \uc6d0 \ub17c\ubb38\uc5d0\uc11c \ubcf4\uace0\ub41c \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud55c LLaVA \uae30\ubc18 \ubc84\uc804\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.  \ub450 \ubc88\uc9f8\ub85c \uc88b\uc740 \uc131\ub2a5\uc740 \ubc11\uc904\uc774 \uadf8\uc5b4\uc838 \uc788\uace0, \ucd5c\uace0 \uc131\ub2a5\uc740 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4 Results"}, {"content": "| Classification, |\n|---|---| \n| VQA, Retrieval|", "caption": "Table 3: Results on XTD benchmark, a text-to-image retrieval task covering seven languages.", "description": "\ud45c 3\uc740 XTD \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. XTD \ubca4\uce58\ub9c8\ud06c\ub294 7\uac1c \uc5b8\uc5b4\ub97c \ud3ec\ud568\ud558\ub294 \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uac80\uc0c9 \uc791\uc5c5\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 Recall@10 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 7\uac1c \uc5b8\uc5b4\uc5d0 \ub300\ud55c \uc131\ub2a5\uacfc \ud3c9\uade0 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub2e4\uad6d\uc5b4 \uba40\ud2f0\ubaa8\ub2ec \uc784\ubca0\ub529 \ubaa8\ub378\uc758 \ub2e4\uad6d\uc5b4 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uace0 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4.3 XTD\uc5d0\uc11c\uc758 \ub2e4\uad6d\uc5b4 \uc131\ub2a5"}, {"content": "| IT\u2192I, T\u2192IT, IT\u2192IT, |\n|---|---|---| \n| I\u2192I, I\u2192T, IT\u2192T, T\u2192I |", "caption": "Table 4: Performances of mmE5 with different MLLMs.", "description": "\uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uae30\ucd08 \ub2e4\uc911 \ubaa8\ub4dc \uc5b8\uc5b4 \ubaa8\ub378(MLLM)\uc744 \uc0ac\uc6a9\ud558\uc5ec mmE5 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  mmE5\ub294 \uc81c\uc548\ub41c \uace0\ud488\uc9c8 \ud569\uc131 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\ub41c \ub2e4\uc911 \ubaa8\ub4dc \ub2e4\uad6d\uc5b4 \uc784\ubca0\ub529 \ubaa8\ub378\uc785\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 MLLM\uc744 \uae30\ubc18\uc73c\ub85c \ud559\uc2b5\ub41c mmE5 \ubaa8\ub378\uc758 MMEB \ubca4\uce58\ub9c8\ud06c \ud3c9\uade0 \uc810\uc218\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c MLLM\uc774 mmE5\uc758 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.4 \ub2e4\ub978 \uae30\ubcf8 MLLM\uc5d0 \ub300\ud55c \uc751\uc6a9"}, {"content": "| Models | Class. | VQA | Retr. | Ground. | IND | OOD | Overall |\n|---|---|---|---|---|---|---|---| \n| *Zero-shot Setting Models* |  |  |  |  |  |  |  |\n| CLIP [Radford et al. (2021)] | 42.8 | 9.1 | 53.0 | 51.8 | - | - | 37.8 |\n| BLIP2 [Li et al. (2023)] | 27.0 | 4.2 | 33.9 | 47.0 | - | - | 25.2 |\n| SigLIP [Zhai et al. (2023)] | 40.3 | 8.4 | 31.6 | 59.5 | - | - | 34.8 |\n| OpenCLIP [Cherti et al. (2023)] | 47.8 | 10.9 | 52.3 | 53.3 | - | - | 39.7 |\n| E5-V [Jiang et al. (2024a)] | 21.8 | 4.9 | 11.5 | 19.0 | - | - | 13.3 |\n| MagicLens [Zhang et al. (2024a)] | 38.8 | 8.3 | 35.4 | 26.0 | - | - | 27.8 |\n| MMRet (w/ 26M synthetic data) | 47.2 | 18.4 | **56.5** | 62.2 | - | - | 44.0 |\n| mmE5 (w/ 560K synthetic data) | **60.6** | **55.7** | 54.7 | **72.4** | - | - | **58.6** |\n| *Partially Supervised Finetuning Models*<sup>\u2020</sup> |  |  |  |  |  |  |  |\n| UniIR [Wei et al. (2024)] | 42.1 | 15.0 | 60.1 | 62.2 | - | - | 42.8 |\n| MM-EMBED [Lin et al. (2024)] | 48.1 | 32.2 | 63.8 | 57.8 | - | - | 50.0 |\n| GME [Zhang et al. (2024b)] | 56.9 | 41.2 | 67.8 | 53.4 | - | - | 55.8 |\n| *Supervised Finetuning Models* |  |  |  |  |  |  |  |\n| CLIP [Radford et al. (2021)] | 55.2 | 19.7 | 53.2 | 62.2 | 47.6 | 42.8 | 45.4 |\n| OpenCLIP [Cherti et al. (2023)] | 56.0 | 21.9 | 55.4 | 64.1 | 50.5 | 43.1 | 47.2 |\n| VLM2Vec [Jiang et al. (2024b)] | **61.2** | 49.9 | 67.4 | **86.1** | 67.5 | 57.1 | 62.9 |\n| MMRet [Zhou et al. (2024a)] | 56.0 | **57.4** | **69.9** | 83.6 | **68.0** | **59.1** | **64.1** |\n| mmE5 (w/ synthetic data + labeled data) | **67.6** | **62.8** | **70.9** | **89.7** | **72.3** | **66.7** | **69.8** |", "caption": "Table 5: Performances of ablated models on MMEB.\nFor efficient test, we conduct zero-shot experiments on 280K synthetic data, which has the same tasks, modality types and languages as the full synthetic data.", "description": "\ud45c 5\ub294 \uc81c\ud55c\ub41c \ubaa8\ub378\uc758 MMEB \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud6a8\uc728\uc801\uc778 \ud14c\uc2a4\ud2b8\ub97c \uc704\ud574 \ub3d9\uc77c\ud55c \uc791\uc5c5, \ubaa8\ub4dc \uc720\ud615 \ubc0f \uc5b8\uc5b4\ub97c \uac00\uc9c4 280K \ud569\uc131 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc81c\ub85c\uc0f7 \uc2e4\ud5d8\uc744 \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ud569\uc131 \ub370\uc774\ud130\uc758 \ub2e4\uc591\ud55c \uce21\uba74(\uc608: \uc2dc\uac01\uc801 \ud574\uc11d, \uc790\uccb4 \ud3c9\uac00, \ubd84\ub958 \ub370\uc774\ud130, VQA \ub370\uc774\ud130, \uac80\uc0c9 \ub370\uc774\ud130, \ud558\ub4dc \ub124\uac70\ud2f0\ube0c)\uc744 \uc81c\uac70\ud588\uc744 \ub54c mmE5 \ubaa8\ub378\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uac01 \uad6c\uc131 \uc694\uc18c\uc758 \uc911\uc694\uc131\uc744 \ud3c9\uac00\ud558\uace0 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.5 \ub370\uc774\ud130 \ud569\uc131 \ud504\ub85c\uc138\uc2a4 \ub17c\uc758"}, {"content": "| Model | it | es | ru | zh | pl | tr | ko | Avg. |\n|---|---|---|---|---|---|---|---|---|\n| ALIGN Jia et al. (2021) | 87.9 | 88.8 | 82.3 | 86.5 | 79.8 | 73.5 | 76.6 | 82.2 |\n| MURAL Jain et al. (2021) | 91.8 | 92.9 | 87.2 | 89.7 | 91.0 | 89.5 | 88.1 | 90.0 |\n| VLM2Vec Jiang et al. (2024b) | 83.7 | 87.1 | 86.7 | 92.8 | 76.1 | 37.2 | 63.9 | 75.4 |\n| jina Koukounas et al. (2024) | 93.6 | 94.1 | 89.8 | 91.8 | 94.3 | 92.7 | 90.1 | 92.3 |\n| M-CLIP Carlsson et al. (2022) | 93.1 | 93.6 | 90.0 | 94.0 | 94.3 | 93.1 | 89.0 | 92.4 |\n| GME Zhang et al. (2024b) | 95.1 | 96.4 | 92.3 | 96.4 | 94.9 | 89.8 | 93.6 | 94.1 |\n| mmE5 (full) | 96.1 | 96.2 | 93.3 | 96.3 | 95.4 | 93.6 | 96.0 | 95.3 |\n| w/ synthetic data only | 90.9 | 89.6 | 86.3 | 90.2 | 90.3 | 87.2 | 86.7 | 88.7 |\n| w/ english synthetic data | 86.3 | 86.3 | 84.2 | 88.8 | 84.9 | 81.0 | 84.4 | 85.1 |", "caption": "Table 6: Statistics of the multimodal synthetic data used for training mmE5.", "description": "\ud45c 6\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ub2e4\uc911 \ubaa8\ub2ec \ud569\uc131 \ub370\uc774\ud130\uc14b mmE5\uc758 \ud1b5\uacc4\ub7c9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub370\uc774\ud130\uc14b\uc740 \uc138 \uac00\uc9c0 \uc8fc\uc694 \ub2e4\uc911 \ubaa8\ub2ec \uc784\ubca0\ub529 \uc791\uc5c5(\ubd84\ub958, VQA, \uac80\uc0c9)\uacfc \uc77c\uacf1 \uac00\uc9c0 \ubaa8\ub2ec \uc870\ud569\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc73c\uba70,  \uac01 \uc791\uc5c5\uacfc \ubaa8\ub2ec \uc870\ud569\ubcc4\ub85c \uc0d8\ud50c \uc218\ub97c \uc0c1\uc138\ud788 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 mmE5 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc758 \uaddc\ubaa8\uc640 \ub2e4\uc591\uc131\uc744 \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.2 \ub370\uc774\ud130 \ud569\uc131 \ud504\ub808\uc784\uc6cc\ud06c"}, {"content": "| Base MLLM | Avg. on MMEB |\n|---|---| \n| Phi-3.5-V [Abdin et al. (2024)](https://arxiv.org/html/2502.08468/bib.bib1) | 61.0 |\n| LLaVA-1.6 [Liu et al. (2023a)](https://arxiv.org/html/2502.08468/bib.bib21) | 65.8 |\n| LLaMA-3-Vision [Meta (2024)](https://arxiv.org/html/2502.08468/bib.bib23) (Ours) | **69.8** |\n| *Baselines (For Reference)* |  |\n| VLM2Vec (Phi-3.5-V) | 60.1 |\n| VLM2Vec (LLaVA-1.6) | 62.9 |\n| MMRet (LLaVA-1.6) | 64.1 |\n| VLM2Vec (LLaMA-3.2) | 64.8 |", "caption": "Table 7: Detailed results of zero-shot setting and supervised setting models on each dataset of MMEB\u00a0Jiang et\u00a0al. (2024b).", "description": "\ud45c 7\uc740 Jiang et al.(2024b)\uc758 MMEB \ubca4\uce58\ub9c8\ud06c\uc5d0 \ud3ec\ud568\ub41c \uac01 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc81c\ub85c\uc0f7 \uc124\uc815\uacfc \uc9c0\ub3c4 \ud559\uc2b5 \uc124\uc815 \ubaa8\ub378\uc758 \uc0c1\uc138 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc81c\ub85c\uc0f7 \uc124\uc815\uc5d0\uc11c\ub294 CLIP, OpenCLIP, SigLIP, BLIP2, MagicLens, E5-V, MMRet, mmE5 \ub4f1 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uace0, \uc9c0\ub3c4 \ud559\uc2b5 \uc124\uc815\uc5d0\uc11c\ub294 VLM2Vec, MMRet, mmE5 \ubaa8\ub378\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \ubd84\ub958, VQA, \uac80\uc0c9, \uc2dc\uac01\uc801 \uadf8\ub77c\uc6b4\ub529 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \ub370\uc774\ud130\uc14b\ubcc4\ub85c \uc790\uc138\ud788 \uc81c\uc2dc\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \uc791\uc5c5 \uc720\ud615\ubcc4(\ubd84\ub958, VQA, \uac80\uc0c9, \uc2dc\uac01\uc801 \uadf8\ub77c\uc6b4\ub529) \ubc0f \ubaa8\ub4e0 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc810\uc218\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4 Experiments"}]