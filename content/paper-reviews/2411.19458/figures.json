[{"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/teaser.png", "caption": "Figure 1: Improving 3D correspondence understanding through finetuning on feature equivariance. Left: finetuning feature equivariance on one synthetic object can already enhance the vision transformer\u2019s ability to generate better 3D feature correspondences on general objects. Right: This improvement further leads to superior performance across multiple 3D tasks, including pose estimation, video tracking, and semantic correspondence.", "description": "\uadf8\ub9bc 1\uc740 \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\ud658\uc5d0 \ub300\ud55c \ubbf8\uc138 \uc870\uc815\uc744 \ud1b5\ud574 3D \ub300\uc751 \uad00\uacc4 \uc774\ud574\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd \ud328\ub110\uc740 \ud569\uc131 \uac1d\uccb4 \ud558\ub098\uc5d0 \ub300\ud55c \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\ud658 \ubbf8\uc138 \uc870\uc815\ub9cc\uc73c\ub85c\ub3c4 \uc77c\ubc18\uc801\uc778 \uac1d\uccb4\uc5d0 \ub300\ud55c 3D \ud2b9\uc9d5 \ub300\uc751 \uad00\uacc4 \uc0dd\uc131 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc624\ub978\ucabd \ud328\ub110\uc740 \uc774\ub7ec\ud55c \ud5a5\uc0c1\uc774 \uc790\uc138 \ucd94\uc815, \ube44\ub514\uc624 \ucd94\uc801, \uc758\ubbf8\ub860\uc801 \ub300\uc751 \uad00\uacc4\ub97c \ud3ec\ud568\ud55c \uc5ec\ub7ec 3D \uc791\uc5c5\uc5d0\uc11c \uc6b0\uc218\ud55c \uc131\ub2a5\uc73c\ub85c \uc774\uc5b4\uc9d0\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \ud569\uc131 \ub370\uc774\ud130\ub85c \ubbf8\uc138 \uc870\uc815\uc744 \ud1b5\ud574 \uc2e4\uc81c \ub370\uc774\ud130\uc5d0\uc11c\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc774\ub04c\uc5b4\ub0bc \uc218 \uc788\ub2e4\ub294 \uac83\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/horse.png", "caption": "Figure 2: Feature visualizations of different models. The sample image is rendered from Objeverse. Colors are computed from the high-dimensional features using PCA. We can see that MAE struggles to distinguish different parts of the content (e.g.similar features between head and body). Both CLIP and DeiT produce inconsistent features for the chest region between View 1 and View 2. DINOv2 gives the best correspondence.", "description": "\uadf8\ub9bc 2\ub294 \ub2e4\uc591\ud55c \ube44\uc804 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378(ViT)\uc758 \ud2b9\uc9d5\uc744 \uc2dc\uac01\ud654\ud558\uc5ec 3D \uacf5\uac04 \uad00\uacc4 \uc774\ud574 \ub2a5\ub825\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uadf8\ub9bc\uc785\ub2c8\ub2e4. Objaverse \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub80c\ub354\ub9c1\ub41c \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac01 \ubaa8\ub378\uc774 \ucd94\ucd9c\ud55c \uace0\ucc28\uc6d0 \ud2b9\uc9d5\uc744 PCA(\uc8fc\uc131\ubd84 \ubd84\uc11d)\ub97c \ud1b5\ud574 \uc800\ucc28\uc6d0\uc73c\ub85c \ubcc0\ud658\ud558\uace0 \uc0c9\uc0c1\uc73c\ub85c \ud45c\ud604\ud588\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc744 \ud1b5\ud574 MAE \ubaa8\ub378\uc740 \uac1d\uccb4\uc758 \uc11c\ub85c \ub2e4\ub978 \ubd80\ubd84(\uc608: \uba38\ub9ac\uc640 \ubab8\ud1b5)\uc744 \uad6c\ubd84\ud558\ub294 \ub370 \uc5b4\ub824\uc6c0\uc744 \uacaa\uace0, CLIP\uacfc DeiT \ubaa8\ub378\uc740 \ud2b9\uc815 \uc601\uc5ed(\uc608: \uac00\uc2b4 \ubd80\uc704)\uc5d0\uc11c \ub450 \uad00\uc810 \uac04\uc758 \ud2b9\uc9d5 \uc77c\uad00\uc131\uc774 \ubd80\uc871\ud568\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubc18\uba74\uc5d0 DINOv2 \ubaa8\ub378\uc740 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \uad00\uc810 \uac04\uc758 \ud2b9\uc9d5 \uc77c\uad00\uc131\uc774 \uac00\uc7a5 \ub192\uc544 3D \uacf5\uac04 \uad00\uacc4\ub97c \uac00\uc7a5 \uc798 \uc774\ud574\ud558\uace0 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2. \ub2e4\uc911 \uad00\uc810 \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\ud658 \ud3c9\uac00"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/correspondence_type.png", "caption": "Figure 3: Correlation between multiview feature equivariance and the task performances. Along the horizontal axis, lower APE indicates better feature equivariance, while the vertical axis reflects higher task performance across all four plots. The data points align roughly along the diagonal from the top left to the bottom right, suggesting a strong correlation between improved feature equivariance and better task performance.", "description": "\uadf8\ub9bc 3\uc740 \ub2e4\uc911 \ubdf0 \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\ud658\uacfc \uc791\uc5c5 \uc131\ub2a5 \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\ub85c\ucd95\uc740 \ub0ae\uc740 APE \uac12\uc774 \ub354 \ub098\uc740 \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\ud658\uc744 \ub098\ud0c0\ub0b4\ub294 \ubc18\uba74, \uc138\ub85c\ucd95\uc740 \ub124 \uac1c\uc758 \uadf8\ub798\ud504 \ubaa8\ub450\uc5d0\uc11c \ub354 \ub192\uc740 \uc791\uc5c5 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ub370\uc774\ud130 \uc810\ub4e4\uc740 \uc67c\ucabd \uc0c1\ub2e8\uc5d0\uc11c \uc624\ub978\ucabd \ud558\ub2e8\uc73c\ub85c \ub300\uac01\uc120 \ubc29\ud5a5\uc73c\ub85c \uac70\uc758 \uc77c\uc9c1\uc120\uc73c\ub85c \uc815\ub82c\ub418\uc5b4 \uc788\uc73c\uba70, \uc774\ub294 \ud5a5\uc0c1\ub41c \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\ud658\uacfc \ub354 \ub098\uc740 \uc791\uc5c5 \uc131\ub2a5 \uac04\uc758 \uac15\ub825\ud55c \uc0c1\uad00\uad00\uacc4\ub97c \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "2.1 \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\ud658\uc740 \ud2b9\uc815 \uc791\uc5c5 \uc131\ub2a5\uacfc \uc0c1\uad00\uad00\uacc4\uac00 \uc788\ub2e4"}, {"figure_path": "https://arxiv.org/html/2411.19458/x1.png", "caption": "Figure 4: Illustration of different types of correspondence tasks evaluated in our work.", "description": "\ubcf8 \ub17c\ubb38\uc758 \uadf8\ub9bc 4\ub294 \uc5f0\uad6c\uc5d0\uc11c \ud3c9\uac00\ud55c \uc138 \uac00\uc9c0 \uc720\ud615\uc758 \ub300\uc751 \uc791\uc5c5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc67c\ucabd\ubd80\ud130 \ucc28\ub840\ub85c, \ub2e8\uc77c \uac1d\uccb4\uc5d0 \ub300\ud55c \uac15\uccb4 \ubcc0\ud658(SE(3)) \ud558\uc5d0\uc11c\uc758 \ub300\uc751 \uad00\uacc4\ub97c \ud3c9\uac00\ud558\ub294 \uc790\uc138 \ucd94\uc815, \ube44\uac15\uccb4 \ub610\ub294 \uad00\uc808 \ubcc0\ud658 \ud558\uc5d0\uc11c \ub3d9\uc77c\ud55c \uac1d\uccb4\uc5d0 \ub300\ud55c \ub300\uc751 \uad00\uacc4\ub97c \ud3c9\uac00\ud558\ub294 \ube44\ub514\uc624 \ucd94\uc801, \uadf8\ub9ac\uace0 \uc758\ubbf8\ub860\uc801\uc73c\ub85c \uc720\uc0ac\ud55c \uc11c\ub85c \ub2e4\ub978 \uac1d\uccb4\ub4e4 \uac04\uc758 \ub300\uc751 \uad00\uacc4\ub97c \ud3c9\uac00\ud558\ub294 \uc758\ubbf8\ub860\uc801 \ub300\uc751\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uc791\uc5c5\uc758 \ubcf5\uc7a1\uc131\uc740 \uc790\uc138 \ucd94\uc815\uc5d0\uc11c \uc758\ubbf8\ub860\uc801 \ub300\uc751\uc73c\ub85c \uc99d\uac00\ud558\uba70, 3D \ub300\uc751 \uc774\ud574\uc758 \ub2e4\uc591\ud55c \uce21\uba74\uc744 \ud3ec\uad04\ud569\ub2c8\ub2e4.", "section": "2.1.1 TASK DEFINITIONS"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/finetune_tracking.png", "caption": "Figure 5: \nGeneralization from synthetic images (Objaverse) to real images (MVImgNet).\nLeft: Data points roughly around the diagonal from the bottom left to the upper right indicate the correlation between the APE tested on the two datasets. The * next to the model name means it is finetuned. All finetuning is done on Objaverse with only synthetic data.\nRight: Finetuned on Objaverse, the feature equivariance of the model (measured in PCDP) improves on MVImgNet.", "description": "\uadf8\ub9bc 5\ub294 \ud569\uc131 \uc774\ubbf8\uc9c0(Objaverse)\uc5d0\uc11c \uc2e4\uc81c \uc774\ubbf8\uc9c0(MVImgNet)\ub85c\uc758 \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd \uadf8\ub798\ud504\ub294 \ub450 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ud3c9\uac00\ub41c \ud3c9\uade0 \ud53d\uc140 \uc624\ucc28(APE) \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub370\uc774\ud130 \ud3ec\uc778\ud2b8\uac00 \uc67c\ucabd \uc544\ub798\uc5d0\uc11c \uc624\ub978\ucabd \uc704\ub85c \ub300\uac01\uc120 \ubc29\ud5a5\uc73c\ub85c \ubd84\ud3ec\ub418\uc5b4 \uc788\ub294 \uac83\uc740 \ub450 \ub370\uc774\ud130\uc14b \uac04\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4. \ubaa8\ub378 \uc774\ub984 \uc606\uc5d0 \uc788\ub294 * \ud45c\uc2dc\ub294 \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ubaa8\ub4e0 \ubbf8\uc138 \uc870\uc815\uc740 Objaverse\uc758 \ud569\uc131 \ub370\uc774\ud130\ub9cc \uc0ac\uc6a9\ud558\uc5ec \uc218\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc624\ub978\ucabd \uadf8\ub798\ud504\ub294 Objaverse\uc5d0\uc11c \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\uc758 \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\uc131(PCDP \uce21\uc815)\uc774 MVImgNet\uc5d0\uc11c \ud5a5\uc0c1\ub418\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2.1 \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\uc131\uc740 \ud2b9\uc815 \uc791\uc5c5 \uc131\ub2a5\uacfc \uc0c1\uad00\uad00\uacc4\uac00 \uc788\uc74c"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/diff_objs.png", "caption": "Figure 6: Feature visualization of DINOv2 before and after finetuning on MVImgNet objects (left two) and TAP-VID-DAVIS scenes (right one). For each example, we select three different views. The first column provides a reference color produced by PCA, while the second and third columns show the predicted feature correspondences. Our finetuned model demonstrates reduced noise and smoother feature boundaries, particularly noticeable in the reduction of jagged edges.", "description": "\uadf8\ub9bc 6\uc740 MVImgNet \uac1d\uccb4(\uc67c\ucabd \ub450 \uc5f4)\uacfc TAP-VID-DAVIS \uc7a5\uba74(\uc624\ub978\ucabd \ud55c \uc5f4)\uc5d0 \ub300\ud55c \ubbf8\uc138 \uc870\uc815 \uc804\ud6c4 DINOv2\uc758 \ud2b9\uc9d5 \uc2dc\uac01\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc608\uc2dc\uc5d0 \ub300\ud574 \uc138 \uac00\uc9c0 \ub2e4\ub978 \ubdf0\ub97c \uc120\ud0dd\ud588\uc2b5\ub2c8\ub2e4. \uccab \ubc88\uc9f8 \uc5f4\uc740 PCA\uc5d0 \uc758\ud574 \uc0dd\uc131\ub41c \uae30\uc900 \uc0c9\uc0c1\uc744 \uc81c\uacf5\ud558\uace0, \ub450 \ubc88\uc9f8\uc640 \uc138 \ubc88\uc9f8 \uc5f4\uc740 \uc608\uce21\ub41c \ud2b9\uc9d5 \ub300\uc751\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\uc740 \ud2b9\ud788 \ud1b1\ub2c8 \ubaa8\uc591 \uac00\uc7a5\uc790\ub9ac\uc758 \uac10\uc18c\uc5d0\uc11c \uc54c \uc218 \uc788\ub4ef\uc774 \ub178\uc774\uc988\uac00 \uc904\uace0 \ud2b9\uc9d5 \uacbd\uacc4\uac00 \ub354 \ub9e4\ub044\ub7ec\uc6cc\uc9d0\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ubbf8\uc138 \uc870\uc815\uc744 \ud1b5\ud574 3D \ud2b9\uc9d5 \ub300\uc751 \uc774\ud574\uac00 \ud5a5\uc0c1\ub418\uc5c8\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uac01\ub3c4\uc5d0\uc11c \ubcf8 \ub3d9\uc77c\ud55c \ubb3c\uccb4\uc758 \ud2b9\uc9d5 \ubca1\ud130\uac00 \uc5bc\ub9c8\ub098 \uc77c\uad00\uc131\uc788\uac8c \ud45c\ud604\ub418\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc785\ub2c8\ub2e4.  \ubbf8\uc138\uc870\uc815 \uc804\uc5d0\ub294 \ud2b9\uc9d5 \ubca1\ud130\uac00 \ubd88\uaddc\uce59\uc801\uc774\uace0 \uc7a1\uc74c\uc774 \ub9ce\uc9c0\ub9cc, \ubbf8\uc138\uc870\uc815 \ud6c4\uc5d0\ub294 \ub354\uc6b1 \ub9e4\ub044\ub7fd\uace0 \uc77c\uad00\ub41c \ud2b9\uc9d5 \ubca1\ud130\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3 FEATURE FINETUNING WITH MULTIVIEW EQUIVARIANCE"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/lerf.jpg", "caption": "Figure 7: One-shot pose estimation results before and after feature equivariance finetuning.", "description": "\uadf8\ub9bc 7\uc740 \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\ud658 \ubbf8\uc138 \uc870\uc815 \uc804\ud6c4\uc758 \uc6d0\uc0f7 \uc790\uc138 \ucd94\uc815 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ube44\uc804 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud574 OnePose-LowTex\uc640 YCB-Video \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \uc790\uc138 \ucd94\uc815 \uc815\ud655\ub3c4 \ubcc0\ud654\ub97c 1cm-1deg, 3cm-3deg, 5cm-5deg \uc624\ucc28 \ud55c\uacc4\ub97c \uae30\uc900\uc73c\ub85c \ud3c9\uac00\ud558\uc5ec \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubbf8\uc138 \uc870\uc815 \uc804\uacfc \ud6c4\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ube44\uad50\ud558\uc5ec \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\ud658 \ubbf8\uc138 \uc870\uc815\uc774 \uc6d0\uc0f7 \uc790\uc138 \ucd94\uc815 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uba85\ud655\ud788 \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uac01 \uadf8\ub798\ud504\ub294 \ubaa8\ub378\ubcc4 \ud3c9\uade0 \uc7ac\ud604\uc728 (AR) \ub610\ub294 \uc790\uc138 \uc815\ud655\ub3c4(%)\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "2.1.1 \uc791\uc5c5 \uc815\uc758 (TASK DEFINITIONS)"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/conv_exp.jpg", "caption": "Figure 8: Video tracking results before and after feature equivariance finetuning.", "description": "\uadf8\ub9bc 8\uc740 \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\ud658 \ubbf8\uc138 \uc870\uc815 \uc804\ud6c4\uc758 \ube44\ub514\uc624 \ucd94\uc801 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc5ec\ub7ec \ubaa8\ub378\uc5d0 \ub300\ud55c \ucd94\uc801 \uacb0\uacfc\uac00 \ub2e4\uc591\ud55c \uc0c9\uc0c1\uc73c\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \uc810\uc758 \uada4\uc801\uc740 \ube68\uac04\uc0c9 \uc120\uc73c\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4. \ubbf8\uc138 \uc870\uc815 \uc804\uc5d0\ub294 \uada4\uc801\uc774 \ub9e4\uc6b0 \ubd88\uaddc\uce59\ud558\uace0 \uc77c\uad00\uc131\uc774 \uc5c6\uc9c0\ub9cc, \ubbf8\uc138 \uc870\uc815 \ud6c4\uc5d0\ub294 \ucd94\uc801\uc774 \ud6e8\uc52c \ub354 \uc548\uc815\uc801\uc774\uace0 \uc815\ud655\ud574\uc9d1\ub2c8\ub2e4. \uc774\ub294 \ud2b9\uc9d5 \ub4f1\uac00\ubcc0\ud658 \ubbf8\uc138 \uc870\uc815\uc774 \ube44\ub514\uc624 \ucd94\uc801 \uc131\ub2a5\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2.1.1 \uc791\uc5c5 \uc815\uc758"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/hemisphere.jpg", "caption": "Figure 9: Semantic correspondence results before and after feature equivariance finetuneing.", "description": "\uadf8\ub9bc 9\ub294 \ud2b9\uc9d5 \ub4f1\ubcc0\ub7c9 \ubbf8\uc138 \uc870\uc815 \uc804\ud6c4\uc758 \uc758\ubbf8\uc801 \ub300\uc751 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\uc591\ud55c \ube44\uc804 \ubcc0\ud658\uae30 \ubaa8\ub378(DINOv2, DINOv2-Reg, MAE, CLIP, DeiT)\uc5d0 \ub300\ud574, \ub3d9\uc77c\ud55c \uac1d\uccb4\uc758 \uc11c\ub85c \ub2e4\ub978 \uad00\uc810\uc5d0\uc11c \ucd2c\uc601\ub41c \uc774\ubbf8\uc9c0 \uc30d\uc5d0 \ub300\ud55c \ud2b9\uc9d5 \ub9e4\uce6d \uc815\ud655\ub3c4\ub97c \ud3c9\uac00\ud569\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc5d0 \ub300\ud574, \ubbf8\uc138 \uc870\uc815 \uc804\uacfc \ud6c4\uc758 PCK(Percentage of Correct Keypoints)@0.05, PCK@0.10, PCK@0.15 \uc9c0\ud45c\ub97c \ub2e4\uc591\ud55c \uad00\uc810(\uc11c\ub85c \ub2e4\ub978 \uad00\uc810\uacfc \ub3d9\uc77c\ud55c \uad00\uc810)\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uc81c\uc548\ub41c \ubbf8\uc138 \uc870\uc815 \uae30\ubc95\uc774 \uc758\ubbf8\uc801 \ub300\uc751 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ud6a8\uacfc\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \ub3d9\uc77c\ud55c \uad00\uc810\uc758 \uc774\ubbf8\uc9c0 \uc30d\uc5d0\uc11c\ub294 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \ub354\uc6b1 \ub450\ub4dc\ub7ec\uc9d1\ub2c8\ub2e4.", "section": "2.1 \ud2b9\uc9d5 \ub4f1\ubcc0\ub7c9\uc131\uc740 \ud2b9\uc815 \uc791\uc5c5 \uc131\ub2a5\uacfc \uc0c1\uad00\uad00\uacc4\uac00 \uc788\ub2e4"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/bg_inv.png", "caption": "Figure 10: Finetuned performances w.r.t.\u00a0 #training objects. We evaluate the performances of the DINOv2 model finetuned with 0, 1, 5, 10, 20, 50, 100 objects on the three tasks.", "description": "\uadf8\ub9bc 10\uc740 DINOv2 \ubaa8\ub378\uc744 0, 1, 5, 10, 20, 50, 100\uac1c\uc758 \uac1d\uccb4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubbf8\uc138 \uc870\uc815\ud588\uc744 \ub54c \uc138 \uac00\uc9c0 \uacfc\uc81c(\uc790\uc138 \ucd94\uc815, \ube44\ub514\uc624 \ucd94\uc801, \uc758\ubbf8\ub860\uc801 \ub300\uc751)\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \uc81c\ud55c\ub41c \uc591\uc758 \ub370\uc774\ud130\ub9cc\uc73c\ub85c\ub3c4 3D \ub300\uc751 \uc774\ud574 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \ub2e8 \ud558\ub098\uc758 \uac1d\uccb4\ub9cc\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\uc744 \uc218\ud589\ud574\ub3c4 \uc0c1\ub2f9\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  \uc774\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \ud6a8\uc728\uc131\uacfc \uc2e4\uc6a9\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "3 FEATURE FINETUNING WITH MULTIVIEW EQUIVARIANCE"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/pose_estimation.jpg", "caption": "Figure 11: Finetuning with different objects. All results are tested with finetuned DINOv2. Dashed lines indicate the performances of the original pretrained model. The feature finetuning method is effective with as few as one single object. It also shows insensitivity to the specific choice of the object, even if the object has limited textures or is uncommon in daily life.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ubb3c\uccb4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud2b9\uc9d5 \ubbf8\uc138\uc870\uc815\uc744 \uc218\ud589\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub4e0 \uacb0\uacfc\ub294 \ubbf8\uc138\uc870\uc815\ub41c DINOv2 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud14c\uc2a4\ud2b8\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc810\uc120\uc740 \uc6d0\ub798 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ub2e8 \ud558\ub098\uc758 \ubb3c\uccb4\ub9cc\uc73c\ub85c\ub3c4 \ud2b9\uc9d5 \ubbf8\uc138\uc870\uc815 \ubc29\ubc95\uc774 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc8fc\uba70, \ubb3c\uccb4\uc758 \uc9c8\uac10\uc774 \uc81c\ud55c\uc801\uc774\uac70\ub098 \uc77c\uc0c1 \uc0dd\ud65c\uc5d0\uc11c \ud754\ud558\uc9c0 \uc54a\uc740 \ubb3c\uccb4\uc77c\uc9c0\ub77c\ub3c4 \ud2b9\uc815 \ubb3c\uccb4\uc758 \uc120\ud0dd\uc5d0 \ub300\ud55c \ubbfc\uac10\ub3c4\uac00 \ub0ae\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \ub2e4\uc591\ud55c \uc885\ub958\uc758 \ubb3c\uccb4\uc5d0 \ub300\ud574\uc11c\ub3c4 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "3 FEATURE FINETUNING WITH MULTIVIEW EQUIVARIANCE"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/tracking.jpg", "caption": "Figure 12: Finetuned DINOv2 performances w.r.t.\u00a0 #training iterations, trained with only one object over 0, 1, 5, 10, 20, 50, 100, 1000, 10000 training iterations.", "description": "\uadf8\ub9bc 12\ub294 \ud558\ub098\uc758 \ubb3c\uccb4\ub9cc\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub41c \ubc18\ubcf5 \ud69f\uc218(0, 1, 5, 10, 20, 50, 100, 1000, 10000)\uc5d0 \ub530\ub978 \ubbf8\uc138 \uc870\uc815\ub41c DINOv2\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub798\ud504\ub294 \ub2e4\uc591\ud55c \ud6c8\ub828 \ubc18\ubcf5 \ud69f\uc218\uc5d0 \ub530\ub978 \uc138 \uac00\uc9c0 \uc8fc\uc694 \uacfc\uc81c(One-shot Pose Estimation, Video Tracking, Semantic Correspondence)\uc5d0\uc11c\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788, \ub180\ub78d\uac8c\ub3c4 \ub2e8 \ud55c \ubc88\uc758 \ubc18\ubcf5\ub9cc\uc73c\ub85c\ub3c4 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \ub098\ud0c0\ub0a8\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uacb0\uacfc\uac00 \uc2dc\uac01\uc801\uc73c\ub85c \uc798 \ub098\ud0c0\ub098 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc81c\ud55c\ub41c \ub370\uc774\ud130\ub85c\ub3c4 3D \uacf5\uac04 \uad00\uacc4 \uc774\ud574 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "3 FEATURE FINETUNING WITH MULTIVIEW EQUIVARIANCE"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/semantic_transfer.jpg", "caption": "Figure 13: FiT and DINOv2 semantic correspondence visualization. We find that FiT significantly disrupts the semantics of certain parts.", "description": "\uadf8\ub9bc 13\uc740 FiT\uacfc DINOv2\uc758 \uc758\ubbf8\uc801 \ub300\uc751 \uad00\uacc4\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. FiT\uc740 \ud2b9\uc815 \ubd80\ubd84\uc758 \uc758\ubbf8\ub97c \uc0c1\ub2f9\ud788 \uc65c\uace1\ud558\ub294 \ubc18\uba74, DINOv2\ub294 \ub354 \ub098\uc740 \uc758\ubbf8\uc801 \uc77c\uad00\uc131\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4. \uc774\ub294 FiT\uc774 \uc774\ubbf8\uc9c0\uc758 \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud558\ub294 \ubc29\uc2dd\uc5d0 \ucc28\uc774\uac00 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  FiT\uc774 \ud2b9\uc815 \ubd80\ubd84\uc5d0 \ub300\ud55c \uc758\ubbf8\uc801 \uc815\ubcf4\ub97c \uc81c\ub300\ub85c \ud3ec\ucc29\ud558\uc9c0 \ubabb\ud558\uc5ec \uc758\ubbf8\uc801 \ub300\uc751 \uad00\uacc4\ub97c \uc815\ud655\ud558\uac8c \ud30c\uc545\ud558\uc9c0 \ubabb\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubc18\uba74, DINOv2\ub294 \uc774\ubbf8\uc9c0\uc758 \uc804\ubc18\uc801\uc778 \uc758\ubbf8\uc801 \ub9e5\ub77d\uc744 \uc798 \uc774\ud574\ud558\uc5ec \ub354 \uc815\ud655\ud55c \ub300\uc751 \uad00\uacc4\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 FiT\uacfc DINOv2\uc758 \ucc28\uc774\uc810\uc744 \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc8fc\uc5b4,  3D \uacf5\uac04\uc801 \uad00\uacc4\ub97c \uc774\ud574\ud558\ub294 \ub370 \uc788\uc5b4 FiT\uc758 \ud55c\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2.1 FEATURE EQUIVARIANCE CORRELATES TO CERTAIN TASK PERFORMANCES"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/semantic_depth.jpg", "caption": "Figure 14: Visualization of LERF relevancy maps for the query \u201cplate\u201d. Our finetuned DINO features produce a more focused and accurate relevancy map compared to the original DINO features, with better localization of the plate region and reduced noise in irrelevant areas such as cookies.", "description": "\uadf8\ub9bc 14\ub294 LERF\uc758 \uad00\ub828\uc131 \ub9f5 \uc2dc\uac01\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \"\uc811\uc2dc\"\ub77c\ub294 \uc9c8\uc758\uc5b4\uc5d0 \ub300\ud574, \ubbf8\uc138 \uc870\uc815\ub41c DINO \ud2b9\uc9d5\uc740 \uc6d0\ub798 DINO \ud2b9\uc9d5\uacfc \ube44\uad50\ud558\uc5ec \ub354\uc6b1 \uc9d1\uc911\uc801\uc774\uace0 \uc815\ud655\ud55c \uad00\ub828\uc131 \ub9f5\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc811\uc2dc \uc601\uc5ed\uc758 \uc704\uce58\ub97c \ub354 \uc798 \ud30c\uc545\ud558\uace0 \ucfe0\ud0a4\uc640 \uac19\uc740 \uad00\ub828 \uc5c6\ub294 \uc601\uc5ed\uc758 \ub178\uc774\uc988\ub97c \uc904\uc600\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ubbf8\uc138 \uc870\uc815\ub41c DINO \ud2b9\uc9d5\uc774 \ub354\uc6b1 \uac1c\uc120\ub41c \uc77c\ubc18\uc801\uc778 \ubaa9\uc801\uc758 \ud2b9\uc9d5\uc744 \uc0dd\uc131\ud558\uace0 \ub2e4\uc591\ud55c \uc751\uc6a9 \ud504\ub85c\uadf8\ub7a8\uc744 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Feature Finetuning with Multiview Equivariance"}, {"figure_path": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/sample_ycbv.png", "caption": "Figure 15: Comparison of feature visualizations with varying convolutional layers. Adding more than one convolutional layer introduces noise and reduces feature coherence, as shown by the highlighted regions.", "description": "\uadf8\ub9bc 15\ub294 \ub2e4\uc591\ud55c \ud569\uc131\uacf1\uce35\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud2b9\uc9d5 \uc2dc\uac01\ud654\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \ud558\ub098 \uc774\uc0c1\uc758 \ud569\uc131\uacf1\uce35\uc744 \ucd94\uac00\ud558\uba74 \uac15\uc870 \ud45c\uc2dc\ub41c \uc601\uc5ed\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774 \ub178\uc774\uc988\uac00 \ubc1c\uc0dd\ud558\uace0 \ud2b9\uc9d5\uc758 \uc77c\uad00\uc131\uc774 \uc800\ud558\ub429\ub2c8\ub2e4. \uc989, \uc5ec\ub7ec \uac1c\uc758 \ud569\uc131\uacf1 \uce35\uc744 \uc0ac\uc6a9\ud558\uba74 \uc2dc\uac01\uc801 \ud2b9\uc9d5 \ud45c\ud604\uc758 \uc9c8\uc774 \ub5a8\uc5b4\uc838\uc11c, \uc774\ubbf8\uc9c0\uc758 \uc138\ubd80 \uc815\ubcf4\ub97c \ub354 \uc798 \ub098\ud0c0\ub0b4\uc9c0 \ubabb\ud558\uace0, \uc624\ud788\ub824 \uc774\ubbf8\uc9c0 \ub0b4\ubd80\uc758 \ub178\uc774\uc988\ub098 \ubd80\uc815\ud655\ud55c \uc815\ubcf4\ub4e4\uc744 \ub354 \uac15\uc870\ud558\ub294 \uacb0\uacfc\ub97c \ubcf4\uc785\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc758 \uc131\ub2a5 \uc800\ud558\ub85c \uc774\uc5b4\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.1 \ucd94\uac00\uc801\uc778 \ud569\uc131\uacf1 \uacc4\uce35 \ud5e4\ub4dc"}, {"figure_path": "https://arxiv.org/html/2411.19458/x2.png", "caption": "Figure 16: Feature visualization of an untextured hemisphere from different viewpoints. Top row: Input hemisphere rendered from four different angles. Middle row: Feature embeddings from DINOv2 visualized using RGB mapping, showing inconsistent features across views and edges (highlighted by white circles). Bottom row: Our fine-tuned DINOv2 produces more consistent features that better preserve correspondences across viewpoints, particularly at edges and inward outward views.", "description": "\uadf8\ub9bc 16\uc740 \ud14d\uc2a4\ucc98\uac00 \uc5c6\ub294 \ubc18\uad6c\uc758 \ub2e4\uc591\ud55c \uc2dc\uc810\uc5d0\uc11c\uc758 \ud2b9\uc9d5 \uc2dc\uac01\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc0c1\ub2e8 \ud589\uc740 \ub124 \uac00\uc9c0 \ub2e4\ub978 \uac01\ub3c4\uc5d0\uc11c \ub80c\ub354\ub9c1\ub41c \uc785\ub825 \ubc18\uad6c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc911\uac04 \ud589\uc740 RGB \ub9e4\ud551\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2dc\uac01\ud654\ub41c DINOv2\uc758 \ud2b9\uc9d5 \uc784\ubca0\ub529\uc744 \ubcf4\uc5ec\uc8fc\ub294\ub370, \uc2dc\uc810 \uac04\uc5d0 \uc77c\uad00\uc131 \uc5c6\ub294 \ud2b9\uc9d5\uacfc \uac00\uc7a5\uc790\ub9ac(\ud770\uc0c9 \uc6d0\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub428)\uac00 \ub098\ud0c0\ub0a9\ub2c8\ub2e4. \ud558\ub2e8 \ud589\uc740 \ubbf8\uc138 \uc870\uc815\ub41c DINOv2\uac00 \uc2dc\uc810 \uac04\uc5d0 \ub300\uc751 \uad00\uacc4\ub97c \ub354 \uc798 \uc720\uc9c0\ud558\ub294 \ub354\uc6b1 \uc77c\uad00\ub41c \ud2b9\uc9d5\uc744 \uc0dd\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 \uac00\uc7a5\uc790\ub9ac\uc640 \uc548\ucabd/\ubc14\uae65\ucabd \uc2dc\uc810\uc5d0\uc11c \uadf8 \ud6a8\uacfc\uac00 \ub450\ub4dc\ub7ec\uc9d1\ub2c8\ub2e4.", "section": "3.1 \ubbf8\uc138 \uc870\uc815\ub41c \ud2b9\uc9d5 \ubd88\ubcc0\uc131 \ubc0f \uc77c\ubc18\ud654"}]