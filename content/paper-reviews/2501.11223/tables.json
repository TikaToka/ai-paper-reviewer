[{"content": "## Reasoning Language Models: A Blueprint\n\n| Scheme | Structure | Step | Strategy | Gen. | Ref. | Agg. | Pr. | Res. | Sel. | BT | Bp. | Inter. | Final. | PM | VM | Inf. | Tr. | DG | Remarks |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Explicit RLMs (Section 5.1)** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| rStar-Math [52] | E Tree | C Thought + Code Block | E MCTS | \u2705 | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |  |\n| PRIME [163, 38] | E Multiple Chains | F Token<br>C Thought | E Best-of-N | \u2705 | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |  |\n| Marco-o1 [174] | E Tree | F Token Sequence<br>C Thought | E MCTS | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |  |\n| Journey Learning (Tr.) [113] | E Tree | E Thought | E Tree Search | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | \u274c | \u2705 | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u00bd* | \u2705 | \u2705 | *Separate Entry |\n| OpenR [145] | E Tree | C Thought | E Best-of-N<br>E Beam Search<br>E MCTS | \u2705 | \u274c | \u274c | \u2705 | \u274c | \u2705 | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |  |\n| LLaMA-Berry [169] | E Tree of Chains | C Solution | E MCTS | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c | \u2705 | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |  |\n| ReST-MCTS* [170] | E Tree | C Thought | E MCTS | \u2705 | \u00bd* | \u274c | \u274c | \u274c | \u2705 | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | *Advice by critic |\n| AlphaMath Almost Zero [23] | E Tree | F Thought | E MCTS | \u2705 | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u00bd* | \u00bd* | \u2705 | \u2705 | *Single model |\n| MCTS-DPO [155] | E Tree | F Token Sequence | E MCTS | \u2705 | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u00bd* | \u00bd* | \u2705 | \u2705 | *Single model |\n| AlphaLLM [141] | E Tree | C Option | E MCTS | \u2705 | \u274c | \u274c | \u2705 | \u274c | \u2705 | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |  |\n| TS-LLM [45] | E Tree | F Token<br>F Sentence | E MCTS<br>E Tree Search | \u2705 | \u274c | \u274c | \u2705 | \u274c | \u2705 | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |  |\n| **Implicit RLMs (Section 5.2)** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| QwQ [140] | I Chain* | F Token | \u274c | \u2705 | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u00bd | \u274c | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | *Linearized Tree |\n| Journey Learning (Inf.) [113] | I Chain* | C Thought | I DFS | \u2705 | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u00bd | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c | *Linearized Tree |\n| **Structured Prompting Schemes (Section 5.3)** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Graph of Thoughts (GoT) [9] | E Graph* | C Thought | E Controller | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | \u274c | \u00bd | \u2705 | \u274c | *DAG |\n| Tree of Thoughts (ToT) [161] | E Tree | C Thought | E Tree Search | \u2705 | \u274c | \u274c | \u2705 | \u274c | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | \u274c | \u00bd | \u2705 | \u274c | \u274c |  |\n| Self-Consistency (SC) [150] | E Multiple Chains | C Thought | E Majority Voting | \u2705 | \u274c | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c |  |\n| Chain of Thought (CoT) [152] | I Chain | C Thought | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c | ", "caption": "TABLE I: Comparison of RLMs with respect to the provided taxonomy (Section\u00a04 and Figure\u00a05).\n\u201cReasoning\u201d: Details of the reasoning approach, specifically what is its Structure and its Strategy?\n\u201cReasoning Operator\u201d: Does a given scheme support operators on the reasoning structure? If yes, which classes (and specific functionalities) are supported Structure (\u201cGen.\u201d: generate, \u201cRef.\u201d: refine, \u201cAgg.\u201d: aggregate, \u201cPr.\u201d: prune, \u201cRes.\u201d: restructure), Traversal (\u201cSel\u201d: select, \u201cBT\u201d: backtrack), Update (\u201cBp.\u201d: backpropagate), and Evaluation of \u201cInter.\u201d: intermediate steps and \u201cFinal.\u201d: final steps?\n\u201cModel\u201c: Does a given scheme use models to implement its operators and if so, which ones (\u201cPM\u201d: policy model, \u201cVM\u201d: value model)?\n\u201cPipeline\u201d: Which pipelines are harnessed by a given scheme (\u201cInf.\u201d: inference, Tr.\u201d: training, \u201cDG\u201d: data generation)?\nWhen describing representations, we use the following abbreviations:\n\u201cE\u201d: explicit,\n\u201cI\u201d: implicit.\n\u201cF\u201d: fine-grained.\n\u201cC\u201d: coarse-grained.\n\u201c\\faBatteryFull\u201d: full support (i.e., YES),\n\u201c\\faBatteryHalf\u201d: partially [supported],\n\u201c\\faTimes\u201d: no support (i.e., NO).", "description": "\ud45c\ub294 \ub17c\ubb38\uc758 4\uc7a5\uacfc \uadf8\ub9bc 5\uc808\uc5d0\uc11c \uc81c\uc2dc\ub41c \ubd84\ub958 \uccb4\uacc4\uc5d0 \ub530\ub77c \ub2e4\uc591\ud55c \ucd94\ub860 \uc5b8\uc5b4 \ubaa8\ub378(RLM)\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  '\ucd94\ub860' \uc5f4\uc5d0\uc11c\ub294 \uac01 RLM\uc758 \ucd94\ub860 \uad6c\uc870(\uccb4\uc778, \ud2b8\ub9ac, \uadf8\ub798\ud504 \ub4f1)\uc640 \uc804\ub7b5(MCTS, \ube54 \uc11c\uce58 \ub4f1)\uc744 \uad6c\uccb4\uc801\uc73c\ub85c \uc124\uba85\ud569\ub2c8\ub2e4. '\ucd94\ub860 \uc5f0\uc0b0\uc790' \uc5f4\uc5d0\uc11c\ub294 \uc8fc\uc5b4\uc9c4 RLM\uc774 \ucd94\ub860 \uad6c\uc870\uc5d0 \ub300\ud55c \uc5f0\uc0b0(\uc0dd\uc131, \uac1c\uc120, \uc9d1\uacc4, \uc81c\uac70, \uc7ac\uad6c\uc131, \ud0d0\uc0c9, \ubc31\ud2b8\ub798\ud0b9, \uc5c5\ub370\uc774\ud2b8 \ub4f1)\uc744 \uc9c0\uc6d0\ud558\ub294\uc9c0 \uc5ec\ubd80\uc640 \uad6c\uccb4\uc801\uc778 \uae30\ub2a5\ub4e4\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. '\ubaa8\ub378' \uc5f4\uc5d0\uc11c\ub294 \uc5f0\uc0b0\uc790 \uad6c\ud604\uc5d0 \uc0ac\uc6a9\ub41c \ubaa8\ub378(\uc815\ucc45 \ubaa8\ub378, \uac00\uce58 \ubaa8\ub378)\uc758 \uc885\ub958\ub97c \ubcf4\uc5ec\uc8fc\uace0, '\ud30c\uc774\ud504\ub77c\uc778' \uc5f4\uc5d0\uc11c\ub294 \uac01 RLM\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \ud30c\uc774\ud504\ub77c\uc778(\ucd94\ub860, \ud559\uc2b5, \ub370\uc774\ud130 \uc0dd\uc131)\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uc57d\uc5b4 'E'\ub294 \uba85\uc2dc\uc801, 'I'\ub294 \uc554\uc2dc\uc801, 'F'\ub294 \uc138\ubd84\ud654\ub41c, 'C'\ub294 \uc870\uc545\ud55c\uc744 \uc758\ubbf8\ud558\uba70, \ubc30\ud130\ub9ac \uc544\uc774\ucf58\uc740 \uac01 \uae30\ub2a5 \uc9c0\uc6d0 \uc815\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  (\uaf49 \ucc2c \ubc30\ud130\ub9ac: \uc644\uc804 \uc9c0\uc6d0, \ubc18\ucbe4 \ucc2c \ubc30\ud130\ub9ac: \ubd80\ubd84 \uc9c0\uc6d0, X: \uc9c0\uc6d0 \uc548 \ud568)", "section": "4. BLUEPRINT FOR REASONING LLMS"}, {"content": "| Symbol | Description |\n|---|---| \n| \\mathcal{M}=(\\mathcal{S},\\mathcal{A},p,r,\\gamma) | Markov Decision Process (MDP) definition. |\n| s\\in\\mathcal{S} | A state in the state space, representing a sequence of reasoning steps. |\n| a\\in\\mathcal{A} | An action in the action space, corresponding to selecting the next reasoning step. |\n| \\mathcal{A}_{s}\\subseteq\\mathcal{A} | a set of actions available in state <math>s</math>. |\n| p(s^{\\prime}\\mid s,a) | The probability of transition to state <math>s^{\\prime}</math> from state <math>s</math> taking action <math>a</math> in state <math>s</math>. |\n| r(s) | The reward received when arriving in state <math>s</math>. |\n| \\gamma\\in[0,1] | Discount factor, determining the present value of future rewards. |\n| \\pi_{\\theta}(a\\mid s) | Policy parameterized by <math>\\theta</math>, representing the probability of taking action <math>a</math> in state <math>s</math>. |\n| V_{\\pi}(s) | Value function under policy <math>\\pi</math>, representing the expected return starting from state <math>s</math>. |\n| Q_{\\pi}(s,a) | State-action value function under policy <math>\\pi_{\\theta}</math>, representing the expected return of taking action <math>a</math> in state <math>s</math>. |\n| \\tau_{\\pi} | A trajectory consisting of states and actions, <math>(s_{0},a_{0},s_{1},\\ldots,s_{T+1})</math> following policy <math>\\pi</math>. |", "caption": "TABLE II: Overview of mathematical notation used in the paper", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc218\ud559\uc801 \ud45c\uae30\ubc95\uc5d0 \ub300\ud55c \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uae30\ud638\uc640 \uadf8\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \uac04\ub7b5\ud558\uac8c \uc81c\uc2dc\ud558\uc5ec \ub17c\ubb38\uc758 \uc218\ud559\uc801 \ub0b4\uc6a9\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.  \ud45c\uae30\ubc95\uc5d0\ub294 \ub9c8\ub974\ucf54\ud504 \uacb0\uc815 \uacfc\uc815(MDP), \ubaac\ud14c \uce74\ub97c\ub85c \ud2b8\ub9ac \ud0d0\uc0c9(MCTS) \uc54c\uace0\ub9ac\uc998, \ubcf4\uc0c1 \ubc0f \uac00\uce58 \ubaa8\ub378 \ub4f1\uc5d0 \uad00\ub828\ub41c \ud45c\uae30\ubc95\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "A.1 \ub9c8\ub974\ucf54\ud504 \uacb0\uc815 \uacfc\uc815"}]