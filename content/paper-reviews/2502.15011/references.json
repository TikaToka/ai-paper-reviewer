{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-XX-XX", "reason": "This paper introduced CLIP, a foundational model for vision-language understanding that heavily influenced the proposed method's cross-modal alignment approach."}, {"fullname_first_author": "Rohit Girdhar", "paper_title": "ImageBind: One embedding space to bind them all", "publication_date": "2023-XX-XX", "reason": "ImageBind's unified embedding space for multiple modalities directly inspired the design of CrossOver's modality-agnostic embedding space."}, {"fullname_first_author": "Ziyu Guo", "paper_title": "PointCLIP: Point cloud understanding by CLIP", "publication_date": "2021-XX-XX", "reason": "PointCLIP's extension of CLIP to 3D data provided a starting point for handling point cloud data within CrossOver's multimodal framework."}, {"fullname_first_author": "Le Xue", "paper_title": "ULIP: Learning unified representation of language, image and point cloud for 3D understanding", "publication_date": "2022-XX-XX", "reason": "ULIP's work on unified representations for images, text, and point clouds is closely related to CrossOver's goal of creating a unified embedding space for various 3D scene modalities."}, {"fullname_first_author": "Yang Miao", "paper_title": "SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs", "publication_date": "2024-XX-XX", "reason": "SceneGraphLoc's approach to cross-modal visual localization, particularly its use of scene graphs, informed the development of CrossOver's scene-level modality alignment strategy."}]}