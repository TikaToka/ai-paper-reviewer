[{"content": "| R-1 | R-2 | R-L |\n|---|---|---|\n| **Long-document QA** |  |  |  |\n| \u221eBench QA (Zhang et al., 2024) | 0.966 | 0.545 | 0.960 |\n| \u221eBench MC (Zhang et al., 2024) | 0.946 | 0.506 | 0.932 |\n| **RAG-style (Multi-doc) QA** |  |  |  |\n| RULER QA (Hsieh et al., 2024) | 0.809 | 0.437 | 0.693 |\n| HELMET (RAG) (Yen et al., 2024) | 0.689 | 0.304 | 0.555 |\n| **Recall-based** |  |  |  |\n| Vanilla NIAH (Kamradt, 2023) | 0.905 | 0.789 | 0.855 |\n| RULER S-NIAH (Hsieh et al., 2024) | 0.571 | 0.461 | 0.500 |\n| BABILong (0K) (Kuratov et al., 2024) | 0.553 | 0.238 | 0.522 |\n| **NoLiMa** | **0.069** | **0.002** | **0.067** |", "caption": "Table 1: ROUGE precision scores between the input document and the question: higher ROUGE scores indicate greater literal matches between the question and the relevant context.", "description": "\ud45c 1\uc740 \uc9c8\ubb38\uacfc \uad00\ub828\ub41c \ucee8\ud14d\uc2a4\ud2b8 \uc0ac\uc774\uc758 \ubb38\uc790 \uc77c\uce58 \uc815\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\ub294 ROUGE \uc815\ubc00\ub3c4 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ROUGE \uc810\uc218\ub294 \uc785\ub825 \ubb38\uc11c\uc640 \uc9c8\ubb38 \uac04\uc758 \uc5b4\ud718 \uc911\ubcf5\uc744 \uce21\uc815\ud569\ub2c8\ub2e4.  \uc810\uc218\uac00 \ub192\uc744\uc218\ub85d \uc9c8\ubb38\uacfc \uad00\ub828 \ucee8\ud14d\uc2a4\ud2b8 \uac04\uc758 \ubb38\uc790 \uc77c\uce58\uac00 \ub9ce\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc774 \ubb38\uc790 \uc77c\uce58\ub97c \uc774\uc6a9\ud558\uc5ec \uc791\uc5c5\uc744 \ub2e8\uc21c\ud654\ud560 \uc218 \uc788\ub294\uc9c0 \ud3c9\uac00\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "2. \uad00\ub828 \uc5f0\uad6c"}, {"content": "| Question | Needles | Keyword Types |\n|---|---|---|\n| Which character has been to $W_q$? | Def. | $W_n$ | Buildings & Landmarks |\n| $W_n$ is next to where [CHAR] lives. | Inv. | $W_q$ | Countries, cities, states |", "caption": "Table 2: An example template of the proposed needle set in NoLiMa (all templates are available in Appendix A.) The placeholders [CHAR], Wqsubscript\ud835\udc4a\ud835\udc5eW_{q}italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT, and Wnsubscript\ud835\udc4a\ud835\udc5bW_{n}italic_W start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT represent the randomly selected character (also the answer), the query keyword, and the needle keyword, respectively. Def.:\ndefault order. Inv.: inverted order.", "description": "\ud45c 2\ub294 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 NoLiMa\uc758 \uc608\uc2dc \ub2c8\ub4e4 \uc138\ud2b8 \ud15c\ud50c\ub9bf\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubd80\ub85d A\uc5d0\ub294 \ubaa8\ub4e0 \ud15c\ud50c\ub9bf\uc774 \uc788\uc2b5\ub2c8\ub2e4. [CHAR],  Wq, Wn\uc740 \ubb34\uc791\uc704\ub85c \uc120\ud0dd\ub41c \uce90\ub9ad\ud130(\uc815\ub2f5), \uc9c8\uc758 \ud0a4\uc6cc\ub4dc, \ub2c8\ub4e4 \ud0a4\uc6cc\ub4dc\ub97c \uac01\uac01 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. Def.\ub294 \uae30\ubcf8 \uc21c\uc11c, Inv.\ub294 \ubc18\uc804 \uc21c\uc11c\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 NoLiMa \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \uc9c8\ubb38\uacfc \ub2c8\ub4e4\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc774\uba70, \uc9c8\ubb38\uacfc \ub2c8\ub4e4 \uc0ac\uc774\uc758 \uc5b4\ud718 \uc911\ubcf5\uc744 \ucd5c\uc18c\ud654\ud558\uc5ec \ubaa8\ub378\uc774 \ub2e8\uc21c\ud55c \ud45c\uba74\uc801\uc778 \ub9e4\uce6d\uc774 \uc544\ub2cc \uc7a0\uc7ac\uc801\uc778 \uc5f0\uad00\uc131\uc744 \ud30c\uc545\ud574\uc57c \ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. NoLiMa"}, {"content": "| Models | Claimed Length | Effective Length | Base Score | 1K | 2K | 4K | 8K | 16K | 32K |\n|---|---|---|---|---|---|---|---|---|---| \n| GPT-4o | 128K | 8K | 99.3 (84.4) | 98.1 | 98.0 | 95.7 | 89.2 | 81.6 | 69.7 |\n| Llama 3.3 70B | 128K | 2K | 97.3 (82.7) | 94.2 | 87.4 | 81.5 | 72.1 | 59.5 | 42.7 |\n| Llama 3.1 405B | 128K | 2K | 94.7 (80.5) | 89.0 | 85.0 | 74.5 | 60.1 | 48.4 | 38.0 |\n| Llama 3.1 70B | 128K | 2K | 94.5 (80.3) | 91.0 | 81.8 | 71.2 | 62.7 | 51.8 | 43.2 |\n| Gemini 1.5 Pro | 2M | 2K | 92.6 (78.7) | 86.4 | 82.7 | 75.4 | 63.9 | 55.5 | 48.2 |\n| Jamba 1.5 Mini | 256K | <1K | 92.4 (78.6) | 76.3 | 74.1 | 70.8 | 62.2 | 52.7 | 43.6 |\n| Command R+ | 128K | <1K | 90.9 (77.3) | 77.0 | 73.5 | 66.3 | 39.5 | 21.3 | 7.4 |\n| Mistral Large 2 | 128K | 2K | 87.9 (74.7) | 86.1 | 85.5 | 73.3 | 51.5 | 32.6 | 18.7 |\n| Claude 3.5 Sonnet | 200K | 4K | 87.6 (74.4) | 85.4 | 84.0 | 77.6 | 61.7 | 45.7 | 29.8 |\n| Gemini 1.5 Flash | 1M | <1K | 84.7 (72.0) | 68.6 | 61.6 | 51.0 | 44.4 | 35.5 | 28.6 |\n| GPT-4o mini | 128K | <1K | 84.9 (72.2) | 67.7 | 58.2 | 44.1 | 32.6 | 20.6 | 13.7 |\n| Llama 3.1 8B | 128K | 1K | 76.7 (65.2) | 65.7 | 54.4 | 44.1 | 31.9 | 22.6 | 14.2 |", "caption": "Table 3: NoLiMa benchmark results on the selected models. Following Hsieh et\u00a0al. (2024), we report the effective length alongside the claimed supported context length for each model. However, we define the effective length as the maximum length at which the score remains above a threshold set at 85% of the model\u2019s base score (shown in parentheses). Scores exceeding this threshold are underlined. Scores that are below 50% of the base score are shaded in red.", "description": "\ud45c 3\uc740 \ub17c\ubb38\uc5d0\uc11c \uc120\ud0dd\ub41c \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud55c NoLiMa \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Hsieh et al. (2024)\uc758 \uc5f0\uad6c\ub97c \ub530\ub77c, \uac01 \ubaa8\ub378\uc758 \uc8fc\uc7a5\ub418\ub294 \ucd5c\ub300 \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774\uc640 \ud568\uaed8 \uc2e4\uc81c \ud6a8\uacfc\uc801\uc778 \uae38\uc774\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ud569\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 \uc810\uc218\uac00 \ubaa8\ub378\uc758 \uae30\uc900 \uc810\uc218(\uad04\ud638 \uc548)\uc758 85% \uc774\uc0c1\uc744 \uc720\uc9c0\ud558\ub294 \ucd5c\ub300 \uae38\uc774\ub97c \uc2e4\uc81c \ud6a8\uacfc\uc801\uc778 \uae38\uc774\ub85c \uc815\uc758\ud569\ub2c8\ub2e4. \uae30\uc900 \uc810\uc218\ub97c \ucd08\uacfc\ud558\ub294 \uc810\uc218\ub294 \ubc11\uc904\uc774 \uadf8\uc5b4\uc838 \uc788\uace0, \uae30\uc900 \uc810\uc218\uc758 50% \ubbf8\ub9cc\uc778 \uc810\uc218\ub294 \ube68\uac04\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Results"}, {"content": "| | 4K | 8K | 16K | 32K |\n|---|---|---|---|---|\n| _One-hop_ |  |  |  |  |\n| - w/o CoT | 90.3 | 84.1 | 73.2 | 56.2 |\n| - w/ CoT | 95.6 | 91.1 | 82.6 | 60.6 |\n| Increase rate | 5.9% | 8.3% | 12.8% | 7.8% |\n| _Two-hop_ |  |  |  |  |\n| - w/o CoT | 70.7 | 57.4 | 42.7 | 25.9 |\n| - w/ CoT | 82.4 | 70.1 | 56.7 | 34.3 |\n| Increase rate | 16.5% | 22.1% | 32.7% | 32.4% |", "caption": "Table 4: Comparison of Chain-of-Thought (CoT) improvements in performance for Llama 3.3 70B, evaluated on both one-hop and two-hop tests.", "description": "\ud45c 4\ub294 Llama 3.3 70B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud55c \ub2e8\uacc4 \ucd94\ub860(one-hop) \ubc0f \ub450 \ub2e8\uacc4 \ucd94\ub860(two-hop) \uc791\uc5c5\uc5d0 \ub300\ud574 Chain-of-Thought(CoT) \ud504\ub86c\ud504\ud305\uc774 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  CoT \ud504\ub86c\ud504\ud305\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c\uc640 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\uc758 \uc815\ud655\ub3c4 \ubcc0\ud654\uc728\uc744 4K, 8K, 16K, 32K \ud1a0\ud070 \uae38\uc774\ubcc4\ub85c \ubcf4\uc5ec\uc8fc\uc5b4, CoT \ud504\ub86c\ud504\ud305\uc774 \ubaa8\ub378\uc758 \ucd94\ub860 \ub2a5\ub825 \ud5a5\uc0c1\uc5d0 \uc5b4\ub5bb\uac8c \uae30\uc5ec\ud558\ub294\uc9c0, \uadf8\ub9ac\uace0 \uadf8 \ud6a8\uacfc\uac00 \ud1a0\ud070 \uae38\uc774\uc5d0 \ub530\ub77c \uc5b4\ub5bb\uac8c \ub2ec\ub77c\uc9c0\ub294\uc9c0 \ubd84\uc11d\ud569\ub2c8\ub2e4. \ud2b9\ud788, \ub450 \ub2e8\uacc4 \ucd94\ub860 \uc791\uc5c5\uc5d0\uc11c CoT \ud504\ub86c\ud504\ud305\uc758 \ud6a8\uacfc\uac00 \ub354 \ud06c\uac8c \ub098\ud0c0\ub098\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.4 Results"}, {"content": "|           | Base       | 4K  | 8K  | 16K | 32K |\n|-----------|------------|-----|-----|-----|-----|\n|           | **Score**   | **4K** | **8K** | **16K** | **32K** |\n|           | <em style=\"font-style:italic\">Llama 3.3 70b</em> |       |       |       |       |\n| - w/o CoT | 98.3        | 55.5 | 37.2 | 16.7 | 8.9  |\n| - w/ CoT  | 97.1        | 73.0 | 51.2 | 31.8 | 10.1 |\n|           | <em style=\"font-style:italic\">Reasoning models</em> |       |       |       |       |\n| GPT-o1    | 99.9        | 92.0 | 78.0 | 60.1 | 31.1 |\n| GPT-o3 Mini| 98.8        | 52.8 | 36.9 | 25.5 | 18.9 |\n| DeepSeek R1-DL-70b | 99.9        | 91.4 | 75.5 | 49.4 | 20.7 |", "caption": "Table 5: Evaluation results of NoLiMa-Hard: Scores falling below 50% of the base score are highlighted in red.", "description": "\ud45c 5\ub294 NoLiMa-Hard\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. NoLiMa-Hard\ub294 \uae30\uc874 NoLiMa \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uac00\uc7a5 \uc5b4\ub824\uc6b4 \uc9c8\ubb38-\ub2c8\ub4e4(needle) \uc30d 10\uac1c\ub97c \uc120\ubcc4\ud55c \ud558\uc704 \uc9d1\ud569\uc785\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub378\uc758 \uae30\ubcf8 \uc810\uc218(base score)\uc640 8K, 16K, 32K \ud1a0\ud070 \uae38\uc774\uc5d0\uc11c\uc758 \uc131\ub2a5\uc774 \ub098\ud0c0\ub098 \uc788\uc2b5\ub2c8\ub2e4. \uae30\ubcf8 \uc810\uc218\ub294 250, 500, 1K \ud1a0\ud070 \uae38\uc774\uc5d0\uc11c\uc758 \ud3c9\uade0 \ucd5c\uace0 \uc810\uc218\ub85c \uacc4\uc0b0\ub429\ub2c8\ub2e4.  \uae30\ubcf8 \uc810\uc218\uc758 50% \ubbf8\ub9cc\uc778 \uc810\uc218\ub294 \ube68\uac04\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5 \uc800\ud558\ub97c \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uae34 \ubb38\ub9e5\uc5d0\uc11c\uc758 \ucd94\ub860 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \uace0\uc548\ub41c \uc5b4\ub824\uc6b4 \uacfc\uc81c\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "4.4. Results"}, {"content": "|                    | 8K   | 16K  | 32K  |\n|--------------------|------|------|------|\n| Direct             | 98.3 | 98.5 | 98.5 |\n| One-hop            | 84.1 | 73.2 | 56.2 |\n| - w/ Literal Match (MC) | 98.7 | 97.4 | 93.1 |\n| Two-hop            | 57.4 | 42.7 | 25.9 |\n| - w/ Literal Match (MC) | 96.3 | 94.6 | 87.2 |", "caption": "Table 6: Results in two literal match setups: direct and multiple choice (MC) questions. Model: Llama 3.3 70B", "description": "\ud45c 6\uc740 Llama 3.3 70B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud55c \ub450 \uac00\uc9c0 \uc720\ud615\uc758 \uc9c8\ubb38(\uc9c1\uc811 \uc9c8\ubb38, \uac1d\uad00\uc2dd \uc9c8\ubb38)\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc9c1\uc811 \uc9c8\ubb38\uc740 \ub2f5\ubcc0\uc5d0 \ud544\uc694\ud55c \uc815\ubcf4\uac00 \uc9c8\ubb38\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \ud3ec\ud568\ub418\uc5b4 \uc788\ub294 \ubc18\uba74, \uac1d\uad00\uc2dd \uc9c8\ubb38\uc740 \ucd94\uac00\uc801\uc778 \ucd94\ub860 \ub2e8\uacc4\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubb38\ub9e5 \ub0b4\uc5d0\uc11c \uba85\uc2dc\uc801\uc778 \uc77c\uce58\uac00 \uc874\uc7ac\ud560 \ub54c \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "4.4.4 ABLATION STUDY: LITERAL MATCH EFFECT"}, {"content": "| Question | Needles | Keyword Types |\n|---|---|---|\n| Which character has been to $W_{q}$? | Def. | $W_{n}$ | Countries, cities, states |\n|  | Inv. | $W_{q}$ | Countries, cities, states |\n| Which character has been to $W_{q}$? | Def. | $W_{n}$ | Buildings & Landmarks |\n|  | Inv. | $W_{q}$ | Countries, cities, states |\n| Which character cannot drink $W_{q}$? | Def. | $W_{n}$ | Dietary restriction\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0(e.g., lactose intolerant) |\n|  | Inv. | $W_{q}$ | Drinks & Beverages |\n| Which character cannot eat $W_{q}$? | Def. | $W_{n}$ | Dietary restriction\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (e.g., vegan) |\n|  | Inv. | $W_{q}$ | Foods |", "caption": "Table 7: Our proposed needle set templates in NoLiMa. The placeholders [CHAR], Wqsubscript\ud835\udc4a\ud835\udc5eW_{q}italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT, and Wnsubscript\ud835\udc4a\ud835\udc5bW_{n}italic_W start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT represent the randomly selected character (also the answer), the query keyword, and the needle keyword, respectively. Def.:\ndefault order. Inv.: inverted order.", "description": "\ud45c 7\uc740 \ub17c\ubb38\uc758 NoLiMa \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc9c8\ubb38-\ubc14\ub298 \uc30d\uc758 \uc608\uc2dc\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. [CHAR], Wq, Wn\uc740 \uac01\uac01 \ubb34\uc791\uc704\ub85c \uc120\ud0dd\ub41c \ub4f1\uc7a5\uc778\ubb3c(\uc815\ub2f5), \uc9c8\ubb38 \ud0a4\uc6cc\ub4dc, \ubc14\ub298 \ud0a4\uc6cc\ub4dc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uac01 \uc9c8\ubb38-\ubc14\ub298 \uc30d\uc740 \uc9c8\ubb38\uacfc \ubc14\ub298 \uc0ac\uc774\uc758 \uc5b4\ud718 \uc911\ubcf5\uc744 \ucd5c\uc18c\ud654\ud558\uc5ec \ubaa8\ub378\uc774 \ub2e8\uc21c\ud55c \ud45c\uba74\uc801\uc778 \uc77c\uce58\ubcf4\ub2e4\ub294 \uc7a0\uc7ac\uc801\uc778 \uc5f0\uad00\uc131\uc744 \ud30c\uc545\ud574\uc57c \ud568\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.  'Def.'\ub294 \uae30\ubcf8 \uc21c\uc11c, 'Inv.'\ub294 \ubc18\uc804 \uc21c\uc11c\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \uc9c8\ubb38\uacfc \ub300\uc751\ud558\ub294 \ubc14\ub298\uc774 \uc5ec\ub7ec \uac1c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378\uc758 \ucd94\ub860 \ub2a5\ub825\uc744 \ub2e4\uac01\uc801\uc73c\ub85c \ud3c9\uac00\ud560 \uc218 \uc788\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "3. NOLIMA"}, {"content": "| Model | Context Length | Open Weights? | Model Revision |\n|---|---|---|---| \n| GPT-4o | 128K | No | gpt-4o-2024-11-20 |\n| GPT-4o mini | 128K | No | gpt-4o-mini-20240718 |\n| Llama 3.3 70B | 128K | Yes | meta-llama/Llama-3.3-70B-Instruct |\n| Llama 3.1 405B | 128K | Yes | meta-llama/Llama-3.1-405B-Instruct |\n| Llama 3.1 70B | 128K | Yes | meta-llama/Llama-3.1-70B-Instruct |\n| Llama 3.1 8B | 128K | Yes | meta-llama/Llama-3.1-8B-Instruct |\n| Gemini 1.5 Pro | 2M | No | gemini-1.5-pro-002 |\n| Gemini 1.5 Flash | 1M | No | gemini-1.5-flash-002 |\n| Claude 3.5 Sonnet | 200K | No | anthropic.claude-3-5-sonnet-20241022-v2 |\n| Jamba 1.5 Mini | 256K | Yes | ai21labs/AI21-Jamba-1.5-Mini |\n| Command R+ | 128K | Yes | CohereForAI/c4ai-command-r-plus-08-2024 |\n| Mistral Large 2 | 128K | Yes | mistralai/Mistral-Large-Instruct-2411 |\n| *Reasoning-based models* |  |  |  |\n| GPT-o1 | 128K | No | gpt-o1-2024-12-17 |\n| GPT-o3 Mini | 128K | No | gpt-o3-mini-2025-01-31 |\n| DeepSeek R1-DL-70b | 128K | Yes | deepseek-ai/DeepSeek-R1-Distill-Llama-70B |", "caption": "Table 8: Details of the selected models used for evaluation.", "description": "\ud45c 8\uc740 \ub17c\ubb38\uc5d0\uc11c \ud3c9\uac00\uc5d0 \uc0ac\uc6a9\ub41c \ubaa8\ub378\ub4e4\uc758 \uc138\ubd80 \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378 \uc774\ub984, \uc8fc\uc7a5\ub418\ub294 \ucd5c\ub300 \ubb38\ub9e5 \uae38\uc774, \uac00\uc911\uce58\uac00 \uacf5\uac1c\ub418\uc5c8\ub294\uc9c0 \uc5ec\ubd80(Open Weights), \uadf8\ub9ac\uace0 \ubaa8\ub378 \ubc84\uc804 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac1c\ubc29\ud615 \uac00\uc911\uce58(Open Weights) \ubaa8\ub378\uc758 \uacbd\uc6b0, \uc0ac\uc6a9\ub41c \ud2b9\uc815 \ubc84\uc804\uacfc \ucd9c\ucc98\uac00 \uba85\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \ubaa8\ub378\uc758 \ud2b9\uc9d5\uc744 \ud55c\ub208\uc5d0 \ud30c\uc545\ud558\ub294 \ub370 \uc720\uc6a9\ud558\uba70, \uc2e4\ud5d8 \uacb0\uacfc \ud574\uc11d\uc5d0 \ud544\uc694\ud55c \ubc30\uacbd \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Mode | Prompt Template |\n|---|---| \n| w/o CoT | You will answer a question based on the following book snippet:  {haystack w/ needle}  Use the information provided in the book snippet to answer the question. Your answer should be short and based on either explicitly stated facts or strong, logical inferences. Question: {question} Return only the final answer with no additional explanation or reasoning. |\n| w/ CoT | You will answer a question based on the following book snippet:  {haystack w/ needle}  Use the information provided in the book snippet to answer the question. Be aware that some details may not be stated directly, and you may need to INFER the answer based on the given information. Begin with a brief explanation of your reasoning in NO MORE THAN THREE (3) sentences. Then, return the final answer on a new line. Question: {question} |", "caption": "Table 9: Details of prompt templates utilized in our evaluation.", "description": "\ud45c 9\ub294 \ubcf8 \ub17c\ubb38\uc758 \uc2e4\ud5d8\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ud504\ub86c\ud504\ud2b8 \ud15c\ud50c\ub9bf\uc5d0 \ub300\ud55c \uc138\ubd80 \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub450 \uac00\uc9c0 \ubaa8\ub4dc(CoT \uc0ac\uc6a9 \uc720\ubb34)\uc5d0 \ub530\ub978 \ud504\ub86c\ud504\ud2b8 \uc608\uc2dc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \ubaa8\ub4dc\uc5d0\uc11c \ubaa8\ub378\uc774 \uc9c8\ubb38\uc5d0 \ub2f5\ubcc0\ud558\ub294 \ubc29\uc2dd\uacfc \ub2f5\ubcc0\uc5d0 \ub300\ud55c \uc9c0\uce68\uc774 \uc790\uc138\ud788 \uc124\uba85\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. CoT(Chain-of-Thought) \ud504\ub86c\ud504\ud2b8\ub294 \ub2e8\uacc4\ubcc4 \ucd94\ub860\uc744 \uc720\ub3c4\ud558\uc5ec \ubaa8\ub378\uc758 \ub2f5\ubcc0 \uc815\ud655\ub3c4\ub97c \ub192\uc774\uae30 \uc704\ud55c \ubaa9\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub4dc\uc5d0 \ub300\ud55c \ud504\ub86c\ud504\ud2b8 \ud15c\ud50c\ub9bf\uacfc \ud568\uaed8 \ubaa8\ub378\uc758 \uc751\ub2f5 \ud615\uc2dd \ubc0f \uc81c\ud55c \uc0ac\ud56d\uc5d0 \ub300\ud55c \uc815\ubcf4\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}]