{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a model that significantly impacts the field by aligning images and text, which is leveraged by many AGIQA methods including those in this paper."}, {"fullname_first_author": "J. Li", "paper_title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "publication_date": "2022-00-00", "reason": "This paper introduces BLIP, another influential vision-language model used as a foundation for several AGIQA methods and improves the alignment between text and images."}, {"fullname_first_author": "Z. Zhang", "paper_title": "A perceptual quality assessment exploration for aigc images", "publication_date": "2023-00-00", "reason": "This paper introduces the AGIQA-1k dataset, one of the earliest and most influential datasets used for AGIQA research, setting a benchmark for subsequent work."}, {"fullname_first_author": "C. Li", "paper_title": "Agiqa-3k: An open database for ai-generated image quality assessment", "publication_date": "2024-00-00", "reason": "This paper introduces the AGIQA-3k dataset, a larger and more comprehensive dataset than AGIQA-1k, providing a more robust benchmark for evaluating AGIQA methods."}, {"fullname_first_author": "J. Wang", "paper_title": "Aigciqa2023: A large-scale image quality assessment database for ai generated images: from the perspectives of quality, authenticity and correspondence", "publication_date": "2023-00-00", "reason": "This paper introduces the AIGCIQA2023 dataset, a significant dataset for AGIQA research that considers multiple dimensions of image quality, prompting further advancements in the field."}]}