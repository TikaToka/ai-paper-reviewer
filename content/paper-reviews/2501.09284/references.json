{"references": [{"fullname_first_author": "Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-XX-XX", "reason": "This paper introduces LoRA, the core low-rank adaptation technique that SEAL builds upon and aims to enhance."}, {"fullname_first_author": "Ding", "paper_title": "Parameter-efficient fine-tuning of large-scale pre-trained language models", "publication_date": "2023-XX-XX", "reason": "This paper discusses PEFT, providing the broader context for efficient fine-tuning methods which are relevant to understanding SEAL's approach."}, {"fullname_first_author": "Fan", "paper_title": "Passport-aware normalization for deep model protection", "publication_date": "2020-XX-XX", "reason": "This paper introduces passport-based watermarking, a crucial concept underlying SEAL's security mechanism."}, {"fullname_first_author": "Uchida", "paper_title": "Embedding watermarks into deep neural networks", "publication_date": "2017-XX-XX", "reason": "This paper is a foundational work in DNN watermarking, providing the baseline methods and challenges that SEAL addresses within the LoRA context."}, {"fullname_first_author": "Zhang", "paper_title": "Memory-efficient low-rank adaptation for large language models fine-tuning", "publication_date": "2023-XX-XX", "reason": "This paper presents LORA-FA, a variant of LoRA which is directly compared against in the paper, showcasing the applicability of SEAL to different LoRA architectures."}]}