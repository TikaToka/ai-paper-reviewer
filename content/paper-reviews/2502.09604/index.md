---
title: "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models"
summary: "SelfCite: LLMì´ ê³ í’ˆì§ˆ ì¸ìš© ìƒì„±ì„ ìœ„í•œ ìê¸° ì§€ë„ í•™ìŠµ ê¸°ë²•"
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Natural Language Processing", "Large Language Models", "ğŸ¢ MIT",]
showSummary: true
date: 2025-02-13
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2502.09604 {{< /keyword >}}
{{< keyword icon="writer" >}} Yung-Sung Chuang et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2025-02-14 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2502.09604" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2502.09604" target="_self" >}}
â†— Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/selfcite-self-supervised-alignment-for" target="_self" >}}
â†— Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì‚¬ìš©ìì—ê²Œ ì •ë³´ë¥¼ ì œê³µí•˜ê³  ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ìœ ìš©í•˜ì§€ë§Œ, **í™˜ê°(hallucination)** ë° **ë¬¸ë§¥ ì˜¤ë¥˜(context error)** ë¬¸ì œë¡œ ì‹ ë¢°ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê¸°ì¡´ ì—°êµ¬ì—ì„œëŠ” **ì¸ìš©(citation)**ì„ í†µí•´ ìƒì„±ëœ ì‘ë‹µì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ê³ ì í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ë°©ë²•ì€ **ìˆ˜ë™ ì£¼ì„(manual annotation)**ì— ì˜ì¡´í•˜ê±°ë‚˜ **ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ë…ì  API**ì— ì˜ì¡´í•˜ëŠ” í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.



ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **SelfCite**ë¼ëŠ” ìƒˆë¡œìš´ ìê¸° ì§€ë„ í•™ìŠµ ê¸°ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. SelfCiteëŠ” LLMì´ ìŠ¤ìŠ¤ë¡œ ë¬¸ë§¥ ì œê±°ë¥¼ í†µí•´ ë³´ìƒ ì‹ í˜¸ë¥¼ ìƒì„±í•˜ì—¬ ì¸ìš© í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.  **ë¬¸ë§¥ ì œê±° ì‹¤í—˜**ì„ í†µí•´ ì¸ìš©ì˜ í•„ìš”ì„±ê³¼ ì¶©ë¶„ì„±ì„ í‰ê°€í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ìµœì ì˜ Nê°œ ìƒ˜í”Œë§(best-of-N sampling)** ë° **ì„ í˜¸ë„ ìµœì í™”(preference optimization)** ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì¸ìš©ì˜ ì •í™•ì„±ê³¼ í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.  ì‹¤í—˜ ê²°ê³¼, SelfCiteëŠ” LongBench-Cite ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ìµœê³  ì„±ëŠ¥ì„ ìƒë‹¹íˆ ëŠ¥ê°€í•˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} SelfCiteëŠ” **ì–´ë…¸í…Œì´ì…˜ ì—†ì´** LLMì˜ ì‘ë‹µì— ëŒ€í•œ ì •í™•í•˜ê³  ì„¸ë¶„í™”ëœ ì¸ìš© ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} ë¬¸ë§¥ ì œê±°(context ablation) ê¸°ë°˜ì˜ ìê¸° ì§€ë„ í•™ìŠµì„ í†µí•´ ì¸ìš©ì˜ **ì •í™•ì„±ê³¼ í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒ**ì‹œí‚µë‹ˆë‹¤. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} LongBench-Cite ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ìµœê³  ì„±ëŠ¥ì„ **ìµœëŒ€ 5.3% ê°œì„ **í•˜ëŠ” ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
ë³¸ ë…¼ë¬¸ì€ **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)**ì´ ìƒì„±í•œ ì‘ë‹µì— ëŒ€í•œ ë¬¸ë§¥ ì†ì„±ì„ ê°œì„ í•˜ëŠ” ë° ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ì§€ë‹™ë‹ˆë‹¤.  **ìê¸° ì§€ë„ í•™ìŠµ(self-supervised learning)**ì„ í†µí•´ ê³ í’ˆì§ˆì˜ ì¸ìš©ì„ ìƒì„±í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì‹œí•¨ìœ¼ë¡œì¨,  **ì–´ë…¸í…Œì´ì…˜(annotation)ì— ëŒ€í•œ ë¹„ìš©ê³¼ ë…¸ë ¥ì„ ì ˆê°**í•˜ê³ , LLMì˜ ì‹ ë¢°ì„±ê³¼ íˆ¬ëª…ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.  ì—°êµ¬ ê²°ê³¼ëŠ” ì¸ìš©ì˜ ì •í™•ì„±ê³¼ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” íš¨ê³¼ì ì¸ ë°©ë²•ì„ ë³´ì—¬ì£¼ë©°, **í–¥í›„ LLMì˜ ë°œì „ ë°©í–¥**ì— ëŒ€í•œ ê·€ì¤‘í•œ í†µì°°ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” íŠ¹íˆ **ê¸´ ë§¥ë½(long-context) ë¬¸ì„œ ì²˜ë¦¬**ì—ì„œ ê·¸ ì¤‘ìš”ì„±ì´ ë”ìš± ì»¤ì§‘ë‹ˆë‹¤.

------
#### Visual Insights



![](https://arxiv.org/html/2502.09604/x1.png)

> ğŸ”¼ ê·¸ë¦¼ 1ì€ SelfCite í”„ë ˆì„ì›Œí¬ê°€ í•„ìš”ì„± ì ìˆ˜(í™•ë¥  ê°ì†Œ)ì™€ ì¶©ë¶„ì„± ì ìˆ˜(í™•ë¥  ìœ ì§€)ë¼ëŠ” ë‘ ê°€ì§€ ì§€í‘œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³´ìƒì„ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë¨¼ì € ì „ì²´ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ, í”„ë ˆì„ì›Œí¬ëŠ” (1) ë¬¸ë§¥ì—ì„œ ì¸ìš©ëœ ë¬¸ì¥ì„ ì œê±°í•˜ê³  (2) ì¸ìš©ëœ ë¬¸ì¥ë§Œ ë¬¸ë§¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ì‘ë‹µì„ ìƒì„±í•  í™•ë¥ ì„ í‰ê°€í•©ë‹ˆë‹¤. í™•ë¥  ê°ì†Œì™€ ìœ ì§€ëŠ” ì´ëŸ¬í•œ í™•ë¥  ì°¨ì´ë¡œë¶€í„° ê³„ì‚°ë˜ë©°, ë‘ ì ìˆ˜ì˜ í•©ì´ ìµœì¢… ë³´ìƒìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ SelfCiteê°€ ì–´ë–»ê²Œ ë¬¸ë§¥ ì œê±°ë¥¼ í†µí•´ LLMì´ ìƒì„±í•œ ì¸ìš©ì˜ ì§ˆì„ ìì²´ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ë³´ìƒ ì‹ í˜¸ë¥¼ ìƒì„±í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 1: The SelfCite framework calculates rewards based on two metrics: necessity score (probability drop) and sufficiency score (probability hold). First, the full context is used to generate a response. Then, the framework evaluates the probability of generating the same response after (1) removing the cited sentences from the context and (2) using only the cited sentences in the context. The probability drop and hold are computed from these probability differences, and their sum is used as the final reward.
> </details>





{{< table-caption >}}
| Model | Longbench-Chat |  |  | MultifieldQA |  |  | HotpotQA |  |  | Dureader |  |  | GovReport |  | Avg. | Citation |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|  | R | P | F1 | R | P | F1 | R | P | F1 | R | P | F1 | R | P | F1 | F1 | Length |
| **Proprietary models** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| GPT-4oâ€  | 46.7 | 53.5 | 46.7 | 79.0 | 87.9 | 80.6 | 55.7 | 62.3 | 53.4 | 65.6 | 74.2 | 67.4 | 73.4 | 90.4 | 79.8 | 65.6 | 220 |
| Claude-3-sonnetâ€  | 52.0 | 67.8 | 55.1 | 64.7 | 85.8 | 71.3 | 46.4 | 65.8 | 49.9 | 67.7 | 89.2 | 75.5 | 77.4 | 93.9 | 84.1 | 67.2 | 132 |
| GLM-4â€  | 47.6 | 53.9 | 47.1 | 72.3 | 80.1 | 73.6 | 47.0 | 50.1 | 44.4 | 73.4 | 82.3 | 75.0 | 82.8 | 93.4 | 87.1 | 65.4 | 169 |
| **Open-source models** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| GLM-4-9B-chatâ€  | 25.9 | 20.5 | 16.7 | 51.1 | 60.6 | 52.0 | 22.9 | 28.8 | 20.1 | 45.4 | 48.3 | 40.9 | 5.7 | 8.2 | 6.3 | 27.2 | 96 |
| Llama-3.1-8B-Instructâ€  | 14.1 | 19.5 | 12.4 | 29.8 | 44.3 | 31.6 | 20.2 | 30.9 | 20.9 | 22.0 | 25.1 | 17.0 | 16.2 | 25.3 | 16.8 | 19.7 | 100 |
| Llama-3.1-70B-Instructâ€  | 25.8 | 32.0 | 23.2 | 53.2 | 65.2 | 53.9 | 29.6 | 37.3 | 28.6 | 38.2 | 46.0 | 35.4 | 53.4 | 77.5 | 60.7 | 40.4 | 174 |
| Mistral-Large-Instructâ€  | 19.8 | 23.9 | 19.0 | 71.8 | 80.7 | 73.8 | 34.5 | 40.9 | 32.1 | 58.3 | 67.0 | 60.1 | 67.9 | 79.6 | 72.5 | 51.5 | 132 |
| **Contributive context attribution** (*with Llama-3.1-8B-Instruct*) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| ContextCite (32 calls) | 56.7 | 76.8 | 58.0 | 76.1 | 87.2 | 78.9 | 40.5 | 54.7 | 43.9 | 58.0 | 82.4 | 65.0 | 67.1 | 88.8 | 75.6 | 64.3 | 92.7 |
| ContextCite (256 calls) | 63.5 | 83.1 | 64.7 | 78.8 | 89.8 | 81.8 | 46.5 | 60.8 | 49.2 | 61.7 | 89.1 | 70.1 | 69.1 | 93.5 | 78.8 | 68.9 | 100.8 |
| **Fine-tuned models** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| LongCite-9Bâ€  | 57.6 | 78.1 | 63.6 | 67.3 | 91.0 | 74.8 | 61.8 | 78.8 | 64.8 | 67.6 | 89.2 | 74.4 | 63.4 | 76.5 | 68.2 | 69.2 | 91 |
| LongCite-8Bâ€  | 62.0 | 79.7 | 67.4 | 74.7 | 93.0 | 80.8 | 59.2 | 72.1 | 60.3 | 68.3 | 85.6 | 73.1 | 74.0 | 86.6 | 78.5 | 72.0 | 85 |
| **Ours: SelfCite** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| LongCite-8B (Our repro.) | 67.0 | 78.1 | 66.6 | 74.8 | 90.7 | 79.9 | 60.8 | 77.9 | 64.1 | 67.1 | 87.2 | 73.7 | 81.6 | 89.3 | 84.5 | 73.8 | 83.5 |
| + BoN | 68.4 | 81.3 | 71.2 | 76.1 | 92.8 | 81.2 | 67.2 | 81.0 | 68.8 | 70.6 | 90.9 | 76.9 | 87.6 | 92.4 | 89.3 | 77.5 | 93.4 |
| + SimPO | 68.1 | 79.5 | 69.1 | 75.5 | 92.6 | 81.0 | 69.4 | 82.3 | 71.5 | 72.7 | 91.6 | 78.9 | 86.4 | 92.9 | 89.1 | 77.9 | 105.7 |
| + SimPO then BoN | 73.3 | 79.4 | 72.8 | 76.7 | 93.2 | 82.2 | 69.4 | 83.0 | 71.1 | 74.2 | 92.2 | 80.3 | 86.7 | 92.7 | 89.2 | 79.1 | 94.7 |
| Llama-3.1-8B-Instruct (*fully self-supervised setting*) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  + SFT on ContextCite | 52.3 | 70.6 | 56.5 | 79.1 | 90.5 | 82.0 | 54.5 | 72.3 | 56.3 | 54.9 | 79.0 | 61.6 | 63.7 | 84.9 | 72.3 | 65.7 | 83.0 |
|  + BoN | 54.8 | 67.6 | 58.1 | 80.4 | 90.5 | 83.0 | 58.3 | 70.0 | 57.5 | 57.6 | 79.0 | 63.1 | 67.2 | 84.8 | 74.6 | 67.3 | 80.4 |
|  + SimPO | 63.3 | 74.3 | 64.6 | 80.2 | 88.9 | 82.4 | 59.7 | 76.9 | 61.0 | 59.0 | 80.9 | 65.4 | 68.5 | 86.6 | 76.1 | 69.9 | 90.2 |
|  + SimPO then BoN | 66.0 | 82.4 | 71.1 | 81.5 | 90.7 | 83.2 | 61.3 | 70.0 | 59.9 | 62.1 | 81.4 | 67.4 | 68.8 | 86.2 | 76.1 | 71.5 | 87.4 |{{< /table-caption >}}

> ğŸ”¼ í‘œ 1ì€ LongBench-Cite ë²¤ì¹˜ë§ˆí¬ì—ì„œ í‰ê°€ëœ ì¸ìš© ì¬í˜„ìœ¨(R), ì¸ìš© ì •ë°€ë„(P), ì¸ìš© F1 ì ìˆ˜(F1) ë° ì¸ìš© ê¸¸ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ë³¸ ë…¼ë¬¸ì˜ ê²°ê³¼ ì¤‘ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ëŠ” êµµê²Œ í‘œì‹œë˜ì–´ ìˆìœ¼ë©°, ì´ì „ ìµœì²¨ë‹¨ ê¸°ìˆ ì˜ ìµœê³  ì„±ëŠ¥ì€ ë°‘ì¤„ì´ ê·¸ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. â€ ëŠ” Zhang ì™¸.(2024)ì˜ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ë©°, 'Our repro.'ëŠ” ë³¸ ë…¼ë¬¸ì—ì„œ ì¬í˜„í•œ ê²°ê³¼ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  ë‹¤ì–‘í•œ ëª¨ë¸(ë…ì  ëª¨ë¸, ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸, ê¸°ì—¬ ë§¥ë½ ì†ì„± ëª¨ë¸, ë¯¸ì„¸ ì¡°ì • ëª¨ë¸, SelfCite ëª¨ë¸)ì˜ ì„±ëŠ¥ì„  LongBench-Cite ë²¤ì¹˜ë§ˆí¬ì˜ ë‹¤ì„¯ ê°€ì§€ ì§ˆë¬¸ì‘ë‹µ ì‘ì—…(Longbench-Chat, MultifieldQA, HotpotQA, DuReader, GovReport)ì— ëŒ€í•´ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 1: Citation recall (R), citation precision (P), citation F1 (F1), and citation length evaluated on LongBench-Cite benchmark. The best of our results are bolded. The best of previous state of the art are underlined. â€  indicates the results taken fromÂ Zhang etÂ al. (2024). Our repro.Â means our reproduced results.
> </details>





### In-depth insights


#### Self-Supervised Citation
ìê¸° ì§€ë„ í•™ìŠµ ê¸°ë°˜ì˜ ì¸ìš© ì‹œìŠ¤í…œì€ **ì‚¬ëŒì˜ ê°œì… ì—†ì´** ì–¸ì–´ ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ì˜ ì •í™•ì„±ê³¼ ì‹ ë¢°ì„±ì„ ë†’ì´ëŠ” í˜ì‹ ì ì¸ ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤.  ê¸°ì¡´ì˜ ë°©ì‹ì²˜ëŸ¼ ì‚¬ëŒì´ ì§ì ‘ ì¸ìš©ì„ ê²€í† í•˜ê³  ìˆ˜ì •í•˜ëŠ” ëŒ€ì‹ , **ëª¨ë¸ ìì²´ê°€ ìƒì„±í•œ ì¸ìš©ì˜ ì§ˆì„ í‰ê°€í•˜ê³  ìŠ¤ìŠ¤ë¡œ ê°œì„ **í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.  ì´ëŠ” ë§¥ë½ ì œê±°(context ablation)ì™€ ê°™ì€ ê¸°ë²•ì„ í†µí•´ ëª¨ë¸ì˜ ì¶œë ¥ í™•ë¥  ë³€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ìš©ì˜ í•„ìš”ì„±ê³¼ ì¶©ë¶„ì„±ì„ íŒë‹¨í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.  **ë¹„ìš© ë° ì‹œê°„ íš¨ìœ¨ì„±ì´ ë›°ì–´ë‚˜ë©°**, ë°©ëŒ€í•œ ë°ì´í„°ì…‹ì´ í•„ìš”í•˜ì§€ ì•Šì•„, íŠ¹íˆ ì¥ë¬¸ì˜ ë¬¸ì„œë¥¼ ë‹¤ë£¨ëŠ” ê²½ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ, **ëª¨ë¸ì˜ ìê¸° í‰ê°€ì— ëŒ€í•œ ì‹ ë¢°ë„**ì™€, ë§¥ë½ ì œê±° ê¸°ë²•ì˜ í•œê³„ë¡œ ì¸í•´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜¤ë¥˜ ê°€ëŠ¥ì„± ë“± **ê°œì„ ì˜ ì—¬ì§€**ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.  **í–¥í›„ ì—°êµ¬**ì—ì„œëŠ” ì´ëŸ¬í•œ í•œê³„ì ì„ ë³´ì™„í•˜ê³ , ìê¸° ì§€ë„ í•™ìŠµ ê¸°ë°˜ ì¸ìš© ì‹œìŠ¤í…œì˜ ì •í™•ë„ì™€ ì‹ ë¢°ì„±ì„ ë”ìš± í–¥ìƒì‹œí‚¤ëŠ” ë°©ì•ˆì„ ëª¨ìƒ‰í•´ì•¼ í•  ê²ƒì…ë‹ˆë‹¤.  **ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸**ê³¼ **ë°ì´í„°ì…‹**ì— ëŒ€í•œ ì ìš©ì„± í™•ëŒ€ì™€, **ë‹¤ë¥¸ ìê¸° ì§€ë„ í•™ìŠµ ê¸°ë²•**ê³¼ì˜ ê²°í•©ì„ í†µí•´ ì„±ëŠ¥ í–¥ìƒì„ ë„ëª¨í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•œ ì—°êµ¬ ë°©í–¥ì…ë‹ˆë‹¤.

#### Context Ablation Reward
ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ 'Context Ablation Reward'ëŠ” **LLMì˜ ìƒì„± ë‹µë³€ì— ëŒ€í•œ ì‹ ë¢°ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ í•µì‹¬ ì „ëµ**ì…ë‹ˆë‹¤.  ê¸°ì¡´ì˜ ì–´ë…¸í…Œì´ì…˜ ê¸°ë°˜ ë°©ë²•ê³¼ ë‹¬ë¦¬, ìê°€ ì§€ë„ í•™ìŠµ ë°©ì‹ì„ í†µí•´ **LLM ìì²´ê°€ ìƒì„±í•œ ë‹µë³€ì˜ ì •í™•ì„±ì„ í‰ê°€**í•˜ë„ë¡ í•©ë‹ˆë‹¤.  ì´ëŠ” ë¬¸ë§¥(Context)ì—ì„œ íŠ¹ì • êµ¬ë¬¸(cited text)ì„ ì œê±°í•˜ê±°ë‚˜, ì˜¤ì§ í•´ë‹¹ êµ¬ë¬¸ë§Œ ë‚¨ê¸´ ì±„ ë‹µë³€ì„ ìƒì„±í•˜ì—¬ í™•ë¥  ë³€í™”ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. êµ¬ë¬¸ ì œê±° í›„ ë‹µë³€ í™•ë¥ ì´ í¬ê²Œ ê°ì†Œí•˜ë©´ í•´ë‹¹ êµ¬ë¬¸ì´ **í•„ìˆ˜ì (Necessity)**ì„ì„, êµ¬ë¬¸ë§Œìœ¼ë¡œë„ ë‹µë³€ í™•ë¥ ì´ ìœ ì§€ë˜ë©´ **ì¶©ë¶„(Sufficiency)**í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  **ë‘ ì ìˆ˜ë¥¼ ì¢…í•©**í•˜ì—¬ ë³´ìƒ ì‹ í˜¸(reward)ë¥¼ ì‚°ì¶œ, ì´ë¥¼ í†µí•´ LLMì´ ë”ìš± ì •í™•í•˜ê³  ì„¸ë°€í•œ ì¸ìš© ì •ë³´ë¥¼ ìƒì„±í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.  **ë¹„ìš© íš¨ìœ¨ì ì´ê³  ìê°€ ì§€ë„ í•™ìŠµ ë°©ì‹**ì´ë¼ëŠ” ì ì—ì„œ ê¸°ì¡´ì˜ ì–´ë…¸í…Œì´ì…˜ ê¸°ë°˜ ë°©ë²•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” í˜ì‹ ì ì¸ ì ‘ê·¼ ë°©ì‹ì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ì´ëŠ” **ë†’ì€ ì •í™•ë„ì˜ ì¸ìš© ìƒì„±ì„ ìœ„í•œ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±**ì„ ì œì‹œí•˜ë©°,  í–¥í›„ LLMì˜ ì‹ ë¢°ì„± í–¥ìƒì— ì¤‘ìš”í•œ ì—­í• ì„ í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

#### Best-of-N & SimPO
ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ Best-of-N ìƒ˜í”Œë§ê³¼ SimPO(Simple Preference Optimization)ëŠ” **ì €ìë“¤ì´ ì œì•ˆí•œ SelfCite í”„ë ˆì„ì›Œí¬ì˜ í•µì‹¬ì ì¸ ë¶€ë¶„**ì…ë‹ˆë‹¤. Best-of-N ìƒ˜í”Œë§ì€ **LLMì´ ìƒì„±í•œ ì¸ìš©êµ¬ í›„ë³´ë“¤ì„ í‰ê°€í•˜ì—¬ ìµœì ì˜ ì¸ìš©êµ¬ë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•**ìœ¼ë¡œ, ê³„ì‚° ë¹„ìš©ì´ ì €ë ´í•œ SelfCite rewardë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ì–´ë…¸í…Œì´ì…˜ ì—†ì´ë„ ì¸ìš©êµ¬ì˜ ì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ë° íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í•˜ì§€ë§Œ Best-of-N ìƒ˜í”Œë§ì€ ì¶”ë¡  ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.  **SimPOëŠ” Best-of-N ìƒ˜í”Œë§ìœ¼ë¡œ ì–»ì€ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ LLM ìì²´ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•**ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Best-of-N ìƒ˜í”Œë§ ì—†ì´ë„ ì¸ìš©êµ¬ ìƒì„± ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ë”ìš± íš¨ìœ¨ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. **SimPOëŠ” Best-of-N ìƒ˜í”Œë§ì˜ ì¥ì ì„ ìœ ì§€í•˜ë©´ì„œ ë‹¨ì ì„ ë³´ì™„**í•˜ì—¬, ì¸ìš©êµ¬ ìƒì„± ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.  **ë‘ ë°©ë²•ì˜ ì¡°í•©**ì„ í†µí•´ LLMì˜ ì¸ìš© ìƒì„± ëŠ¥ë ¥ì„ íšê¸°ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ, **ìê¸° ì§€ë„ í•™ìŠµ ë°©ì‹**ì„ í†µí•´ ì¶”ê°€ì ì¸ ì–´ë…¸í…Œì´ì…˜ ì—†ì´ë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤ëŠ” ì ì—ì„œ ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤.

#### LongBench-Cite Results
LongBench-CiteëŠ” ì¥ë¬¸ ë§¥ë½ ì§ˆì˜ì‘ë‹µ(Long-context Question Answering)ì—ì„œ ì¸ìš©ì˜ ì§ˆì„ í‰ê°€í•˜ê¸° ìœ„í•œ ì¢…í•©ì ì¸ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤. **SelfCite ëª¨ë¸ì€ LongBench-Cite ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ìµœì²¨ë‹¨ ëª¨ë¸ë“¤ì„ ìƒë‹¹íˆ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.** ì´ëŠ” íŠ¹íˆ ì¸ìš©ì˜ ì •í™•ì„±(precision)ê³¼ ì¬í˜„ìœ¨(recall) ì¸¡ë©´ì—ì„œ ë‘ë“œëŸ¬ì§€ëŠ”ë°, **SelfCiteì˜ ë…ì°½ì ì¸ ìê¸° ì§€ë„ í•™ìŠµ ë°©ì‹**ì´ ì´ëŸ¬í•œ ì„±ê³¼ì— í¬ê²Œ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.  **ìê¸° ì§€ë„ í•™ìŠµì„ í†µí•´ ì–´ë…¸í…Œì´ì…˜ ë¹„ìš© ì—†ì´ë„ ë†’ì€ ìˆ˜ì¤€ì˜ ì¸ìš© ìƒì„± í’ˆì§ˆì„ ë‹¬ì„±**í•œ ê²ƒì€ ì£¼ëª©í•  ë§Œí•œ ì ì…ë‹ˆë‹¤.  í•˜ì§€ë§Œ, **ëª¨ë“  LongBench-Cite ë°ì´í„°ì…‹ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ í–¥ìƒ**ì„ ë³´ì—¬ì£¼ì§€ëŠ” ì•Šì•˜ê³ , íŠ¹ì • ë°ì´í„°ì…‹ì—ì„œëŠ” ê¸°ì¡´ ëª¨ë¸ê³¼ì˜ ì„±ëŠ¥ ì°¨ì´ê°€ í¬ì§€ ì•Šì•˜ë‹¤ëŠ” ì ë„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.  **í–¥í›„ ì—°êµ¬ì—ì„œëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì— ëŒ€í•œ SelfCiteì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë”ìš± ê°•í™”**í•˜ê³ ,  **ì¸ìš©ì˜ ê¸¸ì´ ë° ì •í™•ì„±ê³¼ ê°™ì€ ìš”ì†Œë“¤ ê°„ì˜ ê· í˜•**ì„ ê°œì„ í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì—°êµ¬ë¥¼ ì§„í–‰í•˜ëŠ” ê²ƒì´ í•„ìš”í•©ë‹ˆë‹¤.  SelfCiteëŠ” ìê¸° ì§€ë„ í•™ìŠµ ë°©ì‹ì„ í†µí•´ ì¸ìš© ìƒì„±ì˜ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œì‹œí–ˆì§€ë§Œ,  **ë”ìš± ê²¬ê³ í•˜ê³  ì¼ë°˜í™”ëœ ëª¨ë¸ì„ ê°œë°œ**í•˜ê¸° ìœ„í•œ ì§€ì†ì ì¸ ì—°êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

#### Future Work & Limits
ë³¸ ë…¼ë¬¸ì€ **ìê¸° ì§€ë„ í•™ìŠµ ê¸°ë°˜ì˜ ì¸ìš© ìƒì„± ë°©ì‹**ì„ ì œì‹œí•˜ì—¬ LLMì˜ ì¸ìš© ì •í™•ë„ë¥¼ í–¥ìƒì‹œì¼°ë‹¤ëŠ” ì ì—ì„œ í° ì˜ì˜ê°€ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ì—¬ì „íˆ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìœ¼ë©°, **í–¥í›„ ì—°êµ¬ ë°©í–¥**ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ê³ ë ¤í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¨¼ì €, **ë‹¤ì–‘í•œ LLMê³¼ì˜ í˜¸í™˜ì„± í™•ë³´**ë¥¼ ìœ„í•œ ì¶”ê°€ ì—°êµ¬ê°€ í•„ìš”í•˜ë©°, **ë‹¤ë¥¸ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ê³¼ì˜ ê²°í•©**ì„ í†µí•´ ì„±ëŠ¥ í–¥ìƒì„ ë„ëª¨í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ë˜í•œ, **SFT ë‹¨ê³„ì˜ ë¹„ì§€ë„ í•™ìŠµ ë°©ì‹ ê°œì„ **ì„ ìœ„í•œ ë…¸ë ¥ì´ í•„ìš”í•˜ë©°, **ë”ìš± íš¨ìœ¨ì ì¸ ë³´ìƒ ì‹ í˜¸ ì„¤ê³„ ë° ìµœì í™”**ë¥¼ í†µí•œ ì„±ëŠ¥ í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  **ê¸¸ì´ ê· í˜• ë¬¸ì œ í•´ê²° ë° ì˜¤í”„ í´ë¦¬ì‹œ íŠ¹ì„± ì™„í™”**ë¥¼ ìœ„í•œ ì¶”ê°€ì ì¸ ì—°êµ¬ë„ í•„ìš”í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, **ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì§ˆë¬¸ ë‹µë³€ íƒœìŠ¤í¬ ë° ë²¤ì¹˜ë§ˆí¬**ì—ì„œì˜ ì„±ëŠ¥ í‰ê°€ë¥¼ í†µí•´ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì„ ê²€ì¦í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.  ì´ëŸ¬í•œ ë…¸ë ¥ì„ í†µí•´ SelfCiteëŠ” ë”ìš± ê°•ë ¥í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¸ìš© ìƒì„± ì‹œìŠ¤í…œìœ¼ë¡œ ë°œì „í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.


### More visual insights




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Model | Long. | Multi. | Hot. | Dur. | Gov. | Avg |
|---|---|---|---|---|---|---|
| *Answering without citations* |
| LongSFT-8B<sup>â€ </sup> | 68.6 | 83.6 | 69.0 | 62.3 | 54.4 | 67.6 |
| LongSFT-9B<sup>â€ </sup> | 64.6 | 83.3 | 67.5 | 66.3 | 46.4 | 65.6 |
| Llama-3.1-8B-Instruct<sup>â€ </sup> | 61.6 | 73.3 | 64.5 | 39.4 | 62.1 | 60.2 |
| *Answering with citations* |
| LongCite-8B (Our repro.) | 67.6 | 86.7 | 69.3 | 64.0 | 60.4 | 69.6 |
| + SimPO | 67.4 | 86.7 | 67.5 | 66.0 | 61.3 | 69.8 |
| Llama-3.1-8B-Instruct | 67.4 | 87.9 | 73.5 | 67.8 | 62.1 | 71.7 |
| + SFT on ContextCite | 58.8 | 83.4 | 65.8 | 57.8 | 57.5 | 64.6 |
| Â Â Â Â Â Â  + SimPO | 56.8 | 80.9 | 65.3 | 59.5 | 60.9 | 64.7 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 2ëŠ” ì¸ìš©êµ¬ë¥¼ í¬í•¨í•˜ì—¬ ì‘ë‹µí•  ë•Œì™€ ê·¸ë ‡ì§€ ì•Šì„ ë•Œì˜ ë‹µë³€ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. â€ ëŠ” Zhang et al.(2024)ì—ì„œ ê°€ì ¸ì˜¨ ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. í‘œ ë¨¸ë¦¬ê¸€ì—ëŠ” í‘œ 1ê³¼ ë™ì¼í•œ ë‹¤ì„¯ ê°œì˜ ë°ì´í„° ì§‘í•©ì— ëŒ€í•œ ì•½ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í‘œëŠ” ê° ë°ì´í„° ì§‘í•©ì— ëŒ€í•œ ì •í™•ë„ ì ìˆ˜ë¥¼ ë³´ì—¬ì£¼ë©°, ì¸ìš©êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µí•˜ëŠ” ê²ƒì´ ì •í™•ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.  Zhang et al.(2024)ì˜ ê²°ê³¼ì™€ ë¹„êµí•˜ì—¬ SelfCite ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ì¸ìš©êµ¬ê°€ ì „ì²´ì ì¸ ë‹µë³€ ì •í™•ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 2: Answer correctness when responding with or without citations. â€  indicates results taken from Zhang etÂ al. (2024). The header contains abbreviations for the same five datasets in TableÂ 1.
> </details>

{{< table-caption >}}
| Decoding Methods | HotpotQA R | HotpotQA P | HotpotQA F1 | Citation Length |
|---|---|---|---|---|
| LongCite-8B (Our repro.) | 60.8 | 77.9 | 64.1 | 83.5 |
| + BoN by LM log prob | 62.7 | 75.5 | 63.4 | 74.6 |
| + BoN by max citation length | 66.5 | 73.6 | 65.1 | 139.8 |
| + BoN by Prob-Drop | 65.6 | 78.1 | 66.6 | 92.9 |
| + BoN by Prob-Hold | 66.2 | 78.1 | 67.0 | 93.4 |
| + BoN by SelfCite | 67.2 | 81.0 | 68.8 | 93.4 |
| w/ lower length limit (256) | 65.8 | 78.8 | 66.4 | 84.5 |
| w/ higher length limit (512) | 67.0 | 82.2 | 68.5 | 99.2 |
| w/o length limit (âˆ) | 67.9 | 79.3 | 68.1 | 121.9 |{{< /table-caption >}}
> ğŸ”¼ ë³¸ í‘œëŠ” HotpotQA ë°ì´í„°ì…‹ì—ì„œ ë‹¤ì–‘í•œ ë³´ìƒ ê¸°ë²•ì„ ì‚¬ìš©í•œ Best-of-N (BoN) ë””ì½”ë”© ë°©ë²•ì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  êµ¬ì²´ì ìœ¼ë¡œ, HotpotQA ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ì‹œ, ì¸ìš© ì •ë³´ì˜ ì¬í˜„ìœ¨(Recall), ì •ë°€ë„(Precision), F1 ì ìˆ˜ ë° í‰ê·  ì¸ìš© ê¸¸ì´ë¥¼ ì¸¡ì •í•˜ì—¬ ê° ë³´ìƒ ê¸°ë²•ì˜ íš¨ê³¼ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.  ë‹¤ì–‘í•œ ë³´ìƒ ì „ëµ(LM log prob, max citation length, Prob-Drop, Prob-Hold, SelfCite)ì„ ë¹„êµí•˜ì—¬ SelfCite ê¸°ë²•ì˜ ìš°ìˆ˜ì„±ì„ ë³´ì—¬ì£¼ëŠ” í‘œì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 3: Ablation study on HotpotQA citation recall, precision, and F1 (R, P, F1) and citation length for BoN decoding methods.
> </details>

{{< table-caption >}}
| Fine-tuning Methods | HotpotQA R | HotpotQA P | HotpotQA F1 | Citation Length |
|---|---|---|---|---|
| LongCite-8B (Our repro.) | 60.8 | 77.9 | 64.1 | 83.5 |
| + SimPO | 69.4 | 82.3 | 71.5 | 105.7 |
| + SimPO + BoN | 72.0 | 82.7 | 72.9 | 126.9 |
| *+ SimPO w/ or w/o length balancing* |  |  |  |  |
| w/ length balancing | 69.4 | 82.3 | 71.5 | 105.7 |
| w/o length balancing | 64.4 | 62.9 | 60.5 | 152.9 |
| *+ SimPO w/ varying data sizes* |  |  |  |  |
| 1K examples | 62.5 | 78.9 | 65.7 | 90.1 |
| 2K examples | 69.4 | 82.3 | 71.5 | 105.7 |
| 4K examples | 68.5 | 80.4 | 70.3 | 134.1 |
| 8K examples | 63.6 | 75.3 | 64.4 | 195.2 |
| + *SFT on BoN responses* | 68.8 | 77.3 | 68.4 | 98.7 |
| *+ SimPO by denoising perturbed citations* |  |  |  |  |
| On original responses | 40.5 | 50.5 | 41.6 | 88.8 |
| On BoN responses | 42.6 | 50.7 | 42.3 | 79.7 |{{< /table-caption >}}
> ğŸ”¼ ë³¸ í‘œëŠ” HotpotQA ë°ì´í„°ì…‹ì—ì„œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì— ëŒ€í•œ ì¸ìš© ì¬í˜„ìœ¨, ì •ë°€ë„, F1 ì ìˆ˜ ë° ì¸ìš© ê¸¸ì´ì— ëŒ€í•œ ì¶”ê°€ ì—°êµ¬ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ê° ë¯¸ì„¸ ì¡°ì • ë°©ë²•(SimPO, ìµœì -N ìƒ˜í”Œë§, ê¸¸ì´ ê· í˜• ë“±)ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬,  ì¸ìš© í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²•ë¡ ì˜ íš¨ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.  íŠ¹íˆ, SimPOì˜ ë°˜ë³µì  ì ìš©, ë°ì´í„° í¬ê¸°ì˜ ì˜í–¥, ê¸¸ì´ ê· í˜• ê¸°ë²•ì˜ ìœ ë¬´ ë“±ì´ ì¸ìš© ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ìì„¸í•˜ê²Œ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 4: Ablation study on HotpotQA citation recall, precision, and F1 (R, P, F1) and citation length for finetuned models.
> </details>

{{< table-caption >}}
| Sent. ID | Context Sentences (only showing a paragraph due to limited space) |
|---|---| 
| 302 (âœ“) | In general, consumer advocates believe that any <ins>comprehensive federal privacy policy</ins> should complement, and not supplant, sector-specific <ins>privacy legislation</ins> or state-level legislation. |
| 303 (âœ“) | <ins>Finding a global consensus on how to balance open data flows and privacy protection may be key to</ins> maintaining trust in the digital environment and advancing international trade. |
| 304 (âœ—) | One study found that over 120 countries have laws related to personal data protection. |
| 305 (âœ—) | Divergent national privacy approaches raise the costs of doing business and make it harder for governments to collaborate and share data, whether for scientific research, defense, or law enforcement. |
| 306 (âœ“) | <ins>A system for global interoperability</ins> in a least trade-restrictive and nondiscriminatory way between <ins>different national systems</ins> could help minimize costs and allow entities in different jurisdictions with varying online <ins>privacy regimes</ins> to share data via cross-border data flows. |
| Query | Please write a one-page summary of the above government report. |
| Response (only single statement due to space) | [â€¦] The report concludes by noting that <ins>finding a global consensus on how to balance open data flows and privacy protection may be key to maintaining trust in the digital environment and advancing international trade.</ins> The report suggests that Congress may consider <ins>comprehensive privacy legislation</ins> and examine the potential challenges and implications of building <ins>a system of interoperability between different national privacy regimes.</ins> [â€¦] |
|  |  |
| BoN Candidates | Citation Strings (<ins>green: correct</ins>; <ins>red: wrong</ins>) | Missing Citations | SelfCite Reward |
| (1) Best candidate | [302-303][306-306] | â€“ | 0.578 |
| (2) Direct sampling | [303-303][<ins>305</ins>-306] | (302) | 0.547 |
| (3) Other candidate | [303-<ins>304</ins>][<ins>308-308</ins>][<ins>310-311</ins>] | (302, 306) | 0.461 |
| (4) Other candidate | [303-303][<ins>309-309</ins>][<ins>311-311</ins>] | (302, 306) | 0.375 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 5ëŠ” ê¸°ì¤€ ëª¨ë¸ê³¼ BoN(Best-of-N) ìƒ˜í”Œë§ ë°©ë²•ì„ ì‚¬ìš©í–ˆì„ ë•Œ ìƒì„±ëœ ì¸ìš© ì •ë³´ì˜ ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. ë³¸ ì˜ˆì‹œì—ì„œëŠ” ë§¥ë½(context)ê³¼ ì‘ë‹µ(response)ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê°•ì¡°í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤. ê¸°ì¤€ ëª¨ë¸ì€ ë§¥ë½ ë¬¸ì¥ 302ë²ˆê³¼ 306ë²ˆì„ ëˆ„ë½í•˜ê³  ë§¥ë½ ë¬¸ì¥ 305ë²ˆì„ ì˜ëª» ì¸ìš©í•œ ë°˜ë©´, BoN ìƒ˜í”Œë§ ë°©ë²•ì„ ì‚¬ìš©í•œ ëª¨ë¸ì€ ë§¥ë½ ë¬¸ì¥ 302ë²ˆê³¼ 306ë²ˆì„ ì˜¬ë°”ë¥´ê²Œ ì¸ìš©í•˜ê³  ë§¥ë½ ë¬¸ì¥ 305ë²ˆì€ ì œì™¸í•˜ì—¬ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 5: An example of differences in the citation from baseline vs BoN. Related information are highlighted in the context/response.
> </details>

{{< table-caption >}}
| BoN Candidates | Citation Strings (<span class="ltx_text" id="S4.T5.4.1.9.8.1.1.1.2.1.1" style="color:#228B22;">green: correct</span>; <span class="ltx_text" id="S4.T5.4.1.9.8.1.1.1.2.1.2" style="color:#FF0000;">red: wrong</span>) | Missing Citations | SelfCite Reward |
|---|---|---|---|
| (1) Best candidate | <span class="ltx_text ltx_font_typewriter" id="S4.T5.4.1.9.8.1.1.2.2.1" style="font-size:90%;color:#228B22;">[302-303][306-306]</span> | â€“ | 0.578 |
| (2) Direct sampling | <span class="ltx_text ltx_font_typewriter" id="S4.T5.4.1.9.8.1.1.3.2.1" style="font-size:90%;color:#228B22;">[303-303][<span class="ltx_text" id="S4.T5.4.1.9.8.1.1.3.2.1.1" style="color:#FF0000;">305</span>-306]</span> | <span class="ltx_text" id="S4.T5.4.1.9.8.1.1.3.3.1" style="font-size:90%;color:#BF0040;">(302)</span> | 0.547 |
| (3) Other candidate | <span class="ltx_text ltx_font_typewriter" id="S4.T5.4.1.9.8.1.1.4.2.1" style="font-size:90%;color:#228B22;">[303-<span class="ltx_text" id="S4.T5.4.1.9.8.1.1.4.2.1.1" style="color:#FF0000;">304</span>]<span class="ltx_text" id="S4.T5.4.1.9.8.1.1.4.2.1.2" style="color:#FF0000;">[308-308][310-311]</span></span> | <span class="ltx_text" id="S4.T5.4.1.9.8.1.1.4.3.1" style="font-size:90%;color:#BF0040;">(302, 306)</span> | 0.461 |
| (4) Other candidate | <span class="ltx_text ltx_font_typewriter" id="S4.T5.4.1.9.8.1.1.5.2.1" style="font-size:90%;color:#228B22;">[303-303]<span class="ltx_text" id="S4.T5.4.1.9.8.1.1.5.2.1.1" style="color:#FF0000;">[309-309][311-311]</span></span> | <span class="ltx_text" id="S4.T5.4.1.9.8.1.1.5.3.1" style="font-size:90%;color:#BF0040;">(302, 306)</span> | 0.375 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 6ì€ LongBench-Cite ë²¤ì¹˜ë§ˆí¬ì—ì„œ í‰ê°€ëœ ì¸ìš© ì¬í˜„ìœ¨(R), ì¸ìš© ì •ë°€ë„(P), ì¸ìš© F1 ì ìˆ˜(F1) ë° ì¸ìš© ê¸¸ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ê° ì§€í‘œëŠ” ë‹¤ì„¯ ê°€ì§€ ì§ˆë¬¸ì‘ë‹µ(QA) ë°ì´í„°ì…‹(Longbench-Chat, MultiFieldQA, HotpotQA, DuReader, GovReport)ì— ëŒ€í•´ ê³„ì‚°ë©ë‹ˆë‹¤.  í‘œì—ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸(ë…ì  ëª¨ë¸, ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸, ContextCite ê¸°ë°˜ ëª¨ë¸, SelfCite ëª¨ë¸ ë“±)ì˜ ê²°ê³¼ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ SelfCiteì˜ íš¨ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  Zhang et al.(2024)ì˜ ê²°ê³¼ë„ ë¹„êµë¥¼ ìœ„í•´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  ê°€ì¥ ì¢‹ì€ ê²°ê³¼ëŠ” êµµê²Œ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. 
> <details>
> <summary>read the caption</summary>
> Table 6: Citation recall (R), citation precision (P), citation F1 (F1), and citation length evaluated on LongBench-Cite benchmark. The best results are bolded. â€  indicates the results taken fromÂ Zhang etÂ al. (2024).
> </details>

{{< table-caption >}}
| Model | Longbench-Chat |  |  | MultifieldQA |  |  | HotpotQA |  |  | Dureader |  |  | GovReport |  | Avg. | Citation |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|  | R | P | F1 | R | P | F1 | R | P | F1 | R | P | F1 | R | P | F1 | Length |
| **Proprietary models** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| GPT-4o<sup>â€ </sup> | 46.7 | 53.5 | 46.7 | 79.0 | 87.9 | 80.6 | 55.7 | 62.3 | 53.4 | 65.6 | 74.2 | 67.4 | 73.4 | 90.4 | 79.8 | 220 |
| Claude-3-sonnet<sup>â€ </sup> | 52.0 | 67.8 | 55.1 | 64.7 | 85.8 | 71.3 | 46.4 | 65.8 | 49.9 | 67.7 | 89.2 | 75.5 | 77.4 | 93.9 | 84.1 | 132 |
| GLM-4<sup>â€ </sup> | 47.6 | 53.9 | 47.1 | 72.3 | 80.1 | 73.6 | 47.0 | 50.1 | 44.4 | 73.4 | 82.3 | 75.0 | 82.8 | 93.4 | 87.1 | 169 |
| **Ours: SelfCite** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| LongCite-8B (Our repro.) | 67.0 | 78.1 | 66.6 | 74.8 | 90.7 | 79.9 | 60.8 | 77.9 | 64.1 | 67.1 | 87.2 | 73.7 | 81.6 | 89.3 | 84.5 | 83.5 |
| + BoN | 68.4 | 81.3 | 71.2 | 76.1 | 92.8 | 81.2 | 67.2 | 81.0 | 68.8 | 70.6 | 90.9 | 76.9 | 87.6 | 92.4 | 89.3 | 93.4 |
| + SimPO | 68.1 | 79.5 | 69.1 | 75.5 | 92.6 | 81.0 | 69.4 | 82.3 | 71.5 | 72.7 | 91.6 | 78.9 | 86.4 | 92.9 | 89.1 | 105.7 |
| + SimPO then BoN | 73.3 | 79.4 | 72.8 | 76.7 | 93.2 | 82.2 | 69.4 | 83.0 | 71.1 | 74.2 | 92.2 | 80.3 | 86.7 | 92.7 | 89.2 | 94.7 |
| **Topline** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| *Claude Citations* | 61.2 | 81.7 | 67.8 | 76.8 | 98.4 | 84.9 | 61.9 | 94.1 | 72.9 | 88.5 | 99.7 | 93.2 | 79.4 | 99.2 | 87.7 | 88.8 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 7ì€ ê¸°ì¤€ ëª¨ë¸ê³¼ BoN(Best-of-N) ìƒ˜í”Œë§ ë°©ë²•ì„ ì‚¬ìš©í–ˆì„ ë•Œ ì¸ìš© ë¶€ë¶„ì˜ ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. ë§¥ë½(context)ê³¼ ì‘ë‹µ(response)ì—ì„œ ê´€ë ¨ ì •ë³´ê°€ ê°•ì¡° í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¤€ ëª¨ë¸ì€ ê´€ë ¨ ì—†ëŠ” ë¬¸ì¥(42, 47-50)ì„ ì¸ìš©í•œ ë°˜ë©´, BoNì€ ê´€ë ¨ ì—†ëŠ” ë¬¸ì¥ì„ ì œì™¸í•˜ê³  ê´€ë ¨ ë¬¸ì¥(23)ì„ ì¶”ê°€í•˜ì—¬ ë” ì •í™•í•œ ì¸ìš©ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 7: An example of differences in the citation from baseline vs BoN. Related information are highlighted in the context/response.
> </details>

{{< table-caption >}}
| Sent. ID | Context Sentences (only showing cited sentences due to space) |
|---|---| 
| 23 (âœ“) | We explored using a simple and cost-effective procedure to instruction finetune our continually pretrained long models without any human-annotated data. |
| 42 (âœ—) | Collecting human demonstration and preference labels for LLM alignment is a cumbersome and expensive process (Ouyang et al., 2022; Touvron et al., 2023). |
| 45 (âœ“) | In this work, we found that a simple and cheap approach which leverages a pre-built large and diverse short-prompt dataset works surprisingly well on long-context benchmarks. |
| 46 (âœ“) | Specifically, we take the RLHF dataset used in LLAMA 2 CHAT and augment it with synthetic self-instruct (Wang et al., 2022) long data generated by LLAMA 2 CHAT itself, in the hope that the model can learn a diverse set of skills through the large amount of RLHF data and transfer that knowledge to long-context scenarios via self-instruct data. |
| 47-50 (âœ—) | The data generation process focuses on QA-format tasks: starting from a long document in our pretraining corpus, we select a random chunk and prompt LLAMA 2 CHAT to write question-answer pairs based on information in the text chunk. [â€¦] |
| Query | What aspects of the LLAMA Long model proposed above have changed relative to the LLAMA-based model? What improvements have been made? |
| Response (single statement due to space) | [â€¦] 3. Instruction Tuning: The paper proposes a simple and cost-effective procedure to instruction finetune the continually pretrained long models without any human-annotated data. This involves using a pre-built large and diverse short-prompt dataset and augmenting it with synthetic self-instruct long data generated by LLAMA CHAT itself. [â€¦] |
| **Citation Strings (green: correct; red: wrong)** |  |
| Baseline | [42-42][45-50] |
| SelfCite BoN | [23-23][45-45][46-46] |{{< /table-caption >}}
> ğŸ”¼ í‘œ 8ì€ SelfCiteì˜ Best-of-N ìƒ˜í”Œë§ ë°©ë²•ì´ ê¸°ì¡´ì˜ ê¸°ë²•ë³´ë‹¤ ì–´ë–»ê²Œ ë” ì •í™•í•˜ê³  íš¨ìœ¨ì ì¸ ì¸ìš©ì„ ìƒì„±í•˜ëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.  ë³¸ ì˜ˆì‹œëŠ” í•˜ë‚˜ì˜ ë¬¸ë‹¨ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì—ì„œ SelfCiteê°€ ì–´ë–¤ ë¬¸ì¥ì„ ì¸ìš©ìœ¼ë¡œ ì„ íƒí•˜ê³ , ì–´ë–¤ ë¬¸ì¥ì„ ì œì™¸í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.  'Context Sentences'ì—ëŠ” ê´€ë ¨ ì •ë³´ê°€ ê°•ì¡° í‘œì‹œë˜ì–´ ìˆê³ , 'Response'ì—ëŠ” ìƒì„±ëœ ì‘ë‹µì´, 'Citation Strings'ì—ëŠ” SelfCiteì™€ ê¸°ì¡´ ë°©ë²•ì´ ê°ê° ì„ íƒí•œ ì¸ìš© ë¬¸ì¥ë“¤ì´, 'Missing Citations'ì—ëŠ” ê° ë°©ë²•ì—ì„œ ëˆ„ë½ëœ ì¸ìš© ë¬¸ì¥ë“¤ì´, 'SelfCite Reward'ì—ëŠ” SelfCite ì ìˆ˜ê°€ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.  ì´ë¥¼ í†µí•´ SelfCiteê°€ ë” ë†’ì€ ì ìˆ˜ë¥¼ ë°›ìœ¼ë©´ì„œ ë”ìš± ì •í™•í•œ ì¸ìš©ì„ ìƒì„±í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ì´ í‘œëŠ” SelfCiteì˜ ê°•ì ì„ ë³´ë‹¤ êµ¬ì²´ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ì§ˆì ì¸ ë¶„ì„ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 8: An example of differences in the citation from baseline vs BoN. Related information are highlighted in the context/response.
> </details>

{{< table-caption >}}
| Sent. ID | Context Sentences (only showing cited sentences due to space) |
|---|---| 
| 299 (âœ—) | Ribosomes link amino acids together in the order specified by the codons of messenger RNA (mRNA) molecules to form polypeptide chains. |
| 300 (âœ“) | Ribosomes consist of two major components: the small and large ribosomal subunits. |
| 301 (âœ“) | Each subunit consists of one or more ribosomal RNA (rRNA) molecules and many ribosomal proteins (RPs or r-proteins). |
| 302 (âœ“) | The ribosomes and associated molecules are also known as the translational apparatus. |
| 383 (âœ—) | Ribosomes are minute particles consisting of RNA and associated proteins that function to synthesize proteins. |
| 384 (âœ—) | Proteins are needed for many cellular functions such as repairing damage or directing chemical processes. |
| 385 (âœ—) | Ribosomes can be found floating within the cytoplasm or attached to the endoplasmic reticulum. |
| 390 (âœ“) | Each subunit is made of one or more rRNAs and many r-proteins. |
| 391 (âœ—) | The small subunit (30S in bacteria and archaea, 40S in eukaryotes) has the decoding function, whereas the large subunit (50S in bacteria and archaea, 60S in eukaryotes) catalyzes the formation of peptide bonds, referred to as the peptidyl-transferase activity. |
| 392 (âœ—) | The bacterial (and archaeal) small subunit contains the 16S rRNA and 21 r-proteins (Escherichia coli), whereas the eukaryotic small subunit contains the 18S rRNA and 32 r-proteins (Saccharomyces cerevisiae; although the numbers vary between species). |
| 393 (âœ—) | The bacterial large subunit contains the 5S and 23S rRNAs and 34 r-proteins (E. coli), with the eukaryotic large subunit containing the 5S, 5.8S, and 25S / 28S rRNAs and 46 r-proteins (S. cerevisiae; again, the exact numbers vary between species). |
| Query | The large subunit and small subunit that use two types of RNA are major components that make up what? |
| Response (single statement due to space) | [â€¦] The ribosome consists of two major components: the small and large ribosomal subunits.  Each subunit consists of one or more ribosomal RNA (rRNA) molecules and many ribosomal proteins (RPs or r-proteins).  The ribosomes and associated molecules are also known as the translational apparatus. [â€¦] |
| Citation Strings (green: correct; red: wrong) |  | 
| Baseline | [299-302][383-385][390-393] |
| SelfCite BoN | [300-302][390-393] |{{< /table-caption >}}
> ğŸ”¼ í‘œ 9ëŠ” ê¸°ì¤€ ëª¨ë¸ê³¼ BoN(Best-of-N) ìƒ˜í”Œë§ ê²°ê³¼ì˜ ì¸ìš© ë¶€ë¶„ ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. ë³¸ ì˜ˆì‹œì—ì„œëŠ” ë§¥ë½(context)ê³¼ ì‘ë‹µ(response)ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê°•ì¡°í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤. ê¸°ì¤€ ëª¨ë¸ì€ ë§¥ë½ì˜ ì¼ë¶€ë¶„ë§Œì„ ì¸ìš©í•˜ê±°ë‚˜, ê´€ë ¨ ì—†ëŠ” ë¶€ë¶„ì„ ì¸ìš©í•˜ëŠ” ë°˜ë©´, BoNì€ ë§¥ë½ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ë” ì •í™•í•˜ê²Œ ì¸ìš©í•©ë‹ˆë‹¤.  ì´ë¥¼ í†µí•´ SelfCite ë°©ë²•ë¡ ì´ ë§¥ë½ ì •ë³´ë¥¼ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ì •í™•í•œ ì¸ìš©ì„ ìƒì„±í•˜ëŠ” ë° ë„ì›€ì´ ë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 9: An example of differences in the citation from baseline vs BoN. Related information are highlighted in the context/response.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}