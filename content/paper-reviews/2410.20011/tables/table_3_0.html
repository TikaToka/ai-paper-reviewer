<table id='0' style='font-size:14px'><tr><td>Technique</td><td>General Mechanism</td><td colspan="6">Compute Size Space Runtime Training Dataset Inference Memory Storage Latency</td></tr><tr><td rowspan="3">Model Architectures (Sec. 2)</td><td>Lightweight Models (Sec. 2.1)</td><td>V</td><td></td><td>V</td><td>V</td><td></td><td>V</td></tr><tr><td>Efficient Self-Attention (Sec. 2.2)</td><td>V</td><td></td><td>V</td><td>V</td><td></td><td>V</td></tr><tr><td>Neural Arch. Search (Sec. 2.3)</td><td></td><td></td><td>V</td><td>V</td><td>V</td><td></td></tr><tr><td rowspan="2">Training Techniques (Sec. 3)</td><td>Pre-training (Sec. 3.1)</td><td>V</td><td>V</td><td>V</td><td></td><td>V</td><td></td></tr><tr><td>Finetuning (Sec. 3.2)</td><td>V</td><td>V</td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="3">Model Compression (Sec. 4)</td><td>Pruning (Sec. 4.1)</td><td></td><td></td><td>V</td><td>V</td><td>V</td><td>V</td></tr><tr><td>Quantization (Sec. 4.2)</td><td></td><td></td><td>V</td><td>V</td><td>V</td><td>V</td></tr><tr><td>Knowledge Distillation (Sec. 4.3)</td><td></td><td>V</td><td></td><td></td><td></td><td></td></tr></table>