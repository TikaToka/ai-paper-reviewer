{"references": [{"fullname_first_author": "Ruiz", "paper_title": "DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation", "publication_date": "2023-00-00", "reason": "This paper introduces a method for customizing text-to-image diffusion models, which is highly relevant to the paper's focus on personalized video generation."}, {"fullname_first_author": "Yang", "paper_title": "CogVideoX: Text-to-video diffusion models with an expert transformer", "publication_date": "2024-00-00", "reason": "This paper is the foundation of the model architecture used in the current paper, providing the base video diffusion transformer."}, {"fullname_first_author": "Jiang", "paper_title": "VideoBooth: Diffusion-based video generation with image prompts", "publication_date": "2024-00-00", "reason": "This paper is a key reference for customized video generation, which is directly compared against in the current paper's experiments."}, {"fullname_first_author": "Ho", "paper_title": "Video diffusion models", "publication_date": "2022-00-00", "reason": "This is a foundational paper for video diffusion models, providing the underlying technology upon which this paper builds."}, {"fullname_first_author": "Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-00-00", "reason": "This paper is foundational to diffusion models and provides the base technology for several methods referenced in the current paper."}]}