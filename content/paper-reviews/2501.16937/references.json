{"references": [{"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-12-01", "reason": "This paper introduced the foundational concept of knowledge distillation, a technique that TAID builds upon and improves."}, {"fullname_first_author": "Cristian Bucilu\n", "paper_title": "Model compression", "publication_date": "2006-01-01", "reason": "This paper introduced model compression, a key concept in the field that TAID is designed to advance."}, {"fullname_first_author": "Seyed Iman Mirzadeh", "paper_title": "Improved knowledge distillation via teacher assistant", "publication_date": "2020-01-01", "reason": "This paper highlighted the problem of capacity gap in knowledge distillation, a significant challenge that TAID aims to address."}, {"fullname_first_author": "Yuqiao Wen", "paper_title": "f-divergence minimization for sequence-level knowledge distillation", "publication_date": "2023-01-01", "reason": "This paper explored alternative objective functions for knowledge distillation to address mode collapse, a problem that TAID also tackles."}, {"fullname_first_author": "Rishabh Agarwal", "paper_title": "On-policy distillation of language models: Learning from self-generated mistakes", "publication_date": "2024-01-01", "reason": "This paper introduced a method that uses student-generated data during training, a technique that TAID contrasts with and improves upon."}]}