{"references": [{"fullname_first_author": "Tian, L.", "paper_title": "EMO: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions", "publication_date": "2025", "reason": "This paper is foundational to the current work, extending its approach to full body animation and co-speech gestures."}, {"fullname_first_author": "Guo, Y.", "paper_title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning", "publication_date": "2023-07-04", "reason": "This paper introduces the diffusion model architecture used in Stage 2, which enables the generation of realistic facial expressions and body movements."}, {"fullname_first_author": "Ho, J.", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020", "reason": "This paper introduces the foundational diffusion model framework on which the current work's two-stage approach is based."}, {"fullname_first_author": "Radford, A.", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021", "reason": "This paper introduces the CLIP model used for generating the style embedding, which allows for stylistic control over hand motion generation."}, {"fullname_first_author": "Romero, J.", "paper_title": "Embodied hands: Modeling and capturing hands and bodies together", "publication_date": "2017", "reason": "This paper introduces the MANO model used for hand representation, which is crucial to generating realistic hand poses in the first stage."}]}