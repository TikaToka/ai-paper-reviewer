<table id='6' style='font-size:14px'><tr><td>Metric</td><td>Definition</td><td>Description</td></tr><tr><td>IoU</td><td>Area of Overlap IoU = Area of Union TP</td><td>Measures the overlap between predicted and ground truth boxes.</td></tr><tr><td>ReCall</td><td>ReCall = TP + FN N</td><td>Measures how many true positive samples are correctly predicted by the model.</td></tr><tr><td>mAP</td><td>1 mAP = APi N i=1 M 1</td><td>Average precision across all classes, assessing overall model performance.</td></tr><tr><td>mAP@IoU[a:b]</td><td>mAP@IoU[a:b] = mAPIâŒ€U j M j=1</td><td>Computes over a range of IoU thresholds [a, b], calculating at specified intervals and averaged.</td></tr><tr><td>F1-score</td><td>Precision X Recall F1-score = 2 x Precision + Recall</td><td>Balances precision and recall and useful in imbalanced class scenarios.</td></tr></table>