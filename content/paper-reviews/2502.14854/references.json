{"references": [{"fullname_first_author": "Marzena Karpinska", "paper_title": "One Thousand and One Pairs: A \"novel\" challenge for long-context language models", "publication_date": "2024-06-16", "reason": "This paper introduces the NoCha benchmark, which is used for evaluating the model's performance and is the main benchmark used in the paper's experimental section."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-21", "reason": "This paper introduces the Llama-3 model, which is one of the models used in the paper for fine-tuning and comparison, playing a crucial role in the results."}, {"fullname_first_author": "Tianyu Gao", "paper_title": "How to Train Long-Context Language Models (Effectively)", "publication_date": "2024-10-02", "reason": "This paper provides valuable insights into the training strategies for long-context language models, which are directly relevant to the method proposed in this paper."}, {"fullname_first_author": "Qwen", "paper_title": "Qwen2.5 Technical Report", "publication_date": "2024-12-15", "reason": "This paper introduces the Qwen-2.5 model, which is another important model used for comparison and fine-tuning in the paper."}, {"fullname_first_author": "Yushi Bai", "paper_title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models!", "publication_date": "2024-01-18", "reason": "This paper explores a method for long-context alignment of LLMs, which addresses a problem related to the paper's research topic."}]}