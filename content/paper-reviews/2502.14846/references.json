{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces a foundational vision-language model that significantly advanced the field and is directly relevant to the current work's approach to multimodal learning."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-01-01", "reason": "This paper demonstrates the effectiveness of chain-of-thought prompting, a technique that is directly used and analyzed in the present work for improved reasoning capabilities in vision-language models."}, {"fullname_first_author": "Matt Deitke", "paper_title": "Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models", "publication_date": "2024-09-01", "reason": "This paper provides a strong baseline for comparison and shares similar methodological approaches to synthetic data generation, making it a crucial comparative point for this work."}, {"fullname_first_author": "Manoj Acharya", "paper_title": "TallyQA: Answering complex counting questions", "publication_date": "2019-01-01", "reason": "This paper introduces a benchmark dataset relevant to the current work's focus on text-rich visual question answering, serving as a key comparative dataset for evaluating model performance."}, {"fullname_first_author": "Yash Goyal", "paper_title": "Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering", "publication_date": "2017-07-01", "reason": "This paper is a seminal work in Visual Question Answering (VQA), a field highly related to the present work, and its focus on image understanding is directly relevant to improving VLM's interpretation of text-rich images."}]}