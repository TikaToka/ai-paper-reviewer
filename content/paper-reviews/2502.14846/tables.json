[{"content": "| Model | ChartQA | DocVQA | InfoVQA | TableVQA | AI2D | TextVQA | ScreenQA | Average |\n|---|---|---|---|---|---|---|---|---|\n| GPT-4V | 78.1 | 87.2 | 75.1 | 60.5 | 89.4 | 78.0 | 41.6 | 72.8 |\n| Gemini 1.5 Flash | 85.4 | 89.9 | 75.3 | 72.6 | 91.7 | 78.7 | 40.1 | 76.2 |\n| Claude-3 Opus | 80.8 | 89.3 | 55.6 | 70.0 | 88.1 | 67.5 | 39.8 | 70.2 |\n| PaliGemma-3B<sup>\u2020</sup> | 71.4 | 84.8 | 47.8 | 46.4 | 73.3 | 76.5 | 32.2 | 61.8 |\n| BLIP-3-4B<sup>\u2020</sup> | 60.0 | 61.4 | 31.5 | 24.3 | 74.2 | 71.0 | 26.2 | 49.8 |\n| Cambrian-7B<sup>\u2020</sup> | 73.3 | 77.8 | 41.6 | 40.6 | 73.0 | 71.7 | 44.4 | 64.2 |\n| LLaVA-1.5-7B<sup>\u2020</sup><sup>\u2217</sup> | 17.8 | 28.1 | 25.8 | 33.1 | 55.5 | 58.2 | 17.6 | 33.7 |\n| LLaVA-Next-8B<sup>\u2020</sup> | 69.5 | 78.2 | 43.8 | 43.9 | 71.6 | 65.3 | 34.2 | 58.1 |\n| LLaVA-OneVision-7B<sup>\u2020</sup> | 80.0 | 87.5 | 68.8 | 64.6 | 81.4 | 78.3 | 46.3 | 72.4 |\n| Pixtral-12B | 81.8 | 90.7 | 50.8 | 67.0 | 79.0 | 75.7 | 39.4 | 69.2 |\n| Llama 3.2 11B | 83.4 | 88.4 | 63.6 | 51.1 | 91.9 | 73.1 | 87.7 | 77.0 |\n| Ours (7B)<sup>\u2020</sup> | 86.3 | 90.0 | 70.5 | 65.8 | 91.9 | 82.0 | 80.1 | 80.9 |\n| Ours (7B-zero-shot)<sup>\u2020</sup><sup>\u2217</sup> | 80.8 | 82.9 | 59.8 | 64.9 | 83.9 | 72.7 | 78.1 | 74.7 |", "caption": "Table 1: Results on 7 text-rich benchmarks. The result of the best-performing open-source model is bold, and the second-best is underlined. Models with \u2020 stand for open data and code for multimodal training. Models with \u2217 are zero-shot models, which means the models are not trained on instances from any of the evaluation datasets.", "description": "\ud45c 1\uc740 7\uac00\uc9c0 \ud14d\uc2a4\ud2b8\uac00 \ud48d\ubd80\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ube44\uc804-\uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ucd5c\uace0 \uc131\ub2a5\uc758 \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378 \uacb0\uacfc\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5c8\uace0, \ub450 \ubc88\uc9f8\ub85c \uc88b\uc740 \uc131\ub2a5\uc740 \ubc11\uc904\uc774 \uadf8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4.  \u2020 \ud45c\uc2dc\ub294 \ub2e4\uc911 \ubaa8\ub4dc \ud559\uc2b5\uc744 \uc704\ud574 \uc624\ud508 \ub370\uc774\ud130 \ubc0f \ucf54\ub4dc\ub97c \uc0ac\uc6a9\ud55c \ubaa8\ub378\uc784\uc744 \ub098\ud0c0\ub0b4\uace0, * \ud45c\uc2dc\ub294 \ud3c9\uac00 \ub370\uc774\ud130\uc14b\uc758 \uc778\uc2a4\ud134\uc2a4\ub85c \ud559\uc2b5\ub418\uc9c0 \uc54a\uc740 \uc81c\ub85c\uc0f7 \ubaa8\ub378\uc784\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uc989, \uc774 \ud45c\ub294 \uc81c\ub85c\uc0f7 \uc124\uc815\uacfc \ub2e4\uc591\ud55c \ud559\uc2b5 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud55c \uc124\uc815 \ubaa8\ub450\uc5d0\uc11c \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \ud6a8\uacfc\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5 Experimental Setup"}, {"content": "| ChartQA | Average | Machine | Human | \u0394\u2193 |\n|---|---|---|---|---|\n| PaliGemma-3B | 71.4 | 88.5 | 54.2 | 34.3 |\n| ChartPali-5B | 77.3 | 93.7 | 60.9 | 32.8 |\n| Ours (w/o Syn) | 81.4 | 92.2 | 70.4 | 21.8 |\n| Ours (w/ Syn) | 86.3 | 93.4 | 79.1 | 14.2 |", "caption": "Table 2: Results on human and machine-generated questions of ChartQA.\nThe pie charts above display the percentage distribution of two question types in training and testing.\n\u0394\u0394\\Deltaroman_\u0394 (\u2193\u2193\\downarrow\u2193 lower is better) denotes the performance gap between human and machine questions.", "description": "\ud45c 2\ub294 ChartQA \ub370\uc774\ud130\uc14b\uc758 \uc9c8\ubb38 \uc720\ud615\uc744 \uc0ac\ub78c\uc774 \uc791\uc131\ud55c \uc9c8\ubb38\uacfc \uae30\uacc4\uac00 \uc0dd\uc131\ud55c \uc9c8\ubb38\uc73c\ub85c \ub098\ub204\uc5b4 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc704\ucabd\uc758 \uc6d0\ud615 \ucc28\ud2b8\ub294 \ud559\uc2b5 \ubc0f \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub450 \uac00\uc9c0 \uc9c8\ubb38 \uc720\ud615\uc758 \ube44\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \u0394(\uc791\uc744\uc218\ub85d \uc88b\uc74c)\ub294 \uc0ac\ub78c\uc774 \uc791\uc131\ud55c \uc9c8\ubb38\uacfc \uae30\uacc4\uac00 \uc0dd\uc131\ud55c \uc9c8\ubb38 \uac04\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uae30\uacc4\uac00 \uc0dd\uc131\ud55c \uc9c8\ubb38\uc5d0 \ub300\ud574 \ubaa8\ub378\uc774 \uc5bc\ub9c8\ub098 \uacfc\uc801\ud569\ub418\ub294\uc9c0, \uadf8\ub9ac\uace0 \uc0ac\ub78c\uc774 \uc791\uc131\ud55c \uc9c8\ubb38\uc5d0 \ub300\ud55c \uc77c\ubc18\ud654 \ub2a5\ub825\uc740 \uc5b4\ub290 \uc815\ub3c4\uc778\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ub370 \uc911\uc810\uc744 \ub461\ub2c8\ub2e4.", "section": "6.2 \ubd84\uc11d"}, {"content": "| **Model** | **Mobile** |  | **Desktop** |  | **Web** |  | **Avg** |\n|---|---|---|---|---|---|---|---| \n|  | **Text** | **Icon** | **Text** | **Icon** | **Text** | **Icon** |  |\n| GPT-4o | 20.2 | 24.9 | 21.1 | 23.6 | 12.2 | 7.8 | 18.3 |\n| CogAgent | 67.0 | 24.0 | 74.2 | 20.0 | 70.4 | 28.6 | 47.4 |\n| SeeClick | 78.0 | 52.0 | 72.2 | 30.0 | 55.7 | 32.5 | 53.4 |\n| UGround | 82.8 | 60.3 | 82.5 | 63.6 | 80.4 | 70.4 | 73.3 |\n| Synthetic | 90.8 | 53.3 | 78.4 | 58.6 | 80.0 | 47.1 | 68.0 |\n| Human | 84.2 | 59.0 | 88.1 | 52.9 | 76.5 | 50.5 | 68.5 |\n| Combined | 89.0 | 65.1 | 87.6 | 65.7 | 83.0 | 58.7 | 74.9 |", "caption": "Table 3: Click accuracy on ScreenSpot. We report our models trained on different pointing data. Human stands for using the human-annotated data from PixMo-point Deitke et\u00a0al. (2024). Combined means combining human-annotated data with our synthetic pointing data.", "description": "\ud45c 3\uc740 ScreenSpot \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \ud074\ub9ad \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378\uc740 \ub2e4\uc591\ud55c \ud3ec\uc778\ud305 \ub370\uc774\ud130 (PixMo-point\uc758 \uc0ac\ub78c\uc774 \uc9c1\uc811 \uc8fc\uc11d\uc744 \ub2e8 \ub370\uc774\ud130 \ud3ec\ud568)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ub418\uc5c8\uc73c\uba70, Human\uc740 PixMo-point(Deitke et al., 2024)\uc758 \uc0ac\ub78c\uc774 \uc8fc\uc11d\uc744 \ub2e8 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud55c \uacb0\uacfc\ub97c, Combined\ub294 \uc0ac\ub78c\uc774 \uc8fc\uc11d\uc744 \ub2e8 \ub370\uc774\ud130\uc640 \ud569\uc131 \ub370\uc774\ud130\ub97c \ud568\uaed8 \uc0ac\uc6a9\ud55c \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  Human-annotated \ub370\uc774\ud130\uc640 \ud569\uc131 \ub370\uc774\ud130\ub97c \uacb0\ud569\ud55c \uacb0\uacfc\uac00 \uc5b4\ub5bb\uac8c \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6.3 Synthetic Pointing Data"}, {"content": "| Dataset | Image Diversity | Text Diversity |\n|---|---|---|\n| FigureQA | 0.268 | 0.567 |\n| DVQA | 0.307 | 0.752 |\n| PlotQA | 0.420 | 0.743 |\n| ChartQA | 0.340 | 0.742 |\n| Ours (Charts) | **0.596** | **0.823** |", "caption": "Table 4: Compare image and text diversity across different chart datasets. We randomly sample 10K instances from each dataset to compute the results.", "description": "\ud45c 4\ub294 \uc11c\ub85c \ub2e4\ub978 \ucc28\ud2b8 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8 \ub2e4\uc591\uc131\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ubb34\uc791\uc704\ub85c 1\ub9cc \uac1c\uc758 \uc778\uc2a4\ud134\uc2a4\ub97c \uc0d8\ud50c\ub9c1\ud558\uc5ec \uacb0\uacfc\ub97c \uacc4\uc0b0\ud588\uc2b5\ub2c8\ub2e4. \uc774\ubbf8\uc9c0 \ub2e4\uc591\uc131\uc740 CLIP\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0 \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud558\uace0, \ud14d\uc2a4\ud2b8 \ub2e4\uc591\uc131\uc740 Sentence-BERT\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc9c8\ubb38\uacfc \ub2f5\ubcc0 \uc30d\uc758 \uc784\ubca0\ub529\uc744 \uc5bb\uc5b4 \uacc4\uc0b0\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uacb0\uacfc\ub294 FigureQA, DVQA, PlotQA \ubc0f ChartQA \ub370\uc774\ud130\uc14b\uacfc \ube44\uad50\ud558\uc5ec \uc81c\uc2dc\ub429\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ubcf8 \uc5f0\uad6c\uc5d0\uc11c \uc0dd\uc131\ud55c \ud569\uc131 \ucc28\ud2b8 \ub370\uc774\ud130\uc758 \uc774\ubbf8\uc9c0 \ubc0f \ud14d\uc2a4\ud2b8 \ub2e4\uc591\uc131\uc774 \uae30\uc874 \ub370\uc774\ud130\uc14b\ubcf4\ub2e4 \ud6e8\uc52c \ub192\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6.2 Analysis"}, {"content": "| n. of Tools | Diversity | ChartQA |  |  |\n|---|---|---|---|---|\n| Single | 0.572 | 73.9 | 66.5 | 81.5 |\n| Multiple | **0.607** | **75.2** | **68.6** | **82.0** |", "caption": "Table 5: Single vs. Multiple Rendering Tools for Data Generation. Each row uses the same number of 45K synthetic images. Single only uses Matplotlib, while Multiple involves four other rendering tools: HTML, LaTex, Plotly, and VegaLite.", "description": "\ud45c 5\ub294 \ub370\uc774\ud130 \uc0dd\uc131\uc744 \uc704\ud55c \ub2e8\uc77c \ub80c\ub354\ub9c1 \ub3c4\uad6c\uc640 \ub2e4\uc911 \ub80c\ub354\ub9c1 \ub3c4\uad6c \uc0ac\uc6a9\uc758 \ud6a8\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01\uac01\uc758 \uacbd\uc6b0 45,000\uac1c\uc758 \ud569\uc131 \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \ub2e8\uc77c \ub3c4\uad6c \uc870\uac74\uc5d0\uc11c\ub294 Matplotlib\ub9cc\uc744 \uc0ac\uc6a9\ud588\uace0, \ub2e4\uc911 \ub3c4\uad6c \uc870\uac74\uc5d0\uc11c\ub294 HTML, LaTeX, Plotly, VegaLite\ub97c \ucd94\uac00\uc801\uc73c\ub85c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \ud45c\ud604 \ubc29\uc2dd\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud30c\uc545\ud558\uace0, \ub2e4\uc591\ud55c \ub3c4\uad6c \ud65c\uc6a9\uc758 \ud6a8\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "B \ucd94\uac00 \ubd84\uc11d"}, {"content": "| Prompt Type | ChartQA | DocVQA | InfoVQA | TableVQA | AI2D | TextVQA | ScreenQA | NutritionQA |\n|---|---|---|---|---|---|---|---|---|\n| CoT | **86.3** | 87.4 | 63.8 | **65.8** | 86.0 | 70.9 | 79.0 | **76.0** |\n| Short Answer | 83.1 | **90.0** | **70.5** | 64.3 | **91.9** | **82.0** | **80.1** | 62.0 |", "caption": "Table 6: Alation of using chain-of-thought (CoT) in prompts. CoT means letting the model provide reasoning steps before giving the final answer. Short Answer prompts the model to answer with as few words as possible.", "description": "\ud45c 6\uc740 \ud504\ub86c\ud504\ud2b8\uc5d0\uc11c chain-of-thought (CoT) \ucd94\ub860\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. CoT\ub294 \ubaa8\ub378\uc774 \ucd5c\uc885 \ub2f5\ubcc0\uc744 \uc81c\uacf5\ud558\uae30 \uc804\uc5d0 \ucd94\ub860 \ub2e8\uacc4\ub97c \uc81c\uacf5\ud558\ub3c4\ub85d \ud558\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc9e7\uc740 \ub2f5\ubcc0\uc740 \ubaa8\ub378\uc774 \uac00\ub2a5\ud55c \ud55c \uc801\uc740 \ub2e8\uc5b4\ub85c \ub2f5\ubcc0\ud558\ub3c4\ub85d \ud558\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub450 \uac00\uc9c0 \ud504\ub86c\ud504\ud2b8 \uc720\ud615(CoT \ubc0f \uc9e7\uc740 \ub2f5\ubcc0)\uc5d0 \ub300\ud55c 7\uac00\uc9c0 \ud14d\uc2a4\ud2b8 \uae30\ubc18 \ubca4\uce58\ub9c8\ud06c\uc758 \uacb0\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec CoT \ucd94\ub860\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uac01 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ubcf8\ubb38\uc744 \ucc38\uc870\ud558\uc2ed\uc2dc\uc624.", "section": "6.2 \ubd84\uc11d"}, {"content": "| FT Data | ChartQA | DocVQA | InfoVQA | TableVQA<sup>\u2020</sup> | AI2D | TextVQA | ScreenQA<sup>\u2020</sup> | Average |\n|---|---|---|---|---|---|---|---|---|\n| Aux only<sup>\u2217</sup> | 60.7 | 56.2 | 39.7 | 43.1 | 81.7 | 68.5 | 61.3 | 58.7 |\n| Syn only<sup>\u2217</sup> | 79.4 | 80.5 | 60.1 | 64.4 | 68.6 | 63.6 | 76.6 | 70.5 |\n| Aux + Syn<sup>\u2217</sup> | 80.8 | 82.9 | 59.8 | 64.9 | 83.9 | 72.7 | 78.1 | 74.7 |\n| Eval only | 77.4 | 87.4 | 63.8 | 51.8 | 91.3 | 81.1 | 78.1 | 75.9 |\n| Eval + Aux | 81.4 | 87.9 | 68.2 | 53.6 | 91.6 | 81.8 | 77.0 | 77.3 |\n| Eval + Aux + Syn | **86.3** | **90.0** | **70.5** | **65.8** | **91.9** | **82.0** | **80.1** | **80.9** |", "caption": "Table 7: Alation of the data selection for supervised fine-tuning. Aux, Syn, and Eval stand for auxiliary, synthetic, and evaluation datasets, respectively. The rows with \u2217 represent zero-shot models (without using any training examples from any of the evaluation datasets). The datasets with \u2020 are test-only datasets (no training splits), which means all numbers on these datasets are zero-shot performance.", "description": "\ud45c 7\uc740 \uc9c0\ub3c4 \ud559\uc2b5 \ubbf8\uc138 \uc870\uc815\uc744 \uc704\ud55c \ub370\uc774\ud130 \uc120\ud0dd\uc5d0 \ub300\ud55c \uc808\uc0ad \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Aux, Syn, Eval\uc740 \uac01\uac01 \ubcf4\uc870, \ud569\uc131, \ud3c9\uac00 \ub370\uc774\ud130\uc14b\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \u2217 \ud45c\uc2dc\uac00 \uc788\ub294 \ud589\uc740 \ud3c9\uac00 \ub370\uc774\ud130\uc14b\uc758 \ud559\uc2b5 \uc608\uc2dc\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \uc81c\ub85c\uc0f7 \ubaa8\ub378\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \u2020 \ud45c\uc2dc\uac00 \uc788\ub294 \ub370\uc774\ud130\uc14b\uc740 \ud14c\uc2a4\ud2b8 \uc804\uc6a9 \ub370\uc774\ud130\uc14b(\ud559\uc2b5 \ubd84\ud560 \uc5c6\uc74c)\uc73c\ub85c, \ud574\ub2f9 \ub370\uc774\ud130\uc14b\uc758 \ubaa8\ub4e0 \uc218\uce58\ub294 \uc81c\ub85c\uc0f7 \uc131\ub2a5\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "6.2 \ubd84\uc11d"}, {"content": "| LLM for Data Generation | ChartQA | ChartQA | ChartQA |\n|---|---|---|---|\n|  | Average | Machine | Human |\n| GPT-4o | 72.4 | 65.8 | 78.9 |\n| Claude-3.5-sonnet | 77.2 | 71.0 | 83.8 |", "caption": "Table 8: Compare the LLMs used for synthetic data generation. For both LLMs, we create 100K synthetic charts for fine-tuning the VLMs. We report the zero-shot evaluation results on ChartQA.", "description": "\uc774 \ud45c\ub294 \ud569\uc131 \ub370\uc774\ud130 \uc0dd\uc131\uc5d0 \uc0ac\uc6a9\ub41c \ub450 \uac00\uc9c0 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc778 GPT-40\uacfc Claude-3.5-Sonnet\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub450 \ubaa8\ub378 \ubaa8\ub450 10\ub9cc \uac1c\uc758 \ud569\uc131 \ucc28\ud2b8\ub97c \uc0dd\uc131\ud558\uc5ec VLMs(Vision-Language Models)\ub97c \ubbf8\uc138 \uc870\uc815\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\uc5c8\uc73c\uba70, ChartQA(Chart Question Answering) \ubca4\uce58\ub9c8\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc81c\ub85c\uc0f7(zero-shot) \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uace0\ud569\ub2c8\ub2e4.  \uc81c\ub85c\uc0f7 \ud3c9\uac00\ub294 \ubaa8\ub378\uc774 \ud3c9\uac00 \ub370\uc774\ud130\uc14b\uc758 \uc608\uc2dc\ub97c \uc0ac\uc804\uc5d0 \ud559\uc2b5\ud558\uc9c0 \uc54a\uace0 \ud3c9\uac00\ub97c \uc218\ud589\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uac01 LLM\uc774 \uc0dd\uc131\ud55c \ud569\uc131 \ub370\uc774\ud130\uc758 \uc9c8\uacfc VLMs \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "6.1 \uc8fc\uc694 \uacb0\uacfc"}, {"content": "| Pointing Data | Precision | Recall | F1 | Distance \u2193 |\n|---|---|---|---|---|\n| PixMo-point | 49.7 | 49.3 | 52.7 | 17.3 |\n| Synthetic (Ours) | 63.8 | 66.1 | 62.8 | 9.2 |\n| Combined (Ours) | **69.9** | **70.6** | **70.7** | **8.8** |", "caption": "Table 9: Zero-shot Pointing on DocPointQA. We compare the models trained on different pointing data. Combined stands for combining PixMo-point (human-annotated) Deitke et\u00a0al. (2024) with our synthetic data.", "description": "\ud45c 9\ub294 DocPointQA\uc5d0 \ub300\ud55c \uc81c\ub85c\uc0f7 \ud3ec\uc778\ud305 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc11c\ub85c \ub2e4\ub978 \ud3ec\uc778\ud305 \ub370\uc774\ud130\ub85c \ud559\uc2b5\ub41c \ubaa8\ub378\ub4e4\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. Combined\ub294 Deitke et al.(2024)\uc758 \uc0ac\ub78c\uc774 \uc8fc\uc11d\uc744 \ub2e8 PixMo-point \ub370\uc774\ud130\uc640 \ubcf8 \uc5f0\uad6c\uc758 \ud569\uc131 \ub370\uc774\ud130\ub97c \uacb0\ud569\ud55c \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \ud45c\ub294 \uac01 \ubaa8\ub378\uc758 \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728, F1 \uc810\uc218, \uadf8\ub9ac\uace0 \uc608\uce21 \uc9c0\uc810\uacfc \uc2e4\uc81c \uc9c0\uc810 \uac04\uc758 L2 \uac70\ub9ac\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub0ae\uc740 L2 \uac70\ub9ac\ub294 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ud569\uc131 \ud3ec\uc778\ud305 \ub370\uc774\ud130\uac00  \ud3ec\uc778\ud305 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6.3 Synthetic Pointing Data"}]