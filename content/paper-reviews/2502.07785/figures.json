[{"figure_path": "https://arxiv.org/html/2502.07785/x1.png", "caption": "Figure 1: Pippo generates high-resolution, multi-view, studio-quality images from a single photo. In each sample, the left-most image is the input, followed by novel generated views of unseen subjects. First and second rows show generations from Full-body and Face-only photos captured in-the-wild using a mobile phone. Third row shows generation from a Head-only studio image. Last row illustrates Pippo\u2019s capability to faithfully blend observed and generated content, alongside the corresponding ground truth.", "description": "\uc774 \uadf8\ub9bc\uc740 Pippo \ubaa8\ub378\uc774 \ub2e8\uc77c \uc0ac\uc9c4\uc5d0\uc11c \uace0\ud574\uc0c1\ub3c4, \ub2e4\uc911 \ubdf0, \uc2a4\ud29c\ub514\uc624\uae09 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc0d8\ud50c\uc5d0\uc11c \uc67c\ucabd \uc774\ubbf8\uc9c0\ub294 \uc785\ub825 \uc774\ubbf8\uc9c0\uc774\uace0, \uadf8 \ub4a4\uc5d0 \ubcf4\uc774\ub294 \uc774\ubbf8\uc9c0\ub4e4\uc740 \ubcf8 \uc801 \uc5c6\ub294 \ub300\uc0c1\uc758 \uc0c8\ub86d\uac8c \uc0dd\uc131\ub41c \ubdf0\uc785\ub2c8\ub2e4. 1, 2\ud589\uc740 \ud734\ub300\ud3f0\uc73c\ub85c \ucd2c\uc601\ud55c \uc804\uc2e0 \ubc0f \uc5bc\uad74 \uc0ac\uc9c4\ub9cc\uc744 \uc774\uc6a9\ud558\uc5ec \uc0dd\uc131\ud55c \uc774\ubbf8\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\uace0, 3\ud589\uc740 \uc2a4\ud29c\ub514\uc624\uc5d0\uc11c \ucd2c\uc601\ud55c \uba38\ub9ac \ubd80\ubd84\ub9cc \uc788\ub294 \uc774\ubbf8\uc9c0\ub97c \uc774\uc6a9\ud558\uc5ec \uc0dd\uc131\ud55c \uc774\ubbf8\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9 \ud589\uc740 Pippo \ubaa8\ub378\uc774 \uad00\ucc30\ub41c \ucf58\ud150\uce20\uc640 \uc0dd\uc131\ub41c \ucf58\ud150\uce20\ub97c \uc815\ud655\ud558\uac8c \uc870\ud654\uc2dc\ud0a4\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\uace0, \ud574\ub2f9 \uadf8\ub77c\uc6b4\ub4dc \ud2b8\ub8e8\uc2a4\ub97c \ud568\uaed8 \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "Figure 1"}, {"figure_path": "https://arxiv.org/html/2502.07785/x2.png", "caption": "Figure 2: Pipeline overview. This is an illustration of how we train our model. (Left) we use data from a studio capture and train our multi-view diffusion model (right). We condition on a full reference photo and a cropped face, as well as the target view cameras and 2D projected spatial anchor indicating head position and orientation. Our diffusion model also takes in noisy target views and a timestep in order to predict the denoised views (top). In practice, we apply a segmentation mask around the person.", "description": "\uadf8\ub9bc 2\ub294 Pippo \ubaa8\ub378\uc758 \ud6c8\ub828 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac1c\ub150\ub3c4\uc785\ub2c8\ub2e4. \uc67c\ucabd\uc5d0\ub294 \uc2a4\ud29c\ub514\uc624\uc5d0\uc11c \ucd2c\uc601\ud55c \ub2e4\uc591\ud55c \uac01\ub3c4\uc758 \uc774\ubbf8\uc9c0 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec Pippo\ub77c\ub294 \ub2e4\uc911 \ubdf0 \ud655\uc0b0 \ubcc0\ud658 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\ub294 \uacfc\uc815\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud6c8\ub828 \uacfc\uc815\uc5d0\ub294 \uc804\uccb4 \ucc38\uc870 \uc0ac\uc9c4\uacfc \uc5bc\uad74 \ubd80\ubd84\ub9cc \uc798\ub77c\ub0b8 \uc774\ubbf8\uc9c0, \uadf8\ub9ac\uace0 \ubaa9\ud45c\ud558\ub294 \ubdf0\uc758 \uce74\uba54\ub77c \uc704\uce58 \ubc0f \uba38\ub9ac\uc758 3\ucc28\uc6d0 \uc704\uce58\uc640 \ubc29\ud5a5\uc744 \ub098\ud0c0\ub0b4\ub294 2\ucc28\uc6d0 \ud22c\uc601 \uacf5\uac04 \uc575\ucee4(Spatial Anchor)\ub97c \uc870\uac74\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.  \ubaa8\ub378\uc740 \ub178\uc774\uc988\uac00 \ud3ec\ud568\ub41c \ubaa9\ud45c \ubdf0 \uc774\ubbf8\uc9c0\uc640 \uc2dc\uac04 \ub2e8\uacc4 \uc815\ubcf4\ub97c \uc785\ub825\ubc1b\uc544 \ub178\uc774\uc988\ub97c \uc81c\uac70\ud55c \uc774\ubbf8\uc9c0\ub97c \uc608\uce21\ud569\ub2c8\ub2e4. \uc2e4\uc81c\ub85c\ub294 \uc0ac\ub78c \uc8fc\uc704\uc5d0 \ubd84\ud560 \ub9c8\uc2a4\ud06c\ub97c \uc801\uc6a9\ud569\ub2c8\ub2e4.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.07785/x3.png", "caption": "Figure 3: DiT and ControlMLP Block. Our DiT block (left) loosely follows\u00a0[62],\nwith a AdaIn-based timestep modulation.\nWe apply attention and MLP blocks in parallel\u00a0[13],\nand jointly apply self-attention to the noisy generated and identity conditioning tokens.\nControlMLP block (right) is used to provide lightweight spatially-aligned conditioning -\nPl\u00fccker and Spatial Anchor.", "description": "\uadf8\ub9bc 3\uc740 Pippo \ubaa8\ub378\uc758 DiT(Diffusion Transformer) \ube14\ub85d\uacfc ControlMLP \ube14\ub85d\uc758 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc758 DiT \ube14\ub85d\uc740 \ub17c\ubb38 [62]\ub97c \uae30\ubc18\uc73c\ub85c \ud558\uba70, AdaIn(Adaptive Instance Normalization) \uae30\ubc18\uc758 timestep modulation\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc5b4\ud150\uc158\uacfc MLP \ube14\ub85d\uc744 \ubcd1\ub82c\uc801\uc73c\ub85c \uc801\uc6a9\ud558\uace0, \uc0dd\uc131\ub41c \ub178\uc774\uc988 \uc774\ubbf8\uc9c0\uc640 identity conditioning \ud1a0\ud070\uc5d0 \ub300\ud574 \uacf5\ub3d9\uc73c\ub85c self-attention\uc744 \uc801\uc6a9\ud569\ub2c8\ub2e4. \uc624\ub978\ucabd\uc758 ControlMLP \ube14\ub85d\uc740 Pl\u00fccker \uc88c\ud45c\uc640 \uacf5\uac04 \uc575\ucee4(Spatial Anchor)\ub97c \uc774\uc6a9\ud558\uc5ec \uacbd\ub7c9\ud654\ub41c \uacf5\uac04 \uc815\ub82c \uc870\uac74\ud654\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378\uc740 \uc785\ub825 \uc774\ubbf8\uc9c0\uc758 2D \uc815\ubcf4\uc640 3D \uacf5\uac04 \uc815\ubcf4\ub97c \uacb0\ud569\ud558\uc5ec \ub2e4\uc591\ud55c \uc2dc\uc810\uc758 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.1 \uae30\ubcf8 \ubaa8\ub378"}, {"figure_path": "https://arxiv.org/html/2502.07785/x4.png", "caption": "Figure 4: Entropy vs Growth Factor (\u03b3\ud835\udefe\\gammaitalic_\u03b3) for varying number of views (tokens) (Sec.\u00a03.4). We present the entropy results (Y-axis) from our Attention Biasing technique inspired from\u00a0[35] for varying number of tokens (individual line plots), and across different scaling growth factor \u03b3\ud835\udefe\\gammaitalic_\u03b3 introduced in Eq.\u00a0(6) (X-axis). On X-axis, \"No scaling\" refers to the default attention formulation\u00a0[82] and \u03b3=1.0\ud835\udefe1.0\\gamma=1.0italic_\u03b3 = 1.0 refers previous work\u00a0[35] formulation. Empirically, we find that a slightly higher value of \u03b3=1.4\ud835\udefe1.4\\gamma=1.4italic_\u03b3 = 1.4 leads to best visuals.", "description": "\uadf8\ub9bc 4\ub294 \ub2e4\uc591\ud55c \uc218\uc758 \ubdf0(\ud1a0\ud070)\uc5d0 \ub300\ud55c \uc5d4\ud2b8\ub85c\ud53c \ub300 \uc131\uc7a5 \uacc4\uc218(\u03b3)\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4(3.4\uc808 \ucc38\uc870). \uc774 \uadf8\ub9bc\uc740 \uc81c\uc548\ub41c \uc5b4\ud150\uc158 \ubc14\uc774\uc5b4\uc2f1 \uae30\ubc95([35]\uc5d0\uc11c \uc601\uac10\uc744 \uc5bb\uc74c)\uc758 \uc5d4\ud2b8\ub85c\ud53c \uacb0\uacfc(Y\ucd95)\ub97c \ub2e4\uc591\ud55c \ud1a0\ud070 \uc218(\uac1c\ubcc4 \uc120 \uadf8\ub798\ud504) \ubc0f \uc2dd(6)\uc5d0\uc11c \uc81c\uc2dc\ub41c \ub2e4\uc591\ud55c \uc2a4\ucf00\uc77c\ub9c1 \uc131\uc7a5 \uacc4\uc218 \u03b3(X\ucd95)\uc5d0 \ub300\ud574 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. X\ucd95\uc5d0\uc11c '\uc2a4\ucf00\uc77c\ub9c1 \uc5c6\uc74c'\uc740 \uae30\ubcf8 \uc5b4\ud150\uc158 \uacf5\uc2dd([82])\uc744 \ub098\ud0c0\ub0b4\uace0, \u03b3=1.0\ub294 \uc774\uc804 \uc5f0\uad6c([35])\uc758 \uacf5\uc2dd\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc2e4\ud5d8\uc801\uc73c\ub85c \u03b3=1.4\uac00 \uac00\uc7a5 \uc88b\uc740 \uc2dc\uac01\uc801 \uacb0\uacfc\ub97c \uc81c\uacf5\ud568\uc744 \ud655\uc778\ud588\uc2b5\ub2c8\ub2e4.", "section": "3.4 \ucd94\ub860 \uc2dc \ub2e4\uc591\ud55c \ubdf0 \uc218 \ucc98\ub9ac"}, {"figure_path": "https://arxiv.org/html/2502.07785/x5.png", "caption": "Figure 5: Generations under varying strengths of growth factor \u03b3\ud835\udefe\\gammaitalic_\u03b3 (Sec.\u00a03.4). On each row we show the generated views across vanilla attention\u00a0[82] (No scaling), prior work\u00a0[35] and our formulation Eq.\u00a0(6). It can be seen that growth factor (\u03b3\ud835\udefe\\gammaitalic_\u03b3) greater than 1.0 is crucial to mitigate the entropy buildup. We show only 10 views per row subsampled evenly from 60 views generated at 512\u00d7512512512512\\times 512512 \u00d7 512\u00a0resolution. The model was trained to jointly denoised only 12 views (Ni=5\u2217Ntsubscript\ud835\udc41\ud835\udc565subscript\ud835\udc41\ud835\udc61N_{i}=5*N_{t}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 5 \u2217 italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT).", "description": "\uadf8\ub9bc 5\ub294 \ub2e4\uc591\ud55c \uc131\uc7a5 \uacc4\uc218(\u03b3)\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ud589\uc740 \uae30\ubcf8\uc801\uc778 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998 [82](\uc2a4\ucf00\uc77c\ub9c1 \uc5c6\uc74c), \uc774\uc804 \uc5f0\uad6c [35], \uadf8\ub9ac\uace0 \ubcf8 \ub17c\ubb38\uc758 \uc2dd (6)\uc5d0\uc11c \uc81c\uc2dc\ub41c \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ub41c \ubdf0\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc131\uc7a5 \uacc4\uc218(\u03b3)\uac00 1.0\ubcf4\ub2e4 \ud074 \ub54c \uc5d4\ud2b8\ub85c\ud53c \uc99d\uac00\ub97c \uc644\ud654\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud55c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c\ub294 512x512 \ud574\uc0c1\ub3c4\ub85c \uc0dd\uc131\ub41c \ucd1d 60\uac1c\uc758 \ubdf0 \uc911\uc5d0\uc11c \uade0\ub4f1\ud558\uac8c \uc0d8\ud50c\ub9c1\ub41c 10\uac1c\uc758 \ubdf0\ub9cc\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\uc740 \ud6c8\ub828 \uc911\uc5d0 \ub2e8 12\uac1c\uc758 \ubdf0(Ni=5*Nt)\ub9cc\uc744 \ub3d9\uc2dc\uc5d0 \ub514\ub178\uc774\uc9d5\ud558\ub3c4\ub85d \ud6c8\ub828\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "3.4 \ucd94\ub860 \uc2dc \ub2e4\uc591\ud55c \ubdf0 \uac1c\uc218 \ucc98\ub9ac"}, {"figure_path": "https://arxiv.org/html/2502.07785/x6.png", "caption": "Figure 6: Visual comparison with state-of-the-art methods. We visually compare Pippo with state-of-the-art baselines DiffPortrait3D\u00a0[24] (head-only) and SiTH\u00a0[29] (full-body) generation.", "description": "\uadf8\ub9bc 6\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 Pippo \ubaa8\ub378\uacfc \uae30\uc874 \ucd5c\ucca8\ub2e8 \ubaa8\ub378\uc778 DiffPortrait3D (\uc5bc\uad74\ub9cc)\uacfc SiTH (\uc804\uc2e0)\uc758 \uc2dc\uac01\uc801 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e8\uc77c \uc774\ubbf8\uc9c0\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \uac01\ub3c4\uc5d0\uc11c\uc758 \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \ub2a5\ub825\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. Pippo \ubaa8\ub378\uc774 \uae30\uc874 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \ub354\uc6b1 \uc0ac\uc2e4\uc801\uc774\uace0 \ub2e4\uc591\ud55c \uac01\ub3c4\uc5d0\uc11c\uc758 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \ub2a5\ub825\uc774 \ub6f0\uc5b4\ub0a8\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \uc0ac\ub78c\uc758 \uc790\uc138\ub098 \ud45c\uc815\uc744 \uc815\ud655\ud558\uac8c \uc720\uc9c0\ud558\uba74\uc11c, \uc0c8\ub86d\uace0 \ub2e4\uc591\ud55c \uc2dc\uc810\uc744 \uc0dd\uc131\ud558\ub294 Pippo \ubaa8\ub378\uc758 \uc6b0\uc218\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "4.3 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.07785/x7.png", "caption": "Figure 7: High Resolution Multi-view Generation of Unseen subjects. Pippo enables generation of high-resolution 1K images given only a single image as input (left most in each block, separated with a dashed line). First row LHS shows generation from mobile captured photo, while the RHS shows unseen studio subject. Second row shows mobile captured face-only generations. Third and fourth row shows unseen studio subjects. Last two rows demonstrate simultaneous generation of 10 novel views given unseen studio subject.", "description": "\uadf8\ub9bc 7\uc740 Pippo \ubaa8\ub378\uc774 \ub2e8\uc77c \uc774\ubbf8\uc9c0 \uc785\ub825\ub9cc\uc73c\ub85c \uace0\ud574\uc0c1\ub3c4(1K) \ub2e4\uc911 \ubdf0 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ud734\ub300\ud3f0\uc73c\ub85c \ucd2c\uc601\ud55c \uc0ac\uc9c4\uacfc \uc2a4\ud29c\ub514\uc624\uc5d0\uc11c \ucd2c\uc601\ud55c \uc774\ubbf8\uc9c0\ub97c \ubaa8\ub450 \ud3ec\ud568\ud558\ub294 \ub2e4\uc591\ud55c \uc785\ub825\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uccab \ubc88\uc9f8 \ud589\uc740 \ud734\ub300\ud3f0\uc73c\ub85c \ucd2c\uc601\ud55c \uc804\uc2e0 \uc0ac\uc9c4\uacfc \uc2a4\ud29c\ub514\uc624\uc5d0\uc11c \ucd2c\uc601\ud55c \uc804\uc2e0 \uc0ac\uc9c4\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uace0, \ub450 \ubc88\uc9f8 \ud589\uc740 \ud734\ub300\ud3f0\uc73c\ub85c \ucd2c\uc601\ud55c \uc5bc\uad74 \uc0ac\uc9c4\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc138 \ubc88\uc9f8\uc640 \ub124 \ubc88\uc9f8 \ud589\uc740 \uc2a4\ud29c\ub514\uc624\uc5d0\uc11c \ucd2c\uc601\ud55c \uc804\uc2e0 \uc0ac\uc9c4\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uace0, \ub9c8\uc9c0\ub9c9 \ub450 \ud589\uc740 \uc2a4\ud29c\ub514\uc624\uc5d0\uc11c \ucd2c\uc601\ud55c \ub2e8\uc77c \uc774\ubbf8\uc9c0\uc5d0\uc11c 10\uac1c\uc758 \uc0c8\ub85c\uc6b4 \ubdf0\ub97c \ub3d9\uc2dc\uc5d0 \uc0dd\uc131\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ube14\ub85d\uc758 \uc67c\ucabd\uc5d0\ub294 \uc785\ub825 \uc774\ubbf8\uc9c0\uac00 \ud45c\uc2dc\ub418\uba70, \uc810\uc120\uc73c\ub85c \uad6c\ubd84\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.3 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.07785/x8.png", "caption": "Figure 8: Pippo can handle occluded inputs. We show Pippo\u2019s generations given incomplete input images \u2013 such as partially or fully occluded faces, unobserved t-shirt designs on test split of Full-body dataset. We show corresponding ground truth separated with blue dotted line \u2013 it can be seen that Pippo faithfully follows the known content while auto-completing unseen segments.", "description": "\uadf8\ub9bc 8\uc740 Pippo\uac00 \ubd80\ubd84\uc801\uc73c\ub85c \ub610\ub294 \uc644\uc804\ud788 \uac00\ub824\uc9c4 \uc5bc\uad74, \uc804\uccb4 \uc2e0\uccb4 \ub370\uc774\ud130\uc14b\uc758 \ud14c\uc2a4\ud2b8 \ubd84\ud560\uc5d0\uc11c \uad00\ucc30\ub418\uc9c0 \uc54a\uc740 \ud2f0\uc154\uce20 \ub514\uc790\uc778\uacfc \uac19\uc774 \ubd88\uc644\uc804\ud55c \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Pippo\uac00 \uc54c\ub824\uc9c4 \ub0b4\uc6a9\uc744 \ucda9\uc2e4\ud558\uac8c \ub530\ub974\uba74\uc11c \ubcf4\uc774\uc9c0 \uc54a\ub294 \ubd80\ubd84\uc744 \uc790\ub3d9\uc73c\ub85c \uc644\uc131\ud558\ub294\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud574 \ud30c\ub780\uc0c9 \uc810\uc120\uc73c\ub85c \uad6c\ubd84\ub41c \ud574\ub2f9 \uadf8\ub77c\uc6b4\ub4dc \ud2b8\ub8e8\uc2a4\ub3c4 \ud568\uaed8 \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2502.07785/x9.png", "caption": "Figure 9: Generations under varying strengths of growth factor \u03b3\ud835\udefe\\gammaitalic_\u03b3 (Sec.\u00a03.4). On each row we show the generated views across vanilla attention\u00a0[82] (No scaling), prior work\u00a0[35] and our formulation Eq.\u00a0(6). Growth factor (\u03b3\ud835\udefe\\gammaitalic_\u03b3) greater than 1.0 helps mitigate the entropy buildup, however increasing \u03b3\ud835\udefe\\gammaitalic_\u03b3 beyond 1.61.61.61.6 leads to oversaturation artifacts (somewhat akin to high CFG scale). We show only 6 views per row subsampled evenly from 60 views generated at 512\u00d7512512512512\\times 512512 \u00d7 512\u00a0resolution. The model was trained to jointly denoised only 12 views (Ni=5\u2217Ntsubscript\ud835\udc41\ud835\udc565subscript\ud835\udc41\ud835\udc61N_{i}=5*N_{t}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 5 \u2217 italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT).", "description": "\uadf8\ub9bc 9\ub294 \ub2e4\uc591\ud55c \uc131\uc7a5 \uacc4\uc218 \u03b3(\uc139\uc158 3.4)\uc5d0\uc11c \uc0dd\uc131\ub41c \ubdf0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ud589\uc5d0\ub294 \uc77c\ubc18\uc801\uc778 \uc5b4\ud150\uc158 [82](\uc2a4\ucf00\uc77c\ub9c1 \uc5c6\uc74c), \uc774\uc804 \uc5f0\uad6c [35] \ubc0f \uc800\ud76c\uac00 \uc81c\uc548\ud55c \uc2dd (6)\uc758 \uacb0\uacfc\uac00 \ub098\ud0c0\ub0a9\ub2c8\ub2e4. \uc131\uc7a5 \uacc4\uc218(\u03b3)\uac00 1.0\ubcf4\ub2e4 \ud06c\uba74 \uc5d4\ud2b8\ub85c\ud53c \ucd95\uc801\uc744 \uc644\ud654\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub418\uc9c0\ub9cc, \u03b3\uac00 1.6\uc744 \ub118\uc5b4\uc11c\uba74 \uacfc\ud3ec\ud654 \uc544\ud2f0\ud329\ud2b8\uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4(\ub192\uc740 CFG \uc2a4\ucf00\uc77c\uc5d0 \uac00\uae4c\uc6c0). 512x512 \ud574\uc0c1\ub3c4\ub85c \uc0dd\uc131\ub41c 60\uac1c\uc758 \ubdf0 \uc911\uc5d0\uc11c \uade0\ub4f1\ud558\uac8c \ud558\uc704 \uc0d8\ud50c\ub9c1\ub41c 6\uac1c\uc758 \ubdf0\ub9cc \ud45c\uc2dc\ud569\ub2c8\ub2e4. \ubaa8\ub378\uc740 12\uac1c\uc758 \ubdf0(Ni=5*Nt)\ub97c \uacf5\ub3d9\uc73c\ub85c \uc81c\uac70\ud558\ub3c4\ub85d \ud6c8\ub828\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "3.4 \ucd94\ub860\uc5d0\uc11c \ubdf0 \uc218 \ub2e4\uc591\ud558\uac8c \ucc98\ub9ac\ud558\uae30"}, {"figure_path": "https://arxiv.org/html/2502.07785/x10.png", "caption": "Figure 10: Statistics of our curated Humans-3B\u00a0dataset. We bucket these attributes in bins along X-axis and plot their respective sizes normalized between [0,1]01[0,1][ 0 , 1 ] on Y-axis. Filtering with these statistics enable us to retain images with detected-person confidence and image quality.", "description": "\uadf8\ub9bc 10\uc740 \uc5f0\uad6c\uc5d0\uc11c \uc0ac\uc6a9\ub41c Humans-3B \ub370\uc774\ud130\uc14b\uc758 \ud1b5\uacc4 \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac00\ub85c\ucd95(X\ucd95)\uc5d0\ub294 \uc0ac\ub78c \ud0a4, \uc774\ubbf8\uc9c0 \ub108\ube44, \uc0ac\ub78c \uac80\ucd9c \uc2e0\ub8b0\ub3c4, \uac80\ucd9c\ub41c \uc0ac\ub78c\uc758 \ub108\ube44, \uc0c1\uccb4 \uac00\uc2dc\uc131 \uc810\uc218, \uc804\uccb4 \uc2e0\uccb4 \uac00\uc2dc\uc131 \uc810\uc218, \uc774\ubbf8\uc9c0 \ud488\uc9c8 \ud3c9\uc810 \ub4f1\uc758 \uc18d\uc131\ub4e4\uc774 \uad6c\uac04\ud654\ub418\uc5b4 \ud45c\uc2dc\ub418\uace0, \uc138\ub85c\ucd95(Y\ucd95)\uc5d0\ub294 \uac01 \uad6c\uac04\uc5d0 \uc18d\ud558\ub294 \ub370\uc774\ud130\uc758 \ube44\uc728(0\uc5d0\uc11c 1 \uc0ac\uc774\uc758 \uac12)\uc774 \ud45c\uc2dc\ub429\ub2c8\ub2e4.  \uc774\ub7ec\ud55c \ud1b5\uacc4 \uc815\ubcf4\ub97c \uc774\uc6a9\ud558\uc5ec \ub370\uc774\ud130 \ud544\ud130\ub9c1\uc744 \uc218\ud589\ud558\uc5ec \uc0ac\ub78c \uac80\ucd9c \uc2e0\ub8b0\ub3c4\uc640 \uc774\ubbf8\uc9c0 \ud488\uc9c8\uc774 \ub192\uc740 \uc774\ubbf8\uc9c0\ub97c \uc720\uc9c0\ud569\ub2c8\ub2e4.", "section": "4.1 Data"}, {"figure_path": "https://arxiv.org/html/2502.07785/x11.png", "caption": "Figure 11: Qualitiative and ablation visuals from pretrained model. Consistent with quantitative\nevaluation, visual quality of generated images improves with using human-centric filtering, and\nimage-conditioned models generate samples which are visually closer to the domain (casual iPhone captures).\nThere is also an obvious boost in quality due to higher resolution.", "description": "\uadf8\ub9bc 11\uc740 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \uc815\uc131\uc801 \ubc0f \uc5d0\uc774\ube44\uc5d0\uc774\uc158 \uc2dc\uac01 \uc790\ub8cc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc815\ub7c9\uc801 \ud3c9\uac00\uc640 \uc77c\uce58\ud558\uac8c, \uc0ac\ub78c \uc911\uc2ec \ud544\ud130\ub9c1\uc744 \uc0ac\uc6a9\ud558\uba74 \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\uc758 \ud654\uc9c8\uc774 \ud5a5\uc0c1\ub418\uace0, \uc774\ubbf8\uc9c0 \uc870\uac74\ubd80 \ubaa8\ub378\uc740 \uc2dc\uac01\uc801\uc73c\ub85c \uc2e4\uc81c \ub3c4\uba54\uc778(\uc77c\uc0c1\uc801\uc778 \uc544\uc774\ud3f0 \ucea1\ucc98)\uc5d0 \ub354 \uac00\uae4c\uc6b4 \uc0d8\ud50c\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \ub610\ud55c \ud574\uc0c1\ub3c4\uac00 \ub192\uc544\uc9d0\uc5d0 \ub530\ub77c \ud654\uc9c8\uc774 \ud06c\uac8c \ud5a5\uc0c1\ub429\ub2c8\ub2e4.", "section": "4.3 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.07785/x12.png", "caption": "Figure 12: Ablation with Missing or Inconsistent Spatial Anchors. We run inference on the Head-only P1111@512 model with missing spatial anchor, or when it is inconsistent with input head pose rotated downwards towards the floor by 90\u2218.", "description": "\uadf8\ub9bc 12\ub294 \uacf5\uac04 \uc575\ucee4\uc758 \ub204\ub77d \ub610\ub294 \ubd88\uc77c\uce58\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uba38\ub9ac \ubd80\ubd84\ub9cc \uc788\ub294 P1@512 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucd94\ub860\uc744 \uc218\ud589\ud558\uace0, \uacf5\uac04 \uc575\ucee4\ub97c \ub204\ub77d\uc2dc\ud0a4\uac70\ub098 \uc785\ub825 \uba38\ub9ac \uc790\uc138\uc640 \uc77c\uce58\ud558\uc9c0 \uc54a\uac8c(\ubc14\ub2e5\uc744 \ud5a5\ud574 90\ub3c4 \ud68c\uc804) \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4. \uacf5\uac04 \uc575\ucee4\uac00 \ub204\ub77d\ub418\uba74, \uc0dd\uc131\ub41c \ubdf0\uc5d0 \ud53c\uc0ac\uccb4\uc758 \uba38\ub9ac\uac00 \ubcf4\uc774\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud558\uae30 \ub54c\ubb38\uc5d0 Pippo\ub294 \uc885\uc885 \ube48 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098 Pippo\ub294 \uc575\ucee4 \ud68c\uc804\uc5d0 \ub300\ud574 \uac15\uac74\ud558\uba70, \uacf5\uac04 \uc575\ucee4\ub294 \ubc30\uce58 \uc81c\uc5b4\uc5d0\ub9cc \uc0ac\uc6a9\ub418\uace0 \uc785\ub825 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uba38\ub9ac \uc790\uc138\ub97c \ucd94\ub860\ud55c\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3.3 Understanding and Improving Spatial Control"}]