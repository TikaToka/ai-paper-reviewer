{"references": [{"fullname_first_author": "Hu", "paper_title": "LORA: Low-Rank Adaptation of Large Language Models", "publication_date": "2022-XX-XX", "reason": "This paper introduces LoRA, a foundational low-rank adaptation technique for efficient fine-tuning of LLMs, which is extensively built upon in the current paper."}, {"fullname_first_author": "Mu\u00f1oz", "paper_title": "LoNAS: Elastic Low-Rank Adapters for Efficient Large Language Models", "publication_date": "2024-XX-XX", "reason": "This paper introduces LoNAS, a neural architecture search method combined with low-rank adapters, directly addressing the core focus of the current paper on combining these two approaches."}, {"fullname_first_author": "Mu\u00f1oz", "paper_title": "Shears: Unstructured Sparsity with Neural Low-rank Adapter Search", "publication_date": "2024-XX-XX", "reason": "This paper presents Shears, an improvement over LoNAS, which further refines the integration of low-rank adapters and neural architecture search for enhanced efficiency."}, {"fullname_first_author": "Mu\u00f1oz", "paper_title": "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "publication_date": "2024-XX-XX", "reason": "This paper introduces SQFT, which extends the work of LoNAS and Shears by focusing on adapting sparse models with low numerical precision, an important practical consideration for LLM compression."}, {"fullname_first_author": "Dettmers", "paper_title": "QLORA: Efficient Finetuning of Quantized LLMs", "publication_date": "2023-XX-XX", "reason": "This paper introduces QLoRA, which addresses the efficient fine-tuning of quantized LLMs, a technique that is relevant to SQFT's focus on low numerical precision in the current paper."}]}