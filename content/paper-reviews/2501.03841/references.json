{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-08", "reason": "This paper introduces GPT-4, a large language model that is used extensively in the OmniManip system for high-level reasoning and task understanding."}, {"fullname_first_author": "Anthony Brohan", "paper_title": "RT-1: Robotics Transformer for Real-World Control at Scale", "publication_date": "2022-12-06", "reason": "This paper introduces RT-1, a vision-language-action model that is used as a baseline method to compare the performance of OmniManip."}, {"fullname_first_author": "Anthony Brohan", "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control", "publication_date": "2023-07-15", "reason": "This paper is another important baseline that demonstrates the use of vision-language-action models in robotic control, providing context for OmniManip's approach."}, {"fullname_first_author": "Boyuan Chen", "paper_title": "SpatialVLMs: Endowing Vision-Language Models with Spatial Reasoning Capabilities", "publication_date": "2024-00-00", "reason": "This work explores enhancing vision-language models with spatial reasoning, a key aspect of OmniManip's object-centric spatial constraints."}, {"fullname_first_author": "Cheng Chi", "paper_title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "publication_date": "2023-00-00", "reason": "This paper is relevant due to its focus on visuomotor policy learning, offering a related approach to robotic control and a comparison point for OmniManip."}]}