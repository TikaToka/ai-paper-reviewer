{"references": [{"fullname_first_author": "Antoine Roux Albert Q. Jiang", "paper_title": "Mixtral of experts", "publication_date": "2024-01-01", "reason": "This paper introduces Mixtral, a large language model evaluated in the study for its mathematical reasoning capabilities."}, {"fullname_first_author": "Konstantin Chernyshev", "paper_title": "U-math: A university-level benchmark for evaluating mathematical skills in LLMs", "publication_date": "2024-12-01", "reason": "This paper introduces U-MATH, a benchmark dataset used in prior research to assess LLMs' mathematical abilities, providing context for the current study's methodology."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-01-01", "reason": "This work is a seminal paper on evaluating LLMs' mathematical abilities and established a baseline for comparing model performance."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the MATH dataset", "publication_date": "2021-01-01", "reason": "This study introduced the MATH dataset, a significant resource for evaluating LLMs' mathematical reasoning skills, providing a comparison point for the current study's dataset."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2020-09-01", "reason": "This paper introduced the MMLU benchmark, a widely used dataset for evaluating general language understanding, offering valuable context for the current study's focus on mathematical reasoning within LLMs."}]}