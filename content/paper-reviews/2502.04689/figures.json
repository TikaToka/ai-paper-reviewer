[{"figure_path": "https://arxiv.org/html/2502.04689/x1.png", "caption": "Figure 1: ARR motivation. To answer a question, we often need to analyze the question\u2019s intent, retrieve relevant information, and reason step by step.", "description": "\uadf8\ub9bc 1\uc740 ARR(Analyzing, Retrieving, Reasoning)\uc758 \uac1c\ub150\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc9c8\ubb38\uc5d0 \ub2f5\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc9c8\ubb38\uc758 \uc758\ub3c4\ub97c \ubd84\uc11d\ud558\uace0, \uad00\ub828 \uc815\ubcf4\ub97c \uac80\uc0c9\ud558\uace0, \ub2e8\uacc4\uc801\uc73c\ub85c \ucd94\ub860\ud558\ub294 \uc138 \uac00\uc9c0 \ub2e8\uacc4\uac00 \ud544\uc694\ud558\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac1c\ub150\ub3c4\uc785\ub2c8\ub2e4.  \ub2e8\uc21c\ud788 \ub2f5\uc744 \ucc3e\ub294 \uac83\uc774 \uc544\ub2c8\ub77c, \uc9c8\ubb38 \ubd84\uc11d, \uc815\ubcf4 \ud0d0\uc0c9, \ucd94\ub860 \uacfc\uc815\uc744 \uac70\uccd0 \ub17c\ub9ac\uc801\uc73c\ub85c \ub2f5\uc744 \ub3c4\ucd9c\ud558\ub294 ARR\uc758 \ud575\uc2ec \uac1c\ub150\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \uc124\uba85\ud569\ub2c8\ub2e4.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04689/x2.png", "caption": "Figure 2: Question answering with LLMs. We first obtain rationale risubscript\ud835\udc5f\ud835\udc56r_{i}italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT by reasoning generation and then select the optimal option via evaluating the language modeling losses of different context-option combinations.", "description": "\uc774 \uadf8\ub9bc\uc740 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc744 \uc0ac\uc6a9\ud55c \uc9c8\ubb38 \ub2f5\ubcc0 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uba3c\uc800, \ubaa8\ub378\uc740 \uc8fc\uc5b4\uc9c4 \uc9c0\ubb38(passage), \uc9c8\ubb38(question), \uadf8\ub9ac\uace0 \uc120\ud0dd\uc9c0(options)\ub97c \uc785\ub825\ubc1b\uc544 \ucd94\ub860 \uacfc\uc815(reasoning generation)\uc744 \uac70\uccd0 \ucd94\ub860 \uacb0\uacfc(rationale)\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uadf8\ub7f0 \ub2e4\uc74c, \ubaa8\ub378\uc740 \uc0dd\uc131\ub41c \ucd94\ub860 \uacb0\uacfc\uc640 \uac01 \uc120\ud0dd\uc9c0\ub97c \uc870\ud569\ud558\uc5ec \uc5b8\uc5b4 \ubaa8\ub378\ub9c1 \uc190\uc2e4(language modeling loss)\uc744 \uacc4\uc0b0\ud558\uace0, \uc190\uc2e4 \uac12\uc774 \uac00\uc7a5 \uc791\uc740 \uc120\ud0dd\uc9c0\ub97c \ucd5c\uc885 \ub2f5\ubcc0\uc73c\ub85c \uc120\ud0dd\ud569\ub2c8\ub2e4.  \uc989, \ubaa8\ub378\uc774 \ub17c\ub9ac\uc801 \ucd94\ub860 \uacfc\uc815\uc744 \uac70\uccd0 \ucd5c\uc801\uc758 \ub2f5\ubcc0\uc744 \ub3c4\ucd9c\ud558\ub294 \uacfc\uc815\uc744 \ub2e8\uacc4\ubcc4\ub85c \uc2dc\uac01\ud654\ud55c \uac83\uc785\ub2c8\ub2e4.", "section": "3 Question Answering with LLMs"}, {"figure_path": "https://arxiv.org/html/2502.04689/x3.png", "caption": "Figure 3: Model size experiments. The trend of QA performance changes as the model becomes larger.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \ubaa8\ub378 \ud06c\uae30\uac00 \ucee4\uc9d0\uc5d0 \ub530\ub77c \uc9c8\ubb38 \ub2f5\ubcc0(QA) \uc131\ub2a5\uc774 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LLaMA3-Chat \ubaa8\ub378\uc758 \ud06c\uae30(1B, 3B, 8B \ud30c\ub77c\ubbf8\ud130)\uac00 \ucee4\uc9c8\uc218\ub85d QA \uc815\ud655\ub3c4\uac00 \ud5a5\uc0c1\ub418\ub294 \uacbd\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ub354 \ud070 \ubaa8\ub378\uc774 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4.", "section": "6.1 \ubaa8\ub378 \ud06c\uae30"}]