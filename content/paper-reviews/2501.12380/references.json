{"references": [{"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2021-00-00", "reason": "This paper introduces a benchmark for evaluating large language models' proficiency in various tasks, providing a foundation for assessing reasoning capabilities across multiple domains, which is relevant to this paper's focus on multimodal reasoning."}, {"fullname_first_author": "Xiangru Tang", "paper_title": "MMMU: Measuring expert-level multi-discipline multimodal understanding", "publication_date": "2024-00-00", "reason": "This work introduces a benchmark for evaluating multimodal reasoning in image-based tasks, directly informing the design of MMVU, a video-based benchmark, and its methodology."}, {"fullname_first_author": "Yilun Zhao", "paper_title": "MMVU: Measuring expert-level multi-discipline video understanding", "publication_date": "2025-01-21", "reason": "This is the main reference paper in the context, describing the benchmark introduced and used in the analysis."}, {"fullname_first_author": "Xuehai He", "paper_title": "MMWorld: Towards multi-discipline multi-faceted world model evaluation in videos", "publication_date": "2024-00-00", "reason": "This paper focuses on a similar concept of multi-disciplinary video understanding; hence its methodology and evaluation are highly relevant to this paper's research."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Large language models", "publication_date": "2023-00-00", "reason": "This paper provides background on the capabilities of large language models in reasoning, directly relevant to this paper's evaluation of foundation models in video understanding."}]}