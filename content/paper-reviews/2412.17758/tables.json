[{"content": "| Model | Reported | Measured (s/o) | Assessment |\n|---|---|---|---| \n| Llama 65B [Touvron et al. (2023a)](https://arxiv.org/html/2412.17758v1#bib.bib26) | 56.0 | 55.6 / 70.2 |  separation |\n| Llama 2 70B [Touvron et al. (2023b)](https://arxiv.org/html/2412.17758v1#bib.bib27) | 57.4 | 57.4 / 79.6 |  separation |\n| Llama 3 70B [Grattafiori et al. (2024)](https://arxiv.org/html/2412.17758v1#bib.bib14) | 92.9 | 64.2 / 91.3 | options |\n| Mistral 7B [Jiang et al. (2023)](https://arxiv.org/html/2412.17758v1#bib.bib16) | 55.5 | 54.1 / 74.6 |  separation |\n| Mixtral 8x7B [Jiang et al. (2024)](https://arxiv.org/html/2412.17758v1#bib.bib17) | 59.7 | 59.9 / 83.3 |  separation |\n| Mixtral 8x22B [Mistral AI (2024)](https://arxiv.org/html/2412.17758v1#bib.bib20) | 91.3\u2020 | 70.7 / 91.8 | options |\n| DeepSeek 67B [DeepSeek AI et al. (2024a)](https://arxiv.org/html/2412.17758v1#bib.bib7) | 59.0 | 60.1 / 84.6 | options |\n| DeepSeek V2 [DeepSeek AI et al. (2024b)](https://arxiv.org/html/2412.17758v1#bib.bib8) | 92.4\u2020 | 70.3 / 92.2 | options |\n| Qwen 14B [Bai et al. (2023)](https://arxiv.org/html/2412.17758v1#bib.bib2) | 84.4 | 47.3 / 86.6 | options |\n| Yi 6B [01. AI et al. (2024)](https://arxiv.org/html/2412.17758v1#bib.bib1) | 50.3\u2020 | 55.7 / 80.5 |  separation |\n| Gemma 7B [Gemma Team et al. (2024b)](https://arxiv.org/html/2412.17758v1#bib.bib12) | 53.2 | 53.2 / 79.0 |  separation |\n| Gemma 2 27B [Gemma Team et al. (2024a)](https://arxiv.org/html/2412.17758v1#bib.bib11) | 71.4 | 65.8 / 90.0 |  separation |", "caption": "Table 1: Measured and reported ARC Challenge scores with our assessment of the setup used by authors. The 25-shot prompting used in contrast to the 0-shot is denoted by \u2020\u2020\\dagger\u2020 (in the case authors use such a setup in their report).", "description": "\ud45c 1\uc740 \uc5ec\ub7ec \uc800\uc790\ub4e4\uc774 \uc0ac\uc6a9\ud55c \uc124\uc815\uc5d0 \ub300\ud55c \uc800\uc790\ub4e4\uc758 \ud3c9\uac00\uc640 \ud568\uaed8 \uce21\uc815\ub41c \ubc0f \ubcf4\uace0\ub41c ARC Challenge \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 0\uc0f7\uacfc \ub300\uc870\uc801\uc73c\ub85c \uc0ac\uc6a9\ub41c 25\uc0f7 \ud504\ub86c\ud504\ud2b8\ub294 \uc800\uc790\ub4e4\uc774 \uadf8\ub7ec\ud55c \uc124\uc815\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0 \u2020\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc774 ARC Challenge \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc5bc\ub9c8\ub098 \uc798 \uc218\ud589\ub418\uc5c8\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc815\ub7c9\uc801 \ub370\uc774\ud130\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \ubcf4\uace0\ub41c \uc810\uc218\uc640 \uce21\uc815\ub41c \uc810\uc218\uac00 \ud45c\uc2dc\ub418\uba70, \uc800\uc790\ub4e4\uc774 \uc5b4\ub5a4 \ud3c9\uac00 \uc124\uc815(\ubd84\ub9ac \ub610\ub294 \uc635\uc158)\uc744 \uc0ac\uc6a9\ud588\ub294\uc9c0\uc5d0 \ub300\ud55c \ud3c9\uac00\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ub2e4\uc591\ud55c LLM\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uace0 \ud3c9\uac00 \uc124\uc815\uc758 \uc601\ud5a5\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "2 Impact on evaluation results"}, {"content": "| Model | Reported | Measured <br> s / o | Assessment |\n|---|---|---|---| \n| Llama 65B [Touvron et al. (2023a)](https://arxiv.org/html/2412.17758v1#bib.bib26) | 52.3 | 50.3 / 60.1 | separation |\n| Llama 2 70B [Touvron et al. (2023b)](https://arxiv.org/html/2412.17758v1#bib.bib27) | 50.7 | 50.8 / 66.9 | separation |\n| Llama 3 70B [Grattafiori et al. (2024)](https://arxiv.org/html/2412.17758v1#bib.bib14) | 52.2 | 51.2 / 72.9 | separation |\n| Mistral 7B [Jiang et al. (2023)](https://arxiv.org/html/2412.17758v1#bib.bib16) | \u2014<sup>\u22c4</sup> | 50.9 / 62.4 | \u2014 |\n| Mixtral 8x7B [Jiang et al. (2024)](https://arxiv.org/html/2412.17758v1#bib.bib17) | \u2014<sup>\u22c4</sup> | 49.4 / 65.1 | \u2014 |\n| Mixtral 8x22B [Mistral AI (2024)](https://arxiv.org/html/2412.17758v1#bib.bib20) | \u2014 | 51.1 / 67.3 | \u2014 |\n| DeepSeek 67B [DeepSeek AI et al. (2024a)](https://arxiv.org/html/2412.17758v1#bib.bib7) | \u2014 | 51.6 / 61.6 | \u2014 |\n| DeepSeek V2 [DeepSeek AI et al. (2024b)](https://arxiv.org/html/2412.17758v1#bib.bib8) | \u2014 | 52.2 / 70.0 | \u2014 |\n| Qwen 14B [Bai et al. (2023)](https://arxiv.org/html/2412.17758v1#bib.bib2) | 77.9 | 56.2 / 78.6 | options |\n| Yi 6B [01. AI et al. (2024)](https://arxiv.org/html/2412.17758v1#bib.bib1) | \u2014 | 52.5 / 71.0 | \u2014 |\n| Gemma 7B [Gemma Team et al. (2024b)](https://arxiv.org/html/2412.17758v1#bib.bib12) | 51.8 | 51.8 / 60.0 | separation |\n| Gemma 2 27B [Gemma Team et al. (2024a)](https://arxiv.org/html/2412.17758v1#bib.bib11) | 53.7 | 58.3 / 70.0 | separation |", "caption": "Table 2: Measured and reported SIQA scores with our assessment of the setup used by authors. Some authors do not directly report scores but average them with other commonsense reasoning problems (denoted by \u22c4), making our assessment unlikely to succeed.", "description": "\ud45c 2\ub294 \uc5ec\ub7ec \uc800\uc790\ub4e4\uc774 \uc0ac\uc6a9\ud55c SIQA \ud3c9\uac00 \uc124\uc815\uc5d0 \ub300\ud55c \uc800\uc790\ub4e4\uc758 \ud3c9\uac00\uc640 \ud568\uaed8 \uce21\uc815\ub41c \ubc0f \ubcf4\uace0\ub41c SIQA \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc77c\ubd80 \uc800\uc790\ub4e4\uc740 \uc810\uc218\ub97c \uc9c1\uc811 \ubcf4\uace0\ud558\uc9c0 \uc54a\uace0 \ub2e4\ub978 \uc0c1\uc2dd \ucd94\ub860 \ubb38\uc81c\uc640 \ud3c9\uade0\uc744 \ub0b4\ubbc0\ub85c(\u22c4\ub85c \ud45c\uc2dc) \ud3c9\uac00\uac00 \uc131\uacf5\ud560 \uac00\ub2a5\uc131\uc774 \ub0ae\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\uc774 \uc81c\uc2dc\ud55c \ub2f5\ubcc0\uc744 \uac1c\ubcc4\uc801\uc73c\ub85c \ud3c9\uac00\ud588\ub294\uc9c0, \uc544\ub2c8\uba74 \ubaa8\ub4e0 \uc120\ud0dd\uc9c0\ub97c \ud568\uaed8 \uace0\ub824\ud588\ub294\uc9c0 \uc5ec\ubd80\uc5d0 \ub530\ub77c SIQA \uc810\uc218\uac00 \uc5b4\ub5bb\uac8c \ub2ec\ub77c\uc9c0\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. \ub2e4\ub978 \ubca4\uce58\ub9c8\ud06c\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uac00?"}, {"content": "| Model | Reported | Measured  s / o / sb / ob | Assessment |\n|---|---|---|---| \n| Llama 65B [Touvron et al. (2023a)](https://arxiv.org/html/2412.17758/bib.bib26) | 60.2 | 47.0 / 59.0 / 60.2 / 56.2 | Ayseparationb |\n| Llama 2 70B [Touvron et al. (2023b)](https://arxiv.org/html/2412.17758/bib.bib27) | 60.2 | 48.8 / 73.0 / 60.0 / 65.8 | Ayseparationb |\n| Llama 3 70B [Grattafiori et al. (2024)](https://arxiv.org/html/2412.17758/bib.bib14) | 47.6 | 48.6 / 88.4 / 59.4 / 88.5 | Ayseparation |\n| Mistral 7B [Jiang et al. (2023)](https://arxiv.org/html/2412.17758/bib.bib16) | \u2014\u22c4 | 44.2 / 71.6 / 55.0 / 57.8 | \u2014 |\n| Mixtral 8x7B [Jiang et al. (2024)](https://arxiv.org/html/2412.17758/bib.bib17) | \u2014\u22c4 | 47.0 / 80.2 / 55.2 / 78.0 | \u2014 |\n| Mixtral 8x22B [Mistral AI (2024)](https://arxiv.org/html/2412.17758/bib.bib20) | \u2014 | 49.6 / 81.6 / 61.2 / 78.4 | \u2014 |\n| DeepSeek 67B [DeepSeek AI et al. (2024a)](https://arxiv.org/html/2412.17758/bib.bib7) | 60.2 | 47.6 / 76.6 / 62.0 / 76.2 | Ayseparationb |\n| DeepSeek V2 [DeepSeek AI et al. (2024b)](https://arxiv.org/html/2412.17758/bib.bib8) | \u2014 | 38.6 / 82.8 / 62.4 / 84.2 | \u2014 |\n| Qwen 14B [Bai et al. (2023)](https://arxiv.org/html/2412.17758/bib.bib2) | \u2014 | 43.8 / 87.0 / 54.6 / 79.8 | \u2014 |\n| Yi 6B [01. AI et al. (2024)](https://arxiv.org/html/2412.17758/bib.bib1) | \u2014\u22c4 | 40.4 / 68.2 / 53.6 / 67.6 | \u2014 |\n| Gemma 7B [Gemma Team et al. (2024b)](https://arxiv.org/html/2412.17758/bib.bib12) | \u2014 | 44.8 / 65.2 / 58.2 / 65.8 | \u2014 |\n| Gemma 2 27B [Gemma Team et al. (2024a)](https://arxiv.org/html/2412.17758/bib.bib11) | \u2014 | 47.6 / 83.0 / 59.8 / 81.4 | \u2014 |", "caption": "Table 3: Measured and reported OpenBookQA scores with our assessment of the setup used by authors. Some authors do not directly report scores but average them with other commonsense reasoning problems (denoted by \u22c4).", "description": "\ud45c 3\uc740 \uc5ec\ub7ec \uc800\uc790\ub4e4\uc774 \uc0ac\uc6a9\ud55c \uc124\uc815\uc5d0 \ub300\ud55c \uc800\uc790\ub4e4\uc758 \ud3c9\uac00\uc640 \ud568\uaed8 \uce21\uc815\ub41c \ubc0f \ubcf4\uace0\ub41c OpenBookQA \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc77c\ubd80 \uc800\uc790\ub294 \uc810\uc218\ub97c \uc9c1\uc811 \ubcf4\uace0\ud558\uc9c0 \uc54a\uace0 \ub2e4\ub978 \uc0c1\uc2dd \ucd94\ub860 \ubb38\uc81c\uc640 \ud3c9\uade0\uc744 \ub0c5\ub2c8\ub2e4 (\u22c4\ub85c \ud45c\uc2dc). \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\uc758 OpenBookQA \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uba70, \uac01 \ubaa8\ub378\uc774 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub2f5\ubcc0\uc744 \uc120\ud0dd\ud560 \ub54c \ubaa8\ub4e0 \uc635\uc158\uc744 \ud568\uaed8 \uace0\ub824\ud588\ub294\uc9c0(options), \uc544\ub2c8\uba74 \uac01 \uc635\uc158\uc744 \uac1c\ubcc4\uc801\uc73c\ub85c \uace0\ub824\ud588\ub294\uc9c0(separation) \uc5ec\ubd80\uc5d0 \ub530\ub77c \uc131\ub2a5 \ucc28\uc774\ub97c \ubd84\uc11d\ud569\ub2c8\ub2e4.  'separation' \uc124\uc815\uc740 \uac01 \uc635\uc158\uc744 \uac1c\ubcc4\uc801\uc73c\ub85c \ud3c9\uac00\ud558\uc5ec \ubaa8\ub378\uc758 \ub2a5\ub825\uc744 \uacfc\uc18c\ud3c9\uac00\ud560 \uc218 \uc788\uc73c\uba70, 'options' \uc124\uc815\uc740 \ub9e5\ub77d\uc744 \uace0\ub824\ud558\uc5ec \ubcf4\ub2e4 \uc815\ud655\ud55c \ud3c9\uac00\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3. \ub2e4\ub978 \ubca4\uce58\ub9c8\ud06c\ub294 \uc601\ud5a5\uc744 \ubc1b\ub294\uac00?"}, {"content": "| Model |\n|---|---| \n| `huggyllama/llama-65b` |\n| `meta-llama/Llama-2-70b-hf` |\n| `meta-llama/Meta-Llama-3-70B` |\n| `mistralai/Mistral-7B-v0.1` |\n| `mistralai/Mixtral-8x7B-v0.1` |\n| `mistralai/Mixtral-8x22B-v0.1` |\n| `deepseek-ai/deepseek-llm-67b-base` |\n| `deepseek-ai/DeepSeek-V2` |\n| `Qwen/Qwen-14B` |\n| `01-ai/Yi-6B` |\n| `google/gemma-7b` |\n| `google/gemma-2-27b` |", "caption": "Table 4: Exact variants of models used for evaluation.", "description": "\ud45c 4\ub294 \ubcf8 \ub17c\ubb38\uc758 \uc2e4\ud5d8\uc5d0 \uc0ac\uc6a9\ub41c \ubaa8\ub378\ub4e4\uc758 \uc815\ud655\ud55c \ubc84\uc804 \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378 \uc774\ub984\uacfc \ud568\uaed8, \uc2e4\ud5d8\uc5d0 \uc0ac\uc6a9\ub41c \ud2b9\uc815 \ubc84\uc804\uc744 \uba85\uc2dc\ud558\uc5ec \uc7ac\ud604\uc131\uc744 \ub192\uc774\uace0, \ub3c5\uc790\ub4e4\uc774 \ub3d9\uc77c\ud55c \uc124\uc815\uc73c\ub85c \uc2e4\ud5d8\uc744 \ubc18\ubcf5\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. \uc774\ub294 \uc2e4\ud5d8 \uacb0\uacfc\uc758 \uc2e0\ub8b0\ub3c4\uc640 \ube44\uad50 \uac00\ub2a5\uc131\uc744 \ub192\uc774\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "C Evaluation details"}]