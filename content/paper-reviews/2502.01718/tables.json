[{"content": "| Subset | Evol | OSS | Stack Python | Overall |\n|---|---|---|---|---|\n| Before Filtering |  |  |  |  |\n| # Examples | 36,256 | 37,750 | 50,000 | 124,006 |\n| # Avg Test Cases | 19.33 | 17.21 | 18.27 | 18.27 |\n| After Filtering |  |  |  |  |\n| # Examples | 27,853 | 26,346 | 35,223 | 89,422 |\n| # Avg Test Cases | 14.77 | 16.11 | 15.79 | 15.56 |\n| # Pairs | 89,089 | 91,636 | 126,784 | 307,509 |", "caption": "Table 1: Dataset statistics of AceCode-89K before and after test-case filtering.", "description": "AceCode-89K \ub370\uc774\ud130\uc14b\uc758 \ud1b5\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4. \ud544\ud130\ub9c1 \uc804\ud6c4\uc758 \ub370\uc774\ud130\uc14b \ud06c\uae30, \ud3c9\uade0 \ud14c\uc2a4\ud2b8 \ucf00\uc774\uc2a4 \uc218, \uc0dd\uc131\ub41c \ud504\ub85c\uadf8\ub7a8 \uc30d\uc758 \uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud544\ud130\ub9c1 \uacfc\uc815\uc744 \ud1b5\ud574 \ud488\uc9c8\uc774 \ub0ae\uc740 \ud14c\uc2a4\ud2b8 \ucf00\uc774\uc2a4\ub97c \uc81c\uac70\ud558\uc5ec \ub370\uc774\ud130\uc14b\uc758 \uc2e0\ub8b0\ub3c4\ub97c \ub192\uc600\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3 ACECODE-89K"}, {"content": "| Method | # N | HumanEval - | HumanEval Plus | MBPP - | MBPP Plus | BigCodeBench-C Full | BigCodeBench-C Hard | BigCodeBench-I Full | BigCodeBench-I Hard | LiveCodeBench V4 | Average |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| GPT-4o (0806) | 1 | 92.7 | 87.2 | 87.6 | 72.2 | 58.9 | 36.5 | 48.0 | 25.0 | 43.6 | 61.3 |\n| DeepSeek-V2.5 | 1 | 90.2 | 83.5 | 87.6 | 74.1 | 53.2 | 29.1 | 48.9 | 27.0 | 41.8 | 59.5 |\n| DeepSeek-V3 | 1 | 91.5 | 86.6 | 87.6 | 73.0 | 62.2 | 39.9 | 50.0 | 27.7 | 63.5 | 64.6 |\n| Qwen2.5-Coder-32B | 1 | 92.1 | 87.2 | 90.5 | 77.0 | 58.0 | 33.8 | 49.0 | 27.7 | 48.3 | 62.6 |\n| Inference Model = Mistral-7B-Instruct-V0.3 |  |  |  |  |  |  |  |  |  |  |  |\n| Greedy | 1 | 36.6 | 31.1 | 49.5 | 41.3 | 25.9 | 6.1 | 20.1 | 5.4 | 7.3 | 24.8 |\n| Average | 64 | 37.1 | 30.8 | 45.1 | 38.0 | 21.7 | 4.2 | 17.6 | 3.0 | 4.0 | 22.4 |\n| Oracle | 64 | 87.2 | 78.0 | 83.9 | 73.5 | 68.4 | 37.8 | 58.5 | 31.1 | 24.3 | 60.3 |\n| AceCodeRM-7B | 16 | 65.9 | 56.7 | 59.3 | 52.4 | 35.1 | 10.1 | 29.3 | 8.8 | 11.9 | 36.6 |\n|  | 32 | 68.3 | 58.5 | 59.8 | 51.6 | 37.4 | 8.8 | 30.7 | 10.8 | 14.6 | 37.8 |\n|  | 64 | 71.3 | 61.6 | 59.8 | 51.6 | 39.4 | 6.8 | 31.8 | 9.5 | 15.4 | 38.6 |\n| \u0394 (RM-greedy) | - | +34.8 | +30.5 | +10.3 | +11.1 | +13.5 | +4.1 | +11.7 | +5.4 | +8.1 | +13.8 |\n| AceCodeRM-32B | 16 | 68.3 | 61.0 | 58.7 | 49.5 | 37.7 | 11.5 | 30.9 | 10.1 | 12.9 | 37.8 |\n|  | 32 | 72.6 | 65.9 | 51.6 | 40.5 | 9.5 | 33.9 | 13.5 | 16.1 | 40.6 |\n|  | 64 | 75.0 | 64.6 | 50.0 | 42.7 | 15.5 | 35.6 | 13.5 | 17.4 | 41.7 |\n| \u0394 (RM-greedy) | - | +38.4 | +34.8 | +12.2 | +11.1 | +16.8 | +9.5 | +15.5 | +8.1 | +10.1 | +17.4 |\n| Inference Model = Llama-3.1-8B-Instruct |  |  |  |  |  |  |  |  |  |  |  |\n| Greedy | 1 | 68.9 | 62.2 | 67.2 | 54.8 | 38.5 | 12.8 | 31.8 | 13.5 | 18.0 | 40.9 |\n| Average | 64 | 61.7 | 54.9 | 64.5 | 54.5 | 32.8 | 10.1 | 26.6 | 9.0 | 13.8 | 36.4 |\n| Oracle | 64 | 93.9 | 90.2 | 92.1 | 82.3 | 80.0 | 54.7 | 67.9 | 48.6 | 40.8 | 72.3 |\n| AceCodeRM-7B | 16 | 77.4 | 70.7 | 76.5 | 64.3 | 45.8 | 20.3 | 36.4 | 12.2 | 26.1 | 47.7 |\n|  | 32 | 79.9 | 72.6 | 62.4 | 47.6 | 23.0 | 37.3 | 13.5 | 27.3 | 48.9 |\n|  | 64 | 81.7 | 74.4 | 61.9 | 47.8 | 23.6 | 38.1 | 13.5 | 27.6 | 49.3 |\n| \u0394 (RM-greedy) | - | +12.8 | +12.2 | +9.3 | +9.5 | +9.3 | +10.8 | +6.2 | 0.0 | +9.6 | +8.4 |\n| AceCodeRM-32B | 16 | 82.3 | 74.4 | 72.8 | 60.6 | 49.8 | 20.3 | 38.4 | 13.5 | 27.5 | 48.8 |\n|  | 32 | 81.7 | 76.2 | 60.6 | 50.4 | 22.3 | 39.1 | 13.5 | 30.3 | 49.6 |\n|  | 64 | 85.4 | 79.3 | 59.0 | 48.5 | 19.6 | 40.0 | 13.5 | 31.0 | 49.8 |\n| \u0394 (RM-greedy) | - | +16.5 | +17.1 | +9.3 | +9.5 | +11.8 | +10.8 | +8.2 | 0.0 | +13.0 | +10.7 |\n| Inference Model = Qwen2.5-Coder-7B-Instruct |  |  |  |  |  |  |  |  |  |  |  |\n| Greedy | 1 | 91.5 | 86.0 | 82.8 | 71.4 | 49.5 | 19.6 | 41.8 | 20.3 | 34.2 | 55.2 |\n| Average | 64 | 86.0 | 80.1 | 77.9 | 65.6 | 45.3 | 18.6 | 37.3 | 16.2 | 31.8 | 51.0 |\n| Oracle | 64 | 98.2 | 95.7 | 97.4 | 90.7 | 80.9 | 62.8 | 73.5 | 53.4 | 57.4 | 78.9 |\n| AceCodeRM-7B | 16 | 90.2 | 82.9 | 88.6 | 74.9 | 53.8 | 20.9 | 45.0 | 21.6 | 40.1 | 57.6 |\n|  | 32 | 90.9 | 86.0 | 74.1 | 53.4 | 25.0 | 43.9 | 19.6 | 39.8 | 57.8 |\n|  | 64 | 90.9 | 85.4 | 73.8 | 52.9 | 24.3 | 43.5 | 21.6 | 40.1 | 57.8 |\n| \u0394 (RM-greedy) | - | -0.6 | 0.0 | +5.8 | +3.4 | +4.3 | +5.4 | +3.2 | +1.4 | +5.9 | +2.6 |", "caption": "Table 2: AceCode-RM\u2019s best-of-n results. We evaluated the model on HumanEval, MBPP, BigCodeBench, and LiveCodeBench. Specifically, -C means completion split and -I means instruct split.", "description": "\ud45c 2\ub294 AceCode-RM \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 HumanEval, MBPP, BigCodeBench, LiveCodeBench \ub370\uc774\ud130\uc14b\uc5d0\uc11c best-of-n \ubc29\uc2dd\uc73c\ub85c \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  -C\ub294 completion split, -I\ub294 instruction split\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b\ubcc4 Plus, Full, Hard \ub4f1 \uc5ec\ub7ec \ud558\uc704 \ubca4\uce58\ub9c8\ud06c\uc758 \uacb0\uacfc\uc640 \ud3c9\uade0 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uc5b4, \ubaa8\ub378\uc758 \ucf54\ub4dc \uc0dd\uc131 \ub2a5\ub825\uc744 \ub2e4\uac01\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "4.1 Reward Model Training Setup"}, {"content": "| Model | HumanEval - | HumanEval Plus | MBPP - | MBPP Plus | BigCodeBench (C) Full | BigCodeBench (C) Hard | BigCodeBench (I) Full | BigCodeBench (I) Hard | LiveCodeBench V4 | Average |\n|---|---|---|---|---|---|---|---|---|---|---|\n| DeepSeek-V2.5 | 90.2 | 83.5 | 87.6 | 74.1 | 53.2 | 29.1 | 48.9 | 27.0 | 41.8 | 59.5 |\n| Baseline = Qwen2.5-7B-Instruct |  |  |  |  |  |  |  |  |  |  |\n| Baseline | 81.7 | 73.2 | 79.4 | 67.7 | 45.6 | 16.9 | 38.4 | 14.2 | 29.0 | 49.6 |\n| AceCoder<sub>RM</sub> | 83.5 | 77.4 | 83.1 | 71.2 | 46.8 | 16.9 | 39.0 | 14.9 | 30.3 | 51.5 |\n| AceCoder<sub>Rule</sub> | 84.1 | 77.4 | 80.2 | 68.3 | 46.8 | 15.5 | 40.2 | 15.5 | 30.1 | 50.9 |\n| \u0394 (RL-baseline) | +2.4 | +4.3 | +3.7 | +3.4 | +1.2 | 0.0 | +1.8 | +1.4 | +1.3 | +2.1 |\n| Baseline = Qwen2.5-Coder-7B-Base |  |  |  |  |  |  |  |  |  |  |\n| Baseline | 61.6 | 53.0 | 76.9 | 62.9 | 45.8 | 16.2 | 40.2 | 14.2 | 28.7 | 44.4 |\n| AceCoder<sub>RM</sub> | 83.5 | 75.6 | 80.2 | 67.2 | 41.9 | 14.9 | 36.8 | 16.2 | 25.7 | 49.1 |\n| AceCoder<sub>Rule</sub> | 84.1 | 78.0 | 82.3 | 69.3 | 48.6 | 18.2 | 43.2 | 18.2 | 28.5 | 52.3 |\n| \u0394 (RL-baseline) | +22.5 | +25.0 | +5.4 | +6.4 | +2.8 | +2.0 | +3.1 | +4.1 | -0.2 | +7.9 |\n| Baseline = Qwen2.5-Coder-7B-Instruct |  |  |  |  |  |  |  |  |  |  |\n| Baseline | 91.5 | 86.0 | 82.8 | 71.4 | 49.5 | 19.6 | 41.8 | 20.3 | 34.2 | 55.2 |\n| AceCoder<sub>RM</sub> | 89.0 | 84.1 | 86.0 | 72.8 | 50.4 | 18.9 | 42.0 | 19.6 | 35.0 | 55.3 |\n| AceCoder<sub>Rule</sub> | 90.9 | 84.8 | 84.1 | 71.7 | 50.9 | 23.0 | 43.3 | 19.6 | 34.9 | 55.9 |\n| \u0394 (RL-baseline) | -0.6 | -1.2 | +3.2 | +1.3 | +1.4 | +3.4 | +1.5 | -0.7 | +0.8 | +0.7 |", "caption": "Table 3: AceCoder\u2019s Performance after RL tuning using Reinforcement++ algorithm. We start with 3 different initial policy models and 2 kind of reward types, where R\u2062M\ud835\udc45\ud835\udc40RMitalic_R italic_M means using our trained AceCode-RM and R\u2062u\u2062l\u2062e\ud835\udc45\ud835\udc62\ud835\udc59\ud835\udc52Ruleitalic_R italic_u italic_l italic_e means using the binary pass rate. Results show consistent improvement across various benchmarks.", "description": "\ubcf8 \ud45c\ub294 AceCoder\uc758 \uac15\ud654\ud559\uc2b5 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Reinforcement++ \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc138 \uac00\uc9c0 \ucd08\uae30 \uc815\ucc45 \ubaa8\ub378(Qwen2.5-7B-Instruct, Qwen2.5-Coder-7B-Base, Qwen2.5-Coder-7B-Instruct)\uacfc \ub450 \uac00\uc9c0 \ubcf4\uc0c1 \uc720\ud615(AceCode-RM, \uc774\uc9c4 \ud1b5\uacfc\uc728)\uc73c\ub85c \ud559\uc2b5\uc744 \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c(HumanEval, MBPP, BigCodeBench, LiveCodeBench)\uc5d0\uc11c \uc77c\uad00\ub41c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Main Results"}, {"content": "| Method & RM | HumanEval - | HumanEval Plus | MBPP - | MBPP Plus | BigCodeBench-C Full | BigCodeBench-C Hard | BigCodeBench-I Full | BigCodeBench-I Hard | LiveCodeBench V4 | Average |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Greedy | 68.9 | 62.2 | 67.2 | 54.8 | 38.5 | 12.8 | 31.8 | 13.5 | 18.0 | 40.9 |\n| Average | 50.1 | 42.2 | 57.9 | 47.2 | 22.0 | 10.6 | 18.2 | 12.0 | 14.9 | 30.6 |\n| InternLM2-RM-8B | 57.9 | 55.5 | 66.7 | 54.0 | 38.7 | 8.8 | 29.8 | 8.8 | 15.1 | 37.3 |\n| Skywork-Gemma-27B | 73.8 | 67.1 | 64.3 | 53.4 | 40.1 | 14.9 | 32.5 | 12.8 | 23.6 | 42.5 |\n| Skywork-Llama-3.1-8B | 67.7 | 61.6 | 69.6 | 56.9 | 40.6 | 10.8 | 31.8 | 12.2 | 18.8 | 41.1 |\n| \u0394 (max(other RM)-greedy) | +4.9 | +4.9 | +2.4 | +2.1 | +2.1 | +2.0 | +0.6 | -0.7 | +5.6 | +2.7 |\n| AceCode-RM-7B | 77.4 | 70.7 | 76.5 | 64.3 | 45.8 | 20.3 | 36.4 | 12.2 | 26.1 | 47.7 |\n| \u0394 (RM-greedy) | +8.5 | +8.5 | +9.3 | +9.5 | +7.3 | +7.4 | +4.6 | -1.4 | +8.1 | +6.9 |", "caption": "Table 4: AceCode-RM\u2019s performance against other open-sourced reward models in terms of Best-of-16 sampling for Llama-3.1-8B-Inst. We can see the top-ranked RM on Reward Bench get little improvements compared to ours.", "description": "\ubcf8 \ud45c\ub294 Llama-3.1-8B-Inst \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec Best-of-16 \uc0d8\ud50c\ub9c1 \ubc29\uc2dd\uc73c\ub85c \ud3c9\uac00\ud55c AceCode-RM\uc758 \uc131\ub2a5\uc744 \ub2e4\ub978 \uc624\ud508\uc18c\uc2a4 \ub9ac\uc6cc\ub4dc \ubaa8\ub378\ub4e4\uacfc \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Reward Bench\uc5d0\uc11c \ucd5c\uace0 \uc21c\uc704\ub97c \uae30\ub85d\ud55c \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud588\uc744 \ub54c, AceCode-RM\uc774 \uc0c1\ub2f9\ud788 \ub354 \ub098\uc740 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 HumanEval, MBPP, BigCodeBench-C, BigCodeBench-I, LiveCodeBench\uc758 \ub2e4\uc591\ud55c \uc9c0\ud45c\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \ubaa8\ub378\uc758 Greedy \uc131\ub2a5\uacfc \ube44\uad50\ud558\uc5ec AceCode-RM\uc758 \uac1c\uc120 \uc815\ub3c4\ub97c \uba85\ud655\ud558\uac8c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "4.4 \uc8fc\uc694 \uacb0\uacfc"}, {"content": "| Method | HumanEval - | HumanEval Plus | MBPP - | MBPP Plus | BigCodeBench-C Full | BigCodeBench-C Hard | BigCodeBench-I Full | BigCodeBench-I Hard | LiveCodeBench V4 | Average |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Inference Model = Llama-3.1-8B-Instruct |  |  |  |  |  |  |  |  |  |  |\n| RM w/o Test Case Filter | 73.8 | 65.9 | 73.3 | 61.4 | 44.6 | 17.6 | 35.5 | 9.5 | 25.1 | 45.2 |\n| RM w/ Test Filter | 77.4 | 70.7 | 76.5 | 64.3 | 45.8 | 20.3 | 36.4 | 12.2 | 26.1 | 47.7 |\n| \u0394 (w/ Filter - w/o Filter) | +3.7 | +4.9 | +3.2 | +2.9 | +1.2 | +2.7 | +0.9 | +2.7 | +1.0 | +2.6 |\n| Inference Model = Qwen2.5-Coder-7B-Instruct |  |  |  |  |  |  |  |  |  |  |\n| RM w/o Test Case Filter | 91.5 | 86.0 | 86.0 | 72.2 | 52.5 | 21.6 | 43.4 | 19.6 | 36.9 | 56.6 |\n| RM w/ Test Filter | 90.2 | 82.9 | 88.6 | 74.9 | 53.8 | 20.9 | 45.0 | 21.6 | 40.1 | 57.6 |\n| \u0394 (w/ Filter - w/o Filter) | -1.2 | -3.0 | +2.6 | +2.6 | +1.3 | -0.7 | +1.6 | +2.0 | +3.2 | +0.9 |", "caption": "Table 5: Ablation study on test-case filtering. Results are Best-of-16 sampling performance.", "description": "\uc774 \ud45c\ub294 \ud14c\uc2a4\ud2b8 \ucf00\uc774\uc2a4 \ud544\ud130\ub9c1\uc774 Best-of-16 \uc0d8\ud50c\ub9c1 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud14c\uc2a4\ud2b8 \ucf00\uc774\uc2a4 \ud544\ud130\ub9c1\uc744 \uc801\uc6a9\ud588\uc744 \ub54c\uc640 \uc801\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\uc758 HumanEval, MBPP, BigCodeBench, LiveCodeBench \uc131\ub2a5 \ube44\uad50\ub97c \ud1b5\ud574 \ud544\ud130\ub9c1\uc758 \ud6a8\uacfc\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uac01 \uc9c0\ud45c\ubcc4\ub85c Plus, Full, Hard \ub4f1 \ub2e4\uc591\ud55c \ub09c\uc774\ub3c4\uc758 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud558\uc5ec \ud544\ud130\ub9c1 \ud6a8\uacfc\uc758 \ubc94\uc704\ub97c \ud3ed\ub113\uac8c \ub2e4\ub8f9\ub2c8\ub2e4.  Llama-3.1-8B-Instruct \uc640 Qwen2.5-Coder-7B-Instruct \ub450 \ubaa8\ub378\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \uac01\uac01 \uc81c\uc2dc\ud558\uc5ec \ubaa8\ub378 \uc885\ub958\uc5d0 \ub530\ub978 \uc601\ud5a5\ub3c4 \ud655\uc778\ud569\ub2c8\ub2e4.", "section": "4.5 Ablation Studies"}, {"content": "| Method | HumanEval - | HumanEval Plus | MBPP - | MBPP Plus | BigCodeBench-C Full | BigCodeBench-C Hard | BigCodeBench-I Full | BigCodeBench-I Hard | LiveCodeBench V4 | Average |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Inference Model = Llama-3.1-8B-Instruct |\n| AceCode-RM (LLama) | 65.9 | 59.1 | 69.6 | 57.9 | 42.7 | 12.8 | 32.9 | 13.5 | 19.9 | 41.6 |\n| AceCode-RM (Qwen) | 77.4 | 70.7 | 76.5 | 64.3 | 45.8 | 20.3 | 36.4 | 12.2 | 26.1 | 47.7 |\n| \u0394 (Qwen-Llama) | +11.6 | +11.6 | +6.9 | +6.3 | +3.1 | +7.4 | +3.5 | -1.4 | +6.2 | +6.1 |\n| Inference Model = Qwen2.5-Coder-7B-Instruct |\n| AceCode-RM (LLama) | 87.8 | 81.7 | 82.0 | 67.7 | 50.5 | 25.0 | 39.0 | 19.6 | 32.4 | 54.0 |\n| AceCode-RM (Qwen) | 90.2 | 82.9 | 88.6 | 74.9 | 53.8 | 20.9 | 45.0 | 21.6 | 40.1 | 57.6 |\n| \u0394 (Qwen-Llama) | +2.4 | +1.2 | +6.6 | +7.1 | +3.2 | -4.1 | +6.0 | +2.0 | +7.7 | +3.6 |", "caption": "Table 6: Comparison of AceCode-RM\u2019s performance trained on different base model, where AceCode-RM (Llama) is based on Llama-3.1-Inst-8B and AceCode-RM (Qwen) is based on Qwen-Coder-2.5-7B-Inst. Results are Best-of-16 sampling performance.", "description": "\ud45c 6\uc740 AceCode-RM\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  AceCode-RM (Llama)\ub294 Llama-3.1-Inst-8B\ub97c \uae30\ubc18\uc73c\ub85c, AceCode-RM (Qwen)\uc740 Qwen-Coder-2.5-7B-Inst\ub97c \uae30\ubc18\uc73c\ub85c \ud559\uc2b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud45c\ub294 HumanEval, MBPP, BigCodeBench, LiveCodeBench\uc758 \ub124 \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c Best-of-16 \uc0d8\ud50c\ub9c1 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubca4\uce58\ub9c8\ud06c\ub294 \uc5ec\ub7ec \ud558\uc704 \ubca4\uce58\ub9c8\ud06c\ub85c \ub098\ub258\uba70,  \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \uce21\uba74\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. Llama\uc640 Qwen \uae30\ubc18 \ubaa8\ub378\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc8fc\uc5b4, \uae30\ubc18 \ubaa8\ub378 \uc120\ud0dd\uc774 AceCode-RM\uc758 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "4.5 Ablation Studies"}, {"content": "{\n  \"question\": \"Given a string, return the longest palindromic substring within that string.\",\n  \"tests\": [\n    \"assert longestPalindrome(\"babad\") == \"bab\" or \"aba\"\",\n    \"assert longestPalindrome(\"cbbd\") == \"bb\"\",\n    \"assert longestPalindrome(\"a\") == \"a\"\",\n    \"assert longestPalindrome(\"ac\") == \"a\"\",\n    \"assert longestPalindrome(\"aa\") == \"aa\"\",\n    \"assert longestPalindrome(\"bbbab\") == \"bbbab\"\",\n    \"assert longestPalindrome(\"bananas\") == \"anana\"\",\n    \"assert longestPalindrome(\"racecar\") == \"racecar\"\",\n    \"assert longestPalindrome(\"Racecar\") == \"Racecar\"\",\n    \"assert longestPalindrome(\"A man, a plan, a canal: Panama\") == \"amanaplanacanalpanama\"\",\n    \"assert longestPalindrome(\"00000100000\") == \"00000100000\"\",\n    \"assert longestPalindrome(\"12321\") == \"12321\"\",\n    \"assert longestPalindrome(\"909\") == \"909\"\",\n    \"assert longestPalindrome(\"\") == \"\"\",\n    \"assert longestPalindrome(\"aaba\") == \"aba\" or \"aa\"\",\n    \"assert longestPalindrome(\"abccba\") == \"abccba\"\",\n    \"assert longestPalindrome(\"abcba\") == \"abcba\"\",\n    \"assert longestPalindrome(\"xabax\") == \"xabax\"\",\n    \"assert longestPalindrome(\"xabba\") == \"abba\"\",\n    \"assert longestPalindrome(\"xabbaxy\") == \"abba\"\"\n  ]\n}", "caption": "Table 7: Prompt Used for Converting Seed Code Dataset into LeetCode-style Questions and Test Cases", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 3\uc7a5 Methodology, 3.1\uc808 Problem Formulation\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ud504\ub86c\ud504\ud2b8\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud504\ub86c\ud504\ud2b8\ub294 \uae30\uc874 \ucf54\ub4dc \ub370\uc774\ud130\uc14b\uc744 LeetCode \uc2a4\ud0c0\uc77c\uc758 \uc9c8\ubb38\uacfc \ud14c\uc2a4\ud2b8 \ucf00\uc774\uc2a4\ub85c \ubcc0\ud658\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud504\ub86c\ud504\ud2b8\ub294 GPT-40-mini\uc640 \uac19\uc740 \ud070 \uc5b8\uc5b4 \ubaa8\ub378\uc774 \uc9c8\ubb38\uc744 \uc0dd\uc131\ud558\uace0 \ud574\ub2f9 \uc9c8\ubb38\uc5d0 \ub300\ud55c 20\uac1c \uc815\ub3c4\uc758 \ud14c\uc2a4\ud2b8 \ucf00\uc774\uc2a4\ub97c \uc0dd\uc131\ud558\ub3c4\ub85d \uc9c0\uc2dc\ud558\ub294 \uc138\ubd80\uc801\uc778 \uc9c0\uce68\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc9c8\ubb38\uacfc \ud14c\uc2a4\ud2b8 \ucf00\uc774\uc2a4\ub294 \ucd94\ud6c4 reward model\uc744 \ud559\uc2b5\ud558\uace0 \uac15\ud654 \ud559\uc2b5\uc744 \uc218\ud589\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "3 Methodology"}]