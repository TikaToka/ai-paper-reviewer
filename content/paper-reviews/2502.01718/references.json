{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to RLHF, a technique heavily influencing the approach described in the current paper."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-01", "reason": "This paper establishes scaling laws crucial to understanding and improving code generation models."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-01", "reason": "This paper introduces HumanEval, a benchmark heavily used in the current paper for evaluation."}, {"fullname_first_author": "Daya Guo", "paper_title": "DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning", "publication_date": "2025-01-01", "reason": "This paper presents a similar RL approach for mathematical reasoning that inspired the current paper's RL methodology."}, {"fullname_first_author": "Binyuan Hui", "paper_title": "Qwen2.5-Coder technical report", "publication_date": "2024-09-01", "reason": "This paper introduces Qwen2.5-Coder, a strong code generation model used as a baseline and for training in the current paper."}]}