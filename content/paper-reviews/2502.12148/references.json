{"references": [{"fullname_first_author": "Xie", "paper_title": "Show-o: One single transformer to unify multimodal understanding and generation", "publication_date": "2024-08-12", "reason": "This paper is directly compared against throughout the paper and is the backbone model for the proposed method."}, {"fullname_first_author": "Wu", "paper_title": "Liquid: Language models are scalable multimodal generators", "publication_date": "2024-12-04", "reason": "This paper is frequently cited in the introduction and related work sections, and its findings on the synergy between multimodal understanding and generation are discussed and contrasted with the proposed approach."}, {"fullname_first_author": "Wang", "paper_title": "Emu3: Next-token prediction is all you need", "publication_date": "2024-09-18", "reason": "This is a key MLLM model that the authors benchmark against throughout the paper, comparing the understanding and generation performance differences."}, {"fullname_first_author": "Wu", "paper_title": "Janus: Decoupling visual encoding for unified multimodal understanding and generation", "publication_date": "2024-10-13", "reason": "This paper is a major comparative model for the proposed method, frequently mentioned in the qualitative and quantitative results sections."}, {"fullname_first_author": "Dong", "paper_title": "DreamLLM: Synergistic multimodal comprehension and creation", "publication_date": "2023-09-11", "reason": "This paper represents one of the earlier attempts at unified multimodal models which the authors reference to show how their proposed method advances the field."}]}