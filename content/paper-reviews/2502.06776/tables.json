[{"content": "| Method | Acc. | Prec. | Recall |\n|---|---|---|---| \n| _Llama 3.1 70B_ | 85% | 0.77 | **1.00** |\n| _GPT-4o_ | 95% | 0.91 | **1.00** |\n| _Gemini 1.5 Pro_ | **97**% | **0.96** | 0.98 |\n| Human Baseline | 75% | 0.71 | 0.84 |", "caption": "Table 1: Hyperparameters used in our paper. We organize hyperparameters into five sections, including names of language model backbones, parameters of the data generation pipeline, sampling parameters for the OpenAI inference API, training parameters used by corresponding benchmarks, and filtering parameters used to prepare our data for training agents in Section\u00a06.", "description": "\ud45c 1\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc12f \uac00\uc9c0 \uc139\uc158\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc73c\uba70, \uac01\uac01\uc740 \uc5b8\uc5b4 \ubaa8\ub378 \ubc31\ubcf8\uc758 \uc774\ub984, \ub370\uc774\ud130 \uc0dd\uc131 \ud30c\uc774\ud504\ub77c\uc778\uc758 \ub9e4\uac1c\ubcc0\uc218, OpenAI \ucd94\ub860 API\uc758 \uc0d8\ud50c\ub9c1 \ub9e4\uac1c\ubcc0\uc218, \ud574\ub2f9\ud558\ub294 \ubca4\uce58\ub9c8\ud06c\uc5d0 \uc0ac\uc6a9\ub41c \ud559\uc2b5 \ub9e4\uac1c\ubcc0\uc218, \uadf8\ub9ac\uace0 6\uc7a5\uc5d0\uc11c \uc5d0\uc774\uc804\ud2b8 \ud6c8\ub828\uc744 \uc704\ud55c \ub370\uc774\ud130\ub97c \uc900\ube44\ud558\ub294 \ub370 \uc0ac\uc6a9\ub41c \ud544\ud130\ub9c1 \ub9e4\uac1c\ubcc0\uc218\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub2e4\uc591\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc758 \uac1c\uc694\ub97c \uc81c\uacf5\ud558\uc5ec, \uc2e4\ud5d8 \uc124\uc815\uc5d0 \ub300\ud55c \uba85\ud655\ud55c \uc774\ud574\ub97c \ub3d5\uc2b5\ub2c8\ub2e4.", "section": "4.1 Language Model Task Proposer"}, {"content": "| Method | Feasibility Rate |\n|---|---| \n| _Llama 3.1 70B_ | 75% |\n| _GPT-4o_ | 85% |\n| _Gemini 1.5 Pro_ | **89**% |\n| Human Baseline | 54% |", "caption": "Table 2: Cost analysis for different LLM models in the fully-scaled pipeline. This table provides statistics for the number of tokens that were processed by our pipeline, and why serving using a local LLM engine like vLLM is important for bringing down costs.", "description": "\ud45c 2\ub294 \uc644\uc804\ud55c \uaddc\ubaa8\uc758 \ud30c\uc774\ud504\ub77c\uc778\uc5d0\uc11c \uc11c\ub85c \ub2e4\ub978 LLM \ubaa8\ub378\uc5d0 \ub300\ud55c \ube44\uc6a9 \ubd84\uc11d\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ud30c\uc774\ud504\ub77c\uc778\uc5d0\uc11c \ucc98\ub9ac\ub41c \ud1a0\ud070 \uc218\uc5d0 \ub300\ud55c \ud1b5\uacc4\ub97c \uc81c\uacf5\ud558\uace0, vLLM\uacfc \uac19\uc740 \ub85c\uceec LLM \uc5d4\uc9c4\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ube44\uc6a9 \uc808\uac10\uc5d0 \uc911\uc694\ud55c \uc774\uc720\ub97c \uc124\uba85\ud569\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c\ub294, \uc5d0\uc774\uc804\ud2b8, \ud310\ub2e8\uc790 \ubc0f \uc791\uc5c5 \uc81c\uc548\uc790\uc5d0 \uc758\ud574 \ucc98\ub9ac\ub41c \ud1a0\ud070 \uc218\uc640 \ud568\uaed8 GPT-4, Gemini Pro \ubc0f Llama 3.1 70B\uc5d0 \ub300\ud55c \uc608\uc0c1 API \ube44\uc6a9 \ubc0f AWS \ucef4\ud4e8\ud305 \ube44\uc6a9\uc744 \ube44\uad50\ud558\uc5ec Llama 3.1 70B\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ube44\uc6a9\uc744 \uc5bc\ub9c8\ub098 \uc808\uac10\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.2 150,000\uac1c \uc5d0\uc774\uc804\ud2b8\ub85c \ud655\uc7a5"}]