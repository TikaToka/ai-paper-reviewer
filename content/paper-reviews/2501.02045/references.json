{"references": [{"fullname_first_author": "Gonzalo Benegas", "paper_title": "Genomic language models: Opportunities and challenges", "publication_date": "2024-07-01", "reason": "This paper provides a comprehensive overview of genomic language models, highlighting the opportunities and challenges in this rapidly evolving field."}, {"fullname_first_author": "Hugo Dalla-Torre", "paper_title": "The nucleotide transformer: Building and evaluating robust foundation models for human genomics", "publication_date": "2023-01-01", "reason": "This paper introduces a novel transformer-based model for genomic data, demonstrating state-of-the-art performance on various genomic tasks and pushing the boundaries of large-scale genomic modeling."}, {"fullname_first_author": "Yanrong Ji", "paper_title": "DNABERT: pre-trained bidirectional encoder representations from transformers model for DNA-language in genome", "publication_date": "2021-01-01", "reason": "This paper introduces a pre-trained bidirectional encoder model for genomic sequences, effectively leveraging transformer architectures to improve downstream performance on various genomic tasks."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-01-01", "reason": "This paper presents a novel technique called LoRA for efficient fine-tuning of large language models, which is particularly useful when dealing with limited computational resources."}, {"fullname_first_author": "Mitchell Wortsman", "paper_title": "Small-scale proxies for large-scale transformer training instabilities", "publication_date": "2023-01-01", "reason": "This paper studies the training instabilities in large transformer models, providing valuable insights into mitigating such issues, which are crucial for stable and reliable model training."}]}