[{"content": "| Language | GPT-4o | GPT-4o mini | Llama-3.1-8B | IndicBART | IndicBERT | RemBERT | MuRIL |\n|---|---|---|---|---|---|---|---| \n| Hindi | **44.80** | 32.33 | 18.61 | 11.21 | 10.78 | 11.41 | 10.87 |\n| Bengali | **44.38** | 31.11 | N/A | 12.52 | 10.39 | 12.00 | 9.90 |\n| Punjabi | **40.60** | 26.25 | N/A | 11.78 | 10.36 | 11.06 | 10.36 |\n| Marathi | **42.20** | 27.13 | N/A | 11.65 | 10.59 | 12.93 | 11.79 |\n| Urdu | **44.18** | 31.13 | N/A | 12.11 | 11.63 | 11.32 | 11.20 |\n| Gujarati | **41.77** | 28.29 | N/A | 12.14 | 11.06 | 12.13 | 10.79 |\n| Telugu | **41.34** | 26.78 | N/A | 12.05 | 11.36 | 10.20 | 9.96 |\n| Tamil | **38.46** | 35.08 | N/A | 11.70 | 10.96 | 10.98 | 11.00 |\n| Kannada | **38.97** | 25.75 | N/A | 11.51 | 11.71 | 10.87 | 10.62 |", "caption": "Table 1: Performance comparison of language models on the IndicMMLU-Pro benchmark across nine Indic languages, including Indo-Aryan (Hindi, Bengali, Punjabi, Marathi, Urdu, and Gujarati) and Dravidian (Telugu, Tamil, and Kannada) languages. Accuracy scores are shown as percentages. Models compared include GPT-4o, GPT-4o mini, IndicBART, IndicBERT, RemBERT, MuRIL, and Llama-3.1-8B-Instruct.", "description": "\ud45c 1\uc740 9\uac1c\uc758 \uc778\ub3c4 \uc5b8\uc5b4(\uc778\ub3c4-\uc544\ub9ac\uc544\uc5b4 \uacc4\uc5f4: \ud78c\ub514\uc5b4, \ubcb5\uac08\uc5b4, \ud380\uc790\ube0c\uc5b4, \ub9c8\ub77c\ud2f0\uc5b4, \uc6b0\ub974\ub450\uc5b4, \uad6c\uc790\ub77c\ud2b8\uc5b4; \ub4dc\ub77c\ube44\ub2e4\uc5b4 \uacc4\uc5f4: \ud154\ub8e8\uad6c\uc5b4, \ud0c0\ubc00\uc5b4, \uce78\ub098\ub2e4\uc5b4)\ub97c \ub300\uc0c1\uc73c\ub85c IndicMMLU-Pro \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  GPT-4o, GPT-4o mini, IndicBART, IndicBERT, RemBERT, MuRIL, Llama-3.1-8B-Instruct \ubaa8\ub378\ub4e4\uc758 \uc815\ud655\ub3c4 \uc810\uc218(\ubc31\ubd84\uc728)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc778\ub3c4 \uc5b8\uc5b4\ubcc4 \uc131\ub2a5 \ucc28\uc774\uc640 \uc778\ub3c4-\uc544\ub9ac\uc544\uc5b4\uc640 \ub4dc\ub77c\ube44\ub2e4\uc5b4 \uacc4\uc5f4 \uc5b8\uc5b4 \uac04\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3 \uacb0\uacfc"}, {"content": "| Language | XLM-RoBERTa | Navarasa | Airavata | OpenHathi | TamilLlama | MahaMarathi |\n|---|---|---|---|---|---|---|\n| Hindi | 12.33 | 12.43 | 11.60 | 11.65 | - | - |\n| Bengali | 12.68 | 12.08 | - | - | - | - |\n| Punjabi | 12.59 | 11.95 | - | - | - | - |\n| Marathi | 12.57 | 11.88 | - | - | - | 11.60 |\n| Urdu | 12.53 | 10.73 | - | - | - | - |\n| Gujarati | 11.92 | 11.53 | - | - | - | - |\n| Telugu | 12.62 | 11.77 | - | - | 11.53 | - |\n| Tamil | 12.34 | 12.38 | - | - | 11.66 | - |\n| Kannada | 13.16 | 11.88 | - | - | - | - |", "caption": "Table 2: Comparison of language model performance across Indian languages, both Indo-Aryan (i.e., Hindi, Bengali, Punjabi, Marathi, Urdu, and Gujarati) and Dravidan (i.e., Telegu, Tamil, and Kannada). Scores are shown for Llama 3.1, Navarasa, Airavata, OpenHathi, TamilLlama, and MahaMarathi models where available.", "description": "\ud45c 2\ub294 \uc778\ub3c4 \uc544\ub9ac\uc544\uc5b4 \uacc4\uc5f4(\ud78c\ub514\uc5b4, \ubcb5\uac08\uc5b4, \ud380\uc790\ube0c\uc5b4, \ub9c8\ub77c\ud2f0\uc5b4, \uc6b0\ub974\ub450\uc5b4, \uad6c\uc790\ub77c\ud2b8\uc5b4)\uacfc \ub4dc\ub77c\ube44\ub2e4\uc5b4 \uacc4\uc5f4(\ud154\ub8e8\uad6c\uc5b4, \ud0c0\ubc00\uc5b4, \uce78\ub098\ub2e4\uc5b4) \ubaa8\ub450\ub97c \ud3ec\ud568\ud55c \uc778\ub3c4 \uc5b8\uc5b4 \uc804\ubc18\uc5d0 \uac78\uce5c \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4. Llama 3.1, Navarasa, Airavata, OpenHathi, TamilLlama, MahaMarathi \ubaa8\ub378\uc758 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc5b8\uc5b4\ubcc4 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec \uc778\ub3c4 \uc5b8\uc5b4\uc758 \ub2e4\uc591\ud55c \uc5b4\ud718 \ubc0f \uad6c\ubb38\ub860\uc801 \ud2b9\uc9d5\uc5d0 \ub530\ub978 \ubaa8\ub378 \uc131\ub2a5\uc758 \ucc28\uc774\ub97c \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3 \uacb0\uacfc"}, {"content": "| Language | chrF++ | BLEU | METEOR | TER | SacreBLEU |\n|---|---|---|---|---|---| \n| Hindi | 78.06 | 0.59 | 0.56 | 42.27 | 59.07 |\n| Gujarati | 77.67 | 0.58 | 0.55 | 43.09 | 58.28 |\n| Tamil | 74.32 | 0.54 | 0.52 | 46.41 | 53.64 |", "caption": "Table 3: Back-translation evaluation metrics for the IndicMMLU-Pro dataset for 3 Indic languages.", "description": "\ubcf8 \ud45c\ub294 IndicMMLU-Pro \ub370\uc774\ud130\uc14b\uc758 \ud488\uc9c8\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \uc138 \uac00\uc9c0 \uc778\ub3c4 \uc5b8\uc5b4(\ud78c\ub514\uc5b4, \uad6c\uc790\ub77c\ud2b8\uc5b4, \ud0c0\ubc00\uc5b4)\uc5d0 \ub300\ud55c \uc5ed\ubc88\uc5ed \ud3c9\uac00 \uc9c0\ud45c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. chrF++, BLEU, METEOR, TER, SacreBLEU \ub4f1 \ub2e4\uc591\ud55c \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubc88\uc5ed\uc758 \uc815\ud655\uc131\uacfc \uc720\ucc3d\uc131\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uac01 \uc9c0\ud45c\ub294 0\uc5d0\uc11c 100\uae4c\uc9c0\uc758 \uc810\uc218\ub85c \ud45c\ud604\ub418\uba70, \ub192\uc740 \uc810\uc218\ub294 \ub354 \ub192\uc740 \ud488\uc9c8\uc758 \ubc88\uc5ed\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub370\uc774\ud130\uc14b\uc758 \ud488\uc9c8\uc744 \uac1d\uad00\uc801\uc73c\ub85c \ud3c9\uac00\ud558\uace0, \uc778\ub3c4 \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5 \ud3c9\uac00\uc5d0 \uc0ac\uc6a9\ub418\ub294 \ub370\uc774\ud130\uc14b\uc758 \uc2e0\ub8b0\uc131\uc744 \ud655\uc778\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3 \uacb0\uacfc"}, {"content": "| Language | Questions | Choices |\n|---|---|---|\n| Hindi | 0.9109 | 0.9250 |\n| Bengali | 0.9172 | 0.9251 |\n| Telugu | 0.9193 | 0.9287 |\n| Marathi | 0.9126 | 0.9242 |\n| Tamil | 0.9194 | 0.9255 |\n| Gujarati | 0.9164 | 0.9320 |\n| Urdu | 0.9121 | 0.9302 |\n| Kannada | 0.9149 | 0.9238 |\n| Punjabi | 0.9177 | 0.9254 |", "caption": "Table 4: Cosine similarity scores between LaBSE embeddings of IndicMMLU-Pro languages and English MMLU-Pro for questions and multiple-choice options. These scores are used as a measure of semantic similarity, with higher values suggesting closer meaning alignment across languages.", "description": "\ud45c 4\ub294 \uc601\uc5b4 MMLU-Pro\uc640 9\uac00\uc9c0 \uc778\ub3c4\uc5b4 MMLU-Pro \ub370\uc774\ud130\uc14b\uc758 \uc9c8\ubb38\uacfc \uc120\ud0dd\uc9c0\uc5d0 \ub300\ud55c LaBSE \uc784\ubca0\ub529 \uac04\uc758 \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub354 \ub192\uc740 \uc810\uc218\ub294 \uc5b8\uc5b4 \uac04 \uc758\ubbf8\uc801 \uc720\uc0ac\uc131\uc774 \ub354 \ud06c\ub2e4\ub294 \uac83\uc744 \ub098\ud0c0\ub0b4\uba70, \uc774\ub294 \ub2e4\uad6d\uc5b4 \ubaa8\ub378\uc758 \uc758\ubbf8\ub860\uc801 \uc77c\uad00\uc131\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.  \uc989, \ub192\uc740 \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4\ub294 \uc601\uc5b4 \uc9c8\ubb38\uacfc \uc778\ub3c4\uc5b4 \ubc88\uc5ed\ubcf8 \uc9c8\ubb38\uc774 \uc758\ubbf8\uc0c1\uc73c\ub85c \ub9e4\uc6b0 \uac00\uae5d\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubc88\uc5ed \uacfc\uc815\uc758 \uc9c8\uc801 \ud3c9\uac00\uc640 \uc778\ub3c4\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5 \ubd84\uc11d\uc5d0 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "3.4 \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4 \uc810\uc218"}]