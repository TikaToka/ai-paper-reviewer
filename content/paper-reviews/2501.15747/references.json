{"references": [{"fullname_first_author": "Tejaswini Ananthanarayana", "paper_title": "Dynamic cross-feature fusion for american sign language translation", "publication_date": "2021-00-00", "reason": "This paper is relevant to the current work because it also focuses on cross-lingual and cross-modal understanding, which is important for developing robust and inclusive multilingual AI models."}, {"fullname_first_author": "Abhinand Balachandran", "paper_title": "Tamil-llama: A new tamil language model based on llama 2", "publication_date": "2023-00-00", "reason": "This paper is relevant because it introduces a new large language model specifically for the Tamil language, which is one of the languages included in the benchmark."}, {"fullname_first_author": "Charith Chandra Sai Balne", "paper_title": "Parameter efficient fine tuning: A comprehensive analysis across applications", "publication_date": "2024-00-00", "reason": "This paper is relevant because it explores parameter-efficient fine-tuning techniques, which are crucial for developing effective and resource-efficient multilingual AI models."}, {"fullname_first_author": "Rewina Bedemariam", "paper_title": "Potential and perils of large language models as judges of unstructured textual data", "publication_date": "2025-00-00", "reason": "This paper is highly relevant to the current work as it provides a critical analysis of the capabilities and limitations of large language models when evaluating unstructured text data, an important consideration for the benchmark."}, {"fullname_first_author": "Divyanshu Kakwani", "paper_title": "IndicNLP-Suite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages", "publication_date": "2020-00-00", "reason": "This is a foundational paper for Indic NLP, providing resources that the current work builds upon.  It offers crucial corpora, benchmarks, and pre-trained models essential for evaluating Indic language models."}]}