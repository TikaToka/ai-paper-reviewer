<br><table id='10' style='font-size:16px'><tr><td>Competition</td><td>Metric</td><td>Best result</td></tr><tr><td>RuATD</td><td>Accuracy</td><td>0.820</td></tr><tr><td>AuTex-en</td><td>Macro-F1</td><td>0.809</td></tr><tr><td>AuTex-es</td><td>Macro-F1</td><td>0.708</td></tr><tr><td>IberAuTex</td><td>Macro-F1</td><td>0.805</td></tr><tr><td>SemEval24 Mono</td><td rowspan="2">Accuracy</td><td rowspan="2">0.975</td></tr><tr><td></td></tr><tr><td>SemEval24</td><td rowspan="2">Accuracy</td><td rowspan="2">0.959</td></tr><tr><td>Multi</td></tr><tr><td>PAN24</td><td>Avg. of 5 metrics*</td><td>0.924</td></tr><tr><td>DAGPap22</td><td>Avg. F1-score</td><td>0.994</td></tr></table>