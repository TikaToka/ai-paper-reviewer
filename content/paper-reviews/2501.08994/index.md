---
title: "RepVideo: Rethinking Cross-Layer Representation for Video Generation"
summary: "RepVideo: 크로스 레이어 표현 개선으로 더욱 정교하고 일관된 비디오 생성"
categories: ["AI Generated", "🤗 Daily Papers"]
tags: ["Computer Vision", "Video Understanding", "🏢 Nanyang Technological University",]
showSummary: true
date: 2025-01-15
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2501.08994 {{< /keyword >}}
{{< keyword icon="writer" >}} Chenyang Si et el. {{< /keyword >}}
 
{{< keyword >}} 🤗 2025-01-16 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2501.08994" target="_self" >}}
↗ arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2501.08994" target="_self" >}}
↗ Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/repvideo-rethinking-cross-layer" target="_self" >}}
↗ Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

최근 텍스트-비디오 생성 분야에서 확산 모델이 큰 발전을 이루었지만, 생성 비디오의 **공간적 세부 묘사와 시간적 일관성**은 여전히 개선의 여지가 있습니다. 기존 연구는 주로 모델 크기 확장에 초점을 맞추었지만, **표현 자체의 영향**에 대한 연구는 부족했습니다.

본 논문에서는 변환기 기반 확산 모델의 중간 레이어 특징을 분석하여, **레이어 간 불일치로 인한 공간적 표현 불안정성과 시간적 일관성 저하 문제점**을 밝혔습니다. 이를 해결하기 위해, 인접 레이어의 특징들을 통합하여 안정적인 특징을 생성하는 새로운 프레임워크인 RepVideo를 제시합니다. RepVideo는 **시간적 일관성 및 공간적 세부 묘사 모두에서 성능 향상**을 보이며, **비디오 생성의 질적 향상**에 기여합니다.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} 비디오 생성 모델의 중간 레이어 특징 분석을 통해, **공간적 표현의 불안정성과 시간적 일관성 저하** 문제점을 밝힘 {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} RepVideo 프레임워크를 제시하여, **인접 레이어 특징들을 통합**함으로써 안정적인 특징 표현을 구현하고 **공간적 세부 묘사와 시간적 일관성 향상** {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} 실험 결과를 통해 RepVideo가 **공간적 세부 묘사 및 시간적 일관성 모두에서 성능 향상**을 보임을 입증 {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
본 논문은 **비디오 생성 분야의 핵심적인 문제점인 공간적 세부 묘사와 시간적 일관성을 향상시키는 새로운 방법론**을 제시하여, 향후 연구에 중요한 영감을 제공하고 비디오 생성 모델의 성능 향상에 기여할 수 있습니다. 특히, **변환기 기반 확산 모델의 표현에 대한 심층적인 분석**을 통해, 기존 모델의 한계를 극복하고 새로운 연구 방향을 제시했다는 점에서 큰 의의가 있습니다.  **비디오 생성 분야의 발전에 직접적으로 기여**할 뿐 아니라, **다른 시각적 생성 모델 연구에도 시사점**을 제공할 수 있습니다.

------
#### Visual Insights



![](https://arxiv.org/html/2501.08994/x1.png)

> 🔼 RepVideo 모델이 생성한 다양한 비디오 예시들을 보여주는 그림입니다. RepVideo는 시간적 일관성이 향상되고 세밀한 공간적 디테일까지 표현하는 고품질의 다양한 비디오를 효과적으로 생성합니다. 그림에는 달, 호랑이, 풍경, 하트, 그리고 사람 등 다양한 주제의 비디오가 포함되어 있으며, 각 비디오는 부드러운 움직임과 사실적인 묘사를 보여줍니다.
> <details>
> <summary>read the caption</summary>
> Figure 1: The examples generated by RepVideo. RepVideo effectively generates diverse videos with enhanced temporal coherence and fine-grained spatial details.
> </details>





{{< table-caption >}}
| Method | Total Score | Motion Smoothness | Object Class | Multiple Objects | Spatial Relationship |
|---|---|---|---|---|---| 
| LaVie [17] | 77.08% | 96.38% | 91.82% | 33.32% | 34.09% |
| VideoCrafter-2.0 [55] | 80.44% | 97.73% | 92.55% | 40.66% | 35.86% |
| OpenSoraPlan-v1.1 | 78.00% | 98.28% | 76.30% | 40.35% | 53.11% |
| OpenSora-1.2 [56] | 79.76% | 98.50% | 82.22% | 51.83% | 68.56% |
| Gen-3 [57] | 82.32% | 99.23% | 87.81% | 53.64% | 65.09% |
| Gen-2 [58] | 80.58% | 99.58% | 90.92% | 55.47% | 66.91% |
| Pika-1.0 [59] | 80.69% | 99.50% | 88.72% | 43.08% | 61.03% |
| CogVideoX-2B [31] | 80.91% | 97.73% | 83.37% | 62.63% | 69.90% |
| CogVideoX-5B [31] | 81.61% | 96.92% | 85.23% | 62.11% | 66.35% |
| Vchitect-2.0 [60] | 81.57% | 97.76% | 87.81% | 69.35% | 54.64% |
| **RepVideo (Ours)** | **81.94%** | **98.13%** | **87.83%** | **71.18%** | **74.74%** |{{< /table-caption >}}

> 🔼 이 표는 논문에서 제시된 RepVideo 모델의 성능을 기존의 다른 비디오 생성 모델들과 비교 분석한 결과를 보여줍니다. VBench [54]라는 비디오 생성 모델 평가 지표를 사용하여, RepVideo와 기존 모델들의 전체 점수, 동작의 부드러움, 개체 분류, 다수 개체 인식, 공간적 관계 등 다양한 측면에서의 성능을 수치적으로 비교하고 있습니다. 이를 통해 RepVideo 모델이 기존 모델들에 비해 얼마나 향상된 성능을 보이는지 정량적으로 확인할 수 있습니다.
> <details>
> <summary>read the caption</summary>
> TABLE I: Comparison with previous methods on VBench [54] .
> </details>





### In-depth insights


#### RepVideo: Enhanced Video Generation
RepVideo는 비디오 생성에서 **시간적 일관성과 공간적 세부 묘사**를 향상시키는 데 중점을 둔 강화된 표현 방식입니다. 기존의 트랜스포머 기반 비디오 확산 모델은 계층 간의 주의도 분포의 변화로 인해 공간적 의미론이 단편화되고 시간적 일관성이 저하되는 문제점을 가지고 있습니다. RepVideo는 이러한 문제를 해결하기 위해 이웃 계층의 특징을 누적하여 **강화된 표현**을 형성하고, 이를 주의 메커니즘에 입력하여 **안정적인 의미론적 정보**를 포착하는 **특징 캐시 모듈**을 제안합니다.  실험 결과, RepVideo는 **공간적 정확도와 시간적 일관성**을 모두 크게 향상시켜 더욱 다양하고 사실적인 고품질 비디오 생성을 가능하게 합니다. 특히, **세부적인 공간 정보**를 잘 유지하면서 **매끄러운 시간적 전환**을 구현하는 데 효과적임을 보여줍니다.

#### Cross-Layer Feature Analysis
본 논문에서 제시된 'Cross-Layer Feature Analysis'는 **비디오 생성 모델 내부의 다층 표현(multi-layer representation) 특성을 심층 분석**하는 데 초점을 맞춥니다. 특히, **트랜스포머 기반 확산 모델(transformer-based diffusion model)**에서 각 레이어의 어텐션 맵(attention map) 분석을 통해 공간적 표현의 안정성과 일관성을 평가합니다. 분석 결과, 레이어가 깊어질수록 어텐션 맵의 분포가 변화하고, 이는 **인접 프레임 간의 특징 유사성 감소** 및 **시간적 일관성 저하**로 이어질 수 있음을 보여줍니다.  이는 단순히 모델의 크기를 키우는 것보다, **내부 표현의 질적 향상**에 초점을 맞춰야 함을 시사하는 중요한 발견입니다. 따라서 본 분석은 단순한 성능 평가를 넘어, **비디오 생성 모델의 설계 개선 및 성능 향상**에 대한 중요한 통찰력을 제공합니다.  **레이어 간의 상호 작용 및 특징 일관성 확보**를 위한 새로운 아키텍처 설계에 대한 방향성을 제시하며, 효율적인 비디오 생성 모델 개발을 위한 핵심적인 연구 방향을 제시합니다.

#### Feature Cache Mechanism
본 논문에서 제안하는 **Feature Cache Mechanism**은 비디오 생성 모델의 성능 향상을 위한 핵심 구성 요소입니다. 기존의 트랜스포머 기반 비디오 생성 모델은 각 레이어의 어텐션 맵에 상당한 차이가 있어, **공간적 일관성이 부족하고 시간적 일관성이 떨어지는 문제**를 갖고 있습니다. 이러한 문제를 해결하기 위해, Feature Cache Mechanism은 인접한 여러 레이어의 특징을 모아서 통합된 표현을 생성합니다. 이 통합된 표현은 **더욱 안정적이고 의미있는 정보**를 담고 있어, 공간적 세부 사항을 향상시키고 시간적 일관성을 유지하는 데 도움이 됩니다. 특히, **gating mechanism**을 통해 기존의 트랜스포머 입력과 통합된 표현을 동적으로 조합함으로써 레이어별 특징을 보존하면서도 의미론적 풍부함을 더할 수 있습니다.  **결과적으로, Feature Cache Mechanism은 비디오의 공간적 세부 묘사와 시간적 일관성을 모두 향상시켜 고품질 비디오 생성에 기여**합니다.  이는 단순히 모델의 크기를 키우는 것보다 효율적인 방법으로, 비디오 생성 모델의 성능을 획기적으로 개선하는 데 중요한 역할을 합니다.

#### RepVideo: Experimental Results
RepVideo의 실험 결과는 **정량적 및 정성적 측면 모두에서 괄목할 만한 성과**를 보여줍니다.  **VBench 벤치마크**를 통해 기존 최첨단 모델들보다 **전반적인 성능이 향상**되었음을 보여주었고, 특히 **동작의 부드러움과 객체 인식 정확도**에서 눈에 띄는 개선을 이루었습니다. **주관적인 평가**에서도 참가자들은 RepVideo가 생성한 비디오의 **시간적 일관성, 공간적 정확도, 텍스트와의 정합성**이 더 뛰어나다고 평가했습니다.  **에이블레이션 연구**는 RepVideo의 핵심 구성 요소인 **다층 특징 집계 및 게이팅 메커니즘**이 성능 향상에 중요한 역할을 한다는 것을 입증했습니다. **다양한 시각적 예시**는 RepVideo가 복잡한 공간적 관계와 역동적인 움직임을 가진 비디오를 사실적으로 생성할 수 있음을 보여주며, **시간적 일관성과 공간적 디테일** 모두에서 뛰어난 성능을 확인할 수 있었습니다.  **향후 연구**에서는 학습 데이터의 편향성 해소 및 계산 비용 최적화 방안 등을 통해 RepVideo의 성능을 더욱 개선하는 것을 목표로 합니다.

#### Future Research Directions
본 논문에서 제시된 RepVideo는 비디오 생성 분야에 상당한 발전을 가져왔지만, **여전히 개선의 여지가 있으며 추가 연구가 필요한 부분이 존재합니다.** 특히, **사전 훈련된 모델에 대한 의존성을 줄이고 다양성 및 적응성을 높이는 연구가 중요합니다.** 또한, **계산 비용을 줄이면서 효율성을 유지하는 기능 개선이 필요합니다.**  **인간 중심 콘텐츠 생성 및 복잡한 공간적 관계 이해 능력 향상에 대한 연구도 중요합니다.**  **실시간 기능 구현을 위한 효율적인 알고리즘 개발과 다양한 사전 훈련된 모델과의 통합 연구**도 향후 연구 방향으로 제시될 수 있습니다.  **더욱 발전된 RepVideo를 위해서는, 이러한 제한점들을 극복하고  비디오 생성의 질적 향상 및 실용성 확보**를 위한 연구가 지속적으로 필요합니다.  **특히, 다양한 데이터셋을 이용한 추가적인 실험과 모델 성능 평가는 필수적입니다.**


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2501.08994/x2.png)

> 🔼 그림 2는 최근 변환기 기반 비디오 확산 모델의 구조를 보여줍니다. 이러한 모델은 일반적으로 세 가지 핵심 구성 요소로 구성됩니다. 3D VAE는 공간 및 시간 차원을 따라 비디오 데이터를 압축하여 고해상도 비디오를 효율적으로 처리하고 GPU 메모리 사용량을 줄일 수 있는 컴팩트한 잠재 표현을 생성하는 데 사용됩니다. 텍스트 인코더는 입력 텍스트 프롬프트를 처리하여 비디오 생성 과정 전체를 안내하는 의미를 포착하는 임베딩 집합으로 변환합니다. 비디오의 잠재 표현은 토큰 시퀀스로 평평하게 처리되며 텍스트 임베딩 토큰과 함께 변환기 네트워크에 입력됩니다. 변환기의 강력한 어텐션 메커니즘을 활용하여 비디오 시퀀스 내의 복잡한 공간 및 시간적 관계를 학습하고, 생성된 프레임이 일관되고 입력 텍스트 프롬프트의 의미 정보와 정렬되도록 합니다. 이러한 구성 요소를 통합함으로써, 변환기 기반 확산 모델은 고해상도, 장시간 비디오를 생성하는 데 뛰어난 성능을 보여줍니다. 이는 시간적 일관성과 의미적 정렬이 모두 우수한 특징입니다.
> <details>
> <summary>read the caption</summary>
> Figure 2: The architecture of recent transformer-based video diffusion models. These methods typically consist of three core components: a 3D VAE, the text encoder, and a transformer network.
> </details>



![](https://arxiv.org/html/2501.08994/x3.png)

> 🔼 그림 3은 변환기 여러 계층에 걸쳐 전체 토큰 시퀀스에서 각 프레임의 토큰에 대한 어텐션 분포를 시각화한 것입니다. 결과는 계층 간 어텐션 분포의 상당한 차이를 보여줍니다. 깊은 계층일수록 동일한 프레임의 토큰에 더 집중하고 다른 프레임의 토큰에는 어텐션이 약해집니다. 이는 깊은 계층이 이전 계층보다 더욱 특정 프레임의 정보를 강조함으로써 프레임 간의 일관성을 유지하는 데 기여할 수 있음을 시사합니다. 즉, 상위 계층은 전체적인 맥락을 파악하고 하위 계층은 세부적인 정보를 처리하는 역할을 담당합니다. 또한, 각 계층은 서로 다른 특징 공간에 집중하여 다양한 시각적 정보를 포착하는 것을 보여줍니다.
> <details>
> <summary>read the caption</summary>
> Figure 3: The visualization of the attention distribution of each frame’s token across the entire token sequence. The results highlight significant variations in attention distributions across layers, with deeper layers focusing more on tokens from the same frame and exhibiting weaker attention to tokens from other frames.
> </details>



![](https://arxiv.org/html/2501.08994/x4.png)

> 🔼 그림 4는 변압기 여러 계층에 걸쳐 어텐션 맵을 시각화한 것입니다. 각 계층은 서로 다른 영역에 집중하여 다양한 공간적 특징을 포착하지만, 계층 간의 조정 부족으로 인해 특징 표현이 단편화되어 개별 프레임 내에서 일관된 공간적 의미를 확립하는 모델의 능력이 약화됩니다.  즉, 각 계층이 이미지의 다른 부분에 초점을 맞추어 정보를 추출하지만, 서로 정보를 공유하고 조율하는 과정이 부족하여 전체 이미지에 대한 일관된 이해가 어려워지는 현상을 보여줍니다. 이는 비디오 생성의 공간적 일관성에 부정적인 영향을 미칩니다.
> <details>
> <summary>read the caption</summary>
> Figure 4: The visualization of attention maps across transformer layers. Each layer attends to distinct regions, capturing diverse spatial features. However, the lack of coordination across layers results in fragmented feature representations, weakening the model’s ability to establish coherent spatial semantics within individual frames.
> </details>



![](https://arxiv.org/html/2501.08994/x5.png)

> 🔼 그림 5는 디퓨전 레이어와 디노이징 단계에 걸쳐 인접한 프레임 특징들 간의 평균 유사도를 보여줍니다.  특정 디노이징 단계에서 레이어의 깊이가 깊어질수록 유사도가 감소하는데, 이는 더 깊은 레이어에서 특징들이 더욱 차별화됨을 나타냅니다.  또한, 디노이징 과정이 진행됨에 따라 인접 프레임 간의 유사도는 감소하는데, 이는 디퓨전 모델이 디노이징 과정을 통해 프레임 특징들을 더욱 다양화시키기 때문입니다. 이러한 현상은 심층적인 레이어에서 시간적 일관성이 감소하고, 비디오 생성 과정에서 시간적 일관성을 저해할 수 있음을 시사합니다.
> <details>
> <summary>read the caption</summary>
> Figure 5: The average similarity between adjacent frame features across diffusion layers and denoising steps. The similarity decreases as layer depth increases for a given denoising step, indicating greater differentiation in deeper layers. Additionally, similarity between adjacent frames declines as the denoising process progresses.
> </details>



![](https://arxiv.org/html/2501.08994/x6.png)

> 🔼 이 그림은 표준 변압기 계층의 원래 특징 맵과 특징 캐시 모듈에서 집계된 후 얻은 특징 맵을 비교한 것입니다. 집계된 특징 맵은 더욱 포괄적인 의미 정보와 더 명확한 구조적 세부 정보를 보여줍니다.  즉, 여러 변압기 계층의 특징을 통합하여 더욱 풍부하고 안정적인 의미 정보를 얻고,  화질 개선 및 시간적 일관성 향상에 기여합니다.
> <details>
> <summary>read the caption</summary>
> Figure 6: The comparison of the original feature maps from a standard transformer layer with those obtained after aggregation in the Feature Cache Module. The aggregated features demonstrate more comprehensive semantic information and clearer structural details.
> </details>



![](https://arxiv.org/html/2501.08994/x7.png)

> 🔼 그림 7은 변압기 여러 계층의 특징을 집계하는 기능을 가진 특징 캐시 모듈을 통해 얻은 집계된 특징과 원래 변압기 계층에서 얻은 원래 특징 간의 인접 프레임 유사성 비교를 보여줍니다. 집계된 특징은 원래 변압기 계층에 비해 인접 프레임 간의 유사성이 더 높아 시간적 일관성이 향상되었음을 보여줍니다. 이는 특징 캐시 모듈이 여러 계층에서 특징을 통합하여 시간적 일관성을 높이는 데 효과적임을 시사합니다.  즉, 인접한 프레임들의 시각적 차이가 감소하여 비디오가 더 매끄럽게 보이는 효과가 있다는 것입니다.
> <details>
> <summary>read the caption</summary>
> Figure 7: The comparison of adjacent frame similarity between original and aggregated features. The aggregated features from the Feature Cache Module exhibit higher similarity between adjacent frames compared to the original transformer layers, indicating improved temporal coherence.
> </details>



![](https://arxiv.org/html/2501.08994/x8.png)

> 🔼 그림 8은 RepVideo의 향상된 계층 간 표현 구조를 보여줍니다. 기존의 트랜스포머 블록에 Feature Cache Module과 Gating Mechanism을 추가하여 이전 계층들의 특징들을 활용합니다. Feature Cache Module은 여러 계층의 출력 토큰 시퀀스를 저장하고, 이들의 평균을 계산하여 강화된 특징을 생성합니다. Gating Mechanism은 원본 트랜스포머 입력과 강화된 특징을 결합하여 각 트랜스포머 계층에 향상된 입력을 제공합니다. 이를 통해 RepVideo는 공간적 세부 정보를 유지하면서도 프레임 간 일관성을 유지하여 더욱 향상된 시계열 일관성과 공간적 품질을 제공합니다.
> <details>
> <summary>read the caption</summary>
> Figure 8: The architecture of the enhanced cross-layer representation framework.
> </details>



![](https://arxiv.org/html/2501.08994/x9.png)

> 🔼 그림 9는 제안된 RepVideo 모델과 기준 모델인 CogVideoX-2B [31]의 비디오 생성 결과를 정성적으로 비교한 것입니다.  CogVideoX-2B [31] 모델의 결과는 위쪽 행에, RepVideo 모델의 결과는 아래쪽 행에 나열되어 있습니다. 각 행은 동일한 텍스트 프롬프트를 사용하여 생성된 비디오의 일부 프레임을 보여줍니다. RepVideo가 비디오의 질과 일관성 면에서 상당한 개선을 보여주는 것을 보여주기 위해 비디오 시퀀스 전체를 보여주지는 않고,  몇몇 대표적인 프레임들을 선별하여 보여주고 있습니다.  이 그림을 통해 RepVideo가 공간적 세부 묘사와 시간적 일관성 모두에서 향상된 성능을 보임을 직관적으로 확인할 수 있습니다.  특히, RepVideo는 기준모델보다 더욱 세밀하고 정확한 이미지를 생성하며, 움직임의 자연스러움과 연속성 또한 더욱 뛰어남을 알 수 있습니다.
> <details>
> <summary>read the caption</summary>
> Figure 9: The qualitative comparison between our method and the baseline CogVideoX-2B [31]. The first row shows results from the baseline CogVideoX-2B [31], while the second row presents the results generated by RepVideo, demonstrating significant improvements in quality and coherence.
> </details>



![](https://arxiv.org/html/2501.08994/x10.png)

> 🔼 그림 10은 CogVideoX-2B와 RepVideo의 특징 맵을 계층별로 비교한 것입니다. RepVideo는 CogVideoX-2B [31]에 비해 모든 계층에서 더 풍부한 의미 정보를 일관되게 포착하고 더욱 일관성 있는 공간적 세부 정보를 유지합니다.  즉, RepVideo는 이미지의 다양한 측면을 더욱 정확하고 상세하게 표현하며, 이는 영상 생성의 질 향상에 크게 기여합니다.  특히 깊이 있는 레이어에서도 의미 정보의 손실이 적고, 보다 일관된 시각적 정보를 제공하여, 보다 자연스럽고 현실적인 영상 생성에 유리합니다.
> <details>
> <summary>read the caption</summary>
> Figure 10: The Layer-wise comparison of feature maps between CogVideoX-2B and RepVideo. The comparison shows that RepVideo consistently captures richer semantic information and maintains more coherent spatial details across layers compared to CogVideoX-2B [31].
> </details>



![](https://arxiv.org/html/2501.08994/x11.png)

> 🔼 그림 11은 CogVideoX-2B와 RepVideo의 어텐션 맵을 비교하여 RepVideo가 CogVideoX-2B [31]보다 일관된 의미 관계를 유지함을 보여줍니다.  CogVideoX-2B는 깊이가 깊어짐에 따라 어텐션이 여러 프레임에 걸쳐 퍼지고, 의미적으로 관련 없는 영역에 집중하는 반면, RepVideo는 특정 객체나 주제에 대한 어텐션이 더욱 집중적이고 일관되어 시맨틱한 일관성을 유지합니다. 이는 RepVideo의 강화된 표현 방식이 각 프레임의 의미적 연관성을 보다 효과적으로 포착하고, 시각적 일관성을 향상시키는 데 기여함을 시사합니다. 다양한 레이어에서의 어텐션 분포를 시각적으로 비교함으로써, RepVideo가 시맨틱 정보를 보다 효과적으로 유지하고, 시계열 일관성을 개선하는 데 도움이 됨을 보여줍니다.
> <details>
> <summary>read the caption</summary>
> Figure 11: The comparison of attention maps between CogVideoX-2B and RepVideo. The comparison demonstrates that RepVideo could maintain more consistent semantic relationship compared to CogVideoX-2B [31].
> </details>



![](https://arxiv.org/html/2501.08994/x12.png)

> 🔼 그림 12는 여러 레이어에 걸쳐 연속된 프레임 간의 코사인 유사도를 보여줍니다.  각 레이어별로 연속 프레임 쌍의 코사인 유사도를 계산하여, 시간적 일관성을 정량적으로 평가합니다. 높은 코사인 유사도는 프레임 간의 시각적 유사성이 크다는 것을 나타내며, 따라서 더 매끄럽고 시간적 일관성이 높은 비디오 생성을 의미합니다. 이 그림은 RepVideo 모델이 이전 모델인 CogVideoX-2B보다 연속 프레임 간의 유사도가 더 높다는 것을 보여주는 중요한 결과를 제시하며, RepVideo의 시간적 일관성 개선 효과를 시각적으로 보여줍니다.  특히 깊은 레이어에서도 높은 유사도를 유지하는 것은  RepVideo의 특징입니다.
> <details>
> <summary>read the caption</summary>
> Figure 12: The cosine similarity between consecutive frames across layers.
> </details>



</details>






### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}