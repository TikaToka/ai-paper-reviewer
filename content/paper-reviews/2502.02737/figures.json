[{"figure_path": "https://arxiv.org/html/2502.02737/x2.png", "caption": "Figure 1: Performance of models trained on different subsets of FineMath and other math datasets.", "description": "\uadf8\ub9bc 1\uc740 \ub2e4\uc591\ud55c FineMath \ud558\uc704 \uc9d1\ud569\uacfc \ub2e4\ub978 \uc218\ud559 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc138\ubd80\uc801\uc73c\ub85c\ub294 GSM8K, MATH, MMLU-STEM \uc138 \uac00\uc9c0 \uc218\ud559 \uad00\ub828 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c FineMath\uc758 \uc5ec\ub7ec \ud558\uc704 \uc9d1\ud569(FineMath3+, FineMath4+, \uadf8\ub9ac\uace0 Infi-WebMath3+, Infi-WebMath4+)\uacfc \uae30\uc874\uc758 OWM, InfiMM-WebMath \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub41c \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uac01 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ud1a0\ud070 \uc218\uc5d0 \ub530\ub978 \uc815\ud655\ub3c4 \ubcc0\ud654\ub97c \uadf8\ub798\ud504\ub85c \uc81c\uc2dc\ud558\uc5ec, \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\uc640 \ud488\uc9c8\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 FineMath \ub370\uc774\ud130\uc14b\uc758 \uacbd\uc6b0, \uace0\ud488\uc9c8 \uc218\ud559\uc801 \ucd94\ub860 \ub370\uc774\ud130\ub97c \ud3ec\ud568\ud558\uc5ec \ub2e4\ub978 \ub370\uc774\ud130\uc14b\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. Pretraining datasets"}, {"figure_path": "https://arxiv.org/html/2502.02737/x3.png", "caption": "Figure 2: Dataset mixtures across training stages. Detailed descriptions are provided in\u00a0Section\u00a04. The x-axis represents the number of training tokens.", "description": "\uadf8\ub9bc 2\ub294 SmolLM2 \uc0ac\uc804 \ud6c8\ub828\uc758 \uc5ec\ub7ec \ub2e8\uacc4\uc5d0 \uac78\uccd0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \ube44\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 \ud6c8\ub828 \ud1a0\ud070 \uc218\ub97c \ub098\ud0c0\ub0b4\uba70,  \uac01 \ub2e8\uacc4\ubcc4\ub85c \uc6f9 \ud14d\uc2a4\ud2b8, \ucf54\ub4dc, \uc218\ud559 \ub370\uc774\ud130, \uc9c0\uce68 \ub530\ub974\uae30 \ub370\uc774\ud130 \ub4f1 \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b\uc774 \uc5b4\ub5a4 \ube44\uc728\ub85c \ud63c\ud569\ub418\uc5b4 \uc0ac\uc6a9\ub418\uc5c8\ub294\uc9c0 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc139\uc158 4\uc5d0\uc11c\ub294 \uac01 \ub2e8\uacc4\ubcc4 \ub370\uc774\ud130\uc14b \ud63c\ud569 \uc804\ub7b5\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uc124\uba85\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ub370\uc774\ud130 \uc720\ud615\uc758 \ud63c\ud569 \ube44\uc728\uc744 \uc870\uc815\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc744 \ucd5c\uc801\ud654\ud558\ub294 \uacfc\uc815\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "4. \uc0ac\uc804 \ud6c8\ub828"}, {"figure_path": "https://arxiv.org/html/2502.02737/x4.png", "caption": "Figure 3: Learning rate during SmolLM2 training. We used WSD scheduler with 2000 steps warmup, learning rate 5.0\u00d710\u221245.0superscript1045.0\\times 10^{-4}5.0 \u00d7 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT and 10% decay.", "description": "\uadf8\ub9bc 3\uc740 SmolLM2\uc758 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904\ub7ec(WSD)\uc758 \ud559\uc2b5\ub960 \ubcc0\ud654 \uc591\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  2000 \uc2a4\ud15d\uc758 \uc6cc\ubc0d\uc5c5 \uae30\uac04\uc744 \uac70\uce5c \ud6c4, \ud559\uc2b5\ub960\uc740 5.0 x 10^-4\ub85c \uc124\uc815\ub418\uc5c8\uace0, \ud559\uc2b5\uc774 \uc9c4\ud589\ub428\uc5d0 \ub530\ub77c 10%\uc529 \uac10\uc18c\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uadf8\ub9bc\uc740 \ud559\uc2b5 \ud1a0\ud070 \uc218\uc5d0 \ub530\ub978 \ud559\uc2b5\ub960\uc758 \ubcc0\ud654\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\uc5b4, \ubaa8\ub378 \ud559\uc2b5\uc758 \uc548\uc815\uc131\uacfc \ud6a8\uc728\uc131\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.  \uc989, \ud559\uc2b5 \ucd08\ubc18\uc5d0\ub294 \ud559\uc2b5\ub960\uc744 \ucc9c\ucc9c\ud788 \uc99d\uac00\uc2dc\ucf1c \ubaa8\ub378\uc774 \uc548\uc815\uc801\uc73c\ub85c \ud559\uc2b5\uc744 \uc2dc\uc791\ud558\ub3c4\ub85d \ub3d5\uace0, \ud559\uc2b5 \ud6c4\ubc18\uc5d0\ub294 \ud559\uc2b5\ub960\uc744 \uc11c\uc11c\ud788 \uac10\uc18c\uc2dc\ucf1c \uacfc\uc801\ud569\uc744 \ubc29\uc9c0\ud558\uace0 \ucd5c\uc801\uc758 \uc131\ub2a5\uc744 \uc5bb\ub3c4\ub85d \ud558\ub294 WSD \uc2a4\ucf00\uc904\ub7ec\uc758 \uc791\ub3d9 \uc6d0\ub9ac\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Pretraining"}, {"figure_path": "https://arxiv.org/html/2502.02737/x5.png", "caption": "Figure 4: Evaluation of models trained on FineWeb-Edu and DCLM for 350B tokens. FineWeb-Edu excels at knowledge and reasoning tasks, while DCLM demonstrates stronger performance on commonsense reasoning benchmarks. A 60/40 mixture of FineWeb-Edu and DCLM achieves balanced performance across all tasks.", "description": "\uadf8\ub9bc 4\ub294 FineWeb-Edu\uc640 DCLM\uc73c\ub85c 3500\uc5b5 \ud1a0\ud070\uc744 \ud559\uc2b5\uc2dc\ud0a8 \ubaa8\ub378\ub4e4\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. FineWeb-Edu\ub294 \uc9c0\uc2dd \ubc0f \ucd94\ub860 \uacfc\uc81c\uc5d0\uc11c \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \ubc18\uba74, DCLM\uc740 \uc0c1\uc2dd \ucd94\ub860 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub354 \uac15\ub825\ud55c \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. FineWeb-Edu\uc640 DCLM\uc744 60:40\uc73c\ub85c \ud63c\ud569\ud55c \ubaa8\ub378\uc740 \ubaa8\ub4e0 \uacfc\uc81c\uc5d0\uc11c \uade0\ud615 \uc7a1\ud78c \uc131\ub2a5\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc6f9 \ub370\uc774\ud130 \uc18c\uc2a4\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ubcf4\uc5ec\uc8fc\uba70, \ubaa8\ub378 \uc131\ub2a5\uc744 \ucd5c\uc801\ud654\ud558\uae30 \uc704\ud574 \ub370\uc774\ud130 \uc18c\uc2a4\ub97c \uc801\uc808\ud788 \uc870\ud569\ud558\ub294 \uac83\uc774 \uc911\uc694\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3. Pretraining datasets"}, {"figure_path": "https://arxiv.org/html/2502.02737/x6.png", "caption": "Figure 5: Results of annealing ablations comparing OWM and the text component of InfiMM-WebMath. InfiMM-WebMath consistently outperforms OWM on GSM8K, while OWM has a slight advantage on MATH. Despite training on 60B math tokens (equivalent to 5 epochs for OWM and 1.5 epochs for InfiMM-WebMath), performance remains far below state-of-the-art LLMs, highlighting the need for a new math dataset.", "description": "\uadf8\ub9bc 5\ub294 OWM\uacfc InfiMM-WebMath\uc758 \uc131\ub2a5\uc744 GSM8K\uc640 MATH \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ube44\uad50\ud55c \uc5b4\ub2d0\ub9c1 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. InfiMM-WebMath\ub294 GSM8K\uc5d0\uc11c OWM\uc744 \uc9c0\uc18d\uc801\uc73c\ub85c \ub2a5\uac00\ud558\uc9c0\ub9cc, OWM\uc740 MATH\uc5d0\uc11c \uc57d\uac04\uc758 \uc6b0\uc704\ub97c \ubcf4\uc785\ub2c8\ub2e4. OWM\uacfc InfiMM-WebMath \ubaa8\ub450 600\uc5b5 \uac1c\uc758 \uc218\ud559 \ud1a0\ud070(OWM\uc758 \uacbd\uc6b0 5 epoch, InfiMM-WebMath\uc758 \uacbd\uc6b0 1.5 epoch\uc5d0 \ud574\ub2f9)\uc73c\ub85c \ud559\uc2b5\ud588\uc74c\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ucd5c\ucca8\ub2e8 LLM\uc5d0 \ube44\ud574 \uc131\ub2a5\uc774 \ud6e8\uc52c \ub5a8\uc5b4\uc9d1\ub2c8\ub2e4. \uc774\ub294 \uc0c8\ub85c\uc6b4 \uc218\ud559 \ub370\uc774\ud130 \uc138\ud2b8\uc758 \ud544\uc694\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.  \uadf8\ub9bc\uc740 \ub450 \ub370\uc774\ud130\uc14b\uc758 \uc5b4\ub2d0\ub9c1 \uace1\uc120\uc744 \ubcf4\uc5ec\uc8fc\uba70,  \ud1a0\ud070 \uc218\uc5d0 \ub530\ub978 GSM8K\uc640 MATH \uc815\ud655\ub3c4 \ubcc0\ud654\ub97c \ube44\uad50\ud558\uc5ec \uac01 \ub370\uc774\ud130\uc14b\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Pretraining datasets"}, {"figure_path": "https://arxiv.org/html/2502.02737/x7.png", "caption": "Figure 6: Progression of MMLU MCF and MLU CF during the training. We observe above-random (>25%) accuracy on MMLU MCF after 6T tokens of training, while MMLU CF appears to plateau.", "description": "\uadf8\ub9bc 6\uc740 \uc548\uc815\uc801\uc778 \ub2e8\uacc4(\ud559\uc2b5\ub960 \uac10\uc18c \uc5c6\uc74c)\uc5d0\uc11c\uc758 \ud6c8\ub828 \uacfc\uc815 \ub3d9\uc548 MMLU(Massive Multitask Language Understanding) \uc810\uc218\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MMLU MCF(\ub2e4\uc911 \uc120\ud0dd \uc9c8\ubb38)\ub294 6\uc870 \ud1a0\ud070\uc758 \ud559\uc2b5 \ud6c4 25%\ub97c \ub118\ub294 \uc784\uc758\uc131\uc744 \ubc97\uc5b4\ub098\ub294 \uc815\ud655\ub3c4\ub97c \ubcf4\uc774\ub294 \ubc18\uba74, MMLU CF(\ud3d0\uc1c4\ud615 \uc9c8\ubb38)\ub294 \uc815\uccb4\ub418\ub294 \uacbd\ud5a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. \uc774\ub294 \uc18c\uaddc\ubaa8 \ubaa8\ub378\uc758 \uc7a5\uae30 \ud6c8\ub828\uc744 \ud1b5\ud574 \ub300\uaddc\ubaa8 \ubaa8\ub378\uacfc \uc77c\ubc18\uc801\uc73c\ub85c \uad00\ub828\ub41c \uae30\ub2a5\uc744 \uc5bb\uc744 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4. Pretraining"}, {"figure_path": "https://arxiv.org/html/2502.02737/x8.png", "caption": "Figure 7: Needle in the Haystack evaluation of SmolLM2 with 8192 context length.", "description": "\uadf8\ub9bc 7\uc740 8192 \ud1a0\ud070\uc758 \ubb38\ub9e5 \uae38\uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec SmolLM2 \ubaa8\ub378\uc744 \ud3c9\uac00\ud55c Needle in the Haystack \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ubca4\uce58\ub9c8\ud06c\ub294 \ubaa8\ub378\uc774 \uae34 \ubb38\uc11c\uc5d0\uc11c \ud2b9\uc815 \uc815\ubcf4\ub97c \ucc3e\ub294 \ub2a5\ub825\uc744 \uce21\uc815\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc740 SmolLM2 \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \ubb38\ub9e5 \uae38\uc774\uc5d0\uc11c \uc5bc\ub9c8\ub098 \uc815\ud655\ud558\uac8c \uc815\ubcf4\ub97c \ucc3e\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\ub97c \ub098\ud0c0\ub0bc \uac83\uc73c\ub85c \uc608\uc0c1\ub429\ub2c8\ub2e4. \uc989, \uae34 \ubb38\ub9e5\uc744 \ucc98\ub9ac\ud558\ub294 \ubaa8\ub378\uc758 \ub2a5\ub825\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc785\ub2c8\ub2e4.", "section": "G. Long context evaluations"}]