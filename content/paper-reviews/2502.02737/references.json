{"references": [{"fullname_first_author": "Touvron, H.", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023", "reason": "This paper introduces Llama, the foundational architecture for SmolLM2, significantly influencing its design and capabilities."}, {"fullname_first_author": "Dubey, A.", "paper_title": "The llama 3 herd of models", "publication_date": "2024", "reason": "This paper presents Llama 3, a crucial model used for data filtering and quality assessment within the SmolLM2 training process."}, {"fullname_first_author": "Penedo, G.", "paper_title": "The fineweb datasets: Decanting the web for the finest text data at scale", "publication_date": "2024", "reason": "This paper introduces the FineWeb datasets, a key component of SmolLM2's pretraining data, which is crucial for its performance."}, {"fullname_first_author": "Li, R.", "paper_title": "Starcoder: may the source be with you!", "publication_date": "2023", "reason": "This paper introduces the StarCoder dataset, used extensively in SmolLM2's training, especially for incorporating code-related data."}, {"fullname_first_author": "Yang, A.", "paper_title": "Qwen2 technical report", "publication_date": "2024", "reason": "This paper describes Qwen2, a model against which SmolLM2 is compared extensively throughout the paper, providing a benchmark for evaluating its performance."}]}