[{"content": "| Task | FW-Edu | DCLM | 40/60 | 60/40 |\n|---|---|---|---|---|\n| MMLU | **37.5** | 35.5 | 36.5 | 37.0 |\n| ARC | **57.5** | 53.5 | 53.2 | 56.0 |\n| OpenBookQA | **41.9** | 40.8 | 39.0 | **41.9** |\n| HellaSwag | 60.1 | **62.3** | 61.4 | 62.2 |\n| CommonsenseQA | 36.2 | **40.1** | 39.9 | 38.5 |\n| PIQA | 76.2 | **76.9** | 75.7 | 76.4 |", "caption": "Table 1: Evaluation of models trained on FineWeb-Edu and DCLM for 350B tokens. 40/60 and 60/40 denote the FW-Edu/DCLM ratio.", "description": "\ubcf8 \ud45c\ub294 3500\uc5b5 \ud1a0\ud070\uc758 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec FineWeb-Edu\uc640 DCLM\uc73c\ub85c \ud559\uc2b5\ub41c \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. FineWeb-Edu\uc640 DCLM\uc758 \ube44\uc728\uc744 40:60\uacfc 60:40\uc73c\ub85c \uc870\uc815\ud558\uc5ec \uc2e4\ud5d8\ud558\uc600\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 MMLU, ARC, OpenBookQA, HellaSwag, CommonsenseQA, PIQA \ub4f1 \ub2e4\uc591\ud55c \uacfc\uc81c\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5\uacfc \ub450 \ub370\uc774\ud130\uc14b\uc744 \ud63c\ud569\ud558\uc5ec \uc0ac\uc6a9\ud558\ub294 \ud6a8\uacfc\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc608\ub97c \ub4e4\uc5b4 FineWeb-Edu\ub294 \uad50\uc721 \uad00\ub828 \uc9c0\ud45c\uc5d0\uc11c \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ubcf4\uc774\ub294 \ubc18\uba74, DCLM\uc740 \uc0c1\uc2dd \ucd94\ub860 \uacfc\uc81c\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4.  \ub450 \ub370\uc774\ud130\uc14b\uc744 \uc801\uc808\ud788 \ud63c\ud569\ud568\uc73c\ub85c\uc368 \uc804\ubc18\uc801\uc778 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Pretraining datasets"}, {"content": "| Language | StarCoder2Data (B tokens) | Stack-Edu (B tokens) | MultiPL-E (Original \u2192 Filtered) |\n|---|---|---|---| \n| Python | 50.6 | 21.8 | 20.7 \u2192 25.6 |\n| C++ | 69.7 | 16.0 | 16.7 \u2192 24.8 |\n| JavaScript | 45.3 | 11.1 | 18.2 \u2192 22.4 |\n| Java | 45.6 | 42.1 | 17.6 \u2192 22.7 |", "caption": "Table 2: Stack-Edu dataset statistics and MultiPL-E scores for the top 4 (in terms of size) programming languages. We use HumanEval for Python evaluation.", "description": "\ud45c 2\ub294 Stack-Edu \ub370\uc774\ud130\uc14b\uc758 \ud1b5\uacc4\uc640 \uc0c1\uc704 4\uac1c \ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4(\ud06c\uae30 \uae30\uc900)\uc5d0 \ub300\ud55c MultiPL-E \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Stack-Edu\ub294 \uad50\uc721\uc801\uc778 \ucf54\ub4dc\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd98 StarCoder2Data\uc758 \ud544\ud130\ub9c1\ub41c \ud558\uc704 \uc9d1\ud569\uc785\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\uc758 \uc6d0\ub798 \ud06c\uae30(StarCoder2Data \uae30\uc900)\uc640 \ud544\ud130\ub9c1 \ud6c4 Stack-Edu\uc758 \ud06c\uae30\uac00 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4. Python\uc758 \uacbd\uc6b0 HumanEval\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\uc5d0 \ub300\ud55c \uad50\uc721\uc801 \ud544\ud130\ub9c1\uc758 \ud6a8\uacfc\uc640 Stack-Edu \ub370\uc774\ud130\uc14b\uc758 \uaddc\ubaa8\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.4. Code data"}, {"content": "|---|---|---|---|---|\n| **Stage 1** | **Stage 2** | **Stage 3** | **Stage 4** | **Tokens** |\n| 0-6T | 6-8T | 8-10T | 10-11T |  |\n| 55.50 | 56.76 | 57.47 | **60.24** | Knowledge/Reasoning |\n| 3.21 | 3.7 | 7.27 | **22.07** | Math |\n| 8.87 | 10.56 | 16.75 | **23.21** | Code |\n| 31.54 | 31.30 | 34.70 | **36.12** | Generative Tasks |", "caption": "Table 3: Average model performance on different benchmark categories after each training stage. Stages 1-3 are during stable phase (no decay). Full per-benchmark results in Section\u00a0E.1.", "description": "\ud45c 3\uc740 \uac01 \ud559\uc2b5 \ub2e8\uacc4 \ud6c4 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c \ubc94\uc8fc\uc5d0 \ub300\ud55c \ubaa8\ub378 \uc131\ub2a5 \ud3c9\uade0\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 1~3\ub2e8\uacc4\ub294 \uc548\uc815\uc801\uc778 \ub2e8\uacc4(\uac10\uc1e0 \uc5c6\uc74c)\uc774\uba70, \uc804\uccb4 \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub294 E.1\uc808\uc5d0 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \ud559\uc2b5 \ub2e8\uacc4(0-6T \ud1a0\ud070, 6-8T \ud1a0\ud070, 8-10T \ud1a0\ud070, 10-11T \ud1a0\ud070) \uc5d0\uc11c \uc9c0\uc2dd/\ucd94\ub860, \uc218\ud559, \ucf54\ub4dc, \uc0dd\uc131 \uc791\uc5c5 \ub4f1\uc758 \ub124 \uac00\uc9c0 \ubc94\uc8fc\uc5d0 \ub300\ud55c \ubaa8\ub378 \uc131\ub2a5 \ud3c9\uade0\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130 \uc138\ud2b8\uc758 \uc870\ud569\uacfc \ubaa8\ub378\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \ub2e4\uc591\ud55c \ub370\uc774\ud130 \uc720\ud615\uacfc \ud559\uc2b5 \uc804\ub7b5\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud30c\uc545\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.", "section": "4. Pretraining"}, {"content": "| Model family | SmolLM2 | Llama3.2 | Qwen2.5 |\n|---|---|---|---| \n| Parameters | 1.7B | 1B | 1.5B |\n| HellaSwag | **68.7** | 61.2 | 66.4 |\n| ARC | **60.5** | 49.2 | 58.5 |\n| PIQA | **77.6** | 74.8 | 76.1 |\n| CommonsenseQA | **43.6** | 41.2 | 34.1 |\n| Winogrande | **59.4** | 57.8 | 59.3 |\n| OpenBookQA | **42.2** | 38.4 | 40.0 |\n| MMLU-Pro (held-out) | **19.4** | 11.7 | 13.7 |\n| Natural Questions (held-out) | 8.7 | 6.2 | **10.5** |\n| TriviaQA (held-out) | **36.7** | 28.1 | 20.9 |\n| GSM8K (5-shot) | 31.1 | 7.6 | **61.7** |\n| MATH (4-shot) | 11.6 | 3.3 | **34.3** |\n| HumanEval | 22.6 | 18.9 | **37.2** |", "caption": "Table 4: Performance comparison of SmolLM2 and other 1-2B base models across benchmarks. SmolLM2 demonstrates competitive results highlighting its generalization capabilities.", "description": "\ud45c 4\ub294 10\uc5b5~20\uc5b5 \ud30c\ub77c\ubbf8\ud130 \uaddc\ubaa8\uc758 \uae30\ubcf8 \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc744 \uc5ec\ub7ec \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ube44\uad50 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. SmolLM2\ub294 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \ubcf4\uc774\uba70, \ud2b9\ud788 \uc77c\ubc18\ud654 \ub2a5\ub825\uc774 \ub6f0\uc5b4\ub0a8\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc885\ub958\uc758 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud568\uc73c\ub85c\uc368, SmolLM2\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ud30c\uc545\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  \ubca4\uce58\ub9c8\ud06c\ub294 \uc9c0\uc2dd, \ucd94\ub860, \ucf54\ub4dc \uc0dd\uc131 \ubc0f \uc0dd\uc131 \uc791\uc5c5 \ub4f1 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  \ud45c\ub294 \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc218\uce58\ud654\ud558\uc5ec \uc81c\uc2dc\ud558\uba70, SmolLM2\uac00 \ub2e4\uc591\ud55c \uc791\uc5c5\uc5d0\uc11c \uacbd\uc7c1\ub825 \uc788\ub294 \uacb0\uacfc\ub97c \uc5bb\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.7. \uae30\ubcf8 \ubaa8\ub378 \ud3c9\uac00"}, {"content": "| Model | SmolLM2-1.7B | Llama3.2-1B | Qwen2.5-1.5B |\n|---|---|---|---| \n| IFEval (Average) | **56.7** | 53.5 | 47.4 |\n| MT-Bench | 6.13 | 5.48 | **6.52** |\n| OpenRewrite-Eval | 44.9 | 39.2 | **46.9** |\n| ARC | **51.7** | 41.6 | 46.2 |\n| BBH (3-shot) | 32.2 | 27.6 | **35.3** |\n| MMLU-Pro | 19.3 | 12.7 | **24.2** |\n| HellaSwag | **66.1** | 56.1 | 60.9 |\n| PIQA | **74.4** | 72.3 | 73.2 |\n| GSM8K (5-shot) | 48.8 | 37.4 | **63.3** |\n| MATH (4-shot) | **21.0** | 19.5 | 19.6 |\n| HumanEval | 28.1 | **33.5** | 30.5 |", "caption": "Table 5: Comparison of 1-2B instruction-tuned models across benchmarks. SmolLM2-1.7B-Instruct exhibits strong performance in instruction-following, reasoning, and math.", "description": "\ud45c 5\ub294 10\uc5b5~20\uc5b5 \ud30c\ub77c\ubbf8\ud130 \uaddc\ubaa8\uc758 \uc9c0\uc2dc \uc870\uc815\ub41c \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc744 \uc5ec\ub7ec \ubca4\uce58\ub9c8\ud06c \uae30\uc900\uc73c\ub85c \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  SmolLM2-1.7B-Instruct \ubaa8\ub378\uc740 \uc9c0\uc2dc \ub530\ub974\uae30, \ucd94\ub860 \ubc0f \uc218\ud559 \ubb38\uc81c \ud574\uacb0 \uacfc\uc81c\uc5d0\uc11c \uac15\ub825\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc9c0\uc2dc \ub530\ub974\uae30, \ucd94\ub860 \ubc0f \uc218\ud559\uc801 \ub2a5\ub825\uc744 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c \uc810\uc218\ub97c \ud1b5\ud574 \ube44\uad50\ud558\uc5ec, SmolLM2 \ubaa8\ub378\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. Post-training"}, {"content": "| Parameter | Value |\n|---|---| \n| Layers | 24 |\n| Model Dimension | 2,048 |\n| FFN Dimension | 8,192 |\n| Attention Heads | 32 |\n| Sequence Length | 2,048 \u2020 |\n| Token per batch | 2M |\n| Tied embedding | Yes |\n| Positional Embeddings | RoPE (<math alttext='\\theta=10,000' display='inline'>\u03b8=10,000</math>) |\n| Activation Function | SwiGLU |", "caption": "Table 6: Overview of the architecture of SmolLM2. \u2020 This is before extending the context to 8k tokens.", "description": "\ud45c 6\uc740 SmolLM2\uc758 \uc544\ud0a4\ud14d\ucc98 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  SmolLM2\ub294 1.7B \ub9e4\uac1c\ubcc0\uc218\ub97c \uac00\uc9c4 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378\uc774\uba70, \uc774 \ud45c\ub294 \ubaa8\ub378\uc758 \uc8fc\uc694 \uad6c\uc131 \uc694\uc18c\uc778 \ub808\uc774\uc5b4 \uc218, \ubaa8\ub378 \ucc28\uc6d0, FFN \ucc28\uc6d0, \uc5b4\ud150\uc158 \ud5e4\ub4dc \uc218, \uc2dc\ud000\uc2a4 \uae38\uc774, \ubc30\uce58\ub2f9 \ud1a0\ud070 \uc218, \uc784\ubca0\ub529, \uc704\uce58 \uc784\ubca0\ub529, \ud65c\uc131\ud654 \ud568\uc218 \ub4f1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788, \uc2dc\ud000\uc2a4 \uae38\uc774\ub294 \ucee8\ud14d\uc2a4\ud2b8\ub97c 8k \ud1a0\ud070\uc73c\ub85c \ud655\uc7a5\ud558\uae30 \uc804\uc758 \uac12\uc744 \ub098\ud0c0\ub0b4\ub294 \u2020 \ud45c\uc2dc\uac00 \ub418\uc5b4\uc788\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 SmolLM2 \ubaa8\ub378\uc758 \uad6c\uc870\uc640 \ud06c\uae30, \uadf8\ub9ac\uace0 \uc8fc\uc694 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uc774\ud574\ud558\ub294\ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "A. \ud559\uc2b5 \uc124\uc815"}, {"content": "| Language | StarCoder2Data (B tokens) | Stack-Edu (B tokens) |\n|---|---|---|\n| Python | 50.6 | 21.8 |\n| Cpp | 69.7 | 16.0 |\n| Markdown | 80.4 | 14.0 |\n| C | 38.4 | 11.1 |\n| JavaScript | 45.3 | 11.1 |\n| Java | 45.6 | 42.1 |\n| SQL | 13.7 | 9.62 |\n| PHP | 44.9 | 9.07 |\n| C-Sharp | 33.4 | 8.87 |\n| TypeScript | 12.2 | 3.03 |\n| Shell | 4.17 | 3.13 |\n| Swift | 3.71 | 1.83 |\n| Go | 3.67 | 1.80 |\n| Rust | 3.39 | 1.75 |\n| Ruby | 5.76 | 1.61 |", "caption": "Table 7: Stack-Edu dataset statistics across programming languages. The table shows the original dataset size (from StarCoder2Data) and filtered Stack-Edu size for each programming language.", "description": "\ud45c 7\uc740 Stack-Edu \ub370\uc774\ud130\uc14b\uc758 \uac01 \ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\ubcc4 \ud1b5\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. StarCoder2Data\uc5d0\uc11c \uac00\uc838\uc628 \uc6d0\ubcf8 \ub370\uc774\ud130\uc14b \ud06c\uae30\uc640 \ud544\ud130\ub9c1\ub41c Stack-Edu \ub370\uc774\ud130\uc14b \ud06c\uae30\ub97c \uac01 \ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\ubcc4\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, StarCoder2Data\uc758 \uc6d0\ubcf8 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uad50\uc721\uc801\uc778 \uce21\uba74\uc744 \uace0\ub824\ud558\uc5ec \ud544\ud130\ub9c1\uc744 \uac70\uce5c \ud6c4 \ub0a8\uc740 \ub370\uc774\ud130\uc758 \ud06c\uae30\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 Stack-Edu \ub370\uc774\ud130\uc14b\uc774 \uc5b4\ub5bb\uac8c \uad6c\uc131\ub418\uc5c8\ub294\uc9c0, \uac01 \ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\ubcc4 \ub370\uc774\ud130 \uc591\uc774 \uc5bc\ub9c8\ub098 \ub418\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.4. Code data"}, {"content": "|               | Stage 1 | Stage 2 | Stage 3 | Stage 4 |\n|---------------|---------|---------|---------|---------|\n| Tokens        | 0-6T     | 6-8T     | 8-10T    | 10-11T   |\n| MMLU (MCF)    | 29.62    | 37.96    | 42.54    | **48.87** |\n| HellaSwag     | 66.17    | 65.29    | 66.29    | **69.26** |\n| ARC           | 59.95    | 60.08    | 58.66    | **60.99** |\n| OpenBookQA    | 42.00    | 42.40    | 41.40    | **43.60** |\n| WinoGrande    | 58.88    | 58.33    | 58.64    | **61.09** |\n| PIQA          | 76.39    | 76.50    | 77.26    | **77.64** |\n| GSM8K         | 4.32     | 4.62     | 10.01    | **32.60** |\n| MATH          | 2.1      | 2.78     | 4.52     | **11.54** |\n| HumanEval     | 10.97    | 9.15     | 17.68    | **22.60** |\n| Multiple-E Java | 5.70     | 10.12    | 14.56    | **23.42** |\n| Multiple-E JS  | 9.94     | 12.42    | 18.01    | **23.60** |\n| CoQA          | 33.43    | 33.98    | 38.82    | **40.45** |\n| DROP          | 13.69    | 11.36    | 17.19    | **19.22** |\n| Jeopardy      | 23.1     | 22.4     | **25.54** | 23.35    |\n| SQuAD v2      | 55.97    | 57.45    | 57.26    | **61.48** |", "caption": "Table 8: Per-benchmark model performance across training stages. Stages 1-3 are during stable phase (no learning rate decay).", "description": "\ud45c 8\uc740 SmolLM2\uc758 \uc0ac\uc804 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uac01 \ub2e8\uacc4\ubcc4 \ubaa8\ub378 \uc131\ub2a5\uc744 \ubca4\uce58\ub9c8\ud06c\ubcc4\ub85c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ucd1d 4\ub2e8\uacc4\ub85c \uad6c\uc131\ub41c \uc0ac\uc804 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uac01 \ub2e8\uacc4\ub294 \ud2b9\uc815 \ud1a0\ud070 \uc218\ub9cc\ud07c \ud559\uc2b5\uc774 \uc9c4\ud589\ub418\uba70, 1~3\ub2e8\uacc4\ub294 \ud559\uc2b5\ub960 \uac10\uc18c\uac00 \uc5c6\ub294 \uc548\uc815\uc801\uc778 \ub2e8\uacc4\uc774\uace0, 4\ub2e8\uacc4\ub294 \ud559\uc2b5\ub960\uc744 \uc120\ud615\uc801\uc73c\ub85c \uac10\uc18c\uc2dc\ud0a4\ub294 \ub2e8\uacc4\uc785\ub2c8\ub2e4. \ud45c\uc5d0\ub294 MMLU, HellaSwag, ARC, OpenBookQA, Winogrande, PIQA, GSM8K, MATH, HumanEval, Multiple-E Java, Multiple-E JS, COQA, DROP, Jeopardy, SQUAD v2 \ub4f1 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ubaa8\ub378 \uc131\ub2a5\uc774 \ud1a0\ud070 \uc218(0-6T, 6-8T, 8-10T, 10-11T)\uc5d0 \ub530\ub77c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uac01 \ub2e8\uacc4\ubcc4 \ud559\uc2b5\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uacfc \ubca4\uce58\ub9c8\ud06c\ubcc4 \uc131\ub2a5 \ubcc0\ud654\ub97c \uc885\ud569\uc801\uc73c\ub85c \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Pretraining"}, {"content": "| Dataset source | Number of samples in SmolTalk |\n|---|---| \n| _New datasets_ |  | \n| MagPie-Ultra | 431k |\n| Smol-Rewrite | 56.2k |\n| Smol-Constraints | 36.2k |\n| Smol-Summarization | 101k |\n| _Math data_ |  | \n| NuminaMath-CoT | 112k |\n| MetaMathQA | 50k |\n| _Other_ |  | \n| Self-OSS-Starcoder2-Instruct | 50.7k |\n| APIGen-Function-Calling | 87.5k |\n| SystemChats2.0 | 35.9k |\n| LongAlign | 3.73k |\n| Everyday-Conversations | 2.38k |\n| Explore-Instruct-Rewriting | 32k |\n| OpenHermes2.5 | 100k |\n| **Total** | **1.1M** |", "caption": "Table 9: Composition of the SmolTalk dataset. The total dataset contains 1.1M instruction-response pairs from different data sources.", "description": "\ud45c 9\ub294 SmolTalk \ub370\uc774\ud130\uc14b\uc758 \uad6c\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  SmolTalk \ub370\uc774\ud130\uc14b\uc740 \ub2e4\uc591\ud55c \ucd9c\ucc98\uc5d0\uc11c \uac00\uc838\uc628 110\ub9cc \uac1c\uc758 instruction-response \uc30d\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \ub370\uc774\ud130 \uc18c\uc2a4(MagPie-Ultra, SmolTalk\uc758 \uc0c8 \ud558\uc704 \ub370\uc774\ud130\uc14b\ub4e4, \uc218\ud559 \ub370\uc774\ud130, \uae30\ud0c0 \ub370\uc774\ud130)\uc5d0\uc11c \uac00\uc838\uc628 \uc0d8\ud50c \uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130 \uc18c\uc2a4\ub294 \ud2b9\uc815\ud55c \uc720\ud615\uc758 \uc9c0\uc2dc\uc0ac\ud56d \ub610\ub294 \uc791\uc5c5(\uc608: \ub300\ud654, \uc694\uc57d, \ucf54\ub4dc \uc0dd\uc131)\uc5d0 \ud2b9\ud654\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 SmolTalk \ub370\uc774\ud130\uc14b\uc758 \ub2e4\uc591\uc131\uacfc \uaddc\ubaa8\ub97c \ubcf4\uc5ec\uc8fc\uba70, \uc774\ub7ec\ud55c \ub2e4\uc591\uc131\uc774 \ubaa8\ub378\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud588\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "5. Post-training"}, {"content": "| Dataset | IFEval | MTB | GSM8K | MATH | ARC-C | MMLU-Pro |\n|---|---|---|---|---|---|---|\n| Instruction datasets comparison ||||||||||\n| OpenHermes | 30.01 | 1.02 | 42.91 | 12.76 | 40.27 | 20.32 |\n| UltraChat | 27.26 | 4.66 | 30.40 | 9.06 | 41.21 | 15.79 |\n| MagPie-Pro | 30.45 | 4.31 | 14.56 | 6.64 | 36.01 | 12.19 |\n| MagPie-Pro-MT | 31.66 | 5.40 | 20.55 | 7.84 | 36.69 | 11.97 |\n| MagPie-Ultra | 35.49 | 5.22 | 24.34 | 13.56 | 37.71 | 12.01 |\n| MagPie-Ultra+ | 48.16 | 5.28 | 19.94 | 12.74 | 38.91 | 12.43 |\n| Math datasets comparison ||||||||||\n| MagPie-Ultra+ MathInstruct | 47.05 | 5.43 | 30.1 | 14.0 | 38.99 | 13.65 |\n| MagPie-Ultra+ MetaMathQA | 44.98 | 5.02 | 47.08 | 17.56 | 36.77 | 12.18 |\n| MagPie-Ultra+ NuminaMath-CoT | 46.27 | 5.99 | 25.32 | 18.00 | 37.88 | 12.58 |\n| Full SmolTalk ||||||||||\n| SmolTalk | 46.67 | 5.49 | 43.75 | 18.60 | 40.02 | 18.19 |\n| SmolLM2-SFT\u2020 | 57.09 | 6.11 | 47.54 | 19.64 | 42.49 | 19.06 |", "caption": "Table 10: Performance on instruction-tuning datasets. MagPie-Ultra+absent\\overset{+}{}over+ start_ARG end_ARG refers to MagPie-Ultra combined with Smol-Constraints, Smol-Rewrite, and Smol-Summarization. MagPie-Pro-MT is multi-turn while MagPie-Pro is the single turn version.\nAll comparisons were performed by fine-tuning the SmolLM2 base model on each dataset for 1 epoch. SmolLM2-SFT\u2020, the final supervised fine-tuned version of SmolLM2, was trained for 2 epochs on SmolTalk.", "description": "\ud45c 10\uc740 \ub2e4\uc591\ud55c instruction-tuning \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MagPie-Ultra+\ub294 MagPie-Ultra\uc5d0 Smol-Constraints, Smol-Rewrite, Smol-Summarization \ub370\uc774\ud130\uc14b\uc744 \uacb0\ud569\ud55c \uac83\uc785\ub2c8\ub2e4. MagPie-Pro-MT\ub294 \uba40\ud2f0\ud134, MagPie-Pro\ub294 \uc2f1\uae00\ud134 \ubc84\uc804\uc785\ub2c8\ub2e4. \ubaa8\ub4e0 \ube44\uad50\ub294 SmolLM2 \uae30\ubcf8 \ubaa8\ub378\uc744 \uac01 \ub370\uc774\ud130\uc14b\uc73c\ub85c 1 epoch \ub3d9\uc548 fine-tuning\ud558\uc5ec \uc218\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4. SmolLM2-SFT\u2020\ub294 \ucd5c\uc885\uc801\uc73c\ub85c SmolTalk\ub85c 2 epoch \ud559\uc2b5\ub41c SmolLM2\uc758 \ucd5c\uc885 supervised fine-tuned \ubc84\uc804\uc785\ub2c8\ub2e4.", "section": "5. Post-training"}, {"content": "| Metric | SmolLM2-1.7B | Llama3.2-1B | Qwen2.5-1.5B |\n|---|---|---|---| \n| Average-Real | 31.67 | 35.56 | **38.76** |\n| Average-All | 32.61 | 39.61 | **44.40** |\n| Recall | 36.38 | 55.81 | **66.94** |\n| RAG | 47.17 | 42.13 | **47.54** |\n| ICL | 23.20 | 51.20 | **52.00** |\n| Re-rank | 23.31 | 26.93 | **29.29** |\n| LongQA | **33.00** | 21.99 | 26.23 |", "caption": "Table 11: Evaluation results of the base models on the HELMET benchmark using 8k maximum input length.", "description": "\ud45c 11\uc740 \ucd5c\ub300 8000 \ud1a0\ud070\uc758 \uc785\ub825 \uae38\uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec HELMET \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uae30\ubcf8 \ubaa8\ub378\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  HELMET \ubca4\uce58\ub9c8\ud06c\ub294 \ub2e4\uc591\ud55c \uc885\ub958\uc758 \uc5b8\uc5b4 \uc774\ud574 \uc791\uc5c5\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\uba70, \ubcf8 \ud45c\uc5d0\uc11c\ub294  Average-Real, Average-All, Recall, RAG, ICL, Re-rank, LongQA \uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uc9c0\ud45c\ub97c \ud1b5\ud574 SmolLM2-1.7B, Llama3.2-1B, Qwen2.5-1.5B \uc138\uac00\uc9c0 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. \uac01 \uc9c0\ud45c\ub294 \ubaa8\ub378\uc774 \ud574\ub2f9 \uc791\uc5c5\uc744 \uc5bc\ub9c8\ub098 \uc798 \uc218\ud589\ud588\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 \uc218\uce58\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.  \uc774 \ud45c\ub97c \ud1b5\ud574 \uac01 \ubaa8\ub378\uc758 \uc7a5\ub2e8\uc810\uacfc \uc0c1\ub300\uc801\uc778 \uac15\uc810\uc744 \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "G. Long context evaluations"}]