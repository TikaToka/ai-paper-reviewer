[{"figure_path": "https://arxiv.org/html/2501.10021/x1.png", "caption": "Figure 1: \nWe leverage a pretrained diffusion UNet backbone for controlled human image animation, enabling expressive dynamic details and precise motion control. Specifically, we introduce a dynamics adapter D\ud835\udc37Ditalic_D that seamlessly integrates the reference image context as a trainable residual to the spatial attention, in parallel with the denoising process, while preserving the original spatial and temporal attention mechanisms within the UNet. In addition to body pose control via a ControlNet CPsubscript\ud835\udc36\ud835\udc43C_{P}italic_C start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT , we introduce a local face control module CFsubscript\ud835\udc36\ud835\udc39C_{F}italic_C start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT that implicitly learns facial expression control from a synthesized cross-identity face patch. We train our model on a diverse dataset of human motion videos and natural scene videos simultaneously. Our model achieves remarkable transfer of body poses and facial expressions, as well as highly vivid and detailed dynamics for both the human and the scene.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \uc81c\uc548\ub41c X-Dyna \ubaa8\ub378\uc758 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. X-Dyna\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c \ud655\uc0b0 UNet \ubc31\ubcf8\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc81c\uc5b4\ub41c \uc0ac\ub78c \uc774\ubbf8\uc9c0 \uc560\ub2c8\uba54\uc774\uc158\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uc774 \ubaa8\ub378\uc740 \uc5ed\ub3d9\uc801\uc774\uace0 \uc815\uad50\ud55c \ub514\ud14c\uc77c\uacfc \uc815\ubc00\ud55c \ubaa8\uc158 \uc81c\uc5b4\ub97c \uac00\ub2a5\ud558\uac8c \ud569\ub2c8\ub2e4. \ud2b9\ud788, \ucc38\uc870 \uc774\ubbf8\uc9c0 \ucee8\ud14d\uc2a4\ud2b8\ub97c \uacf5\uac04\uc801 \uc5b4\ud150\uc158\uc5d0 \ud6c8\ub828 \uac00\ub2a5\ud55c \uc794\ucc28\ub85c\uc368 \ub9e4\ub044\ub7fd\uac8c \ud1b5\ud569\ud558\ub294 \uc5ed\ud560\uc744 \ud558\ub294 Dynamics Adapter(D)\ub97c \ub3c4\uc785\ud588\uc2b5\ub2c8\ub2e4.  \uc774 \uacfc\uc815\uc740 \uc7a1\uc74c \uc81c\uac70 \ud504\ub85c\uc138\uc2a4\uc640 \ubcd1\ud589\ud558\uc5ec \uc9c4\ud589\ub418\uba70, \uae30\uc874\uc758 UNet \ub0b4 \uacf5\uac04\uc801 \ubc0f \uc2dc\uac04\uc801 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc740 \uadf8\ub300\ub85c \uc720\uc9c0\ub429\ub2c8\ub2e4.  \ub610\ud55c, ControlNet(CP)\uc744 \ud1b5\ud55c \uc2e0\uccb4 \uc790\uc138 \uc81c\uc5b4 \uc678\uc5d0\ub3c4, \ud569\uc131\ub41c \uad50\ucc28 \uc2e0\uc6d0 \uc5bc\uad74 \ud328\uce58\ub85c\ubd80\ud130 \uc5bc\uad74 \ud45c\uc815 \uc81c\uc5b4\ub97c \uc554\ubb35\uc801\uc73c\ub85c \ud559\uc2b5\ud558\ub294 \ub85c\uceec \uc5bc\uad74 \uc81c\uc5b4 \ubaa8\ub4c8(CF)\uc744 \ub3c4\uc785\ud588\uc2b5\ub2c8\ub2e4.  \ubaa8\ub378\uc740 \ub2e4\uc591\ud55c \uc778\uac04 \ub3d9\uc791 \ube44\ub514\uc624\uc640 \uc790\uc5f0 \uc7a5\uba74 \ube44\ub514\uc624 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ub3d9\uc2dc\uc5d0 \ud6c8\ub828\ub429\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c, \uc2e0\uccb4 \uc790\uc138\uc640 \uc5bc\uad74 \ud45c\uc815\uc744 \ub6f0\uc5b4\ub098\uac8c \uc804\uc774\ud558\uace0, \uc0ac\ub78c\uacfc \uc7a5\uba74 \ubaa8\ub450\uc5d0 \ub9e4\uc6b0 \uc0dd\uc0dd\ud558\uace0 \uc0c1\uc138\ud55c \uc5ed\ub3d9\uc131\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.10021/x2.png", "caption": "Figure 2:  a) IP-Adapter\u00a0[50] can generate vivid texture from the reference image but fails to preserve the appearance. b) Though ReferenceNet\u00a0[16] can preserve the identity from the human reference, it generates a static background without any dynamics. c) Dynamics-Adapter provides both expressive details and consistent identities.", "description": "\uadf8\ub9bc 2\ub294 \uc138 \uac00\uc9c0 \ub2e4\ub978 \ubc29\ubc95\uc73c\ub85c \ucc38\uc870 \uc774\ubbf8\uc9c0\uc758 \uc678\uad00\uc744 \ud65c\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc5d0 \uc811\uadfc\ud558\ub294 \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a) IP-Adapter\ub294 \ucc38\uc870 \uc774\ubbf8\uc9c0\ub85c\ubd80\ud130 \uc0dd\uc0dd\ud55c \uc9c8\uac10\uc744 \uc0dd\uc131\ud558\uc9c0\ub9cc \uc678\uad00\uc744 \uc720\uc9c0\ud558\ub294 \ub370 \uc2e4\ud328\ud569\ub2c8\ub2e4. (b) ReferenceNet\uc740 \uc0ac\ub78c \ucc38\uc870 \uc774\ubbf8\uc9c0\uc758 \uc815\uccb4\uc131\uc744 \uc720\uc9c0\ud558\uc9c0\ub9cc \uc5ed\ub3d9\uc801\uc778 \ubc30\uacbd \uc5c6\uc774 \uc815\uc801\uc778 \ubc30\uacbd\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. (c) Dynamics-Adapter\ub294 \ud45c\ud604\ub825 \uc788\ub294 \uc138\ubd80 \uc815\ubcf4\uc640 \uc77c\uad00\ub41c \uc815\uccb4\uc131\uc744 \ubaa8\ub450 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3.1 Preliminary"}, {"figure_path": "https://arxiv.org/html/2501.10021/x3.png", "caption": "Figure 3: a) IP-Adapter\u00a0[50] encodes the reference image as an image CLIP embedding and injects the information into the cross-attention layers in SD as the residual. b) ReferenceNet\u00a0[16] is a trainable parallel UNet and feeds the semantic information into SD via concatenation of self-attention features. c) Dynamics-Adapter encodes the reference image with a partially shared-weight UNet. The appearance control is realized by learning a residual in the self-attention with trainable query and output linear layers. All other components share the same frozen weight with SD.", "description": "\uadf8\ub9bc 4\ub294 \uc138 \uac00\uc9c0 \ub2e4\ub978 \ubc29\uc2dd\uc73c\ub85c \ucc38\uc870 \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubaa8\ub378\uc5d0 \uc678\uad00 \uc815\ubcf4\ub97c \ud1b5\ud569\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. a) IP-Adapter\ub294 \ucc38\uc870 \uc774\ubbf8\uc9c0\ub97c CLIP \uc784\ubca0\ub529\uc73c\ub85c \uc778\ucf54\ub529\ud558\uace0, \uc774 \uc815\ubcf4\ub97c SD\uc758 \ud06c\ub85c\uc2a4 \uc5b4\ud150\uc158 \ub808\uc774\uc5b4\uc5d0 \uc794\ucc28(residual)\ub85c \uc8fc\uc785\ud569\ub2c8\ub2e4. b) ReferenceNet\uc740 \ud559\uc2b5 \uac00\ub2a5\ud55c \ubcd1\ub82c UNet\uc744 \uc0ac\uc6a9\ud558\uace0, \uc790\uccb4 \uc5b4\ud150\uc158 \ud2b9\uc9d5\uc758 \uc5f0\uacb0\uc744 \ud1b5\ud574 \uc758\ubbf8\ub860\uc801 \uc815\ubcf4\ub97c SD\uc5d0 \uacf5\uae09\ud569\ub2c8\ub2e4. c) Dynamics-Adapter\ub294 \ubd80\ubd84\uc801\uc73c\ub85c \uacf5\uc720\ub41c \uac00\uc911\uce58\uc758 UNet\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucc38\uc870 \uc774\ubbf8\uc9c0\ub97c \uc778\ucf54\ub529\ud569\ub2c8\ub2e4. \uc678\uad00 \uc81c\uc5b4\ub294 \ud559\uc2b5 \uac00\ub2a5\ud55c \uc9c8\uc758(query) \ubc0f \ucd9c\ub825 \uc120\ud615 \ub808\uc774\uc5b4\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc790\uccb4 \uc5b4\ud150\uc158 \ub0b4 \uc794\ucc28\ub97c \ud559\uc2b5\ud568\uc73c\ub85c\uc368 \uc218\ud589\ub429\ub2c8\ub2e4. \ub2e4\ub978 \ubaa8\ub4e0 \uad6c\uc131 \uc694\uc18c\ub294 SD\uc640 \ub3d9\uc77c\ud55c \uace0\uc815 \uac00\uc911\uce58\ub97c \uacf5\uc720\ud569\ub2c8\ub2e4.", "section": "3.1 Preliminary"}, {"figure_path": "https://arxiv.org/html/2501.10021/x4.png", "caption": "Figure 4: Qualitative Comparison on Human in Dynamic Scene. While existing SOTA methods struggle to generate consistent and realistic scene dynamics involving humans, our method successfully produces dynamic human-scene interactions while preserving the structure of the reference image.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \uc81c\uc548\ub41c X-Dyna \ubaa8\ub378\uacfc \uae30\uc874 \ucd5c\ucca8\ub2e8(SOTA) \ubaa8\ub378\ub4e4\uc774 \ub3d9\uc801\uc778 \ubc30\uacbd \uc18d \uc778\ubb3c\uc744 \uc560\ub2c8\uba54\uc774\uc158\ud654\ud55c \uacb0\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \uac83\uc785\ub2c8\ub2e4. \uae30\uc874 SOTA \ubaa8\ub378\ub4e4\uc740 \uc778\ubb3c\uacfc \ubc30\uacbd\uc758 \ub3d9\uc801\uc778 \uc0c1\ud638\uc791\uc6a9\uc744 \uc77c\uad00\ub418\uace0 \ud604\uc2e4\uc801\uc73c\ub85c \uc0dd\uc131\ud558\ub294 \ub370 \uc5b4\ub824\uc6c0\uc744 \uacaa\ub294 \ubc18\uba74, X-Dyna \ubaa8\ub378\uc740 \ucc38\uc870 \uc774\ubbf8\uc9c0\uc758 \uad6c\uc870\ub97c \uc720\uc9c0\ud558\uba74\uc11c \uc5ed\ub3d9\uc801\uc778 \uc778\ubb3c-\ubc30\uacbd \uc0c1\ud638\uc791\uc6a9\uc744 \uc131\uacf5\uc801\uc73c\ub85c \uc0dd\uc131\ud558\uc5ec \ub354\uc6b1 \uc0ac\uc2e4\uc801\uc774\uace0 \uc77c\uad00\uc131 \uc788\ub294 \uc560\ub2c8\uba54\uc774\uc158\uc744 \ub9cc\ub4e4\uc5b4\ub0c4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2501.10021/x5.png", "caption": "Figure 5: Qualitative Comparison on Poses and Face Expressions Control. We show each method on test cases using the same reference image and pose skeleton. For improved visualization, a zoomed-in view of the face area is also provided. Our method produces results that most closely match the ground truth and best preserve face identity.", "description": "\uadf8\ub9bc 5\ub294 \uc81c\uc548\ub41c X-Dyna \ubaa8\ub378\uacfc \uae30\uc874\uc758 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc774 \uc5bc\uad74 \ud45c\uc815\uacfc \uc790\uc138 \uc81c\uc5b4\ub97c \uc5bc\ub9c8\ub098 \uc798 \uc218\ud589\ud558\ub294\uc9c0 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub3d9\uc77c\ud55c \uae30\uc900 \uc774\ubbf8\uc9c0\uc640 \uc790\uc138 \uace8\uaca9\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac01 \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uba70, \ub354\uc6b1 \uba85\ud655\ud55c \ube44\uad50\ub97c \uc704\ud574 \uc5bc\uad74 \uc601\uc5ed\uc744 \ud655\ub300\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c, \uc81c\uc548\ub41c X-Dyna \ubaa8\ub378\uc774 \uc2e4\uc81c \uc815\ub2f5\uacfc \uac00\uc7a5 \uc720\uc0ac\ud558\uba70 \uc5bc\uad74 \uc2dd\ubcc4 \uc815\ubcf4\ub97c \uac00\uc7a5 \uc798 \ubcf4\uc874\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2. Evaluations and Comparisons"}]