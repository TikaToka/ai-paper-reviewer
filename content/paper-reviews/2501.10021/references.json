{"references": [{"fullname_first_author": "Andreas Blattmann", "paper_title": "Stable video diffusion: Scaling latent video diffusion models to large datasets", "publication_date": "2023-11-15", "reason": "This paper introduces Stable Video Diffusion, a foundational model for video generation that X-Dyna builds upon and improves with its dynamics adapter."}, {"fullname_first_author": "Di Chang", "paper_title": "MagicPose: Realistic human poses and facial expressions retargeting with identity-aware diffusion", "publication_date": "2023-00-00", "reason": "This work is highly relevant as it focuses on realistic human pose and facial expression retargeting, which is a key capability that X-Dyna also addresses and improves upon."}, {"fullname_first_author": "Li Hu", "paper_title": "Animate Anyone: Consistent and controllable image-to-video synthesis for character animation", "publication_date": "2024-00-00", "reason": "This is a highly relevant prior work that X-Dyna directly builds upon and improves by resolving shortcomings in generating dynamics and maintaining identity."}, {"fullname_first_author": "Michael Dorkenwald", "paper_title": "Stochastic image-to-video synthesis using cinns", "publication_date": "2021-00-00", "reason": "This paper introduces a method for image-to-video synthesis that is relevant to X-Dyna\u2019s goal but X-Dyna improves on its ability to generate vivid and expressive animations."}, {"fullname_first_author": "Yuwei Guo", "paper_title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning", "publication_date": "2023-07-00", "reason": "This paper is relevant to X-Dyna because it introduces AnimateDiff, a temporal module for diffusion models which X-Dyna uses and further enhances to generate high-quality video animations."}]}