[{"figure_path": "https://arxiv.org/html/2501.09756/x1.png", "caption": "Figure 1: SynthLight performs relighting on portraits using an environment map lighting. By learning to re-render synthetic human faces, our diffusion model produces realistic illumination effects on real portrait photographs, including distinct cast shadows on the neck and natural specular highlights on the skin. Despite being trained exclusively on synthetic headshot images for relighting, the model demonstrates remarkable generalization to diverse scenarios, successfully handling half-body portraits and even full-body figurines.", "description": "SynthLight\ub294 \ud658\uacbd \ub9f5 \uc870\uba85\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc778\ubb3c \uc0ac\uc9c4\uc5d0 \ub300\ud55c \uc7ac\uc870\uba85\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \ud569\uc131 \uc778\uac04 \uc5bc\uad74\uc744 \uc7ac \ub80c\ub354\ub9c1\ud558\ub294 \uac83\uc744 \ud559\uc2b5\ud568\uc73c\ub85c\uc368, \ud655\uc0b0 \ubaa8\ub378\uc740 \ubaa9 \ubd80\ubd84\uc758 \ub69c\ub837\ud55c \uadf8\ub9bc\uc790\uc640 \ud53c\ubd80\uc758 \uc790\uc5f0\uc2a4\ub7ec\uc6b4 \ubc18\uc0ac\uad11\uc744 \ud3ec\ud568\ud558\uc5ec \uc2e4\uc81c \uc778\ubb3c \uc0ac\uc9c4\uc5d0 \uc0ac\uc2e4\uc801\uc778 \uc870\uba85 \ud6a8\uacfc\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc7ac\uc870\uba85\uc744 \uc704\ud574 \ud569\uc131 \uba38\ub9ac \uc0ac\uc9c4\uc73c\ub85c\ub9cc \ud559\uc2b5\ub418\uc5c8\uc74c\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0, \uc774 \ubaa8\ub378\uc740 \ub2e4\uc591\ud55c \uc2dc\ub098\ub9ac\uc624\uc5d0 \ub300\ud574 \ub180\ub77c\uc6b4 \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\uba70, \ubc18\uc2e0 \uc0ac\uc9c4\uacfc \uc804\uc2e0 \uc870\uac01\uc0c1\uae4c\uc9c0\ub3c4 \uc131\uacf5\uc801\uc73c\ub85c \ucc98\ub9ac\ud569\ub2c8\ub2e4.", "section": "Figure 1. SynthLight performs relighting on portraits using an environment map lighting. By learning to re-render synthetic human faces, our diffusion model produces realistic illumination effects on real portrait photographs, including distinct cast shadows on the neck and natural specular highlights on the skin. Despite being trained exclusively on synthetic headshot images for relighting, the model demonstrates remarkable generalization to diverse scenarios, successfully handling half-body portraits and even full-body figurines."}, {"figure_path": "https://arxiv.org/html/2501.09756/x2.png", "caption": "Figure 2: Synthetic Faces: Subjects are rendered under various lighting conditions (details in \u00a0Sec.\u00a03.1). We show two examples, where each pair consists of a subject rendered using two different environment maps. The network is trained to re-render synthetic faces by transforming a subject rendered with one environment map into its counterpart rendered with the other environment map.", "description": "\uc774 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc758 3.1\uc808(Synthetic Data for Relighting)\uc5d0\uc11c \uc124\uba85\ud558\ub294 \ud569\uc131 \uc5bc\uad74 \ub370\uc774\ud130\uc14b\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc30d\uc740 \ub3d9\uc77c\ud55c 3D \uc5bc\uad74 \ubaa8\ub378\uc744 \ub450 \uac00\uc9c0 \ub2e4\ub978 \ud658\uacbd \ub9f5 \uc870\uba85(environment map lighting) \uc544\ub798\uc5d0\uc11c \ub80c\ub354\ub9c1\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4.  \uc2e0\uacbd\ub9dd\uc740 \ud558\ub098\uc758 \ud658\uacbd \ub9f5\uc73c\ub85c \ub80c\ub354\ub9c1\ub41c \uc774\ubbf8\uc9c0\ub97c \ub2e4\ub978 \ud658\uacbd \ub9f5\uc73c\ub85c \ub80c\ub354\ub9c1\ub41c \uc774\ubbf8\uc9c0\ub85c \ubcc0\ud658\ud558\ub294 \uc791\uc5c5\uc744 \ud559\uc2b5\ud569\ub2c8\ub2e4. \uc989, \uc870\uba85 \ubcc0\ud654\uc5d0 \ub530\ub978 \uc774\ubbf8\uc9c0\uc758 \ubcc0\ud654\ub97c \ud559\uc2b5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub294 \ucd08\uc0c1\ud654\uc758 \uc7ac\uc870\uba85(relighting)\uc744 \uc704\ud55c \ud569\uc131 \ub370\uc774\ud130\uc14b\uc744 \uc0dd\uc131\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.", "section": "3. Synthetic Data for Relighting"}, {"figure_path": "https://arxiv.org/html/2501.09756/x3.png", "caption": "Figure 3: Training pipeline of SynthLight. We first enable the relighting modeling by training the diffusion backbone with synthetic relighting tuples (Task 1, top row), detailed in Sec.\u00a03.2. To further alleviate the domain gap between synthetic and real image domain, we include a joint training of the text-to-image task (Task 2, bottom row), detailed in Sec.\u00a03.3. Our model is based on LDM [34] and is composed of a VAE and a UNet. For simplicity, VAE is omitted in the diagram.", "description": "SynthLight\uc758 \ud6c8\ub828 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4. \uadf8\ub9bc \uc0c1\ub2e8(Task 1)\uc740 3.2\uc808\uc5d0\uc11c \uc790\uc138\ud788 \uc124\uba85\ud558\ub294 \ud569\uc131 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud655\uc0b0 \ubaa8\ub378\uc758 \uae30\ubcf8 \uad6c\uc870\ub97c \ud6c8\ub828\uc2dc\ucf1c \uc7ac\uc870\uba85 \ubaa8\ub378\ub9c1\uc744 \uac00\ub2a5\ud558\uac8c \ud558\ub294 \uacfc\uc815\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud569\uc131 \uc774\ubbf8\uc9c0\uc640 \uc2e4\uc81c \uc774\ubbf8\uc9c0 \uc601\uc5ed \uac04\uc758 \ucc28\uc774\ub97c \uc904\uc774\uae30 \uc704\ud574 \uc2e4\uc81c \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uc0dd\uc131 \uc791\uc5c5(Task 2, \ud558\ub2e8)\uc744 \ucd94\uac00\ud558\uc5ec \ud568\uaed8 \ud6c8\ub828\uc2dc\ud0b5\ub2c8\ub2e4(3.3\uc808). \uc774 \ubaa8\ub378\uc740 LDM [34]\uc744 \uae30\ubc18\uc73c\ub85c \ud558\uba70 VAE\uc640 U-Net\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c\ub294 \ub2e8\uc21c\ud654\ub97c \uc704\ud574 VAE\ub294 \uc0dd\ub7b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.09756/x4.png", "caption": "Figure 4: We employ the image-conditioning classifier-free guidance during inference to proportionally balance between identity preservation, and relighting effects. The final score estimate is computed as per Eq.\u00a02.", "description": "\uc774 \uadf8\ub9bc\uc740 SynthLight \ubaa8\ub378\uc758 \ucd94\ub860 \uacfc\uc815\uc5d0\uc11c \uc774\ubbf8\uc9c0 \uc870\uac74\ud654 \ubd84\ub958\uae30 \uc5c6\ub294 \uc548\ub0b4(classifier-free guidance)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucd08\uc0c1\ud654\uc758 \uc6d0\ubcf8 \ud2b9\uc9d5\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uc870\uba85 \ud6a8\uacfc\ub97c \uc870\uc808\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, \uc785\ub825 \uc774\ubbf8\uc9c0\uc640 \uc870\uba85 \ud658\uacbd \ub9f5\uc744 \ub124\ud2b8\uc6cc\ud06c\uc5d0 \uc785\ub825\ud558\uc5ec \ucd08\uc0c1\ud654\uc758 \uc7ac\uc870\uba85 \uacb0\uacfc\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uc5ec\uae30\uc11c, \uc774\ubbf8\uc9c0 \uc870\uac74\ud654 \ubd84\ub958\uae30 \uc5c6\ub294 \uc548\ub0b4\ub294 \uc785\ub825 \uc774\ubbf8\uc9c0\uc758 \uc138\ubd80 \uc815\ubcf4\ub97c \ubcf4\uc874\ud558\uba74\uc11c \uc870\uba85 \ud6a8\uacfc\uc758 \uac15\ub3c4\ub97c \uc870\uc808\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \uadf8\ub9bc\uc740 \uc785\ub825 \uc774\ubbf8\uc9c0, \uc870\uba85 \ub9f5, \uadf8\ub9ac\uace0 \ub2e4\uc591\ud55c \uc548\ub0b4 \ube44\uc728(guidance scale, \u03bb\u2081)\uc5d0 \ub530\ub978 \uc7ac\uc870\uba85 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90c\uc73c\ub85c\uc368 \uc548\ub0b4 \ube44\uc728\uc758 \ubcc0\ud654\uac00 \ucd08\uc0c1\ud654\uc758 \ub514\ud14c\uc77c \ubcf4\uc874\uacfc \uc870\uba85 \ud6a8\uacfc \uc0ac\uc774\uc758 \uade0\ud615\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ucd5c\uc885 \uc810\uc218 \uacc4\uc0b0\uc740 \ubcf8\ubb38\uc758 \uc2dd (2)\uc5d0 \ub530\ub77c \uc774\ub8e8\uc5b4\uc9d1\ub2c8\ub2e4.", "section": "3.4 Inference Time Adaptation"}]