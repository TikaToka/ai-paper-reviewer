[{"figure_path": "https://arxiv.org/html/2501.07783/x1.png", "caption": "Figure 1: Different multi-resolution designs in visual perception and multimodal understanding.\n(a)(e) Plain network without multi-scale features.\n(b)(c)(f) Inefficient image pyramid networks using equivalently large models for all scales, either with shared weights or with separate weights and interactions.\n(d) Parameter-direct image pyramid network which processes high-resolution images with large models, leading to high computational cost.\n(g) Multi-resolution approaches on multimodal tasks based on grid partition.\n(h) Our efficient and effective parameter-inverted image pyramid network (PIIP), which pairs models of increasing parameter sizes inversely with images of decreasing resolution. It achieves better performance with much lower computational cost.", "description": "\uadf8\ub9bc 1\uc740 \uc2dc\uac01\uc801 \uc778\uc2dd\uacfc \ub2e4\uc911 \ubaa8\ub4dc \uc774\ud574\uc5d0\uc11c \ub2e4\uc591\ud55c \ub2e4\uc911 \ud574\uc0c1\ub3c4 \uc124\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\uc640 (e)\ub294 \ub2e4\uc911 \uc2a4\ucf00\uc77c \ud2b9\uc9d5\uc774 \uc5c6\ub294 \uc77c\ubc18\uc801\uc778 \ub124\ud2b8\uc6cc\ud06c\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. (b), (c), (f)\ub294 \ubaa8\ub4e0 \uc2a4\ucf00\uc77c\uc5d0\uc11c \ub3d9\uc77c\ud55c \ud06c\uae30\uc758 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\ub294 \ube44\ud6a8\uc728\uc801\uc778 \uc774\ubbf8\uc9c0 \ud53c\ub77c\ubbf8\ub4dc \ub124\ud2b8\uc6cc\ud06c\ub85c, \uac00\uc911\uce58 \uacf5\uc720 \ub610\ub294 \uac1c\ubcc4 \uac00\uc911\uce58\uc640 \uc0c1\ud638 \uc791\uc6a9\uc744 \ud1b5\ud574 \uad6c\ud604\ub429\ub2c8\ub2e4. (d)\ub294 \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\ub97c \ud070 \ubaa8\ub378\ub85c \ucc98\ub9ac\ud558\uc5ec \ub192\uc740 \uacc4\uc0b0 \ube44\uc6a9\uc774 \ub4dc\ub294 \ub9e4\uac1c\ubcc0\uc218 \uc9c1\uc811 \uc774\ubbf8\uc9c0 \ud53c\ub77c\ubbf8\ub4dc \ub124\ud2b8\uc6cc\ud06c\uc785\ub2c8\ub2e4. (g)\ub294 \uadf8\ub9ac\ub4dc \ubd84\ud560\uc744 \uae30\ubc18\uc73c\ub85c \ub2e4\uc911 \ubaa8\ub4dc \uc791\uc5c5\uc5d0 \ub300\ud55c \ub2e4\uc911 \ud574\uc0c1\ub3c4 \uc811\uadfc \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (h)\ub294 \uc81c\uc548\ub41c PIIP(Parameter-Inverted Image Pyramid Network)\ub85c, \uac10\uc18c\ud558\ub294 \ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0\uc640 \ubc18\ub300\ub85c \uc99d\uac00\ud558\ub294 \ub9e4\uac1c\ubcc0\uc218 \ud06c\uae30\uc758 \ubaa8\ub378\uc744 \uc30d\uc73c\ub85c \ubb36\uc5b4 \ud6e8\uc52c \uc801\uc740 \uacc4\uc0b0 \ube44\uc6a9\uc73c\ub85c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.07783/x2.png", "caption": "Figure 2: Overall architecture of PIIP. We use multi-resolution branches to process images of different resolutions, where larger images are handled by smaller models. Each branch leverages pretrained ViTs or CNNs. Interaction units build connections between adjacent branches. Branch merging is inserted after all the blocks or within certain intermediate blocks to combine the features of all branches.", "description": "\uadf8\ub9bc 2\ub294 PIIP(Parameter-Inverted Image Pyramid Networks)\uc758 \uc804\uccb4 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. PIIP\ub294 \ub2e4\uc591\ud55c \ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0\ub97c \ucc98\ub9ac\ud558\uae30 \uc704\ud574 \ub2e4\uc911 \ud574\uc0c1\ub3c4 \ubd84\uae30\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ud070 \uc774\ubbf8\uc9c0\ub294 \uc791\uc740 \ubaa8\ub378\ub85c \ucc98\ub9ac\ub418\uc5b4 \uacc4\uc0b0 \ube44\uc6a9\uc744 \uc808\uac10\ud569\ub2c8\ub2e4. \uac01 \ubd84\uae30\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c ViT(Vision Transformer) \ub610\ub294 CNN(Convolutional Neural Network)\uc744 \ud65c\uc6a9\ud569\ub2c8\ub2e4. \uc0c1\ud638 \uc791\uc6a9 \uc720\ub2db\uc740 \uc778\uc811\ud55c \ubd84\uae30 \uac04\uc758 \uc5f0\uacb0\uc744 \uad6c\ucd95\ud558\uace0, \ubd84\uae30 \ubcd1\ud569\uc740 \ubaa8\ub4e0 \ube14\ub85d \ub4a4 \ub610\ub294 \ud2b9\uc815 \uc911\uac04 \ube14\ub85d \ub0b4\uc5d0\uc11c \ubaa8\ub4e0 \ubd84\uae30\uc758 \ud2b9\uc9d5\uc744 \uacb0\ud569\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \ud574\uc0c1\ub3c4\uc5d0\uc11c \ud6a8\uc728\uc801\uc778 \ub2e4\uc911 \uc2a4\ucf00\uc77c \ud2b9\uc9d5 \ucd94\ucd9c\uc774 \uac00\ub2a5\ud574\uc9d1\ub2c8\ub2e4.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x3.png", "caption": "Figure 3: Illustration of PIIP-LLaVA for multimodal understanding.\nWe use one projector after each branch to align the visual features with the language embedding space of the LLM, and combine the features to obtain the visual features.", "description": "\uadf8\ub9bc 3\uc740 PIIP-LLaVA\uc758 \ub2e4\uc911 \ubaa8\ub4dc \uc774\ud574\ub97c \uc704\ud55c \uad6c\uc870\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. PIIP-LLaVA\ub294 \ub2e4\uc591\ud55c \ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0\ub97c \ucc98\ub9ac\ud558\uae30 \uc704\ud574 \uc5ec\ub7ec \ubd84\uae30(branch)\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uac01 \ubd84\uae30\ub294 \ubbf8\ub9ac \ud6c8\ub828\ub41c Vision Transformer(ViT) \ub610\ub294 Convolutional Neural Network(CNN)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud569\ub2c8\ub2e4. \uac01 \ubd84\uae30\uc758 \ucd9c\ub825\uc740 LLM(Large Language Model)\uc758 \uc5b8\uc5b4 \uc784\ubca0\ub529 \uacf5\uac04\uacfc \uc815\ub82c\ud558\uae30 \uc704\ud574 \ud504\ub85c\uc81d\ud130(projector)\ub97c \uac70\uce58\uace0, \uc774\ub807\uac8c \ucc98\ub9ac\ub41c \uc2dc\uac01\uc801 \ud2b9\uc9d5\ub4e4\uc774 \uacb0\ud569\ub418\uc5b4 \ucd5c\uc885\uc801\uc778 \uc2dc\uac01\uc801 \ud2b9\uc9d5\uc774 \uc0dd\uc131\ub429\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \uc774\ubbf8\uc9c0\uc5d0\uc11c\ub3c4 \ud6a8\uacfc\uc801\uc73c\ub85c \uc2dc\uac01 \uc815\ubcf4\ub97c \ucc98\ub9ac\ud558\uace0 \uc774\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\ub294 \uc138\ubd80 \uc815\ubcf4\uc5d0 \uc9d1\uc911\ud558\uace0, \uc800\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\ub294 \uc804\uccb4\uc801\uc778 \ub9e5\ub77d\uc744 \ud30c\uc545\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \uc0c1\ud638 \ubcf4\uc644\uc801\uc778 \uc5ed\ud560\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.07783/x4.png", "caption": "Figure 4: Detailed structure of the interaction unit. It consists of two deformable attentions with fully-connect layers and feed-forward networks.", "description": "\uadf8\ub9bc 4\ub294 PIIP \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc0c1\ud638 \uc791\uc6a9 \uc720\ub2db\uc758 \uc0c1\uc138 \uad6c\uc870\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uc720\ub2db\uc740 \ub2e4\uc591\ud55c \ud574\uc0c1\ub3c4\uc758 \ud53c\ucc98 \ub9f5 \uac04\uc758 \uc815\ubcf4 \uad50\ud658\uc744 \ub2f4\ub2f9\ud558\uba70, \ub450 \uac1c\uc758 \ubcc0\ud615 \uac00\ub2a5\ud55c \uc5b4\ud150\uc158(Deformable Attention) \ubaa8\ub4c8\uacfc \uc644\uc804 \uc5f0\uacb0 \uacc4\uce35(Fully-connected layer), \uadf8\ub9ac\uace0 \ud53c\ub4dc\ud3ec\uc6cc\ub4dc \ub124\ud2b8\uc6cc\ud06c(Feed-forward network)\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \ubcc0\ud615 \uac00\ub2a5\ud55c \uc5b4\ud150\uc158\uc740 \uc785\ub825 \ud53c\ucc98 \ub9f5\uc758 \ud2b9\uc9d5\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \ud3ec\ucc29\ud558\uace0, \uc644\uc804 \uc5f0\uacb0 \uacc4\uce35\uc740 \ucc28\uc6d0 \ucd95\uc18c \ubc0f \ud2b9\uc9d5 \ucd94\ucd9c\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c \ud53c\ub4dc\ud3ec\uc6cc\ub4dc \ub124\ud2b8\uc6cc\ud06c\ub294 \ucc44\ub110 \uac04\uc758 \uc0c1\ud638 \uc791\uc6a9\uc744 \ud1b5\ud574 \ub2e4\uc591\ud55c \ud574\uc0c1\ub3c4\uc758 \ud53c\ucc98 \ub9f5\uc744 \ud1b5\ud569\ud558\uc5ec \ucd5c\uc885 \ucd9c\ub825\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacfc\uc815\uc744 \ud1b5\ud574 \ub2e4\uc591\ud55c \uc2a4\ucf00\uc77c\uc758 \uc815\ubcf4\uac00 \ud6a8\uacfc\uc801\uc73c\ub85c \uacb0\ud569\ub418\uc5b4 \ubcf4\ub2e4 \uc815\ud655\ud558\uace0 \ud48d\ubd80\ud55c \uc2dc\uac01\uc801 \ud45c\ud604\uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.07783/x5.png", "caption": "Figure 5: Detailed design of branch merging in different tasks. For detection, segmentation and multimodal understanding, output features from all branches are fused together with projection and upsampling, and fed into the subsequent FPN or LLM. For classification, we employ the original classification heads to compute logits, and average them as the final prediction.", "description": "\uadf8\ub9bc 5\ub294 \ub2e4\uc591\ud55c \uc791\uc5c5\uc5d0 \ub300\ud55c \ubd84\uae30 \ubcd1\ud569\uc758 \uc138\ubd80 \uc124\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac1d\uccb4 \ud0d0\uc9c0, \ubd84\ud560 \ubc0f \ub2e4\uc911 \ubaa8\ub4dc \uc774\ud574\uc758 \uacbd\uc6b0 \ubaa8\ub4e0 \ubd84\uae30\uc758 \ucd9c\ub825 \ud2b9\uc9d5\uc740 \ud22c\uc601 \ubc0f \uc5c5\uc0d8\ud50c\ub9c1\uc744 \ud1b5\ud574 \uacb0\ud569\ub418\uc5b4 \ud6c4\uc18d FPN \ub610\ub294 LLM\uc5d0 \uc785\ub825\ub429\ub2c8\ub2e4. \ubd84\ub958\uc758 \uacbd\uc6b0 \uc6d0\ub798 \ubd84\ub958 \ud5e4\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub85c\uc9d3\uc744 \uacc4\uc0b0\ud558\uace0 \ud3c9\uade0\uc744 \ub0b4\uc5b4 \ucd5c\uc885 \uc608\uce21\uc73c\ub85c \ud569\ub2c8\ub2e4.", "section": "III. \ubc29\ubc95\ub860"}, {"figure_path": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/interaction_types/inter_type_v4.png", "caption": "(a) Object detection", "description": "\uadf8\ub9bc 10(a)\ub294 PIIP-SBL \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud55c \uac1d\uccb4 \ud0d0\uc9c0 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. PIIP\uc758 \uace0\ud574\uc0c1\ub3c4 \ucc98\ub9ac \uae30\ub2a5 \ub355\ubd84\uc5d0 \uc791\uc740 \ubb3c\uccb4\uae4c\uc9c0 \uc815\ud655\ud558\uac8c \ud0d0\uc9c0\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uc67c\ucabd \uc0c1\ub2e8 \uc774\ubbf8\uc9c0\uc758 \uc791\uc740 \ubca4\uce58\uc640 \uc0ac\ub78c\ub4e4, \uc624\ub978\ucabd \uc0c1\ub2e8 \uc774\ubbf8\uc9c0\uc758 \uc791\uc740 \ucc28\ub7c9\ub4e4\uc744 \uc815\ud655\ud558\uac8c \uc2dd\ubcc4\ud569\ub2c8\ub2e4. \uc774\ub294 PIIP\uac00 \uc2dc\uac01\uc801 \uc778\uc2dd \uc791\uc5c5\uc5d0\uc11c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "IV. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2501.07783/x6.png", "caption": "(b) Instance segmentation", "description": "\uc774 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc758 \uc2e4\ud5d8 \uacb0\uacfc \uc911 \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560(Instance Segmentation) \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 \ubaa8\ub378\uc774 \uac1d\uccb4\uc758 \uacbd\uacc4\ub97c \uc815\ud655\ud558\uac8c \uc2dd\ubcc4\ud558\uace0 \ubd84\ud560\ud558\ub294 \ub2a5\ub825\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \uc608\uc2dc \uc774\ubbf8\uc9c0\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uc774\ubbf8\uc9c0\ub294 \ubaa8\ub378\uc774 \uc608\uce21\ud55c \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560 \ub9c8\uc2a4\ud06c\uc640 \ud568\uaed8 \ud45c\uc2dc\ub418\uc5b4, \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ud3c9\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0 \ucc98\ub9ac \ub2a5\ub825\uc744 \uac15\uc870\ud558\uba70, \uc791\uc740 \ubb3c\uccb4\uae4c\uc9c0\ub3c4 \uc815\ud655\ud558\uac8c \ubd84\ud560\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "IV. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2501.07783/x7.png", "caption": "Figure 6: Performance of different PIIP variants by adjusting input resolutions on object detection and instance segmentation.", "description": "\uadf8\ub9bc 6\uc740 PIIP\uc758 \uc5ec\ub7ec \ubcc0\ud615 \ubaa8\ub378\ub4e4\uc774 \uac1d\uccb4 \ud0d0\uc9c0 \ubc0f \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560 \uc791\uc5c5\uc5d0\uc11c \uc785\ub825 \ud574\uc0c1\ub3c4 \uc870\uc815\uc5d0 \ub530\ub978 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubcc0\ud615 \ubaa8\ub378\uc740 \uc11c\ub85c \ub2e4\ub978 \ud06c\uae30\uc758 \ubaa8\ub378\ub4e4\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0\ub4e4\uc744 \ucc98\ub9ac\ud569\ub2c8\ub2e4.  x\ucd95\uc740 GFLOPs(\ucd08\ub2f9 \ubd80\ub3d9 \uc18c\uc218\uc810 \uc5f0\uc0b0 \ud69f\uc218)\ub85c \ubaa8\ub378\uc758 \uacc4\uc0b0 \ube44\uc6a9\uc744 \ub098\ud0c0\ub0b4\uace0, y\ucd95\uc740 AP(\ud3c9\uade0 \uc815\ubc00\ub3c4)\ub85c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uac1d\uccb4 \ud0d0\uc9c0\uc640 \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560 \ubaa8\ub450\uc5d0\uc11c PIIP \ubcc0\ud615 \ubaa8\ub378\ub4e4\uc774 \uae30\uc900 \ubaa8\ub378\ubcf4\ub2e4 \ub0ae\uc740 \uacc4\uc0b0 \ube44\uc6a9\uc73c\ub85c \ub354 \ub192\uc740 \uc131\ub2a5\uc744 \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 PIIP\uc758 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc744 \uc785\uc99d\ud569\ub2c8\ub2e4. \ud2b9\ud788 \uace0\ud574\uc0c1\ub3c4 \uc785\ub825\uc5d0 \ub300\ud574 PIIP-TSBL \ubaa8\ub378\uc774 \uac00\uc7a5 \ud6a8\uacfc\uc801\uc778 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "IV. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2501.07783/x8.png", "caption": "TABLE V: Experiments of initializing with different pre-trained weights on COCO val2017 with PIIP-SBL 1568/1120/672.", "description": "\ud45c V\ub294 PIIP-SBL(Parameter-Inverted Image Pyramid Networks, \ud574\uc0c1\ub3c4 1568/1120/672) \ubaa8\ub378\uc5d0 \ub300\ud574 \ub2e4\uc591\ud55c \uc0ac\uc804 \ud6c8\ub828\ub41c \uac00\uc911\uce58\ub97c \uc0ac\uc6a9\ud558\uc5ec COCO val2017 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc2e4\ud5d8\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01\uac01 \ub2e4\ub978 \uc0ac\uc804 \ud6c8\ub828 \ubc29\ubc95(\uc608: AugReg, DeiT III, MAE, Uni-Perceiver, DINOv2, BEiTv2)\uc73c\ub85c \ud559\uc2b5\ub41c ViT-S \ubc0f ViT-B \ubaa8\ub378\uc744 PIIP-SBL\uc758 \uc5ec\ub7ec \ube0c\ub79c\uce58\uc5d0 \uc801\uc6a9\ud558\uc5ec \uc131\ub2a5 \ubcc0\ud654\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \uc885\ub958\uac00 PIIP-SBL \ubaa8\ub378\uc758 \uac1d\uccb4 \ud0d0\uc9c0 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.  APb\uc640 APm \uc9c0\ud45c\ub294 \uac01\uac01 bounding box\uc640 mask\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc815\ubc00\ub3c4(Average Precision)\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "IV. \uc2e4\ud5d8"}]