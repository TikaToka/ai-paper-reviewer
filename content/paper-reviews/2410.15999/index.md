---
title: "Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering"
summary: "SPARE, a novel training-free method, uses sparse autoencoders to precisely control LLMs' knowledge selection, significantly improving accuracy in resolving knowledge conflicts."
categories: ["AI Generated"]
tags: ["ðŸ”– 24-10-21", "ðŸ¤— 24-10-25"]
showSummary: true
date: 2024-10-21
draft: false
---

### TL;DR


{{< lead >}}

Large language models (LLMs) sometimes struggle with knowledge conflicts, where their internal knowledge contradicts information in the given context.  This paper introduces SPARE, a novel method that addresses this issue without requiring any retraining of the LLM. SPARE leverages sparse autoencoders (SAEs) to identify and modify internal LLM activations that influence knowledge selection. SAEs decompose complex LLM activations into simpler, more interpretable features. By carefully manipulating these features, SPARE can guide the LLM to prioritize either the contextual information or its internal knowledge, effectively resolving conflicts. Experiments on question-answering tasks show that SPARE significantly outperforms existing methods in accuracy, demonstrating the effectiveness of this approach in resolving knowledge conflicts and enhancing LLM performance.  The method is highly efficient, operating at inference time without the need for model retraining. This makes SPARE a practical solution for improving LLM performance in real-world applications where knowledge conflicts are common.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.15999" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}

#### Why does it matter?
To summarize the research paper on steering knowledge selection behaviours in LLMs using SAE-based representation engineering.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} SPARE effectively steers LLM knowledge selection between contextual and parametric knowledge using sparse autoencoders. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The method surpasses existing representation engineering and contrastive decoding methods in accuracy. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} SPARE operates at inference time, making it efficient for practical applications. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](figures/figures_1_0.png "ðŸ”¼ Figure 1: In the event of a knowledge conflict, the model can rely on the context or on the parametric knowledge. The figure presents the predictions of Llama2-7B steered by SPARE.")

> The figure shows two examples of how SPARE steers Llama2-7B to use either context or memory when there is a knowledge conflict.





![](charts/charts_3_0.png "ðŸ”¼ Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.")

> The chart displays the AUROC scores achieved when probing different layers of Llama2-7B and Gemma2-9B models to detect knowledge conflicts.





{{< table-caption caption="ðŸ”½ Table 1: Overall performance of steering the utilisation of parametric and contextual knowledge, measured by EMM and EMC. 'Without Controlling' indicates the baseline that we do not use any controlling methods to steer the generation. #ICL is not an inference-time controlling strategy, which controls the behaviours by changing demonstrations. CAD needs additional forwarding for contrastive decoding." >}}
<table id='0' style='font-size:14px'><tr><td rowspan="2">Metric</td><td rowspan="2">Method</td><td colspan="3">NQSwap (Longpre et al., 2021)</td><td colspan="3">Macnoise (Hong et al., 2024)</td></tr><tr><td>Llama3-8B</td><td>Llama2-7B</td><td>Gemma-2-9B</td><td>Llama3-8B</td><td>Llama2-7B</td><td>Gemma-2-9B</td></tr><tr><td colspan="8">Steer to Use Parametric Knowledge</td></tr><tr><td></td><td>Without Controlling</td><td>26.63Â±6.02</td><td>22.23Â±4.75</td><td>26.32Â±1.80</td><td>18.96Â±2.65</td><td>22.37Â±1.89</td><td>17.06Â±3.79</td></tr><tr><td rowspan="8">EMM</td><td>TaskVec (Hendel et al., 2023)</td><td>24.16Â±6.58</td><td>24.88Â±0.85</td><td>29.85Â±0.83</td><td>21.23Â±1.89</td><td>22.93Â±2.31</td><td>28.92Â±1.19</td></tr><tr><td>ActAdd (Turner et al., 2023a)</td><td>37.87 Â±8.96</td><td>31.43Â±3.68</td><td>27.67 Â±0.82</td><td>26.17 Â±0.22</td><td>27.52Â±3.07</td><td>29.75Â±1.68</td></tr><tr><td>SEAlinear (Qiu et al., 2024)</td><td>21.03Â±1.83</td><td>23.73Â±0.86</td><td>24.43Â±0.91</td><td>12.84Â±0.18</td><td>15.64Â±0.24</td><td>28.10Â±2.78</td></tr><tr><td>SEAsqExp (Qiu et al., 2024)</td><td>13.64Â±1.62</td><td>16.66Â±0.55</td><td>23.79Â±1.38</td><td>14.24Â±1.45</td><td>16.24Â±1.06</td><td>28.07Â±1.30</td></tr><tr><td>DoLa (Chuang et al., 2024)</td><td>25.53Â±5.19</td><td>16.50Â±3.91</td><td>20.58Â±1.06</td><td>16.52Â±2.65</td><td>15.66Â±0.88</td><td>19.81Â±2.58</td></tr><tr><td>â™­CAD (Shi et al., 2024)</td><td>33.72Â±0.84</td><td>31.23Â±1.45</td><td>41.17 Â±0.59</td><td>28.58Â±0.75</td><td>30.81Â±0.94</td><td>33.15 Â±2.87</td></tr><tr><td>#ICL (Brown, 2020)</td><td>43.73 å£«1.55</td><td>31.67. å£«5.49</td><td>43.10 å£«3.63</td><td>29.54+4.16</td><td>31.23 Â±0.94</td><td>21.91Â±2.35</td></tr><tr><td>SPARE (Ours)</td><td>47.51Â±1.30</td><td>43.76Â±3.14</td><td>44.11Â±1.30</td><td>30.72Â±1.42</td><td>35.43Â±1.10</td><td>35.53Â±2.07</td></tr><tr><td colspan="8">Steer to Use Contextual Knowledge</td></tr><tr><td></td><td>Without Controlling</td><td>42.69Â±8.40</td><td>41.67 å£«4.66</td><td>45.96Â±2.48</td><td>69.36Â±3.57</td><td>62.38Â±3.05</td><td>59.25Â±2.82</td></tr><tr><td rowspan="8">EMC</td><td>TaskVec (Hendel et al., 2023)</td><td>41.88Â±9.45</td><td>38.25Â±1.23</td><td>45.52Â±1.06</td><td>88.47Â±1.93</td><td>86.91Â±0.44</td><td>59.25Â±1.49</td></tr><tr><td>ActAdd (Turner et al., 2023a)</td><td>51.91Â±8.03</td><td>47.48Â±3.93</td><td>46.90Â±1.89</td><td>73.01Â±1.58</td><td>69.64Â±0.20</td><td>59.66Â±2.89</td></tr><tr><td>SEAlinear (Qiu et al., 2024)</td><td>43.61Â±10.3</td><td>47.73Â±0.43</td><td>52.95Â±1.90</td><td>69.78Â±0.97</td><td>67.32Â±0.28</td><td>60.31Â±2.25</td></tr><tr><td>SEAsqExp (Qiu et al., 2024)</td><td>57.08Â±2.92</td><td>48.04Â±0.45</td><td>61.45Â±0.54</td><td>72.04Â±1.60</td><td>68.20Â±1.10</td><td>61.45Â±0.30</td></tr><tr><td>DoLa (Chuang et al., 2024)</td><td>44.29Â±8.46</td><td>33.54Â±3.38</td><td>15.90Â±10.1</td><td>68.45Â±3.83</td><td>50.95Â±5.15</td><td>23.34Â±10.5</td></tr><tr><td>â™­CAD (Shi et al., 2024)</td><td>65.65Â±5.50</td><td>54.69Â±3.25</td><td>63.10Â±2.32</td><td>78.69Â±3.85</td><td>70.07Â±3.77</td><td>64.12+4.44</td></tr><tr><td>#ICL (Brown, 2020)</td><td>73.35 Â±3.82</td><td>63.33 Â±3.50</td><td>70.19 Â±2.51</td><td>51.75Â±5.60</td><td>47.51Â±1.86</td><td>47.24Â±3.81</td></tr><tr><td>SPARE (Ours)</td><td>77.69 Â±1.24</td><td>69.32Â±1.26</td><td>73.78Â±0.74</td><td>92.24Â±0.49</td><td>87.30Â±1.96</td><td>87.96Â±1.85</td></tr></table>{{< /table-caption >}}

> Table 1 presents the overall performance comparison of different methods in steering the usage of parametric and contextual knowledge in LLMs for two datasets, measured by exact match accuracy.



### More visual insights



<details>
<summary>More on charts
</summary>


![](charts/charts_7_0.png "ðŸ”¼ Figure 4: Detailed evaluation results of controlling capability on NQSwap. We use different colours for different methods and use different shapes for different models. The upper-right area indicates a high performance for all figures. (a) presents the capability of changing the behaviour of LLMs, where x-axis and y-axis are EMCâ†’M and EMMâ†’C, measuring the capability of changing the answer from C to M and from M to C, respectively; (b) presents the capability of maintaining the behaviour when steering to the same behaviour as the original behaviour, where x-axis and y-axis are EMMâ†’M and EMCâ†’C, measuring the maintaining capability of generating M and C, respectively; (c) present the ablation analysis of SPARE, x-axis and y-axis are EMM and EMC.")

> The chart displays a multi-faceted evaluation of SPARE and other methods' abilities to control LLM behavior in knowledge conflict scenarios, assessing their capability to change and maintain knowledge selection behaviours.


![](charts/charts_8_0.png "ðŸ”¼ Figure 5: Effectiveness of SPARE on editing different layers individually.")

> The chart displays the performance of the SPARE model on the Llama3-8B and Gemma2-9B models when editing different layers individually, showing the effectiveness of controlling knowledge selection behaviour at different layers.


![](charts/charts_8_1.png "ðŸ”¼ Figure 4: Detailed evaluation results of controlling capability on NQSwap. We use different colours for different methods and use different shapes for different models. The upper-right area indicates a high performance for all figures. (a) presents the capability of changing the behaviour of LLMs, where x-axis and y-axis are EMCâ†’M and EMMâ†’C, measuring the capability of changing the answer from C to M and from M to C, respectively; (b) presents the capability of maintaining the behaviour when steering to the same behaviour as the original behaviour, where x-axis and y-axis are EMMâ†’M and EMCâ†’C, measuring the maintaining capability of generating M and C, respectively; (c) present the ablation analysis of SPARE, x-axis and y-axis are EMM and EMC.")

> Figure 4 shows the detailed evaluation results of SPARE and baselines on the capability of changing and maintaining knowledge selection behaviors in LLMs under knowledge conflicts.


![](charts/charts_8_2.png "ðŸ”¼ Figure 6: The residual stream changes after applying SPARE to Llama3-8B at the 15th layer.")

> The chart displays the AUROC and kurtosis of the residual stream in Llama3-8B model for different knowledge selection behaviors with and without SPARE applied at the 15th layer.


![](charts/charts_14_0.png "ðŸ”¼ Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.")

> The chart displays the AUROC scores for detecting knowledge conflicts in different layers of Llama2-7B and Gemma2-9B models across various activation types.


![](charts/charts_14_1.png "ðŸ”¼ Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.")

> The chart displays the AUROC (Area Under the Receiver Operating Characteristic Curve) achieved by a linear probing method to detect knowledge conflicts in LLMs across different layers and activation types.


![](charts/charts_14_2.png "ðŸ”¼ Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.")

> The chart displays the AUROC of probing results on Llama2-7B and Gemma2-9B for detecting knowledge conflict across different layers and activation types.


![](charts/charts_14_3.png "ðŸ”¼ Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.")

> The chart displays the knowledge conflict probing results for Llama2-7B and Gemma2-9B models across different activation types (hidden states, MLP, and attention) and layers, showing varying accuracy in detecting knowledge conflicts.


![](charts/charts_14_4.png "ðŸ”¼ Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.")

> The chart displays the AUROC scores for detecting knowledge conflicts in Llama2-7B and Gemma2-9B across different layers and activation types.


![](charts/charts_14_5.png "ðŸ”¼ Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.")

> The chart displays the Area Under the ROC Curve (AUROC) for detecting knowledge conflicts in different layers of Llama2-7B and Gemma2-9B models, using various activation types.


![](charts/charts_16_0.png "ðŸ”¼ Figure 10: The impact of the number of the collected hidden states N on the controlling performance.")

> The chart displays the impact of the number of collected hidden states on the performance of controlling the usage of either parametric or contextual knowledge in LLMs.


![](charts/charts_19_0.png "ðŸ”¼ Figure 11: Proportion of accumulated mutual Information (K) on Gemma2-9B")

> The chart shows the proportion of accumulated mutual information for different numbers of selected activations in three layers (23, 24, and 25) of the Gemma2-9B model.


![](charts/charts_19_1.png "ðŸ”¼ Figure 11: Proportion of accumulated mutual Information (K) on Gemma2-9B")

> The chart displays the proportion of accumulated mutual information (K) against the number of selected activations (k) for different layers (23, 24, and 25) in the Gemma2-9B model.


![](charts/charts_19_2.png "ðŸ”¼ Figure 11: Proportion of accumulated mutual Information (K) on Gemma2-9B")

> The chart displays the relationship between the proportion of accumulated mutual information and the number of selected activations for different layers in the Gemma2-9B model.


![](charts/charts_19_3.png "ðŸ”¼ Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.")

> The chart displays the results of probing experiments to detect knowledge conflicts in LLMs, showing that the signal of conflict is strongest in the middle layers across different activation types.


![](charts/charts_19_4.png "ðŸ”¼ Figure 14: Skewness of the hidden states of Llama2-7B on NQSwap.")

> The chart displays the skewness of hidden states for Llama2-7B model on the NQSwap dataset, differentiating between instances where the model uses parametric knowledge (DM) versus contextual knowledge (DC).


![](charts/charts_19_5.png "ðŸ”¼ Figure 19: L1 norm and L2 norm of the hidden states of Llama2-7B on NQSwap.")

> The chart displays the L1 and L2 norms of hidden states for Llama2-7B on the NQSwap dataset, differentiating between instances where the model uses parametric knowledge (DM) and contextual knowledge (DC).


![](charts/charts_19_6.png "ðŸ”¼ Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.")

> The chart displays the results of probing experiments to detect knowledge conflicts in Llama2-7B and Gemma2-9B models across different activation types and layers.


![](charts/charts_19_7.png "ðŸ”¼ Figure 14: Skewness of the hidden states of Llama2-7B on NQSwap.")

> The chart displays the skewness of hidden states across different layers for Llama2-7B model on NQSwap dataset, categorized by whether the model used parametric knowledge (DM) or contextual knowledge (DC) to generate the answers.


![](charts/charts_19_8.png "ðŸ”¼ Figure 14: Skewness of the hidden states of Llama2-7B on NQSwap.")

> The chart displays the skewness of the hidden states for Llama2-7B model on the NQSwap dataset, differentiating between instances where the model uses parametric knowledge (DM) versus contextual knowledge (DC).


![](charts/charts_20_0.png "ðŸ”¼ Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.")

> The chart displays the results of probing experiments to detect knowledge conflicts in LLMs, showing the Area Under the ROC Curve (AUROC) for different layers and activation types.


![](charts/charts_20_1.png "ðŸ”¼ Figure 14: Skewness of the hidden states of Llama2-7B on NQSwap.")

> The chart displays the skewness of hidden states in Llama2-7B across layers, differentiating between those selecting parametric knowledge (DM) and contextual knowledge (DC).


![](charts/charts_20_2.png "ðŸ”¼ Figure 19: L1 norm and L2 norm of the hidden states of Llama2-7B on NQSwap.")

> The chart displays the L1 and L2 norm values of the hidden states of Llama2-7B model on the NQSwap dataset, categorized by whether the model uses parametric (DM) or contextual (DC) knowledge.


![](charts/charts_20_3.png "ðŸ”¼ Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.")

> The chart displays the AUROC scores for detecting knowledge conflicts in Llama2-7B and Gemma2-9B across different layers and activation types.


![](charts/charts_20_4.png "ðŸ”¼ Figure 14: Skewness of the hidden states of Llama2-7B on NQSwap.")

> The chart displays the skewness of the hidden states of Llama2-7B across different layers when generating answers based on either parametric knowledge (DM) or contextual knowledge (DC) in the NQSwap dataset.


![](charts/charts_20_5.png "ðŸ”¼ Figure 6: The residual stream changes after applying SPARE to Llama3-8B at the 15th layer.")

> The chart displays the changes in the residual stream's probing results and skewness patterns after applying the SPARE method to the Llama3-8B model at the 15th layer, illustrating the impact of SPARE on knowledge selection.


![](charts/charts_20_6.png "ðŸ”¼ Figure 19: L1 norm and L2 norm of the hidden states of Llama2-7B on NQSwap.")

> The chart displays the L1 and L2 norms of hidden states for Llama2-7B model on the NQSwap dataset, categorized by whether the model used parametric or contextual knowledge.


![](charts/charts_20_7.png "ðŸ”¼ Figure 19: L1 norm and L2 norm of the hidden states of Llama2-7B on NQSwap.")

> The chart displays the L1 and L2 norms of the hidden states of the Llama2-7B model on the NQSwap dataset, comparing the use of parametric versus contextual knowledge.


</details>



### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}