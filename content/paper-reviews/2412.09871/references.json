{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open Foundation and Fine-tuned Chat Models", "publication_date": "2023-07-20", "reason": "This paper introduces Llama 2, a powerful open-source language model that serves as a crucial baseline for comparison and byte-ifying in the BLT paper."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The Llama 3 Herd of Models", "publication_date": "2024-12-07", "reason": "This paper details Llama 3, the most powerful open-source language model at the time of BLT's publication, serving as a key comparison point for performance and scaling trends."}, {"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training Compute-optimal Large Language Models", "publication_date": "2022-12-02", "reason": "This paper establishes the concept of compute-optimal training, which guides the training and analysis of both BLT and its baselines."}, {"fullname_first_author": "Lili Yu", "paper_title": "MegaByte: Predicting Million-byte Sequences with Multiscale Transformers", "publication_date": "2023-01-26", "reason": "This work introduces MegaByte, a decoder-only byte-level LLM that serves as a primary inspiration and comparison point for BLT."}, {"fullname_first_author": "Kevin Slagle", "paper_title": "SpaceByte: Towards Deleting Tokenization from Large Language Modeling", "publication_date": "2024-07-10", "reason": "This work builds upon MegaByte by incorporating space patching and provides a comparative study against BLT."}]}