{"references": [{"fullname_first_author": "Gregor Bachmann", "paper_title": "The pitfalls of next-token prediction", "publication_date": "2024-MM-DD", "reason": "This paper critiques the limitations of the standard next-token prediction objective in LLMs, which motivates the proposed CoCoMix method."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-MM-DD", "reason": "This foundational paper introduces the concept of few-shot learning in large language models, providing a benchmark against which CoCoMix is compared."}, {"fullname_first_author": "Trenton Bricken", "paper_title": "Towards monosemanticity: Decomposing language models with dictionary learning", "publication_date": "2023-MM-DD", "reason": "This work introduces the concept of using sparse autoencoders to extract meaningful concepts from LLMs, a key component of the CoCoMix approach."}, {"fullname_first_author": "Hoagy Cunningham", "paper_title": "Sparse autoencoders find highly interpretable features in language models", "publication_date": "2023-MM-DD", "reason": "This paper demonstrates the effectiveness of sparse autoencoders in isolating meaningful latent features in LLMs, supporting CoCoMix's use of SAEs for concept extraction."}, {"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-MM-DD", "reason": "This seminal work on knowledge distillation provides the theoretical foundation for CoCoMix's weak-to-strong supervision experiments, where knowledge from a smaller model is transferred to a larger one."}]}