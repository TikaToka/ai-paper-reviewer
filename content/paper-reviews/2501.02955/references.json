{"references": [{"fullname_first_author": "Jiaben Chen", "paper_title": "Sportsslomo: A new benchmark and baselines for human-centric video frame interpolation", "publication_date": "2024-00-00", "reason": "This paper introduces a benchmark dataset and baselines for human-centric video frame interpolation, which is directly relevant to the task of fine-grained motion understanding."}, {"fullname_first_author": "Tsai-Shien Chen", "paper_title": "Panda-70m: Captioning 70m videos with multiple cross-modality teachers", "publication_date": "2024-00-00", "reason": "This paper presents a large-scale video captioning dataset (Panda-70M) that is used in MotionBench, providing a diverse range of real-world video content for the benchmark."}, {"fullname_first_author": "Zhe Chen", "paper_title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "publication_date": "2023-12-14", "reason": "This paper introduces InternVL, a significant vision language model that serves as a foundation for many of the models compared in MotionBench, allowing for a direct comparison of different model architectures."}, {"fullname_first_author": "Wenyi Hong", "paper_title": "Cogvlm2: Visual language models for image and video understanding", "publication_date": "2024-08-06", "reason": "This paper introduces CogVLM2, a model directly compared in the MotionBench evaluation, allowing for a direct comparison of motion understanding capabilities with other models."}, {"fullname_first_author": "Ridouane Ghermi", "paper_title": "Short film dataset (sfd): A benchmark for story-level video understanding", "publication_date": "2024-06-10", "reason": "This paper introduces a benchmark focusing on story-level video understanding, which provides a comparative context for the proposed MotionBench, highlighting the difference in focus on fine-grained motion."}]}