[{"content": "| Benchmarks | #Videos | #QAs | Perception Level | Data source | Dataset Feature |\n|---|---|---|---|---|---| \n| MVBench [23] | 4,000 | 4,000 | general, motion&lt;30% | existing datasets | general |\n| TempCompass [28] | 410 | 1,580 | general, motion&lt;20% | ShutterStock | temporal concept |\n| VideoMME [8] | 900 | 2,700 | general, motion&lt;20% | Youtube | general |\n| AutoEval-Video [4] | 327 | 327 | event level | Youtube | open-ended QA |\n| EgoSchema [31] | 5,031 | 5031 | event level | ego-centric video | ego-centric |\n| LVBench [39] | 103 | 1,549 | event & story level | Youtube | long video |\n| LongVideoBench [41] | 3,763 | 6,678 | event & story level | web channels | long videos |\n| MovieChat-1K [35] | 130 | 1,950 | story level | movies | movie |\n| Short Film Dataset [9] | 1,078 | 4,885 | story level | short films | story-level |\n| MotionBench | 5,385 | 8,052 | motion level | web videos, movies, synthetic videos, datasets | motion perception |", "caption": "Table 1: The comparison of existing video VLM benchmarks with MotionBench. MotionBench collects various video sources including web videos and synthetic videos, and provides a new evaluation perspective in motion level perception.", "description": "\ud45c 1\uc740 MotionBench\ub97c \ud3ec\ud568\ud55c \uae30\uc874 \ube44\ub514\uc624 VLM \ubca4\uce58\ub9c8\ud06c\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4. MotionBench\ub294 \uc6f9 \ube44\ub514\uc624\uc640 \ud569\uc131 \ube44\ub514\uc624\ub97c \ud3ec\ud568\ud55c \ub2e4\uc591\ud55c \ube44\ub514\uc624 \uc18c\uc2a4\ub97c \uc218\uc9d1\ud558\uba70, \ub3d9\uc791 \uc218\uc900 \uc778\uc2dd\uc5d0 \ub300\ud55c \uc0c8\ub85c\uc6b4 \ud3c9\uac00 \uad00\uc810\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \ubca4\uce58\ub9c8\ud06c\uc758 \ube44\ub514\uc624 \uc218, \uc9c8\ubb38 \uc218, \uc778\uc2dd \uc218\uc900(\uc77c\ubc18, \ub3d9\uc791, \uc774\ubca4\ud2b8, \uc2a4\ud1a0\ub9ac), \ub370\uc774\ud130 \uc18c\uc2a4, \ub370\uc774\ud130 \ud2b9\uc9d5 \ub4f1\uc758 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. MotionBench\ub294 \uae30\uc874 \ubca4\uce58\ub9c8\ud06c\uc640 \ub2ec\ub9ac \ub3d9\uc791 \uc218\uc900 \uc778\uc2dd\uc5d0 \uc911\uc810\uc744 \ub450\uace0 \uc788\uc73c\uba70, \uc2e4\uc81c \uc138\uacc4\uc758 \ub2e4\uc591\ud55c \ube44\ub514\uc624 \ucf58\ud150\uce20\ub97c \ud3ed\ub113\uac8c \ub098\ud0c0\ub0b4\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "2. \uad00\ub828 \uc5f0\uad6c"}, {"content": "| Category | \n|---|---| \n| web videos, movies | \n| synthetic videos, datasets |", "caption": "Table 2: The MotionBench curation process. Categories [1-3] refer to \u201cvideos with intricate interactions\u201d, \u201cvideos from specific fields\u201d and \u201cvirtual videos\u201d, detailed in Sec.\u00a03.1. N. Vid/QA refers to the number of videos and queries in a category. min(H, W) is the minimum of the height and width of the video frames. len refers to the processed video duration. We automatically construct the queries in Virtual scenes, and manually annotate the other QA pairs in MotinBench.", "description": "\ud45c 2\ub294 MotionBench \ub370\uc774\ud130\uc14b \uc81c\uc791 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub370\uc774\ud130\uc14b\uc740 \ubcf5\uc7a1\ud55c \uc0c1\ud638\uc791\uc6a9\uc774 \ud3ec\ud568\ub41c \ube44\ub514\uc624, \ud2b9\uc815 \ubd84\uc57c\uc758 \ube44\ub514\uc624, \uadf8\ub9ac\uace0 \uac00\uc0c1 \ube44\ub514\uc624\uc758 \uc138 \uac00\uc9c0 \ubc94\uc8fc\ub85c \ub098\ub269\ub2c8\ub2e4 (\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 3.1\uc808 \ucc38\uc870).  'N. Vid/QA'\ub294 \uac01 \ubc94\uc8fc\uc5d0 \uc18d\ud55c \ube44\ub514\uc624\uc640 \uc9c8\ubb38-\ub2f5\ubcc0 \uc30d\uc758 \uac1c\uc218\ub97c \ub098\ud0c0\ub0b4\uba70, 'min(H, W)'\ub294 \ube44\ub514\uc624 \ud504\ub808\uc784\uc758 \ub192\uc774\uc640 \ub108\ube44 \uc911 \ucd5c\uc19f\uac12, 'len'\uc740 \ucc98\ub9ac\ub41c \ube44\ub514\uc624\uc758 \uc9c0\uc18d \uc2dc\uac04\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uac00\uc0c1 \ube44\ub514\uc624 \ubc94\uc8fc\uc5d0\uc11c\ub294 \uc9c8\ubb38-\ub2f5\ubcc0 \uc30d\uc774 \uc790\ub3d9\uc73c\ub85c \uc0dd\uc131\ub418\uace0, \ub098\uba38\uc9c0 \ubc94\uc8fc\uc5d0\uc11c\ub294 \uc218\ub3d9\uc73c\ub85c \uc8fc\uc11d\uc774 \ub2ec\ub838\uc2b5\ub2c8\ub2e4.", "section": "3. MotionBench: Motion-Level Benchmarking"}, {"content": "| Category | # Videos/QAs | Source | Collection | Post-process | Annotation |\n|---|---|---|---|---|---| \n| 1 | 2,355/4,922 | Pexels | Self-collected | Directly adopt | Caption & QA |\n|  |  | Pandas-70M [3] | Open-sourced | Segment with scene detection | Caption & QA |\n|  |  | Movie clips | Self-collected | Segment with scene detection | Caption & QA |\n| 2 | 2,430/2,530 | MedVid\u00a0[14] | Open-sourced | min(H,W)>448 & len\u2208[3,60]sec | QA |\n|  |  | SportsSloMo\u00a0[2] | Open-sourced | min(H,W)>448 & len\u2208[3,60]sec | QA |\n|  |  | HA-ViD\u00a0[52] | Open-sourced | min(H,W)>448 & len\u2208[3,60]sec | QA |\n| 3 | 600/600 | Virtual scenes | Self-collected | Remove renderings with occlusion | Automatic QA |", "caption": "Table 3: Evaluation results of the existing video VLMs. Abbreviations: MR (Motion Recognition), LM (Location-related Motion), CM (Camera Motion), MO (Motion-related Objects), AO (Action Order), RC (Repetition Count). We randomly split MotionBench into \u201cdev\u201d and \u201ctest\u201d. We will release the ground truth answers in the \u201cdev\u201d set and set up an online platform for results submission in the \u201ctest\u201d set.", "description": "\ud45c 3\uc740 \uae30\uc874 \ube44\ub514\uc624 VLM\ub4e4\uc758 \uc131\ub2a5 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc57d\uc5b4\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4: MR(\uc6c0\uc9c1\uc784 \uc778\uc2dd), LM(\uc704\uce58 \uad00\ub828 \uc6c0\uc9c1\uc784), CM(\uce74\uba54\ub77c \uc6c0\uc9c1\uc784), MO(\uc6c0\uc9c1\uc784 \uad00\ub828 \uac1d\uccb4), AO(\ub3d9\uc791 \uc21c\uc11c), RC(\ubc18\ubcf5 \ud69f\uc218). MotionBench\ub294 \ubb34\uc791\uc704\ub85c \"dev\" \ubc0f \"test\"\ub85c \ubd84\ud560\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \"dev\" \uc138\ud2b8\uc758 \uc815\ub2f5\uc740 \uacf5\uac1c\ub418\uba70, \"test\" \uc138\ud2b8\uc758 \uacb0\uacfc \uc81c\ucd9c\uc744 \uc704\ud55c \uc628\ub77c\uc778 \ud50c\ub7ab\ud3fc\uc774 \uad6c\ucd95\ub420 \uac83\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uae30\uc874\uc758 \ube44\ub514\uc624 \uc774\ud574 \ubaa8\ub378\ub4e4\uc774 MotionBench \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc5b4\ub5bb\uac8c \uc218\ud589\ub418\uc5c8\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc815\ub7c9\uc801\uc778 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc740 \uc5ec\uc12f \uac00\uc9c0 \uc8fc\uc694 \ub3d9\uc791 \ubc94\uc8fc\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc815\ud655\ub3c4\ub85c \uce21\uc815\ub418\uba70,  \"dev\" \ubc0f \"test\" \uc138\ud2b8\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  \uc628\ub77c\uc778 \uc81c\ucd9c \ud50c\ub7ab\ud3fc\uc740 \uc5f0\uad6c\uc790\ub4e4\uc774 \uc790\uc2e0\uc758 \ubaa8\ub378\uc744 \ud14c\uc2a4\ud2b8\ud558\uace0 MotionBench\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud560 \uc218 \uc788\ub3c4\ub85d \uc9c0\uc6d0\ud569\ub2c8\ub2e4.", "section": "5. Experiments"}, {"content": "| Model | LLM | # Frames | Dev AVG (4020) | Test AVG (4034) | MR | LM | CM | MO | AO | RC |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Random | - | - | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 |\n| *LLM: **Text*** as Input |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o [33] | - | - | 0.33 | 0.33 | 0.31 | 0.34 | 0.36 | 0.37 | 0.42 | 0.23 |\n| *Video VLMs : **Text + Multiple Frames*** as Input |  |  |  |  |  |  |  |  |  |  |\n| Gemini 1.5 Pro [34] | - | 1fps | 0.51 | 0.50 | 0.51 | 0.52 | 0.54 | 0.67 | 0.40 | 0.22 |\n| Qwen2VL-2B [36] | Qwen2 [37] | 1fps | 0.48 | 0.47 | 0.49 | 0.49 | 0.42 | 0.62 | 0.32 | 0.28 |\n| Qwen2VL-7B [36] | Qwen2 [37] | 1fps | 0.52 | 0.52 | 0.52 | 0.55 | 0.49 | 0.68 | 0.39 | 0.32 |\n| Qwen2VL-72B [36] | Qwen2 [37] | 1fps | 0.57 | **0.58** | 0.58 | **0.61** | **0.63** | 0.72 | **0.47** | 0.31 |\n| InternVL-40B [6] | NH-2-Yi-34B [32] | 8 | 0.55 | 0.54 | 0.54 | 0.58 | 0.49 | **0.76** | 0.41 | 0.30 |\n| PLLaVA-34B [44] | Yi-34B [32] | 16 | 0.52 | 0.51 | 0.55 | 0.51 | 0.47 | 0.66 | 0.38 | 0.31 |\n| CogVLM2-Video [15] | LLaMA3-8B [1] | 24 | 0.41 | 0.44 | 0.43 | 0.39 | 0.38 | 0.64 | 0.37 | 0.33 |\n| GLM-4V-plus [15] | GLM4 [10] | 30 | 0.54 | 0.55 | 0.57 | 0.57 | 0.54 | 0.69 | 0.40 | 0.37 |\n| LLaVA-NeXT [50] | Yi-34B [32] | 32 | 0.48 | 0.40 | 0.53 | 0.45 | 0.36 | 0.66 | 0.39 | 0.23 |\n| MiniCPM-V2.6 [46] | Qwen2 [37] | 64 | 0.52 | 0.53 | 0.56 | 0.49 | 0.45 | 0.72 | 0.39 | 0.33 |\n| Oryx-34B [29] | Yi-34B [32] | 64 | 0.49 | 0.49 | 0.48 | 0.52 | 0.44 | 0.65 | 0.42 | 0.32 |\n| TE Fusion (ours) | GLM4-9B [10] | 16 | **0.58** | **0.58** | **0.64** | 0.59 | 0.51 | 0.69 | 0.41 | **0.39** |", "caption": "Table 4: Benchmark results for different compression methods at various compression rates, all using the same sequence length in the VLM decoder. We set Ninputk=4subscript\ud835\udc41input\ud835\udc584\\frac{N_{\\text{input}}}{k}=4divide start_ARG italic_N start_POSTSUBSCRIPT input end_POSTSUBSCRIPT end_ARG start_ARG italic_k end_ARG = 4, with the baseline representing video models that process 4 frames without compression. Note that each compression method is re-implemented on the GLM-4V-9B backbone to ensure a fair comparison.", "description": "\ud45c 4\ub294 VLM \ub514\ucf54\ub354\uc5d0\uc11c \ub3d9\uc77c\ud55c \uc2dc\ud000\uc2a4 \uae38\uc774\ub97c \uc0ac\uc6a9\ud558\ub294 \ub2e4\uc591\ud55c \uc555\ucd95 \ube44\uc728\uc5d0\uc11c \uc5ec\ub7ec \uc555\ucd95 \ubc29\ubc95\uc5d0 \ub300\ud55c \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\uc900\uc120\uc740 \uc555\ucd95 \uc5c6\uc774 4\ud504\ub808\uc784\uc744 \ucc98\ub9ac\ud558\ub294 \ube44\ub514\uc624 \ubaa8\ub378\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uac01 \uc555\ucd95 \ubc29\ubc95\uc740 GLM-4V-9B \ubc31\ubcf8\uc5d0 \ub2e4\uc2dc \uad6c\ud604\ub418\uc5b4 \uacf5\uc815\ud55c \ube44\uad50\ub97c \ubcf4\uc7a5\ud569\ub2c8\ub2e4.  N<sub>input</sub>/k = 4\uc778 \uacbd\uc6b0\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.2. \ube44\ub514\uc624 \ud2b9\uc9d5 \uc555\ucd95 \uc2e4\ud5d8"}, {"content": "| Dev AVG |\n|---|---| \n| (4020) |", "caption": "Table 5:", "description": "\ud45c 5\ub294 \uae30\uc874 \ube44\ub514\uc624 VLM\uc758 \uc131\ub2a5 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Motion Recognition (MR), Location-related Motion (LM), Camera Motion (CM), Motion-related Objects (MO), Action Order (AO), Repetition Count (RC) \ub4f1 \uc5ec\uc12f \uac00\uc9c0 \uc8fc\uc694 \ub3d9\uc791 \uc720\ud615\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \ubaa8\ub378\ubcc4\ub85c \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uac1c\ubc1c(dev) \ubc0f \ud14c\uc2a4\ud2b8(test) \uc138\ud2b8\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc815\ud655\ub3c4, \uadf8\ub9ac\uace0 \uac01 \ub3d9\uc791 \uc720\ud615\ubcc4 \uc815\ud655\ub3c4\ub97c \uc81c\uc2dc\ud558\uc5ec \ubaa8\ub378\uc758 \ubbf8\uc138 \ub3d9\uc791 \uc774\ud574 \ub2a5\ub825\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.  \ub610\ud55c, \ube44\uad50\ub97c \uc704\ud574 \ubb34\uc791\uc704 \uae30\uc900\uc120(Random) \uc131\ub2a5\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \ubaa8\ub378 \uc774\ub984, \uc0ac\uc6a9\ub41c \uc5b8\uc5b4 \ubaa8\ub378(LLM), \uc785\ub825 \ud504\ub808\uc784 \uc218, \uadf8\ub9ac\uace0 \uac01 \ud3c9\uac00 \uc9c0\ud45c\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4 \uc218\uce58\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Test AVG |\n|---|---| \n| (4034) |", "caption": "Table 6: The model configurations of all ablated architectures.", "description": "\ud45c 6\uc740 \ub17c\ubb38\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ub41c \uc5ec\ub7ec \uac00\uc9c0 \ube44\ub514\uc624 \uc555\ucd95 \uad6c\uc870(Pre-Encoder Fusion, Post-Encoder Fusion, Through-Encoder Fusion \ub4f1)\ub97c \uc801\uc6a9\ud55c \ubaa8\ub378\ub4e4\uc758 \uc0c1\uc138 \uc124\uc815 \uac12\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ub808\uc774\uc5b4 \uc218, \ud788\ub4e0 \ud06c\uae30, \uc5b4\ud150\uc158 \ud5e4\ub4dc \uc218, \ud328\uce58 \ud06c\uae30, \uadf8\ub9ac\uace0 \ub2e4\ub978 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac12\ub4e4\uc774 \uba85\uc2dc\ub418\uc5b4 \uc788\uc5b4, \uac01 \ubaa8\ub378\uc758 \uad6c\uc870\uc801 \ucc28\uc774\uc640 \uc131\ub2a5 \ube44\uad50\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ub3d5\uc2b5\ub2c8\ub2e4.  \ud2b9\ud788, \ube44\ub514\uc624 \uc555\ucd95 \ubc29\ubc95\uc5d0 \ub530\ub978 VLM \ub514\ucf54\ub354\uc758 \uc785\ub825 \uc2dc\ud000\uc2a4 \uae38\uc774 \uc870\uc815 \ub4f1\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Model Design: Motion-Level Perception"}, {"content": "| k | Method | MotionBench | MVBench | VideoMME short | VideoMME medium | VideoMME long |\n|---|---|---|---|---|---|---|\n| 1 | baseline | 47.6 | 64.5 | 51.4 | 41.0 | 38.3 |\n| 2 | QFormer | 43.5 | 62.1 | 42.8 | 39.6 | 36.3 |\n|  | Qwen2-VL | 48.0 | 66.5 | 54.1 | 43.1 | 37.8 |\n|  | PLLaVA | 48.5 | 68.8 | 54.9 | 44.9 | 39.6 |\n|  | Kangaroo | 48.4 | 69.2 | 55.4 | 43.0 | 38.8 |\n|  | TE Fusion (ours) | 49.1 | 69.0 | 55.2 | 46.3 | 40.0 |\n| 4 | QFormer | 44.3 | 63.8 | 45.2 | 41.0 | 36.8 |\n|  | Qwen2-VL | 47.6 | 65.6 | 51.8 | 43.4 | 39.4 |\n|  | PLLaVA | 50.5 | 70.2 | 58.9 | 46.4 | 41.3 |\n|  | Kangaroo | 50.0 | 69.8 | 55.3 | 45.6 | 39.5 |\n|  | TE Fusion (ours) | 51.0 | 72.1 | 61.0 | 47.3 | 42.1 |", "caption": "Table 7: Benchmark results for different compression methods at various compression rates, all using the same sequence length in the VLM decoder. We set Ninputk=4,8subscript\ud835\udc41input\ud835\udc5848\\frac{N_{\\text{input}}}{k}=4,8divide start_ARG italic_N start_POSTSUBSCRIPT input end_POSTSUBSCRIPT end_ARG start_ARG italic_k end_ARG = 4 , 8, with the baseline representing video models that process 4 frames without compression. Note that each compression method is re-implemented on the GLM-4V-9B backbone to ensure a fair comparison.", "description": "\ud45c 7\uc740 VLM \ub514\ucf54\ub354\uc5d0\uc11c \ub3d9\uc77c\ud55c \uc2dc\ud000\uc2a4 \uae38\uc774\ub97c \uc0ac\uc6a9\ud558\ub294 \ub2e4\uc591\ud55c \uc555\ucd95 \ube44\uc728\uc5d0\uc11c\uc758 \uc11c\ub85c \ub2e4\ub978 \uc555\ucd95 \ubc29\ubc95\uc5d0 \ub300\ud55c \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\uc900\uc120\uc740 \uc555\ucd95 \uc5c6\uc774 4\ud504\ub808\uc784\uc744 \ucc98\ub9ac\ud558\ub294 \ube44\ub514\uc624 \ubaa8\ub378\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uac01 \uc555\ucd95 \ubc29\ubc95\uc740 \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574 GLM-4V-9B \ubc31\ubcf8\uc5d0 \ub2e4\uc2dc \uad6c\ud604\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  Ninput/k \uac12\uc740 4\uc640 8\ub85c \uc124\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 MotionBench, MVBench, LVBench \ubc0f VideoMME\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc9e7\uc740, \uc911\uac04 \ubc0f \uae34 \ube44\ub514\uc624 \uae38\uc774\uc5d0 \ub300\ud55c \uc131\ub2a5\uc774 \uc138\ubd84\ud654\ub418\uc5b4 \ud45c\uc2dc\ub429\ub2c8\ub2e4.", "section": "5.2. \ube44\ub514\uc624 \ud2b9\uc9d5 \uc555\ucd95 \uc2e4\ud5d8"}, {"content": "| Configurations                                  |\n|-------------------------------------------------|\n| **Configurations**                             |\n| Total steps                                    | 10,000                                     |\n| Warmup steps                                   | 1,000                                      |\n| Global batch size                              | 768                                       |\n| Learning rate                                  | 8e-6                                       |\n| Minimal learning rate                           | 1e-6                                       |\n| Learning rate decay                             | cosine                                      |\n| Optimizer                                      | Adam                                        |\n| Adam \u03f5                                         | 1e-8                                       |\n| Adam \u03b21                                        | 0.9                                        |\n| Adam \u03b22                                        | 0.95                                       |\n| Precision                                      | bf16                                       |", "caption": "Table 8: Model performance variation with respect to different compression ratios k=2,4,8,16\ud835\udc5824816k=2,4,8,16italic_k = 2 , 4 , 8 , 16, given a fixed VLM input frame count of Ninput=16subscript\ud835\udc41input16N_{\\text{input}}=16italic_N start_POSTSUBSCRIPT input end_POSTSUBSCRIPT = 16. Note that each compression method is re-implemented on the GLM-4V-9B backbone to ensure a fair comparison.", "description": "\ud45c 8\uc740 \uace0\uc815\ub41c VLM \uc785\ub825 \ud504\ub808\uc784 \uc218 (Ninput=16)\uc5d0\uc11c \ub2e4\uc591\ud55c \uc555\ucd95 \ube44\uc728(k=2, 4, 8, 16)\uc5d0 \ub530\ub978 \ubaa8\ub378 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc555\ucd95 \ubc29\ubc95\uc740 GLM-4V-9B \ubc31\ubcf8\uc744 \uc0ac\uc6a9\ud558\uc5ec \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574 \uc7ac\uad6c\ud604\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud45c\ub294 \ub2e4\uc591\ud55c \uc555\ucd95 \ube44\uc728\uc5d0\uc11c \uac01 \uc555\ucd95 \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 MotionBench, MVBench, LVBench, VideoMME \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc555\ucd95 \ube44\uc728\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ubaa8\ub378 \uc131\ub2a5\uc774 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0, \uadf8\ub9ac\uace0 \ud2b9\uc815 \uc555\ucd95 \ube44\uc728\uc774 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "5.2. Video Feature Compression \uc2e4\ud5d8"}]