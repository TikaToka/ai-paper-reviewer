{"references": [{"fullname_first_author": "Paul F. Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-12-01", "reason": "This paper is foundational to RLHF and introduces the core concept of using human preferences to train reinforcement learning agents."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This work is a seminal paper detailing the RLHF approach, including the three-stage process of supervised fine-tuning, reward modeling, and reinforcement learning, which is widely used in training large language models."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-01", "reason": "This paper presents the Proximal Policy Optimization (PPO) algorithm, a widely used reinforcement learning algorithm cited in the paper for its effectiveness in aligning models with human preferences."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-01-01", "reason": "This paper introduces Direct Preference Optimization (DPO), an alternative approach to RLHF that eliminates the need for explicit reward modeling, improving efficiency in aligning language models."}, {"fullname_first_author": "Leon Lang", "paper_title": "When your AI deceives you: Challenges with partial observability of human evaluators in reward learning", "publication_date": "2024-02-01", "reason": "This paper addresses the challenges and limitations of RLHF caused by partial human observability, which is a significant issue the current paper also tackles and expands upon."}]}