[{"figure_path": "https://arxiv.org/html/2501.14677/x2.png", "caption": "Figure 1: Our MatAnyone is capable of producing highly detailed and temporally consistent alpha mattes throughout a video.\n(a) It adapts to a variety of frame sizes and media types (e.g., films, games, smartphone videos), achieving fine-grained details at the image-matting level.\n(b) RVM\u00a0[33], an auxiliary-free video matting method, struggles with complex or ambiguous backgrounds. In contrast, our method effectively isolates the target object from such distractors, preserving a clean background and complete foreground parts.\n(c) Our method also excels at consistently tracking the target (i.e., the lady in pink) even in scenes containing multiple salient objects (i.e., the man and the lady). It accurately distinguishes between them even during their interactions.\n(Zoom-in for best view)", "description": "\uadf8\ub9bc 1\uc740 MatAnyone\uc774 \ube44\ub514\uc624 \uc804\uccb4\uc5d0\uc11c \ub9e4\uc6b0 \ub514\ud14c\uc77c\ud558\uace0 \uc2dc\uac04\uc801\uc73c\ub85c \uc77c\uad00\ub41c \uc54c\ud30c \ub9e4\ud2b8\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a) \ub2e4\uc591\ud55c \ud504\ub808\uc784 \ud06c\uae30\uc640 \ubbf8\ub514\uc5b4 \uc720\ud615(\uc608: \uc601\ud654, \uac8c\uc784, \uc2a4\ub9c8\ud2b8\ud3f0 \ube44\ub514\uc624)\uc5d0 \uc801\uc751\ud558\uc5ec \uc774\ubbf8\uc9c0 \ub9e4\ud305 \uc218\uc900\uc5d0\uc11c \ub9e4\uc6b0 \uc138\uc138\ud55c \ubd80\ubd84\uae4c\uc9c0 \ud45c\ud604\ud569\ub2c8\ub2e4. (b) \ubcf4\uc870 \uc815\ubcf4\uac00 \ud544\uc694 \uc5c6\ub294 \ube44\ub514\uc624 \ub9e4\ud305 \ubc29\ubc95\uc778 RVM [33]\uc740 \ubcf5\uc7a1\ud558\uac70\ub098 \ubaa8\ud638\ud55c \ubc30\uacbd\uc5d0\uc11c \uc5b4\ub824\uc6c0\uc744 \uacaa\uc2b5\ub2c8\ub2e4. \ubc18\uba74 MatAnyone\uc740 \uc774\ub7ec\ud55c \ubc29\ud574 \uc694\uc18c\ub85c\ubd80\ud130 \ubaa9\ud45c \uac1d\uccb4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ubd84\ub9ac\ud558\uc5ec \uae68\ub057\ud55c \ubc30\uacbd\uacfc \uc644\ubcbd\ud55c \uc804\uacbd \ubd80\ubd84\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4. (c) MatAnyone\uc740 \uc5ec\ub7ec \uac1c\uc758 \ub450\ub4dc\ub7ec\uc9c4 \uac1d\uccb4(\uc608: \ub0a8\uc131\uacfc \uc5ec\uc131)\uac00 \uc788\ub294 \uc7a5\uba74\uc5d0\uc11c\ub3c4 \ubaa9\ud45c \uac1d\uccb4(\ubd84\ud64d\uc0c9 \uc637\uc744 \uc785\uc740 \uc5ec\uc131)\ub97c \uc77c\uad00\ub418\uac8c \ucd94\uc801\ud558\ub294 \ub370 \ud0c1\uc6d4\ud569\ub2c8\ub2e4. \uc0c1\ud638 \uc791\uc6a9\ud558\ub294 \ub3d9\uc548\uc5d0\ub3c4 \uc815\ud655\ud558\uac8c \uad6c\ubd84\ud569\ub2c8\ub2e4. (\ud655\ub300\ud574\uc11c \ubcf4\uc138\uc694)", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.14677/x3.png", "caption": "Figure 2: \nDefinitions and motivations for MatAnyone.\n(a) In a matting frame, the image can be broadly divided into two areas based on the alpha value: the core (semantic) and the boundary (fine-details). The core includes the background (alpha values of 0) and the solid foreground (alpha values of 1), while the boundary (highlighted in pink) encompasses areas with alpha values between 0 and 1.\n(b) Due to the under-defined setting, auxiliary-free methods like RVM\u00a0[33] are easily confused by ambiguous background. Meanwhile, mask-guided methods like MaGGIe\u00a0[22] tend to break the segmentation prior they aim to leverage, due to the deficiency in video matting data.", "description": "\uadf8\ub9bc 2\ub294 MatAnyone\uc758 \ub3d9\uc791 \uc6d0\ub9ac\ub97c \uc124\uba85\ud558\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4. (a)\ub294 \ub9e4\ud305 \ud504\ub808\uc784\uc5d0\uc11c \uc54c\ud30c \uac12(\ud22c\uba85\ub3c4)\uc5d0 \ub530\ub77c \uc774\ubbf8\uc9c0\ub97c \ub450 \uc601\uc5ed\uc73c\ub85c \ub098\ub20c \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud575\uc2ec \uc601\uc5ed(semantic)\uc740 \ubc30\uacbd(\uc54c\ud30c \uac12 0)\uacfc \uc804\uacbd(\uc54c\ud30c \uac12 1)\uc744 \ud3ec\ud568\ud558\uba70, \uacbd\uacc4 \uc601\uc5ed(fine-details)\uc740 \uc54c\ud30c \uac12\uc774 0\uacfc 1 \uc0ac\uc774\uc778 \uc601\uc5ed\uc785\ub2c8\ub2e4. \uacbd\uacc4 \uc601\uc5ed\uc740 \uba38\ub9ac\uce74\ub77d \ub4f1 \uc138\ubc00\ud55c \ubd80\ubd84\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4. (b)\ub294 \uae30\uc874\uc758 \ubcf4\uc870 \uc815\ubcf4 \uc5c6\ub294(auxiliary-free) \ubc29\ubc95\uc778 RVM [33]\uacfc \ub9c8\uc2a4\ud06c \uae30\ubc18(mask-guided) \ubc29\ubc95\uc778 MaGGIe [22]\uc758 \ud55c\uacc4\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. RVM\uc740 \ubaa8\ud638\ud55c \ubc30\uacbd\uc5d0\uc11c \uac1d\uccb4\ub97c \uc798\ubabb \uc2dd\ubcc4\ud558\uace0, MaGGIe\ub294 \ubd80\uc871\ud55c \uc601\uc0c1 \ub9e4\ud305 \ub370\uc774\ud130\ub85c \uc778\ud574 \ubd84\ud560 \uc804(segmentation prior) \uc815\ubcf4\ub97c \uc81c\ub300\ub85c \ud65c\uc6a9\ud558\uc9c0 \ubabb\ud569\ub2c8\ub2e4. MatAnyone\uc740 \uc774\ub7ec\ud55c \ud55c\uacc4\uc810\uc744 \uadf9\ubcf5\ud558\uae30 \uc704\ud574 \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.14677/x4.png", "caption": "Figure 3: \nAn overview of MatAnyone.\nMatAnyone is a memory-based framework for video matting. Given a target segmentation map in the first frame, our model achieves stable and high-quality matting through consistent memory propagation, with a region-adaptive memory fusion module to combine information from the previous and current frame. To overcome the scarcity of real video matting data, we incorporate a new training strategy that effectively leverages matting data for fine-grained matting details and segmentation data for semantic stability, with designed losses separately.", "description": "\uadf8\ub9bc 3\uc740 MatAnyone\uc758 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MatAnyone\uc740 \uba54\ubaa8\ub9ac \uae30\ubc18 \ube44\ub514\uc624 \ub9e4\ud305 \ud504\ub808\uc784\uc6cc\ud06c\ub85c, \uccab \ubc88\uc9f8 \ud504\ub808\uc784\uc758 \ud0c0\uac9f \ubd84\ud560\ub9f5\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc77c\uad00\ub41c \uba54\ubaa8\ub9ac \uc804\ud30c\ub97c \ud1b5\ud574 \uc548\uc815\uc801\uc774\uace0 \uace0\ud488\uc9c8\uc758 \ub9e4\ud305 \uacb0\uacfc\ub97c \uc5bb\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc774\uc804 \ud504\ub808\uc784\uacfc \ud604\uc7ac \ud504\ub808\uc784\uc758 \uc815\ubcf4\ub97c \uacb0\ud569\ud558\ub294 \uc9c0\uc5ed \uc801\uc751 \uba54\ubaa8\ub9ac \uc735\ud569 \ubaa8\ub4c8\uc744 \ud1b5\ud574 \uac00\ub2a5\ud569\ub2c8\ub2e4. \uc2e4\uc81c \ube44\ub514\uc624 \ub9e4\ud305 \ub370\uc774\ud130 \ubd80\uc871 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574, \ub9e4\ud305 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc138\ubc00\ud55c \ub9e4\ud305 \ub514\ud14c\uc77c\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud65c\uc6a9\ud558\uace0, \ubd84\ud560 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc758\ubbf8\uc801 \uc548\uc815\uc131\uc744 \ud655\ubcf4\ud558\ub294 \uc0c8\ub85c\uc6b4 \ud559\uc2b5 \uc804\ub7b5\uc744 \ud1b5\ud569\ud588\uc2b5\ub2c8\ub2e4. \uac01\uae30 \ub2e4\ub978 \uc190\uc2e4 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc774\ub7ec\ud55c \ubaa9\ud45c\ub97c \ub2ec\uc131\ud569\ub2c8\ub2e4.", "section": "3. \ubc29\ubc95\ub860"}, {"figure_path": "https://arxiv.org/html/2501.14677/x5.png", "caption": "Figure 4: \nQualitative comparisons on real-world videos.\nOur MatAnyone significantly outperforms existing auxiliary-free (RVM\u00a0[33]) and mask-guided (FTP-VM\u00a0[21] and MaGGIe\u00a0[22]) approaches in both detail extraction and semantic accuracy. For the lowest row, while other methods all miss out on important body parts (i.e., head) and mistakenly take background pixels as foreground (due to similar colors), thus generating messy outputs, our method presents an accurate and visually clean output by even identifying the shadow near the boundary.", "description": "\uadf8\ub9bc 4\ub294 \uc2e4\uc81c \uc601\uc0c1\uc5d0 \ub300\ud55c \uc815\uc131\uc801 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc81c\uc2dc\ub41c MatAnyone \ubc29\ubc95\uc740 \uae30\uc874\uc758 \ubcf4\uc870 \uc815\ubcf4 \uc5c6\ub294(RVM [33]) \ubc0f \ub9c8\uc2a4\ud06c \uae30\ubc18(FTP-VM [21], MaGGIe [22]) \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \uc138\ubd80 \uc815\ubcf4 \ucd94\ucd9c \ubc0f \uc758\ubbf8\uc801 \uc815\ud655\ub3c4 \uce21\uba74\uc5d0\uc11c \ud6e8\uc52c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. \ud2b9\ud788, \ub9c8\uc9c0\ub9c9 \ud589\uc758 \uc608\uc2dc\uc5d0\uc11c \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc740 \uc911\uc694\ud55c \uc2e0\uccb4 \ubd80\uc704(\uc608: \uba38\ub9ac)\ub97c \ub204\ub77d\ud558\uac70\ub098 \uc720\uc0ac\ud55c \uc0c9\uc0c1\uc73c\ub85c \uc778\ud574 \ubc30\uacbd \ud53d\uc140\uc744 \uc798\ubabb\ub41c \uc804\uacbd\uc73c\ub85c \uc778\uc2dd\ud558\uc5ec \uacb0\uacfc\ubb3c\uc774 \ud750\ub9bf\ud558\uac8c \ub098\ud0c0\ub098\ub294 \ubc18\uba74, MatAnyone \ubc29\ubc95\uc740 \uadf8\ub9bc\uc790\uae4c\uc9c0\ub3c4 \uc815\ud655\ud558\uac8c \uc2dd\ubcc4\ud558\uc5ec \uc815\ud655\ud558\uace0 \uc2dc\uac01\uc801\uc73c\ub85c \uae68\ub057\ud55c \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "5.1 \ube44\uad50"}, {"figure_path": "https://arxiv.org/html/2501.14677/x6.png", "caption": "Figure 5: \nQuantitative comparisons with MaGGIe\u00a0[22] on instance video matting. Despite MaGGIe using instance mask as guidance for each frame, our method shows better performance, achieving better stability in object tracking and finer alpha matte details.", "description": "\uadf8\ub9bc 5\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uacfc MaGGIe [22]\uc758 \uc778\uc2a4\ud134\uc2a4 \ube44\ub514\uc624 \ub9e4\ud305 \uacb0\uacfc\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. MaGGIe\ub294 \uac01 \ud504\ub808\uc784\uc5d0 \uc778\uc2a4\ud134\uc2a4 \ub9c8\uc2a4\ud06c\ub97c \uc0ac\uc6a9\ud558\uc9c0\ub9cc, \uc81c\uc548\ub41c \ubc29\ubc95\uc740 \uccab \ubc88\uc9f8 \ud504\ub808\uc784\uc5d0\ub9cc \ub9c8\uc2a4\ud06c\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c \uc81c\uc548\ub41c \ubc29\ubc95\uc740 \uac1d\uccb4 \ucd94\uc801\uc758 \uc548\uc815\uc131\uacfc \ub354\uc6b1 \uc138\ubc00\ud55c \uc54c\ud30c \ub9e4\ud2b8 \ub514\ud14c\uc77c \uce21\uba74\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uc774 \uc2dc\uac04\uc801 \uc77c\uad00\uc131\uc744 \uc720\uc9c0\ud558\uba74\uc11c\ub3c4 \ub354\uc6b1 \uc815\ud655\ud558\uace0 \uc138\ubc00\ud55c \ub9e4\ud305 \uacb0\uacfc\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "5.1.2 \uc815\ub7c9\uc801 \ud3c9\uac00"}, {"figure_path": "https://arxiv.org/html/2501.14677/x7.png", "caption": "Figure 6: \nImprovement with Recurrent refinement. (Zoom-in for best view)", "description": "\uadf8\ub9bc 6\uc740 \ubc18\ubcf5\uc801 \uac1c\uc120\uc744 \ud1b5\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uccab \ubc88\uc9f8 \ud504\ub808\uc784\uc758 \uc138\ubd84\ud654\ub41c \ub9c8\uc2a4\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc740 \uc77c\ub828\uc758 \ubc18\ubcf5\uc801\uc778 \uac1c\uc120\uc744 \ud1b5\ud574 \uac01 \ud504\ub808\uc784\uc758 \uc54c\ud30c \ub9e4\ud2b8\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ubc18\ubcf5 \ud69f\uc218\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c (t=1\uc5d0\uc11c t=10\uae4c\uc9c0), \uc54c\ud30c \ub9e4\ud2b8\uc758 \ub514\ud14c\uc77c\uc774 \uc810\uc810 \ub354 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788 \uacbd\uacc4 \uc601\uc5ed\uc5d0\uc11c\uc758 \ub514\ud14c\uc77c\uc774 \ud06c\uac8c \uac1c\uc120\ub418\uc5b4, \uc774\ubbf8\uc9c0 \ub9e4\ud305 \uc218\uc900\uc758 \ud488\uc9c8\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4. \uc774\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uc774 \ubc18\ubcf5\uc801\uc778 \uac1c\uc120\uc744 \ud1b5\ud574 \uc815\ud655\ud558\uace0 \uc138\ubc00\ud55c \ub9e4\ud305 \uacb0\uacfc\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc\uc744 \ud655\ub300\ud558\uc5ec \uc790\uc138\ud788 \uc0b4\ud3b4\ubcf4\uc138\uc694.", "section": "3.3 Practical Inference Strategies"}, {"figure_path": "https://arxiv.org/html/2501.14677/x8.png", "caption": "Figure 7: \nComparison of matting results training with original DDC loss\u00a0[35] and with scaled DDC loss, where the latter gives more stable and natural matting results.", "description": "\uc774 \uadf8\ub9bc\uc740 \uc6d0\ubcf8 DDC \uc190\uc2e4 \ud568\uc218[35]\uc640 \uc870\uc815\ub41c DDC \uc190\uc2e4 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ud55c \ub9e4\ud305 \uacb0\uacfc\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uc870\uc815\ub41c DDC \uc190\uc2e4 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uba74 \ub354\uc6b1 \uc548\uc815\uc801\uc774\uace0 \uc790\uc5f0\uc2a4\ub7ec\uc6b4 \ub9e4\ud305 \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc6d0\ubcf8 DDC \uc190\uc2e4 \ud568\uc218\ub294 \uacbd\uacc4 \uc601\uc5ed\uc5d0\uc11c \uacc4\ub2e8 \ud604\uc0c1\uacfc \uac19\uc740 \uc778\uacf5\uc801\uc778 \uacb0\uacfc\ub97c \uc0dd\uc131\ud558\ub294 \ubc18\uba74, \uc870\uc815\ub41c DDC \uc190\uc2e4 \ud568\uc218\ub294 \ubcf4\ub2e4 \uc790\uc5f0\uc2a4\ub7fd\uace0 \ub9e4\ub044\ub7ec\uc6b4 \uacbd\uacc4\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774\ub294 \uc601\uc0c1 \ub9e4\ud305\uc5d0\uc11c \uc138\ubc00\ud55c \ub514\ud14c\uc77c\uacfc \uacf5\uac04\uc801 \uc77c\uad00\uc131\uc744 \uc720\uc9c0\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "3.2. Core-area Supervision via Segmentation"}, {"figure_path": "https://arxiv.org/html/2501.14677/x9.png", "caption": "Figure 8: \nIssues with VideoMatte240K\u00a0[32].\n(a) Errors in alpha values exist in reflective regions (e.g., \u201ca hole\u201d on glasses).\n(b) Inhomogeneous alpha values exist in core regions (e.g., caused by shadow), where the alpha value should be exactly 0 or 1.", "description": "\uadf8\ub9bc 8\uc740 VideoMatte240K \ub370\uc774\ud130\uc14b\uc758 \ubb38\uc81c\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \ubc18\uc0ac\ub418\ub294 \uc601\uc5ed(\uc608: \uc548\uacbd)\uc5d0\uc11c \uc54c\ud30c\uac12 \uc624\ub958\uac00 \ubc1c\uc0dd\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc548\uacbd\uc758 \uc77c\ubd80\ubd84\uc774 \uc798\ubabb\ub41c \uc54c\ud30c\uac12\uc73c\ub85c \uc778\ud574 \ub9c8\uce58 \uad6c\uba4d\uc774 \ub6ab\ub9b0 \uac83\ucc98\ub7fc \ubcf4\uc785\ub2c8\ub2e4. (b)\ub294 \uadf8\ub9bc\uc790\uc640 \uac19\uc774 \ubcf8\ub798 \uc54c\ud30c\uac12\uc774 0 \ub610\ub294 1\uc774\uc5b4\uc57c \ud558\ub294 \uc601\uc5ed(\ud575\uc2ec \uc601\uc5ed)\uc5d0\uc11c \uc54c\ud30c\uac12\uc774 \ubd88\uade0\uc77c\ud55c \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubb38\uc81c\ub294 \ube44\uc77c\uad00\uc801\uc774\uace0 \ubd80\uc815\ud655\ud55c \ub9c8\uc2a4\ud06c\ub97c \uc0dd\uc131\ud558\uc5ec \uc601\uc0c1 \ub9e4\ud305 \uc131\ub2a5\uc5d0 \uc545\uc601\ud5a5\uc744 \ubbf8\uce60 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Data"}, {"figure_path": "https://arxiv.org/html/2501.14677/x10.png", "caption": "Figure 9: \nGallery for our new training dataset VM800.\nHigh-quality details in the boundary regions and diversity in terms of gender, hairstyles, and aspect ratios could be clearly observed.", "description": "\uadf8\ub9bc 9\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0c8\ub86d\uac8c \uc81c\uc2dc\ud558\ub294 VM800 \uc601\uc0c1 \ub9e4\ud305 \ub370\uc774\ud130\uc14b\uc758 \uc77c\ubd80 \uc774\ubbf8\uc9c0\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\uc874 \ub370\uc774\ud130\uc14b\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \uacbd\uacc4 \uc601\uc5ed\uc758 \ub514\ud14c\uc77c\uc774 \ud6e8\uc52c \ud48d\ubd80\ud558\uba70, \uc131\ubcc4, \ud5e4\uc5b4\uc2a4\ud0c0\uc77c, \uc885\ud6a1\ube44 \uce21\uba74\uc5d0\uc11c \ub2e4\uc591\uc131\uc774 \ud06c\uac8c \ud5a5\uc0c1\ub418\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uc774\ubbf8\uc9c0\ub4e4\uc740 \ub2e4\uc591\ud55c \uc0ac\ub78c\ub4e4\uc758 \ubaa8\uc2b5\uacfc \ubc30\uacbd\uc744 \ub2f4\uace0 \uc788\uc73c\uba70, \uba38\ub9ac\uce74\ub77d, \uc637 \ub4f1 \uc138\ubc00\ud55c \ubd80\ubd84\uae4c\uc9c0 \uc120\uba85\ud558\uac8c \ud45c\ud604\ub418\uc5b4 \uc788\uc74c\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub7ec\ud55c \ub370\uc774\ud130\uc14b\uc758 \uc9c8\uc801 \ud5a5\uc0c1\uc740 \ubcf4\ub2e4 \uc815\ud655\ud558\uace0 \uc790\uc5f0\uc2a4\ub7ec\uc6b4 \uc601\uc0c1 \ub9e4\ud305 \uacb0\uacfc\ub97c \uc5bb\ub294 \ub370 \ud06c\uac8c \uae30\uc5ec\ud560 \uac83\uc73c\ub85c \uc608\uc0c1\ub429\ub2c8\ub2e4.", "section": "4. Data"}, {"figure_path": "https://arxiv.org/html/2501.14677/x11.png", "caption": "Figure 10: \nHarmonization on synthetic benchmarks and its effect on model performance.\nHarmonization\u00a0[23] is an operation that makes the composited frame more natural and realistic, which also effectively makes our YouTubeMatte a more challenging benchmark that is closer to the real distribution.\nIt is observed that while RVM\u00a0[33] is confused by the harmonized frame, our method still yields robust performance.", "description": "\uadf8\ub9bc 10\uc740 \ud569\uc131 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc870\ud654 \ubc0f \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ub300\ud55c \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc870\ud654[23]\ub294 \ud569\uc131\ub41c \ud504\ub808\uc784\uc744 \ub354 \uc790\uc5f0\uc2a4\ub7fd\uace0 \ud604\uc2e4\uc801\uc73c\ub85c \ub9cc\ub4dc\ub294 \uc5f0\uc0b0\uc73c\ub85c, YouTubeMatte\ub97c \uc2e4\uc81c \ubd84\ud3ec\uc5d0 \ub354 \uac00\uae4c\uc6b4 \ub354\uc6b1 \uc5b4\ub824\uc6b4 \ubca4\uce58\ub9c8\ud06c\ub85c \ub9cc\ub4ed\ub2c8\ub2e4. RVM[33]\uc774 \uc870\ud654\ub41c \ud504\ub808\uc784\uc5d0 \ud63c\ub780\uc744 \uacaa\ub294 \ubc18\uba74, \uc81c\uc2dc\ub41c \ubc29\ubc95\uc740 \uc5ec\uc804\ud788 \uac15\ub825\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. \ub370\uc774\ud130"}, {"figure_path": "https://arxiv.org/html/2501.14677/x12.png", "caption": "Figure 11: \n(a) Comparison on results trained with old training data (VideoMatte240K\u00a0[32]) and new training data (our VM800). It could be observed that training with old data will lead to errors in reflective objects (e.g., holes on the sunglasses) and inhomogeneous alpha values in the core regions. However, both issues are fixed when training with our new data, indicating a higher quality.\n(b) Comparison on results trained without and with core-area supervision. It could be observed that training without it will lead to semantics error due to the weak supervision from real segmentation data, while training with core supervision largely improves semantics accuracy thanks to the stronger supervision enabled.", "description": "\uadf8\ub9bc 11\uc740 \ub450 \uac00\uc9c0 \ube44\uad50 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \uae30\uc874 VideoMatte240K \ub370\uc774\ud130\uc14b\uacfc \uc0c8\ub85c \uc81c\uc791\ud55c VM800 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ud55c \uacb0\uacfc\ub97c \ube44\uad50\ud569\ub2c8\ub2e4. \uae30\uc874 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud6c8\ub828\ud588\uc744 \ub54c\ub294 \ubc18\uc0ac \ubb3c\uccb4(\uc608: \uc120\uae00\ub77c\uc2a4\uc758 \uad6c\uba4d)\uc5d0\uc11c \uc624\ub958\uac00 \ubc1c\uc0dd\ud558\uace0, \uc911\uc2ec \uc601\uc5ed\uc758 \uc54c\ud30c \uac12\uc774 \ubd88\uade0\uc77c\ud558\uac8c \ub098\ud0c0\ub098\ub294 \ubb38\uc81c\uac00 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc0c8 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud6c8\ub828\ud588\uc744 \ub54c\ub294 \uc774\ub7ec\ud55c \ubb38\uc81c\ub4e4\uc774 \ud574\uacb0\ub418\uc5b4 \ub354 \ub192\uc740 \ud488\uc9c8\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. (b)\ub294 \uc911\uc2ec \uc601\uc5ed\uc5d0 \ub300\ud55c \ucd94\uac00\uc801\uc778 \uc9c0\ub3c4 \ud559\uc2b5\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c\uc640 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\uc758 \uacb0\uacfc\ub97c \ube44\uad50\ud569\ub2c8\ub2e4. \ucd94\uac00\uc801\uc778 \uc9c0\ub3c4 \ud559\uc2b5 \uc5c6\uc774 \uc2e4\uc81c \ubd84\ud560 \ub370\uc774\ud130\ub9cc\uc73c\ub85c \ud6c8\ub828\ud588\uc744 \ub54c\ub294 \uc758\ubbf8\ub860\uc801 \uc624\ub958\uac00 \ubc1c\uc0dd\ud588\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc911\uc2ec \uc601\uc5ed\uc5d0 \ub300\ud55c \uc9c0\ub3c4 \ud559\uc2b5\uc744 \ud1b5\ud574 \ub354 \uac15\ub825\ud55c \uc9c0\ub3c4 \ud559\uc2b5\uc774 \uac00\ub2a5\ud574\uc9d0\uc5d0 \ub530\ub77c \uc758\ubbf8\ub860\uc801 \uc815\ud655\ub3c4\uac00 \ud06c\uac8c \ud5a5\uc0c1\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "5.1.2 \uc815\ub7c9\uc801 \ud3c9\uac00 \ubc0f \uc815\uc131\uc801 \ud3c9\uac00 (Qualitative Evaluations)"}, {"figure_path": "https://arxiv.org/html/2501.14677/x13.png", "caption": "Figure 12: \nComparison on results with and without Consistent Memory Propagation. It could be observed that when CMP is not applied, semantic errors constantly exist across a wide span of video frames. However, when training with CMP, we observe from the \u201cChange Probability\u201d mask that usually our model only takes pixels near the boundary as \u201cchanged\u201d, and most of the inner regions (i.e., earring) will mainly take the memory values from the last frame. As we can see on the figure, while predictions are both correct at time t\ud835\udc61titalic_t, the model with CMP successfully keeps the correctness and gives stable results, while the model without CMP quickly breaks the correctness and never recovers.", "description": "\uadf8\ub9bc 12\ub294 \uc77c\uad00\ub41c \uba54\ubaa8\ub9ac \uc804\ud30c(CMP) \ubaa8\ub4c8\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. CMP\ub97c \uc801\uc6a9\ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0, \uc2dc\ub9e8\ud2f1 \uc624\ub958\uac00 \ube44\ub514\uc624 \ud504\ub808\uc784 \uc804\uccb4\uc5d0 \uac78\uccd0 \uc9c0\uc18d\uc801\uc73c\ub85c \ubc1c\uc0dd\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 CMP\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ud55c \uacbd\uc6b0, '\ubcc0\ud654 \ud655\ub960' \ub9c8\uc2a4\ud06c\uc5d0\uc11c \ubaa8\ub378\uc774 \uc77c\ubc18\uc801\uc73c\ub85c \uacbd\uacc4 \uadfc\ucc98\uc758 \ud53d\uc140\ub9cc '\ubcc0\uacbd\ub41c' \uac83\uc73c\ub85c \uac04\uc8fc\ud558\uace0, \ub300\ubd80\ubd84\uc758 \ub0b4\ubd80 \uc601\uc5ed(\uc608: \uadc0\uac78\uc774)\uc740 \uc774\uc804 \ud504\ub808\uc784\uc758 \uba54\ubaa8\ub9ac \uac12\uc744 \uc8fc\ub85c \uc0ac\uc6a9\ud558\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774, t\uc2dc\uc810\uc5d0\uc11c \uc608\uce21\uc774 \ubaa8\ub450 \uc815\ud655\ud55c \uacbd\uc6b0, CMP\ub97c \uc0ac\uc6a9\ud55c \ubaa8\ub378\uc740 \uc815\ud655\uc131\uc744 \uc720\uc9c0\ud558\uace0 \uc548\uc815\uc801\uc778 \uacb0\uacfc\ub97c \uc81c\uacf5\ud558\uc9c0\ub9cc, CMP\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \ubaa8\ub378\uc740 \uc815\ud655\uc131\uc774 \ube68\ub9ac \uae68\uc9c0\uace0 \ud68c\ubcf5\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.  \uc989, CMP \ubaa8\ub4c8\uc774 \uc2dc\uac04\uc801 \uc77c\uad00\uc131\uc744 \uc720\uc9c0\ud558\uace0 \uc138\ubd80\uc801\uc778 \ubd80\ubd84\uae4c\uc9c0 \uc815\ud655\ud558\uac8c \uc608\uce21\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \uc218\ud589\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.1.2 \uc815\ub7c9\uc801 \ud3c9\uac00"}, {"figure_path": "https://arxiv.org/html/2501.14677/x14.png", "caption": "Figure 13: \nComparison on results with iterative refinement. A noticeable enhancement on details can be observed even with one iteration of refinement compared with the given segmentation mask. Within 10 iterations, our model is able to achieve matting details at an image-matting level, even better than Matte Anything\u00a0[56], which is an image matting model also based on the results from SAM\u00a0[25].", "description": "\uadf8\ub9bc 13\uc740 \ubc18\ubcf5\uc801\uc778 \uac1c\uc120\uc744 \ud1b5\ud55c \uacb0\uacfc \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc8fc\uc5b4\uc9c4 \ubd84\ud560 \ub9c8\uc2a4\ud06c\uc640 \ube44\uad50\ud558\uc5ec \ub2e8 \ud55c \ubc88\uc758 \uac1c\uc120\ub9cc\uc73c\ub85c\ub3c4 \uc138\ubd80\uc801\uc778 \ubd80\ubd84\uc774 \ub208\uc5d0 \ub744\uac8c \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. 10\ubc88\uc758 \ubc18\ubcf5\uc801\uc778 \uac1c\uc120\uc744 \uac70\uce58\uba74 \ubaa8\ub378\uc740 SAM [25]\uc758 \uacb0\uacfc\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 \uc774\ubbf8\uc9c0 \ub9e4\ud305 \ubaa8\ub378\uc778 Matte Anything [56]\ubcf4\ub2e4 \ub354 \ub098\uc740 \uc218\uc900\uc758 \uc774\ubbf8\uc9c0 \ub9e4\ud305 \uc218\uc900\uc758 \ub9e4\ud305 \ub514\ud14c\uc77c\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.3. Practical Inference Strategies"}, {"figure_path": "https://arxiv.org/html/2501.14677/x15.png", "caption": "Figure 14: \nMore qualitative comparisons on general video matting with SOTA methods. We compare our MatAnyone with both auxiliary-free (AF) method: RVM\u00a0[33] and mask-guided methods: FTP-VM\u00a0[21], and MaGGIe\u00a0[22].\nIt could be observed that our method significantly outperforms others in both detail extraction and semantic accuracy, across diverse and complex real scenarios.\nIt is noteworthy that although sometimes MaGGIe\u00a0[22] seems to give acceptable results when compositing with a green screen, its alpha matte turns out to be noisy (i.e., inhomogeneous in the core foreground region and blurry in the boundary region), while our alpha matte is clean with fine-grained details in the boundary region. As a result, we also include alpha mattes for a more comprehensive comparison. (Zoom in for best view)", "description": "\uadf8\ub9bc 14\ub294 \ub2e4\uc591\ud558\uace0 \ubcf5\uc7a1\ud55c \uc2e4\uc81c \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \ucd5c\ucca8\ub2e8 \ube44\ub514\uc624 \ub9e4\ud305 \ubc29\ubc95\ub4e4\uacfc MatAnyone\uc758 \uc815\uc131\uc801 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MatAnyone\uc740 \ubcf4\uc870 \uc790\ub8cc\uac00 \uc5c6\ub294 RVM[33]\uacfc \ub9c8\uc2a4\ud06c \uae30\ubc18 FTP-VM[21], MaGGIe[22] \ubc29\ubc95\ub4e4\uacfc \ube44\uad50\ub429\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c MatAnyone\uc740 \uc138\ubd80 \uc815\ubcf4 \ucd94\ucd9c\uacfc \uc758\ubbf8\uc801 \uc815\ud655\ub3c4 \uce21\uba74\uc5d0\uc11c \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc744 \ud06c\uac8c \ub2a5\uac00\ud568\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. MaGGIe[22]\ub294 \ub179\uc0c9 \ud654\uba74 \ud569\uc131 \uc2dc \ud5c8\uc6a9\ub418\ub294 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uacbd\uc6b0\uac00 \uc788\uc9c0\ub9cc, \uc54c\ud30c \ub9e4\ud2b8\ub294 \ub178\uc774\uc988\uac00 \ub9ce\uace0(\uc989, \uc804\uacbd \uc601\uc5ed\uc758 \ubd88\uade0\uc77c\uc131\uacfc \uacbd\uacc4 \uc601\uc5ed\uc758 \ud750\ub9bf\ud568), MatAnyone\uc758 \uc54c\ud30c \ub9e4\ud2b8\ub294 \uacbd\uacc4 \uc601\uc5ed\uc5d0\uc11c \uc138\ubd80 \uc815\ubcf4\uac00 \uba85\ud655\ud558\uace0 \uae68\ub057\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c \ubcf4\ub2e4 \ud3ec\uad04\uc801\uc778 \ube44\uad50\ub97c \uc704\ud574 \uc54c\ud30c \ub9e4\ud2b8\ub3c4 \ud3ec\ud568\ud558\uc600\uc2b5\ub2c8\ub2e4. (\ud655\ub300\ud558\uc5ec \ubcf4\uc138\uc694)", "section": "5.1.2 \uc815\uc131\uc801 \ud3c9\uac00"}, {"figure_path": "https://arxiv.org/html/2501.14677/x16.png", "caption": "Figure 15: \nA challenging example of general video matting across a long time span. We compare our MatAnyone with both auxiliary-free (AF) method: RVM\u00a0[33] and mask-guided methods: FTP-VM\u00a0[21], and MaGGIe\u00a0[22].\nIt could be observed that our model is able to track the target object stably even when the object is moving fast in a highly complex scene, where all the other methods present noticeable failures. (Zoom in for best view)", "description": "\uadf8\ub9bc 15\ub294 \ubcf5\uc7a1\ud55c \uc7a5\uba74\uc5d0\uc11c \ube60\ub974\uac8c \uc6c0\uc9c1\uc774\ub294 \uac1d\uccb4\ub97c \uc7a5\uc2dc\uac04\uc5d0 \uac78\uccd0 \ub9e4\ud305\ud558\ub294 \uc5b4\ub824\uc6b4 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MatAnyone\uc744 \uae30\uc874\uc758 \ubcf4\uc870 \uc815\ubcf4 \uc5c6\ub294(auxiliary-free) \ubc29\ubc95\uc778 RVM [33]\uacfc \ub9c8\uc2a4\ud06c \uae30\ubc18(mask-guided) \ubc29\ubc95\uc778 FTP-VM [21], MaGGIe [22]\uc640 \ube44\uad50\ud558\uc5ec, MatAnyone\uc774 \ubcf5\uc7a1\ud55c \uc7a5\uba74\uc5d0\uc11c\ub3c4 \ube60\ub974\uac8c \uc6c0\uc9c1\uc774\ub294 \uac1d\uccb4\ub97c \uc548\uc815\uc801\uc73c\ub85c \ucd94\uc801\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc740 \ub208\uc5d0 \ub744\ub294 \uc2e4\ud328\ub97c \ubcf4\uc774\ub294 \ubc18\uba74, MatAnyone\uc740 \ub300\uc0c1 \uac1d\uccb4\ub97c \uc815\ud655\ud558\uac8c \ub9e4\ud305\ud569\ub2c8\ub2e4. \uc790\uc138\ud788 \ubcf4\uae30 \uc704\ud574 \ud655\ub300\ud574 \ubcf4\uc138\uc694.", "section": "5.1.2 \uc815\uc131\uc801 \ud3c9\uac00 (Qualitative Evaluations)"}, {"figure_path": "https://arxiv.org/html/2501.14677/x17.png", "caption": "Figure 16: \nAnother challenging example of general video matting across a long time span. We compare our MatAnyone with both auxiliary-free (AF) method: RVM\u00a0[33] and mask-guided methods: FTP-VM\u00a0[21], and MaGGIe\u00a0[22].\nThis example showcases that our model is able to track the target objects even in a highly ambiguous background, where the colors for foreground and background are similar, and also multiple humans in the background.\nIn addition, it also demonstrates when there is more than one target object, our model is still able to handle this challenging case well. (Zoom in for best view)", "description": "\uadf8\ub9bc 16\uc740 \ub2e4\uc591\ud55c \ubcf5\uc7a1\ud55c \ubc30\uacbd\uacfc \uc5ec\ub7ec \uba85\uc758 \uc0ac\ub78c\uc774 \uc788\ub294 \uc7a5\uba74\uc5d0\uc11c \uc7a5\uc2dc\uac04\uc5d0 \uac78\uccd0 \ube44\ub514\uc624 \ub9e4\ud305\uc744 \uc218\ud589\ud558\ub294 \uc5b4\ub824\uc6b4 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MatAnyone\uc744 \ubcf4\uc870 \ub3c4\uad6c \uc5c6\uc774 \ub3d9\uc791\ud558\ub294 RVM[33] \ubc0f \ub9c8\uc2a4\ud06c\ub97c \uc0ac\uc6a9\ud558\ub294 FTP-VM[21], MaGGIe[22]\uc640 \ube44\uad50\ud558\uc5ec, MatAnyone\uc740 \uc804\uacbd\uacfc \ubc30\uacbd\uc758 \uc0c9\uc0c1\uc774 \uc720\uc0ac\ud558\uace0 \ubc30\uacbd\uc5d0 \uc5ec\ub7ec \uc0ac\ub78c\uc774 \uc788\ub294 \uc560\ub9e4\ud55c \ubc30\uacbd\uc5d0\uc11c\ub3c4 \ub300\uc0c1 \uac1d\uccb4\ub97c \uc815\ud655\ud558\uac8c \ucd94\uc801\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c, \uc5ec\ub7ec \uac1c\uc758 \ub300\uc0c1 \uac1d\uccb4\uac00 \uc788\uc744 \ub54c\ub3c4 MatAnyone\uc774 \uc774\ub7ec\ud55c \uc5b4\ub824\uc6b4 \uc0c1\ud669\uc744 \uc798 \ucc98\ub9ac\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ud655\ub300\ud558\uc5ec \ud655\uc778\ud558\uc2ed\uc2dc\uc624.", "section": "5.1.2 \uc815\uc131\uc801 \ud3c9\uac00"}]