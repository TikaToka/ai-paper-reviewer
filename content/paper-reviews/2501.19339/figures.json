[{"figure_path": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/head_figure2.jpg", "caption": "Figure 1: PEAP framework: we investigate the possibility of perceive everything as pixels. This framework aligns better with human perception reducing the need for excessive pre-processing. Evaluated on our benchmark PixelWorld, PEAP boosts performance on multimodal tasks (e.g., websites, slides, documents) but struggles with complex, text-centric tasks (e.g., reasoning and coding). Larger models achieve better transferability between pixel- and token-based performance compared to smaller ones. We also observed that text and images exhibit similar attention patterns, and reduced the overhead of model reasoning through patch pruning by PEAP-Fast.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 PEAP(Perceive Everything as Pixels) \ud504\ub808\uc784\uc6cc\ud06c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. PEAP\ub294 \ubaa8\ub4e0 \ubaa8\ub2ec\ub9ac\ud2f0(\ud14d\uc2a4\ud2b8, \ud45c, \ucf54\ub4dc, \ub2e4\uc774\uc5b4\uadf8\ub7a8, \uc774\ubbf8\uc9c0 \ub4f1)\ub97c \ud53d\uc140 \uc785\ub825\uc73c\ub85c \ud1b5\ud569\ud558\uc5ec \ucc98\ub9ac\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.  \uae30\uc874\uc758 \ud1a0\ud070 \uae30\ubc18 \ubc29\uc2dd\uacfc \ub2ec\ub9ac \uc778\uac04\uc758 \uc9c0\uac01 \ubc29\uc2dd\uc5d0 \ub354 \uac00\uae5d\uac8c \uc124\uacc4\ub418\uc5b4 \uc0ac\uc804 \ucc98\ub9ac \uacfc\uc815\uc744 \uc904\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4. PixelWorld \ubca4\uce58\ub9c8\ud06c\ub97c \ud1b5\ud574 \ud3c9\uac00\ud55c \uacb0\uacfc, PEAP\ub294 \uc6f9\uc0ac\uc774\ud2b8, \uc2ac\ub77c\uc774\ub4dc, \ubb38\uc11c\uc640 \uac19\uc740 \ub2e4\uc911 \ubaa8\ub2ec\ub9ac\ud2f0 \uc791\uc5c5\uc5d0\uc11c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc600\uc9c0\ub9cc, \ucd94\ub860 \ubc0f \ucf54\ub529\uacfc \uac19\uc740 \ubcf5\uc7a1\ud55c \ud14d\uc2a4\ud2b8 \uc911\uc2ec \uc791\uc5c5\uc5d0\uc11c\ub294 \uc5b4\ub824\uc6c0\uc744 \uacaa\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \ud070 \ubaa8\ub378\uc77c\uc218\ub85d \ud53d\uc140 \uae30\ubc18\uacfc \ud1a0\ud070 \uae30\ubc18 \uc131\ub2a5 \uac04\uc758 \uc804\uc774\uc131\uc774 \ub354 \ub192\uc558\uc73c\uba70, \ud14d\uc2a4\ud2b8\uc640 \uc774\ubbf8\uc9c0\uac00 \uc720\uc0ac\ud55c \uc5b4\ud150\uc158 \ud328\ud134\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ud655\uc778\ud588\uc2b5\ub2c8\ub2e4. PEAP-Fast\ub97c \ud1b5\ud574 \ud328\uce58 \uac00\uc9c0\uce58\uae30(patch pruning)\uc73c\ub85c \ubaa8\ub378 \ucd94\ub860 \uc624\ubc84\ud5e4\ub4dc\ub97c \uc904\uc77c \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/plt_decompose.png", "caption": "Figure 2: The performance of text-only datasets. The comparison is made between text input and synthesized image input. Most models demonstrate comparable performance on language understanding datasets such as SuperGLUE, GLUE, and ARC. However, notable performance disparities emerge between text-based input and synthesized image input on mathematical reasoning tasks (e.g., MMLU-Pro, GSM8K) and programming tasks (e.g., MBPP). Phi-3.5-Vision exhibits consistently poor performance across all vision tasks, primarily due to its insufficient instruction-following capabilities.", "description": "\uadf8\ub9bc 2\ub294 \ud14d\uc2a4\ud2b8 \uc804\uc6a9 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ubaa8\ub378 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud14d\uc2a4\ud2b8 \uc785\ub825\uacfc \ud569\uc131 \uc774\ubbf8\uc9c0 \uc785\ub825 \uac04\uc758 \ube44\uad50\ub97c \ud1b5\ud574 \uc5b8\uc5b4 \uc774\ud574 \ub370\uc774\ud130\uc14b(SuperGLUE, GLUE, ARC)\uc5d0\uc11c\ub294 \ub300\ubd80\ubd84\uc758 \ubaa8\ub378\uc774 \uc720\uc0ac\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\uc73c\ub098, \uc218\ud559\uc801 \ucd94\ub860 \uacfc\uc81c(MMLU-Pro, GSM8K)\uc640 \ud504\ub85c\uadf8\ub798\ubc0d \uacfc\uc81c(MBPP)\uc5d0\uc11c\ub294 \ud14d\uc2a4\ud2b8 \uae30\ubc18 \uc785\ub825\uacfc \ud569\uc131 \uc774\ubbf8\uc9c0 \uc785\ub825 \uac04\uc5d0 \uc0c1\ub2f9\ud55c \uc131\ub2a5 \ucc28\uc774\uac00 \ubc1c\uc0dd\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 Phi-3.5-Vision \ubaa8\ub378\uc740 \ubaa8\ub4e0 \uc2dc\uac01\uc801 \uacfc\uc81c\uc5d0\uc11c \uc77c\uad00\ub418\uac8c \ub0ae\uc740 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c8\ub294\ub370, \uc774\ub294 \uc9c0\uc2dc\uc0ac\ud56d\uc744 \uc81c\ub300\ub85c \ub530\ub974\uc9c0 \ubabb\ud558\ub294 \ub2a5\ub825 \ubd80\uc871 \ub54c\ubb38\uc73c\ub85c \ud574\uc11d\ub429\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \ud14d\uc2a4\ud2b8 \uc774\ud574 \ubc0f \ucd94\ub860 \ub2a5\ub825\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\uba70, \ubaa8\ub378\uc758 \ud06c\uae30\uc640 \ubcf5\uc7a1\ud55c \uacfc\uc81c \ud574\uacb0 \ub2a5\ub825 \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3.1. \ud14d\uc2a4\ud2b8 \uc785\ub825"}, {"figure_path": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/plt_decompose_sc32.png", "caption": "Figure 3: The performance of the structured dataset. We report all the subsets for the TableBench. In the semi setting, questions were presented as text, while tables were rendered as synthetic images. We observed that for tasks involving reasoning (numerical reasoning) and coding (visualization subset), synthetic images yielded inferior performance compared to text. However, for tasks emphasizing semantic understanding, such as data analysis and fact checking, synthetic images achieved performance comparable to or even surpassing text. Additionally, we found that the semi approach often performed worse than either text or synthetic images individually, providing insights into potential limitations and future directions for leveraging vision-language models (VLMs).", "description": "\uadf8\ub9bc 3\uc740 \uad6c\uc870\ud654\ub41c \ub370\uc774\ud130\uc14b(TableBench)\uc5d0 \ub300\ud55c \ubaa8\ub378 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. TableBench\uc758 \ubaa8\ub4e0 \ud558\uc704 \uc791\uc5c5\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \uc81c\uc2dc\ud558\uba70, \u2018\ubc18\ucabd(semi)\u2019 \uc124\uc815\uc5d0\uc11c\ub294 \uc9c8\ubb38\uc740 \ud14d\uc2a4\ud2b8\ub85c, \ud45c\ub294 \ud569\uc131 \uc774\ubbf8\uc9c0\ub85c \ud45c\ud604\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c \ucd94\ub860(\uc218\uce58\uc801 \ucd94\ub860) \ubc0f \ucf54\ub529(\uc2dc\uac01\ud654 \ud558\uc704 \uc791\uc5c5)\uc774 \ud544\uc694\ud55c \uc791\uc5c5\uc758 \uacbd\uc6b0 \ud569\uc131 \uc774\ubbf8\uc9c0\ub294 \ud14d\uc2a4\ud2b8\uc5d0 \ube44\ud574 \uc131\ub2a5\uc774 \ub5a8\uc5b4\uc84c\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \ub370\uc774\ud130 \ubd84\uc11d \ubc0f \uc0ac\uc2e4 \ud655\uc778\uacfc \uac19\uc774 \uc758\ubbf8\ub97c \uc774\ud574\ud558\ub294 \ub370 \uc911\uc810\uc744 \ub454 \uc791\uc5c5\uc5d0\uc11c\ub294 \ud569\uc131 \uc774\ubbf8\uc9c0\uc758 \uc131\ub2a5\uc774 \ud14d\uc2a4\ud2b8\uc640 \ube44\uc2b7\ud558\uac70\ub098 \ub354 \ub098\uc740 \uacbd\uc6b0\ub3c4 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \u2018\ubc18\ucabd\u2019 \ubc29\uc2dd\uc740 \ud14d\uc2a4\ud2b8 \ub610\ub294 \ud569\uc131 \uc774\ubbf8\uc9c0\ub9cc \uc0ac\uc6a9\ud55c \uacbd\uc6b0\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub5a8\uc5b4\uc9c0\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc558\ub294\ub370, \uc774\ub294 \uc2dc\uac01 \uc5b8\uc5b4 \ubaa8\ub378(VLMs)\uc758 \ud55c\uacc4\uc640 \ud5a5\ud6c4 \uac1c\uc120 \ubc29\ud5a5\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3.2. Structured Input"}, {"figure_path": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/plot_MMMUPro_charts.png", "caption": "Figure 4: The performance of the multimodal dataset (MMMU-Pro). We adopt the result reported by the origin paper. We can observe that strong models perform better in PEAP.", "description": "\uadf8\ub9bc 4\ub294 \ub2e4\uc911 \ubaa8\ub4dc \ub370\uc774\ud130\uc14b(MMMU-Pro)\uc5d0 \ub300\ud55c \ubaa8\ub378 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc6d0 \ub17c\ubb38\uc5d0\uc11c \ubcf4\uace0\ub41c \uacb0\uacfc\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uac15\ub825\ud55c \ubaa8\ub378\uc77c\uc218\ub85d PEAP\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  MMMU-Pro \ub370\uc774\ud130\uc14b\uc740 \ub2e4\uc591\ud55c \ubaa8\ub4dc(\ud14d\uc2a4\ud2b8, \uc774\ubbf8\uc9c0, \ube44\ub514\uc624 \ub4f1)\uc758 \ub370\uc774\ud130\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uc73c\uba70, \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \uc785\ub825\uc744 \uc5bc\ub9c8\ub098 \uc798 \ucc98\ub9ac\ud558\ub294\uc9c0\ub97c \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 PEAP(Perceive Everything as Pixels) \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \ubaa8\ub378 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uba70,  PEAP\uac00 \ub2e4\uc911 \ubaa8\ub2ec\ub9ac\ud2f0 \uc791\uc5c5\uc5d0 \ud6a8\uacfc\uc801\uc778\uc9c0 \uc5ec\ubd80\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378\uc758 \ud06c\uae30\uc640 \uc131\ub2a5 \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub3c4 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. ", "section": "3.3. Multimodal Input"}, {"figure_path": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/plt_decompose_sc33.png", "caption": "Figure 5: The performance of the multimodal datasets (except MMMU-Pro). We compare text-only and vision-only subsets in Mathverse, while SlidesVQA and WikiSS-QA are evaluated as VQA tasks. Larger models perform better on text-based tasks with more modalities. GPT-4o tends to generate longer responses in long-context QA, leading to performance degradation on WikiSS-QA.", "description": "\uadf8\ub9bc 5\ub294 MathVerse \ub370\uc774\ud130\uc14b\uc758 \ud14d\uc2a4\ud2b8 \uc804\uc6a9 \ubc0f \ube44\uc804 \uc804\uc6a9 \ud558\uc704 \uc9d1\ud569\uacfc SlidesVQA \ubc0f WikiSS-QA\uc640 \uac19\uc740 VQA \uc791\uc5c5\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \ubaa8\ub378 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  MathVerse\uc5d0\uc11c\ub294 \ud14d\uc2a4\ud2b8 \uae30\ubc18 \uc791\uc5c5\uc5d0 \ub354 \ub9ce\uc740 \ubaa8\ub2ec\ub9ac\ud2f0\uac00 \uc788\ub294 \uacbd\uc6b0 \ub354 \ud070 \ubaa8\ub378\uc774 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \ud558\uc9c0\ub9cc, WikiSS-QA\uc640 \uac19\uc740 \uc7a5\ubb38\uc758 \uc9c8\uc758\uc751\ub2f5 \uc791\uc5c5\uc5d0\uc11c\ub294 GPT-40\uc774 \ub354 \uae34 \uc751\ub2f5\uc744 \uc0dd\uc131\ud558\ub294 \uacbd\ud5a5\uc774 \uc788\uc5b4 \uc131\ub2a5 \uc800\ud558\ub97c \uac00\uc838\uc624\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc694\uc57d\ud558\uc790\uba74, \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ubaa8\ub2ec\ub9ac\ud2f0\ub97c \uc0ac\uc6a9\ud55c \uba40\ud2f0\ubaa8\ub2ec \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ubaa8\ub378 \ud06c\uae30\uc640 \uc131\ub2a5\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc2dc\uac01 \uc790\ub8cc\uc785\ub2c8\ub2e4.", "section": "3.3. Multimodal Input"}, {"figure_path": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/example_input.png", "caption": "Figure 6: Last Layer Attention Heatmap on QWen2VL-7B between token-based (left) and pixel-based (right) inference.", "description": "\ubcf8 \uadf8\ub9bc\uc740 QWen2VL-7B \ubaa8\ub378\uc758 \ub9c8\uc9c0\ub9c9 \ub808\uc774\uc5b4\uc5d0\uc11c \uc5b4\ud150\uc158 \ud788\ud2b8\ub9f5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc740 \ud1a0\ud070 \uae30\ubc18 \uc785\ub825, \uc624\ub978\ucabd\uc740 \ud53d\uc140 \uae30\ubc18 \uc785\ub825\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub294 \ub450 \uac00\uc9c0 \ub2e4\ub978 \uc785\ub825 \ubc29\uc2dd\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \uc5b4\ud150\uc158 \ud328\ud134 \ucc28\uc774\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ube44\uad50\ud558\uc5ec, \ud53d\uc140 \uae30\ubc18 \uc785\ub825\uc774 \ubaa8\ub378\uc758 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc5d0 \uc5b4\ub5a4 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub450 \uc785\ub825 \ubc29\uc2dd \ubaa8\ub450 \uc9c8\ubb38\uacfc \uad00\ub828\ub41c \ub2e8\uc5b4 \ub610\ub294 \uc774\ubbf8\uc9c0 \uc601\uc5ed\uc5d0 \uc5b4\ud150\uc158\uc774 \uc9d1\uc911\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788 \ud53d\uc140 \uae30\ubc18 \uc785\ub825\uc758 \uacbd\uc6b0, \ube48 \uacf5\uac04\uc5d0\ub3c4 \uc5b4\ub290 \uc815\ub3c4\uc758 \uc5b4\ud150\uc158\uc774 \ubd84\ud3ec\ud558\ub294 \uac83\uc744 \uad00\ucc30\ud560 \uc218 \uc788\ub294\ub370, \uc774\ub294 \ucd94\uac00\uc801\uc778 \ubd84\uc11d\uc744 \ud1b5\ud574 \uc5b4\ub5a4 \uc758\ubbf8\ub97c \uc9c0\ub2c8\ub294\uc9c0 \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.1 Q1: Does PEAP have the same attention?"}, {"figure_path": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/106_full.png", "caption": "Figure 7: An example input of GSM8K dataset, using Direct Prompt.", "description": "\uadf8\ub9bc 7\uc740 \ub17c\ubb38\uc758 \uc2e4\ud5d8 \ubd80\ubd84\uc5d0\uc11c \uc0ac\uc6a9\ub41c GSM8K \ub370\uc774\ud130\uc14b\uc758 \uc608\uc2dc \uc785\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Direct Prompt \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubb38\uc81c\uac00 \uc81c\uc2dc\ub41c \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\ub294 \uc218\ud559 \ubb38\uc81c\uc640 \ud568\uaed8 \ubb38\uc81c\ub97c \ud478\ub294 \ub370 \ud544\uc694\ud55c \ud15c\ud50c\ub9bf\uacfc \ub2f5\ubcc0 \ud615\uc2dd\uc774 \ud568\uaed8 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub294 \ubaa8\ub378\uc774 GSM8K \ub370\uc774\ud130\uc14b\uc758 \ubb38\uc81c\ub97c \uc774\ud574\ud558\uace0 \ub2f5\uc744 \uc0dd\uc131\ud558\ub294 \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.  \ubb38\uc81c \uc790\uccb4\ub294 \uac04\ub2e8\ud55c \uc218\ud559 \ubb38\uc81c\uc774\uc9c0\ub9cc, \ubaa8\ub378\uc774 \uc774\ub97c \uc5b4\ub5bb\uac8c \ud574\uc11d\ud558\uace0 \ub2f5\uc744 \uc0dd\uc131\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "3. Experiments"}]