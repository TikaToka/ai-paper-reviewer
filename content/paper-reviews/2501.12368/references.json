{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to RLHF, a core concept in the current paper's approach to improving LVLMs."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-06", "reason": "This paper introduces the PPO algorithm, which the current paper uses for training its IXC-2.5-Chat model."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "CLIP, introduced in this paper, is a significant component in the development of multi-modal LVLMs, which are the focus of the current paper."}, {"fullname_first_author": "Nathan Lambert", "paper_title": "RewardBench: Evaluating reward models for language modeling", "publication_date": "2024-03-13", "reason": "This paper introduces a benchmark dataset used for evaluating the performance of the current paper's IXC-2.5-Reward model."}, {"fullname_first_author": "Lei Li", "paper_title": "VLRewardBench: A challenging benchmark for vision-language generative reward models", "publication_date": "2024-11-17", "reason": "This paper introduces another benchmark dataset that is specifically designed for evaluating multi-modal reward models, and is used to evaluate the current model."}]}