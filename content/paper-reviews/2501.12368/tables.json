[{"content": "| Category | Dataset |\n|---|---| \n| **Text** |  | \n| IF General | Tulu-3-IF-augmented-on-policy-8b [40], UltraFeedback [17] |\n| Safety | hhh alignment [5], PKU-Safe [18], SHP [24], Anthropic-hhrlhf [6] |\n| **Image** |  | \n| Chat | WildVision-Battle [62] |\n| General | LLaVA-Critic [94], VL-Feedback [44], RLAIF-V [101], MIA-DPO [54] |", "caption": "Table 3: Evaluation results on VLRewardBench [43]. The best and second-best results for proprietary models and open-source models are highlighted in bold and underlined, respectively.", "description": "\ud45c 3\uc740 VLRewardBench [43]\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub3c5\uc810 \ubaa8\ub378\uacfc \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uac01 \ubaa8\ub378\uc758 \uc804\ubc18\uc801\uc778 \uc815\ud655\ub3c4\uc640 \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5(\uc77c\ubc18\uc801\uc778 \uc774\ud574, \ud658\uac01 \uac10\uc18c, \ucd94\ub860)\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub3c5\uc810 \ubaa8\ub378\uacfc \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378 \uac01\uac01\uc5d0 \ub300\ud574 \ucd5c\uace0 \uc131\ub2a5\uacfc \ub450 \ubc88\uc9f8\ub85c \ub192\uc740 \uc131\ub2a5\uc744 \ub2ec\uc131\ud55c \uacb0\uacfc\ub294 \uad75\uc740 \uae00\uc528\uc640 \ubc11\uc904\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub3c5\uc810 \ubaa8\ub378\uacfc \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378 \uac04\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uba85\ud655\ud788 \ud30c\uc545\ud558\uace0, IXC-2.5-Reward \ubaa8\ub378\uc758 \uacbd\uc7c1\ub825\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. IXC2.5-Reward"}, {"content": "| Category | Dataset |\n|---|---| \n| **Image** |  | \n| IF General | in-house (will release) |\n|  | KVQA [76], A-OKVQA [73], PMC-VQA [114] |\n| **Text-Rich** |  | \n|  | AI2D [37], IconQA [56], TQA [38] |\n|  | ChartQA [63], DVQA [36], ScienceQA [57] |\n| **Reasoning** |  | \n|  | GeoQA [11], CLEVR-Math [47] |\n|  | Super-CLEVR [45], TabMWP [58] |\n| **Video** |  | \n| General | TrafficQA [96], FunQA [93], MiraData [35] |", "caption": "Table 4: Evaluation results on Reward Bench [41]. We report the performance of selective representative language-only RMs and previous multi-modal generative RMs.", "description": "\ud45c 4\ub294 Reward Bench [41]\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc120\ubcc4\ub41c \ub300\ud45c\uc801\uc778 \uc5b8\uc5b4 \uc804\uc6a9 \ubcf4\uc0c1 \ubaa8\ub378(RMs)\uacfc \uc774\uc804\uc758 \ub2e4\uc911 \ubaa8\ub4dc \uc0dd\uc131\ud615 RMs\uc758 \uc131\ub2a5\uc744 \ubcf4\uace0\ud569\ub2c8\ub2e4.  \uc5b8\uc5b4 \uc804\uc6a9 \ubaa8\ub378\uacfc \ub2e4\uc911 \ubaa8\ub4dc \uc0dd\uc131 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \ub2e4\uc911 \ubaa8\ub4dc \ubcf4\uc0c1 \ubaa8\ub378\uc758 \ud6a8\uacfc\uc131\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \uac83\uc785\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ucc44\ud305, \ucc44\ud305 \uc5b4\ub824\uc6c0, \uc548\uc804, \ucd94\ub860 \uc131\ub2a5\uc5d0 \ub300\ud55c \uc810\uc218\ub97c \ubcf4\uc5ec\uc8fc\uba70, \ud3c9\uade0 \uc810\uc218\ub97c \ud1b5\ud574 \uc885\ud569\uc801\uc778 \uc131\ub2a5\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.", "section": "5.1.2 Reward Bench \ubc0f RM-Bench\uc5d0 \ub300\ud55c \uacb0\uacfc"}, {"content": "| Models | #Param | General | Hallucination | Reasoning | Overall Acc | Macro Acc |\n|---|---|---|---|---|---|---|\n| <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Proprietary Models</span> |  |  |  |  |  |  |\n| Gemini-1.5-Flash (2024-09-24) [83] | - | 47.8 | 59.6 | 58.4 | 57.6 | 55.3 |\n| Gemini-1.5-Pro (2024-09-24) [83] | - | 50.8 | 72.5 | 64.2 | 67.2 | 62.5 |\n| Claude-3.5-Sonnet (2024-06-22) [4] | - | 43.4 | 55.0 | 62.3 | 55.3 | 53.6 |\n| GPT-4o-mini (2024-07-18) [3] | - | 41.7 | 34.5 | 58.2 | 41.5 | 44.8 |\n| GPT-4o (2024-08-06) [3] | - | 49.1 | 67.6 | 70.5 | 65.8 | 62.4 |\n| <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Open-Source Models</span> |  |  |  |  |  |  |\n| LLaVA-OneVision-7B-ov [42] | 7B | 32.2 | 20.1 | 57.1 | 29.6 | 36.5 |\n| Qwen2-VL-7B [90] | 7B | 31.6 | 19.1 | 51.1 | 28.3 | 33.9 |\n| Molmo-7B [20] | 7B | 31.1 | 31.8 | 56.2 | 37.5 | 39.7 |\n| InternVL2-8B [85] | 8B | 35.6 | 41.1 | 59.0 | 44.5 | 45.2 |\n| LLaVA-Critic-8B [94] | 8B | 54.6 | 38.3 | 59.1 | 41.2 | 44.0 |\n| Llama-3.2-11B [84] | 11B | 33.3 | 38.4 | 56.6 | 42.9 | 42.8 |\n| Pixtral-12B [1] | 12B | 35.6 | 25.9 | 59.9 | 35.8 | 40.4 |\n| Molmo-72B [20] | 72B | 33.9 | 42.3 | 54.9 | 44.1 | 43.7 |\n| Qwen2-VL-72B [90] | 72B | 38.1 | 32.8 | 58.0 | 39.5 | 43.0 |\n| NVLM-D-72B [19] | 72B | 38.9 | 31.6 | 62.0 | 40.1 | 44.1 |\n| Llama-3.2-90B [84] | 90B | 42.6 | 57.3 | 61.7 | 56.2 | 53.9 |\n| IXC-2.5-Reward (Ours) | 7B | 84.7 | 62.5 | 62.9 | 65.8 | 70.0 |", "caption": "Table 5: Evaluation results on RM-Bench [51]. We classify reward models into three types: sequence classifiers (Seq.), generative models, and implicit DPO models. Performance is reported across four domains (Chat, Math, Code, Safety) and three difficulty levels (Easy, Normal, Hard), along with average scores (Avg).", "description": "\ud45c 5\ub294 RM-Bench [51]\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubcf4\uc0c1 \ubaa8\ub378\uc740 \uc2dc\ud000\uc2a4 \ubd84\ub958\uae30(Seq.), \uc0dd\uc131 \ubaa8\ub378, \uc554\uc2dc\uc801 DPO \ubaa8\ub378\uc758 \uc138 \uac00\uc9c0 \uc720\ud615\uc73c\ub85c \ubd84\ub958\ub429\ub2c8\ub2e4. \uc131\ub2a5\uc740 \ucc44\ud305, \uc218\ud559, \ucf54\ub4dc, \uc548\uc804\uc758 \ub124 \uac00\uc9c0 \uc601\uc5ed\uacfc \uc26c\uc6c0, \ubcf4\ud1b5, \uc5b4\ub824\uc6c0\uc758 \uc138 \uac00\uc9c0 \ub09c\uc774\ub3c4 \uc218\uc900\uc5d0 \uac78\uccd0 \ud3c9\uac00\ub418\uba70, \ud3c9\uade0 \uc810\uc218(Avg)\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ub429\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc740 \uac01 \uc601\uc5ed \ubc0f \ub09c\uc774\ub3c4\ubcc4\ub85c \uc138\ubd84\ud654\ub418\uc5b4 \ud45c\uc2dc\ub429\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ubaa8\ub378\uacfc \ub09c\uc774\ub3c4\uc5d0\uc11c\uc758 \uc0c1\ub300\uc801 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.1.1 VL-RewardBench \uacb0\uacfc"}, {"content": "| Model Name | LLM | Chat | Chat Hard | Safety | Reasoning | Avg Score |\n|---|---|---|---|---|---|---|\n| <br> *Language-Only Reward Models* |  |  |  |  |  |  |\n| InternLM2-7B-Reward [8] | InternLM2-7B | 99.2 | 69.5 | 87.2 | 94.5 | 87.6 |\n| InternLM2-20B-Reward [8] | InternLM2-20B | 98.9 | 76.5 | 89.5 | 95.8 | 90.2 |\n| Skyword-Reward-Llama3.1-8B [48] | Llama3.1-8B | 95.8 | 87.3 | 90.8 | 96.2 | 92.5 |\n| INF-ORM-Llama3.1-70B [97] | Llama3.1-70B | 96.6 | 91.0 | 93.6 | 99.1 | 95.1 |\n| <br> *Multi-Modal Reward Models* |  |  |  |  |  |  |\n| QWen2-VL-7B [90] | QWen2-7B | 96.6 | 57.0 | 73.9 | 94.3 | 83.8 |\n| LLaVA-Critic-8B [94] | LLaMA3-7B | 96.9 | 52.8 | 81.7 | 83.5 | 80.0 |\n| IXC-2.5-Reward (Ours) | InternL2-7B | 90.8 | 83.8 | 87.8 | 90.0 | 88.6 |", "caption": "Table 6: Evaluation results of our IXC-2.5-Chat model against previous SOTA proprietary and open-source models \u2264\\leq\u226410B (results are copied from OpenVLM Leaderboard and Open LMM Reasoning Leaderboard, accessed 01-Jan-2025).\nBest and second best results are highlighted.", "description": "\ud45c 6\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c IXC-2.5-Chat \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 100\uc5b5 \ud30c\ub77c\ubbf8\ud130 \uc774\ud558\uc758 \uae30\uc874 \ucd5c\uace0 \uc131\ub2a5(SOTA) \ub3c5\uc810 \ubaa8\ub378 \ubc0f \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378\uacfc \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  OpenVLM \ub9ac\ub354\ubcf4\ub4dc\uc640 Open LMM \ucd94\ub860 \ub9ac\ub354\ubcf4\ub4dc(2025\ub144 1\uc6d4 1\uc77c \uc811\uadfc)\uc758 \ub370\uc774\ud130\ub97c \ubc14\ud0d5\uc73c\ub85c \ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uc9c0\uc2dc\uc0ac\ud56d \ub530\ub974\uae30, \uc9c0\uc2dd, \ucd94\ub860, \ud14d\uc2a4\ud2b8 \ud48d\ubd80 \uc601\uc5ed \ub4f1 \ub2e4\uc591\ud55c \ubc94\uc8fc\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uba70, \ucd5c\uace0 \ubc0f \ucc28\uc21c\uc704 \uc131\ub2a5\uc774 \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.2 \ud3c9\uac00 \uacb0\uacfc: IXC-2.5-Chat"}, {"content": "| Model Name | Type | Chat | Math | Code | Safety | Easy | Normal | Hard | Avg |\n|---|---|---|---|---|---|---|---|---|---| \n| <span class=\"ltx_text\" style=\"font-size:90%;\">Language-Only Reward Models</span> |  |  |  |  |  |  |  |  |  |\n| Tulu-2-dpo-13b [32] | Implicit | 66.4 | 51.4 | 51.8 | 85.4 | 86.9 | 66.7 | 37.7 | 63.8 |\n| InternLM2-7B-Reward [8] | Seq. | 61.7 | 71.4 | 49.7 | 85.5 | 85.4 | 70.7 | 45.1 | 67.1 |\n| InternLM2-20B-Reward [8] | Seq. | 63.1 | 66.8 | 56.7 | 86.5 | 82.6 | 71.6 | 50.7 | 68.3 |\n| Nemotron-4-340B-Reward [92] | Generative | 71.2 | 59.8 | 59.4 | 87.5 | 81.0 | 71.4 | 56.1 | 69.5 |\n| URM-LLaMa-3.1-8B [55] | Seq. | 71.2 | 61.8 | 54.1 | 93.1 | 84.0 | 73.2 | 53.0 | 70.0 |\n| Skyword-Reward-Llama3.1-8B [48] | Seq. | 69.5 | 60.6 | 54.5 | 95.7 | 89.0 | 74.7 | 46.6 | 70.1 |\n| <span class=\"ltx_text\" style=\"font-size:90;\">Multi-Modal Reward Models</span> |  |  |  |  |  |  |  |  |  |\n| IXC-2.5-Reward (Ours) | Seq. | 65.5 | 55.9 | 51.7 | 93.8 | 87.5 | 71.3 | 47.4 | 68.8 |", "caption": "Table 7: Ablation Studies of the impact of response length constraints of reward models that guided training IXC-2.5-Chat.", "description": "\ud45c 7\uc740 IXC-2.5-Chat \ubaa8\ub378 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ubcf4\uc0c1 \ubaa8\ub378\uc758 \uc751\ub2f5 \uae38\uc774 \uc81c\uc57d \uc870\uac74\uc774 \ubbf8\uce58\ub294 \uc601\ud5a5\uc5d0 \ub300\ud55c \ucd94\uac00 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc751\ub2f5 \uae38\uc774 \uc81c\uc57d \uc870\uac74\uc744 \uc801\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\uc640 \ube44\uad50\ud558\uc5ec  WildVision, MIA, MM-MT, MM-Vet v2 \ub4f1 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uc751\ub2f5 \uae38\uc774 \uc81c\uc57d\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ubd84\uc11d\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. IXC-2.5-Reward\uc758 \uc138 \uac00\uc9c0 \ud65c\uc6a9 \uc0ac\ub840"}, {"content": "| Category | Benchmark | Evaluation | Proprietary API | Previous-SOTA | Previous-SOTA | IXC-2.5 | IXC-2.5-Chat |\n|---|---|---|---|---|---|---|---| \n| Instruction | WildVision<sub>(0617)</sub> [62] | Open | 89.2 [31] | 67.3 [94] | 37.5 | 74.6 |\n| Following | MIA<sub>(val)</sub> [68] | Open | 88.6 [31] | 80.7 [90] | 80.4 | 84.0 |\n| & Chat | MM-MT<sub>(val)</sub> [1] | Open | 7.72 [31] | 5.45 [90] | 3.85 | 5.70 |\n|  | MM-Vet v2<sub>(0613)</sub> [103] | Open | 71.8 [4] | 58.1 [14] | 45.8 | 54.8 |\n| Knowledge | MMBench<sub>(v1.1)</sub> [52] | MCQ | 85.7 [74] | 82.7 [61] | 79.4 | 79.0 |\n|  | MMMU<sub>(val)</sub> [106] | MCQ | 70.7 [31] | 56.2 [14] | 42.9 | 44.1 |\n|  | MMStar [12] | MCQ | 72.7 [74] | 63.2 [14] | 59.9 | 59.6 |\n| Reasoning | MathVista<sub>(mini)</sub> [59] | VQA | 78.4 [74] | 66.5 [60] | 63.7 | 63.4 |\n|  | MathVerse<sub>(vision-only)</sub> [113] | VQA | 54.8 [26] | 26.6 [50] | 16.2 | 19.0 |\n|  | MathVision<sub>(full)</sub> [89] | VQA | 43.6 [26] | 22.0 [50] | 17.8 | 18.8 |\n| Text-Rich | TextVQA<sub>(val)</sub> [79] | VQA | 82.0 [64] | 78.5 [42] | 78.2 | 81.3 |\n|  | ChartQA<sub>(test)</sub> [63] | VQA | 81.2 [64] | 82.4 [99] | 82.2 | 80.5 |\n|  | OCRBench [49] | VQA | 89.4 [74] | 82.2 [14] | 69.0 | 70.0 |", "caption": "Table 8: Results of Best-of-N\ud835\udc41Nitalic_N (BoN) sampling for test-time scaling with IXC-2.5-Reward.", "description": "\ud45c 8\uc740 IXC-2.5-Reward\ub97c \uc0ac\uc6a9\ud55c \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \ud655\uc7a5\uc744 \uc704\ud55c Best-of-N (BoN) \uc0d8\ud50c\ub9c1 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  N \uac12(\ud6c4\ubcf4 \uc751\ub2f5 \uac1c\uc218)\uc744 \ubcc0\ud654\uc2dc\ud0a4\uba74\uc11c \ud3c9\uade0 \ud1a0\ud070 \uc218, WildVision, MIA, MM-MT, MM-Vet v2 \ubca4\uce58\ub9c8\ud06c \uc810\uc218 \ubcc0\ud654\ub97c \ud655\uc778\ud569\ub2c8\ub2e4.  BoN \uc0d8\ud50c\ub9c1\uc744 \ud1b5\ud574 \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \ud2b9\ud788, \ub2e8\uc21c\ud788 \uc751\ub2f5 \uae38\uc774\ub97c \ub298\ub9ac\ub294 \uac83\uc774 \uc544\ub2c8\ub77c \uc2e4\uc81c\ub85c \ub354 \uc9c8 \ub192\uc740 \uc751\ub2f5\uc744 \uc120\ud0dd\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub418\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2 IXC-2.5-Reward for Test-Time Scaling"}]