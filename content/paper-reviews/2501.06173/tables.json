[{"content": "| Datasets | Modality | Type | # Images | Text Length |\n|---|---|---|---|---|\n| Flintstones | Image | Comic | 122k | 86 |\n| Pororo | Image | Comic | 74k | 74 |\n| StorySalon | Image | Comic | 160k | 106 |\n| StoryStream | Image | Comic | 258k | 146 |\n| VIST | Image | Real world | 210K | ~70 |\n| CookGen | Video | Real world | 39M | 763.8 |", "caption": "Table 1: Comparison with multi-modal narrative datasets. Most existing datasets focus on image-based comic story generation. In contrast, our dataset consists of long narrative videos, containing 150\u00d7\\times\u00d7 the number of frames and 5\u00d7\\times\u00d7 the dense text annotations compared to the previous largest dataset, StoryStream.", "description": "\ud45c 1\uc740 \uae30\uc874\uc758 \ub2e4\uc911 \ubaa8\ub4dc \ub0b4\ub7ec\ud2f0\ube0c \ub370\uc774\ud130\uc14b\uacfc \uc81c\uc548\ub41c CookGen \ub370\uc774\ud130\uc14b\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. \uae30\uc874 \ub370\uc774\ud130\uc14b\ub4e4\uc740 \uc8fc\ub85c \uc774\ubbf8\uc9c0 \uae30\ubc18\uc758 \ub9cc\ud654 \uc2a4\ud1a0\ub9ac \uc0dd\uc131\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd98 \ubc18\uba74, CookGen \ub370\uc774\ud130\uc14b\uc740 \uae34 \uc11c\uc0ac\uc801 \ube44\ub514\uc624\ub97c \ud3ec\ud568\ud558\uba70, \uc774\uc804 \ucd5c\ub300 \ub370\uc774\ud130\uc14b\uc778 StoryStream\uc5d0 \ube44\ud574 \ud504\ub808\uc784 \uc218\ub294 150\ubc30, \ud14d\uc2a4\ud2b8 \uc8fc\uc11d\uc758 \ubc00\ub3c4\ub294 5\ubc30 \ub354 \ub192\uc2b5\ub2c8\ub2e4.  \uc774\ub294 \uae34 \uc11c\uc0ac\uc801 \ube44\ub514\uc624 \uc0dd\uc131\uc744 \uc704\ud55c \uace0\ud488\uc9c8 \ub370\uc774\ud130\uc758 \ubd80\uc871\uc774\ub77c\ub294 \uae30\uc874 \uc5f0\uad6c\uc758 \uc5b4\ub824\uc6c0\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574 CookGen \ub370\uc774\ud130\uc14b\uc774 \uc81c\uc548\ub418\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Long Narrative Video Data"}, {"content": "| Data Source | # Vid. (train/val) | # Clips | Clip Len. | # Clips / Vid. |\n|---|---|---|---|---|\n| YouCook2 | 1333 / 457 | ~10K | 19.6s | 7.7 |\n| HowTo100M (subset) | 30039 / 933 | ~183K | 9.5s | 5.9 |", "caption": "Table 2: Long narrative dataset sources. Our dataset is built upon Youcook2 and a cooking subset of Howto100M.", "description": "\ud45c 2\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc694\ub9ac \uad00\ub828 \ube44\ub514\uc624 \ub370\uc774\ud130\uc14b\uc758 \ucd9c\ucc98\uc640 \uad6c\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubcf8 \ub17c\ubb38\uc758 \ub370\uc774\ud130\uc14b\uc740 YouCook2\uc640 HowTo100M\uc758 \uc694\ub9ac \uad00\ub828 \ubd80\ubd84\uc9d1\ud569\uc744 \uae30\ubc18\uc73c\ub85c \uc81c\uc791\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ub370\uc774\ud130\uc14b\uc758 \ube44\ub514\uc624 \uc218, \ud559\uc2b5 \ubc0f \uac80\uc99d\uc5d0 \uc0ac\uc6a9\ub41c \ube44\ub514\uc624 \uc218, \ud074\ub9bd \uc218, \ud074\ub9bd \uae38\uc774, \ube44\ub514\uc624\ub2f9 \ud074\ub9bd \uc218 \ub4f1\uc758 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub370\uc774\ud130\uc14b\uc758 \uaddc\ubaa8\uc640 \ud2b9\uc9d5\uc744 \uac04\ub7b5\ud558\uac8c \uc694\uc57d\ud558\uc5ec, \ud6c4\uc18d \uc2e4\ud5d8\uc758 \uae30\ubc18\uc774 \ub418\ub294 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ub3d5\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "3. Long Narrative Video Data"}, {"content": "| Validation Set | w/. GT keyframe | W/o. GT keyframe |\n|---|---|---|\n| # Clips | FVD | FVD |\n| 5504 | **116.3** | 561.1 |", "caption": "Table 3: Inverse video generation. Evaluation of caption quality through inverse video generation with and without keyframes. FVD scores reflect reasonable video reconstruction quality.", "description": "\uc774 \ud45c\ub294 \uc5ed\ubc29\ud5a5 \ube44\ub514\uc624 \uc0dd\uc131\uc744 \ud1b5\ud574 \ube44\ub514\uc624 \ucea1\uc158\uc758 \uc9c8\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud0a4\ud504\ub808\uc784\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0\uc640 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0 \ub450 \uac00\uc9c0 \uc0c1\ud669\uc5d0\uc11c \uc5ed\ubc29\ud5a5 \ube44\ub514\uc624 \uc0dd\uc131\uc744 \uc218\ud589\ud558\uc5ec \uc6d0\ubcf8 \ube44\ub514\uc624\ub97c \uc7ac\uad6c\uc131\ud558\ub294 \ud488\uc9c8\uc744 \ud3c9\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4. FVD (Fr\u00e9chet Video Distance) \uc810\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc7ac\uad6c\uc131\ub41c \ube44\ub514\uc624\uc758 \ud488\uc9c8\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \uce21\uc815\ud558\uc600\uc2b5\ub2c8\ub2e4. \ub0ae\uc740 FVD \uc810\uc218\ub294 \ub192\uc740 \ud488\uc9c8\uc758 \uc7ac\uad6c\uc131\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \ud45c\ub294 \ud0a4\ud504\ub808\uc784\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c\uc640 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\uc758 FVD \uc810\uc218\ub97c \ube44\uad50\ud558\uc5ec \ucea1\uc158\uc758 \uc9c8\uacfc \ud0a4\ud504\ub808\uc784\uc758 \uc911\uc694\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.3.1 Inverse Video Generation"}, {"content": "| Score (0-100) | GPT-4o Evaluation |  | Human Evaluation |  | \n|---|---|---|---|---| \n|  | Qwen2-VL-72B | Ours | Qwen2-VL-72B | Ours | \n| 98.0 | 95.2 | **98.0** | 79.3 | **82.0** | ", "caption": "Table 4: Caption Quality Evaluation. We compare the caption quality between our captioner and the Qwen2-VL-72B model by both GPT-4o and human annotators. Our model achieves competitive results despite a much smaller model size.", "description": "\ud45c 4\ub294 \uc81c\uc548\ub41c captioner\uc640 Qwen2-VL-72B \ubaa8\ub378\uc758 caption \ud488\uc9c8\uc744 GPT-4\uc640 \uc0ac\ub78c \ud3c9\uac00\uc790\uc758 \ub450 \uac00\uc9c0 \uad00\uc810\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  \ubaa8\ub378 \ud06c\uae30\uac00 \ud6e8\uc52c \uc791\uc74c\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0, \uc81c\uc548\ub41c captioner\uac00 Qwen2-VL-72B \ubaa8\ub378\uacfc \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  GPT-4\uc640 \uc0ac\ub78c \ud3c9\uac00\uc790 \ubaa8\ub450\uc5d0\uac8c\uc11c \uacbd\uc7c1\ub825 \uc788\ub294 \uacb0\uacfc\ub97c \uc5bb\uc5c8\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c\ub294 caption\uc758 \uc644\uc131\ub3c4\uc640 hallucination \uc5ec\ubd80\ub97c \ud3c9\uac00 \uc9c0\ud45c\ub85c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.", "section": "3.3 \ud3c9\uac00: \uc0dd\uc131 \ubc0f \uc774\ud574"}, {"content": "| Method | Autoencoder Style | VL Aligned. | Recon. Ability | CLIP-T | FID |\n|---|---|---|---|---|---| \n| SDXL-VAE | Variational U-Net | \u2717 | High | 13.2 | 286.6 |\n| EMU-2 | CLIP-Diffusion | \u2713 | Medium | **25.4** | 76.7 |\n| SEED-X | CLIP-Diffusion | \u2713 | Low | 25.1 | **30.1** |", "caption": "Table 5: Visual latent spaces for visual regression. The VAE latent space is challenging for auto-regressive models to regress in a single step due to its limited correlation with language. In contrast, the language-aligned latent spaces (EMU-2 and SEED-X) allow for easier and effective regression in an interleaved manner.", "description": "\ud45c 5\ub294 \uc2dc\uac01\uc801 \ud68c\uadc0\ub97c \uc704\ud55c \uc2dc\uac01\uc801 \uc7a0\uc7ac \uacf5\uac04\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc5b8\uc5b4\uc640\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \uc81c\ud55c\uc801\uc774\uae30 \ub54c\ubb38\uc5d0 VAE \uc7a0\uc7ac \uacf5\uac04\uc740 \ub2e8\uc77c \ub2e8\uacc4\uc5d0\uc11c \uc790\uae30 \ud68c\uadc0 \ubaa8\ub378\uc774 \ud68c\uadc0\ud558\uae30 \uc5b4\ub835\uc2b5\ub2c8\ub2e4. \ubc18\uba74\uc5d0 EMU-2 \ubc0f SEED-X\uc640 \uac19\uc740 \uc5b8\uc5b4 \uc815\ub82c \uc7a0\uc7ac \uacf5\uac04\uc740 \uc0c1\ud638 \uc791\uc6a9 \ubc29\uc2dd\uc73c\ub85c \ub354 \uc27d\uace0 \ud6a8\uacfc\uc801\uc778 \ud68c\uadc0\ub97c \ud5c8\uc6a9\ud569\ub2c8\ub2e4.  \uc989, VAE\ub294 \uc774\ubbf8\uc9c0 \uc7ac\uad6c\uc131 \uc131\ub2a5\uc740 \uc6b0\uc218\ud558\uc9c0\ub9cc \uc5b8\uc5b4\uc640\uc758 \uc5f0\uad00\uc131\uc774 \ub5a8\uc5b4\uc838 \uc2dc\uac01\uc801 \uc0dd\uc131 \ubaa8\ub378\uc5d0 \uc801\uc6a9\ud558\uae30 \uc5b4\ub835\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubc18\uba74 CLIP \uae30\ubc18\uc758 EMU-2 \uc640 SEED-X\ub294 \uc2dc\uac01-\uc5b8\uc5b4 \ud1b5\ud569\uc774 \uc798 \ub418\uc5b4 \uc788\uc5b4 \uc790\uae30 \ud68c\uadc0 \ubaa8\ub378\uc5d0 \uc801\uc6a9\uc774 \uc6a9\uc774\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc7a0\uc7ac \uacf5\uac04\uc5d0\uc11c\uc758 \ud68c\uadc0 \uc131\ub2a5 \ucc28\uc774\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uc5b4\ub5a4 \ubc29\uc2dd\uc758 \uc7a0\uc7ac \uacf5\uac04\uc774 \uc790\uae30 \ud68c\uadc0 \uc2dc\uac01\uc801 \uc0dd\uc131 \ubaa8\ub378\uc5d0 \uc801\ud569\ud55c\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "5.2. Interleaved Narrative Director"}, {"content": "| Loss Type |  | SEED-X Latent |  |  |  | EMU-2 Latent |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|\n|  |  | Training |  | Validation |  | Training |  | Validation |  |\n| MSE | Cos. | L2 Dist. | Cosine. | CLIP | FID | L2 Dist. | Cosine. | CLIP | FID |\n| \u2713 | \u2717 | 0.41 | 0.82 | 23.6 | 31.9 | **1.3** | 0.78 | 25.1 | 80.1 |\n| \u2717 | \u2713 | 1.1 | 0.82 | 24.1 | 32.1 | 2.5 | 0.79 | 23.5 | 115.3 |\n| \u2713 | \u2713 | **0.41** | **0.83** | **25.1** | **30.1** | 1.4 | **0.79** | **25.4** | **76.7** |", "caption": "Table 6: Regression loss with scale and direction. We track the training convergence and evaluate models with the CLIP-T and FID metrics on the validation set. Both Seed-X and EMU-2 latent space show that a combination of both MSE loss and Cosine Similarity loss considering both scale and direction performs best. SEED-X and EMU-2 original regression loss setting is grayed.", "description": "\ud45c 6\uc740 \ud06c\uae30\uc640 \ubc29\ud5a5\uc744 \uace0\ub824\ud55c \ud68c\uadc0 \uc190\uc2e4\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud6c8\ub828 \uacfc\uc815\uc758 \uc218\ub834 \ucd94\uc138\ub97c \ucd94\uc801\ud558\uace0, \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c CLIP-T \ubc0f FID \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. Seed-X \ubc0f EMU-2 \uc7a0\uc7ac \uacf5\uac04 \ubaa8\ub450 MSE \uc190\uc2e4\uacfc \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4 \uc190\uc2e4\uc744 \ud568\uaed8 \uc0ac\uc6a9\ud558\uc5ec \ud06c\uae30\uc640 \ubc29\ud5a5\uc744 \ubaa8\ub450 \uace0\ub824\ud588\uc744 \ub54c \ucd5c\uc0c1\uc758 \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. Seed-X\uc640 EMU-2\uc758 \uc6d0\ub798 \ud68c\uadc0 \uc190\uc2e4 \uc124\uc815\uc740 \ud68c\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.2. Interleaved Narrative Director"}, {"content": "| Regression Task |  | Training |  | Validation |  |\n|---|---|---|---|---|---|---|\n| Action \u2192 Vis. Embed. |  | L2 Dist. | Cosine Sim. | CLIP-T | FID |  |\n|  |  | 0.43 | 0.82 | 22.7 | 27.9 |  |\n| Caption \u2192 Vis. Embed. |  | 0.41 | 0.82 | 25.7 | 26.1 |  |\n| Action \u2192 Caption \u2192 Vis. Embed. |  | **0.41** | **0.83** | **26.1** | **25.3** |  |", "caption": "Table 7: From \u201cActions\u201d to \u201cVisual States\u201d. We report the L2 distance and cosine similarity scores for tracking the training convergence and evaluate the generation images with CLIP score and FID score. Models are trained and evaluated on the collected Howto100M subset. SEED-X latent is used for visual regression.", "description": "\ud45c 7\uc740 \"\ud589\ub3d9(Actions)\"\uc5d0\uc11c \"\uc2dc\uac01\uc801 \uc0c1\ud0dc(Visual States)\"\ub85c\uc758 \uc804\uc774 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud6c8\ub828 \uacfc\uc815\uc758 \uc218\ub834\uc744 \ucd94\uc801\ud558\uae30 \uc704\ud574 L2 \uac70\ub9ac\uc640 \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4 \uc810\uc218\ub97c \ubcf4\uace0\ud558\uba70, \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\ub294 CLIP \uc810\uc218\uc640 FID \uc810\uc218\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4. \ubaa8\ub378\uc740 \uc218\uc9d1\ub41c HowTo100M \ud558\uc704 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828 \ubc0f \ud3c9\uac00\ub418\uc5c8\uace0, \uc2dc\uac01\uc801 \ud68c\uadc0\uc5d0\ub294 SEED-X \uc7a0\uc7ac \ubca1\ud130\uac00 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "4.1.1 Interleaved Image-Text Director"}, {"content": "| Latent condition | Gen. strategy | Aesthetic | Realistic | Visual consist. | Narrative |\n|---|---|---|---|---|---|---|\n| EMU-2 Latent | Interleaved | 0.7 | 1.2 | 2.9 | 2.2 |\n| SEED-X Latent | Interleaved | 2.1 | 4.3 | 4.5 | 4.4 |\n| Text (SDXL) | Language-centric | 4.0 | 2.9 | 3.3 | 4.0 |\n| Text (FLUX.1-s) | Language-centric | 4.8 | 3.1 | 3.4 | 4.4 |", "caption": "Table 8: Human Evaluation \u2013 Interleaved vs. Language-centric. This table compares different latent conditioning and generation strategies based on semantic alignment, aesthetic quality, visual consistency, and narrative coherence. Each aspect is scored with five tiers: 1\u223csimilar-to\\sim\u223c5, score higher is better.", "description": "\ud45c 8\uc740 \ub2e4\uc591\ud55c \uc7a0\uc7ac\uc801 \uc870\uac74\ud654 \ubc0f \uc0dd\uc131 \uc804\ub7b5\uc744 \uc758\ubbf8\uc801 \uc815\ub82c, \ubbf8\uc801 \ud488\uc9c8, \uc2dc\uac01\uc801 \uc77c\uad00\uc131 \ubc0f \uc11c\uc0ac\uc801 \uc77c\uad00\uc131\uc744 \uae30\uc900\uc73c\ub85c \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. \uac01 \uce21\uba74\uc740 1\uc810(\ube44\uc2b7\ud568)\uc5d0\uc11c 5\uc810(\ub9e4\uc6b0 \uc88b\uc74c)\uae4c\uc9c0 5\ub2e8\uacc4\ub85c \ud3c9\uac00\ub429\ub2c8\ub2e4. \uc810\uc218\uac00 \ub192\uc744\uc218\ub85d \uc88b\uc2b5\ub2c8\ub2e4.", "section": "5.2. Interleaved Narrative Director"}, {"content": "| Visual Condition | YouCook2 CLIP-T | YouCook2 FVD | HowTo100M CLIP-T | HowTo100M FVD |\n|---|---|---|---|---|\n| Keyframe | 25.9 | 557.7 | 26.6 | 541.1 |\n| Embedding | **26.4** | **512.6** | **27.3** | **520.7** |", "caption": "Table 9: Keyframes vs. Visual Embeddings. Evaluate CLIP-T and FVD scores for video generation conditioned on keyframes versus visual embeddings generated by our interleaved director.", "description": "\ud45c 9\ub294 \ud575\uc2ec \ud504\ub808\uc784\uacfc \ube44\uad50\ud558\uc5ec, \uc81c\uc548\ub41c \uc778\ud130\ub9ac\ube0c \ub514\ub809\ud130\uac00 \uc0dd\uc131\ud55c \uc2dc\uac01\uc801 \uc784\ubca0\ub529\uc744 \uc870\uac74\uc73c\ub85c \uc0dd\uc131\ub41c \ube44\ub514\uc624\uc5d0 \ub300\ud55c CLIP-T \ubc0f FVD \uc810\uc218\ub97c \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud575\uc2ec \ud504\ub808\uc784\uc744 \uc870\uac74\uc73c\ub85c \uc0ac\uc6a9\ud55c \uacbd\uc6b0\uc640 \uc81c\uc548\ub41c \uc778\ud130\ub9ac\ube0c \ub514\ub809\ud130\uac00 \uc0dd\uc131\ud55c \uc2dc\uac01\uc801 \uc784\ubca0\ub529\uc744 \uc870\uac74\uc73c\ub85c \uc0ac\uc6a9\ud55c \uacbd\uc6b0\uc758 \ube44\ub514\uc624 \uc0dd\uc131 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uac01 \uc870\uac74\uc5d0 \ub530\ub978 \ube44\ub514\uc624 \ud488\uc9c8 \ucc28\uc774\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \uc81c\uc2dc\ud569\ub2c8\ub2e4. CLIP-T \uc810\uc218\ub294 \ube44\ub514\uc624\uc758 \uc758\ubbf8\uc801 \uc77c\uad00\uc131\uc744, FVD \uc810\uc218\ub294 \ube44\ub514\uc624\uc758 \uc2dc\uac01\uc801 \ucda9\uc2e4\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "5.3. Visual-Conditioned Video Generation"}, {"content": "| Matching Tier | Action (Important Info.) | Object (Important Info.) | Score |\n|---|---|---|---|\n| Very Match | Good Coverage, No Hallucination | Good Coverage, No Hallucination | **100** |\n| Good Match | Good Coverage, Limited Hallucination | Good Coverage, Limited Hallucination | **85** |\n| Somehow Match | Fair Coverage, Some Hallucination | Fair Coverage, Some Hallucination | **70** |\n| Not Match | Little Coverage or High Hallucination | Little Coverage or High Hallucination | **0** |", "caption": "Table 10: Human Evaluation Matching Rules. Captions are rated based on coverage and hallucination levels, using four matching tiers.", "description": "\ud45c 10\uc740 \uc0ac\ub78c \ud3c9\uac00\uc790\ub4e4\uc774 \ube44\ub514\uc624 \uc790\ub9c9\uc758 \ud488\uc9c8\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ud55c \uaddc\uce59\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc790\ub9c9\uc740 \ub124 \uac00\uc9c0 \ub9e4\uce6d \ub4f1\uae09(\ub9e4\uc6b0 \uc77c\uce58, \uc591\ud638 \uc77c\uce58, \uc5b4\ub290 \uc815\ub3c4 \uc77c\uce58, \ubd88\uc77c\uce58)\uc73c\ub85c \ud3c9\uac00\ub418\uba70, \uac01 \ub4f1\uae09\uc740 \ube44\ub514\uc624 \uc694\uc18c\uc758 \uc801\uc6a9 \ubc94\uc704\uc640 \ud658\uac01 \uc218\uc900\uc5d0 \ub530\ub77c \uacb0\uc815\ub429\ub2c8\ub2e4.  \uc801\uc6a9 \ubc94\uc704\ub294 \uc790\ub9c9\uc774 \ube44\ub514\uc624\uc758 \ub0b4\uc6a9\uc744 \uc5bc\ub9c8\ub098 \uc798 \ud3ec\uad04\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\uace0, \ud658\uac01 \uc218\uc900\uc740 \uc790\ub9c9\uc5d0 \ube44\ub514\uc624\uc5d0 \uc5c6\ub294 \ub0b4\uc6a9\uc774 \uc5bc\ub9c8\ub098 \ud3ec\ud568\ub418\uc5b4 \uc788\ub294\uc9c0\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uac01 \ub9e4\uce6d \ub4f1\uae09\uc5d0 \ub300\ud55c \uc124\uba85\uacfc \uc810\uc218\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "C.4. Human Evaluation on Captions"}, {"content": "| Configuration | Setting |\n|---|---| \n| Image resolution | 448 \u00d7 448 |\n| Optimizer | AdamW |\n| Optimizer hyperparameters | \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.98, \u03f5 = 10\u207b\u2076 |\n| Peak learning rate | 2 \u00d7 10\u207b\u2074 |\n| Learning rate schedule | Linear warm-up, cosine decay |\n| Gradient clip | 1.0 |\n| Total training steps | 2,500 |\n| Warm-up steps | 200 |\n| Batch size | 512 |\n| Numerical precision | bfloat16 |\n| Training context pairs | [2, 8] |\n| Inference context pairs | 8 |", "caption": "Table 11: Implementation details of the interleaved auto-regressive model.", "description": "\ud45c 11\uc740 \ub17c\ubb38\uc758 4.1.1\uc808 \"Interleaved Image-Text Director\"\uc5d0\uc11c \uc81c\uc2dc\ub41c, \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8\uac00 \uad50\ucc28\uc801\uc73c\ub85c \uc0dd\uc131\ub418\ub294 \ubaa8\ub378\uc758 \uad6c\ud604 \uc138\ubd80 \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4, \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998(AdamW), \ud559\uc2b5\ub960, \ud559\uc2b5\ub960 \uc870\uc815 \ubc29\uc2dd(\uc120\ud615 \uc6cc\ubc0d\uc5c5, \ucf54\uc0ac\uc778 \uac10\uc1e0), \uadf8\ub798\ub514\uc5b8\ud2b8 \ud074\ub9ac\ud551 \uac12, \ucd1d \ud559\uc2b5 \ub2e8\uacc4, \uc6cc\ubc0d\uc5c5 \ub2e8\uacc4, \ubc30\uce58 \ud06c\uae30, \uc218\uce58 \uc815\ubc00\ub3c4, \ud559\uc2b5 \uc2dc \uc0ac\uc6a9\ud558\ub294 \ucee8\ud14d\uc2a4\ud2b8 \uc30d \uac1c\uc218, \ucd94\ub860 \uc2dc \uc0ac\uc6a9\ud558\ub294 \ucee8\ud14d\uc2a4\ud2b8 \uc30d \uac1c\uc218 \ub4f1 \ubaa8\ub378 \ud559\uc2b5 \ubc0f \ucd94\ub860 \uacfc\uc815\uc5d0 \ub300\ud55c \uc138\ubd80\uc801\uc778 \uc124\uc815 \uac12\ub4e4\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc815\ubcf4\ub294 \ubaa8\ub378\uc758 \uc7ac\ud604\uc131\uc744 \ud655\ubcf4\ud558\uace0 \ub2e4\ub978 \uc5f0\uad6c\uc790\ub4e4\uc774 \ub3d9\uc77c\ud55c \uc2e4\ud5d8\uc744 \uc7ac\ud604\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "4. Method"}, {"content": "| Configuration | Setting |\n|---|---| \n| Image/Video resolution | 448x448xT |\n| Optimizer | AdamW |\n| Optimizer hyperparameters | \u03b2\u2081=0.9,\u03b2\u2082=0.95,\u03f5=10\u207b\u2078 |\n| Peak learning rate | 1x10\u207b\u2075 |\n| Learning rate schedule | Linear warm-up, constant |\n| Gradient clip | 1.0 |\n| Total training steps | 20,000 |\n| Warm-up steps | 1,000 |\n| Batch size | 64 |\n| Numerical precision | bfloat16 |", "caption": "Table 12: Implementation details of the visual-conditioned video generation model.", "description": "\ud45c 12\ub294 \ubcf8 \ub17c\ubb38\uc758 4.2\uc808 \"Visual-Conditioned Video Generation\" \uc139\uc158\uc5d0 \ud3ec\ud568\ub41c \ud45c\ub85c, \uc2dc\uac01\uc801 \uc870\uac74\ud654 \ube44\ub514\uc624 \uc0dd\uc131 \ubaa8\ub378\uc758 \uad6c\ud604 \uc138\ubd80 \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, \uc774\ubbf8\uc9c0/\ube44\ub514\uc624 \ud574\uc0c1\ub3c4, \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998, \ucd5c\uc801\ud654 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130, \ucd5c\ub300 \ud559\uc2b5\ub960, \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904, \uadf8\ub798\ub514\uc5b8\ud2b8 \ud074\ub9ac\ud551, \ucd1d \ud559\uc2b5 \ub2e8\uacc4, \uc6dc\uc5c5 \ub2e8\uacc4, \ubc30\uce58 \ud06c\uae30, \uadf8\ub9ac\uace0 \uc218\uce58\uc801 \uc815\ubc00\ub3c4 \ub4f1\uc758 \uc138\ubd80 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub7ec\ud55c \uc815\ubcf4\ub294 \ubaa8\ub378\uc758 \ud559\uc2b5 \ubc0f \ucd94\ub860 \uacfc\uc815\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "4. Method"}]