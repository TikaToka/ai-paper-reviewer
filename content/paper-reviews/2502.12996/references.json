{"references": [{"fullname_first_author": "Aditya Agrawal", "paper_title": "exmy: A data type and technique for arbitrary bit precision quantization", "publication_date": "2024-05-17", "reason": "This paper introduces a novel data type and technique for quantization that is used to improve the efficiency of the training process."}, {"fullname_first_author": "Arthur Douillard", "paper_title": "DiLoCo: Distributed low-communication training of language models", "publication_date": "2024-00-00", "reason": "This paper introduces DiLoCo, a distributed optimization method that is used as the foundation for the proposed method in this paper."}, {"fullname_first_author": "H. Brendan McMahan", "paper_title": "Communication-efficient learning of deep networks from decentralized data", "publication_date": "2017-00-00", "reason": "This paper introduces federated averaging (FedAvg), a communication-efficient method for training deep learning models on decentralized data that is highly relevant to the approach used in this paper."}, {"fullname_first_author": "Sashank Reddi", "paper_title": "Adaptive federated optimization", "publication_date": "2021-00-00", "reason": "This paper introduces FedOpt, a more general framework for federated learning that is extended and applied in this paper."}, {"fullname_first_author": "Sebastian U. Stich", "paper_title": "Local SGD converges fast and communicates little", "publication_date": "2019-00-00", "reason": "This paper analyzes the convergence properties of Local SGD, a distributed optimization algorithm that is closely related to the methods used in this paper."}]}