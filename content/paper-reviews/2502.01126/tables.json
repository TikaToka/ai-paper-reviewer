[{"content": "| Category | Dataset | Direct | Hybrid SC | Elo Rating | TrueSkill | Bradley-Terry |\n|---|---|---|---|---|---|---|\n|  | GPQA | 0.356 | 0.293 | 0.453 | 0.454 | 0.451 |\n|  | MedQA | 0.864 | 0.859 | 0.914 | 0.918 | 0.915 |\n|  | OBQA | 0.926 | 0.959 | 0.970 | 0.969 | 0.968 |\n|  | Physics | 0.862 | 0.793 | 0.907 | 0.934 | 0.938 |\n|  | Algebra | 0.378 | 0.448 | 0.467 | 0.466 | 0.476 |\n|  | Chem | 0.585 | 0.486 | 0.747 | 0.751 | 0.746 |\n| STEM | Security | 0.861 | 0.899 | 0.895 | 0.910 | 0.908 |\n|  | Law | 0.749 | 0.747 | 0.813 | 0.834 | 0.825 |\n|  | Ethics | 0.889 | 0.963 | 0.922 | 0.922 | 0.917 |\n|  | Econ | 0.711 | 0.703 | 0.778 | 0.770 | 0.748 |\n| Social Sciences | Policy | 0.961 | 0.993 | 0.989 | 0.987 | 0.990 |\n|  | TQA | 0.861 | 0.899 | 0.876 | 0.874 | 0.877 |\n|  | CSQA | 0.837 | 0.920 | 0.865 | 0.868 | 0.870 |\n| Commonsense Reasoning | SIQA | 0.830 | 0.879 | 0.868 | 0.867 | 0.871 |\n|  | Average | 0.762 | 0.774 | 0.819 | 0.823 | 0.821 |", "caption": "Table 1: Llama 3.1 405B AUCs All Methods. We show the dataset-level results for Llama 3.1 405B, for the Direct and Hybrid SC absolute confidence baselines and for relative confidence estimation with different rank aggregation methods (Elo Rating, TrueSkill, Bradley-Terry). Relative confidences outperform absolute confidences for all STEM datasets, whereas absolute confidences with self-consistency (Hybrid SC) work best for commonsense reasoning tasks. Overall, relative confidences with TrueSkill rank aggregation lead to a 6.1% improvement over direct prompting and a 4.9% improvement over self-consistency prompting.", "description": "\ubcf8 \ud45c\ub294 Llama 3.1 405B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \uc9c8\ubb38\uc751\ub2f5 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574 \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uacfc \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95\uc73c\ub85c\ub294 \uc9c1\uc811 \ud504\ub86c\ud504\ud305(Direct)\uacfc \uc790\uae30\uc77c\uad00\uc131 \ud504\ub86c\ud504\ud305(Hybrid SC)\uc774 \uc0ac\uc6a9\ub418\uc5c8\uace0, \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95\uc73c\ub85c\ub294 Elo, TrueSkill, Bradley-Terry \uc138 \uac00\uc9c0 \uc21c\uc704 \uc9d1\uacc4 \uc54c\uace0\ub9ac\uc998\uc774 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \ub370\uc774\ud130\uc14b\ubcc4 AUC (Area Under the Curve) \uac12\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, STEM(\uacfc\ud559, \uae30\uc220, \uacf5\ud559, \uc218\ud559) \ub370\uc774\ud130\uc14b\uc5d0\uc11c\ub294 \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95\uc774 \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95\ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc6b0\uc218\ud55c \ubc18\uba74, \uc0c1\uc2dd \ucd94\ub860(Commonsense Reasoning) \ub370\uc774\ud130\uc14b\uc5d0\uc11c\ub294 \uc790\uae30\uc77c\uad00\uc131 \ud504\ub86c\ud504\ud305\uc774 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, TrueSkill \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud55c \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95\uc740 \uc9c1\uc811 \ud504\ub86c\ud504\ud305 \ubc29\uc2dd\ubcf4\ub2e4 6.1%, \uc790\uae30\uc77c\uad00\uc131 \ud504\ub86c\ud504\ud305 \ubc29\uc2dd\ubcf4\ub2e4 4.9% \ud5a5\uc0c1\ub41c \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4.", "section": "5 Results"}, {"content": "| # Model Calls | % Gains GPT-4o | % Gains Llama 3.1 |\n|---|---|---|\n| 5 | 0.9% | 2.2% |\n| 10 | 1.8% | 3.2% |\n| 15 | 1.8% | 4.9% |", "caption": "Table 2: Gains by scaling up comparisons. We report the gains of relative confidence estimation over self-consistency across different numbers of model calls.", "description": "\uc774 \ud45c\ub294 \uc9c8\ubb38\ub2f9 \ubaa8\ub378 \ud638\ucd9c \uc218\ub97c 5, 10, 15\ud68c\ub85c \ub298\ub824\uac00\uba70 \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc790\uae30 \uc77c\uad00\uc131 \uae30\ubc95\uacfc \ube44\uad50\ud558\uc5ec \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubc31\ubd84\uc728(%)\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. GPT-40 \ubc0f Llama 3.1 \ubaa8\ub378\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uba70, \ubaa8\ub378 \ud638\ucd9c \uc218\ub97c \ub298\ub9b4\uc218\ub85d \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc758 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5 \uacb0\uacfc"}, {"content": "| Category | Dataset | Direct | Hybrid SC | Elo Rating | TrueSkill | Bradley-Terry |\n|---|---|---|---|---|---|---|\n|  | GPQA | 0.480 | 0.421 | **0.530** | 0.528 | 0.522 |\n|  | MedQA | 0.923 | 0.931 | **0.944** | 0.943 | 0.943 |\n|  | OBQA | 0.971 | 0.983 | **0.987** | 0.986 | **0.987** |\n|  | Physics | 0.898 | 0.914 | 0.940 | 0.944 | **0.946** |\n|  | Algebra | 0.655 | **0.743** | 0.722 | 0.710 | 0.694 |\n|  | Chem | 0.741 | 0.700 | 0.795 | **0.806** | 0.802 |\n| STEM | Security | 0.880 | 0.913 | **0.930** | 0.927 | 0.922 |\n|  | Law | 0.859 | **0.872** | **0.872** | 0.867 | 0.867 |\n|  | Ethics | 0.960 | **0.969** | 0.962 | 0.962 | 0.959 |\n|  | Econ | 0.799 | 0.824 | **0.837** | 0.833 | 0.833 |\n| Social Sciences | Policy | 0.962 | 0.965 | **0.983** | **0.983** | 0.980 |\n|  | TQA | 0.906 | **0.935** | 0.908 | 0.911 | 0.911 |\n|  | CSQA | 0.864 | **0.900** | 0.886 | 0.887 | 0.884 |\n| Commonsense Reasoning | SIQA | 0.855 | 0.884 | 0.905 | 0.905 | **0.908** |\n|  | Average | 0.840 | 0.854 | **0.872** | 0.871 | 0.868 |", "caption": "Table 3: GPT-4o AUCs All Methods. We show the dataset-level results for GPT-4o, for the Direct and Hybrid SC absolute confidence baselines and for relative confidence estimation with different rank aggregation methods (Elo Rating, TrueSkill, Bradley-Terry). Relative confidences outperform absolute confidences for the majority of STEM and social science datasets, while absolute confidences with self-consistency tend to work better for commonsense reasoning tasks.", "description": "\ud45c 3\uc740 GPT-40 \ubaa8\ub378\uc758 \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc0f \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc740 \uc9c1\uc811 \ud504\ub86c\ud504\ud305(Direct)\uacfc \uc790\uae30\uc77c\uad00\uc131 \ud504\ub86c\ud504\ud305(Hybrid SC) \ub450 \uac00\uc9c0 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\uace0, \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc5d0\ub294 Elo \ub4f1\uae09, TrueSkill, Bradley-Terry \uc138 \uac00\uc9c0 \uc21c\uc704 \uc9d1\uacc4 \uc54c\uace0\ub9ac\uc998\uc744 \uc801\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \ud45c\ub294 \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b(STEM, \uc0ac\ud68c\uacfc\ud559, \uc0c1\uc2dd \ucd94\ub860)\uc5d0 \ub300\ud55c \uac01 \ubc29\ubc95\uc758 AUC(Selective Classification AUC) \uac12\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c, STEM \ubc0f \uc0ac\ud68c\uacfc\ud559 \ub370\uc774\ud130\uc14b\uc758 \ub300\ubd80\ubd84\uc5d0\uc11c \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc774 \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\uc73c\uba70, \uc0c1\uc2dd \ucd94\ub860 \uacfc\uc81c\uc5d0\uc11c\ub294 \uc790\uae30\uc77c\uad00\uc131 \ud504\ub86c\ud504\ud305\uc774 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c8\uc2b5\ub2c8\ub2e4.", "section": "5 Results"}, {"content": "| Category | Dataset | Direct | Hybrid SC | Elo Rating | TrueSkill | Bradley-Terry |\n|---|---|---|---|---|---|---|\n|  | GPQA | 0.441 | 0.457 | 0.454 | 0.460 | **0.466** |\n|  | MedQA | 0.904 | **0.920** | 0.893 | 0.900 | 0.901 |\n|  | OBQA | 0.979 | 0.984 | 0.984 | 0.984 | **0.985** |\n|  | Physics | 0.909 | 0.911 | 0.925 | **0.928** | 0.927 |\n|  | Algebra | 0.802 | **0.811** | 0.806 | 0.805 | 0.804 |\n|  | Chem | 0.832 | 0.840 | **0.864** | 0.854 | 0.850 |\n| STEM | Security | 0.920 | 0.930 | **0.934** | 0.930 | 0.917 |\n|  | Law | 0.789 | 0.809 | 0.799 | 0.815 | **0.816** |\n|  | Ethics | 0.956 | 0.964 | **0.971** | 0.970 | 0.966 |\n|  | Econ | 0.798 | 0.822 | 0.825 | **0.829** | 0.819 |\n| Social Sciences | Policy | 0.991 | **0.995** | 0.982 | 0.980 | 0.982 |\n|  | TQA | 0.880 | 0.889 | **0.918** | 0.917 | 0.917 |\n|  | CSQA | 0.885 | **0.887** | 0.873 | 0.869 | 0.872 |\n| Commonsense Reasoning | SIQA | 0.861 | **0.897** | 0.861 | 0.856 | 0.863 |\n|  | Average | 0.853 | **0.865** | 0.863 | 0.864 | 0.863 |", "caption": "Table 4: Claude 3.5 Sonnet AUCs All Methods. We show the dataset-level results for Claude 3.5 Sonnet, for the Direct and Hybrid SC absolute confidence baselines and for relative confidence estimation with different rank aggregation methods (Elo Rating, TrueSkill, Bradley-Terry). Relative confidences outperform absolute confidence baselines for 9 out of 14 datasets across STEM, social science, and commonsense reasoning. On average, relative confidences closely match the performance of the best absolute confidence methods (only 0.1% lower AUC than self-consistency prompting).", "description": "\ud45c 4\ub294 Claude 3.5 Sonnet \ubaa8\ub378\uc5d0 \ub300\ud55c \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uacfc \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc740 \uc9c1\uc811 \ud504\ub86c\ud504\ud2b8 \ubc29\uc2dd\uacfc \uc790\uae30 \uc77c\uad00\uc131 \ud504\ub86c\ud504\ud2b8 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uba70, \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc740 Elo Rating, TrueSkill, Bradley-Terry \uc138 \uac00\uc9c0 \uc21c\uc704 \uc9d1\uacc4 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.  \ud45c\ub294 14\uac1c \ub370\uc774\ud130\uc14b(STEM, \uc0ac\ud68c\uacfc\ud559, \uc0c1\uc2dd \ucd94\ub860)\uc5d0 \ub300\ud55c \ub370\uc774\ud130\uc14b\ubcc4 AUC (Selective Classification AUC)\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc740 14\uac1c \ub370\uc774\ud130\uc14b \uc911 9\uac1c\uc5d0\uc11c \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\uc2dd\ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc6b0\uc218\ud558\uba70, \ud3c9\uade0\uc801\uc73c\ub85c \uc790\uae30 \uc77c\uad00\uc131 \ud504\ub86c\ud504\ud2b8 \ubc29\uc2dd\uacfc \uac70\uc758 \ub3d9\uc77c\ud55c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4(AUC \ucc28\uc774\ub294 0.1% \ubbf8\ub9cc).", "section": "4. \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uacfc \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ube44\uad50"}, {"content": "| Category | Dataset | Direct | Hybrid SC | Elo Rating | TrueSkill | Bradley-Terry |\n|---|---|---|---|---|---|---|\n|  | GPQA | 0.395 | **0.424** | 0.410 | 0.409 | 0.413 |\n|  | MedQA | 0.794 | **0.831** | 0.725 | 0.786 | 0.792 |\n|  | OBQA | 0.955 | 0.959 | 0.979 | **0.985** | **0.985** |\n|  | Physics | 0.878 | 0.922 | 0.927 | **0.936** | 0.934 |\n|  | Algebra | 0.603 | 0.612 | 0.629 | **0.651** | 0.640 |\n|  | Chem | 0.717 | 0.762 | 0.806 | **0.851** | 0.842 |\n| STEM | Security | **0.868** | 0.863 | 0.850 | 0.824 | 0.837 |\n|  | Law | 0.695 | 0.727 | 0.766 | 0.776 | **0.778** |\n|  | Ethics | 0.903 | 0.910 | 0.949 | 0.954 | **0.957** |\n|  | Econ | 0.684 | 0.734 | **0.747** | 0.732 | 0.736 |\n| Social Sciences | Policy | 0.961 | 0.957 | **0.980** | 0.979 | 0.978 |\n|  | TQA | 0.876 | **0.891** | 0.861 | 0.853 | 0.854 |\n|  | CSQA | 0.835 | **0.889** | 0.860 | 0.869 | 0.872 |\n| Commonsense Reasoning | SIQA | 0.854 | **0.874** | 0.840 | 0.854 | 0.848 |\n|  | Average | 0.787 | 0.811 | 0.809 | 0.818 | **0.819** |", "caption": "Table 5: Gemini 1.5 Pro AUCs All Methods. We show the dataset-level AUC results for Gemini 1.5 Pro. On average, relative confidence estimation with Bradley-Terry leads to the best AUC with a 3.2% improvement over direct prompting and a 0.8% improvement over self-consistency prompting.", "description": "\ud45c 5\ub294 Gemini 1.5 Pro \ubaa8\ub378\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95(\uc9c1\uc811 \ud504\ub86c\ud504\ud2b8, \uc790\uae30 \uc77c\uad00\uc131 \ud504\ub86c\ud504\ud2b8, \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815)\uc758 \ub370\uc774\ud130\uc14b\ubcc4 AUC \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud3c9\uade0\uc801\uc73c\ub85c Bradley-Terry \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc774 \uc9c1\uc811 \ud504\ub86c\ud504\ud2b8 \ubc29\uc2dd\ubcf4\ub2e4 3.2%, \uc790\uae30 \uc77c\uad00\uc131 \ud504\ub86c\ud504\ud2b8 \ubc29\uc2dd\ubcf4\ub2e4 0.8% \ud5a5\uc0c1\ub41c AUC\ub97c \uae30\ub85d\ud588\uc2b5\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc790\uc138\ud55c AUC \uc810\uc218\uc640 \uc138 \uac00\uc9c0 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \ud1b5\ud574 \ubaa8\ub378\uc758 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \uc131\ub2a5\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5 \uacb0\uacfc"}, {"content": "| Category | Dataset | Direct | Hybrid SC | Elo Rating | TrueSkill | Bradley-Terry |\n|---|---|---|---|---|---|---|\n|  | GPQA | 0.393 | 0.383 | 0.377 | 0.404 | 0.394 |\n|  | MedQA | 0.841 | 0.893 | 0.870 | 0.875 | 0.864 |\n|  | OBQA | 0.966 | 0.979 | 0.990 | 0.990 | 0.989 |\n|  | Physics | 0.818 | 0.851 | 0.908 | 0.918 | 0.917 |\n|  | Algebra | 0.587 | 0.650 | 0.642 | 0.651 | 0.663 |\n|  | Chem | 0.682 | 0.774 | 0.797 | 0.805 | 0.795 |\n| STEM | Security | 0.911 | 0.916 | 0.933 | 0.927 | 0.922 |\n|  | Law | 0.716 | 0.741 | 0.722 | 0.753 | 0.754 |\n|  | Ethics | 0.870 | 0.915 | 0.908 | 0.914 | 0.911 |\n|  | Econ | 0.634 | 0.638 | 0.717 | 0.714 | 0.725 |\n| Social Sciences | Policy | 0.959 | 0.973 | 0.970 | 0.971 | 0.971 |\n|  | TQA | 0.892 | 0.926 | 0.865 | 0.869 | 0.872 |\n|  | CSQA | 0.831 | 0.868 | 0.835 | 0.841 | 0.837 |\n| Commonsense Reasoning | SIQA | 0.851 | 0.872 | 0.886 | 0.888 | 0.887 |\n|  | Average | 0.782 | 0.813 | 0.816 | 0.823 | 0.821 |", "caption": "Table 6: GPT-4 AUCs All Methods. For GPT-4, relative confidences with TrueSkill lead to the best average AUC with a 4.1% improvement over direct prompting and a 1.0% improvement over self-consistency.", "description": "\ud45c 6\uc740 GPT-4 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95(\uc9c1\uc811 \ud504\ub86c\ud504\ud305, \uc790\uae30 \uc77c\uad00\uc131 \ud504\ub86c\ud504\ud305)\uacfc \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95(Elo \ub4f1\uae09, TrueSkill, Bradley-Terry)\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. TrueSkill \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc774 \uc9c1\uc811 \ud504\ub86c\ud504\ud305 \ubc29\uc2dd\ubcf4\ub2e4 \ud3c9\uade0 AUC \uae30\uc900 4.1% \ud5a5\uc0c1, \uc790\uae30 \uc77c\uad00\uc131 \ud504\ub86c\ud504\ud305 \ubc29\uc2dd\ubcf4\ub2e4 1.0% \ud5a5\uc0c1\ub41c \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \ubc94\uc8fc(STEM, \uc0ac\ud68c\uacfc\ud559, \uc0c1\uc2dd \ucd94\ub860)\uc758 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uac01 \ubc29\ubc95\uc758 AUC \uc810\uc218\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5 \uacb0\uacfc"}, {"content": "| Model | Direct | Hybrid SC | Elo Rating | TrueSkill | Bradley-Terry |\n|---|---|---|---|---|---| \n| Llama 3.1 405B | 0.575 | 0.774 | 0.849 | **0.856** | 0.852 |\n| GPT-4 | 0.642 | **0.730** | 0.708 | 0.719 | 0.713 |\n| Gemini 1.5 Pro | 0.627 | 0.700 | 0.689 | **0.713** | 0.712 |\n| GPT-4o | 0.698 | **0.774** | 0.762 | 0.763 | 0.758 |\n| Claude 3.5 Sonnet | 0.685 | **0.726** | 0.711 | 0.713 | 0.713 |\n| Average Across Models | 0.645 | 0.741 | 0.744 | **0.753** | 0.749 |", "caption": "Table 7: Model AUROCs. Relative confidences with TrueSkill lead to the best average AUROC for 2 out of 5 models, and a 10.8% gain over direct prompting and a 1.2% gain over self-consistency across all models.", "description": "\ud45c 7\uc740 \ub2e4\uc12f \uac1c\uc758 \ucd5c\ucca8\ub2e8 \uc5b8\uc5b4 \ubaa8\ub378\uc5d0 \ub300\ud574 \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95(\uc9c1\uc811 \ud504\ub86c\ud504\ud2b8, \uc790\uae30 \uc77c\uad00\uc131 \ud504\ub86c\ud504\ud305)\uacfc \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95(Elo \ub4f1\uae09, TrueSkill, Bradley-Terry)\uc744 \ube44\uad50\ud558\uc5ec AUROC(Area Under the Receiver Operating Characteristic Curve) \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. TrueSkill\uc744 \uc0ac\uc6a9\ud55c \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc774 \ub450 \uac00\uc9c0 \uc808\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ubc29\ubc95\ubcf4\ub2e4 \ud3c9\uade0\uc801\uc73c\ub85c \ub354 \ub192\uc740 AUROC\ub97c \uae30\ub85d\ud588\uc73c\uba70, \uc9c1\uc811 \ud504\ub86c\ud504\ud2b8 \ubc29\uc2dd\ubcf4\ub2e4 10.8%, \uc790\uae30 \uc77c\uad00\uc131 \ud504\ub86c\ud504\ud2b8 \ubc29\uc2dd\ubcf4\ub2e4 1.2% \ud5a5\uc0c1\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ub2e4\uc12f \uac1c \ubaa8\ub378 \uc911 \ub450 \uac1c \ubaa8\ub378\uc5d0\uc11c\ub294 TrueSkill\uc744 \uc0ac\uc6a9\ud55c \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc774 \uac00\uc7a5 \ub192\uc740 AUROC\ub97c \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc0c1\ub300\uc801 \uc2e0\ub8b0\ub3c4 \ucd94\uc815 \ubc29\ubc95\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc99d\uac70\uc785\ub2c8\ub2e4.", "section": "5 Results"}, {"content": "| Elo Rating |  |  | TrueSkill |  |  |  | Bradley-Terry |  |\n|---|---|---|---|---|---|---|---|---|---|\n| Initial Score | K | # iterations | \u03bc | \u03c3 | \u03b2 | \u03c4 | max # iterations | \u03bb |\n|---|---|---|---|---|---|---|---|---|---|\n| 1000 | 400 | 1 | 25.0 | \u03bc/3.0 | \u03bc/6.0 | \u03bc/300.0 | 5 | 0.01 |", "caption": "Table 8: Rank Aggregation Hyperparameter Values.", "description": "\ud45c 8\uc740 \ub17c\ubb38\uc758 4.2\uc808 \"\uc21c\uc704 \uc9d1\uacc4\"\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc138 \uac00\uc9c0 \uc21c\uc704 \uc9d1\uacc4 \uc54c\uace0\ub9ac\uc998(Elo \ub4f1\uae09, TrueSkill, Bradley-Terry)\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac12\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc54c\uace0\ub9ac\uc998\uc5d0 \ub300\ud55c \ucd08\uae30\uac12, \ubc18\ubcf5 \ud69f\uc218, \uc815\uaddc\ud654 \uc0c1\uc218 \ub4f1\uc758 \uac12\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \uc54c\uace0\ub9ac\uc998\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uc870\uc815\ud558\uc5ec \ucd5c\uc801\uc758 \uacb0\uacfc\ub97c \uc5bb\uae30 \uc704\ud55c \uacfc\uc815\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "4.2 \uc21c\uc704 \uc9d1\uacc4"}, {"content": "| Algorithm | Parameter | Values |\n|---|---|---|\n| Elo Rating | # iters | [1-20] |\n| TrueSkill | \u03c3 | [\u03bc/3.0, \u03bc/2.5, \u03bc/2.2, \u03bc/2.0] |\n|  | \u03b2 | [\u03bc/6.0, \u03bc/5.0, \u03bc/4.0, \u03bc/3.0] |\n|  | \u03c4 | [\u03bc/300.0, \u03bc/250.0, \u03bc/200.0, \u03bc/150.0] |\n| Bradley-Terry | max # iters | [1-20] |", "caption": "Table 9: Rank Aggregation Hyperparameter Ranges.", "description": "\ud45c 9\ub294 \ub17c\ubb38\uc758 4.2\uc808 \"\uc21c\uc704 \uc9d1\uacc4\" \uc5d0\uc11c \ub2e4\ub8e8\ub294 \uc21c\uc704 \uc9d1\uacc4 \uc54c\uace0\ub9ac\uc998(Elo, TrueSkill, Bradley-Terry)\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ubc94\uc704\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc54c\uace0\ub9ac\uc998\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ubcc4\ub85c \uc5ec\ub7ec \uac12\ub4e4\uc744 \ud14c\uc2a4\ud2b8\ud558\uc5ec \ucd5c\uc801\uc758 \uc131\ub2a5\uc744 \ucc3e\uae30 \uc704\ud55c \ubc94\uc704\ub97c \uc9c0\uc815\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ubc94\uc704\ub294 \uc2e4\ud5d8\uc744 \ud1b5\ud574 \uacb0\uc815\ub418\uc5c8\uc73c\uba70, \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\ub098 \ubaa8\ub378\uc758 \ud2b9\uc131\uc5d0 \ub530\ub77c \uc870\uc815\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.2 \uc21c\uc704 \uc9d1\uacc4"}]