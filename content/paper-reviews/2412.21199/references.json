{"references": [{"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-08", "reason": "This paper introduced HumanEval, a benchmark widely used in evaluating LLMs' code generation capabilities, which the current paper builds upon and extends."}, {"fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-09", "reason": "This paper introduced MBPP, another significant benchmark for assessing LLMs' code generation skills, which is also extended in the current research."}, {"fullname_first_author": "Terry Yue Zhuo", "paper_title": "Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions", "publication_date": "2024-06-26", "reason": "This paper introduced BigCodeBench, a large-scale benchmark focusing on complex, realistic coding problems; this paper uses BigCodeBench to demonstrate the generalizability of its approach."}, {"fullname_first_author": "Alex Gu", "paper_title": "Cruxeval: A benchmark for code reasoning, understanding and execution", "publication_date": "2024-05-01", "reason": "This paper is relevant because it focuses on evaluating LLMs' code reasoning abilities, which is closely related to the concept of self-invoking code generation explored in the current paper."}, {"fullname_first_author": "Ziyang Luo", "paper_title": "WizardCoder: Empowering code large language models with evolving instruction tuning", "publication_date": "2023-06-15", "reason": "This paper is cited because it explores instruction tuning, a relevant training methodology, that is compared to the performance of base models in the current paper."}]}