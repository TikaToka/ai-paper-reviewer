{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2019-06-02", "reason": "BERT is a foundational model in NLP, and its use in this paper demonstrates the state-of-the-art advancements in contextualized word embeddings for information retrieval."}, {"fullname_first_author": "Vladimir Karpukhin", "paper_title": "Dense Passage Retrieval for Open-Domain Question Answering", "publication_date": "2020-11-16", "reason": "This paper introduced a significant advancement in dense retrieval techniques, which is directly compared to and improved upon by the proposed Hypencoder model."}, {"fullname_first_author": "Omar Khattab", "paper_title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "publication_date": "2020-07-25", "reason": "ColBERT is a highly cited and influential paper in neural information retrieval, and its comparison with Hypencoder highlights the relative strengths and weaknesses of different approaches."}, {"fullname_first_author": "Sebastian Hofst\u00e4tter", "paper_title": "Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation", "publication_date": "2020-10-26", "reason": "This paper explores knowledge distillation techniques for efficient neural ranking, a key aspect also considered and improved upon in the Hypencoder model."}, {"fullname_first_author": "Shulong Tan", "paper_title": "Fast Item Ranking under Neural Network based Measures", "publication_date": "2020-02-03", "reason": "This work proposes efficient approximate search methods using neural networks, which is directly relevant to the Hypencoder's approximate search algorithm and efficiency analysis."}]}