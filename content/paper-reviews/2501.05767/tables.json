[{"content": "| Models | Spontaneous Grounding |  | Referential Grounding |  | AVE |\n|---|---|---|---|---|---|---|\n| | Difference | Similarity | Visual Reference | Textual | Visual+Textual |  |\n| | Static | Robust | Common | OT | MV | Region | Refer | GG | Reason | Co-Re |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Human Performance |  |  |  |  |  |  |  |  |  |  |  |\n| Human | 99.50* | 97.87 | 98.00* | 100.00 | 96.88 | 100.00* | 98.99 | 91.06* | 92.08 | 97.44 | 97.18 |\n| 70B-Scale MLLMs |  |  |  |  |  |  |  |  |  |  |  |\n| LLaVA-OV-72B | 13.26 | 5.34 | 26.84 | 12.91 | 7.64 | 2.14 | 17.83 | 21.60 | 11.88 | 8.55 | 13.65 |\n| InternVL2-76B | 15.91 | 10.64 | 36.40 | 30.73 | 20.83 | 5.74 | 46.46 | 41.28 | 32.67 | 26.50 | 26.72 |\n| Qwen2-VL-72B | 46.12 | 46.81 | 64.46 | 26.73 | 22.57 | 18.62 | 33.33 | 62.53 | 50.50 | 17.09 | 38.88 |\n| 7B-Scale MLLMs |  |  |  |  |  |  |  |  |  |  |  |\n| Mantis | 1.52 | 0.00 | 3.31 | 12.18 | 2.08 | 1.00 | 1.01 | 10.02 | 0.00 | 0.85 | 3.20 |\n| LLaVA-OV-7B | 6.06 | 3.19 | 3.43 | 0.18 | 1.04 | 1.08 | 9.09 | 15.43 | 6.93 | 0.85 | 4.73 |\n| Minicpm2.6 | 14.58 | 2.13 | 14.34 | 9.82 | 6.25 | 1.75 | 11.11 | 10.02 | 2.97 | 2.56 | 7.55 |\n| mPLUG-Owl3 | 18.56 | 6.38 | 34.93 | 8.55 | 7.64 | 2.41 | 7.07 | 22.85 | 9.09 | 5.98 | 12.35 |\n| InternVL2-8B | 6.92 | 7.45 | 25.49 | 20.73 | 9.72 | 3.49 | 28.28 | 30.26 | 17.82 | 9.40 | 15.96 |\n| Qwen2-VL-7B | 27.84 | 38.30 | 19.36 | 20.73 | 11.81 | 25.95 | 23.23 | 58.52 | 48.51 | 11.97 | 28.62 |\n| mPLUG-Owl3<sub>+CoT</sub> | 16.29 | 8.51 | 55.39 | 44.36 | 25.35 | 19.04 | 36.36 | 30.86 | 18.81 | 10.26 | 26.52 |\n| InternVL2-8B<sub>+CoT</sub> | 14.58 | 7.45 | 72.54 | 40.91 | 27.78 | 28.60 | 67.68 | 44.49 | 41.58 | 11.97 | 35.76 |\n| Qwen2-VL-7B<sub>+CoT</sub> | 23.48 | 40.43 | 63.85 | 62.73 | 42.71 | 24.85 | 54.55 | 43.29 | 51.49 | 30.77 | 43.82 |\n| Migician | **65.15** | **46.81** | **84.19** | **70.73** | **60.07** | **74.31** | **76.77** | **66.53** | **59.41** | **34.19** | **63.82** |", "caption": "Table 1: Performance comparison of different models on MIG-Bench. OT, MV, GG and Co-Re respectively means object tracking, multi-view grounding, group grounding and correspondence. For values marked with *, we randomly sample 20% testing examples for human evaluation on the corresponding task.", "description": "\ud45c 1\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c MIG-Bench(\ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc811\uc9c0 \ubca4\uce58\ub9c8\ud06c)\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc740 \ub2e4\uc591\ud55c \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc811\uc9c0 \uc791\uc5c5(Object Tracking, Multi-view Grounding, Group Grounding, Correspondence)\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ubc1b\uc558\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\uac00 \uc218\uce58\ub85c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, * \ud45c\uc2dc\ub294 \ud574\ub2f9 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc778\uac04 \ud3c9\uac00\ub97c \uc704\ud574 \ubb34\uc791\uc704\ub85c \uc0d8\ud50c\ub9c1\ub41c 20%\uc758 \ud14c\uc2a4\ud2b8 \uc0ac\ub840\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 Migician \ubaa8\ub378\uc774 \ub2e4\ub978 \ub300\uaddc\ubaa8 \ub2e4\uc911 \ubaa8\ub4dc \uc5b8\uc5b4 \ubaa8\ub378\uc5d0 \ube44\ud574 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "4. Results"}, {"content": "| Model | MuirBench | BLINK val | MIBench | Mantis_eval | MMIU | AVE |\n|---|---|---|---|---|---|---|\n| **Closed-Source Model** |  |  |  |  |  |  |\n| GPT-4o | 62.31 | 60.04 | 71.88 | 62.67 | 55.7 | 62.52 |\n| Gemini-Pro | 49.35 | 45.16 | \u2014 | \u2014 | 53.4 | 49.30 |\n| **Open-Source Model** |  |  |  |  |  |  |\n| LLaVA-1.5 | 23.46 | 37.13 | 26.83 | 31.34 | 19.20 | 27.59 |\n| CogVLM | 20.85 | 41.54 | \u2014 | 45.16 | 23.57 | 32.78 |\n| Idefics2-8B | 26.08 | \u2014 | 46.39 | 48.85 | 27.80 | 37.28 |\n| mPLUG-Owl3 | 39.67 | 50.30 | 56.66 | 63.10 | 21.72 | 46.29 |\n| InternVL2-8B | 48.70 | 50.57 | 52.91 | 60.37 | 42.00 | 50.05 |\n| Mantis | _44.50_ | 49.05 | 45.09 | 57.14 | 45.60 | 48.28 |\n| LLaVA-OV-7B | 41.80 | 48.20 | _71.29_ | 64.20 | 44.46 | 53.99 |\n| Minicpm2.6 | 42.65 | 51.45 | 71.09 | _69.12_ | 50.19 | 56.90 |\n| Qwen2-VL-7B | 42.04 | **52.35** | 68.06 | **70.97** | _54.36_ | _57.56_ |\n| **Migician** | **53.69** | _51.53_ | **71.42** | _69.12_ | **60.32** | **61.51** |", "caption": "Table 6: The ablation study about the contribution of different data subsets.", "description": "\ud45c 6\uc740 \ub2e4\uc591\ud55c \ub370\uc774\ud130 \ud558\uc704 \uc9d1\ud569\uc758 \uae30\uc5ec\ub3c4\uc5d0 \ub300\ud55c \ucd94\uac00 \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ub370\uc774\ud130 \uc18c\uc2a4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uac01 \ub370\uc774\ud130 \uc720\ud615(\uba40\ud2f0 \uc774\ubbf8\uc9c0 \uadf8\ub77c\uc6b4\ub529, \uc77c\ubc18\uc801\uc778 \uba40\ud2f0 \uc774\ubbf8\uc9c0 \uc774\ud574 \ub370\uc774\ud130 \ub4f1)\uc774 \ucd5c\uc885 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788, \uac01 \ub370\uc774\ud130 \uc720\ud615\uc744 \uc81c\uac70\ud588\uc744 \ub54c\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \ud1b5\ud574 \uac01 \ub370\uc774\ud130\uc758 \uc911\uc694\uc131\uc744 \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "6.4. \ub2e8\uc77c \uc774\ubbf8\uc9c0 \uadf8\ub77c\uc6b4\ub529 \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc"}, {"content": "| V* Bench | Attribute | Spatial | Overall |\n|---|---|---|---| \n| Human Level | 98.26 | 100.00 | 98.95 |\n| Random Guess | 26.73 | 50.00 | 35.99 |\n| **Tool-using Pipeline** |  |  |  |\n| MM-React | 34.78 | 51.31 | 41.36 |\n| Visprog | 31.30 | 56.57 | 41.36 |\n| SEAL | 74.78 | 76.31 | 75.39 |\n| **End-to-end MLLMs** |  |  |  |\n| InternVL2-8B | 29.56 | 56.57 | 43.07 |\n| Gemini Pro | 40.86 | 59.21 | 48.16 |\n| LLaVA-1.5 | 43.47 | 56.57 | 48.68 |\n| Minicpm2.6 | 40.86 | 64.47 | 52.67 |\n| GPT-4V | 51.30 | 60.53 | 54.97 |\n| Migician<sub>zero_shot</sub> | 59.16 | 60.53 | 59.85 |\n| Migician<sub>slice</sub> | 77.49 | 67.11 | 72.30 |", "caption": "Table 7: Comparison between different training methods. We compare the learning efficiency between multi-task learning, separate learning and merging all these task-specialized modes. We mainly focus on the in-domain tasks that M-Grounding dataset covers.", "description": "\ud45c 7\uc740 \ub2e4\uc591\ud55c \ud559\uc2b5 \ubc29\ubc95\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4. \ub2e4\uc911 \uc791\uc5c5 \ud559\uc2b5, \uac1c\ubcc4 \uc791\uc5c5 \ud559\uc2b5 \ubc0f \uc791\uc5c5\ubcc4 \uc804\ubb38 \ubaa8\ub378 \ubcd1\ud569 \uc138 \uac00\uc9c0 \ubc29\ubc95\uc758 \ud559\uc2b5 \ud6a8\uc728\uc131\uc744 \ube44\uad50\ud558\uc5ec, \uc5b4\ub5a4 \ubc29\ubc95\uc774 M-Grounding \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5\uc744 \uac00\uc7a5 \uc798 \ubc18\uc601\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ucc38\uc870, \uac1d\uccb4 \ucd94\uc801, \uadf8\ub8f9 \uc9c0\uc815, \uc601\uc5ed \uc9c0\uc815, \uc815\uc801 \ucc28\uc774 \ubc0f \uacf5\ud1b5 \uac1d\uccb4 \ub4f1 \uc5ec\ub7ec \uc791\uc5c5\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.3 MIG\uc5d0 \ub300\ud55c \uc9c0\uce68 \ubbf8\uc138 \uc870\uc815"}, {"content": "| Model | RefCOCO val | RefCOCO testA | RefCOCO testB | RefCOCO+ val | RefCOCO+ testA | RefCOCO+ testB | RefCOCOg val | RefCOCOg test | AVE |\n|---|---|---|---|---|---|---|---|---|---|---| \n| VisionLLM v2 [40] | 79.20 | 82.30 | 77.00 | 68.90 | 75.80 | 61.80 | 73.30 | 74.80 | 74.14 |\n| Shikra [5] | 87.00 | 90.60 | 80.20 | 81.60 | 87.40 | 72.10 | 82.30 | 82.20 | 82.97 |\n| InternVL2-8B [3] | 87.10 | 91.10 | 80.70 | 79.80 | 87.90 | 71.40 | 82.70 | 82.70 | 82.94 |\n| GroundingGPT [25] | 88.02 | 91.55 | 82.47 | 81.61 | 87.18 | 73.18 | 81.67 | 81.99 | 83.57 |\n| Griffon v2 [49] | 89.6 | 91.80 | 86.50 | 81.90 | 85.50 | 76.20 | 85.00 | 86.00 | 85.30 |\n| InternVL2-8B [3] | 87.10 | 91.10 | 80.70 | 79.80 | 87.90 | 71.40 | 82.70 | 82.70 | 82.94 |\n| Qwen2-VL-7B [38] | 91.70 | 93.60 | 87.30 | 85.80 | 90.50 | 79.50 | 87.30 | 87.80 | 87.96 |\n| Migician | 91.62 | 93.49 | 87.22 | 86.13 | 91.06 | 79.93 | 88.06 | 87.80 | 88.16 |", "caption": "Table 8: Comparison of different answering forms. For random guess, we set the default answer as (0,0),(999,999).", "description": "\ud45c 8\uc740 \ub2e4\uc591\ud55c \uc751\ub2f5 \ud615\uc2dd(\uc804\uccb4 \uacbd\uacc4 \uc0c1\uc790 \uc88c\ud45c \uc9c1\uc811 \uc0dd\uc131 vs. \uac01 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud55c \uac1c\ubcc4 \uacbd\uacc4 \uc0c1\uc790 \uc88c\ud45c \uc0dd\uc131)\uc5d0 \ub530\ub978 \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubb34\uc791\uc704 \ucd94\uce21\uc758 \uacbd\uc6b0, \uae30\ubcf8 \uc751\ub2f5\uc73c\ub85c (0,0),(999,999)\ub97c \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc811\uc9c0 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub41c \uc5ec\ub7ec \uac00\uc9c0 \uc811\uadfc \ubc29\uc2dd\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc740 \uc5ec\ub7ec \uc9c8\uc758 \uc720\ud615\uc5d0 \ub300\ud574 \ud3c9\uac00\ub418\uba70, \uc804\uccb4 \uacbd\uacc4 \uc0c1\uc790\ub97c \uc9c1\uc811 \uc0dd\uc131\ud558\ub294 \uac83\ubcf4\ub2e4 \uac01 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 \uac1c\ubcc4\uc801\uc73c\ub85c \uacbd\uacc4 \uc0c1\uc790\ub97c \uc0dd\uc131\ud558\ub294 \uac83\uc774 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6.2 MIG-Bench \uacb0\uacfc"}, {"content": "| Models | Spontaneous | Referential | AVE |\n|---|---|---|---| \n| mPLUG-Owl3 | 19.96 | 9.08 | 13.04 |\n| mPLUG-Owl3<sub>+mCoT</sub> | 23.78 | 14.10 | 17.62 |\n| mPLUG-Owl3<sub>+CoT</sub> | 26.73 | 26.43 | 26.54 |\n| InternVL2-8B | 13.29 | 17.10 | 15.71 |\n| InternVL2-8B<sub>+mCoT</sub> | 23.78 | 21.99 | 22.64 |\n| InternVL2-8B<sub>+CoT</sub> | 31.52 | 37.57 | 35.37 |\n| Qwen2-VL-7B | 19.96 | 28.67 | 28.61 |\n| Qwen2-VL-7B<sub>+mCoT</sub> | 41.83 | 26.23 | 31.90 |\n| Qwen2-VL-7B<sub>+CoT</sub> | 42.59 | 44.34 | 43.70 |", "caption": "Table 9: Performance Comparison of 70B scale models equipped with CoT.", "description": "\ud45c 9\ub294 70B \ud06c\uae30\uc758 \ub2e4\uc911 \ubaa8\ub4dc \uc5b8\uc5b4 \ubaa8\ub378(MLLM)\uc5d0 \uc0ac\uace0 \uacfc\uc815(Chain-of-Thought, CoT) \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc801\uc6a9\ud588\uc744 \ub54c\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc811\uc9c0(grounding) \uc791\uc5c5\uc5d0 \ub300\ud55c \uc5ec\ub7ec \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc740 \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc811\uc9c0\uc758 \uc5ec\ub7ec \uce21\uba74(\uc608: \ucc28\uc774\uc810, \uc720\uc0ac\uc131, \uc815\uc801 \ucc28\uc774, \uac15\ub825\ud55c \ucc28\uc774, \uacf5\ud1b5 \uac1d\uccb4)\uc744 \ud3c9\uac00\ud558\uc5ec \uc885\ud569\uc801\uc73c\ub85c \ube44\uad50\ub429\ub2c8\ub2e4.  \uc774\ub294 \ub2e8\uc77c \uc774\ubbf8\uc9c0 \uc811\uc9c0 \uc131\ub2a5\ubfd0 \uc544\ub2c8\ub77c \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc774\ud574 \ub2a5\ub825\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud558\ub294 \uc9c0\ud45c\uc785\ub2c8\ub2e4.", "section": "4.3. MIG\ub97c \uc704\ud55c \uc9c0\uc2dc\uc5b4 \ubbf8\uc138 \uc870\uc815"}, {"content": "| Setting | MuirBench | BLINK | MIBench | Mantis | MMIU | MIG |\n|---|---|---|---|---|---|---|\n| **Base** | 42.04 | 52.35 | 68.06 | 70.97 | 54.36 | 28.62 |\n| **Full data (Stage-1)** | 53.77 | 51.27 | 71.76 | 66.36 | 53.31 | 62.79 |\n| **-w/o grounding** | 44.54<sub>(-9.23)</sub> | 51.32<sub>(+0.42)</sub> | 71.68<sub>(-0.08)</sub> | 67.74<sub>(+1.38)</sub> | 52.12<sub>(-1.19)</sub> | 22.43<sub>(-40.36)</sub> |\n| **-w/o general** | 53.62<sub>(-0.15)</sub> | 49.25<sub>(-2.02)</sub> | 65.22<sub>(-6.54)</sub> | 64.52<sub>(-1.84)</sub> | 48.61<sub>(-4.70)</sub> | 62.21<sub>(-0.58)</sub> |", "caption": "Table 10: Training data proportion for two stages.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 \ub450 \ubc88\uc9f8 \uc139\uc158\uc778 \ub370\uc774\ud130 \uad6c\uc131(Data Construction)\uc5d0\uc11c \uc124\uba85\ud558\ub294 MGrounding-630k \ub370\uc774\ud130\uc14b\uc758 \ub450 \uac00\uc9c0 \ud559\uc2b5 \ub2e8\uacc4(Stage)\uc5d0 \ub300\ud55c \ub370\uc774\ud130 \ube44\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Stage 1\uc740 \ub2e4\uc591\ud55c \ub2e8\uc77c \uc774\ubbf8\uc9c0 \ubc0f \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc791\uc5c5\uc744 \ud3ec\ud568\ud558\uc5ec \ubaa8\ub378\uc758 \uc77c\ubc18\uc801\uc778 \uc774\ud574\uc640 \uae30\ucd08\uc801\uc778 \uadf8\ub77c\uc6b4\ub529 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \uc911\uc810\uc744 \ub461\ub2c8\ub2e4. Stage 2\ub294 \uc790\uc720 \ud615\uc2dd\uc758 \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uadf8\ub77c\uc6b4\ub529 \uc791\uc5c5\uc5d0 \uc9d1\uc911\ud558\uc5ec \ubaa8\ub378\uc758 \uc720\uc5f0\uc131\uacfc \uc801\uc751\ub825\uc744 \ub192\uc785\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \ub2e8\uacc4\uc758 \ub370\uc774\ud130\uc14b \uad6c\uc131 \ubc29\uc2dd\uacfc \ube44\uc728\uc774 \uc790\uc138\ud788 \uc124\uba85\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b\uc758 \ucd9c\ucc98\uc640 \uc0ac\uc6a9 \ube44\uc728\uc744 \uba85\uc2dc\ud558\uc5ec \ub370\uc774\ud130 \uad6c\uc131 \ubc29\uc2dd\uc744 \ud22c\uba85\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2. \ub370\uc774\ud130 \uad6c\uc131"}]