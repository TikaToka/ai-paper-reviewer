[{"figure_path": "https://arxiv.org/html/2501.05767/x3.png", "caption": "Figure 1: Left: Examples of free-form multi-image grounding. The task is to identify and localize relevant visual regions across multiple images based on a free-form query. Right: Our proposed model, Migician, significantly outperforms other MLLMs on various multi-image grounding tasks.", "description": " \uadf8\ub9bc 1\uc740 \ub450 \ubd80\ubd84\uc73c\ub85c \ub098\ub269\ub2c8\ub2e4. \uc67c\ucabd\uc740 \uc790\uc720 \ud615\uc2dd \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc811\uc9c0\uc758 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uc791\uc5c5\uc740 \uc790\uc720 \ud615\uc2dd \ucffc\ub9ac\uc5d0 \ub530\ub77c \uc5ec\ub7ec \uc774\ubbf8\uc9c0\uc5d0\uc11c \uad00\ub828 \uc2dc\uac01\uc801 \uc601\uc5ed\uc744 \uc2dd\ubcc4\ud558\uace0 \ucc3e\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4. \uc624\ub978\ucabd\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ub41c \ubaa8\ub378\uc778 Migician\uc774 \ub2e4\uc591\ud55c \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc811\uc9c0 \uc791\uc5c5\uc5d0\uc11c \ub2e4\ub978 MLLM\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ud6e8\uc52c \ub6f0\uc5b4\ub0a8\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Migician\uc740 \uc790\uc720 \ud615\uc2dd \uc9c8\ubb38\uc5d0 \ub300\ud55c \uc751\ub2f5\uc73c\ub85c \uc5ec\ub7ec \uc774\ubbf8\uc9c0\uc5d0\uc11c \uad00\ub828 \uc2dc\uac01\uc801 \uc601\uc5ed\uc744 \uc815\ud655\ud558\uac8c \uc2dd\ubcc4\ud558\uace0 \uc704\uce58\ub97c \uc9c0\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "1. \uc18c\uac1c"}, {"figure_path": "https://arxiv.org/html/2501.05767/x4.png", "caption": "Figure 2: \nAn illustration of the multi-image grounding tasks included in MIG-Bench. These tasks are divided into two categories: spontaneous grounding and referential grounding, depending on the whether there are explicit referential requirements.", "description": "MIG-Bench\uc5d0 \ud3ec\ud568\ub41c \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc811\uc9c0 \uc791\uc5c5\uc758 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc791\uc5c5\ub4e4\uc740 \uba85\uc2dc\uc801\uc778 \ucc38\uc870 \uc694\uad6c \uc0ac\ud56d\uc774 \uc788\ub294\uc9c0 \uc5ec\ubd80\uc5d0 \ub530\ub77c \uc790\ubc1c\uc801 \uc811\uc9c0\uc640 \ucc38\uc870 \uc811\uc9c0\uc758 \ub450 \uac00\uc9c0 \ubc94\uc8fc\ub85c \ub098\ub269\ub2c8\ub2e4. \uc790\ubc1c\uc801 \uc811\uc9c0\ub294 \uc5ec\ub7ec \uc774\ubbf8\uc9c0 \uac04\uc758 \uad00\uacc4\ub97c \ubb38\ub9e5\uc801 \ub2e8\uc11c\ub85c \uc0ac\uc6a9\ud558\uc5ec \uc790\uc728\uc801\uc73c\ub85c \uac1d\uccb4\ub97c \uc2dd\ubcc4\ud558\uace0 \uc704\uce58\ub97c \ud30c\uc545\ud558\ub294 \ubc18\uba74, \ucc38\uc870 \uc811\uc9c0\ub294 \ub300\uc0c1 \uac1d\uccb4\uc5d0 \ub300\ud55c \uba85\uc2dc\uc801\uc778 \ucc38\uc870\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ucc38\uc870\ub294 \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8 \uc124\uba85\uc758 \uc784\uc758 \uc870\ud569\uc73c\ub85c \ud45c\ud604\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\ub294 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc811\uc9c0 \uc791\uc5c5\uc774 \uc790\uc138\ud558\uac8c \uc124\uba85\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. Task Definition"}, {"figure_path": "https://arxiv.org/html/2501.05767/x5.png", "caption": "Figure 3: Illustration of the CoT framework and its failure case. Different from (a) direct inference, the (b) CoT method decomposes the task into two subtasks, solving each task deploying the model\u2019s existing capabilities. A failure case of CoT is shown in (c) where the model struggles at handling abstract visual information. Green and red background colors indicate correct and incorrect answers, respectively.", "description": "\uadf8\ub9bc 3\uc740 Chain-of-Thought(CoT) \ud504\ub808\uc784\uc6cc\ud06c\uc640 \uadf8 \uc2e4\ud328 \uc0ac\ub840\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a) \uc9c1\uc811 \ucd94\ub860\uacfc \ub2ec\ub9ac, (b) CoT \ubc29\uc2dd\uc740 \uc791\uc5c5\uc744 \ub450 \uac1c\uc758 \ud558\uc704 \uc791\uc5c5\uc73c\ub85c \ubd84\ud574\ud558\uc5ec \ubaa8\ub378\uc758 \uae30\uc874 \uae30\ub2a5\uc744 \ud65c\uc6a9\ud558\uc5ec \uac01 \ud558\uc704 \uc791\uc5c5\uc744 \ud574\uacb0\ud569\ub2c8\ub2e4. (c)\ub294 \ucd94\uc0c1\uc801\uc778 \uc2dc\uac01 \uc815\ubcf4\ub97c \ucc98\ub9ac\ud558\ub294 \ub370 \uc5b4\ub824\uc6c0\uc744 \uacaa\ub294 CoT\uc758 \uc2e4\ud328 \uc0ac\ub840\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub179\uc0c9\uacfc \ube68\uac04\uc0c9 \ubc30\uacbd\uc740 \uac01\uac01 \uc815\ub2f5\uacfc \uc624\ub2f5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  CoT\ub294 \uba3c\uc800 \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc774\ud574\ub97c \ud1b5\ud574 \ud14d\uc2a4\ud2b8 \ucc38\uc870 \uc9c8\uc758\ub97c \uc0dd\uc131\ud55c \ub2e4\uc74c, \ub2e8\uc77c \uc774\ubbf8\uc9c0 \uc811\uc9c0 \uae30\ub2a5\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud574\ub2f9 \uc9c8\uc758\ub97c \uae30\ubc18\uc73c\ub85c \uac1d\uccb4\ub97c \ucc3e\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \ucd94\uc0c1\uc801\uc778 \uc2dc\uac01\uc801 \uc758\ubbf8\ub97c \uc124\uba85\ud558\ub294 \ub370 \uc5b4\ub824\uc6c0\uc744 \uacaa\uace0, \ub450 \ub2e8\uacc4 \uacfc\uc815\uc73c\ub85c \uc778\ud574 \ucd94\ub860 \uc2dc\uac04\uc774 \ub450 \ubc30\ub85c \ub298\uc5b4\ub0a9\ub2c8\ub2e4.", "section": "4. Methods"}, {"figure_path": "https://arxiv.org/html/2501.05767/x6.png", "caption": "Figure 4: Statistics of the MGrounding-630k dataset and MIG-Bench.", "description": "\uadf8\ub9bc 4\ub294 \ub17c\ubb38\uc758 \ub370\uc774\ud130\uc14b \ubc0f \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud1b5\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MGrounding-630k \ub370\uc774\ud130\uc14b\uc740 \ub2e4\uc591\ud55c \uba40\ud2f0 \uc774\ubbf8\uc9c0 \uc811\uc9c0 \uc791\uc5c5\uc744 \uc704\ud55c 63\ub9cc \uac1c \uc774\uc0c1\uc758 \ub370\uc774\ud130\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. MIG-Bench\ub294 10\uac00\uc9c0 \uba40\ud2f0 \uc774\ubbf8\uc9c0 \uc811\uc9c0 \uc791\uc5c5, 5,900\uac1c \uc774\uc0c1\uc758 \uc774\ubbf8\uc9c0, 4,200\uac1c \uc774\uc0c1\uc758 \ud14c\uc2a4\ud2b8 \uc778\uc2a4\ud134\uc2a4\ub97c \ud3ec\ud568\ud558\ub294 \uc885\ud569\uc801\uc778 \ubca4\uce58\ub9c8\ud06c\uc785\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \uac01 \uc791\uc5c5\uc758 \ub370\uc774\ud130\uc14b \ud06c\uae30, \uc774\ubbf8\uc9c0 \uac1c\uc218, \ub370\uc774\ud130 \ubd84\ud3ec \ub4f1\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\uc5b4 \ub370\uc774\ud130\uc14b\uacfc \ubca4\uce58\ub9c8\ud06c\uc758 \uaddc\ubaa8\uc640 \ub2e4\uc591\uc131\uc744 \ud55c\ub208\uc5d0 \uc774\ud574\ud558\ub3c4\ub85d \ub3c4\uc640\uc90d\ub2c8\ub2e4.", "section": "4. \ub370\uc774\ud130 \uad6c\uc131"}, {"figure_path": "https://arxiv.org/html/2501.05767/x7.png", "caption": "Figure 5: Above are the four representative failure patterns of the single-image CoT. From left to right, top to bottom, they are (a) special multi-image format, (b) abstract visual information, (c) CoT error propagation, (d) step-2 inference error.", "description": "\uadf8\ub9bc 5\ub294 \ub2e8\uc77c \uc774\ubbf8\uc9c0 \uae30\ubc18 Chain-of-Thought(CoT) \ucd94\ub860 \uacfc\uc815\uc5d0\uc11c \ubc1c\uc0dd\ud558\ub294 \ub124 \uac00\uc9c0 \uc804\ud615\uc801\uc778 \uc624\ub958 \uc720\ud615\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \ud2b9\uc218\ud55c \ub2e4\uc911 \uc774\ubbf8\uc9c0 \ud615\uc2dd, (b)\ub294 \ucd94\uc0c1\uc801\uc778 \uc2dc\uac01 \uc815\ubcf4, (c)\ub294 CoT \uc624\ub958 \uc804\ud30c, (d)\ub294 2\ub2e8\uacc4 \ucd94\ub860 \uc624\ub958\ub97c \uac01\uac01 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  (a)\uc758 \uacbd\uc6b0, \ub2e4\uc911 \uc774\ubbf8\uc9c0\uac00 \ud2b9\uc815 \ud615\uc2dd\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \ubaa8\ub4e0 \uc2dc\uac01 \uc815\ubcf4\ub97c \ud1b5\ud569\ud574\uc57c\ub9cc MIG \uc791\uc5c5\uc744 \uc218\ud589\ud560 \uc218 \uc788\ub294 \uacbd\uc6b0\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4. (b)\ub294 \uc2dc\uac01 \uc815\ubcf4\uac00 \ud14d\uc2a4\ud2b8\ub85c \ucda9\ubd84\ud788 \ud45c\ud604\ub418\uc9c0 \ubabb\ud558\uc5ec \uc815\ud655\ud55c \ucd94\ub860\uc774 \uc5b4\ub824\uc6b4 \uacbd\uc6b0\uc774\uace0, (c)\ub294 CoT\uc758 \ub2e4\ub2e8\uacc4 \uacfc\uc815\uc5d0\uc11c \uc624\ub958\uac00 \uc804\ud30c\ub418\uc5b4 \ucd94\ub860\uc758 \uc815\ud655\ub3c4\uac00 \uc800\ud558\ub418\ub294 \uc0c1\ud669\uc785\ub2c8\ub2e4. (d)\ub294 \ucd94\ub860 \uacfc\uc815\uc758 \ub450 \ubc88\uc9f8 \ub2e8\uacc4\uc5d0\uc11c \uc624\ub958\uac00 \ubc1c\uc0dd\ud558\uc5ec \ucd5c\uc885 \uacb0\uacfc\uac00 \uc798\ubabb\ub418\ub294 \uacbd\uc6b0\uc785\ub2c8\ub2e4.  \uc774\ub7ec\ud55c \uc624\ub958 \uc720\ud615\ub4e4\uc744 \ubd84\uc11d\ud558\uc5ec Migician \ubaa8\ub378\uc758 \ud55c\uacc4\uc810\uc744 \ud30c\uc545\ud558\uace0 \uac1c\uc120 \ubc29\ud5a5\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "4.1 A Chain-of-Thought Framework"}, {"figure_path": "https://arxiv.org/html/2501.05767/x8.png", "caption": "Table 2: Performance comparison on various multi-image understanding benchmarks. The\nhighest score is highlighted in bold and the second highest score is underlined for all open-source models.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc774\ud574 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc5ec\ub7ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5 \uc810\uc218\uac00 \uc5ec\ub7ec \ubca4\uce58\ub9c8\ud06c\ubcc4\ub85c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uc624\ud508 \uc18c\uc2a4 \ubaa8\ub378\uc758 \uacbd\uc6b0 \ucd5c\uace0 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uace0, \ub450 \ubc88\uc9f8\ub85c \ub192\uc740 \uc810\uc218\ub294 \ubc11\uc904\uc774 \uadf8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \ubaa8\ub378 \uc131\ub2a5\uc744 \ud55c\ub208\uc5d0 \ube44\uad50\ud558\uace0, \ud2b9\ud788 \uc624\ud508 \uc18c\uc2a4 \ubaa8\ub378\ub4e4 \uac04\uc758 \uc0c1\ub300\uc801 \uc131\ub2a5\uc744 \uba85\ud655\ud558\uac8c \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ubca4\uce58\ub9c8\ud06c\ub294 \ub2e4\uc911 \uc774\ubbf8\uc9c0 \uc774\ud574 \ub2a5\ub825\uc758 \ub2e4\uc591\ud55c \uce21\uba74\uc744 \ud3c9\uac00\ud558\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc744 \uac83\uc785\ub2c8\ub2e4.", "section": "5. MIG-Bench"}, {"figure_path": "https://arxiv.org/html/2501.05767/x9.png", "caption": "Table 3: On V* Bench, Migician generalizes well to the hyper-resolution single image in a zero-shot manner.", "description": "\ud45c 3\uc740 Migician \ubaa8\ub378\uc774 \ucd08\uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 \uc81c\ub85c\uc0f7 \ubc29\uc2dd\uc73c\ub85c \uc5bc\ub9c8\ub098 \uc798 \uc77c\ubc18\ud654\ub418\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\uc874\uc758 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\ub41c \ubaa8\ub378\uc774,  \ubcf8\ub798 \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uc5d0 \uc5c6\ub358 \ucd08\uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\uc5d0\uc11c\ub3c4 \ub192\uc740 \uc131\ub2a5\uc744 \uc720\uc9c0\ud558\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uc774\ub294 Migician \ubaa8\ub378\uc758 \uac15\uc778\uc131\uacfc \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "6. \uc2e4\ud5d8 \uacb0\uacfc"}]