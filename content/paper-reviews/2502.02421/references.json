{"references": [{"fullname_first_author": "Aljundi, R.", "paper_title": "Memory aware synapses: Learning what (not) to forget.", "publication_date": "2018-XX-XX", "reason": "This paper introduces a continual learning method that is highly relevant to the problem of catastrophic forgetting in model merging, a key concern addressed in the main paper."}, {"fullname_first_author": "Kirkpatrick, J.", "paper_title": "Overcoming catastrophic forgetting in neural networks.", "publication_date": "2017-XX-XX", "reason": "This paper presents Elastic Weight Consolidation (EWC), a regularization-based continual learning technique that is directly related to the AIM approach and helps preserve critical weights from the base model."}, {"fullname_first_author": "Lin, J.", "paper_title": "Activation-aware weight quantization for llm compression and acceleration", "publication_date": "2024-XX-XX", "reason": "This paper introduces Activation-aware Weight Quantization (AWQ), which directly inspires the AIM method by showing the importance of activation space information for model compression and is directly referenced in the methodology."}, {"fullname_first_author": "Wortsman, M.", "paper_title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.", "publication_date": "2022-XX-XX", "reason": "This paper introduces the concept of model merging by simply averaging weights, a foundational method that the current paper builds upon and improves with the proposed AIM technique."}, {"fullname_first_author": "Yu, L.", "paper_title": "Language models are super mario: Absorbing abilities from homologous models as a free lunch.", "publication_date": "2024-XX-XX", "reason": "This paper introduces a novel merging method that, like the current paper, focuses on improving model performance by merging multiple fine-tuned LLMs; it is used as a comparison method in the experimental evaluation."}]}