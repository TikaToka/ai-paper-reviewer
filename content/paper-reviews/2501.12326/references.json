{"references": [{"fullname_first_author": "Aaron Hurst", "paper_title": "GPT-40 system card", "publication_date": "2024-10-21", "reason": "This paper is frequently cited as a benchmark for comparison, showcasing its significant influence in the field of GUI agents."}, {"fullname_first_author": "Anas Awadalla", "paper_title": "MINT-1T: Scaling open-source multimodal data by 10x: A multimodal dataset with one trillion tokens", "publication_date": "2024-06-11", "reason": "This paper is crucial for the development of UI-TARS's perception capabilities, providing a large-scale dataset of screenshots for training."}, {"fullname_first_author": "Boyu Gou", "paper_title": "Navigating the digital world as humans do: Universal visual grounding for GUI agents", "publication_date": "2024-10-05", "reason": "This paper introduces UGround, a model that significantly improves the grounding ability of UI-TARS, enabling precise interaction with GUI elements."}, {"fullname_first_author": "Wenyi Hong", "paper_title": "CogAgent: A visual language model for GUI agents", "publication_date": "2024-00-00", "reason": "CogAgent serves as a strong baseline for comparison, highlighting UI-TARS's superior performance in several benchmarks."}, {"fullname_first_author": "Tianbao Xie", "paper_title": "OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments", "publication_date": "2024-04-07", "reason": "This paper introduces a challenging benchmark, OSWorld, which is used to evaluate UI-TARS and demonstrate its state-of-the-art capabilities in handling complex, real-world scenarios."}]}