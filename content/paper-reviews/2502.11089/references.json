{"references": [{"fullname_first_author": "J. Ainslie", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-05-13", "reason": "This paper proposes GQA, a significant advancement in attention architecture that improves decoding efficiency, directly influencing NSA's design choices."}, {"fullname_first_author": "J. Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-07", "reason": "This work highlights the importance of long-context modeling for complex tasks like program synthesis, providing critical context for NSA's focus on long-context capabilities."}, {"fullname_first_author": "Y. Bai", "paper_title": "LongBench: A bilingual, multitask benchmark for long context understanding", "publication_date": "2023-08-14", "reason": "LongBench is a key benchmark used to evaluate NSA's performance, showcasing the relevance and impact of NSA in addressing real-world long-context challenges."}, {"fullname_first_author": "M. Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-03", "reason": "This paper provides valuable insights into the capabilities and limitations of large language models trained on code, which directly informs the design and evaluation of NSA in handling code-related tasks."}, {"fullname_first_author": "D. Dai", "paper_title": "DeepSeekMOE: Towards ultimate expert specialization in mixture-of-experts language models", "publication_date": "2024-01-06", "reason": "DeepSeekMOE, a key component in NSA's architecture, is introduced and evaluated here; this paper offers critical details about the underlying mechanism impacting the efficiency and performance of NSA."}]}