[{"content": "| Model | MMLU | MMLU-PRO | CMMLU | BBH | GSM8K | MATH | DROP | MBPP | HumanEval | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Full Attn | 0.567 | 0.279 | 0.576 | 0.497 | 0.486 | 0.263 | 0.503 | 0.482 | 0.335 | 0.443 |\n| NSA | 0.565 | 0.286 | 0.587 | 0.521 | 0.520 | 0.264 | 0.545 | 0.466 | 0.348 | 0.456 |", "caption": "Table 1: Pretraining performance comparison between the full attention baseline and NSA on general benchmarks, across knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding (MBPP, HumanEval) tasks. NSA achieves superior average performance on most benchmarks despite high sparsity.", "description": "\ubcf8 \ud45c\ub294 270\uc5b5 \uac1c\uc758 \ud1a0\ud070\uc73c\ub85c \ud6c8\ub828\ub41c 270\uc5b5 \ub9e4\uac1c\ubcc0\uc218 \ubaa8\ub378\uc5d0\uc11c \uc804\uccb4 \uc5b4\ud150\uc158 \uae30\uc900\uacfc NSA \uae30\uc900\uc758 \uc0ac\uc804 \ud6c8\ub828 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc778 \ubca4\uce58\ub9c8\ud06c, \uc9c0\uc2dd(MMLU, MMLU-PRO, CMMLU), \ucd94\ub860(BBH, GSM8K, MATH, DROP), \ucf54\ub529(MBPP, HumanEval) \uc791\uc5c5\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  NSA\ub294 \ub192\uc740 \uc2a4\ud30c\uc2a4\uc131\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ub300\ubd80\ubd84\uc758 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc6b0\uc218\ud55c \ud3c9\uade0 \uc131\ub2a5\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Model | MFQA-en | MFQA-zh | Qasper | HPQ | 2Wiki | GovRpt | Dur | PassR-en | PassR-zh | LCC | Code | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| H2O | 0.428 | 0.429 | 0.308 | 0.112 | 0.101 | 0.231 | 0.208 | 0.704 | 0.421 | 0.092 | 0.303 |  |\n| InfLLM | 0.474 | 0.517 | 0.356 | 0.306 | 0.250 | 0.277 | 0.257 | 0.766 | 0.486 | 0.143 | 0.383 |  |\n| Quest | 0.495 | 0.561 | 0.365 | 0.295 | 0.245 | 0.293 | 0.257 | 0.792 | 0.478 | 0.135 | 0.392 |  |\n| Exact-Top | 0.502 | 0.605 | 0.397 | 0.321 | 0.288 | 0.316 | 0.291 | 0.810 | 0.548 | 0.156 | 0.423 |  |\n| Full Attn | 0.512 | 0.623 | 0.409 | 0.350 | 0.305 | 0.324 | 0.294 | 0.830 | 0.560 | 0.163 | 0.437 |  |\n| NSA | 0.503 | 0.624 | 0.432 | 0.437 | 0.356 | 0.307 | 0.341 | 0.905 | 0.550 | 0.232 | 0.469 | |", "caption": "Table 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single document QA, multi-document QA, synthetic and code task categories. NSA outperformed most of the baselines including Full Attention.", "description": "\ud45c 2\ub294 LongBench\ub77c\ub294 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc5d0\uc11c NSA(Natively Trainable Sparse Attention) \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uae30\uc874\uc758 \ub2e4\ub978 \uc2a4\ud30c\uc2a4 \uc5b4\ud150\uc158 \ubaa8\ub378\ub4e4\uacfc Full Attention \ubaa8\ub378\uacfc \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LongBench\ub294 \ub2e8\uc77c \ubb38\uc11c \uc9c8\uc758\uc751\ub2f5, \ub2e4\uc911 \ubb38\uc11c \uc9c8\uc758\uc751\ub2f5, \ud569\uc131 \ub370\uc774\ud130, \ucf54\ub4dc \uc0dd\uc131 \ub4f1 \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5\ub4e4\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \uacb0\uacfc\ub294 NSA \ubaa8\ub378\uc774 Full Attention \ubaa8\ub378\uc744 \ud3ec\ud568\ud55c \ub300\ubd80\ubd84\uc758 \uae30\uc874 \ubaa8\ub378\ub4e4\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4.3. Performance Comparison"}, {"content": "| Generation Token Limit | \u00a0\u00a0 8192 | \u00a0\u00a0 16384 |\n|---|---|---|\n| Full Attention-R | 0.046 | 0.092 |\n| NSA-R | **0.121** | **0.146** |", "caption": "Table 3: AIME Instruction-based Evaluating after supervised fine-tuning. Our NSA-R demonstrates better performance than Full Attention-R at both 8k and 16k sequence lengths", "description": "\ud45c 3\uc740 \uc9c0\ub3c4 \ud559\uc2b5 \ubbf8\uc138 \uc870\uc815 \ud6c4 AIME(American Invitational Mathematics Examination) \uc9c0\uc2dc \uae30\ubc18 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  NSA-R(Native Sparse Attention-R) \ubaa8\ub378\uc740 8k\uc640 16k \uc2dc\ud000\uc2a4 \uae38\uc774 \ubaa8\ub450\uc5d0\uc11c \uae30\uc900 Full Attention-R \ubaa8\ub378\ubcf4\ub2e4 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc7a5\ubb38 \ub9e5\ub77d(Long-Context) \ubb38\uc81c \ud574\uacb0\uc744 \uc704\ud55c NSA-R \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uacfc \uc815\ud655\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Context Length | 8192 | 16384 | 32768 | 65536 |\n|---|---|---|---|---|\n| Full Attention | 8192 | 16384 | 32768 | 65536 |\n| NSA | 2048 | 2560 | 3584 | 5632 |\n| Expected Speedup | 4\u00d7 | 6.4\u00d7 | 9.1\u00d7 | 11.6\u00d7 |", "caption": "Table 4: Memory access volume (in equivalent number of tokens) per attention operation during decoding. Due to the low arithmetic intensity and memory-bound nature of decoding, the expected speedup is approximately linear with the volume of memory access.", "description": "\ud45c 4\ub294 \ub514\ucf54\ub529 \uc911 \uc5b4\ud150\uc158 \uc5f0\uc0b0\ub2f9 \uba54\ubaa8\ub9ac \uc811\uadfc\ub7c9(\ud1a0\ud070 \uc218)\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub514\ucf54\ub529\uc740 \uc5f0\uc0b0 \ub300\ube44 \uba54\ubaa8\ub9ac \uc811\uadfc \ube44\uc728\uc774 \ub0ae\uc544 \uba54\ubaa8\ub9ac \uc81c\uc57d\uc801\uc774\uae30 \ub54c\ubb38\uc5d0 \uc608\uc0c1\ub418\ub294 \uc18d\ub3c4 \ud5a5\uc0c1\uc740 \uba54\ubaa8\ub9ac \uc811\uadfc\ub7c9\uc5d0 \uac70\uc758 \uc120\ud615\uc801\uc73c\ub85c \ube44\ub840\ud569\ub2c8\ub2e4.", "section": "5. Efficiency Analysis"}]