[{"figure_path": "https://arxiv.org/html/2502.11089/x1.png", "caption": "Figure 1: Comparison of performance and efficiency between Full Attention model and our NSA. Left: Despite being sparse, NSA surpasses Full Attention baseline on average across general benchmarks, long-context tasks, and reasoning evaluation. Right: For 64k-length sequence processing, NSA achieves substantial computational speedup compared to Full Attention in all stages: decoding, forward propagation, and backward propagation.", "description": "\uadf8\ub9bc 1\uc740 \uc81c\uc548\ub41c NSA(Natively trainable Sparse Attention)\uc640 \uae30\uc874\uc758 Full Attention \ubaa8\ub378\uc758 \uc131\ub2a5\uacfc \ud6a8\uc728\uc131\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uc67c\ucabd \uadf8\ub9bc\uc740 \uc77c\ubc18\uc801\uc778 \ubca4\uce58\ub9c8\ud06c, \uc7a5\ubb38 \ucee8\ud14d\uc2a4\ud2b8 \uc791\uc5c5 \ubc0f \ucd94\ub860 \ud3c9\uac00\uc5d0\uc11c NSA\uac00 Full Attention \ubaa8\ub378\uc744 \ud3c9\uade0\uc801\uc73c\ub85c \ub2a5\uac00\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  NSA\ub294 \uc2a4\ud30c\uc2a4 \ubaa8\ub378\uc784\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc624\ub978\ucabd \uadf8\ub9bc\uc740 64k \uae38\uc774\uc758 \uc2dc\ud000\uc2a4 \ucc98\ub9ac\uc5d0\uc11c NSA\uac00 \ub514\ucf54\ub529, \uc21c\uc804\ud30c \ubc0f \uc5ed\uc804\ud30c \ubaa8\ub4e0 \ub2e8\uacc4\uc5d0\uc11c Full Attention \ubaa8\ub378\uc5d0 \ube44\ud574 \uc0c1\ub2f9\ud55c \uacc4\uc0b0 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud588\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 NSA\uac00 \ubaa8\ub378 \uc218\uba85 \uc8fc\uae30 \uc804\uccb4\uc5d0\uc11c \ud6a8\uc728\uc131\uc744 \uc720\uc9c0\ud568\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.11089/x2.png", "caption": "Figure 2: Overview of NSA\u2019s architecture. Left: The framework processes input sequences through three parallel attention branches: For a given query, preceding keys and values are processed into compressed attention for coarse-grained patterns, selected attention for important token blocks, and sliding attention for local context. Right: Visualization of different attention patterns produced by each branch. Green areas indicate regions where attention scores need to be computed, while white areas represent regions that can be skipped.", "description": "\uadf8\ub9bc 2\ub294 NSA(Native Sparse Attention)\uc758 \uc544\ud0a4\ud14d\ucc98\ub97c \uac1c\uad04\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc740 \uc138 \uac1c\uc758 \ubcd1\ub82c\uc801\uc778 \uc5b4\ud150\uc158 \ubd84\uae30(compressed attention, selected attention, sliding attention)\ub97c \ud1b5\ud574 \uc785\ub825 \uc2dc\ud000\uc2a4\uac00 \ucc98\ub9ac\ub418\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubd84\uae30\ub294 \uc9c8\uc758(query)\uc5d0 \ub300\ud574 \uc774\uc804 \ud0a4(key)\uc640 \uac12(value)\uc744 \ucc98\ub9ac\ud558\uc5ec,  compressed attention\uc740 \uc870\uc545\ud55c \ud328\ud134\uc744, selected attention\uc740 \uc911\uc694\ud55c \ud1a0\ud070 \ube14\ub85d\uc744, \uadf8\ub9ac\uace0 sliding attention\uc740 \uc9c0\uc5ed\uc801 \ub9e5\ub77d\uc744 \ubcf4\uc874\ud569\ub2c8\ub2e4. \uc624\ub978\ucabd\uc740 \uac01 \ubd84\uae30\uc5d0\uc11c \uc0dd\uc131\ub41c \uc11c\ub85c \ub2e4\ub978 \uc5b4\ud150\uc158 \ud328\ud134\uc744 \uc2dc\uac01\ud654\ud569\ub2c8\ub2e4. \ub179\uc0c9 \uc601\uc5ed\uc740 \uc5b4\ud150\uc158 \uc810\uc218\ub97c \uacc4\uc0b0\ud574\uc57c \ud558\ub294 \uc601\uc5ed\uc774\uace0, \ud770\uc0c9 \uc601\uc5ed\uc740 \uac74\ub108\ub6f8 \uc218 \uc788\ub294 \uc601\uc5ed\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2502.11089/x3.png", "caption": "Figure 3: Kernel design for NSA. The kernel loads queries by GQA groups (Grid Loop), fetches corresponding sparse KV blocks (Inner Loop), and performs attention computation on SRAM. Green blocks indicate data on SRAM, while blue indicates data on HBM.", "description": "\uadf8\ub9bc 3\uc740 NSA\uc758 \ucee4\ub110 \uc124\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  NSA \ucee4\ub110\uc740 GQA \uadf8\ub8f9 \ub2e8\uc704\ub85c \ucffc\ub9ac\ub97c \ub85c\ub4dc\ud558\uace0(Grid Loop), \ub300\uc751\ud558\ub294 \uc2a4\ud30c\uc2a4 KV \ube14\ub85d\uc744 \uac00\uc838\uc624\uba70(Inner Loop), SRAM\uc5d0\uc11c \uc5b4\ud150\uc158 \uc5f0\uc0b0\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \ucd08\ub85d\uc0c9 \ube14\ub85d\uc740 SRAM\uc5d0 \uc788\ub294 \ub370\uc774\ud130\ub97c, \ud30c\ub780\uc0c9 \ube14\ub85d\uc740 HBM\uc5d0 \uc788\ub294 \ub370\uc774\ud130\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  Grid Loop\ub294 GQA \uadf8\ub8f9\uc758 \ucffc\ub9ac\ub4e4\uc744 \ub3d9\uc2dc\uc5d0 \ucc98\ub9ac\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \ub192\uc774\uace0, Inner Loop\ub294 \uc5f0\uc18d\uc801\uc778 KV \ube14\ub85d\uc744 \uba54\ubaa8\ub9ac\uc5d0\uc11c \uac00\uc838\uc640\uc11c \uba54\ubaa8\ub9ac \uc811\uadfc \ud69f\uc218\ub97c \uc904\uc785\ub2c8\ub2e4.  \uc774\ub7ec\ud55c \uc124\uacc4\ub294 \uba54\ubaa8\ub9ac \ub300\uc5ed\ud3ed\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \uc0ac\uc6a9\ud558\uace0,  \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \uadf9\ub300\ud654\ud558\uc5ec \ucc98\ub9ac\ub7c9\uc744 \ub192\uc785\ub2c8\ub2e4.", "section": "3.4 \ucee4\ub110 \uc124\uacc4"}, {"figure_path": "https://arxiv.org/html/2502.11089/x4.png", "caption": "Figure 4: Pretraining loss comparison between Full Attention and our NSA on 27B-parameter model. Both models exhibit stable convergence, with NSA achieving lower loss values.", "description": "\uadf8\ub9bc 4\ub294 270\uc5b5 \ub9e4\uac1c\ubcc0\uc218 \ubaa8\ub378\uc5d0 \ub300\ud55c NSA\uc640 \uc804\uccb4 \uc5b4\ud150\uc158 \ubc29\uc2dd\uc758 \uc0ac\uc804 \ud559\uc2b5 \uc190\uc2e4 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub450 \ubaa8\ub378 \ubaa8\ub450 \uc548\uc815\uc801\uc778 \uc218\ub834\uc744 \ubcf4\uc774\uba70, NSA\ub294 \ub354 \ub0ae\uc740 \uc190\uc2e4 \uac12\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4. \uc774\ub294 NSA\uac00 \uc804\uccb4 \uc5b4\ud150\uc158\uacfc \ub3d9\ub4f1\ud55c \uc131\ub2a5\uc744 \uc720\uc9c0\ud558\uba74\uc11c \ub354 \ud6a8\uc728\uc801\uc778 \ud559\uc2b5\uc744 \uac00\ub2a5\ud558\uac8c \ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  \uadf8\ub798\ud504\ub294 \ubc18\ubcf5 \ud69f\uc218\uc5d0 \ub530\ub978 \uc190\uc2e4 \uac12\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\uc5b4, NSA\uac00 \uc804\uccb4 \uc5b4\ud150\uc158 \ubc29\uc2dd\ubcf4\ub2e4 \ub354 \ube60\ub974\uac8c \uc218\ub834\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ud6a8\uc728\uc801\uc778 \ub9e4\uac1c\ubcc0\uc218 \uc0ac\uc6a9 \ubc0f \ucd5c\uc801\ud654\ub41c \ud559\uc2b5 \uc804\ub7b5\uc744 \ud1b5\ud574 \uc774\ub904\uc9c4 \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.11089/x5.png", "caption": "Figure 5: Needle-in-a-Haystack retrieval accuracy across context positions with 64k context length. NSA achieves perfect accuracy through its hierarchical sparse attention design.", "description": "\uadf8\ub9bc 5\ub294 64k \uae38\uc774\uc758 \ubb38\ub9e5\uc744 \uac00\uc9c4 Needle-in-a-Haystack \uac80\uc0c9 \uc791\uc5c5\uc5d0\uc11c \ubb38\ub9e5 \uc704\uce58\uc5d0 \ub530\ub978 \uac80\uc0c9 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. NSA(Native Sparse Attention)\ub294 \uacc4\uce35\uc801 \uc2a4\ud30c\uc2a4 \uc5b4\ud150\uc158 \uc124\uacc4\ub97c \ud1b5\ud574 \uc644\ubcbd\ud55c \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud569\ub2c8\ub2e4. \uc774\ub294 NSA\uac00 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \ud1a0\ud070(\uc555\ucd95 \ud1a0\ud070, \uc120\ud0dd\uc801 \ud1a0\ud070, \uc2ac\ub77c\uc774\ub529 \uc708\ub3c4\uc6b0 \ud1a0\ud070)\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \ucc98\ub9ac\ud558\uc5ec \uc804\uc5ed\uc801 \ubb38\ub9e5 \uc778\uc2dd\uacfc \uc9c0\uc5ed\uc801 \uc815\ubc00\ub3c4\ub97c \ubaa8\ub450 \uc720\uc9c0\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \uadf8\ub9bc\uc740 64k \uae38\uc774\uc758 \ubb38\ub9e5 \ub0b4\uc5d0\uc11c \ubaa8\ub4e0 \uc704\uce58\uc5d0\uc11c NSA \ubaa8\ub378\uc774 \uc644\ubcbd\ud55c \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud588\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.11089/x6.png", "caption": "Figure 6: Comparison of Triton-based NSA kernel with Triton-based FlashAttention-2 kernel. Our implementation significantly reduces latency across all context lengths, with the improvement becoming more pronounced as input length increases.", "description": "\uadf8\ub9bc 6\uc740 Triton \uae30\ubc18\uc758 NSA \ucee4\ub110\uacfc Triton \uae30\ubc18\uc758 FlashAttention-2 \ucee4\ub110\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. NSA\ub294 \ubaa8\ub4e0 \ubb38\ub9e5 \uae38\uc774\uc5d0 \uac78\uccd0 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \uc0c1\ub2f9\ud788 \uc904\uc774\uba70, \uc785\ub825 \uae38\uc774\uac00 \uae38\uc5b4\uc9c8\uc218\ub85d \uc131\ub2a5 \ud5a5\uc0c1\uc774 \ub354\uc6b1 \ub450\ub4dc\ub7ec\uc9d0\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 NSA\uc758 \ud558\ub4dc\uc6e8\uc5b4 \ucd5c\uc801\ud654\ub41c \uc124\uacc4 \ub355\ubd84\uc785\ub2c8\ub2e4. \ud2b9\ud788, NSA\ub294 \uba54\ubaa8\ub9ac \uc561\uc138\uc2a4 \ud328\ud134\uc744 \ucd5c\uc801\ud654\ud558\uc5ec \ud150\uc11c \ucf54\uc5b4 \ud65c\uc6a9\ub3c4\ub97c \uadf9\ub300\ud654\ud558\uace0, \ud6a8\uc728\uc801\uc778 \ub8e8\ud504 \uc2a4\ucf00\uc904\ub9c1\uc73c\ub85c \ubd88\ud544\uc694\ud55c KV \uc804\uc1a1\uc744 \uc81c\uac70\ud568\uc73c\ub85c\uc368 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.", "section": "5. Efficiency Analysis"}, {"figure_path": "https://arxiv.org/html/2502.11089/x7.png", "caption": "Figure 7: Compare training loss on a 3B-parameter model with Full Attention and different token selection strategies and. Our NSA achieves better performance.", "description": "\uadf8\ub9bc 7\uc740 30\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uac00\uc9c4 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec, \uc804\uccb4 \uc5b4\ud150\uc158(Full Attention)\uacfc \ub2e4\uc591\ud55c \ud1a0\ud070 \uc120\ud0dd \uc804\ub7b5\ub4e4\uc744 \ube44\uad50\ud558\uc5ec \ud559\uc2b5 \uc190\uc2e4(training loss)\uc744 \ub098\ud0c0\ub0b8 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud1a0\ud070 \uc120\ud0dd \uc804\ub7b5\ub4e4\uc740 \ud6a8\uc728\uc801\uc778 \uc2a4\ud30c\uc2a4 \uc5b4\ud150\uc158(sparse attention)\uc744 \uad6c\ud604\ud558\uae30 \uc704\ud55c \uc5ec\ub7ec \uac00\uc9c0 \ubc29\ubc95\ub4e4\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uadf8\ub798\ud504\ub294 NSA(Native Sparse Attention) \uae30\ubc95\uc774 \ub2e4\ub978 \ud1a0\ud070 \uc120\ud0dd \uc804\ub7b5\ub4e4\ubcf4\ub2e4 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \ub354 \ub0ae\uc740 \uc190\uc2e4\uc744 \ub2ec\uc131\ud558\uc5ec, \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, NSA\uac00 \ub354 \ud6a8\uacfc\uc801\uc73c\ub85c \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a4\ub294 \uac83\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4.", "section": "3. Algorithm Design"}, {"figure_path": "https://arxiv.org/html/2502.11089/x8.png", "caption": "Figure 8: Visualization of Attention Map on a Full Attention transformer. Light-colored regions indicate higher attention values. As shown in the figure, attention scores exhibit blockwise clustering distribution.", "description": "\uadf8\ub9bc 8\uc740 \uc644\uc804\ud55c \uc5b4\ud150\uc158 \ubcc0\ud658\uae30(Full Attention Transformer)\uc758 \uc5b4\ud150\uc158 \ub9f5\uc744 \uc2dc\uac01\ud654\ud55c \uac83\uc785\ub2c8\ub2e4. \ubc1d\uc740 \uc0c9 \uc601\uc5ed\uc740 \ub354 \ub192\uc740 \uc5b4\ud150\uc158 \uac12\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774 \uc5b4\ud150\uc158 \uc810\uc218\ub294 \ube14\ub85d \ub2e8\uc704\ub85c \uad70\uc9d1\ud654\ub41c \ubd84\ud3ec\ub97c \ubcf4\uc785\ub2c8\ub2e4. \uc774\ub294 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc774 \uc5f0\uc18d\uc801\uc778 \ud1a0\ud070 \ube14\ub85d \ub0b4\uc5d0\uc11c \uc720\uc0ac\ud55c \uc758\ubbf8\uc801 \uad00\uacc4\ub97c \uac16\ub294 \ud1a0\ud070\ub4e4\uc5d0 \ub354 \ub192\uc740 \uc5b4\ud150\uc158 \uac12\uc744 \ud560\ub2f9\ud558\ub294 \uacbd\ud5a5\uc774 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ube14\ub85d \ub2e8\uc704\uc758 \ud328\ud134\uc740 NSA(Natively Sparse Attention)\uc640 \uac19\uc740 \ud76c\uc18c \uc5b4\ud150\uc158 \ubc29\ubc95\uc744 \uc124\uacc4\ud560 \ub54c \uc911\uc694\ud55c \uace0\ub824 \uc0ac\ud56d\uc774 \ub429\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \ud76c\uc18c \uc5b4\ud150\uc158\uc740 \uc774\ub7ec\ud55c \ud328\ud134\uc744 \ud65c\uc6a9\ud558\uc5ec \uacc4\uc0b0 \ube44\uc6a9\uc744 \uc904\uc77c \uc218 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.", "section": "6.2 \uc2dc\uac01\ud654"}]