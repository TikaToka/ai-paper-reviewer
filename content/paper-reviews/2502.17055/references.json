{"references": [{"fullname_first_author": "Ashkboos, S.", "paper_title": "Halo: Hadamard-assisted lossless optimization for efficient llm training and fine-tuning.", "publication_date": "2025-01-01", "reason": "This paper proposes a novel method for low-precision LLM training, directly addressing a key challenge discussed in the main paper."}, {"fullname_first_author": "Huang, T.", "paper_title": "Spam: Spike-aware adam with momentum reset for stable Ilm training.", "publication_date": "2025-01-01", "reason": "This paper introduces SPAM, a key optimizer that the main paper builds upon and improves."}, {"fullname_first_author": "Kingma, D. P.", "paper_title": "Adam: A method for stochastic optimization.", "publication_date": "2014-12-01", "reason": "This paper introduces the Adam optimizer, a widely used method that is compared against in the main paper."}, {"fullname_first_author": "Shazeer, N.", "paper_title": "Adafactor: Adaptive learning rates with sublinear memory cost.", "publication_date": "2018-01-01", "reason": "This paper introduces Adafactor, another commonly used optimizer compared against in the main paper."}, {"fullname_first_author": "Touvron, H.", "paper_title": "Llama 2: Open foundation and fine-tuned chat models.", "publication_date": "2023-07-01", "reason": "This paper introduces the LLaMA models, which are used as the basis for experiments in the main paper."}]}