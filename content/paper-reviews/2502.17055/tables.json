[{"content": "|                     | INT4 Training         |             |             | FP4 Training         |             |             |\n| :------------------ | :--------------------: | :----------: | :----------: | :--------------------: | :----------: | :----------: |\n| **130M**           |             | **350M**     | **1B**       |             | **350M**     | **1B**       |\n| Adam                | 26.4                  | 24.14        | 21.59        | 28.9                  | 24.59        | 22.01        |\n| Adam+GradClip       | 26.30                 | 21.64        | 19.74        | 28.27                 | 20.84        | 20.25        |\n| Adafactor           | 25.11                 | 20.45        | 20.65        | 26.89                 | 20.53        | 20.03        |\n| SPAM                | 25.03                 | 20.19        | 19.98        | 26.78                 | 20.35        | 19.74        |\n| Stable-SPAM         | **24.33**              | **17.76**     | **17.42**     | **26.31**              | **19.49**     | **18.48**     |\n| Adam (BF16)        | 24.53                 | 21.38        | 19.73        | 24.53                 | 21.38        | 19.73        |\n| Training Tokens     | \n2.2B                  |             |             |             |             |             |", "caption": "Table 1: Comparison of various optimizers of INT4 and FP4 training of LLaMA models on C4444. Perplexity is reported.", "description": "\ud45c 1\uc740 C4 \ub370\uc774\ud130\uc14b\uc5d0\uc11c LLaMA \ubaa8\ub378\uc744 INT4 \ubc0f FP4\ub85c \ud559\uc2b5\uc2dc\ud0a8 \ub2e4\uc591\ud55c \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  \uac01 \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc5d0 \ub300\ud574 LLaMA \ubaa8\ub378\uc758 \ud06c\uae30\ubcc4(130M, 350M, 1B \ud30c\ub77c\ubbf8\ud130) \ud3c9\uac00 \uc190\uc2e4(perplexity)\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc800\ube44\ud2b8 \uc815\ubc00\ub3c4 \ud559\uc2b5 \uc124\uc815\uc5d0\uc11c Stable-SPAM\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  INT4\uc640 FP4\ub294 \uac01\uac01 4\ube44\ud2b8 \uc815\uc218 \ubc0f 4\ube44\ud2b8 \ubd80\ub3d9\uc18c\uc218\uc810\uc744 \ub098\ud0c0\ub0b4\ub294 \uc800\uc815\ubc00\ub3c4 \ud45c\ud604 \ubc29\uc2dd\uc785\ub2c8\ub2e4. perplexity\ub294 \ubaa8\ub378\uc774 \uc0dd\uc131\ud55c \ud14d\uc2a4\ud2b8\uc758 \uc608\uce21 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0b4\ub294 \uc9c0\ud45c\ub85c, \uac12\uc774 \ub0ae\uc744\uc218\ub85d \uc88b\uc2b5\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Optimizer | 60M | 130M | 350M | 1B |\n|---|---|---|---|---|\n| Adam-mini | 34.10 | 24.85 | 19.05 | 16.07 |\n| Adam | 34.09 | 24.91 | 18.77 | 16.13 |\n| Adam + GradClip | 33.33 | 24.88 | 18.51 | 15.22 |\n| Adafactor | 32.57 | 23.98 | 17.74 | 15.19 |\n| SPAM | 30.46 | 23.36 | 17.42 | 14.66 |\n| Stable-SPAM | **28.84** | **22.21** | **16.85** | **13.90** |\n| Training Tokens | 1.1B | 2.2B | 6.4B | 11.6B |", "caption": "Table 2: Comparison among various optimizers on BF16 training. Perplexity is reported.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud55c BF16(Brain Float16) \ubc29\uc2dd\uc758 LLM(Large Language Model) \ud559\uc2b5\uc5d0 \ub300\ud55c \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud06c\uae30\uc758 LLaMA \ubaa8\ub378\uc744 C4 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\uc2dc\ucf30\uc744 \ub54c, \uac01 \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998(Adam-mini, Adam, Adafactor, SPAM, Stable-SPAM)\uc758 \uc131\ub2a5\uc744 perplexity \uac12\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. perplexity\ub294 \ub0ae\uc744\uc218\ub85d \uc88b\uc740 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 Stable-SPAM\uc774 BF16 \ud559\uc2b5\uc5d0\uc11c\ub3c4 \ub2e4\ub978 \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90c\uc744 \ud655\uc778\ud558\uae30 \uc704\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "4.3. BF16 LLM \ud559\uc2b5 \uc131\ub2a5"}, {"content": "| Optimizers | INT4 Training |  | FP4 Training |  |\n|---|---|---|---|---| \n|  | 60M | 130M | 60M | 130M |\n| Lion | 39.36 | 35.28 | 39.89 | 34.20 |\n| Lion+<span class=\"ltx_text ltx_font_typewriter\">AdaGN</span>+<span class=\"ltx_text ltx_font_typewriter\">AdaClip</span> | **38.49** | **29.40** | **36.75** | **31.63** |\n| Adam-mini | 34.84 | 29.79 | 36.37 | 32.95 |\n| Adam-mini+<span class=\"ltx_text ltx_font_typewriter\">AdaGN</span>+<span class=\"ltx_text ltx_font_typewriter\">AdaClip</span> | **34.61** | **29.65** | **34.65** | **32.39** |\n| Training Tokens | 1.1B |  |  |  |", "caption": "Table 3: Performence of AdaGN and AdaClip on Lion and Adam-mini optimizers. Experiments are based on LLaMA-60M/130M with 4-Bit training.", "description": "\ud45c 3\uc740 AdaGN\uacfc AdaClip\uc744 Lion \ubc0f Adam-mini \ucd5c\uc801\ud654\uae30\uc5d0 \uc801\uc6a9\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc2e4\ud5d8\uc740 4\ube44\ud2b8\ub85c \ud559\uc2b5\ub41c LLaMA-60M \ubc0f LLaMA-130M \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c \uc9c4\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ucd5c\uc801\ud654\uae30(Lion, Adam-mini)\ub97c \ub2e8\ub3c5\uc73c\ub85c \uc0ac\uc6a9\ud588\uc744 \ub54c\uc640 AdaGN \ubc0f AdaClip\uc744 \ucd94\uac00\ud588\uc744 \ub54c\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec, AdaGN\uacfc AdaClip\uc774 \ub2e4\ub978 \ucd5c\uc801\ud654\uae30\uc5d0 \ub300\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uc5bc\ub9c8\ub098 \ud6a8\uacfc\uc801\uc778\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  INT4 \ubc0f FP4 \uc591\uc790\ud654 \ubc29\uc2dd \ubaa8\ub450\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.", "section": "4.4. \ub2e4\ub978 \ucd5c\uc801\ud654\uae30\uc640\uc758 \ud1b5\ud569"}, {"content": "| Optimizer | FP4 | BF16 |\n|---|---|---|\n| Adam | 35.47 | 34.09 |\n| Adam + `MoRet` | 32.4 | 31.47 |\n| Adam + `MoRet` + `AdaClip` | 31.97 | 30.29 |\n| Adam + `MoRet` + `AdaGN` | 32.26 | 28.96 |\n| Adam + `MoRet` + `AdaGN` + `AdaClip` (**Stable-SPAM**) | 31.40 | 28.84 |\n| Adam + `MoRet` + `AdaGN` + <span style=\"color:#0000FF;\">SpikeClip</span> (Huang et al., 2025) | 32.01 | 28.90 |\n| Adam + `MoRet` + <span style=\"color:#0000FF;\">GradClip</span> (Goodfellow, 2016) + `AdaClip` | 31.95 | 29.87 |\n| Adam + `MoRet` + `AdaGN` + `AdaClip` (**Stable-SPAM**) | 31.40 | 28.84 |\n| Training Tokens | 1.1B |  |", "caption": "Table 4: Ablations on Stable-SPAM. Experiments are based on LLaMA-60M and C4.", "description": "\ud45c 4\ub294 Stable-SPAM\uc758 \uc131\ub2a5\uc5d0 \ub300\ud55c ablation study \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LLaMA-60M \ubaa8\ub378\uacfc C4 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec, Stable-SPAM\uc744 \uad6c\uc131\ud558\ub294 \uc138 \uac00\uc9c0 \uc8fc\uc694 \uc694\uc18c\uc778 MoRet, AdaGN, AdaClip\uc758 \uac01\uac01\uc758 \uae30\uc5ec\ub3c4\uc640 \uc870\ud569 \ud6a8\uacfc\ub97c \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uac01 \uc694\uc18c\ub97c \uc21c\ucc28\uc801\uc73c\ub85c \ucd94\uac00\ud558\uba74\uc11c \uc131\ub2a5 \ubcc0\ud654\ub97c \uce21\uc815\ud558\uace0, AdaClip \ub300\uc2e0 SpikeClip, AdaGN \ub300\uc2e0 GradClip\uc744 \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0\uc640 \ube44\uad50\ud558\uc5ec Stable-SPAM\uc758 \ub3c5\ucc3d\uc801\uc778 \ubd80\ubd84\uc758 \ud6a8\uacfc\ub97c \ud3c9\uac00\ud569\ub2c8\ub2e4.  BF16\uacfc FP4 \ub450 \uac00\uc9c0 \uc815\ubc00\ub3c4 \uc124\uc815\uc5d0\uc11c \uc2e4\ud5d8\uc744 \uc218\ud589\ud558\uc5ec, \uac01 \uc694\uc18c\uc758 \uc601\ud5a5\uc774 \uc815\ubc00\ub3c4\uc5d0 \ub530\ub77c \uc5b4\ub5bb\uac8c \ub2ec\ub77c\uc9c0\ub294\uc9c0\ub3c4 \ud655\uc778\ud569\ub2c8\ub2e4.", "section": "4.6. Ablation Study"}, {"content": "| Params | Hidden | Intermediate | Heads | Layers |\n|---|---|---|---|---|\n| 60M | 512 | 1376 | 8 | 8 |\n| 130M | 768 | 2048 | 12 | 12 |\n| 350M | 1024 | 2736 | 16 | 24 |\n| 1B | 2048 | 5461 | 24 | 32 |", "caption": "Table 5: Configurations of LLaMA models used in this paper.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c LLaMA \ubaa8\ub378\ub4e4\uc758 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378 \ud06c\uae30(\ud30c\ub77c\ubbf8\ud130 \uc218),  \ud788\ub4e0 \ucc28\uc6d0,  \uc911\uac04 \ucc28\uc6d0,  \ud5e4\ub4dc \uc218,  \ub808\uc774\uc5b4 \uc218 \ub4f1\uc758 \uc815\ubcf4\ub97c \ub2f4\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uc815\ubcf4\ub294 4\ube44\ud2b8 \ubc0f BF16 \uc0ac\uc804 \ud6c8\ub828 \uc2e4\ud5d8 \ubaa8\ub450\uc5d0 \uc801\uc6a9\ub429\ub2c8\ub2e4.  \ud45c\ub97c \ud1b5\ud574 \uac01 \ubaa8\ub378\uc758 \uad6c\uc870\uc801 \ucc28\uc774\ub97c \uba85\ud655\ud788 \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"content": "| Hyper-Parameters | LLaMA-130M | LLaMA-350M | LLaMA-1B |\n|---|---|---|---|\n| LR | 1e-3 | 4e-4 | 2e-4 |\n| \u0394T | 1000 | 1000 | 1000 |\n| \u03b3\u2081 | 0.7 | 0.7 | 0.7 |\n| \u03b3\u2082 | 0.9 | 0.9 | 0.9 |\n| \u03b3\u2083 | 0.999 | 0.999 | 0.999 |", "caption": "Table 6: Hyperparameters of Stable-SPAM for 4-bit pre-training experiments in this paper.", "description": "\uc774 \ud45c\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc218\ud589\ub41c 4\ube44\ud2b8 \uc0ac\uc804 \ud6c8\ub828 \uc2e4\ud5d8\uc5d0 \uc0ac\uc6a9\ub41c Stable-SPAM\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LLaMA-130M, LLaMA-350M, LLaMA-1B \uc138 \uac00\uc9c0 \ubaa8\ub378\uc5d0 \ub300\ud55c \ud559\uc2b5\ub960(LR), \ubaa8\uba58\ud140 \uc7ac\uc124\uc815 \uac04\uaca9(\u0394T), \uc801\uc751\uc801 \uadf8\ub798\ub514\uc5b8\ud2b8 \uc815\uaddc\ud654 \ubc0f \ud074\ub9ac\ud551\uc744 \uc704\ud55c \uc138 \uac00\uc9c0 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130(\u03b31, \u03b32, \u03b33) \uac12\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4. \uc774 \uac12\ub4e4\uc740 \ubaa8\ub378 \ud06c\uae30\uc5d0 \ub530\ub77c \uc57d\uac04\uc529 \ub2e4\ub974\uac8c \uc124\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 Stable-SPAM \ucd5c\uc801\ud654\uae30\uc758 \uc124\uc815\uc744 \uc774\ud574\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"content": "| Hyper-Parameters | LLaMA-60M | LLaMA-130M | LLaMA-350M | LLaMA-1B |\n|---|---|---|---|---|\n| LR | 1e-3 | 8e-4 | 4e-4 | 2e-4 |\n| \u0394T | 1000 | 1000 | 1000 | 1000 |\n| \u03b3\u2081 | 0.85 | 0.85 | 0.85 | 0.85 |\n| \u03b3\u2082 | 0.99999 | 0.99999 | 0.99999 | 0.99999 |\n| \u03b3\u2083 | 0.999 | 0.999 | 0.999 | 0.999 |", "caption": "Table 7: Hyperparameters of Stable-SPAM for BF6 pre-training experiments in this paper.", "description": "\ud45c 7\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c BF16 \uc0ac\uc804 \ud6c8\ub828 \uc2e4\ud5d8\uc744 \uc704\ud55c Stable-SPAM\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LLaMA-60M, LLaMA-130M, LLaMA-350M, LLaMA-1B \ub124 \uac00\uc9c0 \ubaa8\ub378 \ud06c\uae30\uc5d0 \ub300\ud574 \ud559\uc2b5\ub960(LR), \ubaa8\uba58\ud140 \uc7ac\uc124\uc815 \uac04\uaca9(\u0394T), AdaGN \ubc0f AdaClip\uc5d0 \uc0ac\uc6a9\ub418\ub294 \uac10\uc1e0\uc728(\u03b31, \u03b32, \u03b33) \ub4f1\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac12\uc744 \ud45c\ub85c \uc815\ub9ac\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\ub97c \ud1b5\ud574 \uac01 \ubaa8\ub378 \ud06c\uae30\uc5d0 \ub530\ub978 \ucd5c\uc801\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}]