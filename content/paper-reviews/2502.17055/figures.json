[{"figure_path": "https://arxiv.org/html/2502.17055/x1.png", "caption": "Figure 1: Performance of 4-bit LLM training. Experiments are conducted with LLaMA-130M/350M/1B models on C4 Dataset. Adam-BF16 denotes that the model is trained with BF16 by Adam. Perplexity on validation set is reported.", "description": "\uadf8\ub9bc 1\uc740 4\ube44\ud2b8 \uc815\ubc00\ub3c4\ub85c \ud559\uc2b5\ub41c \ub2e4\uc591\ud55c \ud06c\uae30\uc758 LLaMA \uc5b8\uc5b4 \ubaa8\ub378(1\uc5b5 3\ucc9c\ub9cc, 3\uc5b5 5\ucc9c\ub9cc, 10\uc5b5 \ud30c\ub77c\ubbf8\ud130)\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. C4 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ud588\uc73c\uba70, Adam-BF16\uc740 Adam \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \uc0ac\uc6a9\ud558\uc5ec BF16(Brain Float 16) \uc815\ubc00\ub3c4\ub85c \ud559\uc2b5\ud55c \ubaa8\ub378\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c \uce21\uc815\ud55c perplexity(\uc5bc\ub9c8\ub098 \uc608\uce21\uc774 \uc5b4\ub824\uc6b4\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 \uc9c0\ud45c, \ub0ae\uc744\uc218\ub85d \uc88b\uc74c)\ub97c \ud1b5\ud574 \ubaa8\ub378 \uc131\ub2a5\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.  4\ube44\ud2b8 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \uc635\ud2f0\ub9c8\uc774\uc800(Adam, Stable-SPAM \ub4f1)\uc640 BF16 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec, \ub0ae\uc740 \ube44\ud2b8 \uc815\ubc00\ub3c4\uc5d0\uc11c\ub3c4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\uae30 \uc704\ud55c \uc635\ud2f0\ub9c8\uc774\uc800\uc758 \ud6a8\uacfc\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. 4-bit LLM Training\uc758 \uc131\ub2a5"}, {"figure_path": "https://arxiv.org/html/2502.17055/x2.png", "caption": "Figure 2: Final validation loss when training LLaMA-130M on C4, sweeping across learning rates (LR). The vertical dotted line indicates that the model cannot be trained further as increasing the learning rate, i.e. Training loss becomes NaN. Red dashed horizontal lines indicate the best performance achieved.", "description": "\uadf8\ub9bc 2\ub294 \ud559\uc2b5\ub960(LR)\uc744 \ubcc0\ud654\uc2dc\ud0a4\uba74\uc11c LLaMA-130M \ubaa8\ub378\uc744 C4 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\uc2dc\ucf30\uc744 \ub54c\uc758 \ucd5c\uc885 \uac80\uc99d \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc218\uc9c1 \uc810\uc120\uc740 \ud559\uc2b5\ub960 \uc99d\uac00\uc5d0 \ub530\ub77c \ud559\uc2b5 \uc190\uc2e4\uc774 NaN\uc774 \ub418\uc5b4 \ub354 \uc774\uc0c1 \ud559\uc2b5\uc744 \uc9c4\ud589\ud560 \uc218 \uc5c6\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ube68\uac04\uc0c9 \uc810\uc120\uc740 \uac01 \ucd5c\uc801\ud654 \uae30\ubc95\uc5d0\uc11c \ub2ec\uc131\ud55c \ucd5c\uace0 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ucd5c\uc801\ud654 \uae30\ubc95\ub4e4\uc758 \ud559\uc2b5\ub960 \uc548\uc815\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "2. 4-bit Training Stability Investigation"}, {"figure_path": "https://arxiv.org/html/2502.17055/x3.png", "caption": "Figure 3: Effect of SpikeClip\u00a0(Huang et\u00a0al., 2025) on stabilizing training. Left: gradient norms before and after performing gradient spike clip. Right: training loss with and without gradient spike clip. Models are trained by Adam optimizer based on LLaMA-130M and C4.", "description": "\uadf8\ub9bc 3\uc740 2025\ub144 Huang \ub4f1\uc758 \uc5f0\uad6c\uc5d0\uc11c \uc81c\uc548\ub41c SpikeClip \uae30\ubc95\uc774 \ud6c8\ub828 \uc548\uc815\uc131\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc\uc758 \uc67c\ucabd\uc740 \uae30\uc6b8\uae30 \uc2a4\ud30c\uc774\ud06c \ud074\ub9ac\ud551\uc744 \uc218\ud589\ud558\uae30 \uc804\uacfc \ud6c4\uc758 \uae30\uc6b8\uae30 \ub188(norm)\uc744 \ube44\uad50\ud558\uace0, \uc624\ub978\ucabd\uc740 \uae30\uc6b8\uae30 \uc2a4\ud30c\uc774\ud06c \ud074\ub9ac\ud551 \uc801\uc6a9 \uc720\ubb34\uc5d0 \ub530\ub978 \ud6c8\ub828 \uc190\uc2e4\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.  LLaMA-130M \ubaa8\ub378\uacfc C4 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec Adam \uc635\ud2f0\ub9c8\uc774\uc800 \uae30\ubc18\uc73c\ub85c \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ucf30\uc2b5\ub2c8\ub2e4. 4\ube44\ud2b8 \ud6c8\ub828\uc5d0\uc11c \ubc1c\uc0dd\ud558\ub294 \ubd88\uc548\uc815\ud55c \uae30\uc6b8\uae30 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud55c SpikeClip\uc758 \ud6a8\uacfc\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4. \uae30\uc6b8\uae30 \ub188\uc758 \ubcc0\ud654\uc640 \ud6c8\ub828 \uc190\uc2e4\uc758 \ubcc0\ud654\ub97c \ud1b5\ud574 SpikeClip\uc774 \uae30\uc6b8\uae30 \uc2a4\ud30c\uc774\ud06c\ub97c \uc644\ud654\ud558\uace0 \ud6c8\ub828 \uc548\uc815\uc131\uc744 \uac1c\uc120\ud558\ub294 \ud6a8\uacfc\ub97c \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2. 4-bit Training Stability Investigation"}, {"figure_path": "https://arxiv.org/html/2502.17055/x4.png", "caption": "Figure 4: Training loss and gradient norm of Adam using various learning rates with BF16 and FP4 precision. Experiments are conducted under the same training configuration with LLaMA-130M/350M.", "description": "\uadf8\ub9bc 4\ub294 LLaMA-130M \ubc0f 350M \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec BF16 \ubc0f FP4 \uc815\ubc00\ub3c4\ub85c \ub2e4\uc591\ud55c \ud559\uc2b5\ub960\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c Adam\uc758 \ud559\uc2b5 \uc190\uc2e4\uacfc \uadf8\ub798\ub514\uc5b8\ud2b8 \uaddc\ubc94\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc2e4\ud5d8\uc740 \ub3d9\uc77c\ud55c \ud559\uc2b5 \uad6c\uc131\uc73c\ub85c \uc218\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \uac01 \uadf8\ub798\ud504\ub294 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uc190\uc2e4\uacfc \uadf8\ub798\ub514\uc5b8\ud2b8 \uaddc\ubc94\uc758 \ubcc0\ud654\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ub098\ud0c0\ub0b4\uc5b4, \ud559\uc2b5\ub960\uc5d0 \ub530\ub978 \ubaa8\ub378\uc758 \uc548\uc815\uc131\uacfc \uc218\ub834 \uc18d\ub3c4\ub97c \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \ud2b9\ud788, FP4 \uc815\ubc00\ub3c4\uc5d0\uc11c \ud559\uc2b5\ub960\uc774 \ub192\uc544\uc9c8\uc218\ub85d \uadf8\ub798\ub514\uc5b8\ud2b8 \uaddc\ubc94\uc758 \ubd88\uc548\uc815\uc131\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4-bit \ud559\uc2b5 \uc548\uc815\uc131 \ubd84\uc11d"}, {"figure_path": "https://arxiv.org/html/2502.17055/x5.png", "caption": "Figure 5: StableSPAM under Extremely Low-Precision Training. Experiments are conducted with 350M models on C4 Dataset. BF16-Adam denotes that the model is trained with BF16 by Adam. The final loss on validation set is reported.", "description": "\uadf8\ub9bc 5\ub294 \ub9e4\uc6b0 \ub0ae\uc740 \uc815\ubc00\ub3c4(extremely low-precision) \ud559\uc2b5 \ud658\uacbd\uc5d0\uc11c StableSPAM\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc2e4\ud5d8\uc740 3\uc5b5 5\ucc9c\ub9cc(350M) \ud30c\ub77c\ubbf8\ud130\ub97c \uac00\uc9c4 LLaMA \ubaa8\ub378\uc744 C4 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\uc2dc\ucf1c \uc9c4\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4. BF16-Adam\uc740 Adam \uc635\ud2f0\ub9c8\uc774\uc800\ub97c BF16(Brain Float 16) \uc815\ubc00\ub3c4\ub85c \ud559\uc2b5\uc2dc\ud0a8 \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uadf8\ub798\ud504\ub294 \uac80\uc99d \uc138\ud2b8(validation set)\uc5d0\uc11c\uc758 \ucd5c\uc885 \uc190\uc2e4(final loss)\uc744 \ubcf4\uc5ec\uc8fc\uba70, StableSPAM\uc774 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc5d0 \ube44\ud574 \ub0ae\uc740 \uc190\uc2e4\uc744 \ub2ec\uc131\ud568\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \ub0ae\uc740 \ube44\ud2b8 \uc815\ubc00\ub3c4 \ud658\uacbd\uc5d0\uc11c\ub3c4 StableSPAM\uc774 \ud6a8\uacfc\uc801\uc73c\ub85c \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0ac \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "4.2. \ub9e4\uc6b0 \ub0ae\uc740 \uc815\ubc00\ub3c4 \ud559\uc2b5\uc758 \uc131\ub2a5(Performence of Extremely Low-Precision Training)"}, {"figure_path": "https://arxiv.org/html/2502.17055/x6.png", "caption": "Figure 6: Performance of BF16 training with various model sizes. Experiments are based on LLaMA models trained on C4 Dataset.", "description": "\uadf8\ub9bc 6\uc740 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 LLaMA \ubaa8\ub378\uc744 C4 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\uc2dc\ud0a8 BF16(Brain Floating Point 16-bit) \ud559\uc2b5\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uadf8\ub798\ud504\ub294 \ubaa8\ub378 \ud06c\uae30\ubcc4(LLaMA-130M, LLaMA-350M, LLaMA-1B) \uac80\uc99d \uc138\ud2b8\uc758 perplexity\ub97c \uc5c5\ub370\uc774\ud2b8 \ub2e8\uacc4\uc5d0 \ub530\ub77c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  Stable-SPAM\uacfc Adam \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc744 \ube44\uad50\ud558\uc5ec, Stable-SPAM\uc774 \ub2e4\uc591\ud55c \ubaa8\ub378 \ud06c\uae30\uc5d0\uc11c Adam\ubcf4\ub2e4 \ub0ae\uc740 perplexity\ub97c \ub2ec\uc131\ud558\uace0 \ub354 \ube60\ub974\uac8c \uc218\ub834\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\uac83\uc740 Stable-SPAM\uc758 \ud6a8\uc728\uc131\uacfc \uc548\uc815\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "4.3. BF16 LLM \ud559\uc2b5 \uc131\ub2a5"}, {"figure_path": "https://arxiv.org/html/2502.17055/x7.png", "caption": "Figure 7: Effect of AdaGN and AdaClip on stabilizing FP4 LLM training. The left two figures use LLaMA-130M (LR = 3e-3), and the right two figures use LLaMA-60M.", "description": "\uadf8\ub9bc 7\uc740 \uc81c\uc548\ub41c AdaGN\uacfc AdaClip \uae30\ubc95\uc774 4\ube44\ud2b8 FP4 LLM \ud559\uc2b5 \uc548\uc815\ud654\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd \ub450 \uadf8\ub798\ud504\ub294 LLaMA-130M \ubaa8\ub378(\ud559\uc2b5\ub960 3e-3)\uc744, \uc624\ub978\ucabd \ub450 \uadf8\ub798\ud504\ub294 LLaMA-60M \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. AdaGN\uacfc AdaClip\uc744 \ucd94\uac00\ud568\uc73c\ub85c\uc368, \ud559\uc2b5 \uc190\uc2e4\uc758 \ubc1c\uc0b0\uc744 \ubc29\uc9c0\ud558\uace0 \uae30\uc6b8\uae30 \uaddc\ubc94\uc758 \uae09\uaca9\ud55c \ubcc0\ud654\ub97c \uc644\ud654\ud558\uc5ec \ud559\uc2b5 \uacfc\uc815\uc758 \uc548\uc815\uc131\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788 AdaClip\uc758 \ucd94\uac00\ub294 \ubd88\uc548\uc815\uc131\uc744 \ub354\uc6b1 \uc644\ud654\ud558\uc5ec \ud559\uc2b5\ub960 \ubcc0\ud654\uc5d0\ub3c4 \uc548\uc815\uc801\uc778 \uc131\ub2a5\uc744 \uc720\uc9c0\ud558\ub294 \ub370 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.5. \ud6a8\uacfc\uc801\uc778 \uc548\uc815\ud654 \uc601\uc5ed"}, {"figure_path": "https://arxiv.org/html/2502.17055/x8.png", "caption": "Figure 8:  Hyper-parameter Analysis. Experiments are conducted with FP4 training on LLaMA-60M and C4 with 1.1B tokens.", "description": "\uadf8\ub9bc 8\uc740 Stable-SPAM\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LLaMA-60M \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec C4 \ub370\uc774\ud130\uc14b\uc758 11\uc5b5 \ud1a0\ud070\uc73c\ub85c FP4(1-bit \uc9c0\uc218, 2-bit \uac00\uc218\ubd80) \uc815\ub7c9\ud654 \uc778\uc2dd \ud6c8\ub828\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4.  \u03b3\u2081, \u03b3\u2082, \u03b3\u2083, \u0394T \ub124 \uac00\uc9c0 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\uc758 \ubcc0\ud654\uc5d0 \ub530\ub978 \ucd5c\uc885 perplexity \uac12\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\ub85c, \uac01 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\uc758 \uc801\uc808\ud55c \uac12\uc744 \ucc3e\ub294 \uc2e4\ud5d8 \uacb0\uacfc\uc785\ub2c8\ub2e4.  \uac01 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\uc758 \uac12\uc744 \ubcc0\ud654\uc2dc\ucf1c\uac00\uba70 \ucd5c\uc801\uc758 perplexity\ub97c \ub3c4\ucd9c\ud558\uae30 \uc704\ud55c \uc2e4\ud5d8\uc744 \ubc18\ubcf5\ud55c \uacb0\uacfc\ub97c \uc2dc\uac01\ud654\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ucd5c\uc801\uac12\uc744 \ucc3e\ub294 \uacfc\uc815\uacfc \uac01 \uac12\uc758 \uc601\ud5a5\uc744 \uc774\ud574\ud558\ub294\ub370 \ub3c4\uc6c0\uc774 \ub418\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4.", "section": "4.7 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ubd84\uc11d"}, {"figure_path": "https://arxiv.org/html/2502.17055/x9.png", "caption": "Figure 9: Test Loss during Training Process on Weather Time-series Data. Anomalous data is generated by adding Gaussian noise to 10% of randomly selected input values. Specifically, the anomalies data are conducted with X=X+Gaussin\u2062(0,Severity\u2217Max\u2062(X))\ud835\udc4b\ud835\udc4bGaussin0SeverityMax\ud835\udc4bX=X+\\texttt{Gaussin}(0,\\texttt{Severity}*\\texttt{Max}(X))italic_X = italic_X + Gaussin ( 0 , Severity \u2217 Max ( italic_X ) ) where X\ud835\udc4bXitalic_X is the inputs.", "description": "\uadf8\ub9bc 9\ub294 \uae30\uc0c1 \uc2dc\uac04 \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud55c \ud6c8\ub828 \uacfc\uc815\uc5d0\uc11c\uc758 \ud14c\uc2a4\ud2b8 \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\uc0c1 \ub370\uc774\ud130\ub294 \ubb34\uc791\uc704\ub85c \uc120\ud0dd\ub41c \uc785\ub825\uac12\uc758 10%\uc5d0 \uac00\uc6b0\uc2dc\uc548 \ub178\uc774\uc988\ub97c \ucd94\uac00\ud558\uc5ec \uc0dd\uc131\ub429\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c, \uc774\uc0c1 \ub370\uc774\ud130\ub294 X=X+Gaussin(0,Severity*Max(X)) \uacf5\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ub429\ub2c8\ub2e4. \uc5ec\uae30\uc11c X\ub294 \uc785\ub825\uac12\uc774\uace0, Severity\ub294 \uc774\uc0c1\uac12\uc758 \uc2ec\uac01\ub3c4\ub97c \ub098\ud0c0\ub0b4\ub294 \ub9e4\uac1c\ubcc0\uc218\uc785\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc774\uc0c1\uac12 \uc2ec\uac01\ub3c4(A=0%, A=5%, A=10%)\uc5d0\uc11c Adam, SPAM, Stable-SPAM \uc138 \uac00\uc9c0 \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec Stable-SPAM\uc758 \uac15\uac74\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "B. \uc2dc\uac04 \uc2dc\uacc4\uc5f4 \uc608\uce21 \uc791\uc5c5"}]