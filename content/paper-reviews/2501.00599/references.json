{"references": [{"fullname_first_author": "Zesen Cheng", "paper_title": "VideoLLaMA 2: Advancing spatial-temporal modeling and audio understanding in video-LLMs", "publication_date": "2024-06-07", "reason": "This paper is the basis for the VideoRefer model developed in this work, providing foundational advancements in video-LLM architecture."}, {"fullname_first_author": "Tsai-Shien Chen", "paper_title": "Panda-70M: Captioning 70M videos with multiple cross-modality teachers", "publication_date": "2024-00-00", "reason": "This paper provides the large-scale dataset (Panda-70M) used to create VideoRefer-700K, which is crucial for the model's training and evaluation."}, {"fullname_first_author": "Zhe Chen", "paper_title": "How far are we to GPT-4V? Closing the gap to commercial multimodal models with open-source suites", "publication_date": "2024-04-16", "reason": "This paper introduces InternVL-26B, the model used as the annotator in the VideoRefer-700K dataset creation process."}, {"fullname_first_author": "Jordi Pont-Tuset", "paper_title": "The 2017 DAVIS challenge on video object segmentation", "publication_date": "2017-04-00", "reason": "This paper is a source of video data for the VideoRefer-Bench benchmark, contributing to a comprehensive evaluation of the model."}, {"fullname_first_author": "Henghui Ding", "paper_title": "MeVis: A large-scale benchmark for video segmentation with motion expressions", "publication_date": "2023-10-00", "reason": "This paper provides another source of video data for VideoRefer-Bench, enhancing the robustness and diversity of the benchmark."}]}