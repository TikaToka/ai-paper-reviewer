{"references": [{"fullname_first_author": "Denis Kocetkov", "paper_title": "The stack: 3 tb of permissively licensed source code", "publication_date": "2022-00-00", "reason": "This paper is the primary dataset used for deduplication in creating The Heap, thus fundamentally impacting its creation and ensuring contamination-free evaluation."}, {"fullname_first_author": "Anton Lozhkov", "paper_title": "Starcoder 2 and the stack v2: The next generation", "publication_date": "2024-02-19", "reason": "As a more recent and significantly larger dataset compared to the original Stack, it's crucial for comprehensive deduplication, minimizing the chance of contamination in The Heap."}, {"fullname_first_author": "Leo Gao", "paper_title": "The pile: An 800gb dataset of diverse text for language modeling", "publication_date": "2020-01-01", "reason": "Being one of the largest and most influential language modeling datasets, its inclusion in the deduplication process is essential for ensuring The Heap's independence from widely used training data."}, {"fullname_first_author": "Together Computer", "paper_title": "Redpajama: An open source recipe to reproduce llama training dataset", "publication_date": "2023-00-00", "reason": "This dataset, used in deduplication, helps ensure The Heap's independence from other commonly used large language model training datasets."}, {"fullname_first_author": "Jonathan Katzy", "paper_title": "An exploratory investigation into code license infringements in large language model training datasets", "publication_date": "2024-00-00", "reason": "This paper directly addresses the issue of data contamination in LLM training datasets, providing valuable context and justification for The Heap's methodology."}]}