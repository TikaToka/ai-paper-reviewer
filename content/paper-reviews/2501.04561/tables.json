[{"content": "| Hyperparameter | I | II | III | IV | V |\n|---|---|---|---|---|---| \n| batch size | 256 | 128 | 128 | 32 | 32 |\n| lr | 1e-3 | 1e-3 | 5e-5 | 5e-4 | 5e-4 |\n| warmup ratio | 0.3 | 0.3 | 0.3 | 0.3 | 0.3 |\n| epoch | 1 | 1 | 1 | 3 | 3 |\n| freeze LLM | \u2714 | \u2714 | \u2718 | \u2714 | \u2714 |\n| optimizer | AdamW | AdamW | AdamW | AdamW | AdamW |\n| cost | 40 GPU\u22c5H | 80 GPU\u22c5H | 500 GPU\u22c5H | 36 GPU\u22c5H | 8 GPU\u22c5H |\n| dataset | 1-1 | 2-1 | 2-2 | 3-1 | 3-2 |\n| loss | $\nmathcal{L}_{s2t}$ | $\nmathcal{L}_{i2t}$ | $\nmathcal{L}_{i2t}^{I}$ | $\nmathcal{L}_{ctc}$ | $\nmathcal{L}_{dpo}$ |", "caption": "Table 1: The detailed training setup for OpenOmni and the hyper-parameters across the training stage. All experiments are conducted in 8xA100 setting. Please refer to Appendix Sec.\u00a06 for more details about dataset.", "description": "\ud45c 1\uc740 OpenOmni \ubaa8\ub378\uc758 \ud559\uc2b5 \uc124\uc815\uacfc \uac01 \ud559\uc2b5 \ub2e8\uacc4\ubcc4 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  8\uac1c\uc758 A100 GPU\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub4e0 \uc2e4\ud5d8\uc744 \uc9c4\ud589\ud588\uc73c\uba70, \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ubd80\ub85d 6\uc808\uc744 \ucc38\uc870\ud558\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ubc30\uce58 \ud06c\uae30, \ud559\uc2b5\ub960, \uc6dc\uc5c5 \ube44\uc728, \uc5d0\ud3ed, LLM \uace0\uc815 \uc5ec\ubd80, \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998, \uc0ac\uc6a9\ub41c GPU \uc218, \ub370\uc774\ud130\uc14b, \uc190\uc2e4 \ud568\uc218 \ub4f1\uc758 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac12\uc740 \ud559\uc2b5 \ub2e8\uacc4(I~V)\uc5d0 \ub530\ub77c \ub2e4\ub974\uac8c \uc124\uc815\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. \ubc29\ubc95"}, {"content": "| Method | Action & Activity | Story & Description | Plot & Inference | Identification & Description | Contextual & Environmental | Identity & Relationship | Text & Symbols | Count & Quantity | Overall |\n|---|---|---|---|---|---|---|---|---|---| \n| AnyGPT (7B) [51] | 5.98 | 8.70 | 7.59 | 4.74 | 5.67 | 12.50 | 8.00 | 20.00 | 7.01 |\n| Video-SALMONN (13B) [39] | 28.69 | 25.65 | 24.47 | 23.22 | 29.08 | 21.83 | 52.00 | 26.63 | 26.53 |\n| UnifiedIO2-Large (1.1B) [29] | 28.29 | 22.17 | 32.49 | 30.81 | 28.37 | 21.83 | 16.00 | 13.33 | 27.76 |\n| UnifiedIO2-XLarge (3.2B) [29] | 30.28 | 26.52 | 30.38 | 31.75 | 28.37 | 18.75 | 28.00 | 26.63 | 29.16 |\n| UnifiedIO2-XXLarge (6.8B) [29] | 27.49 | 23.04 | 28.69 | 25.59 | 26.95 | 12.50 | 12.00 | 46.67 | 25.92 |\n| VITA (7x8B) [15] | 33.47 | 34.35 | 27.00 | 36.02 | 43.97 | 31.25 | 24.00 | 6.67 | 33.45 |\n| OpenOmni (7B) | 36.65 | 45.65 | 32.91 | 44.08 | 48.23 | 34.38 | 24.00 | 33.33 | 37.40 |", "caption": "Table 2: Overall omni-understanding results on OmniBench. We conducte a performance comparison of omni-understanding among various fully open-source Omnimodal Large Language Models (OLLMs) on OmniBench. Due to a concurrent work, EMOVA\u00a0[5], not being open-sourced, we are unable to present its results. Notably, compared to the state-of-the-art OLLM, VITA\u00a0[15], which was trained on tri-modal data, OpenOmni achieves comparable advanced performance using significantly less training data and a smaller model size.", "description": "\ubcf8 \ud45c\ub294 OmniBench\ub77c\ub294 \ubca4\uce58\ub9c8\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \uc624\ud508\uc18c\uc2a4 \ub2e4\uc911 \ubaa8\ub4dc \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(OLLM)\uc758 \uc804\ubc18\uc801\uc778 \ub2e4\uc911 \ubaa8\ub4dc \uc774\ud574 \ub2a5\ub825\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  EMOVA[5]\ub294 \ube44\uacf5\uac1c \ubaa8\ub378\uc774\ubbc0\ub85c \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, OpenOmni\ub294 \ucd5c\ucca8\ub2e8 OLLM\uc778 VITA[15](3\ubaa8\ub4dc \ub370\uc774\ud130\ub85c \ud559\uc2b5)\uc5d0 \ube44\ud574 \ud6e8\uc52c \uc801\uc740 \ud559\uc2b5 \ub370\uc774\ud130\uc640 \uc791\uc740 \ubaa8\ub378 \ud06c\uae30\ub85c \ube44\uc2b7\ud558\uac70\ub098 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ub2ec\uc131\ud588\ub2e4\ub294 \uc810\uc774 \uc8fc\ubaa9\ud560 \ub9cc\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Model | w/ Audio IO | PT | IT | MMStar | MMB | MMB<sup>CN</sup> | HallBench | MathVista<sup>M</sup> | MMMU<sup>V</sup> | AI2D | RWQA |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| **Proprietary Models** |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o | \u2713 | \u2013 | \u2013 | - | 83.4 | 82.1 | 55.0 | 63.8 | 69.1 | - | 75.4 |\n| GPT-4o-mini | \u2713 | \u2013 | \u2013 | - | - | - | 46.1 | 52.4 | 60.0 | - | 67.1 |\n| **Weight Open-Source** |  |  |  |  |  |  |  |  |  |  |  |\n| MiniCPM-V2.5 (8B) [48] | \u2717 | 570M | 9.1M | 51.3 | 76.7 | 73.3 | 42.5 | 54.3 | 45.8 | - | 63.5 |\n| Qwen2-VL-Chat (7B) [4] | \u2717 | 1.4B | - | 60.7 | 86.4 | 81.9 | 50.6 | 58.2 | 52.7 | - | 69.7 |\n| Baichuan-Omni (7B) [23] | \u2713 | \u2013 | 8M | - | 76.2 | 74.9 | 47.8 | 51.9 | 47.3 | - | 62.6 |\n| EMOVA (8B) [5] | \u2713 | 7.4M | 4.4M | - | 82.8 | - | - | 61.1 | - | 82.8 | 64.3 |\n| **Fully Open-Source** |  |  |  |  |  |  |  |  |  |  |  |\n| Cambrain-I (8B) [42] | \u2717 | 2.5M | 7M | **50.7** | - | - | 34.3 | 47.0 | 41.8 | 73.1 | 64.2 |\n| MMEvol (7B) [32] | \u2717 | 0.6M | 1.5M | 51.6 | 74.6 | 74.3 | 42.9 | 52.4 | 45.1 | 74.7 | 63.9 |\n| VITA (8x7B) [15] | \u2713 | \u2013 | 5M | - | 74.7 | 71.4 | 39.7 | 44.9 | **45.3** | 74.3 | 59.0 |\n| OpenOmni (7B) | \u2713 | 0.6M | 1.8M | 51.8 | **76.2** | **76.4** | **44.2** | **52.7** | 44.8 | **74.8** | **64.0** |", "caption": "Table 3: Comparison with state-of-the-art methods on visual-language benchmarks, with the indication of audio input/output support. We mark the best performance bold among fully open-source models. The results indicate that incorporating audio input and output can further enhance the model\u2019s visual-language capabilities.", "description": "\ud45c 3\uc740 \uc2dc\uac01 \uc5b8\uc5b4 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ucd5c\ucca8\ub2e8 \ubc29\ubc95\uacfc OpenOmni \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. \uc624\ub514\uc624 \uc785\ucd9c\ub825 \uc9c0\uc6d0 \uc5ec\ubd80\ub97c \ud45c\uc2dc\ud558\uace0, \uc644\uc804\ud788 \uacf5\uac1c\ub41c \ubaa8\ub378 \uc911 \ucd5c\uace0 \uc131\ub2a5\uc744 \uad75\uac8c \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0 \ub530\ub974\uba74, \uc624\ub514\uc624 \uc785\ucd9c\ub825\uc744 \ud1b5\ud569\ud558\uba74 \uc2dc\uac01 \uc5b8\uc5b4 \uae30\ub2a5\uc774 \ud5a5\uc0c1\ub428\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. OpenOmni\ub294 \ub2e4\ub978 \uc644\uc804\ud788 \uacf5\uac1c\ub41c \ubaa8\ub378\uc5d0 \ube44\ud574 \uc0c1\ub300\uc801\uc73c\ub85c \uc801\uc740 \uc591\uc758 \ub370\uc774\ud130\uc640 \uc791\uc740 \ubaa8\ub378 \ud06c\uae30\ub85c \uacbd\uc7c1\ub825 \uc788\ub294 \uacb0\uacfc\ub97c \ub2ec\uc131\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Model | AIShell-2(ZH-CER) |  |  | Librispeech(EN-WER) |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n|  | Dev |  |  | Test_clean |  |  | Test_other |  |\n|  | S2T | T2S | S2T | T2S | S2T | T2S | S2T | T2S |\n| **Speech LLM** |  |  |  |  |  |  |  |  |\n| SpeechT5 [2] | - | - | - | - | 2.4 | - | 5.8 | - |\n| SALMONN [40] | - | - | - | - | 2.1 | - | 4.9 | - |\n| Mini-Omni [47] | - | - | - | - | 4.7 | - | 9.4 | - |\n| Freeze-Omni [44] | - | - | - | - | 3.2 | - | 7.7 | - |\n| Qwen2-Audio [8] | 3.1 | - | 3.3 | - | 2.0 | - | 4.5 | - |\n| **Omnimodal LLM** |  |  |  |  |  |  |  |  |\n| AnyGPT [51] | - | - | - | - | 8.5 | - | - | - |\n| VITA [15] | - | - | - | - | 8.1 | - | 18.4 | - |\n| EMOVA [5] | 10.3 | 7.9 | - | - | 4.0 | 3.4 | - | - |\n| OpenOmni | 6.8 | 7.3 | 6.9 | 13.1 | 3.1 | 2.6 | 4.1 | 5.6 |", "caption": "Table 4: Comparison with state-of-the-art methods on speech-language benchmarks. We mark the best performance bold.", "description": "\ud45c 4\ub294 \ucd5c\ucca8\ub2e8 \uc74c\uc131 \uc5b8\uc5b4 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4.  \uc790\uc138\ud788 \uc0b4\ud3b4\ubcf4\uba74, \uc74c\uc131-\ud14d\uc2a4\ud2b8 \ubcc0\ud658(Speech-to-Text, S2T) \ubc0f \ud14d\uc2a4\ud2b8-\uc74c\uc131 \ubcc0\ud658(Text-to-Speech, T2S) \uc791\uc5c5\uc5d0 \ub300\ud55c \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5 \uc9c0\ud45c(\uc608: \ub2e8\uc5b4 \uc624\ub958\uc728, \ubb38\uc790 \uc624\ub958\uc728)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \ucd5c\uace0 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0b4\ub294 \ubaa8\ub378\uc774 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 OpenOmni \ubaa8\ub378\uc758 \uc74c\uc131 \uc5b8\uc5b4 \ucc98\ub9ac \ub2a5\ub825\uc744 \uae30\uc874 \ucd5c\uace0 \uc131\ub2a5 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50 \ud3c9\uac00\ud558\uc5ec \uadf8 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc8fc\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Model | Lang | Angry & Disgusted | Fearful | Happy | Neutral | Other | Sad | Surprised | Overall |\n|---|---|---|---|---|---|---|---|---|---| \n| OpenOmni | ZH | 89.7 | 54.8 | 33.3 | 92.3 | 51.6 | 60.2 | 23.7 | 57.9 |\n| w/ DPO | ZH | **96.6** | **78.4** | **37.7** | **97.1** | **62.8** | **90.7** | **29.8** | **70.4** |\n| OpenOmni | EN | 89.2 | 68.7 | 57.5 | 91.9 | 48.0 | 75.6 | 7.5 | 62.6 |\n| w/ DPO | EN | **91.3** | **70.4** | **60.6** | **94.6** | **49.6** | **77.3** | **13.9** | **65.4** |", "caption": "Table 5: Overall self-aware emotional speech generation results on the test set of bilingual EO2S-9K. Through the emotional speech direct preference optimization algorithm, OpenOmni has consistently improved emotional speech generation capabilities for both Chinese and English. The accuracy of emotional speech generation has improved by an average of 7.6% (from 60.3% to 67.9%), with particularly notable improvements observed in the generation of emotional speech categories such as Fearful, Surprised, and Sad.", "description": "\ud45c 5\ub294 \uc774\uc911 \uc5b8\uc5b4 EO2S-9K \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0 \ub300\ud55c OpenOmni\uc758 \uc804\ubc18\uc801\uc778 \uc790\uae30 \uc778\uc2dd \uac10\uc815\uc801 \uc74c\uc131 \uc0dd\uc131 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac10\uc815\uc801 \uc74c\uc131 \uc9c1\uc811 \uc120\ud638\ub3c4 \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc744 \ud1b5\ud574 OpenOmni\ub294 \uc911\uad6d\uc5b4\uc640 \uc601\uc5b4 \ubaa8\ub450\uc5d0\uc11c \uac10\uc815\uc801 \uc74c\uc131 \uc0dd\uc131 \uae30\ub2a5\uc774 \uc9c0\uc18d\uc801\uc73c\ub85c \ud5a5\uc0c1\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uac10\uc815\uc801 \uc74c\uc131 \uc0dd\uc131 \uc815\ud655\ub3c4\ub294 \ud3c9\uade0 7.6% \ud5a5\uc0c1(60.3%\uc5d0\uc11c 67.9%\ub85c)\ub418\uc5c8\uc73c\uba70, \ud2b9\ud788 \ub450\ub824\uc6c0, \ub180\ub78c, \uc2ac\ud514\uacfc \uac19\uc740 \uac10\uc815\uc801 \uc74c\uc131 \ubc94\uc8fc \uc0dd\uc131\uc5d0\uc11c \ub208\uc5d0 \ub744\ub294 \uac1c\uc120\uc774 \uad00\ucc30\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"content": "| Layers | Experts | Wenetspeech(ZH)\nTest_Net | Wenetspeech(ZH)\nTest_Meeting | Librispeech(EN)\nTest_clean | Librispeech(EN)\nTest_other | \n|---|---|---|---|---|---| \n| 2 | 1 | 113.6 | 129.7 | 87.8 | 96.5 | \n| 2 | 2 | 16.7 | 22.3 | 10.7 | 14.6 | \n| 2 | 4 | 8.5 | 8.4 | 4.2 | 4.7 | \n| 4 | 4 | 7.3 | 7.9 | 3.8 | 4.3 | \n| 6 | 4 | 6.4 | 6.7 | 4.1 | 4.5 |", "caption": "Table 6: Ablation study on the number of layers and experts in the speech decoder. Increasing experts in the mixture of experts module stabilizes CTC loss during training and enhances speech generation capacity. Deeper transformer layers improve English and Chinese speech generation, with greater benefits for Chinese.", "description": "\ubcf8 \ud45c\ub294 \uc74c\uc131 \ub514\ucf54\ub354\uc758 \uacc4\uce35\uacfc \uc804\ubb38\uac00 \uc218\uc5d0 \ub300\ud55c ablation \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc804\ubb38\uac00 \ud63c\ud569 \ubaa8\ub4c8\uc5d0\uc11c \uc804\ubb38\uac00 \uc218\ub97c \ub298\ub9ac\uba74 \ud559\uc2b5 \uc911 CTC \uc190\uc2e4\uc774 \uc548\uc815\ud654\ub418\uace0 \uc74c\uc131 \uc0dd\uc131 \uc6a9\ub7c9\uc774 \ud5a5\uc0c1\ub429\ub2c8\ub2e4. \ub354 \uae4a\uc740 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uacc4\uce35\uc740 \uc601\uc5b4\uc640 \uc911\uad6d\uc5b4 \uc74c\uc131 \uc0dd\uc131\uc744 \ubaa8\ub450 \uac1c\uc120\ud558\uba70, \uc911\uad6d\uc5b4\uc5d0 \ub300\ud55c \ud6a8\uacfc\uac00 \ub354 \ud07d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uacc4\uce35 \uc218\uc640 \uc804\ubb38\uac00 \uc218\ub97c \ubcc0\uacbd\ud588\uc744 \ub54c, \ub2e4\uc591\ud55c \uc124\uc815(\uacc4\uce35 \uc218, \uc804\ubb38\uac00 \uc218)\uc5d0\uc11c\uc758  \uc601\uc5b4(Librispeech)\uc640 \uc911\uad6d\uc5b4(Wenetspeech) \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ud14c\uc2a4\ud2b8 \uacb0\uacfc (Test_Net, Test_Meeting, Test_clean, Test_other) \uac00 \ub098\ud0c0\ub098 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uc124\uc815\uc5d0 \ub530\ub978 \uc131\ub2a5(WER \ud639\uc740 CER)\uc744 \ube44\uad50\ud558\uc5ec \ucd5c\uc801\uc758 \ubaa8\ub378 \uad6c\uc870\ub97c \ud30c\uc545\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.", "section": "4.1 Implementation Details"}, {"content": "| Method | Timbre | Tone | Melody | Space | Time | Hall | Intricacy | Overall |\n|---|---|---|---|---|---|---|---|---|\n| OneLLM (7B) [21] | 25.0 | 25.5 | 21.5 | 37.5 | 29.3 | 25.5 | 38.4 | 27.4 |\n| PandaGPT (7B) [38] | 23.5 | 23.2 | 27.6 | 45.0 | 23.8 | 28.0 | 23.9 | 26.7 |\n| Video-LLaMA (7B) [54] | 25.5 | 22.3 | 24.4 | 30.0 | 26.2 | 25.0 | 30.7 | 26.1 |\n| Video-LLaMA2(7B) [7] | 24.1 | 25.5 | 26.4 | 30.0 | 27.2 | 33.0 | 34.5 | 26.8 |\n| AnyGPT (7B) [51] | 24.6 | 25.0 | 26.4 | 27.5 | 29.2 | 29.0 | 25.7 | 26.1 |\n| NexTGPT (7B) [45] | 23.3 | 20.9 | 27.8 | 30.0 | 28.8 | 28.5 | 23.6 | 25.5 |\n| VITA (7x8B) [15] | 24.1 | 26.4 | 27.8 | 22.5 | 26.3 | 31.0 | 36.8 | 26.4 |\n| OpenOmni (7B) | 23.9 | 27.7 | 25.9 | 60.0 | 25.2 | 29.5 | 37.6 | 32.8 |", "caption": "Table 7: Overall omni-understanding results on AV-Odyssey Bench. We conducte a performance comparison of omni-understanding among various fully open-source Omnimodal Large Language Models (OLLMs) on AV-Odyssey Bench. Compared to the state-of-the-art OLLM, VITA\u00a0[15], which was trained on tri-modal data, OpenOmni achieves comparable advanced performance using significantly less training data and a smaller model size.", "description": "\ud45c 7\uc740 AV-Odyssey \ubca4\uce58\ub9c8\ud06c\ub97c \uc0ac\uc6a9\ud55c \ub2e4\uc591\ud55c \uc624\ud508\uc18c\uc2a4 \ub2e4\uc911 \ubaa8\ub2ec \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(OLLM)\uc758 \uc804\ubc18\uc801\uc778 \ub2e4\uc911 \ubaa8\ub2ec \uc774\ud574 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \uae30\uc874 \ucd5c\ucca8\ub2e8 \ubaa8\ub378\uc778 VITA [15]\uc640 \ube44\uad50\ud558\uc5ec OpenOmni \ubaa8\ub378\uc740 \ud6e8\uc52c \uc801\uc740 \uc591\uc758 \ud559\uc2b5 \ub370\uc774\ud130\uc640 \uc791\uc740 \ubaa8\ub378 \ud06c\uae30\ub97c \uc0ac\uc6a9\ud558\uba74\uc11c\ub3c4 \ube44\uc2b7\ud558\uac70\ub098 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub378\uc758 \ub2e4\uc591\ud55c \ub2e4\uc911 \ubaa8\ub2ec \uc774\ud574 \uce21\uba74(Timbre, Tone, Melody, Space, Time, Hall, Intricacy)\uc5d0 \ub300\ud55c \uc810\uc218\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, OpenOmni\uac00 \uc5ec\ub7ec \uce21\uba74\uc5d0\uc11c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}]