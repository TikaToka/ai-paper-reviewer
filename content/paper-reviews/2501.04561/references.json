{"references": [{"fullname_first_author": "Junyi Ao", "paper_title": "SpeechT5: Unified-modal encoder-decoder pre-training for spoken language processing", "publication_date": "2021-10-01", "reason": "This paper is foundational for speech-to-text generation, a core component of the OpenOmni system."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2024-01-01", "reason": "This work is highly influential in image-text alignment, a key element of the OpenOmni architecture."}, {"fullname_first_author": "Run Luo", "paper_title": "Mmevol: Empowering multimodal large language models with evol-instruct", "publication_date": "2024-09-01", "reason": "This paper details a crucial dataset used in training the OpenOmni model, significantly impacting its performance."}, {"fullname_first_author": "Chaoyou Fu", "paper_title": "VITA: Towards open-source interactive omni-modal LLM", "publication_date": "2024-08-01", "reason": "This is a state-of-the-art open-source model, used as a benchmark comparison, indicating the significance of OpenOmni's achievements."}, {"fullname_first_author": "Kai Chen", "paper_title": "Emova: Empowering language models to see, hear and speak with vivid emotions", "publication_date": "2024-09-01", "reason": "This paper addresses the critical aspect of emotional speech generation which is a key feature of OpenOmni."}]}