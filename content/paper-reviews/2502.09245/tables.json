[{"content": "| Model | ARC-E | ARC-C | Winogrande | COPA | MultiRC | RTE | HellaSwag | PIQA | Avg |\n|---|---|---|---|---|---|---|---|---|---| \n| LLaMA | 69.5 | 38.7 | 55.2 | 75.0 | 42.8 | 54.5 | 53.1 | 72.5 | 57.7 |\n| HC | 70.1 | 38.4 | 53.0 | 77.0 | 42.9 | 51.6 | 54.4 | 73.5 | 57.6 |\n| LIMe Dynamic | 72.7 | 39.5 | 53.1 | 79.0 | 43.0 | 52.4 | 54.4 | 72.9 | 58.4 |\n| LIMe Static | 71.1 | 39.3 | 56.2 | 75.0 | 43.1 | 55.2 | 53.9 | 72.2 | 58.3 |", "caption": "Table 1: LM Evaluation Harness benchmarks (accuracies in %) on 1.2B models with num-fewshots 1. The rightmost column shows average accuracy across the tasks. Proposed methods outperform both LLaMA and HyperConnections (Zhu et\u00a0al., 2024) baselines. See Section 5.1 for more details.", "description": "\ud45c 1\uc740 1.2B \ub9e4\uac1c\ubcc0\uc218 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec num-fewshots\ub97c 1\ub85c \uc124\uc815\ud558\uace0 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\ub9c1 \ud3c9\uac00 \ubca4\uce58\ub9c8\ud06c\ub97c \uc218\ud589\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud3c9\uac00 \ubca4\uce58\ub9c8\ud06c\ub294 ARC-E, ARC-C, Winogrande, COPA, MultiRC, RTE, HellaSwag, PIQA \ub4f1 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \uc774\ud574 \uacfc\uc81c\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.  \uac01 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4(\uc815\ud655\uc728)\ub294 \ubc31\ubd84\uc728(%)\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \ub9c8\uc9c0\ub9c9 \uc5f4\uc740 \ubaa8\ub4e0 \uacfc\uc81c\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc81c\uc548\ub41c LIMe \uae30\ubc95(LIMe Static \ubc0f LIMe Dynamic)\uc740 \uae30\uc874\uc758 LLaMA \ubc0f HyperConnections \ubaa8\ub378\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4.  \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ubcf8 \ub17c\ubb38 5.1\uc808\uc744 \ucc38\uc870\ud558\uc2ed\uc2dc\uc624.", "section": "5.1 Language Modeling"}, {"content": "| L | R | H | Semantic circuit description | Tokens examples |\n|---|---|---|---|---|\n| 4 | 0 | 23 | Primarily partial word segments that illustrate English morphological composition. | lex, ache, isters, ique, ley, elling, ets, ry. |\n| 9 | 1 | 3 | A range of English suffixes or near-suffix fragments that highlight morphological building blocks and transformations. | ist, ised, ishing, osed, ized, ense, istic, ish, ened, inch. |\n| 8 | 0 | 10 | Primarily affixes and stems that indicate morphological processes in English. | izing, ically, ified, ission, ational, ist, ering. |\n| 15 | 1 | 23 | A collection of intensifiers, qualifiers, and comparative modifiers that adjust tone and degree in writing. | very, various, respective, relatively, highly, latter, largely, particularly. |\n| 10 | 1 | 18 | Primarily subordinating conjunctions and discourse markers for conditions or reasons, illustrating causation, contingency, and contrast. | Because, If, Although, While, There, According, Unlike, However, It, Even. |", "caption": "Table 2: Table with examples of tokens where semantic circuits activate in Dynamic LIMe. L \u2013 layer which makes the query, R \u2013 level of queried representation, H \u2013 head number. This result indicates that the model learns depthwise circuits to bypass information without change to a further layer. See Section 5.2 for more details.", "description": "\uc774 \ud45c\ub294 Dynamic LIMe \ubaa8\ub378\uc5d0\uc11c \uc758\ubbf8\uc801 \ud68c\ub85c\uac00 \ud65c\uc131\ud654\ub418\ub294 \ud1a0\ud070\uc758 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ud589\uc740 \ud2b9\uc815 \ud1a0\ud070(\ub2e8\uc5b4 \ub610\ub294 \uc5b4\uc808)\uacfc, \uadf8 \ud1a0\ud070\uc758 \uc758\ubbf8\uc801 \ud68c\ub85c\uac00 \ud65c\uc131\ud654\ub418\ub294 \ub808\uc774\uc5b4(L), \ucc38\uc870\ub418\ub294 \ud45c\ud604\uc758 \ub808\ubca8(R), \ud5e4\ub4dc \ubc88\ud638(H)\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \uacb0\uacfc\ub294 \ubaa8\ub378\uc774 \ub354 \uae4a\uc740 \ub808\uc774\uc5b4\ub85c \uc815\ubcf4\ub97c \ubcc0\uacbd\ud558\uc9c0 \uc54a\uace0 \uac74\ub108\ub6f8 \uc218 \uc788\ub294 \uc2ec\uce35 \ud68c\ub85c\ub97c \ud559\uc2b5\ud55c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 5.2\uc808\uc744 \ucc38\uc870\ud558\uc2ed\uc2dc\uc624.", "section": "5.2 Analysing Learned Routings in LIMe"}, {"content": "| Hyperparameter | Value |\n|---|---| \n| Optimizer | AdamW |\n| Learning Rate | 0.001 |\n| LIMe Router Learning Rate | 0.01 |\n| Weight Decay | 0.1 |\n| \u03b2\u2081 | 0.9 |\n| \u03b2\u2082 | 0.95 |\n| \u03f5 | 1e-8 |\n| Scheduler | cosine |\n| Warmup Steps | 200 |\n| Min LR | 1e-6 |\n| Mixed Precision | bf16 |\n| Gradient Clipping | 1.0 |\n| Sequence Length | 2048 |\n| Batch Size | 1024 |\n| Training Steps | 20000 |", "caption": "Table 3: Key training hyperparameters used in all experiments.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 \ubaa8\ub4e0 \uc2e4\ud5d8\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc8fc\uc694 \ud6c8\ub828 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998, \ud559\uc2b5\ub960, \uac00\uc911\uce58 \uac10\uc1e0, \ubca0\ud0c01, \ubca0\ud0c02, \uc5d0\ud504\uc2e4\ub860, \uc2a4\ucf00\uc904\ub7ec, \uc6dc\uc5c5 \ub2e8\uacc4, \ucd5c\uc18c \ud559\uc2b5\ub960, \ud63c\ud569 \uc815\ubc00\ub3c4, \uae30\uc6b8\uae30 \ud074\ub9ac\ud551, \uc2dc\ud000\uc2a4 \uae38\uc774, \ubc30\uce58 \ud06c\uae30, \ud6c8\ub828 \ub2e8\uacc4 \ub4f1\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac12\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub294 \ubaa8\ub378 \ud559\uc2b5 \uacfc\uc815\uc5d0 \uc911\uc694\ud55c \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc694\uc18c\uc774\uba70, \uc774 \ud45c\ub294 \uc2e4\ud5d8\uc758 \uc7ac\ud604\uc131\uacfc \uacb0\uacfc \ud574\uc11d\uc5d0 \ud544\uc218\uc801\uc778 \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "5.1 \uc5b8\uc5b4 \ubaa8\ub378\ub9c1"}, {"content": "| Parameter | Value |\n|---|---| \n| Vocab Size | 50,257 |\n| Hidden Size | 2048 |\n| Intermediate Size | 8192 |\n| Number of Hidden Layers | 16 |\n| Number of Attention Heads | 32 |\n| Tie Word Embeddings | True |", "caption": "Table 4: Model architecture for all variants (LLaMa, Hyper Connections, and LIMe) at the 1.2B scale.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c \uc138 \uac00\uc9c0 \ubaa8\ub378 \ubcc0\ud615(LLaMa, Hyper Connections, LIMe)\uc5d0 \ub300\ud55c \uc544\ud0a4\ud14d\ucc98\ub97c 12\uc5b5 \ub9e4\uac1c\ubcc0\uc218 \uaddc\ubaa8\uc5d0\uc11c \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc5b4\ud718 \ud06c\uae30, \uc740\ub2c9\uce35 \ud06c\uae30, \uc911\uac04\uce35 \ud06c\uae30, \uc740\ub2c9\uce35 \uc218, \uc5b4\ud150\uc158 \ud5e4\ub4dc \uc218, \uac00\uc911\uce58 \uacf5\uc720 \uc5ec\ubd80 \ub4f1\uc758 \uc8fc\uc694 \uc544\ud0a4\ud14d\ucc98 \ub9e4\uac1c\ubcc0\uc218 \uac12\uc774 \ud45c\uc5d0 \uc790\uc138\ud788 \ub098\uc5f4\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uc138 \uac00\uc9c0 \ubaa8\ub378\uc758 \uc544\ud0a4\ud14d\ucc98 \ucc28\uc774\ub97c \uba85\ud655\ud558\uac8c \uc774\ud574\ud558\uace0, \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5 \ube44\uad50 \ubd84\uc11d\uc5d0 \uc720\uc6a9\ud55c \uae30\uc900\uc810\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "5.1 Language Modeling"}, {"content": "| Model Depth | Total Router Weights | Pruned Weights (%) |\n|---|---|---|\n| 32-layer | 7,936 | 1,845 (23%) |\n| 64-layer | 32,256 | 6,795 (21%) |\n| 128-layer | 130,048 | 24,632 (19%) |", "caption": "Table 5: Number of pruned LIMe Router\u2019s weights at top-p=0.9\ud835\udc5d0.9p=0.9italic_p = 0.9 for various model depths. As we can see in deep models\u2019 training results (5.4), retrieval paths pruning does not stop LIMe from being superior compared to LLaMA.", "description": "\ud45c 5\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378 \uae4a\uc774\uc5d0 \ub300\ud574 top-p = 0.9\ub85c LIMe \ub77c\uc6b0\ud130 \uac00\uc911\uce58\ub97c \uac00\uc9c0\uce58\uae30\ud55c \uac1c\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  5.4\uc808\uc758 \uc2ec\uce35 \ubaa8\ub378 \ud559\uc2b5 \uacb0\uacfc\uc5d0\uc11c \uc54c \uc218 \uc788\ub4ef\uc774, \uac80\uc0c9 \uacbd\ub85c \uac00\uc9c0\uce58\uae30\ub294 LIMe\uac00 LLaMA\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ub0b4\ub294 \uac83\uc744 \ub9c9\uc9c0 \ubabb\ud588\uc2b5\ub2c8\ub2e4.  \uc989, LIMe\ub294 \ubaa8\ub378\uc758 \uae4a\uc774\uac00 \uae4a\uc5b4\uc9d0\uc5d0 \ub530\ub77c \uc0c1\ub300\uc801\uc73c\ub85c \ub354 \uc801\uc740 \uc218\uc758 \ub77c\uc6b0\ud130 \uac00\uc911\uce58\ub97c \uac00\uc9c0\uce58\uae30\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \uc720\uc9c0\ud558\uba74\uc11c\ub3c4 \uc131\ub2a5 \uc800\ud558 \uc5c6\uc774 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \uc720\uc9c0\ud588\uc2b5\ub2c8\ub2e4.", "section": "5.1 Language Modeling"}, {"content": "| Model | # Parameters (B) | FLOPs (T) | Peak Memory Overhead over LLaMA excluding parameters | Peak Memory Overhead over LLaMA excluding parameters |\n|---|---|---|---|---|\n|  |  |  | Train | Inference |\n| **LLaMA** | 1.1767 | 2.97 | 0 | 0 |\n| **LIMe Static** | 1.1768 (+0.008%) | 2.98 (+0.3%) | (L-1)BTHD | BTHD<sup>(*)</sup> |\n| **LIMe Dynamic** | 1.1856 (+0.075%) | 3.01 (+1.3%) | BTH(L(L+1)/2-1)+(L-1)BTHD | LBTH+BTHD<sup>(*)</sup> |\n| **HC Dynamic** | 1.1771 (+0.030%) | 2.98 (+0.3%) | 2LBT[(R-1)D+R(R+2))] | BT[(R-1)D+R(R+2))] |", "caption": "Table 6: Comparing efficiency for all 1.21.21.21.2B models: both Dynamic and Static LIMe enjoy negligible parameter and FLOPs increase, and smaller peak memory than HC during training. When the Key-Value cache is utilized, this memory advantage extends to inference as well (*). H \u2013 number of heads, L \u2014 number of layers, T \u2014 sequence length, D \u2014 hidden dimension, R \u2014 Hyper Connections (Zhu et\u00a0al., 2024) expansion rate.", "description": "\ud45c 6\uc740 1.2B \ud06c\uae30\uc758 \uc5b8\uc5b4 \ubaa8\ub378\ub4e4(LLaMA, LIMe Static, LIMe Dynamic, HC)\uc758 \ud6a8\uc728\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  \ub9e4\uac1c\ubcc0\uc218 \uc218, FLOPs (\ubd80\ub3d9\uc18c\uc218\uc810 \uc5f0\uc0b0 \uc218), \ucd5c\ub300 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9 \ub4f1\uc744 \ube44\uad50\ud558\uc5ec LIMe \ubaa8\ub378\uc774 \uae30\uc874 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \ub9e4\uac1c\ubcc0\uc218 \ubc0f FLOPs \uc99d\uac00\ub294 \ubbf8\ubbf8\ud558\uc9c0\ub9cc \ud6c8\ub828 \uc2dc \ucd5c\ub300 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc740 \ub354 \uc801\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 \ud0a4-\ubc38\ub958 \uce90\uc2dc\ub97c \uc0ac\uc6a9\ud558\uba74 \ucd94\ub860 \uc2dc\uc5d0\ub3c4 \uba54\ubaa8\ub9ac \uc774\uc810\uc774 \ud655\uc7a5\ub429\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub378\uc758 \ud5e4\ub4dc \uc218(H), \ub808\uc774\uc5b4 \uc218(L), \uc2dc\ud000\uc2a4 \uae38\uc774(T), \uc740\ub2c9 \ucc28\uc6d0(D), \ud558\uc774\ud37c \ucee4\ub125\uc158\uc758 \ud655\uc7a5 \ube44\uc728(R) \ub4f1\uc758 \uc815\ubcf4\ub3c4 \ud568\uaed8 \uc81c\uacf5\ub429\ub2c8\ub2e4.", "section": "5.1 Language Modeling"}]