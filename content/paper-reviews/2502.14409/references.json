{"references": [{"fullname_first_author": "Akari Asai", "paper_title": "OpenScholar: Synthesizing Scientific Literature with Retrieval-Augmented Language Models", "publication_date": "2024-11-14", "reason": "This paper introduces OpenScholar, a benchmark for long-context summarization with evidence citation, directly relevant to the current work's focus on unstructured evidence citation in query-focused summarization."}, {"fullname_first_author": "Philippe Laban", "paper_title": "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems", "publication_date": "2024-00-00", "reason": "This paper introduces the SummHay dataset, a key resource in the field of long-context summarization, directly compared against in the current work for measuring the capabilities of different models."}, {"fullname_first_author": "Nelson F. Liu", "paper_title": "Lost in the Middle: How Language Models Use Long Contexts", "publication_date": "2024-00-00", "reason": "This highly influential paper highlights positional bias in LLMs, a key problem addressed by the current work, which uses a novel dataset to mitigate the issue."}, {"fullname_first_author": "Tianyi Zhang", "paper_title": "Benchmarking Large Language Models for News Summarization", "publication_date": "2024-00-00", "reason": "This paper offers a comprehensive benchmark for evaluating LLMs on summarization tasks, providing valuable context for evaluating the performance of models in the current study."}, {"fullname_first_author": "Mathieu Ravaut", "paper_title": "On Context Utilization in Summarization with Large Language Models", "publication_date": "2024-00-00", "reason": "This paper directly addresses the problem of positional bias in long-context summarization, a key focus of the current work, and offers valuable insights into improving performance."}]}