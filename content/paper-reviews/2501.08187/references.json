{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-01", "reason": "This paper introduces a foundational large language model architecture that serves as a basis for the multi-modal approach in INSTRUCTCELL."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-01-01", "reason": "This paper introduces the T5 model, which is a key component used in the INSTRUCTCELL architecture, showcasing its ability to handle diverse NLP tasks."}, {"fullname_first_author": "Fan Yang", "paper_title": "scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data", "publication_date": "2022-01-01", "reason": "This is a highly relevant single-cell foundation model that demonstrates the effectiveness of leveraging pre-trained language models for single-cell analysis tasks."}, {"fullname_first_author": "Haotian Cui", "paper_title": "scgpt: toward building a foundation model for single-cell multi-omics using generative ai", "publication_date": "2024-01-01", "reason": "This paper explores the use of generative AI models, like scGPT, which directly influenced INSTRUCTCELL\u2019s design for single-cell data generation."}, {"fullname_first_author": "Daniel LeVine", "paper_title": "Cell2sentence: Teaching large language models the language of biology", "publication_date": "2024-01-01", "reason": "This paper is highly relevant to INSTRUCTCELL as it explores using LLMs to process single-cell data, directly addressing challenges that INSTRUCTCELL aims to overcome."}]}