[{"content": "| Model | Param | Tokens | ARC-E | ARC-C | HellaSwag | MMLU | OBQA | PiQA | SciQ | WinoGrande |\n|---|---|---|---|---|---|---|---|---|---|---|\n| random |  |  | 25.0 | 25.0 | 25.0 | 25.0 | 25.0 | 50.0 | 25.0 | 50.0 |\n| Amber | 7B | 1.2T | 65.70 | 37.20 | 72.54 | 26.77 | 41.00 | 78.73 | 88.50 | 63.22 |\n| Pythia-2.8b | 2.8B | 0.3T | 58.00 | 32.51 | 59.17 | 25.05 | 35.40 | 73.29 | 83.60 | 57.85 |\n| Pythia-6.9b | 6.9B | 0.3T | 60.48 | 34.64 | 63.32 | 25.74 | 37.20 | 75.79 | 82.90 | 61.40 |\n| Pythia-12b | 12B | 0.3T | 63.22 | 34.64 | 66.72 | 24.01 | 35.40 | 75.84 | 84.40 | 63.06 |\n| OLMo-1B | 1B | 3T | 57.28 | 30.72 | 63.00 | 24.33 | 36.40 | 75.24 | 78.70 | 59.19 |\n| OLMo-7B | 7B | 2.5T | 68.81 | 40.27 | 75.52 | 28.39 | 42.20 | 80.03 | 88.50 | 67.09 |\n| OLMo-7B-0424 | 7B | 2.05T | 75.13 | 45.05 | 77.24 | 47.46 | 41.60 | 80.09 | 96.00 | 68.19 |\n| OLMo-7B-0724 | 7B | 2.75T | 74.28 | 43.43 | 77.76 | 50.18 | 41.60 | 80.69 | 95.70 | 67.17 |\n| OLMo-2-1124 | 7B | 4T | 82.79 | 57.42 | 80.50 | 60.56 | 46.20 | 81.18 | 96.40 | 74.74 |\n| Ours, (r=4) | 3.5B | 0.8T | 57.19 | 22.95 | 36.07 | 23.32 | 18.60 | 65.12 | 84.80 | 55.24 |\n| Ours, (r=8) | 3.5B | 0.8T | 66.07 | 32.50 | 45.08 | 24.88 | 22.00 | 70.72 | 91.5 | 55.64 |\n| Ours, (r=16) | 3.5B | 0.8T | 68.43 | 34.38 | 48.65 | 29.21 | 24.00 | 73.99 | 93.60 | 57.77 |\n| Ours, (r=32) | 3.5B | 0.8T | 69.91 | 38.23 | 65.21 | 31.38 | 38.80 | 76.22 | 93.50 | 59.43 |", "caption": "Table 1: Results on lm-eval-harness tasks zero-shot without chat template across various open-source models. We show ARC (Clark et\u00a0al., 2018), HellaSwag (Zellers et\u00a0al., 2019), MMLU (Hendrycks et\u00a0al., 2021b), OpenBookQA (Mihaylov et\u00a0al., 2018), PiQA (Bisk et\u00a0al., 2020), SciQ (Johannes\u00a0Welbl, 2017), and WinoGrande (Sakaguchi et\u00a0al., 2021). We report normalized accuracy when provided.", "description": "\ubcf8 \ud45c\ub294 \ub2e4\uc591\ud55c \uc624\ud508\uc18c\uc2a4 \uc5b8\uc5b4 \ubaa8\ub378\uc5d0 \ub300\ud55c lm-eval-harness \uc791\uc5c5\uc758 \uc81c\ub85c\uc0f7 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ucc44\ud305 \ud15c\ud50c\ub9bf \uc5c6\uc774 \ud3c9\uac00\ub418\uc5c8\uc73c\uba70, ARC, HellaSwag, MMLU, OpenBookQA, PiQA, SciQ, WinoGrande \ub4f1 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\uaddc\ud654\ub41c \uc815\ud655\ub3c4 \uc810\uc218\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218\uc640 \ud1a0\ud070 \uc218\ub3c4 \ud568\uaed8 \ud45c\uc2dc\ub418\uc5b4 \ubaa8\ub378 \ud06c\uae30\uc640 \uc131\ub2a5 \uac04\uc758 \uad00\uacc4\ub97c \ud30c\uc545\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "5. \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc"}, {"content": "| Model | GSM8K | GSM8k CoT | Minerva MATH | MathQA |\n|---|---|---|---|---|\n| Random | 0.00 | 0.00 | 0.00 | 20.00 |\n| Amber | 3.94/4.32 | 3.34/5.16 | 1.94 | 25.26 |\n| Pythia-2.8b | 1.59/2.12 | 1.90/2.81 | 1.96 | 24.52 |\n| Pythia-6.9b | 2.05/2.43 | 2.81/2.88 | 1.38 | 25.96 |\n| Pythia-12b | 3.49/4.62 | 3.34/4.62 | 2.56 | 25.80 |\n| OLMo-1B | 1.82/2.27 | 1.59/2.58 | 1.60 | 23.38 |\n| OLMo-7B | 4.02/4.09 | 6.07/7.28 | 2.12 | 25.26 |\n| OLMo-7B-0424 | 27.07/27.29 | 26.23/26.23 | 5.56 | 28.48 |\n| OLMo-7B-0724 | 28.66/28.73 | 28.89/28.89 | 5.62 | 27.84 |\n| OLMo-2-1124-7B | 66.72/66.79 | 61.94/66.19 | 19.08 | 37.59 |\n| Our w/o sys. prompt ($r=32$) | 28.05/28.20 | 32.60/34.57 | 12.58 | 26.60 |\n| Our w/ sys. prompt ($r=32$) | 24.87/38.13 | 34.80/42.08 | 11.24 | 27.97 |", "caption": "Table 4: Baseline comparison, recurrent versus non-recurrent model trained in the same training setup and data. Comparing the recurrent model with its non-recurrent baseline, we see that even at 180B tokens, the recurrent substantially outperforms on harder tasks.", "description": "\ud45c 4\ub294 \ub3d9\uc77c\ud55c \ud559\uc2b5 \uc124\uc815 \ubc0f \ub370\uc774\ud130\ub85c \ud6c8\ub828\ub41c \uc21c\ud658 \ubaa8\ub378\uacfc \ube44\uc21c\ud658 \ubaa8\ub378\uc758 \uae30\uc900 \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc21c\ud658 \ubaa8\ub378\uc740 \uc5b4\ub824\uc6b4 \uc791\uc5c5\uc5d0\uc11c 180B \ud1a0\ud070\uc5d0\uc11c\ub3c4 \ube44\uc21c\ud658 \uae30\uc900 \ubaa8\ub378\ubcf4\ub2e4 \uc0c1\ub2f9\ud788 \uc131\ub2a5\uc774 \ub6f0\uc5b4\ub0a9\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\ub9c1 \ubca4\uce58\ub9c8\ud06c(GSM8K COT, ARC-E, ARC-C, HellaSwag, MMLU, OBQA, PiQA, SciQ, WinoGrande)\uc5d0\uc11c \uc21c\ud658 \ubaa8\ub378\uacfc \ube44\uc21c\ud658 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec, \uc21c\ud658 \ubaa8\ub378\uc774 \ud2b9\ud788 \ucd94\ub860 \ub2a5\ub825\uc774 \ud544\uc694\ud55c \uc5b4\ub824\uc6b4 \uc791\uc5c5\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ube44\uc21c\ud658 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uae30\uc900\uc73c\ub85c \uc21c\ud658 \ubaa8\ub378\uc758 \uc131\ub2a5 \ud5a5\uc0c1 \uc815\ub3c4\ub97c \ud30c\uc545\ud558\uc5ec,  \ubaa8\ub378\uc758 \ucd94\ub860 \ub2a5\ub825 \ud5a5\uc0c1\uc5d0 \uc21c\ud658 \uad6c\uc870\uac00 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "5. \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc"}, {"content": "| Model | Param | Tokens | MBPP | HumanEval |\n|---|---|---|---|---|\n| Random |  |  | 0.00 | 0.00 |\n| starcoder2-3b | 3B | 3.3T | 43.00 | 31.09 |\n| starcoder2-7b | 7B | 3.7T | 43.80 | 31.70 |\n| Amber | 7B | 1.2T | 19.60 | 13.41 |\n| Pythia-2.8b | 2.8B | 0.3T | 6.70 | 7.92 |\n| Pythia-6.9b | 6.9B | 0.3T | 7.92 | 5.60 |\n| Pythia-12b | 12B | 0.3T | 5.60 | 9.14 |\n| OLMo-1B | 1B | 3T | 0.00 | 4.87 |\n| OLMo-7B | 7B | 2.5T | 15.6 | 12.80 |\n| OLMo-7B-0424 | 7B | 2.05T | 21.20 | 16.46 |\n| OLMo-7B-0724 | 7B | 2.75T | 25.60 | 20.12 |\n| OLMo-2-1124-7B | 7B | 4T | 21.80 | 10.36 |\n| Ours ($r=32$) | 3.5B | 0.8T | 24.80 | 23.17 |", "caption": "Table 5: Comparison of Open and Closed QA Performance (%) (Mihaylov et\u00a0al., 2018). In the open exam, a relevant fact is provided before the question is asked. In this setting, our smaller model closes the gap to other open-source models, indicating that the model is capable, but has fewer facts memorized.", "description": "\ud45c 5\ub294 Mihaylov \ub4f1(2018)\uc758 OpenBookQA \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ud3d0\uc1c4\ud615(Closed QA) \ubc0f \uac1c\ubc29\ud615(Open QA) \uc9c8\ubb38\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. \ud3d0\uc1c4\ud615 \uc9c8\ubb38\uc740 \uc9c8\ubb38\ub9cc \uc8fc\uc5b4\uc9c0\uace0, \uac1c\ubc29\ud615 \uc9c8\ubb38\uc740 \uc9c8\ubb38\uc5d0 \uc55e\uc11c \uad00\ub828 \uc815\ubcf4\uac00 \uc81c\uacf5\ub429\ub2c8\ub2e4. \ubcf8 \ub17c\ubb38\uc758 \uc18c\ud615 \ubaa8\ub378\uc740 \uac1c\ubc29\ud615 \uc9c8\ubb38 \uc124\uc815\uc5d0\uc11c \ub2e4\ub978 \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378\ub4e4\uacfc\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uc904\uc600\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc774 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub2f5\ubcc0 \ub2a5\ub825\uc740 \uc788\uc9c0\ub9cc, \uc554\uae30\ub41c \uc0ac\uc2e4\uc758 \uc591\uc740 \uc0c1\ub300\uc801\uc73c\ub85c \uc801\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "5. \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc"}, {"content": "| Model | Tokens | ARC-E | ARC-C | HellaSwag | MMLU | OBQA | PiQA | SciQ | WinoGrande | GSM8K CoT |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Fixed-Depth Baseline | 0.18T | 46.42 | 26.96 | 37.34 | 24.16 | 29.60 | 64.47 | 73.20 | 51.78 | 1.82/2.20 |\n| Ours, early ckpt, (r=32) | 0.18T | 53.62 | 29.18 | 48.80 | 25.59 | 31.40 | 68.88 | 80.60 | 52.88 | 9.02/10.24 |\n| Ours, early ckpt, (r=1) | 0.18T | 34.01 | 23.72 | 29.19 | 23.47 | 25.60 | 53.26 | 54.10 | 53.75 | 0.00/0.15 |\n| Ours, (r=32) | 0.8T | 69.91 | 38.23 | 65.21 | 31.38 | 38.80 | 76.22 | 93.50 | 59.43 | 34.80/42.08 |\n| Ours, (r=1) | 0.8T | 34.89 | 24.06 | 29.34 | 23.60 | 26.80 | 55.33 | 47.10 | 49.41 | 0.00/0.00 |", "caption": "Table 6: First turn scores and standard errors on 1-turn MT-Bench for various inference time schemes that are native to the recurrent-depth model. Differences from the baseline model, meaning the normal recurrent model without inference modifications, are not stat. significant.", "description": "\ubcf8 \ud45c\ub294 \uc21c\ud658 \uc2ec\uce35 \ubaa8\ub378\uc758 \uace0\uc720\ud55c \ucd94\ub860 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud55c 1\ud68c\uc804 MT-Bench \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uae30\ubcf8 \ubaa8\ub378(\ucd94\ub860 \uc218\uc815 \uc5c6\uc774 \uc77c\ubc18\uc801\uc778 \uc21c\ud658 \ubaa8\ub378)\uacfc \ube44\uad50\ud558\uc5ec \uc5ec\ub7ec \uac00\uc9c0 \ucd94\ub860 \uc2dc\uac04 \uacc4\ud68d\uc5d0 \ub300\ud55c 1\ud68c\uc804 \uc810\uc218\uc640 \ud45c\uc900 \uc624\ucc28\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ud45c\uc900 \uc624\ucc28\ub294 \uae30\ubcf8 \ubaa8\ub378\uacfc\uc758 \ucc28\uc774\uac00 \ud1b5\uacc4\uc801\uc73c\ub85c \uc720\uc758\ubbf8\ud558\uc9c0 \uc54a\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc2dc \ub9d0\ud574, \uc81c\uc2dc\ub41c \ub2e4\uc591\ud55c \ucd94\ub860 \ubc29\uc2dd\uc740 \uae30\ubcf8 \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \ud1b5\uacc4\uc801\uc73c\ub85c \uc720\uc758\ubbf8\ud55c \uc601\ud5a5\uc744 \ubbf8\uce58\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.", "section": "5. \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc"}, {"content": "| Model | Closed | Open | \u0394 |\n|---|---|---|---| \n| Amber | 41.0 | 46.0 | +5.0 |\n| Pythia-2.8b | 35.4 | 44.8 | +9.4 |\n| Pythia-6.9b | 37.2 | 44.2 | +7.0 |\n| Pythia-12b | 35.4 | 48.0 | +12.6 |\n| OLMo-1B | 36.4 | 43.6 | +7.2 |\n| OLMo-7B | 42.2 | 49.8 | +7.6 |\n| OLMo-7B-0424 | 41.6 | 50.6 | +9.0 |\n| OLMo-7B-0724 | 41.6 | 53.2 | +11.6 |\n| OLMo-2-1124 | 46.2 | 53.4 | +7.2 |\n| Ours (r=32) | 38.2 | 49.2 | +11.0 |", "caption": "Table 7: Datasets used for model pre-training (Part 1: Standard sources)", "description": "\ubcf8 \ub17c\ubb38\uc758 \ud45c 7\uc740 \ubaa8\ub378 \uc0ac\uc804 \ud6c8\ub828\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  (1\ubd80: \ud45c\uc900 \ub370\uc774\ud130 \uc18c\uc2a4) \ud45c\uc5d0\ub294 \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc774\ub984, \uc8fc\uc18c, \ub77c\uc774\uc120\uc2a4, \ubc94\uc8fc, \uac00\uc911\uce58(W), \uba40\ud2f0\uadf8\ub798\ud504(MG) \uc5ec\ubd80, \uadf8\ub9ac\uace0 \uc778\uc6a9 \ucd9c\ucc98\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac00\uc911\uce58\ub294 \ub370\uc774\ud130\uc14b\uc758 \uc0c1\ub300\uc801 \uc911\uc694\ub3c4\ub97c \ub098\ud0c0\ub0b4\uba70, \uba40\ud2f0\uadf8\ub798\ud504\ub294 \uc5ec\ub7ec \uac1c\uc758 \ub370\uc774\ud130 \uc18c\uc2a4\uac00 \uacb0\ud569\ub418\uc5c8\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ubaa8\ub378\uc758 \uc0ac\uc804 \ud6c8\ub828\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc758 \uc885\ub958\uc640 \uc591\uc5d0 \ub300\ud55c \uc0c1\uc138\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\uc5ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ub192\uc774\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "4. \ub300\uaddc\ubaa8 \ud655\ub960\uc801 \uc2ec\uce35 \uc5b8\uc5b4 \ubaa8\ub378 \ud559\uc2b5"}, {"content": "| Model | MT-Bench | Std. Error |\n|---|---|---|\n| cache compression, s=4 | 5.856 | 0.395 |\n| baseline, 64 iterations | 5.693 | 0.386 |\n| cache compression, s=16 | 5.687 | 0.402 |\n| baseline, 32 iterations | 5.662 | 0.388 |\n| cache compression, s=8 | 5.631 | 0.384 |\n| KL exit, t=5\u00d710\u207b\u2074 | 5.562 | 0.389 |", "caption": "Table 8: Datasets used for model pre-training (Part 2: Instruction Data)", "description": "\ubcf8 \ub17c\ubb38\uc758 \ud45c 8\uc740 \ubaa8\ub378 \uc0ac\uc804 \ud6c8\ub828\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b \uc911 \uc9c0\uc2dc \ub370\uc774\ud130(Instruction Data) \ubd80\ubd84\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc774\ub984, \uc8fc\uc18c, \ub77c\uc774\uc120\uc2a4, \ubc94\uc8fc, \uac00\uc911\uce58(W), \ub2e4\uc911 \uc5b8\uc5b4 \uc9c0\uc6d0 \uc5ec\ubd80(MG), \uadf8\ub9ac\uace0 \uc778\uc6a9 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ubcf8 \ud45c\ub294 \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \uc885\ub958\uc758 \uc9c0\uc2dc \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub418\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc0c1\uc138\ud55c \ubaa9\ub85d\uc785\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc740 \ud2b9\uc815 \uc720\ud615\uc758 \uc9c0\uc2dc \ub610\ub294 \uc9c8\ubb38\uc5d0 \uc9d1\uc911\ub418\uc5b4 \uc788\uc73c\uba70, \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378\uc758 \ub2e4\uc591\ud55c \uc791\uc5c5\uc5d0 \ub300\ud55c \uc801\uc751\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "4. \ud6c8\ub828 \uc124\uc815 (Training Setup)"}]