<table id='2' style='font-size:14px'><tr><td>Llama 3 report</td><td>Read the following paper about Llama 3. Answer the question based on the paper. \n\n The Llama 3 Herd of Models\n Llama Team, AI @ Meta 1\n\n 1A detailed contributor list can be found in the appendix of this paper.Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers...</td></tr><tr><td>Question</td><td>At which stages does the llama3 team average the models (The answer is in Section 4.1.5 of Llama Team (2024) )</td></tr><tr><td>Llama 3 RoPE</td><td>obtained from experiments using various versions of data or hyperparameters at each RM, SFT, or DPO stage? The answer is 4.1.6, Iterative Rounds, and 4.1.6, Iterative RoundsX(not fluent)</td></tr><tr><td>Llama 3 STRING</td><td>The Llama 3 team averages models obtained from experiments using various versions of data or hyperparameters at each RM, SFT, or DPO stage.V</td></tr><tr><td>Question</td><td>Describe the sources of Llama3's SFT data (The answer is in Section 4.2.2 of Llama Team (2024) )</td></tr><tr><td>Llama 3 RoPE</td><td>Llama3's SFT data is sourced from various places, including human annotations, other NLP tasks, and translated data. Human annotations are collected from linguists and native speakers, and are used to create open-ended prompts that represent real-world use cases. X(hallucination! it describes the multilingual SFT data)</td></tr><tr><td>Llama 3 STRING</td><td>In the paper, it is stated that the SFT data is comprised of the following sources: "prompts from our human annotation collection with rejection-sampled responses" and "synthetic data targeting specific capabilities" Â· Additionally, it is mentioned that the SFT data is also comprised of "human-curated data" and "small amounts of human-curated data" for certain capabilities. The Llama3 team uses a combination of model-based and rule- based techniques to filter out low-quality training samples and improve overall model performance.V (More accurate)</td></tr><tr><td>Question</td><td>From what is Llama-3's multilingual SFT data derived (The answer is in Section 4.3.2 ) of Llama Team (2024))</td></tr><tr><td>Llama 3 RoPE</td><td>Human annotations, data from other NLP tasks, rejection sampled data, and translated reasoning data. V</td></tr><tr><td>Llama 3 STRING</td><td>The multilingual SFT data is primarily derived from sources such as human annotations, data from other NLP tasks, rejection sampled data, and translated reasoning data. V</td></tr><tr><td>Question</td><td>How do the authors verify the long context code reasoning of Llama-3 (The answer is in Section 4.3.4 of Llama Team (2024))</td></tr><tr><td>Llama 3 RoPE</td><td>They select the most commonly depended-upon files, specifically those referenced by at least five other files, and remove one of these key files from a repository.X(not addressing the question)</td></tr><tr><td>Llama 3 STRING</td><td>The authors verify the long context code reasoning of Llama-3 by parsing Python files to identify import statements and determine their dependencies. From here, they select the most commonly depended-upon files, specifically those referenced by at least five other files. They remove one of these key files from a repository and prompt the model to identify which files depended on the missing file and to generate the necessary missing code. V</td></tr><tr><td>Question</td><td>How does the llama3 team solve the Hallucinations issues (The answer is in Section 4.3.6 of Llama Team (2024))</td></tr><tr><td>Llama 3 RoPE</td><td>They follow the principle that post-training should align the model to "know what it knows" rather than add knowledge, and generate data that aligns model generations with subsets of factual data present in the pre-training data.X(in complete)</td></tr><tr><td>Llama 3 STRING</td><td>The authors use a knowledge probing technique to align the model generations with subsets of factual data present in the pre-training data. This data generation process involves extracting a data snippet from the pre-training data, generating a factual question about these snippets, sampling responses from the model, scoring the correctness of the generations, scoring the informativeness of the generations, and generating a refusal for responses that are consistently informative and incorrect across the generations. V</td></tr></table>