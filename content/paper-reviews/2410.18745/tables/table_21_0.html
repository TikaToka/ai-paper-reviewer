<table id='2' style='font-size:14px'><tr><td>Llama 3 report</td><td>Read the following paper about Llama 3. Answer the question based on the paper. \n\n The Llama 3 Herd of Models\n Llama Team, AI @ Metal\n\n 1A detailed contributor list can be found in the appendix of this paper.Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality...</td></tr><tr><td>Question Llama 3 RoPE 3 STRING</td><td>How many long-context training stages does Llama3 have? Please describe them (The answer is in Section 3.4.2 of Llama Team (2024) ) Llama3 is a large language model developed by Meta AI, and it has several training stages to achieve its impressive performance. The training stages are as follows: 1 Initial Pre-Training: In this stage, Llama3 is pre-trained using AdamW with a peak learning rate of 8x10-5, a linear warm-up of 8,000 steps, and a cosine learning rate schedule decaying to 8x10-7 over 1,200,000 steps. 2. Long Context Pre-Training: In this stage, Llama3 is trained on long sequences to support context windows of up to 128K tokens. The model is gradually adapted to the increased context length in increments, starting from the original 8K context window and ending in the final 128K X(cannot follow instruction) Llama3 has six stages of long-context training, starting from an 8K context window and ending in a 128K context window. This long-context pre-training stage was performed using approximately 800B training tokens. V totally the same as decribed in Section 3.4.2 Answer explanation: The question requires the reader to identify the number of long- context training stages in Llama3 and describe them</td></tr><tr><td>Llama</td><td>Describe the differences in model architecture between Llama3 and Llama2 (The Llama Team (2024) ) The paper describes Llama3 as a dense Transformer model with 405B parameters, while Llama2 is not specified to have a specific model architecture. However, based on the context, it can be inferred that Llama2 is likely a dense Transformer model as well, but with fewer parameters than Llama3. The paper highlights that Llama3 makes minimal architectural modifications to Llama2, suggesting that the main differences between the two models lie in the scale and hyperparameters rather than the architecture itself. X(not the</td></tr><tr><td>Question Llama 3 RoPE Llama 3 STRING</td><td>answer is in Section 3.2 of addressing question) The main differences between Llama-3 and Llama-2 are: 1. Llama-3 uses grouped query attention (GQA) with 8 key-value heads to improve inference speed and reduce the size of key-value caches during decoding. 2. Llama-3 uses an attention mask that prevents self- attention between different documents within the same sequence. 3. Llama-3 has a larger vocabulary with 128K tokens, which includes 100K tokens from the tiktoken tokenizer and 28K additional tokens to better support non-English languages. 4. Llama-3 increases the RoPE base frequency hyperparameter to 500,000, which enables better support for longer contexts.âŒ€</td></tr><tr><td>Question Llama 3 RoPE</td><td>How does Llama3 perform context parallelism in training (The answer is in Section 3.3.2 of Llama Team (2024)) A. By using a combination of tensor parallelism, pipeline parallelism, and data parallelism. B. By using a combination of pipeline parallelism and data parallelism. C. By using a combination of tensor parallelism and data parallelism. D. By using a combination of context parallelism, pipeline parallelism, and data parallelism.E. By using only data parallelism. Answer: DX(deviated from the question) in training is implemented the communication and computation in the critical path. V</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Llama 3 STRING</td><td>The context parallelism Llama-3 using an all-gather based method, where the key (K) and value (V) tensors are first all-gathered and then the attention output is computed for the local query (Q tensor chunk. This approach is used to support different types of attention masks, such as the document mask, and to balance</td></tr><tr><td>Question Llama 3 RoPE</td><td>What are the two criteria used to assess successful adaptation in long-context training (The answer is in Section 3.4.2 of Llama Team (2024)) Model performance on short-context evaluations and needle-in-a-haystack tasks. V correct but incomplete</td></tr><tr><td>Llama 3 STRING</td><td>The two criteria used to assess successful adaptation in long-context training are: 1. Model performance on short-context evaluations has recovered completely. 2. The model perfectly solves "needle in a haystack" tasks up to that length. These criteria are mentioned in Section 3.4.2 of the paper. V</td></tr></table>