[{"figure_path": "https://arxiv.org/html/2501.12206/extracted/6145987/llava.png", "caption": "Figure 1: Architecture of LLaVA-1.5", "description": "LLaVA-1.5 \ubaa8\ub378\uc758 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4.  \uadf8\ub9bc\uc5d0\ub294 \uc774\ubbf8\uc9c0 \uc778\ucf54\ub354, \uc5b8\uc5b4 \ubaa8\ub378, \uadf8\ub9ac\uace0 \uc774 \ub458\uc744 \uc5f0\uacb0\ud558\ub294 \ud22c\uc601(Projection) \ub808\uc774\uc5b4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ubbf8\uc9c0 \uc778\ucf54\ub354\ub294 \uc774\ubbf8\uc9c0\ub97c \uc785\ub825\ubc1b\uc544 \ud2b9\uc9d5 \ubca1\ud130\ub97c \ucd94\ucd9c\ud558\uace0, \uc5b8\uc5b4 \ubaa8\ub378\uc740 \uc774 \ubca1\ud130\uc640 \ud568\uaed8 \uc5b8\uc5b4 \uba85\ub839\uc5b4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud14d\uc2a4\ud2b8 \uc751\ub2f5\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.  \ud22c\uc601 \ub808\uc774\uc5b4\ub294 \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8 \uc815\ubcf4\ub97c \ud1b5\ud569\ud558\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 LLaVA-1.5 \ubaa8\ub378\uc774 \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \uacb0\ud569\ud558\uc5ec \ube44\uc804-\uc5b8\uc5b4 \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub294 \ubc29\uc2dd\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.12206/x1.png", "caption": "Figure 2: An example of in-context hallucination in LLaVA-1.5. The responses that are not grounded in the image are highlighted in red.", "description": "\uadf8\ub9bc 2\ub294 LLaVA-1.5 \ubaa8\ub378\uc758 \ubb38\ub9e5 \ub0b4 \ud658\uac01 \ud604\uc0c1\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.  \uc0ac\uc6a9\uc790\uc758 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \uc751\ub2f5 \uc911 \uc774\ubbf8\uc9c0\uc5d0 \uadfc\uac70\ud558\uc9c0 \uc54a\uc740 \ubd80\ubd84(\ud658\uac01)\uc740 \ube68\uac04\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uccab \ubc88\uc9f8 \uc9c8\ubb38\uc740 \uc8fc\ubc29\uc5d0 \uc788\ub294 \ubb3c\uac74\ub4e4\uc744 \uc790\uc138\ud788 \uc124\uba85\ud558\ub294 \uac83\uc774\uc5c8\uace0, \ub450 \ubc88\uc9f8\uc640 \uc138 \ubc88\uc9f8 \uc9c8\ubb38\uc740 \ub0c9\uc7a5\uace0\uc758 \uc874\uc7ac \uc5ec\ubd80\uc640 \ubaa8\uc591\uc5d0 \ub300\ud55c \uc9c8\ubb38\uc774\uc5c8\uc2b5\ub2c8\ub2e4.  \ubaa8\ub378\uc740 \uccab \ubc88\uc9f8 \uc9c8\ubb38\uc5d0\ub294 \uc774\ubbf8\uc9c0\ub97c \uc815\ud655\ud558\uac8c \ubb18\uc0ac\ud558\uc9c0\ub9cc, \ub450 \ubc88\uc9f8 \uc9c8\ubb38\uc5d0\uc11c\ub294 \ub0c9\uc7a5\uace0\uac00 \uc5c6\ub2e4\uace0 \uc798\ubabb\ub41c \ub2f5\ubcc0\uc744 \ud558\uace0, \uc138 \ubc88\uc9f8 \uc9c8\ubb38\uc5d0\uc11c\ub294 \ub0c9\uc7a5\uace0\uac00 \uc788\uc9c0\ub9cc \uc2e4\uc81c \uc774\ubbf8\uc9c0\uc640 \ub2e4\ub978 \ubaa8\uc2b5\uc73c\ub85c \ubb18\uc0ac\ud558\ub294 \uc624\ub958\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc774 \uc774\ubbf8\uc9c0 \uc815\ubcf4\ub97c \uc815\ud655\ud558\uac8c \uc774\ud574\ud558\uace0 \ud65c\uc6a9\ud558\uc9c0 \ubabb\ud558\uace0, \ubb38\ub9e5\uc5d0 \ub9de\uc9c0 \uc54a\ub294 \uc815\ubcf4\ub97c \uc0dd\uc131\ud558\ub294 \ud658\uac01 \ud604\uc0c1\uc758 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.12206/extracted/6145987/modality-gap.png", "caption": "Figure 3: Visualization of the embeddings of visual tokens and text tokens in the semantic space, along with the full token vocabulary of Vicuna-7B. The figure clearly shows that the visual tokens, projected by the MLP into the text embedding space, are significantly distant from the text token embeddings, indicating a modality gap.", "description": "\uadf8\ub9bc 3\uc740 Vicuna-7B\uc758 \uc804\uccb4 \ud1a0\ud070 \uc5b4\ud718\uc640 \ud568\uaed8 \uc758\ubbf8 \uacf5\uac04\uc5d0\uc11c \uc2dc\uac01 \ud1a0\ud070\uacfc \ud14d\uc2a4\ud2b8 \ud1a0\ud070\uc758 \uc784\ubca0\ub529\uc744 \uc2dc\uac01\ud654\ud55c \uac83\uc785\ub2c8\ub2e4. MLP\uc5d0 \uc758\ud574 \ud14d\uc2a4\ud2b8 \uc784\ubca0\ub529 \uacf5\uac04\uc73c\ub85c \ud22c\uc601\ub41c \uc2dc\uac01 \ud1a0\ud070\uc774 \ud14d\uc2a4\ud2b8 \ud1a0\ud070 \uc784\ubca0\ub529\uacfc \uc0c1\ub2f9\ud788 \uba40\ub9ac \ub5a8\uc5b4\uc838 \uc788\uc74c\uc744 \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc8fc\uc5b4, \ubaa8\ub2ec\ub9ac\ud2f0 \uac04\uaca9(modality gap)\uc774 \uc788\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc989, \uc2dc\uac01 \uc815\ubcf4\uc640 \uc5b8\uc5b4 \uc815\ubcf4 \uac04\uc758 \ud45c\ud604 \uacf5\uac04 \ucc28\uc774\uac00 \ud06c\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774\ub294 \uc2dc\uac01 \uc5b8\uc5b4 \ubaa8\ub378\uc774 \uc2dc\uac01 \uc815\ubcf4\uc640 \uc5b8\uc5b4 \uc815\ubcf4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ud1b5\ud569\ud558\ub294 \ub370 \uc5b4\ub824\uc6c0\uc744 \uacaa\uc744 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.12206/extracted/6145987/visualize-attn.png", "caption": "Figure 4: Attention maps during the decoding process of a model response for LLaVA-1.5-7B. The visual tokens are highlighted in red. In the shallow layers (e.g., layer 1), attention is relatively evenly distributed across both visual and text tokens. However, in the deeper layers (e.g., layer 16 and 32), attention becomes concentrated on system prompt tokens (text that is prepended before the visual tokens as part of the instruction), prompt tokens, and output tokens, while paying very little attention to the visual tokens.", "description": "\uadf8\ub9bc 4\ub294 LLaVA-1.5-7B \ubaa8\ub378\uc758 \uc751\ub2f5 \uc0dd\uc131 \uacfc\uc815\uc5d0\uc11c\uc758 \uc5b4\ud150\uc158 \ub9f5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube68\uac04\uc0c9\uc73c\ub85c \uac15\uc870\ub41c \ubd80\ubd84\uc774 \uc2dc\uac01 \ud1a0\ud070\uc785\ub2c8\ub2e4. \uc595\uc740 \ub808\uc774\uc5b4(\uc608: \ub808\uc774\uc5b4 1)\uc5d0\uc11c\ub294 \uc2dc\uac01 \ud1a0\ud070\uacfc \ud14d\uc2a4\ud2b8 \ud1a0\ud070 \ubaa8\ub450\uc5d0 \uac78\uccd0 \uc5b4\ud150\uc158\uc774 \ube44\uad50\uc801 \uace0\ub974\uac8c \ubd84\ud3ec\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uae4a\uc740 \ub808\uc774\uc5b4(\uc608: \ub808\uc774\uc5b4 16\uacfc 32)\uc5d0\uc11c\ub294 \uc2dc\uac01 \ud1a0\ud070 \uc55e\uc5d0 \ucd94\uac00\ub41c \uc2dc\uc2a4\ud15c \ud504\ub86c\ud504\ud2b8 \ud1a0\ud070, \ud504\ub86c\ud504\ud2b8 \ud1a0\ud070 \ubc0f \ucd9c\ub825 \ud1a0\ud070\uc5d0 \uc5b4\ud150\uc158\uc774 \uc9d1\uc911\ub418\ub294 \ubc18\uba74, \uc2dc\uac01 \ud1a0\ud070\uc5d0\ub294 \uac70\uc758 \uc5b4\ud150\uc158\uc774 \uc8fc\uc5b4\uc9c0\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.", "section": "3.1 \ub2e4\uc911 \ubaa8\ub2ec LLMs\uc5d0\uc11c\uc758 \uc5b4\ud150\uc158 \ud328\ud134 \uc774\ud574"}]