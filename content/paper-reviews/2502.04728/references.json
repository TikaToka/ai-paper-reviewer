{"references": [{"fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-07", "reason": "This paper is foundational for understanding the capabilities and limitations of LLMs in program synthesis, a relevant area to this paper's focus on task planning."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This work is highly influential due to its demonstration of LLMs' few-shot learning capabilities, which is directly relevant to this paper's approach of scaling LLMs for test-time planning."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-14", "reason": "This is relevant as it shows that training LLMs on reasoning tasks such as mathematical word problems can improve their planning capabilities, related to this paper's method."}, {"fullname_first_author": "Lin Guan", "paper_title": "Leveraging pre-trained large language models to construct and utilize world models for model-based task planning", "publication_date": "2023-12-01", "reason": "This paper directly addresses the challenge of using LLMs for task planning and provides a relevant baseline method that this paper aims to improve upon."}, {"fullname_first_author": "Tim Z Xiao", "paper_title": "Verbalized machine learning: Revisiting machine learning with language models", "publication_date": "2024-06-04", "reason": "This work introduces the core concept of verbalized machine learning, which is directly adopted and extended in the proposed iVML method in this paper."}]}