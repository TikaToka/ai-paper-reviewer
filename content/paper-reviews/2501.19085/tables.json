[{"content": "| Model | Size | Julia | Lua | R | Racket | Java | Python |\n|---|---|---|---|---|---|---|---| \n| DeepSeek Coder - Instruct | 1B | 19.2 | 31.3 | 14.3 | 7.0 | 41.9 | 61.3 |\n| DeepSeek Coder - Instruct | 7B | 41.2 | 51.6 | 30.4 | 20.9 | 58.1 | 74.9 |\n| DeepSeek Coder - Instruct | 33B | 43.3 | 53.8 | 30.9 | 33.1 | 57.8 | 67.5 |\n| Code Llama - Instruct | 7B | 28.4 | 32.3 | 14.2 | 11.5 | 30.6 | 33.7 |\n| Code Llama - Instruct | 13B | 33.4 | 32.6 | 15.6 | 15.2 | 38.7 | 44.5 |\n| GitHub Copilot | Unknown | 53.5 | 61.4 | 32.9 | 24.7 | 57.2 | 61.7 |\n| Average |  | 36.5 | 43.8 | 23.1 | 18.7 | 47.4 | 57.3 |", "caption": "TABLE I: Pass@1 rates of the models on the different languages. Black cells indicate best performance per model, white cells indicate worst performance. Performance in between is gray-scaled.", "description": "\ud45c I\uc740 \ub2e4\uc591\ud55c \ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\uc5d0 \ub300\ud55c \uc5ec\ub7ec \ubaa8\ub378\uc758 Pass@1 \uc131\uacf5\ub960\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac80\uc740\uc0c9 \uc140\uc740 \ubaa8\ub378\ubcc4 \ucd5c\uace0 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0b4\uace0, \ud770\uc0c9 \uc140\uc740 \ucd5c\uc800 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uadf8 \uc911\uac04 \uc131\ub2a5\uc740 \ud68c\uc0c9 \uc74c\uc601\uc73c\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4(Julia, Lua, R, Racket, Java, Python)\uc5d0 \ub300\ud574 6\uac00\uc9c0 \uc11c\ub85c \ub2e4\ub978 \ud06c\uae30\uc758 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc758 \ucf54\ub4dc \uc0dd\uc131 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uc800\uc790\uc6d0 \uc5b8\uc5b4\uc640 \uace0\uc790\uc6d0 \uc5b8\uc5b4 \uac04\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc740 Pass@1 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uce21\uc815\ub418\uba70, \uc774\ub294 \ubaa8\ub378\uc774 \uc0dd\uc131\ud55c \ucf54\ub4dc\uac00 \ud14c\uc2a4\ud2b8\ub97c \ud1b5\uacfc\ud560 \ud655\ub960\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "III. \uace0\uc790\uc6d0 \ubc0f \uc800\uc790\uc6d0 \uc5b8\uc5b4\uc5d0 \ub300\ud55c LLM \uae30\ubc18 \ucf54\ub4dc \uc0dd\uc131 \uc131\ub2a5 \ucc28\uc774"}, {"content": "| Languages | DS-1B | DS-7B | DS-33B | CL-7B | CL-13B | Copilot |\n|---|---|---|---|---|---|---|\n| Java _vs._ Julia | 5.93 | 2.69 | 2.61 | 1.27 | 1.65 | 1.34 |\n| Java _vs._ Lua | 2.04 | 1.50 | 1.28 | 0.85 | 1.75 | 0.72 |\n| Java _vs._ R | 8.32 | 4.31 | 4.05 | 4.16 | 4.97 | 4.82 |\n| Java _vs._ Racket | 22.88 | 12.08 | 4.33 | 11.21 | 9.17 | 8.91 |\n| Python _vs._ Julia | 19.34 | 8.66 | 5.40 | 1.79 | 2.59 | 1.70 |\n| Python _vs._ Lua | 9.31 | 4.27 | 2.47 | 1.13 | 2.69 | 1.02 |\n| Python _vs._ R | 25.61 | 11.56 | 7.21 | 5.72 | 10.93 | 6.00 |\n| Python _vs._ Racket | 251.59 | 42.97 | 8.06 | 11.88 | 16.56 | 9.97 |\n| Julia _vs._ R | 1.74 | 2.09 | 2.10 | 3.98 | 4.37 | 3.79 |\n| Julia _vs._ Racket | 4.98 | 4.39 | 1.80 | 9.92 | 5.93 | 7.88 |\n| Lua _vs._ R | 4.80 | 3.90 | 3.63 | 4.96 | 4.02 | 6.57 |\n| Lua _vs._ Racket | 14.16 | 8.13 | 2.91 | 9.69 | 6.24 | 12.43 |", "caption": "TABLE II: Odds ratios computed between pairs of languages for each model evaluated (DS = DeepSeek Coder, CL = Code Llama). Values in bold are statistically significant (p\ud835\udc5dpitalic_p-value <<< 0.05).", "description": "\ud45c II\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\uc5d0\uc11c \uac01 \uc5b8\uc5b4 \uc30d \uac04\uc758 \uc2b9\uc0b0\ube44\ub97c \uacc4\uc0b0\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (DS = DeepSeek Coder, CL = Code Llama).  \ubcfc\ub4dc\uccb4 \uac12\uc740 \ud1b5\uacc4\uc801\uc73c\ub85c \uc720\uc758\ubbf8\ud55c \uacb0\uacfc(p-\uac12 < 0.05)\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uac01 \ubaa8\ub378\uc774 \ud2b9\uc815 \uc5b8\uc5b4 \uc30d\uc5d0\uc11c \uc5bc\ub9c8\ub098 \ub2e4\ub978 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294\uc9c0, \uc989 \uc5b4\ub5a4 \uc5b8\uc5b4 \uc30d\uc5d0\uc11c \ubaa8\ub378\uc758 \uc131\ub2a5 \ucc28\uc774\uac00 \ub354 \ud070\uc9c0(\uc2b9\uc0b0\ube44\uac00 \ub192\uc744\uc218\ub85d \ucc28\uc774\uac00 \ud07c)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc608\ub97c \ub4e4\uc5b4 Java\uc640 Julia\ub97c \ube44\uad50\ud588\uc744 \ub54c \uc2b9\uc0b0\ube44\uac00 \ub192\ub2e4\uba74 \ubaa8\ub378\uc774 Java\uc5d0\uc11c Julia\ubcf4\ub2e4 \ud6e8\uc52c \ub354 \uc815\ud655\ud55c \ucf54\ub4dc\ub97c \uc0dd\uc131\ud568\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "III. \uace0\uc131\ub2a5 \ubc0f \uc800\uc131\ub2a5 \ub9ac\uc18c\uc2a4 \uae30\ubc18 \uc5b8\uc5b4\uc5d0 \ub300\ud55c LLM \uae30\ubc18 \ucf54\ub4dc \uc0dd\uc131 \uc131\ub2a5 \ucc28\uc774"}, {"content": "| Model | Size | Technique | R | Racket |\n|---|---|---|---|---|\n| DeepSeek Coder - Instruct | 1B | Baseline | 13.9 | 7.0 |\n|  |  | In-context Learning \u2013 Translation Examples | 13.8 | 7.7 |\n|  |  | In-context Learning \u2013 Translation Rules | 13.4 | 6.5 |\n|  |  | In-context Learning \u2013 Few-shot Examples | 14.1 | 8.4 |\n|  |  | Fine-tuning \u2013 Code Generation | 16.7 | 18.1 |\n|  |  | Pre-training & Fine-tuning \u2013 Code Translation and Generation | 16.0 | 18.4 |\n| DeepSeek Coder - Instruct | 7B | Baseline | 29.6 | 20.4 |\n|  |  | In-context Learning \u2013 Translation Examples | 32.1 | 22.5 |\n|  |  | In-context Learning \u2013 Translation Rules | 30.0 | 20.0 |\n|  |  | In-context Learning \u2013 Few-shot Examples | 30.9 | 24.6 |\n|  |  | Fine-tuning \u2013 Code Generation | 26.4 | 31.7 |\n|  |  | Pre-training & Fine-tuning \u2013 Code Translation and Generation | 25.0 | 30.4 |\n| DeepSeek Coder - Instruct | 33B | Baseline | 30.2 | 32.5 |\n|  |  | In-context Learning \u2013 Translation Examples | 36.5 | 36.3 |\n|  |  | In-context Learning \u2013 Translation Rules | 33.6 | 35.8 |\n|  |  | In-context Learning \u2013 Few-shot Examples | 38.3 | 36.2 |\n|  |  | Fine-tuning \u2013 Code Generation | 25.3 | 28.0 |\n|  |  | Pre-training & Fine-tuning \u2013 Code Translation and Generation | 25.8 | 26.8 |\n| Code Llama - Instruct | 7B | Baseline | 13.9 | 11.2 |\n|  |  | In-context Learning \u2013 Translation Examples | 15.8 | 12.1 |\n|  |  | In-context Learning \u2013 Translation Rules | 12.3 | 11.1 |\n|  |  | In-context Learning \u2013 Few-shot Examples | 14.6 | 12.7 |\n|  |  | Fine-tuning \u2013 Code Generation | 14.6 | 22.0 |\n|  |  | Pre-training & Fine-tuning \u2013 Code Translation and Generation | 15.7 | 19.7 |\n| Code Llama - Instruct | 13B | Baseline | 15.2 | 14.8 |\n|  |  | In-context Learning \u2013 Translation Examples | 18.9 | 16.1 |\n|  |  | In-context Learning \u2013 Translation Rules | 17.2 | 14.2 |\n|  |  | In-context Learning \u2013 Few-shot Examples | 19.7 | 13.9 |\n|  |  | Fine-tuning \u2013 Code Generation | 16.6 | 22.3 |\n|  |  | Pre-training & Fine-tuning \u2013 Code Translation and Generation | 15.6 | 20.7 |\n| GitHub Copilot | Unknown | Baseline | 32.7 | 24.3 |\n|  |  | In-context Learning \u2013 Translation Examples | 37.3 | 27.1 |\n|  |  | In-context Learning \u2013 Translation Rules | 34.4 | 25.1 |\n|  |  | In-context Learning \u2013 Few-shot Examples | 41.1 | 25.7 |", "caption": "TABLE III: Pass@1 rates by model, size, and technique. Light gray rows are in-context learning-based techniques, dark gray rows are fine-tuning-based techniques. Values in bold depict the best-performing technique per model, size and language.", "description": "\ud45c III\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378 \ud06c\uae30\uc640 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub0ae\uc740 \uc790\uc6d0 \uc5b8\uc5b4\uc5d0 \ub300\ud55c \ucf54\ub4dc \uc0dd\uc131 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\ub294 \ubaa8\ub378(DeepSeek Coder, Code Llama, GitHub Copilot), \ubaa8\ub378 \ud06c\uae30(1B, 7B, 13B, 33B \ud30c\ub77c\ubbf8\ud130), \uadf8\ub9ac\uace0 \uc801\uc6a9\ub41c \uae30\ubc95(\ucee8\ud14d\uc2a4\ud2b8 \ud559\uc2b5 \uae30\ubc18 \uae30\ubc95, \ud30c\uc778\ud29c\ub2dd \uae30\ubc18 \uae30\ubc95)\ubcc4\ub85c \ub098\ub204\uc5b4\uc838 \uc788\uc73c\uba70, \uac01 \uc140\uc5d0\ub294 R\uacfc Racket \uc5b8\uc5b4\uc5d0 \ub300\ud55c Pass@1 \uc810\uc218\uac00 \ud45c\uc2dc\ub429\ub2c8\ub2e4.  \uc5f0\ud68c\uc0c9 \ud589\uc740 \ucee8\ud14d\uc2a4\ud2b8 \ud559\uc2b5 \uae30\ubc18 \uae30\ubc95, \uc9c4\ud55c \ud68c\uc0c9 \ud589\uc740 \ud30c\uc778\ud29c\ub2dd \uae30\ubc18 \uae30\ubc95\uc744 \ub098\ud0c0\ub0b4\uba70, \uac01 \ubaa8\ub378, \ud06c\uae30, \uc5b8\uc5b4\ubcc4\ub85c \uac00\uc7a5 \uc131\ub2a5\uc774 \uc88b\uc740 \uae30\ubc95\uc740 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub294 \ub0ae\uc740 \uc790\uc6d0 \uc5b8\uc5b4\uc5d0\uc11c \ucf54\ub4dc \uc0dd\uc131 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uae30 \uc704\ud55c \ub2e4\uc591\ud55c \uae30\ubc95\ub4e4\uc758 \ud6a8\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "IV. \ub0ae\uc740 \uc790\uc6d0 \uc5b8\uc5b4\uc5d0 \ub300\ud55c \ucf54\ub4dc \uc0dd\uc131 \ud5a5\uc0c1 \uae30\ubc95 \uc5f0\uad6c"}]