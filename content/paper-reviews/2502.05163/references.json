{"references": [{"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational for the concept of reward models and reinforcement learning with human feedback (RLHF), which is directly relevant to the development and training of LLM guardrails."}, {"fullname_first_author": "Inan, H.", "paper_title": "LlamaGuard: LLM-based input-output safeguard for human-AI conversations", "publication_date": "2023-10-26", "reason": "This paper introduces LlamaGuard, a key prior work for multilingual LLM guardrails, which serves as a baseline for comparison in evaluating the proposed DuoGuard model."}, {"fullname_first_author": "Zeng, W.", "paper_title": "ShieldGemma: Generative AI content moderation based on GEMMA", "publication_date": "2024-07-21", "reason": "This paper introduces ShieldGemma, another significant prior work in LLM guardrails, providing a strong comparative baseline for DuoGuard's performance evaluation."}, {"fullname_first_author": "Rafailov, R.", "paper_title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "publication_date": "2023-05-18", "reason": "This paper proposes Direct Preference Optimization (DPO), a method crucial for efficient reward modeling and training the generator in the DuoGuard framework."}, {"fullname_first_author": "Azar, M. G.", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "publication_date": "2024-01-01", "reason": "This paper provides theoretical foundations for the two-player RL framework used in DuoGuard, proving convergence to Nash Equilibrium, which is essential for the algorithm's stability and effectiveness."}]}