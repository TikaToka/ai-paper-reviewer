<table id='1' style='font-size:14px'><tr><td>Hyperparameters</td><td>Stage 1</td><td>Stage 2</td><td>Stage 3</td></tr><tr><td>Learning rate</td><td>1.0 x 10-3</td><td>1 x 10-4</td><td>2.0 x 10-5</td></tr><tr><td>LR scheduler</td><td>Cosine</td><td>Constant</td><td>Constant</td></tr><tr><td>Weight decay</td><td>0.0</td><td>0.0</td><td>0.1</td></tr><tr><td>Gradient clip</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>Optimizer</td><td colspan="3">AdamW (B1 = 0.9, B2 = 0.95)</td></tr><tr><td>Warm-up steps</td><td>300</td><td>5, 000</td><td>0</td></tr><tr><td>Training steps</td><td>10,000</td><td>180, 000</td><td>24, 000</td></tr><tr><td>Batch size</td><td>256</td><td>512</td><td>256</td></tr><tr><td>Data Ratio</td><td>1 : 0 : 1</td><td>2 : 3 : 5</td><td>7 : 3 : 10</td></tr></table>