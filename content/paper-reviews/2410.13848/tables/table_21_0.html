<table id='0' style='font-size:18px'><tr><td>[46]</td><td>T. Li, Y. Tian, H. Li, M. Deng, and K. He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024.</td></tr><tr><td>[47]</td><td>X. Li, F. Zhang, H. Diao, Y. Wang, X. Wang, and L.-Y. Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. arXiv preprint arXiv:2407.08303, 2024.</td></tr><tr><td>[48]</td><td>Y. Li, Y. Du, K. Zhou,J. Wang, W. X. Zhao, and J.-R. Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.</td></tr><tr><td>[49]</td><td>Z. Li, X. Yang, K. Choi, W. Zhu, R. Hsieh, H. Kim, J. H. Lim, S. Ji, B. Lee, X. Yan, et al. Mmsci: A multimodal multi-discipline dataset for phd-level scientific comprehension. arXiv preprint arXiv:2407.04903, 2024.</td></tr><tr><td>[50]</td><td>H. Liu, C. Li, Y. Li, and Y.J. Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296-26306, 2024.</td></tr><tr><td>[51]</td><td>H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.</td></tr><tr><td>[52]</td><td>H. Liu, W. Yan, M. Zaharia, and P. Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024.</td></tr><tr><td>[53]</td><td>M. Liu, R. Shi, K. Kuang, Y. Zhu, X. Li, S. Han, H. Cai, F. Porikli, and H. Su. Openshape: Scaling up 3d shape representation towards open-world understanding. Advances in neural information processing systems, 36, 2024.</td></tr><tr><td>[54]</td><td>Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan,J. Wang, C. He, Z. Liu, etal. Mm- bench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.</td></tr><tr><td>[55]</td><td>H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, Y. Sun, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024.</td></tr><tr><td>[56]</td><td>P. Lu, L. Qiu, J. Chen, T. Xia, Y. Zhao, W. Zhang, Z. Yu, X. Liang, and S.-C. Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021.</td></tr><tr><td>[57]</td><td>madebyollin. Megalith-huggingface. https://huggingface · co/datasets/madebyol lin/megalith-10m, 2024.</td></tr><tr><td>[58]</td><td>mehdidc. Yfcc-huggingface. https://huggingface · co/datasets/mehdidc/yfcc15 m, 2024.</td></tr><tr><td>[59]</td><td>A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.</td></tr><tr><td>[60]</td><td>J. Pan, K. Sun, Y. Ge, H. Li, H. Duan, X. Wu, R. Zhang, A. Zhou, Z. Qin, Y. Wang, J. Dai, Y. Qiao, and H. Li. Journeydb: A benchmark for generative image understanding, 2023.</td></tr><tr><td>[61]</td><td>Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei. Beit v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022.</td></tr></table>