<table id='0' style='font-size:18px'><tr><td>[15]</td><td>X. Chu, L. Qiao, X. Zhang, S. Xu, F. Wei, Y. Yang, X. Sun, Y. Hu, X. Lin, B. Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint arXiv:2402.03766, 2024.</td></tr><tr><td>[16]</td><td>W. Dai, J. Li, D. Li, A. M. H. Tiong,J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.</td></tr><tr><td>[17]</td><td>dclure. Laion-aesthetics-umap. https: / /huggingface Â· co/ datasets/dclure/laion -aesthetics-12m-umap, 2022.</td></tr><tr><td>[18]</td><td>J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchi- cal image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.</td></tr><tr><td>[19]</td><td>J. Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</td></tr><tr><td>[20]</td><td>P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780-8794, 2021.</td></tr><tr><td>[21]</td><td>R. Dong, C. Han, Y. Peng, Z. Qi, Z. Ge,J. Yang, L. Zhao,J. Sun, H. Zhou, H. Wei, etal. Dream- llm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023.</td></tr><tr><td>[22]</td><td>A. Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</td></tr><tr><td>[23]</td><td>Echo840. Detailed caption dataset. https : / /huggingface . co/datasets/echo840/ Detailed_ Caption, 2023.</td></tr><tr><td>[24]</td><td>P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873-12883, 2021.</td></tr><tr><td>[25]</td><td>C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.</td></tr><tr><td>[26]</td><td>O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and Y. Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In European Conference on Computer Vision, pages 89-106. Springer, 2022.</td></tr><tr><td>[27]</td><td>Y. Ge, Y. Ge, Z. Zeng, X. Wang, and Y. Shan. Planting a seed of vision in large language model. arXiv preprint arXiv:2307.08041, 2023.</td></tr><tr><td>[28]</td><td>Y. Ge, S. Zhao, Z. Zeng, Y. Ge, C. Li, X. Wang, and Y. Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023.</td></tr><tr><td>[29]</td><td>Y. Ge, S. Zhao,J. Zhu, Y. Ge, K. Yi, L. Song, C. Li, X. Ding, and Y. Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024.</td></tr><tr><td>[30]</td><td>D. Ghosh, H. Hajishirzi, and L. Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024.</td></tr></table>