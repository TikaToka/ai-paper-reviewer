{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-08", "reason": "This paper provides technical details of GPT-4, a large language model used as a baseline in the study's comparisons and analysis."}, {"fullname_first_author": "Bradley Brown", "paper_title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling", "publication_date": "2024-07-21", "reason": "This paper introduces a method for improving LLMs by using parallel sampling, a technique that is extended and improved upon in this study."}, {"fullname_first_author": "Daya Guo", "paper_title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "publication_date": "2025-01-12", "reason": "This paper presents DeepSeek-R1, a reasoning model that is compared against the proposed method, showcasing the advancement of S* in comparison."}, {"fullname_first_author": "Naman Jain", "paper_title": "LiveCodeBench: Holistic and Contamination-Free Evaluation of Large Language Models for Code", "publication_date": "2024-03-07", "reason": "This paper introduces LiveCodeBench, a benchmark dataset extensively used for evaluating code generation models, forming the basis of the experiments in the study."}, {"fullname_first_author": "Niklas Muennighoff", "paper_title": "S1: Simple Test-Time Scaling", "publication_date": "2025-01-19", "reason": "This paper explores test-time scaling, a related concept to the study's main approach, providing a comparison point and contextual understanding of the field."}]}