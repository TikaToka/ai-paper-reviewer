<table id='0' style='font-size:14px'><tr><td colspan="2">Algorithm 1: RARe - Training</td></tr><tr><td colspan="2">Input: Training set D, embedder E( ·), BM25, the number of in-context examples k, mini-batch size B.</td></tr><tr><td colspan="2">1: for each training iteration do</td></tr><tr><td>2:</td><td>Sample mini-batch B of size B from D</td></tr><tr><td>3:</td><td>for (ti, q, d+ , d ) E B do</td></tr><tr><td>4:</td><td>In-Context Example Retrieval:</td></tr><tr><td>5:</td><td>{qi c ic qik } ← Retrieve nearest neighbor queries of q from D using BM25 , 92 , · · · ,</td></tr><tr><td>6:</td><td>2 c+ {d+ : (q', d+ ) E D,q E {qic , · · · , qik }} {d1 c+ dic+ , · · · , d k } ← ,</td></tr><tr><td>7:</td><td>Dic ← {(gic dic+ ) , · . · , (qk, dic+)} ,</td></tr><tr><td>8:</td><td>Query Augmentation:</td></tr><tr><td>9:</td><td>inst+ic {qic}; Document: {dic+} ...; Query: {q} q = Instruct: {ti}; Query:</td></tr><tr><td>10:</td><td>Training with Contrastive Loss:</td></tr><tr><td>11:</td><td>Compute the mini-batch contrastive loss LRARe as described in Equation 5</td></tr><tr><td>12:</td><td>Update E(・) by minimizing LRARe.</td></tr><tr><td colspan="2">Output: Trained embedder E(·)</td></tr></table>