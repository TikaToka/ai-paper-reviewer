{"references": [{"fullname_first_author": "Jaech et al.", "paper_title": "OpenAI o1 System Card", "publication_date": "2024-12-16", "reason": "This paper introduces the o1 model, which serves as the primary inspiration for the research in this paper, outlining its reasoning capabilities and methodology which this work aims to reproduce without distillation."}, {"fullname_first_author": "Li et al.", "paper_title": "From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline", "publication_date": "2024-06-11", "reason": "This paper provides the Arena-Hard benchmark dataset used for evaluation in this research, a key component in assessing the effectiveness of the proposed method."}, {"fullname_first_author": "Zheng et al.", "paper_title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena", "publication_date": "2023-06-05", "reason": "This paper introduces the MT-Bench benchmark dataset, another crucial element used in the evaluation process to test the capabilities of the developed LongCoT models."}, {"fullname_first_author": "Wang et al.", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-01-11", "reason": "This foundational paper introduces the concept of chain-of-thought prompting, a central technique underlying the research, demonstrating the improvement in LLM reasoning with this method."}, {"fullname_first_author": "Schulman et al.", "paper_title": "Proximal Policy Optimization algorithms", "publication_date": "2017-07-06", "reason": "This widely cited paper introduces the Proximal Policy Optimization (PPO) algorithm, a reinforcement learning method used in this research and discussed in related work, crucial for training and improving LLMs."}]}