[{"content": "| Seq_len | Page Size | Page Size | Page Size | Page Size | Max Slowdown |\n|---|---|---|---|---|---| \n|  | 16 | 32 | 64 | 128 |  |\n| 512 | 11.0 ms | 10.7 ms | **10.5 ms** | 10.5 ms |  |\n| 1024 | 13.8 ms | 13.0 ms | **12.7 ms** | 12.7 ms |  |\n| 2048 | 22.1 ms | 20.1 ms | **18.3 ms** | 18.2 ms |  |\n| 4096 | 35.7 ms | 31.6 ms | **28.1 ms** | 28.1 ms |  |\n| 8192 | **77.1 ms** | **63.0 ms** | **51.0 ms** | 50.6 ms |  |\n| Max Slowdown | 1.52\u00d7 | 1.25\u00d7 | 1.01\u00d7 | 1.00\u00d7 |  |", "caption": "Table 1: Page size significantly impacts the LLM serving system\u2019s efficiency: Larger page size is more hardware-friendly as it improves contiguity of memory layout and the GPU bandwidth utilization during attention computation. For example, simply shrinking the page size in QServe\u00a0Lin et\u00a0al. (2024b) leads to prominent slow-down of the end-to-end system. We evaluate the per-step decoding latency (ms / step) of QServe on a single A100 GPU for demonstration. We use Llama3-8B model architecture, with the batch size of 32.", "description": "\ud45c 1\uc740 \ud398\uc774\uc9c0 \ud06c\uae30\uac00 LLM \uc11c\ube59 \uc2dc\uc2a4\ud15c\uc758 \ud6a8\uc728\uc131\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud070 \ud398\uc774\uc9c0 \ud06c\uae30\ub294 \uba54\ubaa8\ub9ac \ub808\uc774\uc544\uc6c3\uc758 \uc5f0\uc18d\uc131\uacfc \uc5b4\ud150\uc158 \uc5f0\uc0b0 \uc911 GPU \ub300\uc5ed\ud3ed \uc0ac\uc6a9\ub960\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ubbc0\ub85c \ud558\ub4dc\uc6e8\uc5b4 \uce5c\ud654\uc801\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, QServe (Lin et al., 2024b)\uc5d0\uc11c \ud398\uc774\uc9c0 \ud06c\uae30\ub97c \uc904\uc774\uba74 \uc2dc\uc2a4\ud15c \uc804\uccb4 \uc18d\ub3c4\uac00 \ud06c\uac8c \uc800\ud558\ub429\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e8\uc77c A100 GPU\uc5d0\uc11c QServe\uc758 \ub2e8\uacc4\ubcc4 \ub514\ucf54\ub529 \uc9c0\uc5f0 \uc2dc\uac04(ms/\ub2e8\uacc4)\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub370\ubaa8\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. Llama3-8B \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\ub97c \uc0ac\uc6a9\ud558\uace0 \ubc30\uce58 \ud06c\uae30\ub294 32\uc785\ub2c8\ub2e4.", "section": "3 LSERVE: LONG-SEQUENCE SERVING WITH UNIFIED SPARSE ATTENTION"}, {"content": "| Model | Llama-3-8B |  | Llama-2-7B |  |\n|---|---|---|---|---|\n| Benchmark | Dense | LServe | Dense | LServe |\n| 2WikiMQA | 30.3 | 31.6 | 35.4 | 35.1 |\n| DuReader | 30.3 | 30.8 | 25.4 | 24.7 |\n| HotpotQA | 41.7 | 42.7 | 47.4 | 49.6 |\n| MultiNews | 27.7 | 27.7 | 26.6 | 26.6 |\n| Qasper | 31.7 | 29.3 | 32.6 | 29.5 |\n| QMSum | 23.8 | 24.0 | 21.0 | 21.3 |\n| SamSum | 41.2 | 39.3 | 41.8 | 41.5 |\n| TriviaQA | 84.9 | 83.7 | 86.2 | 86.5 |\n| **Average** | **38.9** | **38.6** | **39.5** | **39.4** |", "caption": "Table 2: Accuracy evaluation on LongBench\u00a0Bai et\u00a0al. (2023). We compare our method with vanilla dense attention on 2 models and 10 different benchmarks.", "description": "\ud45c 2\ub294 LongBench(Bai et al., 2023) \ubca4\uce58\ub9c8\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec LServe\uc758 \uc815\ud655\ub3c4\ub97c \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub450 \uac00\uc9c0 \ubaa8\ub378(Llama-3-8B\uc640 Llama-2-7B)\uacfc 10\uac00\uc9c0 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud574 LServe\uc640 \uae30\uc874\uc758 \uc644\uc804\ud55c(dense) \uc5b4\ud150\uc158 \ubc29\uc2dd\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.  \uac01 \ubca4\uce58\ub9c8\ud06c\ub294 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \uc790\uc5f0\uc5b4 \ucc98\ub9ac \uc791\uc5c5\uc744 \ud3ec\ud568\ud558\uba70, \uacb0\uacfc\ub294 LServe\uac00 \ub2e4\uc591\ud55c \uc791\uc5c5\uc5d0\uc11c \uc644\uc804\ud55c \uc5b4\ud150\uc158 \ubc29\uc2dd\uacfc \uc720\uc0ac\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 \ud3c9\uac00"}, {"content": "| Model | 32K | 64K | 128K | 160K | 192K | 256K |\n|---|---|---|---|---|---|---|\n| Llama-3-8B | 90.5 | 86.8 | 83.8 | 79.3 | 79.6 | 79.4 |\n| LServe-4096 | 91.0 | 85.6 | 81.0 | 79.0 | 76.1 | 75.7 |\n| LServe-8192 | 91.8 | 86.1 | 81.7 | 81.2 | 79.7 | 79.1 |", "caption": "Table 3: Accuracy evaluation on RULER\u00a0Hsieh et\u00a0al. (2024). We evaluate the accuracy of Llama-3-8B on RULER benchmarks, including challenging tasks such as multi-hop tracing and aggregation to test behaviors beyond searching from context. LServe-N\ud835\udc41Nitalic_N denotes that the token budget for dynamic sparsity is N\ud835\udc41Nitalic_N. Note that for long-context inputs, latency is not dominated by attention alone in LServe, with page selector and GEMM also contributing to it. Experiments reveal that LServe-8192 is only up to 6% slower than LServe-4096 when the sequence length exceeds 128K.", "description": "\ud45c 3\uc740 RULER \ubca4\uce58\ub9c8\ud06c(Hsieh et al., 2024)\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\uc591\ud55c \ub09c\uc774\ub3c4\uc758 \uc791\uc5c5(\uba40\ud2f0 \ud649 \ucd94\uc801 \ubc0f \uc9d1\uacc4 \ub4f1)\uc744 \ud3ec\ud568\ud558\uc5ec \ubb38\ub9e5 \ud0d0\uc0c9 \uc774\uc0c1\uc758 \ub3d9\uc791\uc744 \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4. LServe-N\uc740 \ub3d9\uc801 \uc2a4\ud30c\uc2a4\uc131\uc5d0 \ub300\ud55c \ud1a0\ud070 \uc608\uc0b0\uc744 N\uc73c\ub85c \uc124\uc815\ud588\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uae34 \ubb38\ub9e5 \uc785\ub825\uc758 \uacbd\uc6b0 LServe\uc758 \uc9c0\uc5f0 \uc2dc\uac04\uc740 \uc5b4\ud150\uc158\ub9cc\uc73c\ub85c \uacb0\uc815\ub418\uc9c0 \uc54a\uace0 \ud398\uc774\uc9c0 \uc120\ud0dd\uae30\uc640 GEMM\ub3c4 \uc601\ud5a5\uc744 \ubbf8\uce69\ub2c8\ub2e4. \uc2e4\ud5d8 \uacb0\uacfc\uc5d0 \ub530\ub974\uba74 \uc2dc\ud000\uc2a4 \uae38\uc774\uac00 128K\ub97c \ucd08\uacfc\ud558\ub294 \uacbd\uc6b0 LServe-8192\ub294 LServe-4096\ubcf4\ub2e4 \ucd5c\ub300 6% \ub290\ub9b4 \ubfd0\uc785\ub2c8\ub2e4.", "section": "4 \ud3c9\uac00"}, {"content": "| Stage | System | Sequence Length 4K | Sequence Length 8K | Sequence Length 16K | Sequence Length 32K | Sequence Length 64K |\n|---|---|---|---|---|---|---|\n|Prefilling Latency (s)|Quest|0.51|0.82|1.62|3.61|OOM|\n| |LServe|0.24|0.49|1.08|2.32|5.27|\n|Speedup| |2.1\u00d7|1.7\u00d7|1.5\u00d7|1.6\u00d7|/|\n|Decoding Latency (ms)|Quest|13.13|13.58|14.08|14.86|OOM|\n| |LServe|10.02|10.29|10.22|10.24|11.54|\n|Speedup| |1.3\u00d7|1.3\u00d7|1.4\u00d7|1.5\u00d7|/|", "caption": "Table 4: LServe achieves lower latency over Quest\u00a0Tang et\u00a0al. (2024) system in both prefilling stage and decoding stage. We benchmark the two systems on Llama-2-7B model, since Quest does not support GQA\u00a0Ainslie et\u00a0al. (2023) architecture.", "description": "\ubcf8 \ud45c\ub294 LServe\uc640 Quest (Tang et al., 2024) \uc2dc\uc2a4\ud15c\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Quest \uc2dc\uc2a4\ud15c\uc774 GQA(Ainslie et al., 2023) \uc544\ud0a4\ud14d\ucc98\ub97c \uc9c0\uc6d0\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 Llama-2-7B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub450 \uc2dc\uc2a4\ud15c\uc758  Prefilling \ub2e8\uacc4\uc640 Decoding \ub2e8\uacc4\uc758 \uc9c0\uc5f0 \uc2dc\uac04(Latency)\uc744 \ubca4\uce58\ub9c8\ud06c\ud588\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\ub4ef\uc774, LServe\ub294 Quest \uc2dc\uc2a4\ud15c\ubcf4\ub2e4 Prefilling \ub2e8\uacc4\uc640 Decoding \ub2e8\uacc4 \ubaa8\ub450\uc5d0\uc11c \ub354 \ub0ae\uc740 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \uae30\ub85d\ud558\uc5ec \uc131\ub2a5 \uc6b0\uc704\ub97c \ubcf4\uc600\uc2b5\ub2c8\ub2e4.  \uc774\ub294 LServe\uc758 \ud6a8\uc728\uc801\uc778 \uc544\ud0a4\ud14d\ucc98\uc640 \ucd5c\uc801\ud654 \uae30\ubc95\uc774 \uc7a5\ubb38\uc758 \uc2dc\ud000\uc2a4 \ucc98\ub9ac\uc5d0 \ud6a8\uacfc\uc801\uc784\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.3 End-to-end Efficiency"}, {"content": "| Reuse Interval | Dense | 1 | 2 | 4 | 8 | 16 |\n|---|---|---|---|---|---|---|\n| LServe-4096 | 86.8 | 86.2 | 85.6 | 85.6 | 84.8 | 83.2 |\n| LServe-8192 | 86.8 | 86.1 | 85.8 | 85.5 | 85.6 | 84.8 |", "caption": "Table 5: The reusable page selector in LServe preserves the model\u2019s long-context accuracy while significantly reducing selection overhead by 4\u00d7\\times\u00d7 with a reuse interval of 4. We evaluate Llama-3-8B on RULER\u00a0Hsieh et\u00a0al. (2024) at a sequence length of 64K. LServe-N\ud835\udc41Nitalic_N denotes that the token budget for dynamic sparsity is N\ud835\udc41Nitalic_N.", "description": "\ud45c 5\ub294 LServe\uc758 \uc7ac\uc0ac\uc6a9 \uac00\ub2a5\ud55c \ud398\uc774\uc9c0 \uc120\ud0dd\uae30\uac00 \ubaa8\ub378\uc758 \uc7a5\uae30 \ucee8\ud14d\uc2a4\ud2b8 \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud558\uba74\uc11c \uc7ac\uc0ac\uc6a9 \uac04\uaca9 4\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc120\ud0dd \uc624\ubc84\ud5e4\ub4dc\ub97c 4\ubc30\uae4c\uc9c0 \ud06c\uac8c \uc904\uc774\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Llama-3-8B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec RULER \ubca4\uce58\ub9c8\ud06c(Hsieh et al., 2024)\uc5d0\uc11c \uc2dc\ud000\uc2a4 \uae38\uc774 64K\uc5d0 \ub300\ud574 \ud3c9\uac00\ub97c \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. LServe-N\uc740 \ub3d9\uc801 \uc2a4\ud30c\uc2a4\uc131\uc5d0 \ub300\ud55c \ud1a0\ud070 \uc608\uc0b0\uc774 N\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "3.5 Mitigating Page Selection Overhead"}]