{"references": [{"fullname_first_author": "Ainslie, J.", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-05-13", "reason": "This paper introduces Grouped-Query Attention (GQA), a crucial architectural modification for long-sequence LLMs that improves efficiency and is directly compared against in the current paper."}, {"fullname_first_author": "Kwon, W.", "paper_title": "Efficient memory management for large language model serving with PagedAttention", "publication_date": "2023-00-00", "reason": "This paper introduces PagedAttention, a key memory management technique for LLMs that LServe builds upon, significantly impacting efficiency and is directly compared against in the current paper."}, {"fullname_first_author": "Lin, Y.", "paper_title": "QServe: W4A8KV4 quantization and system co-design for efficient LLM serving", "publication_date": "2024-05-00", "reason": "This work presents QServe, a highly efficient LLM serving system that LServe improves upon, making it a significant comparative baseline in terms of performance and efficiency."}, {"fullname_first_author": "Jiang, H.", "paper_title": "MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention", "publication_date": "2024-07-00", "reason": "MInference is a key comparative baseline for LServe, focusing on accelerating the prefilling stage through dynamic sparse attention, a technique that LServe also utilizes and compares against."}, {"fullname_first_author": "Tang, J.", "paper_title": "Quest: Query-aware sparsity for efficient long-context LLM inference", "publication_date": "2024-06-00", "reason": "Quest is another important comparative baseline that focuses on decoding-stage optimization using dynamic sparsity, a technique directly compared and improved upon by LServe."}]}