{"references": [{"fullname_first_author": "Brown, T. B.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-XX-XX", "reason": "This paper is foundational to the field of large language models (LLMs), introducing the concept of few-shot learning, which is directly relevant to the core ideas of the Control LLM paper."}, {"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-XX-XX", "reason": "This paper introduced the Transformer architecture, the basis of most modern LLMs, including the ones used in this study, therefore having a fundamental impact on the field."}, {"fullname_first_author": "Kirkpatrick, J.", "paper_title": "Overcoming catastrophic forgetting in neural networks", "publication_date": "2017-XX-XX", "reason": "This paper tackles the problem of catastrophic forgetting in neural networks, a critical challenge addressed directly by the Control LLM paper's methods for knowledge retention."}, {"fullname_first_author": "Hu, E. J.", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-XX-XX", "reason": "This paper introduces the LoRA method, a parameter-efficient fine-tuning technique that is compared to and contrasted against the Control LLM method, showcasing the relative merits of each."}, {"fullname_first_author": "Wu, C.", "paper_title": "LLaMA Pro: Progressive LLaMA with block expansion", "publication_date": "2024-XX-XX", "reason": "This paper explores model expansion methods, directly related to the design choices made in Control LLM, providing context for the architectural decisions in the presented work."}]}