[{"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/control_llm_sota_comparison.png", "caption": "Figure 1: Comparison: Ours vs SOTA Llama-tuned models.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c Control LLM \ubaa8\ub378\uacfc \uae30\uc874 \ucd5c\ucca8\ub2e8(SOTA) Llama \uae30\ubc18 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5(Math-Hard, MMLU, MBPP-PLUS, CMMLU, GPQA, ARC)\uc5d0\uc11c Control LLM\uc774 SOTA \ubaa8\ub378\ub4e4\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc810\uc218\ub97c \ud1b5\ud574 Control LLM\uc758 \uc131\ub2a5 \uac1c\uc120 \uc815\ub3c4\ub97c \uba85\ud655\ud558\uac8c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/catastrophic_forgetting_openmath.png", "caption": "Figure 2: [Result] Comparison of CF - our method vs others on open-source datasets: (left) OpenMath, (right) OpenCoder.", "description": "\uadf8\ub9bc 2\ub294 \uc81c\uc548\ub41c Control LLM \ubc29\ubc95\uacfc \ub2e4\ub978 \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378\ub4e4\uc758 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  OpenMath\uc640 OpenCoder \ub450 \uac00\uc9c0 \uc624\ud508\uc18c\uc2a4 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc9c0\uc18d\uc801 \uc0ac\uc804 \ud6c8\ub828(CPT)\uacfc \uc9c0\uc18d\uc801 \uc9c0\ub3c4 \ubbf8\uc138 \uc870\uc815(CSFT) \uacfc\uc815 \uc911 \ubc1c\uc0dd\ud558\ub294  catastrophic forgetting(CF) \ud604\uc0c1\uc744 \uc5bc\ub9c8\ub098 \uc798 \uadf9\ubcf5\ud558\ub294\uc9c0 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. \uc67c\ucabd\uc740 \uc218\ud559\uc801 \ucd94\ub860(OpenMath \ub370\uc774\ud130\uc14b)\uc5d0 \ub300\ud55c \uacb0\uacfc\uc774\uace0, \uc624\ub978\ucabd\uc740 \ucf54\ub529(OpenCoder \ub370\uc774\ud130\uc14b)\uc5d0 \ub300\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. \uac01 \uadf8\ub798\ud504\uc758 \ub9c9\ub300\ub294 \ud2b9\uc815 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0b4\uba70, Control LLM\uc774 \ub2e4\ub978 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 CF\ub97c \ub354 \ud6a8\uacfc\uc801\uc73c\ub85c \uc644\ud654\ud558\uba74\uc11c \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ucf30\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/catastrophic_forgetting_opencoder.png", "caption": "Figure 3: [Why] Hidden State Alignment Comparison: Best Alignment(Control LLM) vs Worst Alignment(Full-Parameter Tuning).", "description": "\ubcf8 \uadf8\ub9bc\uc740 Control LLM\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ubcd1\ub82c \uc804\ucc98\ub9ac \ubc0f \ud655\uc7a5\ub41c \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ube14\ub85d\uc758 \uc740\ub2c9 \uc0c1\ud0dc \uc815\ub82c\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Control LLM\uc740 \uae30\uc874\uc758 \uc0ac\uc804 \ud6c8\ub828\ub41c \ube14\ub85d\uacfc \uc0c8\ub86d\uac8c \ud559\uc2b5\ub418\ub294 \ube14\ub85d\uc758 \uc740\ub2c9 \uc0c1\ud0dc\ub97c \ubcf4\uac04\ubc95\uc744 \ud1b5\ud574 \uc815\ub82c\ud558\uc5ec \uae30\uc874 \uacfc\uc81c\uc758 \uc131\ub2a5\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uc0c8\ub85c\uc6b4 \uc9c0\uc2dd\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud1b5\ud569\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ucd5c\uc801\uc758 \uc815\ub82c\uc744 \ub2ec\uc131\ud55c Control LLM\uacfc \ucd5c\uc545\uc758 \uc815\ub82c\uc744 \ubcf4\uc778 \uc804\uccb4 \ub9e4\uac1c\ubcc0\uc218 \ubbf8\uc138 \uc870\uc815 \ubc29\ubc95\uc744 \ube44\uad50\ud558\uc5ec,  \uc798 \uc815\ub82c\ub41c \uc740\ub2c9 \uc0c1\ud0dc\uac00 \uae30\uc874\uc758 \ub2a5\ub825\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uc0c8\ub85c\uc6b4 \uacfc\uc81c\ub97c \ud559\uc2b5\ud558\ub294 \ub370 \uc911\uc694\ud568\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788, \uc218\ud559 \uacfc\uc81c(OpenMath2)\uc5d0\uc11c Control LLM\uc740 Math Hard \uc815\ud655\ub3c4\ub97c 14.4% \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ubc18\uba74 MMLU\ub294 4.3%\ub9cc \uac10\uc18c\uc2dc\ud0a4\ub294 \ubc18\uba74, \uc798\ubabb \uc815\ub82c\ub41c \ubc29\ubc95\uc740 Math Hard\uc5d0\uc11c +14.8%\ub97c \ub2ec\uc131\ud558\uc9c0\ub9cc MMLU\ub294 65.1%\ub098 \uac10\uc18c\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_comparison.png", "caption": "Figure 4: [How] Control LLM Architecture. (a) Expanded blocks added every N\u22121\ud835\udc411N-1italic_N - 1 layers connect to frozen blocks via interpolators. (b) Interpolators align hidden-states to produce final representations. (c) Different interpolation strategies are explored.", "description": "\uadf8\ub9bc 4\ub294 Control LLM\uc758 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 N-1\uac1c\uc758 \ub808\uc774\uc5b4\ub9c8\ub2e4 \ucd94\uac00\ub41c \ud655\uc7a5 \ube14\ub85d\uc774 \uc778\ud130\ud3f4\ub808\uc774\ud130\ub97c \ud1b5\ud574 \uace0\uc815\ub41c \ube14\ub85d\uc5d0 \uc5f0\uacb0\ub418\ub294 \ubc29\uc2dd\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. (b)\ub294 \uc778\ud130\ud3f4\ub808\uc774\ud130\uac00 \ucd5c\uc885 \ud45c\ud604\uc744 \uc0dd\uc131\ud558\uae30 \uc704\ud574 \ud788\ub4e0 \uc0c1\ud0dc\ub97c \uc815\ub82c\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (c)\ub294 \ub2e4\uc591\ud55c \uc778\ud130\ud3f4\ub808\uc774\uc158 \uc804\ub7b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 \uc5ec\ub7ec \uc778\ud130\ud3f4\ub808\uc774\uc158 \uae30\ubc95(Linear Interpolation, Dynamic Linear Interpolation, Progressive Linear Interpolation \ub4f1)\uc744 \uc0ac\uc6a9\ud558\uc5ec, pre-trained\ub41c \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ube14\ub85d\uacfc fine-tuning\uc744 \ud1b5\ud574 \uc0c8 \uc9c0\uc2dd\uc744 \ud559\uc2b5\ud558\ub294 expanded \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ube14\ub85d\uc758 \ud788\ub4e0 \uc0c1\ud0dc\ub97c \uc815\ub82c\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uae30\uc874 \uacfc\uc81c\uc758 \uc131\ub2a5\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uc0c8\ub85c\uc6b4 \uc9c0\uc2dd\uc744 \ud1b5\ud569\ud558\ub294 Control LLM\uc758 \ud575\uc2ec \uba54\ucee4\ub2c8\uc998\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/control_llm_architecture.png", "caption": "Figure 5: [How] Structure analysis: (concat) the default dual structure. (stack) stack the expanded block following LLaMA Pro. (hybrid) hybrid structure of concat and stack.", "description": "\uadf8\ub9bc 5\ub294 Control LLM\uc758 \uc138 \uac00\uc9c0 \ubaa8\ub378 \ud655\uc7a5 \uc804\ub7b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (concat)\ub294 \uae30\ubcf8 \uc774\uc911 \uad6c\uc870\ub85c, \uae30\uc874\uc758 \uc0ac\uc804 \ud6c8\ub828\ub41c \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ube14\ub85d\uc5d0 \uc0c8\ub85c\uc6b4 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ube14\ub85d\uc744 \ubcd1\ub82c\ub85c \ucd94\uac00\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. (stack)\ub294 LLaMA Pro \ubaa8\ub378\uc744 \ub530\ub974\ub294 \ubc29\uc2dd\uc73c\ub85c, \uc0c8\ub85c\uc6b4 \ube14\ub85d\uc744 \uae30\uc874 \ube14\ub85d \uc704\uc5d0 \uc313\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. (hybrid)\ub294 concat\uacfc stack \ubc29\uc2dd\uc744 \ud63c\ud569\ud55c \ubc29\uc2dd\uc785\ub2c8\ub2e4. \uac01 \uc804\ub7b5\uc740 \ud2b9\uc815 \ub808\uc774\uc5b4\uc5d0 \uc801\uc6a9\ub418\uba70, \ubaa8\ub378\uc758 \uc131\ub2a5\uacfc \uc5f0\uc0b0 \ube44\uc6a9\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce69\ub2c8\ub2e4.", "section": "3.2 Control LLM \uc544\ud0a4\ud14d\ucc98"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/control_llm_structure_analysis.png", "caption": "Figure 6: [OpenMath + Llama-3.1-8B-Instruct] Benchmark comparison of training methods. (a) Full Parameter Tuning. (b) Partial Parameter Tuning: Freeze all except 1 of every 4 transformer layers (8 total). (c) Stack Expansion: Add 8 transformer layers (1 per 4) while freezing originals. (d) Concat-Lerp Expansion: Add 8 transformer layers connected via interpolator with MSE divergence loss.", "description": "\uadf8\ub9bc 6\uc740 OpenMath \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec Llama-3.1-8B-Instruct \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud558\ub294 \ub124 \uac00\uc9c0 \ubc29\ubc95\uc758 \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  (a) \uc804\uccb4 \ud30c\ub77c\ubbf8\ud130 \ubbf8\uc138 \uc870\uc815\uc740 \ubaa8\ub4e0 \ud30c\ub77c\ubbf8\ud130\ub97c \ud559\uc2b5\ud558\uc9c0\ub9cc, \uae30\uc874 \uc131\ub2a5 \uc800\ud558\ub97c \ucd08\ub798\ud569\ub2c8\ub2e4. (b) \ubd80\ubd84\uc801 \ud30c\ub77c\ubbf8\ud130 \ubbf8\uc138 \uc870\uc815\uc740 \ubaa8\ub4e0 4\uac1c\uc758 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ub808\uc774\uc5b4 \uc911 1\uac1c\ub9cc \ud559\uc2b5\ud558\uc5ec \uae30\uc874 \uc131\ub2a5\uc744 \ubcf4\uc874\ud558\ub824\uace0 \uc2dc\ub3c4\ud558\uc9c0\ub9cc, \uc7a5\uae30\uc801\uc778 \uc131\ub2a5 \uc720\uc9c0\ub97c \uc5b4\ub835\uac8c \ub9cc\ub4ed\ub2c8\ub2e4. (c) \uc2a4\ud0dd \ud655\uc7a5\uc740 \uae30\uc874 \ub808\uc774\uc5b4\ub97c \ub3d9\uacb0\ud558\uba74\uc11c 8\uac1c\uc758 \uc0c8\ub85c\uc6b4 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ub808\uc774\uc5b4\ub97c \ucd94\uac00\ud558\uc5ec \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub824\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. (d) Concat-Lerp \ud655\uc7a5\uc740 \ubcf4\uac04\uae30\ub97c \uc0ac\uc6a9\ud558\uc5ec 8\uac1c\uc758 \uc0c8\ub85c\uc6b4 \ub808\uc774\uc5b4\ub97c \uae30\uc874 \ub808\uc774\uc5b4\uc5d0 \uc5f0\uacb0\ud558\ub294 Control LLM\uc758 \ubc29\ubc95\uc73c\ub85c, MSE \ubc1c\uc0b0 \uc190\uc2e4\uc744 \ud1b5\ud574 \uae30\uc874 \uc131\ub2a5\uc744 \ubcf4\uc874\ud558\uba74\uc11c \uc0c8\ub85c\uc6b4 \uc791\uc5c5 \ud559\uc2b5\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.  \uac01 \uadf8\ub798\ud504\ub294 Math-Hard(\uc0c8\ub85c\uc6b4 \uc791\uc5c5)\uc640 MMLU(\uae30\uc874 \uc791\uc5c5)\uc758 \uc815\ud655\ub3c4\ub97c \uae00\ub85c\ubc8c \ub2e8\uacc4\uc5d0 \ub530\ub77c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/ControlLLM_CF_Plot_Math.png", "caption": "Figure 7: [OpenCoder SFT Phase2 + Llama-3.1-8B-Instruct] Benchmark comparison of training methods. (a) Full Parameter Tuning. (b) Partial Parameter Tuning: Freeze all except 1 of every 4 transformer layers (8 total). (c) Stack Expansion: Add 8 transformer layers (1 per 4) while freezing originals. (d) Concat-Lerp Expansion: Add 8 transformer layers connected via interpolator with MSE divergence loss.", "description": "\uadf8\ub9bc 7\uc740 OpenCoder SFT Phase2 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec Llama-3.1-8B-Instruct \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud558\ub294 \ub2e4\uc591\ud55c \ubc29\ubc95\ub4e4\uc758 \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \ubaa8\ub4e0 \ud30c\ub77c\ubbf8\ud130\ub97c \ubbf8\uc138 \uc870\uc815\ud558\ub294 \ubc29\ubc95, (b)\ub294 4\uac1c\uc758 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ub808\uc774\uc5b4 \uc911 3\uac1c\ub97c \uace0\uc815\ud558\uace0 1\uac1c\ub9cc \ubbf8\uc138 \uc870\uc815\ud558\ub294 \ubd80\ubd84\uc801 \ud30c\ub77c\ubbf8\ud130 \ubbf8\uc138 \uc870\uc815 \ubc29\ubc95, (c)\ub294 \uc6d0\ubcf8 \ub808\uc774\uc5b4\ub97c \uace0\uc815\ud558\uace0 8\uac1c\uc758 \uc0c8\ub85c\uc6b4 \ub808\uc774\uc5b4\ub97c \ucd94\uac00\ud558\ub294 \uc2a4\ud0dd \ud655\uc7a5 \ubc29\ubc95, \uadf8\ub9ac\uace0 (d)\ub294 \ubcf4\uac04\uae30\ub97c \uc0ac\uc6a9\ud558\uc5ec 8\uac1c\uc758 \uc0c8\ub85c\uc6b4 \ub808\uc774\uc5b4\ub97c \uc5f0\uacb0\ud558\ub294 Concat-Lerp \ud655\uc7a5 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubc29\ubc95\uc740 MBPP-Plus \ubc0f MMLU \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \ucd5c\uc801\uc758 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uac01 \uadf8\ub798\ud504\ub294 \ud6c8\ub828 \ub2e8\uacc4\ubcc4 MBPP-Plus(\uc0c8\ub85c\uc6b4 \ucf54\ub529 \uae30\uc220 \uc2b5\ub4dd \ub2a5\ub825)\uc640 MMLU(\uae30\uc874 \ub2a5\ub825 \uc720\uc9c0 \ub2a5\ub825) \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574  \uc0c8\ub85c\uc6b4 \uc9c0\uc2dd\uc744 \ud559\uc2b5\ud558\uba74\uc11c \uae30\uc874 \ub2a5\ub825\uc744 \uc5bc\ub9c8\ub098 \uc798 \uc720\uc9c0\ud558\ub294\uc9c0 \uc2dc\uac01\uc801\uc73c\ub85c \ube44\uad50\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/ControlLLM_CF_Plot_Code.png", "caption": "(a) Pre-trained Model - Math Hard 0.237 - MMLU 0.724", "description": "\uadf8\ub9bc\uc740 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378(Pre-trained Model)\uacfc \ucee8\ud2b8\ub864 LLM\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 Math Hard \uc810\uc218\uac00 0.237, MMLU \uc810\uc218\uac00 0.724\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub294 \ucee8\ud2b8\ub864 LLM\uc774 \uae30\uc874 \uc9c0\uc2dd\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uc0c8\ub85c\uc6b4 \uc9c0\uc2dd\uc744 \ud559\uc2b5\ud558\ub294 \ub370 \uc5bc\ub9c8\ub098 \ud6a8\uacfc\uc801\uc778\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uae30\uc900\uc810 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.  Math Hard\ub294 \uc0c8\ub85c\uc6b4 \uc218\ud559 \ubb38\uc81c \ud574\uacb0 \ub2a5\ub825\uc744, MMLU\ub294 \ub2e4\uc591\ud55c \uc9c0\uc2dd \uc601\uc5ed\uc5d0 \ub300\ud55c \uc77c\ubc18\uc801\uc778 \uc5b8\uc5b4 \uc774\ud574 \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uc989, \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc740 MMLU \uc810\uc218\uac00 \ub192\uc9c0\ub9cc \uc0c8\ub85c\uc6b4 \uc218\ud559 \ubb38\uc81c(Math Hard)\uc5d0\ub294 \uc57d\ud558\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774\ud6c4 \uadf8\ub9bc\uc758 \ub2e4\ub978 \ubd80\ubd84\uc5d0\uc11c\ub294 \ucee8\ud2b8\ub864 LLM\uc758 \ub2e4\uc591\ud55c \uad6c\uc131\uacfc \uadf8\uc5d0 \ub530\ub978 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "B. Hidden State Alignment"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_pretrain.png", "caption": "(b) Lerp with MSE - Math Hard 0.360 - MMLU 0.716", "description": "\uadf8\ub9bc (b)\ub294 Control LLM\uc758 \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c Lerp(Linear Interpolation) \ubcf4\uac04\ubc95\uacfc MSE(Mean Squared Error) \uc190\uc2e4 \ud568\uc218\ub97c \uc0ac\uc6a9\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'Math Hard' \uc791\uc5c5\uc5d0\uc11c\ub294 0.360\uc758 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud558\uc600\uace0, 'MMLU' \uc791\uc5c5\uc5d0\uc11c\ub294 0.716\uc758 \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc0c8\ub85c\uc6b4 \uc9c0\uc2dd\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud1b5\ud569\ud558\uba74\uc11c \uae30\uc874\uc758 \uc131\ub2a5\uc744 \uc798 \uc720\uc9c0\ud558\ub294 Control LLM\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.  'Math Hard'\ub294 \uc218\ud559\uc801 \ucd94\ub860 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \uc9c0\ud45c\uc774\uace0, 'MMLU'\ub294 \ub2e4\uc591\ud55c \uc9c0\uc2dd \uc601\uc5ed\uc5d0 \ub300\ud55c \uc77c\ubc18\uc801\uc778 \uc5b8\uc5b4 \uc774\ud574 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \uc9c0\ud45c\uc785\ub2c8\ub2e4.", "section": "B. Hidden State Alignment"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_best.png", "caption": "(c) Dlerp without Divergence Loss - Math Hard 0.357 - MMLU 0.66", "description": "\uadf8\ub9bc\uc740 OpenMath2\uc640 Llama-3.1-8B-Instruct \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \uc815\ub82c \uc804\ub7b5\uc758 \uc601\ud5a5\uc744 \uc2dc\uac01\ud654\ud55c \uac83\uc785\ub2c8\ub2e4.  (c)\ub294 Dlerp(Dynamic Linear Interpolation) \uae30\ubc95\uc744 \uc0ac\uc6a9\ud588\uc9c0\ub9cc, \ubc1c\uc0b0 \uc190\uc2e4(Divergence Loss) \uc5c6\uc774 \ud559\uc2b5\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Math Hard \uc791\uc5c5\uc5d0\uc11c\ub294 35.7%\uc758 \uc815\ud655\ub3c4\ub97c, MMLU(Massive Multitask Language Understanding) \uc791\uc5c5\uc5d0\uc11c\ub294 66%\uc758 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.  \ubc1c\uc0b0 \uc190\uc2e4\uc774 \uc5c6\uc5b4 \uae30\uc874 \uc9c0\uc2dd\uc758 \ubcf4\uc874\uc5d0 \uc5b4\ub824\uc6c0\uc744 \uacaa\uc5c8\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "B. Hidden State Alignment"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_good.png", "caption": "(d) Lerp with Cosine Alignment - Math Hard 0.362 - MMLU 0.54", "description": "\uadf8\ub9bc (d)\ub294 \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4\ub97c \uc0ac\uc6a9\ud55c \uc120\ud615 \ubcf4\uac04(Lerp) \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc218\uce58\uc801\uc73c\ub85c\ub294 Math Hard \ubb38\uc81c \uc815\ud655\ub3c4\uac00 0.362, MMLU \uc815\ud655\ub3c4\uac00 0.54\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4.  \uc774 \ubc29\ubc95\uc740 \uc774\uc804\uc5d0 \ud559\uc2b5\ub41c \uc9c0\uc2dd\uacfc \uc0c8\ub85c \ud559\uc2b5\ud558\ub294 \uc9c0\uc2dd \uac04\uc758 \uc870\ud654\ub97c \ucc3e\ub294 \ub370 \ucd08\uc810\uc744 \ub9de\ucd94\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \ubcf8 \uadf8\ub9bc\uc740  Control LLM\uc758 \ub2e4\uc591\ud55c \ubc29\ubc95\ub860\ub4e4\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ubd80\ubd84\uc5d0\uc11c \uc81c\uc2dc\ub41c \uac83\uc73c\ub85c, \ud2b9\ud788  hidden-state alignment \uc804\ub7b5\uc758 \ud6a8\uacfc\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \ub0ae\uc740 MMLU \uc815\ud655\ub3c4\ub294 \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4 \uae30\ubc18\uc758 \uc815\ub82c \uc804\ub7b5\uc774 catastrophic forgetting \ubb38\uc81c\ub97c \uc644\ud654\ud558\ub294 \ub370 \ubd80\uc871\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "B. Hidden State Alignment"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_cosine_loss_euclidean.png", "caption": "(e) Lerp without Divergence Loss - Math Hard 0.359 - MMLU 0.41", "description": "\uadf8\ub9bc\uc740 OpenMath2 + Llama-3.1-8B-Instruct \ubaa8\ub378\uc5d0\uc11c \ub2e4\uc591\ud55c \uc815\ub82c \uc804\ub7b5\uc758 \uc601\ud5a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4.  (e)\ub294 \ubc1c\uc0b0 \uc190\uc2e4 \uc5c6\uc774 \uc120\ud615 \ubcf4\uac04(Lerp) \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Math Hard \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub294 0.359, MMLU \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub294 0.41\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4.  \uc774\ub294 \ubc1c\uc0b0 \uc190\uc2e4\uc744 \uc801\uc6a9\ud558\uc9c0 \uc54a\uc544 \uae30\uc874 \uc9c0\uc2dd(MMLU)\uc774 \uc0c1\ub2f9\ud788 \uc190\uc0c1\ub418\uc5c8\uc74c\uc744 \uc758\ubbf8\ud558\uba70, \uc0c8\ub85c\uc6b4 \uc791\uc5c5(Math Hard) \ud559\uc2b5\uc5d0\ub9cc \uc9d1\uc911\ub41c \uacb0\uacfc\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc120\ud615 \ubcf4\uac04 \uc790\uccb4\ub294 \ud6a8\uacfc\uc801\uc774\ub098 \ubc1c\uc0b0 \uc190\uc2e4\uc758 \ubd80\uc7ac\ub85c \uc778\ud574 catastrophic forgetting \ud604\uc0c1\uc774 \ubc1c\uc0dd\ud55c \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "B. Hidden State Alignment"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_worse.png", "caption": "(f) Full Parameter Training - Math Hard 0.368 - MMLU 0.07", "description": "\uadf8\ub9bc\uc740 OpenMath2 + Llama-3.1-8B-Instruct \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc218\ud589\ud55c \ubbf8\uc138 \uc870\uc815 \uc2e4\ud5d8\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  (f)\ub294 \uc804\uccb4 \ub9e4\uac1c\ubcc0\uc218 \ubbf8\uc138 \uc870\uc815(Full Parameter Training) \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  Math Hard \uc791\uc5c5\uc5d0\uc11c\ub294 0.368\uc758 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud588\uc9c0\ub9cc, MMLU \uc791\uc5c5\uc5d0\uc11c\ub294 0.07\uc758 \uc815\ud655\ub3c4\ub85c \uc131\ub2a5\uc774 \ub9e4\uc6b0 \uc800\ud558\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc0c8\ub85c\uc6b4 \uc791\uc5c5 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uae30\uc874 \uc9c0\uc2dd\uc774 \uc2ec\uac01\ud558\uac8c \uc190\uc2e4\ub418\ub294 \ud30c\uad6d\uc801 \ub9dd\uac01(Catastrophic Forgetting) \ud604\uc0c1\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.  \ubcf8 \uadf8\ub9bc\uc740 \ud30c\uad6d\uc801 \ub9dd\uac01 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud55c Control LLM\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud55c \ube44\uad50 \ub300\uc870\uad70\uc73c\ub85c \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "B. Hidden State Alignment"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_worst.png", "caption": "Figure 8: [OpenMath2 + Llama-3.1-8B-Instruct] Comparison of various alignment strategies and their impact on model performance.\n(a) Pre-trained Model: Hidden states from [Pretrained] and [Expanded] blocks are identical for all layers. 1) Semantic Stability: Analogous sentences exhibit high similarity in both cosine and Euclidean distances. 2) Layer-wise Clustering: Distinct clusters are formed for each layer.\n(b) Control LLM (MSE Divergence Loss): Maintains both semantic stability and layer-wise clustering, exhibiting strong catastrophic forgetting (CF) mitigation.\n(c) Dlerp without Divergence Loss: Shows distances approximately 2x larger than (b), reducing alignment quality.\n(d) Lerp with Cosine Divergence Loss: Preserves layer-wise clustering but weakens semantic stability. Nearest neighbors for [Pretrained] drift in Euclidean distance, while cosine similarity is maintained.\n(e) Lerp without Divergence Loss: Semantic stability degrades further between [Pretrained] and [Expanded] blocks, particularly in cosine similarity.\n(f) Full Parameter Tuning: Semantic drift is observed between [Expanded] and [Pretrained], with no interpolation applied.", "description": "\uadf8\ub9bc 8\uc740 OpenMath2 \ub370\uc774\ud130\uc14b\uc73c\ub85c Llama-3.1-8B-Instruct \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud558\ub294 \ub2e4\uc591\ud55c \uc815\ub82c \uc804\ub7b5\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  (a)\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\ub85c, [\uc0ac\uc804 \ud6c8\ub828] \ubc0f [\ud655\uc7a5] \ube14\ub85d\uc758 \uc740\ub2c9 \uc0c1\ud0dc\uac00 \ubaa8\ub4e0 \uacc4\uce35\uc5d0\uc11c \ub3d9\uc77c\ud569\ub2c8\ub2e4.  \ub192\uc740 \uc758\ubbf8\uc801 \uc548\uc815\uc131\uacfc \uacc4\uce35\ubcc4 \ud074\ub7ec\uc2a4\ud130\ub9c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (b) Control LLM (MSE \ubc1c\uc0b0 \uc190\uc2e4)\uc740 \uc758\ubbf8\uc801 \uc548\uc815\uc131\uacfc \uacc4\uce35\ubcc4 \ud074\ub7ec\uc2a4\ud130\ub9c1\uc744 \uc720\uc9c0\ud558\uba70, \uac15\ub825\ud55c \uae09\uaca9\ud55c \ub9dd\uac01(CF) \uc644\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (c) \ubc1c\uc0b0 \uc190\uc2e4 \uc5c6\ub294 Dlerp\ub294 (b)\ubcf4\ub2e4 \uac70\ub9ac\uac00 \uc57d 2\ubc30 \ub354 \ud07c\uc744 \ubcf4\uc5ec\uc8fc\uba70, \uc815\ub82c \ud488\uc9c8\uc774 \uc800\ud558\ub428\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. (d) \ucf54\uc0ac\uc778 \ubc1c\uc0b0 \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud558\ub294 Lerp\ub294 \uacc4\uce35\ubcc4 \ud074\ub7ec\uc2a4\ud130\ub9c1\uc744 \uc720\uc9c0\ud558\uc9c0\ub9cc \uc758\ubbf8\uc801 \uc548\uc815\uc131\uc774 \uc57d\ud654\ub429\ub2c8\ub2e4.  (e) \ubc1c\uc0b0 \uc190\uc2e4\uc774 \uc5c6\ub294 Lerp\ub294 \uc758\ubbf8\uc801 \uc548\uc815\uc131\uc774 \ub354\uc6b1 \uc800\ud558\ub429\ub2c8\ub2e4. (f) \uc804\uccb4 \ub9e4\uac1c\ubcc0\uc218 \ubbf8\uc138 \uc870\uc815\uc740 \ubcf4\uac04\uc774 \uc5c6\uc774 [\ud655\uc7a5]\uacfc [\uc0ac\uc804 \ud6c8\ub828] \uac04\uc5d0 \uc758\ubbf8\uc801 \ucc28\uc774\uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/ControlLLM_CF_Plot_Ablation_Study_Math.png", "caption": "Figure 9: [OpenMath2 + Llama-3.1-8B-Instruct] Comparison of benchmark results of different ablation study settings every 1K steps during training. (a) Lerp Interporation Strategy with MSE loss. (b) Lerp Interporation Strategy without Divergence Loss. (c) Dlerp Interporation Strategy with MSE loss. (d) Dlerp Interporation Strategy without Divergence Loss. (e) Hybrid Expansion Strategy. (e) Plerp Interporation Strategy. (f) DlerpIn Interporation Strategy. (e) MoE gating.", "description": "\uadf8\ub9bc 9\ub294 OpenMath2 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec Llama-3.1-8B-Instruct \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud558\ub294 \ub3d9\uc548 \ub2e4\uc591\ud55c \uc808\uc0ad \uc5f0\uad6c \uc124\uc815\uc758 \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ub9e4 1K \ub2e8\uacc4\ub9c8\ub2e4 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4.  \uac01 \ud558\uc704 \uadf8\ub9bc\uc740 \ub2e4\ub978 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a) MSE \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud558\ub294 Lerp \ubcf4\uac04 \uc804\ub7b5, (b) \ubc1c\uc0b0 \uc190\uc2e4 \uc5c6\uc774 Lerp \ubcf4\uac04 \uc804\ub7b5, (c) MSE \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud558\ub294 Dlerp \ubcf4\uac04 \uc804\ub7b5, (d) \ubc1c\uc0b0 \uc190\uc2e4 \uc5c6\uc774 Dlerp \ubcf4\uac04 \uc804\ub7b5, (e) \ud558\uc774\ube0c\ub9ac\ub4dc \ud655\uc7a5 \uc804\ub7b5, (f) Plerp \ubcf4\uac04 \uc804\ub7b5, (g) DlerpIn \ubcf4\uac04 \uc804\ub7b5, (h) MoE \uac8c\uc774\ud305\uc785\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \uc11c\ub85c \ub2e4\ub978 \uc124\uc815\uc774 \ubaa8\ub378\uc758 \uc131\ub2a5, \ud2b9\ud788 \uac11\uc791\uc2a4\ub7ec\uc6b4 \ub9dd\uac01(catastrophic forgetting) \ubc0f \uc0c8\ub85c\uc6b4 \uc791\uc5c5 \ud559\uc2b5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.10979/extracted/6135550/ControlLLM_CF_Plot_Ablation_Study_Code.png", "caption": "Figure 10: [OpenCoder + Llama-3.1-8B-Instruct] Comparison of benchmark results of different ablation study settings during training. (a) Lerp Interporation Strategy with MSE Loss. (b) Lerp Interporation Strategy with MSE Loss. Trained 3X longer than (a) (c) Hybrid Expansion Strategy. (d) MoE gating.", "description": "\uadf8\ub9bc 10\uc740 OpenCoder\uc640 Llama-3.1-8B-Instruct \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \uc81c\uac70 \uc5f0\uad6c \uc124\uc815\uc5d0\uc11c \ud6c8\ub828 \uc911 \uc131\ub2a5 \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ube44\uad50\ud55c \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. (a)\ub294 MSE \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud55c Lerp \ubcf4\uac04 \uc804\ub7b5, (b)\ub294 (a)\ubcf4\ub2e4 3\ubc30 \ub354 \uae34 \uc2dc\uac04 \ub3d9\uc548 \ud6c8\ub828\ub41c MSE \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud55c Lerp \ubcf4\uac04 \uc804\ub7b5, (c)\ub294 \ud558\uc774\ube0c\ub9ac\ub4dc \ud655\uc7a5 \uc804\ub7b5, (d)\ub294 MoE \uac8c\uc774\ud305\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uadf8\ub798\ud504\ub294 \ud6c8\ub828 \uc911 \ud2b9\uc815 \uc9c0\ud45c\uc758 \ubcc0\ud654\ub97c \uc2dc\uac04\uc5d0 \ub530\ub77c \ubcf4\uc5ec\uc8fc\uc5b4, \uac01 \uc124\uc815\uc774 \uc0c8\ub85c\uc6b4 \uc791\uc5c5 \ud559\uc2b5\uacfc \uae30\uc874 \uae30\ub2a5 \uc720\uc9c0\ub97c \uc5b4\ub5bb\uac8c \uc870\ud654\uc2dc\ud0a4\ub294\uc9c0 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}]