{"references": [{"fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-07", "reason": "This paper is a foundational work on using large language models for program synthesis, a crucial capability for code-related reasoning tasks."}, {"fullname_first_author": "Zhe Chen", "paper_title": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling", "publication_date": "2024-12-05", "reason": "This paper explores innovative methods for improving multimodal model performance through scaling techniques, relevant to the paper's focus on efficient and scalable AI systems."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-14", "reason": "This paper presents a novel approach to solving mathematical word problems using neural networks, directly relevant to the paper's emphasis on reasoning capabilities."}, {"fullname_first_author": "Yuyang Ding", "paper_title": "Unleashing reasoning capability of LLMs via scalable question synthesis from scratch", "publication_date": "2024-10-18", "reason": "This work focuses on improving the reasoning capabilities of LLMs by enhancing data quality and synthesis, a key aspect of the paper's methodology."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2020-09-03", "reason": "This paper introduces a comprehensive benchmark for evaluating multitask language understanding, providing a standard framework used in the paper's evaluation."}]}