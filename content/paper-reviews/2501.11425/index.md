---
title: "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training"
summary: "Agent-Rì€ ì–¸ì–´ ëª¨ë¸ ì—ì´ì „íŠ¸ì˜ ì‹¤ì‹œê°„ ìê¸° ë°˜ì„± ë° ì˜¤ë¥˜ ìˆ˜ì • ê¸°ëŠ¥ì„ í–¥ìƒì‹œì¼œ ë³µì¡í•œ ìƒí˜¸ ì‘ìš© í™˜ê²½ì—ì„œ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤."
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Natural Language Processing", "Large Language Models", "ğŸ¢ Fudan University",]
showSummary: true
date: 2025-01-20
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2501.11425 {{< /keyword >}}
{{< keyword icon="writer" >}} Siyu Yuan et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2025-01-22 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2501.11425" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2501.11425" target="_self" >}}
â†— Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/agent-r-training-language-model-agents-to" target="_self" >}}
â†— Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ê¸°ë°˜ ì—ì´ì „íŠ¸ëŠ” ìƒí˜¸ ì‘ìš© í™˜ê²½ì—ì„œ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë° ì ì  ì¤‘ìš”í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê¸°ì¡´ì˜ í–‰ë™ ë³µì œ ë°©ì‹ì€ ì‹¤ì œ í™˜ê²½ì—ì„œ ì˜¤ë¥˜ ë³µêµ¬ê°€ ì–´ë ¤ì›Œ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  íŠ¹íˆ, ë‹¨ê³„ë³„ ë¹„í‰ ë°ì´í„° ìˆ˜ì§‘ì˜ ì–´ë ¤ì›€ì€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ë”ìš± ì•…í™”ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤. 

Agent-Rì€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë°˜ë³µì ì¸ ìê¸° í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ë‹¨ìˆœíˆ ì •í™•ì„±ì— ë”°ë¼ í–‰ë™ì„ ë³´ìƒí•˜ê±°ë‚˜ ì²˜ë²Œí•˜ëŠ” ê¸°ì¡´ ë°©ì‹ê³¼ ë‹¬ë¦¬, MCTSë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•˜ëŠ” í›ˆë ¨ ìƒ˜í”Œì„ ìƒì„±í•©ë‹ˆë‹¤.  **ì—ì´ì „íŠ¸ëŠ” ì˜¤ë¥˜ ë°œìƒ ì‹œì ì— ë°”ë¡œ ìˆ˜ì •ì„ ì‹œë„**,  íš¨ìœ¨ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì„¸ ê°€ì§€ ëŒ€í‘œì ì¸ ìƒí˜¸ ì‘ìš© í™˜ê²½ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼, **Agent-Rì€ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ 5.59% í–¥ìƒ**ë˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.  **ì—ì´ì „íŠ¸ê°€ ì˜¤ë¥˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì‹ë³„í•˜ê³  ìˆ˜ì •í•˜ë©´ì„œ ë£¨í”„ë¥¼ ë°©ì§€**í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ëŠ” ê²°ê³¼ì…ë‹ˆë‹¤.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Agent-Rì€ MCTSë¥¼ í™œìš©í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ ì‹¤ì‹œê°„ ìê¸° ë°˜ì„± ë° ì˜¤ë¥˜ ìˆ˜ì • ê¸°ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} Agent-Rì€ ë°˜ë³µì ì¸ ìê¸° í•™ìŠµì„ í†µí•´ ì—ì´ì „íŠ¸ì˜ ì˜¤ë¥˜ ìˆ˜ì • ëŠ¥ë ¥ê³¼ ë°ì´í„°ì…‹ ìƒì„± ëŠ¥ë ¥ì„ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¨ë‹¤. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Agent-Rì€ ë‹¤ì–‘í•œ ìƒí˜¸ ì‘ìš© í™˜ê²½ì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì—ì´ì „íŠ¸ì˜ ì§€ëŠ¥ê³¼ ììœ¨ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
ë³¸ ë…¼ë¬¸ì€ **ìƒí˜¸ì‘ìš© í™˜ê²½ì—ì„œ ì–¸ì–´ ëª¨ë¸ ì—ì´ì „íŠ¸ì˜ ì˜¤ë¥˜ ìˆ˜ì • ëŠ¥ë ¥ í–¥ìƒ**ì´ë¼ëŠ” ì¤‘ìš”í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì ‘ê·¼ ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , **ìƒˆë¡œìš´ ìê¸° ê°œì„  íŒ¨ëŸ¬ë‹¤ì„**ì„ ì œì‹œí•˜ì—¬ ì—ì´ì „íŠ¸ì˜ **ì‹¤ì‹œê°„ ìê¸° ë°˜ì„± ë° ì˜¤ë¥˜ ìˆ˜ì • ëŠ¥ë ¥**ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ëŠ” **ììœ¨ì ì´ê³  ì§€ëŠ¥ì ì¸ ì—ì´ì „íŠ¸ ê°œë°œ**ì— ì¤‘ìš”í•œ í•¨ì˜ë¥¼ ê°€ì§€ë©°, ê´€ë ¨ ì—°êµ¬ ë¶„ì•¼ì˜ ë°œì „ì— í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.

------
#### Visual Insights



![](https://arxiv.org/html/2501.11425/x1.png)

> ğŸ”¼ ê·¸ë¦¼ 1ì€ ì—ì´ì „íŠ¸ê°€ ìƒì„±í•˜ëŠ” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ìˆ˜ì •ì— ì–´ë ¤ì›€ì„ ê²ªëŠ” ì–¸ì–´ ì—ì´ì „íŠ¸ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ì˜¤ë¥˜ëŠ” ì—ì´ì „íŠ¸ê°€ ë°˜ë³µì ì¸ ë™ì‘ì„ í•˜ê²Œ ë§Œë“¤ì–´ ì¥ê¸°ì ì¸ ê²½ë¡œ ë³µêµ¬ë¥¼ ë°©í•´í•˜ê³  ìµœì ì´ ì•„ë‹Œ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Agent-Rì€ ì—ì´ì „íŠ¸ê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ì˜¤ë¥˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê°ì§€í•˜ê³  í•´ê²°í•˜ì—¬ ì¥ê¸°ì ì¸ ì‘ì—…ì„ ì²˜ë¦¬í•˜ê³  ë°˜ë³µì ì¸ ë™ì‘ì„ í”¼í•˜ë©´ì„œ í–¥ìƒëœ ìê¸° ë°˜ì„± ëŠ¥ë ¥ì„ ê°–ë„ë¡ í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 1: Illustration of language agents struggling with error correction in trajectory generation. These errors can cause agents to enter loops, hindering recovery in long trajectories and resulting in suboptimal outcomes. Agent-R enables agents to effectively detect and address errors in real-time, handling long-horizon tasks and avoiding loops with greater self-reflection capabilities.
> </details>





{{< table-caption >}}
| Dataset | Simulation # | MCTS w/ Iter 1 Revision | MCTS w/ Iter 1 Good | MCTS w/ Iter 2 Revision | MCTS w/ Iter 2 Good | MCTS w/ Iter 3 Revision | MCTS w/ Iter 3 Good | Test Size |
|---|---|---|---|---|---|---|---|---|
| WebShop | 300 | 5500 | 600 | 8500 | 1800 | 9000 | 2000 | 200 |
| SciWorld | 200 | 8000 | 2500 | 4200 | 1800 | 5400 | 1200 | 200 |
| TextCraft | 200 | 6000 | 2000 | 7000 | 3200 | 8000 | 4000 | 100 |{{< /table-caption >}}

> ğŸ”¼ í‘œ 1ì€ ì„¸ ê°€ì§€ ì—ì´ì „íŠ¸ ìƒí˜¸ì‘ìš© í™˜ê²½ì˜ í†µê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ë°ì´í„°ì…‹, ì‹œë®¬ë ˆì´ì…˜ ìˆ˜, MCTSë¥¼ ì‚¬ìš©í•œ ë°˜ë³µ íšŸìˆ˜, ìˆ˜ì • ë°ì´í„°ì™€ ì •ë‹µ ë°ì´í„° ìˆ˜, í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸° ë“±ì˜ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.  ì„ íƒ ê¸°ì¤€ì— ë”°ë¼ MCTSëŠ” í•™ìŠµ ë‹¨ê³„ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ìˆ˜ì˜ ìˆ˜ì • ë°ì´í„°ì™€ ì •ë‹µ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ì¦‰, ê° í™˜ê²½ì— ë”°ë¼ ë‹¤ì–‘í•œ íšŸìˆ˜ì˜ ìˆ˜ì • ë°ì´í„°ì™€ ì„±ê³µì ì¸ ì‹œë„ ë°ì´í„°ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ í•™ìŠµì´ ì§„í–‰ë¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 1: Statistics of three agent interactive environments. Due to different selection thresholds, MCTS can collect different numbers of revision data and good data for the training phase.
> </details>





### In-depth insights


#### Agent-R Framework
Agent-R í”„ë ˆì„ì›Œí¬ëŠ” ëŒ€í™”í˜• í™˜ê²½ì—ì„œ ì–¸ì–´ ëª¨ë¸ ì—ì´ì „íŠ¸ì˜ **ìê¸° ë°˜ì„±ì  ì˜¤ë¥˜ ìˆ˜ì • ëŠ¥ë ¥**ì„ í–¥ìƒì‹œí‚¤ëŠ” í˜ì‹ ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. **MCTS(Monte Carlo Tree Search)**ë¥¼ í™œìš©í•˜ì—¬ ì—ì´ì „íŠ¸ê°€ ìì‹ ì˜ í–‰ë™ì— ëŒ€í•œ **ì¦‰ê°ì ì¸ í”¼ë“œë°±**ì„ ìƒì„±í•˜ê³ , ì˜ëª»ëœ ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ëŠ” **ìˆ˜ì • ê²½ë¡œ**ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŠ” ê¸°ì¡´ì˜ ì „ì ìœ¼ë¡œ ì •ë‹µë§Œì„ í•™ìŠµí•˜ëŠ” ë°©ì‹ê³¼ ë‹¬ë¦¬, **ì‹¤ì‹œê°„ ì˜¤ë¥˜ ê°ì§€ ë° ìˆ˜ì •**ì— ì¤‘ì ì„ ë‘ì–´ ë³´ë‹¤ ê°•ê±´í•˜ê³  íš¨ìœ¨ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.  **ë°˜ë³µì ì¸ ìê¸° í•™ìŠµ**ì„ í†µí•´ ì—ì´ì „íŠ¸ëŠ” ì˜¤ë¥˜ ìˆ˜ì • ëŠ¥ë ¥ê³¼ ë°ì´í„° ìƒì„± ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ë©°, ì¥ê¸°ì ì¸ ëŒ€í™”ì—ì„œë„ **ë£¨í”„ì— ë¹ ì§€ì§€ ì•Šê³  ìµœì ì˜ ê²½ë¡œ**ë¥¼ ì°¾ì•„ë‚˜ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  **ëª¨ë¸ ê¸°ë°˜ì˜ ìˆ˜ì • ë©”ì»¤ë‹ˆì¦˜**ì€ ì—ì´ì „íŠ¸ê°€ ìì‹ ì˜ í˜„ì¬ ëŠ¥ë ¥ ë‚´ì—ì„œ ì˜¤ë¥˜ë¥¼ ì‹ë³„í•˜ê³ , íš¨ìœ¨ì ì¸ ìˆ˜ì •ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.  Agent-Rì€ ë‹¤ì–‘í•œ ëŒ€í™”í˜• í™˜ê²½ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, **ì—ì´ì „íŠ¸ì˜ ìê¸° ë°˜ì„± ëŠ¥ë ¥**ì„ ê³ ë„í™”í•˜ëŠ” ì¤‘ìš”í•œ ë°œê±¸ìŒì…ë‹ˆë‹¤.

#### Iterative Self-Training
ë°˜ë³µì  ìê¸° í•™ìŠµì€ **ì—ì´ì „íŠ¸ê°€ ìŠ¤ìŠ¤ë¡œì˜ ì‹¤ìˆ˜ë¥¼ ì¸ì§€í•˜ê³  ìˆ˜ì •í•˜ëŠ” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” í•µì‹¬ ì „ëµ**ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì€ ê³¼ê±°ì˜ ì˜ëª»ëœ í–‰ë™ì„ ë°”íƒ•ìœ¼ë¡œ ë” ë‚˜ì€ ê²°ì •ì„ ë‚´ë¦¬ê³ , ì¥ê¸°ì ì¸ ìƒí˜¸ ì‘ìš© í™˜ê²½ì—ì„œ ë°˜ë³µë˜ëŠ” ì˜¤ë¥˜ë¥¼ í”¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. **ëª¬í…Œ ì¹´ë¥¼ë¡œ íŠ¸ë¦¬ íƒìƒ‰(MCTS)ê³¼ ê°™ì€ ì•Œê³ ë¦¬ì¦˜**ì€ ì—ì´ì „íŠ¸ê°€ ë‹¤ì–‘í•œ ê²½ë¡œë¥¼ íƒìƒ‰í•˜ê³  ìµœì ì˜ ê²½ë¡œë¥¼ ì°¾ëŠ” ë° ë„ì›€ì´ ë˜ì–´, **ìê¸° ë°˜ì„±ì„ ìœ„í•œ ê³ í’ˆì§ˆì˜ í•™ìŠµ ë°ì´í„° ìƒì„±**ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ **ëª¨ë¸ì˜ ìê¸° ìˆ˜ì • ëŠ¥ë ¥ê³¼ ë°ì´í„° ìƒì„± ëŠ¥ë ¥ì„ ë™ì‹œì— í–¥ìƒì‹œí‚¤ëŠ” ë°˜ë³µì  ê°œì„ **ì€ ì„±ëŠ¥ í–¥ìƒì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.  **ì‹¤ì‹œê°„ ì˜¤ë¥˜ ìˆ˜ì •ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì ì‹œ ìˆ˜ì •**ì€ ì—°ì‡„ì ì¸ ì‹¤íŒ¨ë¥¼ ë§‰ê³  ìµœì ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤.  **ë‹¤ì–‘í•œ ëŒ€í™”í˜• í™˜ê²½ì—ì„œì˜ ê´‘ë²”ìœ„í•œ ì‹¤í—˜**ì€ ì´ ì ‘ê·¼ ë°©ì‹ì˜ íš¨ê³¼ë¥¼ ì…ì¦í•˜ê³  ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

#### MCTS-Guided Reflection
**MCTS ê¸°ë°˜ ë°˜ì„± ê¸°ë²•ì€ ì—ì´ì „íŠ¸ê°€ ìƒí˜¸ì‘ìš© í™˜ê²½ ë‚´ì—ì„œ ìì‹ ì˜ í–‰ë™ì„ ë°˜ì¶”í•˜ê³  ìˆ˜ì •í•˜ëŠ” ê³¼ì •ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì•ˆë‚´í•˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤.**  **ëª¬í…Œ ì¹´ë¥¼ë¡œ íŠ¸ë¦¬ íƒìƒ‰(MCTS) ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•˜ì—¬ ì—ì´ì „íŠ¸ëŠ” ë‹¤ì–‘í•œ í–‰ë™ ê²½ë¡œë¥¼ íƒìƒ‰í•˜ê³ , ê·¸ ì¤‘ì—ì„œ ìµœì ì˜ ê²½ë¡œë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.** ì´ ê³¼ì •ì—ì„œ ì—ì´ì „íŠ¸ëŠ” **ì˜ëª»ëœ ê²½ë¡œë¥¼ ì‹ë³„í•˜ê³ , ì´ë¥¼ ìˆ˜ì •í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.** ì´ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ëŠ” ì¥ê¸°ì ì¸ ìƒí˜¸ì‘ìš©ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” **ëˆ„ì  ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê³ , ë³´ë‹¤ íš¨ìœ¨ì ì´ê³  ì„±ê³µì ì¸ ê²°ê³¼ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.** MCTS ê¸°ë°˜ ë°˜ì„±ì€ ë‹¨ìˆœíˆ ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ë§Œì„ í‰ê°€í•˜ëŠ” ê¸°ì¡´ì˜ ê°•í™” í•™ìŠµ ë°©ì‹ê³¼ ë‹¬ë¦¬, ì—ì´ì „íŠ¸ì˜ **ê° í–‰ë™ ë‹¨ê³„ì— ëŒ€í•œ ë°˜ì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•¨ìœ¼ë¡œì¨, ë³´ë‹¤ ì •êµí•˜ê³  ì§€ëŠ¥ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.**  **ì‹¤ì‹œê°„ìœ¼ë¡œ ì˜¤ë¥˜ ìˆ˜ì •ì„ ê°€ëŠ¥í•˜ê²Œ í•¨ìœ¼ë¡œì¨, ì—ì´ì „íŠ¸ëŠ” ì¥ê¸°ì ì¸ ìƒí˜¸ì‘ìš©ì—ë„ íš¨ê³¼ì ìœ¼ë¡œ ì ì‘í•˜ê³ , ìµœì ì˜ ì„±ê³¼ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.**  ì „ì²´ì ìœ¼ë¡œ, MCTS ê¸°ë°˜ ë°˜ì„±ì€ ì—ì´ì „íŠ¸ì˜ ìê¸° ê°œì„  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ë³µì¡í•œ ìƒí˜¸ì‘ìš© í™˜ê²½ì—ì„œì˜ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ ê°•í™”í•˜ëŠ” ë° í¬ê²Œ ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. íŠ¹íˆ, **ì‹¤ì‹œê°„ ì˜¤ë¥˜ ìˆ˜ì • ë° ì¥ê¸°ì ì¸ ìƒí˜¸ì‘ìš© ì ì‘ ëŠ¥ë ¥ í–¥ìƒ**ì´ë¼ëŠ” ì¸¡ë©´ì—ì„œ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ì§€ë‹Œë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Multi-task Learning
ë³¸ ë…¼ë¬¸ì—ì„œ ë‹¤ë£¨ëŠ” ë‹¤ì¤‘ ì‘ì—… í•™ìŠµ(Multi-task Learning)ì€ **ì—¬ëŸ¬ ì‘ì—…ì„ ë™ì‹œì— í•™ìŠµí•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•**ì…ë‹ˆë‹¤.  íŠ¹íˆ, ìƒí˜¸ì‘ìš©ì  í™˜ê²½ì—ì„œ ì—ì´ì „íŠ¸ì˜ ìê¸° ë°˜ì„± ëŠ¥ë ¥ í–¥ìƒì— ì´ˆì ì„ ë§ì¶˜ ë³¸ ì—°êµ¬ëŠ” ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì„ í†µí•´ ì—ì´ì „íŠ¸ê°€ **ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ì •í•˜ê³ , ë°˜ë³µì ì¸ ì‹¤ìˆ˜ë¥¼ í”¼í•˜ë©°, ì¥ê¸°ì ì¸ ìƒí˜¸ì‘ìš© ê³¼ì œë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜í–‰**í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.  **ê°œë³„ ì‘ì—…ì— ëŒ€í•œ ì „ë¬¸ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ë™ì‹œì— ë‹¤ì–‘í•œ ì‘ì—… ê°„ì˜ ì§€ì‹ ì „ì´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬** ëª¨ë¸ì˜ ê²¬ê³ ì„±ê³¼ ì ì‘ë ¥ì„ ë†’ì´ëŠ” íš¨ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  **ëª¨ë¸ì´ ì—¬ëŸ¬ ì‘ì—…ì—ì„œ ì¶•ì í•œ ê²½í—˜ê³¼ ì§€ì‹ì„ ê³µìœ í•¨ìœ¼ë¡œì¨ ê°œë³„ ì‘ì—…ì— ëŒ€í•œ í•™ìŠµ íš¨ìœ¨ì„ ë†’ì´ê³ , ìƒˆë¡œìš´ ì‘ì—…ì— ëŒ€í•œ ì ì‘ë ¥ì„ í–¥ìƒ**ì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ì ì´ í•µì‹¬ì…ë‹ˆë‹¤. ë‹¤ë§Œ, **ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì˜ ì„±ëŠ¥ì€ ì‘ì—… ê°„ì˜ ìƒê´€ê´€ê³„ì— í¬ê²Œ ì¢Œìš°ë˜ë¯€ë¡œ**,  ì‘ì—… ì„ ì • ë° ì„¤ê³„ì— ëŒ€í•œ ì‹ ì¤‘í•œ ê³ ë ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.  ë˜í•œ, **ê³¼ë„í•œ ì‘ì—…ì˜ ì¶”ê°€ëŠ” ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ë¥¼ ì•¼ê¸°í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ** ì ì ˆí•œ ì‘ì—… ìˆ˜ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

#### Error Correction
ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ ì—ëŸ¬ ìˆ˜ì • ê¸°ë²•ì€ **LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ì˜ ìê°€ ìˆ˜ì • ëŠ¥ë ¥ì„ í–¥ìƒ**ì‹œí‚¤ëŠ” ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ê°•í™” í•™ìŠµ ë°©ì‹ê³¼ ë‹¬ë¦¬, ì—ì´ì „íŠ¸ëŠ” ë‹¨ìˆœíˆ ë³´ìƒì´ë‚˜ ì²˜ë²Œì„ í†µí•´ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ì‹¤íŒ¨í•œ ì‹œë„ì—ì„œë¶€í„° ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ ìŠ¤ìŠ¤ë¡œ ì°¾ì•„ê°€ëŠ” ê³¼ì •**ì„ ê±°ì¹©ë‹ˆë‹¤. ì´ëŠ” MCTS(Monte Carlo Tree Search) ê¸°ë°˜ì˜ ëª¨ë¸ ê°€ì´ë“œí˜• ì—ëŸ¬ ìˆ˜ì • ê²½ë¡œ ìƒì„± ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ êµ¬í˜„ë©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ëŠ” ì‹¤íŒ¨í•œ ì‹œë„ì˜ ì²« ë²ˆì§¸ ì—ëŸ¬ ì§€ì ì„ íŒŒì•…í•˜ê³ , ì´ ì§€ì ë¶€í„°ëŠ” ì„±ê³µ ê²½ë¡œë¥¼ ë”°ë¼ê°€ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.  **ì‹¤ì‹œê°„ ì—ëŸ¬ ìˆ˜ì •**ì´ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆì–´, ì—°ì†ì ì¸ ì‹¤íŒ¨ë¥¼ ë°©ì§€í•˜ê³  ì‘ì—… ìˆ˜í–‰ íš¨ìœ¨ì„ ë†’ì…ë‹ˆë‹¤.  **ë°˜ë³µì ì¸ ìê°€ í•™ìŠµ** ê³¼ì •ì„ ê±°ì¹˜ë©´ì„œ ì—ì´ì „íŠ¸ì˜ ì—ëŸ¬ ìˆ˜ì • ëŠ¥ë ¥ê³¼ ë°ì´í„° ìƒì„± ëŠ¥ë ¥ì´ ë™ì‹œì— ê°œì„ ë˜ëŠ” ëª¨ìŠµì„ ë³´ì…ë‹ˆë‹¤.  **ë‹¤ì–‘í•œ ìƒí˜¸ ì‘ìš© í™˜ê²½**ì—ì„œ ì‹¤í—˜ì„ í†µí•´, ì œì‹œëœ ë°©ë²•ì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•˜ì˜€ê³ , ê¸°ì¡´ ë°©ë²•ë“¤ì— ë¹„í•´ ë›°ì–´ë‚œ ì„±ëŠ¥ í–¥ìƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. íŠ¹íˆ, ì¥ê¸°ì ì¸ ìƒí˜¸ ì‘ìš©ì´ í•„ìš”í•œ ì‘ì—…ì—ì„œ ë”ìš± íš¨ê³¼ì ì´ë©°, ì—ì´ì „íŠ¸ê°€ ë°˜ë³µì ì¸ ë£¨í”„ì— ë¹ ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ë°ì—ë„ ë„ì›€ì´ ë©ë‹ˆë‹¤.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2501.11425/x2.png)

> ğŸ”¼ ê·¸ë¦¼ 2ëŠ” Agent-Rì˜ í”„ë ˆì„ì›Œí¬ë¥¼ ë‘ ë‹¨ê³„ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. 1ë‹¨ê³„ì—ì„œëŠ” MCTSì™€ ëª¨ë¸ ê¸°ë°˜ì˜ ë°˜ì„± ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì •ëœ ê²½ë¡œ(revision trajectories)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 2ë‹¨ê³„ì—ì„œëŠ” ìˆ˜ì§‘ëœ ìˆ˜ì •ëœ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤. ì´ ë‘ ë‹¨ê³„ëŠ” ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  rsëŠ” ìˆ˜ì • ì‹ í˜¸(revision signal), tâ€²ëŠ” ì˜ëª»ëœ ê²½ë¡œì™€ ì˜¬ë°”ë¥¸ ê²½ë¡œ ì‚¬ì´ì˜ ì „í™˜ì (transition point), L(Î¸)ëŠ” ìµœì í™”í•´ì•¼ í•  ì†ì‹¤ í•¨ìˆ˜(loss function)ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  ìˆ˜ì •ëœ ê²½ë¡œëŠ” ì˜ëª»ëœ ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì—¬ ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì—ì„œ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.  ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ìˆ˜ì •ëœ ê²½ë¡œë¥¼ í†µí•´ ì—ì´ì „íŠ¸ì˜ ìê°€ í•™ìŠµ ë° ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì–´ë‚´ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 2: The framework of Agent-R consists of two phases. In Phase I, we adopt MCTS and a model-guided reflection mechanism to construct revision trajectories. In Phase II, the agents are trained using the collected revision trajectories. These two phases can be repeated iteratively. rs is the revision signal, tâ€²superscriptğ‘¡â€²t^{\prime}italic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT is the transition point between the bad and good trajectories, and Lâ¢(Î¸)ğ¿ğœƒL(\theta)italic_L ( italic_Î¸ ) is the loss function to be optimized.
> </details>



![](https://arxiv.org/html/2501.11425/x3.png)

> ğŸ”¼ ê·¸ë¦¼ 3ì€ ì„¸ ê°€ì§€ ìƒí˜¸ì‘ìš© í™˜ê²½(WebShop, SciWorld, TextCraft)ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ë°˜ë³µ íšŸìˆ˜ì— ë”°ë¥¸ ë‹¤ì–‘í•œ í›ˆë ¨ ê²½ë¡œì˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° í™˜ê²½ì— ëŒ€í•´ ì„¸ ê°€ì§€ í›ˆë ¨ ë°©ë²•(ê¸°ì¤€, ìµœì  ê²½ë¡œ, Agent-R)ì˜ í‰ê·  ìµœì¢… ì ìˆ˜ë¥¼ ë°˜ë³µ íšŸìˆ˜ë³„ë¡œ ë¹„êµ ë¶„ì„í•˜ì—¬ Agent-Rì˜ ì„±ëŠ¥ í–¥ìƒì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì„¸ ê°€ì§€ í™˜ê²½ ëª¨ë‘ì—ì„œ Agent-Rì„ ì‚¬ìš©í•œ í›ˆë ¨ì´ ë°˜ë³µ íšŸìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì„±ëŠ¥ì´ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” Agent-Rì´ ë°˜ë³µì ì¸ ìê¸° í•™ìŠµì„ í†µí•´ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° íš¨ê³¼ì ì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 3: Results of different training trajectories under different iterations on three interactive environments.
> </details>



![](https://arxiv.org/html/2501.11425/x4.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ ì„¸ ê°€ì§€ ìƒí˜¸ ì‘ìš© í™˜ê²½(WebShop, SciWorld, TextCraft)ì—ì„œ ë°˜ë³µë˜ëŠ” í–‰ë™ì˜ í‰ê·  íšŸìˆ˜ë¥¼ ë‹¤ì–‘í•œ í›ˆë ¨ ê²½ë¡œì™€ ë°˜ë³µ íšŸìˆ˜ì— ë”°ë¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ê° í™˜ê²½ì— ëŒ€í•´, ì´ˆê¸° ìƒíƒœ(Vanilla), ìµœì  ê²½ë¡œë§Œ ì‚¬ìš©í•œ í›ˆë ¨(w/ Optimal), Agent-Rì„ ì‚¬ìš©í•œ í›ˆë ¨(w/ Agent-R #Iter1, #Iter2, #Iter3)ì˜ ë„¤ ê°€ì§€ ê²½ìš°ì— ëŒ€í•œ ë°˜ë³µ í–‰ë™ ì‹œí€€ìŠ¤ì˜ í‰ê·  ê¸¸ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  Agent-R í›ˆë ¨ì˜ ë°˜ë³µ íšŸìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ë°˜ë³µë˜ëŠ” í–‰ë™ì˜ íšŸìˆ˜ê°€ ì¤„ì–´ë“œëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ì´ëŠ” Agent-Rì´ ì—ì´ì „íŠ¸ê°€ ë°˜ë³µì ì¸ í–‰ë™ì— ë¹ ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ë° íš¨ê³¼ì ì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 4: Average count of repeated action lengths for different training trajectories and different iterations in three interactive environments.
> </details>



![](https://arxiv.org/html/2501.11425/x5.png)

> ğŸ”¼ ê·¸ë¦¼ 5ëŠ” ì„¸ ê°€ì§€ ìƒí˜¸ ì‘ìš© í™˜ê²½(WebShop, SciWorld, TextCraft)ì—ì„œ Agent-Rì˜ ë°˜ë³µ í›ˆë ¨ íšŸìˆ˜ì— ë”°ë¥¸ í‰ê·  ìˆ˜ì • ê¸¸ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ìˆ˜ì • ê¸¸ì´ëŠ” ì˜ëª»ëœ ê²½ë¡œë¥¼ ì²˜ìŒ ì¸ì‹í•œ ì‹œì ë¶€í„° ì˜¬ë°”ë¥¸ ê²½ë¡œë¡œ ì „í™˜í•˜ê¸°ê¹Œì§€ ê±¸ë¦° í–‰ë™(action)ì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  ë°˜ë³µ í›ˆë ¨ì´ ì§„í–‰ë ìˆ˜ë¡ í‰ê·  ìˆ˜ì • ê¸¸ì´ê°€ ì§§ì•„ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” Agent-Rì´ ì˜ëª»ëœ í–‰ë™ì„ ë”ìš± ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì¸ì§€í•˜ê³  ìˆ˜ì •í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  ì´ëŠ” ì—ì´ì „íŠ¸ì˜ ìê¸° ë°˜ì„± ëŠ¥ë ¥ í–¥ìƒê³¼ ë” ë‚˜ì€ ê²½ë¡œ ê³„íš ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ì§€í‘œì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 5: Average revision length of different iterations on three interactive environments.
> </details>



![](https://arxiv.org/html/2501.11425/x6.png)

> ğŸ”¼ ê·¸ë¦¼ 6ì€ ì„¸ ê°€ì§€ ìƒí˜¸ì‘ìš© í™˜ê²½(WebShop, SciWorld, TextCraft)ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ í•™ìŠµ ë°©ë²•(ë‹¨ì¼ ì‘ì—… í•™ìŠµ, ì§ì ‘ ìˆ˜ì •, Agent-Rì˜ ë‹¤ì¤‘ ì‘ì—… í•™ìŠµ)ì˜ ì„±ëŠ¥ì„ ë¹„êµí•œ ê·¸ë˜í”„ì…ë‹ˆë‹¤. Agent-Rì˜ ë‹¤ì¤‘ ì‘ì—… í•™ìŠµ ë°©ë²•ì´ ë‹¤ë¥¸ ë°©ë²•ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ ë°˜ë³µ í•™ìŠµì„ ê±°ë“­í• ìˆ˜ë¡ ê·¸ ì°¨ì´ê°€ ë”ìš± ì»¤ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 6: Comparison of different training methods on three interactive environments.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Method | WebShop | SciWorld | TextCraft | Average |
|---|---|---|---|---|
| *Close-sourced Models* |  |  |  |  |
| DeepSeek-Chat [2024a] | 11.00 | 16.80 | 23.00 | 16.93 |
| Claude-3-Haiku [2024] | 5.50 | 0.83 | 0.00 | 2.11 |
| Claude-3-Sonnet [2024] | 1.50 | 2.78 | 38.00 | 14.09 |
| GPT-3.5-Turbo [2022] | 12.50 | 7.64 | 47.00 | 22.38 |
| GPT-4-Turbo [2023] | 15.50 | 14.38 | 77.00 | 35.63 |
| GPT-4o [2023] | 25.48 | 46.91 | 64.00 | 45.46 |
| *Open-sourced Models* |  |  |  |  |
| Llama2-Chat-13B [2023] | 1.00 | 0.83 | 0.00 | 0.61 |
| AgentLM-7B [2024a] | 36.50 | 2.75 | 0.00 | 13.08 |
| AgentLM-13B [2024a] | 39.50 | 10.68 | 4.00 | 18.06 |
| Agent-FLAN [2024d] | 40.35 | 28.64 | 16.00 | 28.33 |
| Llama-3.1-8B-Instruct [2024] | 19.65 | 14.36 | 26.00 | 20.00 |
| w/ ETO [2024b] | 52.80 | 67.55 | 75.00 | 65.12 |
| w/ Direct-Revision | 54.44 | 66.65 | 66.00 | 62.36 |
| 1-5
w/ Agent-R | **63.91** | **70.23** | **78.00** | **70.71** |{{< /table-caption >}}
> ğŸ”¼ í‘œ 2ëŠ” ì„¸ ê°€ì§€ ìƒí˜¸ì‘ìš© í™˜ê²½(WebShop, SciWorld, TextCraft)ì—ì„œ ì„¸ ê°€ì§€ ëª¨ë¸(Close-sourced, Open-sourced, Agent-R)ì˜ ì„±ëŠ¥ì„ ë¹„êµí•œ í‘œì…ë‹ˆë‹¤.  Agent-Rì€ Llama-3.1-8B ëª¨ë¸ì— Agent-Rì—ì„œ ìƒì„±ëœ ìˆ˜ì •ëœ ê²½ë¡œ(revision trajectories)ë¥¼ ì‚¬ìš©í•˜ì—¬ 3ë²ˆì˜ ë°˜ë³µ í•™ìŠµì„ ê±°ì¹œ ëª¨ë¸ì…ë‹ˆë‹¤.  í‘œì—ëŠ” ë‹¤ì–‘í•œ ê¸°ì¤€ ëª¨ë¸ë“¤ê³¼ ë¹„êµí•˜ì—¬ Agent-Rì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  Close-sourced ëª¨ë¸ì€ GPT-3.5-Turbo, GPT-4-Turbo, GPT-40, Claude 3, DeepSeek-Chat ë“±ì˜ ìƒìš© ëª¨ë¸ë“¤ì´ê³ , Open-sourced ëª¨ë¸ì€ Llama2-Chat, AgentLM, Agent-FLAN, Llama-3.1-8B-Instruct ë“±ì˜ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ë“¤ì…ë‹ˆë‹¤.  Agent-Rì€ ê¸°ì¤€ ëª¨ë¸ë“¤ì— ë¹„í•´ ìƒë‹¹íˆ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 2: Results of three interactive environments. We train Llama-3.1-8B on revision trajectories collected from Agent-R for three iterations and compare its performance with various models.
> </details>

{{< table-caption >}}
| Method | WebShop | SciWorld | TextCraft | Average |
|---|---|---|---|---|
| GPT-4o | 24.10 | 35.56 | 8.22 | 22.63 |
| AgentLM-7B | 20.96 | 0.00 | 0.00 | 6.99 |
| AgentLM-13B | 23.20 | 0.22 | 0.00 | 7.81 |
| Agent-FLAN | 19.37 | 0.00 | 0.00 | 6.46 |
| _Llama-3.1-8B_ |  |  |  |  |
| w/ Optimal | 31.38 | 1.15 | 0.00 | 10.84 |
| w/ Optimal+ | 25.03 | 0.00 | 0.00 | 8.34 |
| w/ ETO | 35.78 | 3.47 | 0.00 | 13.08 |
| _Llama-3.1-8B w/ Direct-Revision Trajectories_ |  |  |  |  |
| # Iter1 | 40.26 | 19.09 | 11.35 | 23.57 |
| # Iter2 | 40.35 | 27.45 | 22.97 | 30.26 |
| # Iter3 | 42.02 | 35.27 | 29.73 | 35.67 |
| _Llama-3.1-8B w/ Trajectories from Agent-R_ |  |  |  |  |
| # Iter1 | 46.51 | 40.82 | 32.43 | 39.92 |
| # Iter2 | 47.51 | 45.22 | 35.14 | 42.62 |
| # Iter3 | 48.22 | 48.79 | 43.24 | 46.75 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 3ì€ ë‹¤ì–‘í•œ ë°©ë²•ì˜ ìˆ˜ì • ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  Llama-3.1-8B ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ëœ ìµœì ì˜ ê¶¤ì ì´ GPT-40ì—ì„œ ìƒì„±ë˜ì—ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ëŠ” 'w/ Optimal+'ì€ ìµœì ì˜ ê¶¤ì ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  í‘œëŠ” ë‹¤ì–‘í•œ ë°©ë²•(ìµœì  ê¶¤ì ë§Œ ì‚¬ìš©, ìµœì  ë° ìš°ìˆ˜ ê¶¤ì  ê²°í•©, ì§ì ‘ ìˆ˜ì • ê¶¤ì  ë“±)ì„ ì‚¬ìš©í•˜ì—¬ Llama-3.1-8B ëª¨ë¸ì„ í•™ìŠµì‹œì¼°ì„ ë•Œì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ Agent-Rì˜ íš¨ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 3: The revision results of different methods. w/ Optimal+ means the golden trajectories for training Llama-3.1-8B are generated from GPT-4o.
> </details>

{{< table-caption >}}
| Action: Action | Observation: Observation |
|---|---| 
| Log: Task Description: `{task description}` |  {history log} |
| Current Action: `{node action}` | Current Observation: `{node observation}` |
|  | You need to verify whether the current action is good or bad or uncertain. |
|  | - A good action is one that is greatly helpful to solve the task. |
|  | - A bad action is one that is greatly harmful to solve the task. |
|  | - An uncertain action is one that is neither good nor bad. You cannot judge based on the current information. |
|  | You must give reasons first and then give the response with the format: Judgment: <Good or Bad or Uncertain> |{{< /table-caption >}}
> ğŸ”¼ í‘œ 4ëŠ” Agent-Rì—ì„œ ì „í™˜ì ì„ ê²°ì •í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  Agent-Rì€ ì—ì´ì „íŠ¸ê°€ ì˜ëª»ëœ ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì—¬ ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë•ëŠ” ë°˜ë³µì ì¸ ìê¸° í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ í”„ë¡¬í”„íŠ¸ëŠ” ì—ì´ì „íŠ¸ê°€ í˜„ì¬ ì‘ì—…ì´ ì„±ê³µì ì¸ì§€ ì‹¤íŒ¨í–ˆëŠ”ì§€ íŒë‹¨í•˜ê³ , ì‹¤íŒ¨í–ˆë‹¤ë©´ ì–´ë””ì„œë¶€í„° ì˜ëª»ë˜ì—ˆëŠ”ì§€ íŒŒì•…í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.  ì´ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ëŠ” ì˜ëª»ëœ ê²½ë¡œì—ì„œ ë‚˜ì•„ê°€ ì˜¬ë°”ë¥¸ ê²½ë¡œë¡œ ì´ì–´ì§€ëŠ” ìˆ˜ì • ê²½ë¡œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ëŠ” ì—ì´ì „íŠ¸ì˜ í–‰ë™, ê´€ì°°, ì‘ì—… ì„¤ëª…, ê·¸ë¦¬ê³  í˜„ì¬ ì‹œì ê¹Œì§€ì˜ ì´ë ¥ ë¡œê·¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 4: Prompt templates of determining transition point in Agent-R.
> </details>

{{< table-caption >}}
| Revision Thoughts |
|---|---| 
| Thought 1: I realize my approach was flawed. I need to revise it. |
| Thought 2: I took the wrong actions. I need to identify the right path. |
| Thought 3: My actions were incorrect. I must adjust my strategy. |
| Thought 4: I see an error in my actions. I need to fix it. |
| Thought 5: My judgment was incorrect. I need to rethink it. |
| Thought 6: I overlooked something important. I need to address it. |
| Thought 7: I recognize my mistake. Letâ€™s find a better solution. |
| Thought 8: I recognize my failure. I need to learn and move forward. |
| Thought 9: My decision was wrong. I should reevaluate. |
| Thought 10: I made an error. I must determine how to correct it. |{{< /table-caption >}}
> ğŸ”¼ í‘œ 5ëŠ” ì—ì´ì „íŠ¸ê°€ ì˜ëª»ëœ ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ê³  ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” 10ê°€ì§€ ìˆ˜ì • ì‚¬ê³  ë°©ì‹ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê°ê°ì˜ ìˆ˜ì • ì‚¬ê³ ëŠ” ì˜ëª»ëœ í–‰ë™ì„ ìˆ˜ì •í•˜ê³  ì‘ì—…ì„ ì™„ë£Œí•˜ê¸° ìœ„í•œ ì—ì´ì „íŠ¸ì˜ ì‚¬ê³  ê³¼ì •ì„ ë°˜ì˜í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ìˆ˜ì • ì‚¬ê³ ë“¤ì€ ì—ì´ì „íŠ¸ê°€ ìŠ¤ìŠ¤ë¡œ ë°˜ì„±í•˜ê³  ë³µêµ¬í•˜ëŠ” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ìê¸° í•™ìŠµ í”„ë ˆì„ì›Œí¬ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 5: Ten revision thoughts to construct revision trajectories.
> </details>

{{< table-caption >}}
| Dataset | Iteration | Method | WebShop | SciWorld | TextCraft | Average |
|---|---|---|---|---|---|---|
| Direct-Revision + Good Trajectory | 1 | Single | 48.06 | 64.52 | 57.00 | 56.53 |
|  |  | Multi | 42.66 | 63.10 | 48.00 | 51.25 |
| 2-7 | 2 | Single | 55.56 | 64.52 | 63.00 | 61.03 |
|  |  | Multi | 46.93 | 64.23 | 64.00 | 58.39 |
| 2-7 | 3 | Single | 58.82 | 65.51 | 71.00 | 65.11 |
|  |  | Multi | 54.44 | 66.65 | 66.00 | 62.36 |
| Trajectory from Agent-R | 1 | Single | 49.80 | 64.02 | 60.00 | 57.94 |
|  |  | Multi | 45.84 | 66.71 | 64.00 | 58.85 |
| 2-7 | 2 | Single | 56.34 | 65.62 | 71.00 | 64.32 |
|  |  | Multi | 52.05 | 68.28 | 76.00 | 65.44 |
| 2-7 | 3 | Single | 60.66 | 67.22 | 76.00 | 67.96 |
|  |  | Multi | 63.91 | 70.23 | 78.00 | 70.71 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 6ì€ ì„¸ ê°€ì§€ ìƒí˜¸ ì‘ìš© í™˜ê²½(WebShop, SciWorld, TextCraft)ì—ì„œ ë‹¤ì–‘í•œ í•™ìŠµ ë°©ë²•(ë‹¨ì¼ ì‘ì—… í•™ìŠµ, ë‹¤ì¤‘ ì‘ì—… í•™ìŠµ, ì§ì ‘ ìˆ˜ì • ë° ìš°ìˆ˜ ì‚¬ë¡€ ë³‘í•©)ì„ ì‚¬ìš©í–ˆì„ ë•Œì˜ ìƒì„¸ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° í™˜ê²½ì— ëŒ€í•´ ë°˜ë³µ íšŸìˆ˜(1, 2, 3)ë³„ë¡œ ì„±ê³µë¥ ì„ ë¹„êµí•˜ì—¬ Agent-Rì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ë°©ë²•ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.  í‘œëŠ” ê° í•™ìŠµ ë°©ë²•ê³¼ ë°˜ë³µ íšŸìˆ˜ì— ë”°ë¥¸ ì„¸ ê°€ì§€ í™˜ê²½ì—ì„œì˜ í‰ê·  ìµœì¢… ë³´ìƒ ë˜ëŠ” ì„±ê³µë¥ ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 6: Detailed Results of different training methods on three interactive environments.
> </details>

{{< table-caption >}}
| WebShop |  |  |
|---|---|---|
| **WebShop** |  |  |
| *Example 1:* |  |  |
| `Assistant`: Thought: I think I should search for light blue shorts in a specific size and price range to get more accurate results. Action: search[light blue 36w 30l price:40] |  |  |
| `Human`: [Search Result] |  |  |
| `Assistant`: Thought: I went in the wrong direction with my search. I need to reset and try again. |  |  |
| `Human`: ok. |  |  |
| `Assistant`: Thought: I need to back to search. Action: click[Back to Search] |  |  |
| `Human`: [Search Result] |  |  |
| *Example 2:* |  |  |
| `Assistant`: Thought: I think the second option is a bit off-topic, but itâ€™s a good price, so Iâ€™ll consider it. Action: click[B08226NDZW] |  |  |
| `Human`: [Search Result] |  |  |
| `Assistant`: Thought: I recognize my mistake. Letâ€™s find a better solution. Action: wait |  |  |
| `Human`: ok. |  |  |
| `Assistant`: Thought: I think the search query is not specific enough, so Iâ€™ll try to refine it by adding more keywords. Action: search[machine wash menâ€™s dress shirts polyester heathered cotton classic fit men 3t price:50.00] |  |  |
| `Human`: [Search Result] |  |  |
| **Sciworld** |  |  |
| *Example 1:* |  |  |
| `Assistant`: Thought: I should open door to kitchen. Action: open door to kitchen |  |  |
| `Human`: The door is now open. |  |  |
| `Assistant`: Thought: I took the wrong steps. I need to identify the right path. Action: wait |  |  |
| `Human`: ok. |  |  |
| `Assistant`: Thought: I should go to kitchen. Action: go to kitchen |  |  |
| `Human`: You move to the kitchen. |  |  |
| *Example 2:* |  |  |
| `Assistant`: Thought: I think I should try to determine the temperature of the unknown substance B. Action: use thermometer on unknown substance B |  |  |
| `Human`: the thermometer measures a temperature of 19 degrees celsius |  |  |
| `Assistant`: Thought: I misunderstood the situation. Time to reassess. Action: wait |  |  |
| `Human`: ok. |  |  |
| `Assistant`: Thought: I think I should check the unknown substance B again to make sure the temperature is accurate. Action: use thermometer on unknown substance B |  |  |
| `Human`: the thermometer measures a temperature of 16 degrees celsius |  |  |
| **TextCraft** |  |  |
| *Example 1:* |  |  |
| `Assistant`: Thought: I should check if I can get blackstone directly from the environment or the inventory. Action: inventory |  |  |
| `Human`: Inventory: You are not carrying anything. |  |  |
| `Assistant`: Thought: I recognize my mistake. Letâ€™s find a better solution. Action: wait |  |  |
| `Human`: ok. |  |  |
| `Assistant`: Thought: I should go get the blackstone from the environment. Action: get 3 blackstone |  |  |
| `Human`: Got 3 blackstone |  |  |
| *Example 2:* |  |  |
| `Assistant`: Thought: I can use the quartz slab to craft something else that might be useful. Action: craft quartz wall using 4 quartz slab |  |  |
| `Human`: Could not find enough items to craft minecraft:quartz_wall |  |  |
| `Assistant`: Thought: I realize I chose poorly. I must change direction. Action: wait |  |  |
| `Human`: ok. |  |  |
| `Assistant`: Thought: I think I should try to get more quartz to craft the quartz wall. Action: get 2 quartz slab |  |  |
| `Human`: Got 2 quartz slab |  |  |{{< /table-caption >}}
> ğŸ”¼ í‘œ 7ì€ Agent-Rì—ì„œ ìƒì„±ëœ ê¶¤ì ì„ ì‚¬ìš©í•˜ì—¬ ì„¸ ë²ˆì§¸ ë°˜ë³µì—ì„œ Llama-3.1-8Bì— ì˜í•´ ìƒì„±ëœ ê¶¤ì ì—ì„œ ì˜¤ë¥˜ ìˆ˜ì •ì˜ ì˜ˆë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. WebShopì˜ ê²€ìƒ‰ ê²°ê³¼ëŠ” ê¸¸ì´ê°€ ê¸¸ì–´ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤. ì´ í‘œëŠ” ì˜ëª»ëœ í–‰ë™ì„ ì‹ë³„í•˜ê³  ìˆ˜ì •í•˜ëŠ” Agent-Rì˜ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê° ì˜ˆì‹œëŠ” ì´ˆê¸° ì˜ëª»ëœ í–‰ë™, Agent-Rì˜ ë¶„ì„ ë° ìˆ˜ì •ëœ í–‰ë™ì„ ë³´ì—¬ì£¼ëŠ” ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì œì‹œë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Agent-Rì´ ì–´ë–»ê²Œ ê¸´ ì‹œí€€ìŠ¤ì˜ í–‰ë™ì—ì„œ ì˜¤ë¥˜ë¥¼ ê°ì§€í•˜ê³  ìˆ˜ì •í•˜ëŠ”ì§€ ë³´ë‹¤ ìì„¸íˆ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 7: Example of error correction from the trajectory generated by Llama-3.1-8B, trained on trajectories from Agent-R during the third iteration. The search results in WebShop are omitted due to their length.
> </details>

{{< table-caption >}}
| Platform | Judgment Category | Description |
|---|---|---|
| WebShop | Invalid Actions | The action is bad because it does not match the original search criteria. Therefore, Judgment: Bad |
|  | Mismatch Observation | However, the product does not match the description of a machine wash menâ€™s dress shirt with polyester heathers, heathers cotton, cotton heather, needle sleeve, classic fit, and color heather grey, as specified in the task description. Therefore, Judgment: Bad |
|  | Irrelevant Actions | The action is bad because it is not relevant to the task of finding menâ€™s shirts with specific characteristics. Therefore, Judgment: Bad |
| Sciworld | Invalid Actions | There is no orange mentioned in the environment, so this action is invalid. Itâ€™s not a helpful action, but rather an invalid input. Therefore, Judgment: Bad |
|  | Mismatch Observation | The current task is to determine if the aluminum foil is electrically conductive. The agent has already connected the aluminum foil to the battery and the green light bulb, and observed that the light bulb turned on. This suggests that the aluminum foil is electrically conductive. Moving the aluminum foil to the yellow box would mean that the agent is giving up on the possibility of the aluminum foil being conductive, which is not in line with the current observation. Therefore, Judgment: Bad |
|  | Irrelevant Actions | The agent is not using the art studioâ€™s resources effectively, as the cup containing red paint is not related to the task of creating green paint. Therefore, Judgment: Bad |
| TextCraft | Invalid Actions | The agent is stuck in an infinite loop of trying to execute an invalid action, which is not helpful in solving the task. Therefore, Judgment: Bad |
|  | Mismatch Observation | The action is not good because it is not possible to craft a dried kelp block with 9 dried kelp. The observation suggests that the agent lacks the necessary items to craft the block. This action is actually harmful to the task of crafting the block. Therefore, Judgment: Bad |
|  | Irrelevant Actions | The agent is trying to craft diamond boots, which requires 4 diamonds. The agent has already obtained 4 diamonds, which is sufficient to craft the boots. However, the agent is trying to craft a diamond block instead, which requires 9 diamonds. This action is not helpful in achieving the goal of crafting diamond boots, as it will waste the 4 diamonds the agent already has. Therefore, Judgment: Bad |{{< /table-caption >}}
> ğŸ”¼ í‘œ 8ì€ ì ì‘í˜• ì „í™˜ì  í‰ê°€ì˜ ì˜ˆì‹œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ íŒë‹¨ë“¤ì€ ëª¨ë‘ Agent-Rì—ì„œ ìƒì„±ëœ ê¶¤ì ì„ ì‚¬ìš©í•˜ì—¬ ì„¸ ë²ˆì§¸ ë°˜ë³µ ê³¼ì •ì—ì„œ í›ˆë ¨ëœ Llama-3.1-8Bì— ì˜í•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ í‘œëŠ” ì„¸ ê°€ì§€ ìœ í˜•ì˜ ì˜ëª»ëœ í–‰ë™(ë¬´íš¨ í–‰ë™, ë¶ˆì¼ì¹˜ ê´€ì¸¡, ë¬´ê´€ í–‰ë™)ì„ ë³´ì—¬ì£¼ëŠ” ì„¸ ê°€ì§€ ì˜ˆì‹œë¥¼ ì œì‹œí•˜ì—¬ Agent-Rì´ ìƒì„±í•œ í›ˆë ¨ ë°ì´í„°ê°€ ì—ì´ì „íŠ¸ì˜ ìê°€ ë°˜ì„± ëŠ¥ë ¥ í–¥ìƒì— ì–´ë–»ê²Œ ê¸°ì—¬í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 8: Example results from the evaluation of adaptive transition points. These judgments are all generated by Llama-3.1-8B, trained on trajectories from Agent-R during the third iteration.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}