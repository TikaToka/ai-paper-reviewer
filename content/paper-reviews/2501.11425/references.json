{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper introduces Llama 2, a foundational large language model used as a backbone model in the experiments, significantly impacting the results and comparative analysis."}, {"fullname_first_author": "Siyu Yuan", "paper_title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training", "publication_date": "2025-01-20", "reason": "This is the current paper, it is the main focus of the work, with all the other references being used to support or compare against this central work."}, {"fullname_first_author": "Zhiheng Xi", "paper_title": "AgentGym: Evolving large language model-based agents across diverse environments", "publication_date": "2024-06-04", "reason": "This paper introduces AgentGym, the experimental platform used, and directly influences the experimental design and the resulting data in the current work."}, {"fullname_first_author": "Shunyu Yao", "paper_title": "WebShop: Towards scalable real-world web interaction with grounded language agents", "publication_date": "2022-11-10", "reason": "This paper describes WebShop, one of the three interactive environments used in the experiments, significantly contributing to the scope and diversity of the experimental results."}, {"fullname_first_author": "Aohan Zeng", "paper_title": "AgentTuning: Enabling generalized agent abilities for LLMs", "publication_date": "2024-08-01", "reason": "This paper introduces AgentTuning, a training strategy adopted in the current work, affecting the training methodology and consequently the performance evaluation."}]}