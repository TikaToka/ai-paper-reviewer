{"references": [{"fullname_first_author": "Robin Staab", "paper_title": "Beyond memorization: Violating privacy via inference with large language models", "publication_date": "2024-00-00", "reason": "This paper directly addresses privacy concerns in LLMs, a central theme of the current paper, highlighting the importance of private inference."}, {"fullname_first_author": "Niloofar Mireshghallah", "paper_title": "Can LLMs keep a secret? testing privacy implications of language models via contextual integrity theory", "publication_date": "2024-00-00", "reason": "This paper also focuses on privacy issues in LLMs, providing a different perspective on the risks, and thus complements the findings of Staab et al.."}, {"fullname_first_author": "Aman Priyanshu", "paper_title": "Are chatbots ready for privacy-sensitive applications? an investigation into input regurgitation and prompt-induced sanitization", "publication_date": "2023-00-00", "reason": "This paper investigates specific privacy challenges in chatbot applications, offering insights relevant to the practical deployment of privacy-preserving LLMs."}, {"fullname_first_author": "Wen-jie Lu", "paper_title": "Bumblebee: Secure two-party inference framework for large transformers", "publication_date": "2025-00-00", "reason": "This paper presents a practical system for private inference of large transformers, which is directly related to the challenges and solutions explored in the current paper."}, {"fullname_first_author": "Xiaoyang Hou", "paper_title": "Ciphergpt: Secure two-party gpt inference", "publication_date": "2023-00-00", "reason": "This paper provides another concrete approach to private inference, offering a comparative perspective on different techniques for achieving this goal."}]}