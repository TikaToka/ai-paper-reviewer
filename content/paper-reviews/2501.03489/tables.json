[{"content": "Abbreviation|Architectural configuration\n---|---| \n<span class=\"ltx_text\" style=\"color:#0000FF;\">SM</span> + <span class=\"ltx_text\" style=\"color:#800080;\">LN</span> + <span class=\"ltx_text\" style=\"color:#FF0000;\">G</span>|\\mathbf{X}_{\\text{out}}=\\text{FFN}_{\\text{GELU}}(\\text{LayerNorm}_{2}(\\text{MHA}(\\text{Attn}_{\\text{Softmax}}(\\text{LayerNorm}_{1}(\\mathbf{X}_{\\text{in}})))))\n<span class=\"ltx_text\" style=\"color:#0000FF;\">SM</span> + <span class=\"ltx_text\" style=\"color:#800080;\">LN</span> + <span class=\"ltx_text\" style=\"color:#FF0000;\">R</span>|\\mathbf{X}_{\\text{out}}=\\text{FFN}_{\\text{ReLU}}(\\text{LayerNorm}_{2}(\\text{MHA}(\\text{Attn}_{\\text{Softmax}}(\\text{LayerNorm}_{1}(\\mathbf{X}_{\\text{in}})))))\n<span class=\"ltx_text\" style=\"color:#0000FF;\">SM</span> + <span class=\"ltx_text\" style=\"color:#800080;\">LN</span>|\\mathbf{X}_{\\text{out}}=\\text{FFN}_{\\text{Identity}}(\\text{LayerNorm}_{2}(\\text{MHA}(\\text{Attn}_{\\text{Softmax}}(\\text{LayerNorm}_{1}(\\mathbf{X}_{\\text{in}})))))\n<span class=\"ltx_text\" style=\"color:#0000FF;\">SM</span> + <span class=\"ltx_text\" style=\"color:#FF0000;\">G</span>|\\mathbf{X}_{\\text{out}}=\\text{FFN}_{\\text{GELU}}(\\text{MHA}(\\text{Attn}_{\\text{Softmax}}(\\mathbf{X}_{\\text{in}})))\n<span class=\"ltx_text\" style=\"color:#0000FF;\">SM</span> + <span class=\"ltx_text\" style=\"color:#FF0000;\">R</span>|\\mathbf{X}_{\\text{out}}=\\text{FFN}_{\\text{ReLU}}(\\text{MHA}(\\text{Attn}_{\\text{Softmax}}(\\mathbf{X}_{\\text{in}})))\n<span class=\"ltx_text\" style=\"color:#0000FF;\">SM</span>|\\mathbf{X}_{\\text{out}}=\\text{FFN}_{\\text{Identity}}(\\text{MHA}(\\text{Attn}_{\\text{Softmax}}(\\mathbf{X}_{\\text{in}})))", "caption": "Table 1: Architectural configurations of nonlinearities in LLMs, illustrating the combinations of Softmax (SM), LayerNorm (LN), GELU (G), and ReLU (R) functions (see Eq. 1, 2, 3 and 4).", "description": "\uc774 \ud45c\ub294 Transformer \uae30\ubc18 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc5d0\uc11c \ube44\uc120\ud615 \ud568\uc218\ub4e4\uc758 \ub2e4\uc591\ud55c \uc870\ud569\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Softmax(SM), Layer Normalization(LN), GELU, \uadf8\ub9ac\uace0 ReLU \ud568\uc218\ub4e4\uc758 \uc5ec\ub7ec \uac00\uc9c0 \uc870\ud569\uc774  \ubaa8\ub378\uc758 \uc544\ud0a4\ud14d\ucc98\uc5d0 \uc5b4\ub5bb\uac8c \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\ub4e4\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uac01 \uc904\uc740 \ub2e4\ub978 \uc544\ud0a4\ud14d\ucc98\ub97c \ub098\ud0c0\ub0b4\uba70,  'SM + LN + G' \uc640 \uac19\uc774 \uc57d\uc5b4\ub85c \ud45c\uc2dc\ub41c \ud568\uc218 \uc870\ud569\uc740 \ubcf8 \ub17c\ubb38\uc758 \uc218\uc2dd (1), (2), (3), (4) \uc5d0\uc11c \uc124\uba85\ud558\ub294 \uad6c\uccb4\uc801\uc778 \uc544\ud0a4\ud14d\ucc98\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294  '\ube44\uc120\ud615\uc131 \uac10\uc18c'  \uc804\ub7b5\uc744 \uc774\ud574\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "2 Preliminaries"}, {"content": "| Configurations | PPL | +\u0394(%) | \n|---|---|---| \n| SM + LN + G | 2.69 | 0.00 | \n| SM + LN + R | 2.76 | 2.53 | \n| SM + LN | 3.38 | 25.58 | \n| SM + G | 3.20 | 18.92 | \n| SM + R | 2.94 | 9.20 | \n| SM | NaNs | - |", "caption": "(a) Headwise entropy distribution", "description": "\ud45c 2(a)\ub294 \ub2e4\uc591\ud55c \ube44\uc120\ud615\uc131 \uad6c\uc131\uc744 \uc0ac\uc6a9\ud55c GPT-2 (\uc18c\ud615) \ubaa8\ub378\uc758 \ud5e4\ub4dc\ubcc4 \uc5d4\ud2b8\ub85c\ud53c \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc5d0 \ub300\ud574, \uc5d4\ud2b8\ub85c\ud53c \uac12\uc758 \ubc94\uc704 (0~max, max~3max \ub4f1)\uc5d0 \uc18d\ud558\ub294 \uc5b4\ud150\uc158 \ud5e4\ub4dc\uc758 \ube44\uc728\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ube44\uc120\ud615\uc131\uc774 \uc81c\uac70\ub41c \ubaa8\ub378\uc5d0\uc11c \uc5d4\ud2b8\ub85c\ud53c \uacfc\ubd80\ud558 \ub610\ub294 \ubd95\uad34 \ud604\uc0c1\uc774 \ubc1c\uc0dd\ud558\ub294\uc9c0 \ud655\uc778\ud558\uace0,  \ube44\uc120\ud615\uc131\uc758 \uc5ed\ud560\uacfc \ubaa8\ub378\uc758 \uc548\uc815\uc131 \ubc0f \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  max\ub294 \ubaa8\ub4e0 \ud5e4\ub4dc \uc911 \uad00\ucc30\ub41c \ucd5c\ub300 \uc5d4\ud2b8\ub85c\ud53c \uac12\uc785\ub2c8\ub2e4.", "section": "3 Information-Theoretic Analysis of Nonlinearity in LLMs"}, {"content": "| Baseline | Network Arch. | PPL | #Nonlinear Ops | #FLOPs (FFN) | #FLOPs (Attn.) | Comm. (GB) | Lat. (min.) | Savings (Comm.) | Savings (Lat.) |\n|---|---|---|---|---|---|---|---|---|---| \n| <img src=\"https://arxiv.org/html/2501.03489/S5.T2.12.4.4.5.1.1.1.1.1.png\" alt=\"Baseline\" width=36.0pt height=36pt style=\"vertical-align:-14.5pt; transform: rotate(-90deg)\"> | SM+LN+G | 2.69 | 14.5B | 7.7B | 25.32 | 8.21 | 1x | 1x |\n|  | SM: 144\u00d7\u211d<sup>128\u00d7128</sup> |  |  |  |  |  |  |  |  |\n|  | LN: 24\u00d7\u211d<sup>128\u00d7768</sup> |  |  |  |  |  |  |  |  |\n|  | G: 12\u00d7\u211d<sup>128\u00d73072</sup> |  |  |  |  |  |  |  |  |\n|  | SM+LN+R | 2.76 | 14.5B | 7.7B | 9.44 | 6.06 | 2.68x | 1.35x |\n|  | SM: 144\u00d7\u211d<sup>128\u00d7128</sup> |  |  |  |  |  |  |  |  |\n|  | LN: 24\u00d7\u211d<sup>128\u00d7768</sup> |  |  |  |  |  |  |  |  |\n|  | R: 12\u00d7\u211d<sup>128\u00d73072</sup> |  |  |  |  |  |  |  |  |\n|  | SM+ScFuFFN | 3.48 | 1.8B | 7.7B | 6.43 | 4.76 | 3.94x | 1.72x |\n| <span style=\"background-color:#CCFFCC;\">EReg(SM(t)+ScFuFFN)</span> | <span style=\"background-color:#CCFFCC;\">SM: 144\u00d7\u211d<sup>128\u00d7128</sup></span> | <span style=\"background-color:#CCFFCC;\">3.21</span> | <span style=\"background-color:#CCFFCC;\">1.8B</span> | <span style=\"background-color:#CCFFCC;\">7.7B</span> | <span style=\"background-color:#CCFFCC;\">6.43</span> | <span style=\"background-color:#CCFFCC;\">4.76</span> | <span style=\"background-color:#CCFFCC;\">3.94x</span> | <span style=\"background-color:#CCFFCC;\">1.72x</span> |", "caption": "(b) Loss curve", "description": "\ud45c (b)\ub294 CodeParrot \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uacfc\uc18c\uc120\ud615\ud654\ub41c GPT-2 (small) \ubaa8\ub378\uc758 \uc190\uc2e4 \uace1\uc120\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 \uc5d0\ud3ec\ud06c(epoch)\uc774\uace0 y\ucd95\uc740 \ud3c9\uac00 \uc190\uc2e4(eval loss)\uc785\ub2c8\ub2e4.  \uc5ec\ub7ec \uac00\uc9c0 \ube44\uc120\ud615\uc131 \uad6c\uc131(\uc608: Softmax + LayerNorm + GELU, Softmax + LayerNorm + ReLU \ub4f1)\uc5d0 \ub530\ub978 \uc190\uc2e4 \uac12\uc758 \ubcc0\ud654\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ub098\ud0c0\ub0b4\uc5b4, \uac01 \uad6c\uc131\uc758 \ud559\uc2b5 \uc548\uc815\uc131\uacfc \ud6a8\uc728\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ud65c\uc6a9\ub429\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ube44\uc120\ud615\uc131 \uad6c\uc131\uc744 \uc801\uc6a9\ud588\uc744 \ub54c \uc190\uc2e4\uc774 \uc5b4\ub5bb\uac8c \uac10\uc18c\ud558\ub294\uc9c0, \uc989 \ubaa8\ub378\uc774 \ub370\uc774\ud130\uc5d0 \uc5bc\ub9c8\ub098 \uc798 \uc801\uc751\ud558\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc9c0\ud45c\uc785\ub2c8\ub2e4. \ud2b9\ud788, \ube44\uc120\ud615\uc131\uc744 \uc904\uc600\uc744 \ub54c \uc190\uc2e4\uc774 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0\uc5d0 \ub300\ud55c \uc911\uc694\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3 Information-Theoretic Analysis of Nonlinearity in LLMs"}, {"content": "| Network Arch. | Eval PPL (1.2B) | Eval PPL (2.4B) | Eval PPL (4.8B) | #Nonlinear Ops | #FLOPs (FFN) | #FLOPs (Attn.) | Comm. (GB) | Lat. (min.) | \n|---|---|---|---|---|---|---|---|---|\n| Baseline <br> SM+LN+G | 25.71 | 23.32 | 21.29 | SM: 144\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u2075\u00b9\u00b2 <br> LN: 24\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u2077\u2076\u2078 <br> G: 12\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u00b3\u2070\u2077\u00b2 | 58.0B | 36.2B | 145.24 | 30.74 |\n| SM+LN+R | 26.06 | 23.55 | 21.58 | SM: 144\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u2075\u00b9\u00b2 <br> LN: 24\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u2077\u2076\u2078 <br> R: 12\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u00b3\u2070\u2077\u00b2 | 58.0B | 36.2B | 81.71 | 23.54 |\n| SM+ScFuFFN | 33.77 | 30.82 | 28.59 | SM: 144\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u2075\u00b9\u00b2 | 7.3B | 36.2B | 69.68 | 19.44 |\n| EReg(SM(t)+ScFuFFN) | 31.54 | 28.70 | 26.55 | SM: 144\u00d7\u211d\u2075\u00b9\u00b2\u02e3\u2075\u00b9\u00b2 | 7.3B | 36.2B | 69.68 | 19.44 |", "caption": "Table 2: Results on GPT-2 (L\ud835\udc3fLitalic_L=12, H\ud835\udc3bHitalic_H=12, d\ud835\udc51ditalic_d=768), trained from scratch on the CodeParrot dataset (2.1B tokens, T\ud835\udc47Titalic_T=128).", "description": "\ud45c 2\ub294 CodeParrot \ub370\uc774\ud130\uc14b(21\uc5b5 \ud1a0\ud070, \ubb38\ub9e5 \uae38\uc774 128)\uc5d0\uc11c \ucc98\uc74c\ubd80\ud130 \ud559\uc2b5\ub41c GPT-2 \ubaa8\ub378(\ub808\uc774\uc5b4 12\uac1c, \ud5e4\ub4dc 12\uac1c, \ucc28\uc6d0 768)\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98(SM+LN+G, SM+LN+R, SM+LN, SM+G, SM+R, SM) \ubcc4\ub85c perplexity, \ube44\uc120\ud615 \uc5f0\uc0b0 \uc218,  \ud1b5\uc2e0\ub7c9(GB), \uc9c0\uc5f0 \uc2dc\uac04(\ubd84) \ub4f1\uc744 \ube44\uad50\ud558\uc5ec  \uac01 \uc544\ud0a4\ud14d\ucc98\uc758 \ud6a8\uc728\uc131\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  \ud2b9\ud788,  \ube44\uc120\ud615 \uc5f0\uc0b0 \uac10\uc18c\ub97c \ud1b5\ud55c \ud6a8\uc728\uc131 \uac1c\uc120\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd94\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5 Experimental Results"}, {"content": "| Linear layers | Eval PPL(Weight Normalization) | Eval PPL(Spectral Normalization) |\n|---|---|---|\n| QK | 3.89 | 4.25 |\n| FFN | 3.64 | 3.63 |\n| QK+FFN | 3.88 | 4.23 |\n| QKV+FFN | 3.93 | 4.26 |\n| QKVO+FFN | 3.98 | 4.34 |", "caption": "Table 3: Results on GPT-2 (L\ud835\udc3fLitalic_L=12, H\ud835\udc3bHitalic_H=12, d\ud835\udc51ditalic_d=768) model, trained from scratch on Languini [20] (T\ud835\udc47Titalic_T=512)", "description": "\uc774 \ud45c\ub294 CodeParrot \ub370\uc774\ud130\uc14b\uc774 \uc544\ub2cc Languini \ub370\uc774\ud130\uc14b(20\uc5b5 \ud1a0\ud070, \ubb38\ub9e5 \uae38\uc774 512)\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucc98\uc74c\ubd80\ud130 \ud559\uc2b5\uc2dc\ud0a8 GPT-2 \ubaa8\ub378 (12 \ub808\uc774\uc5b4, 12 \ud5e4\ub4dc, \ucc28\uc6d0 768)\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ube44\uc120\ud615 \uc5f0\uc0b0(Softmax, LayerNorm, GELU, ReLU)\uc758 \uc870\ud569\uc744 \uac00\uc9c4 \uc5ec\ub7ec \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\uc758 \uc131\ub2a5\uc744 \ud37c\ud50c\ub809\uc11c\ud2f0(PPL), \ube44\uc120\ud615 \uc5f0\uc0b0 \uc218, FLOPs, \ud1b5\uc2e0\ub7c9(GB), \uc9c0\uc5f0 \uc2dc\uac04(\ubd84)\uc73c\ub85c \ube44\uad50\ud569\ub2c8\ub2e4. \ud2b9\ud788, \uc5d4\ud2b8\ub85c\ud53c \uaddc\uc81c \uae30\ubc95\uc744 \uc801\uc6a9\ud55c Softmax-only \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "5 Experimental Results"}, {"content": "| Weight Normalization | Spectral Normalization | Scaled-FFN |\n|---|---|---|\n| Eval PPL | 3.640 | 3.624 | 3.478 |", "caption": "Table 4: Comparison of weight normalization [17] and spectral normalization [18] when employed in Softmax-only GPT-2 (L\ud835\udc3fLitalic_L=12, H\ud835\udc3bHitalic_H=12, d\ud835\udc51ditalic_d=768) models, and trained from scratch on CodeParrot dataset with 128 input context length. FFN weight normalization yield the similar results; whereas, weight normalization works better in other linear layers.", "description": "\uc774 \ud45c\ub294 Softmax-only GPT-2 \ubaa8\ub378(12\uac1c \ub808\uc774\uc5b4, 12\uac1c \ud5e4\ub4dc, 768\ucc28\uc6d0)\uc5d0 \uac00\uc911\uce58 \uc815\uaddc\ud654\uc640 \uc2a4\ud399\ud2b8\ub7fc \uc815\uaddc\ud654\ub97c \uc801\uc6a9\ud588\uc744 \ub54c\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. CodeParrot \ub370\uc774\ud130\uc14b(128\uac1c \uc785\ub825 \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774)\uc5d0\uc11c \ud559\uc2b5\ub41c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. FFN(Feed-Forward Network) \ub808\uc774\uc5b4\uc5d0 \uac00\uc911\uce58 \uc815\uaddc\ud654\ub97c \uc801\uc6a9\ud588\uc744 \ub54c\uc640 \ub2e4\ub978 \uc120\ud615 \ub808\uc774\uc5b4\uc5d0 \uac00\uc911\uce58 \uc815\uaddc\ud654\ub97c \uc801\uc6a9\ud588\uc744 \ub54c\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\uc911\uce58 \uc815\uaddc\ud654\ub294 FFN \ub808\uc774\uc5b4\uc5d0\uc11c \ube44\uc2b7\ud55c \uacb0\uacfc\ub97c \ubcf4\uc600\uc9c0\ub9cc \ub2e4\ub978 \uc120\ud615 \ub808\uc774\uc5b4\uc5d0\uc11c\ub294 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4.", "section": "C \ucd94\uac00 \uacb0\uacfc"}, {"content": "|       | Network Arch. | PPL | #Nonlinear Ops | FFN #FLOPs | Attn. #FLOPs | Comm.(GB) | Lat.(min.) | Comm. Savings | Lat. Savings |\n| :----: | :---------------: | :-: | :-------------: | :----------: | :----------: | :--------: | :---------: | :-------------: | :-------------: |\n| Baseline | <math>{\numeric{SM+LN+G}}</math> | 2.35 | SM: <math>144 \\times \\mathbb{R}^{256 \\times 256}</math><br>LN: <math>24 \\times \\mathbb{R}^{256 \\times 768}</math><br>G: <math>12 \\times \\mathbb{R}^{256 \\times 3072}</math> | 29.0B | 16.3B | 58.51 | 16.57 | 1<math>\\times</math> | 1<math>\\times</math> |\n|       | <math>{\numeric{SM+LN+R}}</math> | 2.41 | SM: <math>144 \\times \\mathbb{R}^{256 \\times 256}</math><br>LN: <math>24 \\times \\mathbb{R}^{256 \\times 768}</math><br>R: <math>12 \\times \\mathbb{R}^{256 \\times 3072}</math> | 29.0B | 16.3B | 26.73 | 12.59 | 2.19<math>\\times</math> | 1.32<math>\\times</math> |\n|       | <math>{\numeric{SM+ScFuFFN}}</math> | 3.03 | SM: <math>144 \\times \\mathbb{R}^{256 \\times 256}</math> | 3.6B | 16.3B | 20.72 | 10.45 | 2.82<math>\\times</math> | 1.59<math>\\times</math> |\n|       | <math>\\text{EReg}({\\tt SM(t)+ScFuFFN})</math> | 2.92 | SM: <math>144 \\times \\mathbb{R}^{256 \\times 256}</math> | 3.6B | 16.3B | 20.72 | 10.45 | 2.82<math>\\times</math> | 1.59<math>\\times</math> |", "caption": "Table 5: Perplexity comparison of weight normalization, spectral normalization, and learnable scaling employed in FFN of softmax-only GPT-2 model, when trained from scratch on CodeParrot dataset with 128 input context length.", "description": "\uc774 \ud45c\ub294 CodeParrot \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ud559\uc2b5\ub41c Softmax-only GPT-2 \ubaa8\ub378\uc758 FFN(Feed-Forward Network) \ubd80\ubd84\uc5d0 \uc801\uc6a9\ub41c \uac00\uc911\uce58 \uc815\uaddc\ud654, \uc2a4\ud399\ud2b8\ub7fc \uc815\uaddc\ud654, \uadf8\ub9ac\uace0 \ud559\uc2b5 \uac00\ub2a5\ud55c \uc2a4\ucf00\uc77c\ub9c1 \uae30\ubc95\ub4e4\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc785\ub825 \ubb38\ub9e5 \uae38\uc774\uac00 128 \ud1a0\ud070\uc778 \uacbd\uc6b0\uc758 perplexity \uac12\uc744 \ube44\uad50\ud558\uc5ec \uac01 \uae30\ubc95\uc758 \ud6a8\uacfc\ub97c \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uc989, \ube44\uc120\ud615\uc131\uc744 \uc904\uc778 Transformer \ubaa8\ub378\uc5d0\uc11c \ub2e4\uc591\ud55c \uc815\uaddc\ud654 \uae30\ubc95\uc774 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.", "section": "C \ucd94\uac00 \uacb0\uacfc (C Additional Results)"}, {"content": "|---|---|---|---|---|---|---|---|\n| **Baseline** | Network Arch. | PPL | #Nonlinear Ops | FFN | Attn. | Comm. (GB) | Lat. (min.) | Savings |\n|---|:---|:---:|:---|:---:|:---:|:---:|:---:|:---:|\n| <img src=\"https://arxiv.org/html/2501.03489/A3.T7.12.4.4.5.1.1.1.1.1.png\" style=\"width:5.7pt;height:36pt;vertical-align:-14.5pt;transform:rotate(-90deg);\" /> | SM+LN+G | 2.56 | 21.7B | 11.6B | 37.17 | 10.77 | 1\u00d7 | 1\u00d7 |\n|  | SM: 216\u00d7\u211d<sup>128\u00d7128</sup> |  |  |  |  |  |  |  |\n|  | LN: 36\u00d7\u211d<sup>128\u00d7768</sup> |  |  |  |  |  |  |  |\n|  | G: 18\u00d7\u211d<sup>128\u00d73072</sup> |  |  |  |  |  |  |  |\n|  | SM+LN+R | 2.63 | 21.7B | 11.6B | 13.34 | 8.04 | 2.79\u00d7 | 1.34\u00d7 |\n|  | SM: 216\u00d7\u211d<sup>128\u00d7128</sup> |  |  |  |  |  |  |  |\n|  | LN: 36\u00d7\u211d<sup>128\u00d7768</sup> |  |  |  |  |  |  |  |\n|  | R: 18\u00d7\u211d<sup>128\u00d73072</sup> |  |  |  |  |  |  |  |\n|  | SM+ScFuFFN | 3.24 | 2.7B | 11.6B | 8.83 | 6.07 | 4.21\u00d7 | 1.77\u00d7 |\n|  | <span style=\"background-color:#CCFFCC;\">EReg(SM(t)+ScFuFFN)</span> | <span style=\"background-color:#CCFFCC;\">3.13</span> | <span style=\"background-color:#CCFFCC;\">2.7B</span> | <span style=\"background-color:#CCFFCC;\">11.6B</span> | <span style=\"background-color:#CCFFCC;\">8.83</span> | <span style=\"background-color:#CCFFCC;\">6.07</span> | <span style=\"background-color:#CCFFCC;\">4.21\u00d7</span> | <span style=\"background-color:#CCFFCC;\">1.77\u00d7</span> |\n", "caption": "Table 6: Results on GPT-2 (L\ud835\udc3fLitalic_L=12, H\ud835\udc3bHitalic_H=12, d\ud835\udc51ditalic_d=768), trained from scratch on the CodeParrot dataset (2.1B tokens, T\ud835\udc47Titalic_T=256).", "description": "\ud45c 6\uc740 CodeParrot \ub370\uc774\ud130\uc14b(21\uc5b5 \ud1a0\ud070, T=256)\uc5d0\uc11c \ucc98\uc74c\ubd80\ud130 \ud559\uc2b5\ub41c GPT-2 \ubaa8\ub378(L=12, H=12, d=768)\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ube44\uc120\ud615\uc131 \uad6c\uc131(Softmax, Layer Normalization, GELU, ReLU \uc0ac\uc6a9 \uc5ec\ubd80 \ub4f1)\uc5d0 \ub530\ub978 perplexity, \ube44\uc120\ud615 \uc5f0\uc0b0 \uc218, \ud1b5\uc2e0\ub7c9, \uc9c0\uc5f0 \uc2dc\uac04 \ub4f1\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uc81c\uc548\ub41c \uc5d4\ud2b8\ub85c\ud53c \uae30\ubc18 \ubc29\ubc95\uc758 \ud6a8\uc728\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.", "section": "5 Experimental Results"}]