[{"content": "| Model Name | $\n\\lambda_{i}\n$ | $\n\\mu_{i}\n$ | $\n\\sigma_{i}\n$ |\n|---|---|---|---|\n| Qwen/Qwen1.5-0.5B | 21.2340 | 1.18e-15 | 3.9044 |\n| Qwen/Qwen1.5-1.8B | 21.1198 | 1.00e-15 | 3.8795 |\n| google/gemma-2b | 20.7799 | 2.56e-14 | 4.8182 |\n| google/gemma-7b | 18.9374 | 9.78e-15 | 4.5854 |\n| Qwen/Qwen1.5-7B | 18.0948 | 1.41e-19 | 4.6136 |\n| openai/whisper-small | 294604.7393 | 90.9031 | 22.4477 |\n| meta-llama/Llama-2-7b | 17.2144 | 1.04e-17 | 8.8424 |\n| stabilityai/stable-diffusion-xl-base-1.0 | 16.9046 | 5.80e-11 | 7.8304 |\n| BAAI/EVA | 454253.6120 | 95.8721 | 23.0329 |\n| mistralai/Mistral-7B-Instruct-v0.2 | 16.1882 | 7.18e-15 | 7.7386 |\n| meta-llama/Llama-2-7b-hf | 15.3191 | 1.76e-14 | 4.9636 |\n| mistralai/Mistral-7B-v0.1 | 15.9177 | 1.03e-15 | 8.2057 |\n| meta-llama/Llama-2-7b-chat-hf | 15.2853 | 9.88e-12 | 5.5452 |\n| meta-llama/Llama-3.1-8B-Instruct | 0.5* | 2.0* | 0.5* |\n| meta-llama/Llama-3.1-8B | 0.5* | 2.0* | 0.5* |\n| allenai/DREAM | 24.2332 | 4.9102 | 9.2243 |\n| meta-llama/Meta-Llama-3-8B-Instruct | 15.9664 | 1.47e-10 | 10.6965 |\n| openai/whisper-tiny | 13.4653 | 2.04e-15 | 4.1449 |\n| microsoft/phi-2 | 15.2437 | 8.83e-18 | 9.5035 |\n| openai/whisper-large-v3 | 528070.6635 | 66.4680 | 15.8209 |\n| openai/whisper-medium | 460695.9213 | 88.9759 | 21.2067 |\n| Qwen/Qwen2-1.5B | 16.0543 | 4.44e-12 | 6.1988 |\n| meta-llama/Meta-Llama-3-8B | 15.2420 | 1.06e-10 | 11.5625 |\n| meta-llama/Llama-3.2-3B-Instruct | 0.5* | 2.0* | 0.5* |\n| meta-llama/Llama-3.2-1B-Instruct | 0.5* | 2.0* | 0.5* |\n| microsoft/Phi-3-mini-4k-instruct | 114364.7070 | 142.1125 | 37.0978 |\n| microsoft/speecht5_tts | 12.3327 | 6.40e-10 | 3.5563 |\n| openai/whisper-large-v2 | 68.7205 | 13.4940 | 10.0765 |\n| meta-llama/Llama-3.2-1B | 0.5* | 2.0* | 0.5* |\n| Qwen/Qwen2-1.5B-Instruct | 15.1078 | 1.70e-17 | 4.9109 |\n| apple/AIM | 120131.6996 | 66.9603 | 17.3784 |\n| Qwen/Qwen2-0.5B | 32058.6364 | 76.6518 | 21.8903 |\n| Qwen/Qwen2-7B-Instruct | 415361.3050 | 78.3713 | 18.9740 |\n| openai/whisper-base | 11.2185 | 6.13e-20 | 2.7420 |\n| google/gemma-2-2b | 0.5* | 2.0* | 0.5* |\n| meta-llama/Llama-3.2-3B | 0.5* | 2.0* | 0.5* |\n| mistralai/Mistral-7B-Instruct-v0.1 | 13.4460 | 7.33e-15 | 8.2182 |\n| google/gemma-2-2b-it | 0.5* | 2.0* | 0.5* |\n| facebook/opt-125m | 9.2155 | 1.68e-14 | 1.4702 |\n| Salesforce/BLIP | 11.6421 | 0.2335 | 2.7321 |\n| mistralai/Mistral-7B-Instruct-v0.3 | 14.0439 | 3.31e-09 | 7.2751 |\n| microsoft/resnet-50 | 9.0884 | 4.48e-21 | 1.6266 |\n| facebook/esm2_t12_35M_UR50D | 11.4140 | 6.74e-19 | 3.5063 |\n| google/flan-t5-base | 10.3708 | 1.28e-19 | 1.9899 |\n| google/flan-t5-large | 11.8440 | 8.27e-14 | 4.6042 |\n| openai/whisper-large | 364711.2741 | 64.2591 | 15.3622 |\n| microsoft/Phi-3.5-mini-instruct | 0.5* | 2.0* | 0.5* |\n| microsoft/phi-1.5 | 12.9090 | 6.94e-10 | 9.6670 |\n| google/gemma-2-9b-it | 280939.5667 | 102.4015 | 25.2924 |\n| Qwen/Qwen2.5-7B-Instruct | 0.5* | 2.0* | 0.5* |", "caption": "Table 1: Summary of model parameters (\u03bbisubscript\ud835\udf06\ud835\udc56\\lambda_{i}italic_\u03bb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, \u03bcisubscript\ud835\udf07\ud835\udc56\\mu_{i}italic_\u03bc start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, \u03c3isubscript\ud835\udf0e\ud835\udc56\\sigma_{i}italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) for different top 50 models with the largest number of fine-tuned models. Here, \u201c*\u201d indicates the framework equation 1 failed to fit the empirical data.", "description": "\ud45c 1\uc740 \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378 \uc218\uac00 \uac00\uc7a5 \ub9ce\uc740 \uc0c1\uc704 50\uac1c \ubaa8\ub378\uc5d0 \ub300\ud55c \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218 (\u03bbi, \u03bci, \u03c3i)\ub97c \uc694\uc57d\ud55c \uac83\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c '*'\ub294 \ud504\ub808\uc784\uc6cc\ud06c \ubc29\uc815\uc2dd 1\uc774 \uacbd\ud5d8\uc801 \ub370\uc774\ud130\uc5d0 \ub9de\uc9c0 \uc54a\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \ubaa8\ub378\uc758 \uc801\ud569\ub3c4(\u03bbi), \ucc44\ud0dd \uc18d\ub3c4(\u03bci), \uc601\ud5a5 \uc9c0\uc18d \uc2dc\uac04(\u03c3i)\uc744 \ubcf4\uc5ec\uc8fc\uc5b4 \ubaa8\ub378\uc758 \uc778\uae30\uc640 \uc601\ud5a5\ub825\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  \uac01 \ub9e4\uac1c\ubcc0\uc218\ub294 \ubaa8\ub378\uc758 \ucc44\ud0dd \uace1\uc120\uc758 \ubaa8\uc591\uc744 \uacb0\uc815\ud558\uba70, *\ud45c\uc2dc\ub294 \ubaa8\ub378\uc758 \ucc44\ud0dd \ud328\ud134\uc774 \uc81c\uc548\ub41c \ud504\ub808\uc784\uc6cc\ud06c\ub85c\ub294 \uc801\uc808\ud788 \uc124\uba85\ub418\uc9c0 \uc54a\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "2.1 \ubaa8\ub378\uc744 \uacbd\ud5d8\uc801 \ub370\uc774\ud130\uc5d0 \uc801\ud569"}]