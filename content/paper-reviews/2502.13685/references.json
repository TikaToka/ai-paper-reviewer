{"references": [{"fullname_first_author": "Angelos Katharopoulos", "paper_title": "Transformers are rnns: Fast autoregressive transformers with linear attention", "publication_date": "2020-00-00", "reason": "This paper introduces linear attention mechanisms, a core concept that the current paper builds upon for increased efficiency in sequence modeling."}, {"fullname_first_author": "Albert Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "publication_date": "2022-00-00", "reason": "This paper presents the S4 model, a foundational linear sequence modeling method that inspires aspects of the current work's memory management and efficiency goals."}, {"fullname_first_author": "Songlin Yang", "paper_title": "Gated delta networks: Improving mamba2 with delta rule", "publication_date": "2024-00-00", "reason": "This paper directly improves upon the Mamba model, a key linear sequence model that is closely related to the current work's approach, enhancing its memory update mechanism."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-00-00", "reason": "This foundational paper on Mixture-of-Experts (MoE) layers informs the current work's multi-memory architecture, which draws inspiration from MoE's efficiency in handling large models."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This highly influential paper introduces the Transformer architecture, which is compared against in the current paper, establishing a benchmark for evaluating the performance of linear sequence models."}]}