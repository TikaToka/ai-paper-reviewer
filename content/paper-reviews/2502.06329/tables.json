[{"content": "Model Name|Baseline|Mispelled (\u0394)|Incomplete (\u0394)|Out-of-Domain (\u0394)|OCR Context (\u0394)|Robustness (\u0394)\n---|---|---|---|---|---|---\nGemini 2.0 Flash Exp|0.95|0.95 (0.0)|0.95 (0.0)|0.88 (\u21930.07)|0.91 (\u21930.04)|0.83 (\u21930.12)\nGemini 1.5 Pro 002|0.96|0.96 (0.0)|0.94 (\u21930.02)|0.92 (\u21930.04)|0.92 (\u21930.04)|0.84 (\u21930.12)\nOpenAI GPT-4o|0.95|0.94 (\u21930.01)|0.94 (\u21930.01)|0.92 (\u21930.03)|0.95 (0.0)|0.85 (\u21930.1)\nOpenAI o1|0.97|0.95 (\u21930.02)|0.94 (\u21930.03)|0.89 (\u21930.08)|0.94 (\u21930.03)|0.81 (\u21930.16)\nOpenAI o3-mini|0.98|0.96 (\u21930.02)|0.96 (\u21930.02)|0.95 (\u21930.03)|0.90 (\u21930.08)|0.90 (\u21930.08)\nDeepSeek-R1-Distill-Llama-8B|0.83|0.85 (\u21910.02)|0.82 (\u21930.01)|0.87 (\u21910.04)|0.72 (\u21930.11)|0.64 (\u21930.19)\nDeepSeek-R1-Distill-Qwen-14B|0.95|0.90 (\u21930.05)|0.92 (\u21930.03)|0.93 (\u21930.02)|0.86 (\u21930.09)|0.82 (\u21930.13)\nDeepSeek-R1-Distill-Qwen-32B|0.95|0.97 (\u21910.02)|0.95 (0.0)|0.92 (\u21930.03)|0.89 (\u21930.06)|0.86 (\u21930.09)\nDeepSeek-R1-Distill-Llama-70B|0.96|0.97 (\u21910.01)|0.95 (\u21930.01)|0.94 (\u21930.02)|0.93 (\u21930.03)|0.89 (\u21930.07)\nDeepSeek-R1|0.94|0.94 (0.0)|0.93 (\u21930.01)|0.91 (\u21930.03)|0.88 (\u21930.06)|0.80 (\u21930.14)\nMeta-Llama-3.1-8B-Instruct|0.91|0.90 (\u21930.01)|0.86 (\u21930.05)|0.82 (\u21930.09)|0.80 (\u21930.11)|0.70 (\u21930.21)\nMeta-Llama-3.1-70B-Instruct|0.94|0.92 (\u21930.02)|0.94 (0.0)|0.87 (\u21930.07)|0.88 (\u21930.06)|0.80 (\u21930.14)\nMeta-Llama-3.3-70B-Instruct|0.95|0.92 (\u21930.03)|0.93 (\u21930.02)|0.90 (\u21930.05)|0.89 (\u21930.06)|0.82 (\u21930.13)\nQwen2.5-7B-Instruct|0.92|0.91 (\u21930.01)|0.90 (\u21930.02)|0.85 (\u21930.07)|0.80 (\u21930.12)|0.75 (\u21930.17)\nQwen2.5-14B-Instruct|0.95|0.94 (\u21930.01)|0.94 (\u21930.01)|0.94 (\u21930.01)|0.88 (\u21930.07)|0.86 (\u21930.09)\nQwen2.5-32B-Instruct|0.95|0.94 (\u21930.01)|0.93 (\u21930.02)|0.92 (\u21930.03)|0.92 (\u21930.03)|0.85 (\u21930.1)\nQwen2.5-72B-Instruct|0.94|0.94 (0.0)|0.94 (0.0)|0.92 (\u21930.02)|0.91 (\u21930.03)|0.84 (\u21930.1)\nQwen2.5-7B-Instruct-1M|0.91|0.91 (0.0)|0.91 (0.0)|0.86 (\u21930.05)|0.77 (\u21930.14)|0.74 (\u21930.17)\nQwen2.5-14B-Instruct-1M|0.95|0.92 (\u21930.03)|0.91 (\u21930.04)|0.91 (\u21930.04)|0.89 (\u21930.06)|0.80 (\u21930.15)\nNemotron-70B-Instruct-HF|0.94|0.94 (0.0)|0.93 (\u21930.01)|0.90 (\u21930.04)|0.91 (\u21930.03)|0.82 (\u21930.12)\nPhi-3-mini-128k-Instruct|0.86|0.85 (\u21930.01)|0.78 (\u21930.08)|0.79 (\u21930.07)|0.69 (\u21930.17)|0.58 (\u21930.28)\nPhi-3-small-128k-Instruct|0.88|0.84 (\u21930.04)|0.88 (0.0)|0.83 (\u21930.05)|0.78 (\u21930.1)|0.70 (\u21930.18)\nPhi-3-medium-128k-Instruct|0.89|0.84 (\u21930.05)|0.84 (\u21930.05)|0.81 (\u21930.08)|0.72 (\u21930.17)|0.63 (\u21930.26)\nPalmyra-Fin-128k-Instruct|0.96|0.93 (\u21930.03)|0.92 (\u21930.04)|0.90 (\u21930.06)|0.89 (\u21930.07)|0.83 (\u21930.13)", "caption": "Table 1: Robustness Results. Misspelled and incomplete queries seem manageable for all models; however, the most significant drop in performance, reaching up to 0.170.170.170.17 for Phi-3-mini-128k-Instruct and Phi-3-medium-128k-Instruct, is observed in cases involving OCRed queries. While the baseline performance appears relatively straightforward for all models, with scores ranging from 0.980.980.980.98 to 0.810.810.810.81, the point-wise minimum across all perturbations\u2014indicative of robustness\u2014reveals that models face challenges in consistently adapting to various input types. Even the most robust model, OpenAI o3-mini, experiences a decrease of 0.080.080.080.08 relative to the baseline. The best results in each category are in bold and second best are underlined.", "description": "\ud45c 1\uc740 \ub2e4\uc591\ud55c \uc785\ub825 \ubcc0\ud615\uc5d0 \ub300\ud55c \uc5ec\ub7ec \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uac15\uac74\uc131\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc624\ud0c0 \ubc0f \ubd88\uc644\uc804\ud55c \uc9c8\uc758\ub294 \ub300\ubd80\ubd84\uc758 \ubaa8\ub378\uc5d0\uc11c \ub2e4\ub8f0 \uc218 \uc788\uc5c8\uc9c0\ub9cc, OCR \uc624\ub958\uac00 \ud3ec\ud568\ub41c \uc9c8\uc758\uc5d0\uc11c\ub294 Phi-3-mini-128k-Instruct \ubc0f Phi-3-medium-128k-Instruct \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \ucd5c\ub300 0.17\uae4c\uc9c0 \uac10\uc18c\ud558\ub294 \ub4f1 \uc0c1\ub2f9\ud55c \uc131\ub2a5 \uc800\ud558\uac00 \uad00\ucc30\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uae30\uc900 \uc131\ub2a5\uc740 \ub300\ubd80\ubd84\uc758 \ubaa8\ub378\uc5d0\uc11c 0.98~0.81 \ubc94\uc704\uc600\uc9c0\ub9cc, \ubaa8\ub4e0 \ubcc0\ud615\uc5d0 \ub300\ud55c \ucd5c\uc18c\uac12\uc744 \ube44\uad50\ud588\uc744 \ub54c \ubaa8\ub378\ub4e4\uc774 \ub2e4\uc591\ud55c \uc785\ub825 \uc720\ud615\uc5d0 \uc77c\uad00\ub418\uac8c \uc801\uc751\ud558\ub294 \ub370 \uc5b4\ub824\uc6c0\uc744 \uacaa\ub294\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uac00\uc7a5 \uac15\uac74\ud55c \ubaa8\ub378\uc778 OpenAI o3-mini\uc870\ucc28\ub3c4 \uae30\uc900 \uc131\ub2a5\ubcf4\ub2e4 0.08 \uac10\uc18c\ud588\uc2b5\ub2c8\ub2e4. \uac01 \ubc94\uc8fc\ubcc4 \ucd5c\uace0 \uc131\ub2a5\uc740 \uad75\uac8c \ud45c\uc2dc\ud558\uace0, \ub450 \ubc88\uc9f8\ub85c \ub192\uc740 \uc131\ub2a5\uc740 \ubc11\uc904\ub85c \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4.", "section": "4 Evaluation"}, {"content": "| Model Name | Irrelevant Ctx | No Ctx | Ctx Grounding QA | Ctx Grounding TG | Ctx Grounding | Robustness | Compliance |\n|---|---|---|---|---|---|---|---| \n| Gemini 2.0 Flash Exp | 0.81 | 0.66 | 0.77 | 0.46 | 0.74 | 0.83 | 0.76 |\n| Gemini 1.5 Pro 002 | 0.74 | 0.64 | 0.72 | 0.53 | 0.69 | 0.84 | 0.72 |\n| OpenAI GPT-4o | 0.52 | 0.43 | 0.50 | 0.25 | 0.47 | 0.85 | 0.52 |\n| OpenAI o1 | 0.56 | 0.55 | 0.57 | 0.45 | 0.55 | 0.81 | 0.59 |\n| OpenAI o3-mini | 0.67 | 0.51 | 0.63 | 0.27 | 0.59 | 0.90 | 0.63 |\n| DeepSeek-R1-Distill-Llama-8B | 0.32 | 0.27 | 0.30 | 0.25 | 0.30 | 0.64 | 0.34 |\n| DeepSeek-R1-Distill-Qwen-14B | 0.49 | 0.21 | 0.36 | 0.27 | 0.35 | 0.82 | 0.40 |\n| DeepSeek-R1-Distill-Qwen-32B | 0.54 | 0.24 | 0.40 | 0.35 | 0.39 | 0.86 | 0.44 |\n| DeepSeek-R1-Distill-Llama-70B | 0.50 | 0.27 | 0.41 | 0.22 | 0.38 | 0.89 | 0.43 |\n| DeepSeek-R1 | 0.51 | 0.22 | 0.39 | 0.20 | 0.37 | 0.80 | 0.41 |\n| Meta-Llama-3.1-8B-Instruct | 0.67 | 0.63 | 0.70 | 0.27 | 0.65 | 0.70 | 0.66 |\n| Meta-Llama-3.1-70B-Instruct | 0.46 | 0.47 | 0.48 | 0.37 | 0.47 | 0.80 | 0.51 |\n| Meta-Llama-3.3-70B-Instruct | 0.50 | 0.40 | 0.47 | 0.31 | 0.45 | 0.82 | 0.49 |\n| Qwen2.5-7B-Instruct | 0.75 | 0.64 | 0.75 | 0.31 | 0.70 | 0.75 | 0.71 |\n| Qwen2.5-14B-Instruct | 0.75 | 0.61 | 0.70 | 0.55 | 0.68 | 0.86 | 0.71 |\n| Qwen2.5-32B-Instruct | 0.89 | 0.68 | 0.82 | 0.55 | 0.79 | 0.85 | 0.80 |\n| Qwen2.5-72B-Instruct | 0.69 | 0.60 | 0.68 | 0.39 | 0.64 | 0.84 | 0.67 |\n| Qwen2.5-7B-Instruct-1M | 0.63 | 0.58 | 0.65 | 0.29 | 0.60 | 0.74 | 0.62 |\n| Qwen2.5-14B-Instruct-1M | 0.78 | 0.53 | 0.69 | 0.37 | 0.65 | 0.80 | 0.68 |\n| Nemotron-70B-Instruct-HF | 0.52 | 0.48 | 0.52 | 0.39 | 0.50 | 0.82 | 0.54 |\n| Phi-3-mini-128k-Instruct | 0.54 | 0.34 | 0.47 | 0.24 | 0.44 | 0.58 | 0.46 |\n| Phi-3-small-128k-Instruct | 0.37 | 0.26 | 0.34 | 0.10 | 0.31 | 0.70 | 0.35 |\n| Phi-3-medium-128k-Instruct | 0.36 | 0.25 | 0.33 | 0.14 | 0.30 | 0.63 | 0.34 |\n| Palmyra-Fin-128k-Instruct | 0.95 | 0.66 | 0.83 | 0.65 | 0.80 | 0.83 | 0.81 |", "caption": "Table 2: Context Grounding Results. Missing context poses the biggest challenge for almost all tested models, except for Qwen2.5-32B-Instruct and Palmyra-Fin-128k-Instruct, which are also the most compliant models in our tests. The most robust model, OpenAI o3-mini, achieves 0.590.590.590.59 Context Grounding, leading to comparably low Compliance score of 0.630.630.630.63. Text generation queries (TG) achieve much lower Context Grounding results than question answering (QA) requests. In our calculation of LLM Compliance, we used \u03b2=0.5\ud835\udefd0.5\\beta=0.5italic_\u03b2 = 0.5. The best results in each category are in bold and second best are underlined.", "description": "\ud45c 2\ub294 \ubaa8\ub378\uc758 \ub9e5\ub77d \uc9c0\uac01 \ub2a5\ub825(Context Grounding)\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uacb0\uce21\ub41c \ub9e5\ub77d(Missing Context)\uc740 \ub300\ubd80\ubd84\uc758 \ubaa8\ub378\uc5d0\uac8c \uac00\uc7a5 \ud070 \uc5b4\ub824\uc6c0\uc744 \uc8fc\ub294 \uc694\uc18c\uc600\uc9c0\ub9cc, Qwen2.5-32B-Instruct\uc640 Palmyra-Fin-128k-Instruct \ubaa8\ub378\uc740 \uc608\uc678\uc801\uc73c\ub85c \ub192\uc740 \uc21c\uc751\ub3c4(Compliance)\ub97c \ubcf4\uc600\uc2b5\ub2c8\ub2e4.  \uac00\uc7a5 \uacac\uace0\ud55c(robust) \ubaa8\ub378\uc778 OpenAI 03-mini\ub294 \ub9e5\ub77d \uc9c0\uac01 \uc810\uc218\uac00 0.59\uc5d0 \uadf8\ucce4\uace0, \uc774\uc5d0 \ub530\ub77c \uc21c\uc751\ub3c4 \uc810\uc218\ub3c4 0.63\uc73c\ub85c \uc0c1\ub300\uc801\uc73c\ub85c \ub0ae\uc558\uc2b5\ub2c8\ub2e4.  \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \uc9c8\uc758(TG)\ub294 \uc9c8\ubb38\uc751\ub2f5 \uc9c8\uc758(QA)\ubcf4\ub2e4 \ub9e5\ub77d \uc9c0\uac01 \uc810\uc218\uac00 \ud6e8\uc52c \ub0ae\uc558\uc2b5\ub2c8\ub2e4.  LLM \uc21c\uc751\ub3c4 \uc810\uc218 \uacc4\uc0b0\uc5d0\ub294 \u03b2=0.5\uac00 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uac01 \ud56d\ubaa9\ubcc4 \ucd5c\uace0 \uc810\uc218\ub294 \ubcfc\ub4dc\uccb4\ub85c, \ub450 \ubc88\uc9f8\ub85c \ub192\uc740 \uc810\uc218\ub294 \ubc11\uc904\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3 Metrics"}]