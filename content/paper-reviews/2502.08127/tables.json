[{"content": "| Dataset | Size | Data Types | Average Token |\n|---|---|---|---| \n| FinQA | 1100 | Tables and Texts | 1,128 |\n| DM-Simplong | 100 | Tables and Texts | 4,330 |\n| XBRL-Math | 90 | Texts and Equations | 397 |", "caption": "Table 1: Overview of the datasets used in the study.", "description": "\ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc138 \uac00\uc9c0 \ub370\uc774\ud130\uc14b(FinQA, DM-Simplong, XBRL-Math)\uc5d0 \ub300\ud55c \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30, \ub370\uc774\ud130 \uc720\ud615(\ud45c\uc640 \ud14d\uc2a4\ud2b8, \ub610\ub294 \ubc29\uc815\uc2dd \ud3ec\ud568), \ud3c9\uade0 \ud1a0\ud070 \uc218\ub97c \ud3ec\ud568\ud558\uc5ec \ubaa8\ub378\uc758 \uc7ac\ubb34 \ucd94\ub860 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub41c \ub2e4\uc591\ud55c \uce21\uba74\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2.1 \ub370\uc774\ud130\uc14b"}, {"content": "| Model Name | Parameters | Reasoning Enhanced | Context Window Size | Close/Open Source | Reasoning Enhanced Training Data |\n|---|---|---|---|---|---| \n| GPT-4o | Unknown | No | 128k | Closed | - |\n| GPT-o1 | Unknown | Yes | 128k | Closed | Public and properity data (Human-annotated CoT, MCTS-assisted Synthetic data) |\n| GPT-o3-mini | Unknown | Yes | 128k | Closed | Public and properity data |\n| DeepSeek-V3 | 671B | No | 128k | Open | - |\n| DeepSeek-R1 | 671B | Yes | 128k | Open | Cold-start data generation, post-processing data |\n| Qwen2.5-72B-Instruct | 72B | No | 128k | Open | - |\n| Qwen2.5-72B-Instruct-Math | 72B | Yes | 128k | Open | Synthetic data from Qwen, high-quality mathematical data, CoT, TIR |\n| DeepSeek-R1-Distill-Llama-70B | 70B | Yes | 128k | Open | Distilled from R1 |\n| Llama3-70B-Instruct | 70B | No | 8k | Open | - |\n| Llama3.1-70B-Instruct | 70B | No | 128k | Open | - |\n| Llama3.3-70B-Instruct | 70B | No | 128k | Open | - |\n| DeepSeek-R1-Distill-Qwen-32B | 32B | Yes | 128k | Open | Distilled from R1 |\n| DeepSeek-R1-Distill-Qwen-14B | 14B | Yes | 128k | Open | Distilled from R1 |\n| DeepSeek-R1-Distill-Llama-8B | 8B | Yes | 128k | Open | Distilled from R1 |\n| Llama3-8B-Instruct | 8B | No | 8k | Open | - |\n| Llama3.1-8B-Instruct | 8B | No | 128k | Open | - |", "caption": "Table 2: Summary of evaluated large language models, including their parameter sizes, reasoning capabilities, input limits, source availability, and reasoning enhancement strategies. MCTS refers to Monte Carlo Tree Search. CoT refers to chain-of-thought reasoning. TIR means tool-integrated reasoning.", "description": "\ud45c 2\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \ud3c9\uac00\ud55c \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc758 \uc694\uc57d \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218 \ud06c\uae30, \ucd94\ub860 \ub2a5\ub825, \uc785\ub825 \uae38\uc774 \uc81c\ud55c, \ucd9c\ucc98(\uc624\ud508 \uc18c\uc2a4 \uc5ec\ubd80), \uadf8\ub9ac\uace0 \uc0ac\uc6a9\ub41c \ucd94\ub860 \ud5a5\uc0c1 \uc804\ub7b5 \ub4f1\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  MCTS\ub294 Monte Carlo Tree Search, CoT\ub294 Chain-of-Thought \ucd94\ub860, TIR\uc740 Tool-Integrated Reasoning\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ud2b9\uc9d5\uc744 \ud55c\ub208\uc5d0 \ud30c\uc545\ud558\uc5ec \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.", "section": "2 Methods"}, {"content": "| Models | FinQA | DM-Simplong | XBRL-Math | Average |\n|---|---|---|---|---|\n| GPT-4o | 72.49 | 60.00 | 72.22 | 68.24 |\n| GPT-o1 | 49.07 | 56.00 | 74.44 | 59.84 |\n| GPT-o3-mini | 60.87 | 59.00 | 76.67 | 65.51 |\n| DeepSeek-V3 | 73.20 | 53.00 | 76.67 | 67.62 |\n| DeepSeek-R1 | 65.13 | 53.00 | 86.67 | 68.93 |\n| Qwen2.5-72B-Instruct | 73.38 | 59.00 | 67.78 | 66.72 |\n| Qwen2.5-72B-Instruct-Math | 69.74 | 42.00 | 83.33 | 65.69 |\n| DeepSeek-R1-Distill-Llama-70B | 66.73 | 53.00 | 86.67 | 68.80 |\n| Llama3-70B-Instruct | 58.92 | 41.00 | 56.67 | 52.20 |\n| Llama3.1-70B-Instruct | 63.18 | 48.00 | 63.33 | 58.17 |\n| Llama3.3-70B-Instruct | 68.15 | 54.00 | 70.00 | 64.05 |\n| DeepSeek-R1-Distill-Qwen-32B | 65.48 | 55.00 | 84.44 | 68.97 |\n| DeepSeek-R1-Distill-Qwen-14B | 63.27 | 44.00 | 84.44 | 63.90 |\n| DeepSeek-R1-Distill-Llama-8B | 45.96 | 33.00 | 81.11 | 53.36 |\n| Llama3-8B-Instruct | 41.97 | 29.00 | 48.89 | 39.95 |\n| Llama3.1-8B-Instruct | 54.13 | 34.00 | 62.22 | 50.12 |\n| Fino1-8B | 60.87 | 40.00 | 82.22 | 61.03 |", "caption": "Table 3: Performance of different LLMs on three tested financial datasets.", "description": "\ud45c 3\uc740 \uc138 \uac00\uc9c0 \uae08\uc735 \ub370\uc774\ud130 \uc138\ud2b8(FinQA, DM-Simplong, XBRL-Math)\uc5d0\uc11c \ub2e4\uc591\ud55c \ud06c\uae30\uc640 \uae30\ub2a5(\uc77c\ubc18 \ubc0f \ucd94\ub860 \ud5a5\uc0c1)\uc744 \uac00\uc9c4 16\uac1c\uc758 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 FinQA, DM-Simplong, XBRL-Math \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4 \uc810\uc218\uc640 \ud3c9\uade0 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c LLM \uc544\ud0a4\ud14d\ucc98\uc640 \ucd94\ub860 \uc804\ub7b5\uc758 \uae08\uc735 \ub370\uc774\ud130 \ucc98\ub9ac \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3 \uacb0\uacfc"}]