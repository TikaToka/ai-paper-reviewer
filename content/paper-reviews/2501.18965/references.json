{"references": [{"fullname_first_author": "Ilya Loshchilov", "paper_title": "SGDR: stochastic gradient descent with warm restarts", "publication_date": "2017-00-00", "reason": "This paper introduced the cosine annealing schedule, a widely used learning rate schedule in deep learning, which is extensively discussed and compared to in the current paper."}, {"fullname_first_author": "Jordan Hoffmann", "paper_title": "An empirical analysis of compute-optimal large language model training", "publication_date": "2022-00-00", "reason": "This paper empirically showed that using a cosine schedule with a cycle length of one is optimal for large language model training, providing valuable empirical support for the theoretical analysis in the current paper."}, {"fullname_first_author": "Aaron Defazio", "paper_title": "Optimal linear decay learning rate schedules and further refinements", "publication_date": "2023-10-26", "reason": "This paper provides the theoretical foundation of the current paper's analysis by establishing a last-iterate convergence bound for SGD with general learning rate schedules that matches empirical observations, especially regarding the importance of cooldown."}, {"fullname_first_author": "Alexander H\u00e4gele", "paper_title": "Scaling laws and compute-optimal training beyond fixed training durations", "publication_date": "2024-05-00", "reason": "This paper, also authored by several co-authors of the current paper, empirically shows that a simpler schedule called wsd (warmup-stable-decay) matches the performance of cosine, while being more computationally efficient, and provides a practical basis for the current study."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This highly influential paper established the effectiveness of large language models and spurred significant interest in their development and training, providing a context for the current study on training large models."}]}