[{"content": "| Clamp | Comp | Quantile | Sim \u2191 | MSE \u2193 | SNR \u2191 |\n|---|---|---|---|---|---| \n| \u00d7 | \u2014 | \u2014 | 92.19% | 0.1055 | 8.31 |\n| \u221a | \u00d7 | 99.9 | 98.83% | 0.0366 | 14.25 |\n| \u221a | \u221a | 99.9 | 99.61% | 0.0245 | 15.31 |\n| \u221a | \u221a | 99 | 100% | 0.0099 | 18.38 |\n| \u221a | \u221a | 97 | 100% | 0.0068 | 20.88 |", "caption": "Table 1: Quantitative analysis of mathematical accuracy between original and quantized activation tensors. Results represent the average values obtained across all activation tensors on the 30,000 training iterations of the LLaMA 1.3B model.", "description": "\ud45c 1\uc740 LLaMA 1.3B \ubaa8\ub378\uc758 30,000\ubc88\uc758 \ud559\uc2b5 \ubc18\ubcf5\uc5d0\uc11c \ubaa8\ub4e0 \ud65c\uc131\ud654 \ud150\uc11c\uc5d0 \ub300\ud574 \uc5bb\uc740 \ud3c9\uade0\uac12\uc744 \ub098\ud0c0\ub0b4\ub294 \uc6d0\ubcf8 \ubc0f \uc591\uc790\ud654\ub41c \ud65c\uc131\ud654 \ud150\uc11c \uac04\uc758 \uc218\uce58\uc801 \uc815\ud655\ub3c4 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc6d0\ubcf8 \ud65c\uc131\ud654 \ud150\uc11c\uc640 \uc591\uc790\ud654\ub41c \ud65c\uc131\ud654 \ud150\uc11c \uac04\uc758 \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4(SIM), \ud3c9\uade0 \uc81c\uacf1 \uc624\ucc28(MSE), \uc2e0\ud638 \ub300 \uc7a1\uc74c\ube44(SNR)\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uc591\uc790\ud654 \uacfc\uc815\uc5d0\uc11c\uc758 \uc815\ubcf4 \uc190\uc2e4 \uc815\ub3c4\ub97c \ud3c9\uac00\ud569\ub2c8\ub2e4.  \ud2b9\ud788, CLAMP, COMP, QUANTILE \ub9e4\uac1c\ubcc0\uc218\uc758 \uc870\ud569\uc5d0 \ub530\ub978 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\uc5b4 \uc591\uc790\ud654 \uc804\ub7b5\uc758 \ud6a8\uacfc\ub97c \ub2e4\uac01\uc801\uc73c\ub85c \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "3.2 Outlier Clamping and Compensation"}, {"content": "| Model Size | Precision | Average | PiQA | Hellaswag | ObQA | Arc-C | Arc-E | BoolQ | LogiQA | SciQ | Lambada |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| 1.3B | BF16 | **53.23** | 71.11 | 50.80 | 36.60 | 36.69 | 68.60 | 57.83 | 30.26 | 83.30 | 43.84 |\n|  | FP4(Ours) | **53.13** | 70.89 | 50.82 | 36.20 | 36.86 | 67.47 | 58.23 | 29.49 | 83.90 | 44.30 |\n| 7B | BF16 | **53.87** | 71.22 | 52.03 | 37.40 | 38.99 | 67.47 | 60.55 | 27.65 | 85.00 | 44.56 |\n|  | FP4(Ours) | **54.42** | 71.87 | 52.97 | 38.40 | 39.85 | 67.97 | 62.20 | 27.96 | 84.70 | 43.88 |\n| 13B | BF16 | **54.44** | 72.80 | 53.56 | 38.60 | 38.82 | 67.97 | 57.40 | 29.65 | 86.30 | 44.87 |\n|  | FP4(Ours) | **54.95** | 73.78 | 54.12 | 39.60 | 39.68 | 67.89 | 55.90 | 30.88 | 85.80 | 46.89 |", "caption": "Table 2: Zero-shot evaluation for downstream tasks between BF16 models and FP4 models under different model sizes.", "description": "\ud45c 2\ub294 \uc11c\ub85c \ub2e4\ub978 \ud06c\uae30\uc758 \uc5b8\uc5b4 \ubaa8\ub378\uc5d0\uc11c BF16 \ubaa8\ub378\uacfc FP4 \ubaa8\ub378 \uac04\uc758 \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc5d0 \ub300\ud55c \uc81c\ub85c\uc0f7 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  BF16\uc740 \ubc18\uc815\ubc00\ub3c4 \ubd80\ub3d9\uc18c\uc218\uc810 \ubc29\uc2dd\uc774\uace0, FP4\ub294 4\ube44\ud2b8 \ubd80\ub3d9\uc18c\uc218\uc810 \uc591\uc790\ud654 \ubc29\uc2dd\uc785\ub2c8\ub2e4.  \ud45c\ub294 \uc5ec\ub7ec \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5(PiQA, HellaSwag, ObQA, Arc-C, Arc-E, BoolQ, LogiQA, SciQ, Lambada)\uc5d0 \ub300\ud55c \uac01 \ubaa8\ub378 \ud06c\uae30(1.3B, 7B, 13B \ud30c\ub77c\ubbf8\ud130)\ubcc4 \uc815\ud655\ub3c4 \uc810\uc218\ub97c \ube44\uad50\ud558\uc5ec FP4 \uc591\uc790\ud654\uac00 \uc815\ud655\ub3c4\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  \uc81c\ub85c\uc0f7 \ud3c9\uac00\ub294 \uc0ac\uc804 \ud559\uc2b5\ub41c \ubaa8\ub378\uc744 \ucd94\uac00\uc801\uc778 \ubbf8\uc138 \uc870\uc815 \uc5c6\uc774 \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5\uc5d0 \uc801\uc6a9\ud558\uc5ec \uc131\ub2a5\uc744 \uce21\uc815\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.", "section": "4.2 \uc8fc\uc694 \uacb0\uacfc"}, {"content": "| Format | 1111 | 1110 | 1101 | 1100 | 1011 | 1010 | 1001 | 1000/0000 | 0001 | 0010 | 0011 | 0100 | 0101 | 0110 | 0111 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| `e1m2` | -3.5 | -3 | -2.5 | -2 | -1.5 | -1 | -0.5 | \u00b10 | 0.5 | 1 | 1.5 | 2 | 2.5 | 3 | 3.5 |\n| `e2m1` | -6 | -4 | -3 | -2 | -1.5 | -1 | -0.5 | \u00b10 | 0.5 | 1 | 1.5 | 2 | 3 | 4 | 6 |\n| `e3m0` | -16 | -8 | -4 | -2 | -1 | -0.5 | -0.25 | \u00b10 | 0.25 | 0.5 | 1 | 2 | 4 | 8 | 16 |", "caption": "Table 3: FP4 Quantization Table under different FP4 formats.", "description": "\ud45c 3\uc740 \uc11c\ub85c \ub2e4\ub978 FP4 \ud615\uc2dd(E1M2, E2M1, E3M0) \ud558\uc5d0\uc11c \uac01 \ube44\ud2b8\uc758 \uc9c0\uc218\uc640 \uac00\uc218 \ubd80\ubd84\uc758 \uc870\ud569\uc5d0 \ub530\ub978 \ud45c\ud604 \uac00\ub2a5\ud55c \ubaa8\ub4e0 \uc22b\uc790 \uac12\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ud615\uc2dd\uc740 \uc9c0\uc218 \ube44\ud2b8 \uc218\uc640 \uac00\uc218 \ube44\ud2b8 \uc218\uac00 \ub2ec\ub77c \ud45c\ud604 \uac00\ub2a5\ud55c \uc22b\uc790\uc758 \ubc94\uc704\uc640 \uc815\ubc00\ub3c4\uac00 \ub2e4\ub985\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub17c\ubb38\uc758 FP4 \uc591\uc790\ud654 \ubc29\ubc95\uc744 \uc774\ud574\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.  E2M1 \ud615\uc2dd\uc740 \ub3d9\uc801 \ubc94\uc704\uc640 \uc815\ubc00\ub3c4 \uac04\uc758 \uade0\ud615\uc744 \uace0\ub824\ud558\uc5ec \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc120\ud0dd\ub41c \ud615\uc2dd\uc785\ub2c8\ub2e4.", "section": "2. Preliminaries"}]