{"references": [{"fullname_first_author": "Micikevicius", "paper_title": "Mixed precision training", "publication_date": "2017-10-03", "reason": "This paper introduced mixed precision training, a foundational technique for efficient deep learning that the current paper builds upon."}, {"fullname_first_author": "Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-20", "reason": "This paper established scaling laws for LLMs, providing a theoretical foundation for understanding the computational demands of training increasingly large models, a key context for the current work."}, {"fullname_first_author": "Dettmers", "paper_title": "GPT3.int8(): 8-bit matrix multiplication for transformers at scale", "publication_date": "2022-00-00", "reason": "This paper demonstrated the feasibility of 8-bit quantization for LLMs, directly inspiring the current research to explore the even more challenging 4-bit quantization."}, {"fullname_first_author": "Peng", "paper_title": "FP8-LM: Training FP8 large language models", "publication_date": "2023-10-23", "reason": "This paper showed the effectiveness of FP8 training for LLMs, offering a direct comparison point and a stepping stone for the current paper's exploration of FP4 training."}, {"fullname_first_author": "Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper introduced Llama 2, a large language model used in the current research for experiments, making it a crucial contextual reference."}]}