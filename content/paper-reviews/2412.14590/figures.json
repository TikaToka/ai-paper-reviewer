[{"figure_path": "https://arxiv.org/html/2412.14590/x1.png", "caption": "Figure 1: Illustration of the quantization with mixed-precision between output features and kernel execution.", "description": "\uadf8\ub9bc 1\uc740 \ud63c\ud569 \uc815\ubc00\ub3c4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucd9c\ub825 \ud2b9\uc9d5 \uac04\uc758 \uc591\uc790\ud654\uc640 \ucee4\ub110 \uc2e4\ud589\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub0ae\uc740 \uc911\uc694\ub3c4\uc758 \ucd9c\ub825 \ud2b9\uc9d5\uc5d0\ub294 4\ube44\ud2b8 \uc591\uc790\ud654\ub97c, \ub192\uc740 \uc911\uc694\ub3c4\uc758 \ucd9c\ub825 \ud2b9\uc9d5\uc5d0\ub294 8\ube44\ud2b8 \uc591\uc790\ud654\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ub192\uc774\uace0 \uba54\ubaa8\ub9ac \uc18c\ube44\ub97c \uc904\uc774\ub294 \ubc29\ubc95\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc120\ud615 \uac00\uc911\uce58\ub294 4\ube44\ud2b8 \ub610\ub294 8\ube44\ud2b8\ub85c \uc591\uc790\ud654\ub418\uace0, \ud65c\uc131\ud654\ub294 8\ube44\ud2b8\ub85c \uc591\uc790\ud654\ub429\ub2c8\ub2e4.  \ub450 \uac1c\uc758 MatMul \uc5f0\uc0b0\uc774 \ubcd1\ub82c\ub85c \uc218\ud589\ub418\uace0, \uc735\ud569\ub41c \uc0b0\ud3ec \uc5f0\uc0b0\uc744 \ud1b5\ud574 \ucd5c\uc885 \ucd9c\ub825\uc774 \uc0dd\uc131\ub429\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 MixLLM\uc758 \ud575\uc2ec\uc801\uc778 \uc591\uc790\ud654 \uc804\ub7b5\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2412.14590/x2.png", "caption": "Figure 2: The percentage of high-salient out features within each linear layer of Llama 3.1 8B model according to each feature\u2019s contribution to the final loss after quantizing to 4-bit, with 10% high-salient features globally.\nEach decoder layer contains q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, and down_proj in order.", "description": "\uadf8\ub9bc 2\ub294 Llama 3.1 8B \ubaa8\ub378\uc758 \uac01 \uc120\ud615 \ub808\uc774\uc5b4 \ub0b4\uc5d0\uc11c \uc911\uc694\ub3c4\uac00 \ub192\uc740 \ucd9c\ub825 \ud2b9\uc9d5(out feature)\uc758 \ube44\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc804\uc5ed\uc801\uc73c\ub85c \uc911\uc694\ub3c4\uac00 \ub192\uc740 \ud2b9\uc9d5\uc758 \ube44\uc728\uc744 10%\ub85c \uc124\uc815\ud558\uace0, 4\ube44\ud2b8\ub85c \uc591\uc790\ud654\ud588\uc744 \ub54c \ucd5c\uc885 \uc190\uc2e4\uc5d0 \ub300\ud55c \uac01 \ud2b9\uc9d5\uc758 \uae30\uc5ec\ub3c4\ub97c \uae30\uc900\uc73c\ub85c \uacc4\uc0b0\ud588\uc2b5\ub2c8\ub2e4. \uac01 \ub514\ucf54\ub354 \ub808\uc774\uc5b4\ub294 \uc21c\uc11c\ub300\ub85c q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.  \uc989, \uadf8\ub9bc\uc740 \ubaa8\ub378\uc758 \uac01 \ub808\uc774\uc5b4\uc5d0\uc11c \ucd5c\uc885 \uc190\uc2e4\uc5d0 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \ucd9c\ub825 \ud2b9\uc9d5\uc774 \uc5b4\ub290 \uc815\ub3c4\uc758 \ube44\uc728\uc744 \ucc28\uc9c0\ud558\ub294\uc9c0 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc785\ub2c8\ub2e4. \ud2b9\uc815 \ub808\uc774\uc5b4\uc5d0\uc11c \uc911\uc694\ub3c4\uac00 \ub192\uc740 \ucd9c\ub825 \ud2b9\uc9d5\uc758 \ube44\uc728\uc774 \ub2e4\ub978 \ub808\uc774\uc5b4\uc5d0 \ube44\ud574 \ud604\uc800\ud788 \ub192\uac70\ub098 \ub0ae\uc740 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.1 MixLLM\uc758 \uc591\uc790\ud654 \uc124\uacc4 \ubc0f \uacb0\uc815"}, {"figure_path": "https://arxiv.org/html/2412.14590/x3.png", "caption": "Figure 3: The float and integer value of binary (010010110xx...x), each within a consecutive range.", "description": "\uadf8\ub9bc 3\uc740 \ubd80\ub3d9 \uc18c\uc218\uc810 \uac12\uacfc \uc815\uc218 \uac12\uc744 \uc774\uc9c4\uc218 (010010110xx...x)\ub85c \ud45c\ud604\ud558\uace0, \uac01\uac01 \uc5f0\uc18d\uc801\uc778 \ubc94\uc704 \ub0b4\uc5d0 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\uc9c4\uc218\uc758 \uc55e\ubd80\ubd84 9\ube44\ud2b8\ub294 \uace0\uc815\ub418\uc5b4 \uc788\uace0, \ub098\uba38\uc9c0 23\ube44\ud2b8\ub294 \uac00\ubcc0\uc801\uc785\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 2\ub2e8\uacc4 \uc591\uc790\ud654 \uacfc\uc815\uc5d0\uc11c 8\ube44\ud2b8 Tensor Core\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 \uc815\uc218 \uac12\uc744 \ubd80\ub3d9 \uc18c\uc218\uc810 \uac12\uc73c\ub85c \ube60\ub974\uac8c \ubcc0\ud658\ud558\ub294 \ubc29\ubc95\uc744 \uc124\uba85\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \ud2b9\ud788, \ud2b9\uc815 \ubc94\uc704 \ub0b4\uc5d0\uc11c\ub294 \uc815\uc218 \uac12\uacfc \ubd80\ub3d9 \uc18c\uc218\uc810 \uac12\uc774 \uac19\uc740 \uc774\uc9c4 \ud45c\ud604\uc744 \uac16\ub294\ub2e4\ub294 \uc810\uc744 \uc774\uc6a9\ud558\uc5ec \ube60\ub978 \ubcc0\ud658\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \uc774\ub294 \ucd94\uac00\uc801\uc778 \uacc4\uc0b0 \uc624\ubc84\ud5e4\ub4dc\ub97c \uc904\uc774\uace0 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \uae30\uc5ec\ud569\ub2c8\ub2e4.", "section": "3.3 \ud6a8\uc728\uc801\uc778 \uc591\uc790\ud654 \uacc4\uc0b0 \uc2dc\uc2a4\ud15c"}, {"figure_path": "https://arxiv.org/html/2412.14590/x4.png", "caption": "Figure 4: The GPU kernel software pipeline of group-wise W4A8/W8A8 quantized MatMul.\nIt assumes perfect overlapping.\nG2S: load global to shared memory;\nS2R: load shared memory to register;\nMMA: matrix multiply-accumulation;\nI2F: integer to float conversion;\ndeq: dequantize;\nacc: accumulate.", "description": "\uadf8\ub9bc 4\ub294 \uadf8\ub8f9 \ub2e8\uc704\ub85c W4A8/W8A8 \uc591\uc790\ud654\ub41c MatMul\uc5d0 \ub300\ud55c GPU \ucee4\ub110 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \ud30c\uc774\ud504\ub77c\uc778\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc644\ubcbd\ud55c \uc911\ucca9\uc744 \uac00\uc815\ud569\ub2c8\ub2e4. G2S\ub294 \uc804\uc5ed \uba54\ubaa8\ub9ac\uc5d0\uc11c \uacf5\uc720 \uba54\ubaa8\ub9ac\ub85c \ub85c\ub4dc\ud558\ub294 \uac83\uc744 \ub098\ud0c0\ub0b4\uace0, S2R\uc740 \uacf5\uc720 \uba54\ubaa8\ub9ac\uc5d0\uc11c \ub808\uc9c0\uc2a4\ud130\ub85c \ub85c\ub4dc\ud558\ub294 \uac83\uc744, MMA\ub294 \ud589\ub82c \uacf1\uc148 \ub204\uc801\uc744, I2F\ub294 \uc815\uc218-\ubd80\ub3d9 \uc18c\uc218\uc810 \ubcc0\ud658\uc744, deq\ub294 \uc591\uc790\ud654 \ud574\uc81c\ub97c, acc\ub294 \ub204\uc801\uc744 \uac01\uac01 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 GPU\uc5d0\uc11c \ud6a8\uc728\uc801\uc778 \uc591\uc790\ud654 \uc5f0\uc0b0\uc744 \uc704\ud55c MixLLM\uc758 \uc2dc\uc2a4\ud15c \uc124\uacc4\ub97c \uc790\uc138\ud788 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc73c\ub85c, \uba54\ubaa8\ub9ac \uc811\uadfc, \uc591\uc790\ud654 \ud574\uc81c, MatMul \uc5f0\uc0b0\uc758 \uc911\ucca9\uc744 \ud1b5\ud574 \uc131\ub2a5\uc744 \ucd5c\uc801\ud654\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.3 Efficient Quantization Computation System"}, {"figure_path": "https://arxiv.org/html/2412.14590/x5.png", "caption": "Figure 5: The speedup of two types of single linear layers over torch float16 baseline on the A100 GPU.", "description": "\uadf8\ub9bc 5\ub294 A100 GPU\uc5d0\uc11c \ub450 \uac00\uc9c0 \uc720\ud615\uc758 \ub2e8\uc77c \uc120\ud615 \uacc4\uce35\uc5d0 \ub300\ud55c Torch float16 \uae30\uc900\uc120\uc5d0 \ub300\ud55c \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 \ud1a0\ud070 \uc218\ub97c \ub098\ud0c0\ub0b4\uace0 y\ucd95\uc740 \uc18d\ub3c4 \ud5a5\uc0c1 \ubc30\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ub450 \uac00\uc9c0 \uc720\ud615\uc758 \uc120\ud615 \uacc4\uce35\uc740 \uc785\ub825 \ud2b9\uc9d5\uc774 4096\uac1c\uc774\uace0 \ucd9c\ub825 \ud2b9\uc9d5\uc774 4096\uac1c\uc778 \uacbd\uc6b0\uc640 \ucd9c\ub825 \ud2b9\uc9d5\uc774 14336\uac1c\uc778 \uacbd\uc6b0\uc785\ub2c8\ub2e4. \uac01 \uc120\ud615 \uacc4\uce35\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \ube44\ud2b8 \uc218(W4A8, W4.4A8, W8A8)\ub85c \uce21\uc815\ud558\uc5ec float16 \uae30\uc900\uc120\uc5d0 \ub300\ud55c \uc131\ub2a5 \uac1c\uc120 \uc815\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \ud1a0\ud070 \uc218\uc5d0 \ub530\ub978 MixLLM\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uace0, \ud6a8\uc728\uc131\uc744 \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4 Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.14590/x6.png", "caption": "Figure 6: The perplexity (wikitext2) of Llama 3.1 8B model with different configurations.", "description": "\uadf8\ub9bc 6\uc740 Llama 3.1 8B \ubaa8\ub378\uc5d0 \ub300\ud55c \uc5ec\ub7ec \uac00\uc9c0 \uc124\uc815(weight-only quantization, activation quantization, mixed-precision quantization \ub4f1) \ud558\uc5d0\uc11c\uc758 wikitext2 perplexity \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc124\uc815\uc758 perplexity \uac12\uc744 \ube44\uad50\ud558\uc5ec \uc5b4\ub5a4 \uc124\uc815\uc774 \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294\uc9c0, \uadf8\ub9ac\uace0 \uac01 \uc124\uc815 \uc694\uc18c\ub4e4\uc774 perplexity\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uae30\ubcf8 \uc124\uc815(Basic)\ubd80\ud130 \uc2dc\uc791\ud558\uc5ec, activation quantization, \ube44\ub300\uce6d \uac00\uc911\uce58, \uadf8\ub8f9\ud654\ub41c \uac00\uc911\uce58, \uadf8\ub8f9\ud654\ub41c \ud65c\uc131\ud654 \ud568\uc218, 10%\uc758 8-bit \ucc44\ub110 \ucd94\uac00, Fisher \uc815\ubcf4 \ud589\ub82c \uc0ac\uc6a9 \uc5ec\ubd80, GPTQ \uc801\uc6a9 \uc5ec\ubd80 \ub4f1 \ub2e4\uc591\ud55c \uc124\uc815 \ubcc0\ud654\uc5d0 \ub530\ub978 perplexity \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.5.1 Ablation Study"}]