[{"figure_path": "https://arxiv.org/html/2502.11831/x1.png", "caption": "Figure 1: Video prediction in representation space (V-JEPA) achieves an understanding of intuitive physics. (A) Video models are evaluated on three intuitive physics datasets using the Violation of Expectation paradigm (IntPhys, GRASP, and InfLevel). V-JEPA is significantly more \u2018surprised\u2019 by implausible videos. Random initializations of V-JEPA (untrained networks) show near-chance performance, and state-of-the-art video models based on text or pixel prediction are much closer to chance. Confidence intervals at 95% are obtained via bootstrapping, except for untrained networks (n=20\ud835\udc5b20n=20italic_n = 20) which use a normal distribution assumption. (B) V-JEPA is trained to \u2019inpaint\u2019 natural videos in a learned representation space. Starting from a video and a corrupted version, representations are first extracted. The goal is then to predict the representation of the original video from the representation of the corrupted ones. (C) From a trained V-JEPA, we compute a surprise metric by predicting representations of N future frames based on M past ones and comparing the predictions to the representations of observed events. The surprise metric is then used to decide which of the two videos contains a physical violation.", "description": "\uadf8\ub9bc 1\uc740 \uc77c\ubc18\uc801\uc778 \ubb3c\ub9ac\uc801 \uc9c1\uad00\uc744 \uc774\ud574\ud558\ub294 V-JEPA(Video Joint Embedding Predictive Architecture) \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (A)\ub294 \uae30\uc874\uc758 \uc138 \uac00\uc9c0 \ubb3c\ub9ac\uc801 \uc9c1\uad00 \ub370\uc774\ud130\uc14b(IntPhys, GRASP, InfLevel)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc608\uc0c1 \uc704\ubc18 \ud328\ub7ec\ub2e4\uc784\uc73c\ub85c \ube44\ub514\uc624 \ubaa8\ub378\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. V-JEPA\ub294 \ube44\ud604\uc2e4\uc801\uc778 \ube44\ub514\uc624\uc5d0 \ub300\ud574 \ud6e8\uc52c \ub354 \ub180\ub77c\ub294(surprise) \ubc18\uc751\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. \ubc18\uba74\uc5d0, V-JEPA\uc758 \ubb34\uc791\uc704 \ucd08\uae30\ud654 \ubaa8\ub378(\ud6c8\ub828\ub418\uc9c0 \uc54a\uc740 \ub124\ud2b8\uc6cc\ud06c)\uc740 \uc6b0\uc5f0 \uc218\uc900\uc758 \uc131\ub2a5\uc744 \ubcf4\uc600\uace0, \ud14d\uc2a4\ud2b8 \ub610\ub294 \ud53d\uc140 \uc608\uce21 \uae30\ubc18\uc758 \ucd5c\ucca8\ub2e8 \ube44\ub514\uc624 \ubaa8\ub378\uc740 \uc6b0\uc5f0 \uc218\uc900\uc5d0 \uac00\uae4c\uc6b4 \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. (B)\ub294 V-JEPA\uac00 \ud559\uc2b5\ub41c \ud45c\uc0c1 \uacf5\uac04\uc5d0\uc11c \uc790\uc5f0 \ube44\ub514\uc624\uc758 \uc190\uc0c1\ub41c \ubd80\ubd84\uc744 \ubcf5\uc6d0\ud558\ub3c4\ub85d \ud6c8\ub828\ub418\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube44\ub514\uc624\uc640 \uc190\uc0c1\ub41c \ubc84\uc804\uc73c\ub85c\ubd80\ud130 \ud45c\uc0c1\uc744 \ucd94\ucd9c\ud55c \ud6c4, \uc190\uc0c1\ub41c \ud45c\uc0c1\uc73c\ub85c\ubd80\ud130 \uc6d0\ubcf8 \ube44\ub514\uc624\uc758 \ud45c\uc0c1\uc744 \uc608\uce21\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4. (C)\ub294 \ud6c8\ub828\ub41c V-JEPA\ub97c \uc0ac\uc6a9\ud558\uc5ec \uacfc\uac70 M\uac1c\uc758 \ud504\ub808\uc784\uc744 \uae30\ubc18\uc73c\ub85c \ubbf8\ub798 N\uac1c\uc758 \ud504\ub808\uc784 \ud45c\uc0c1\uc744 \uc608\uce21\ud558\uace0, \uad00\ucc30\ub41c \uc774\ubca4\ud2b8\uc758 \ud45c\uc0c1\uacfc \ube44\uad50\ud558\uc5ec \ub180\ub77c\uc6c0 \uc9c0\ud45c\ub97c \uacc4\uc0b0\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub180\ub77c\uc6c0 \uc9c0\ud45c\ub294 \ub450 \ube44\ub514\uc624 \uc911 \uc5b4\ub290 \uac83\uc774 \ubb3c\ub9ac\uc801 \uc704\ubc18\uc744 \ud3ec\ud568\ud558\ub294\uc9c0 \ud310\ub2e8\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "Video Prediction for intuitive physics understanding"}, {"figure_path": "https://arxiv.org/html/2502.11831/x2.png", "caption": "Figure 2: V-JEPA accuracy increase relative to randomly-initialized models and humans across different physical properties and benchmarks. (A) Because some benchmarks contain low-level biases, we test the model performance against a set of randomly initialized networks (n=20\ud835\udc5b20n=20italic_n = 20). V-JEPA models (n=5\ud835\udc5b5n=5italic_n = 5) have higher relative classification accuracy on intuitive physics benchmarks for most, but not all concepts. (B) V-JEPA relative (left) and absolute (right) accuracy on the IntPhys test set across different conditions compared to naive human performance, showing a high correlation between human and machine errors. The V-JEPA score uses the maximum surprise from each video, which generalizes better for single-video classification. Human data are taken from\u00a0(Riochet et\u00a0al., 2022).", "description": "\uadf8\ub9bc 2\ub294 \ub2e4\uc591\ud55c \ubb3c\ub9ac\uc801 \uc18d\uc131\uacfc \ubca4\uce58\ub9c8\ud06c\uc5d0 \uac78\uccd0 \ubb34\uc791\uc704\ub85c \ucd08\uae30\ud654\ub41c \ubaa8\ub378\uacfc \uc0ac\ub78c\uc5d0 \ube44\ud574 V-JEPA\uc758 \uc815\ud655\ub3c4 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (A) \uc77c\ubd80 \ubca4\uce58\ub9c8\ud06c\uc5d0\ub294 \uc800\uc218\uc900\uc758 \ud3b8\ud5a5\uc774 \uc788\uc73c\ubbc0\ub85c \ubaa8\ub378 \uc131\ub2a5\uc744 20\uac1c\uc758 \ubb34\uc791\uc704\ub85c \ucd08\uae30\ud654\ub41c \ub124\ud2b8\uc6cc\ud06c\uc640 \ube44\uad50\ud558\uc5ec \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4. V-JEPA \ubaa8\ub378(5\uac1c)\uc740 \ub300\ubd80\ubd84\uc758 \uac1c\ub150\uc5d0 \ub300\ud574 \uc9c1\uad00\uc801\uc778 \ubb3c\ub9ac\ud559 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub354 \ub192\uc740 \uc0c1\ub300\uc801 \ubd84\ub958 \uc815\ud655\ub3c4\ub97c \ubcf4\uc785\ub2c8\ub2e4. (B) IntPhys \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0\uc11c \ub2e4\uc591\ud55c \uc870\uac74\uc5d0 \uac78\uccd0 \uc0ac\ub78c\uc758 \ub2e8\uc21c\ud55c \uc131\ub2a5\uacfc \ube44\uad50\ud55c V-JEPA\uc758 \uc0c1\ub300\uc801(\uc67c\ucabd) \ubc0f \uc808\ub300\uc801(\uc624\ub978\ucabd) \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\uba70, \uc0ac\ub78c\uacfc \uae30\uacc4 \uc624\ub958 \uac04\uc758 \ub192\uc740 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. V-JEPA \uc810\uc218\ub294 \uac01 \ube44\ub514\uc624\uc758 \ucd5c\ub300 \ub180\ub77c\uc6c0\uc744 \uc0ac\uc6a9\ud558\uba70, \ub2e8\uc77c \ube44\ub514\uc624 \ubd84\ub958\uc5d0 \ub354 \uc801\ud569\ud569\ub2c8\ub2e4. \uc778\uac04 \ub370\uc774\ud130\ub294 (Riochet et al., 2022)\uc5d0\uc11c \uac00\uc838\uc654\uc2b5\ub2c8\ub2e4.", "section": "Measuring intuitive physics understanding"}, {"figure_path": "https://arxiv.org/html/2502.11831/x3.png", "caption": "Figure 3: Influence of type of mask, type and amount of training data, and model size on V-JEPA IntPhys scores. (A) When pretrained on VM2M, V-JEPA exhibits an understanding of intuitive physics with every masking strategy. (B) Of the three training datasets, two give high accuracies when trained separately (K710 and Howto100M). High scores are found with only 1289 hours of Howto100M (the largest dataset), and even 128h gives better than chance performance. (C) While larger encoders improve performance, we find that the performance remains non-trivial across sizes when pretraining on HowTo100M. Confidence intervals obtained via bootstrapping.", "description": "\uadf8\ub9bc 3\uc740 V-JEPA \ubaa8\ub378\uc758 IntPhys \uc810\uc218\uc5d0 \ub300\ud55c \ub9c8\uc2a4\ud06c \uc720\ud615, \ud6c8\ub828 \ub370\uc774\ud130 \uc720\ud615 \ubc0f \uc591, \ubaa8\ub378 \ud06c\uae30\uc758 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (A) VM2M\uc73c\ub85c \uc0ac\uc804 \ud6c8\ub828\ub41c \uacbd\uc6b0, V-JEPA\ub294 \ubaa8\ub4e0 \ub9c8\uc2a4\ud0b9 \uc804\ub7b5\uc5d0\uc11c \uc9c1\uad00\uc801\uc778 \ubb3c\ub9ac\ud559\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (B) \uc138 \uac00\uc9c0 \ud6c8\ub828 \ub370\uc774\ud130\uc14b \uc911 \ub450 \uac00\uc9c0(K710 \ubc0f HowTo100M)\ub294 \uac1c\ubcc4\uc801\uc73c\ub85c \ud6c8\ub828\ub418\uc5c8\uc744 \ub54c \ub192\uc740 \uc815\ud655\ub3c4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. HowTo100M\uc758 1289\uc2dc\uac04\ub9cc\uc73c\ub85c\ub3c4 \ub192\uc740 \uc810\uc218\ub97c \uc5bb\uc5c8\uc73c\uba70, 128\uc2dc\uac04\ub9cc\uc73c\ub85c\ub3c4 \uc6b0\uc5f0\ubcf4\ub2e4 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. (C) \ub354 \ud070 \uc778\ucf54\ub354\ub294 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uc9c0\ub9cc, HowTo100M\uc73c\ub85c \uc0ac\uc804 \ud6c8\ub828\ud588\uc744 \ub54c \ud06c\uae30\uc5d0 \uad00\uacc4\uc5c6\uc774 \uc131\ub2a5\uc774 \uc758\ubbf8 \uc788\ub294 \uc218\uc900\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4. \uc2e0\ub8b0 \uad6c\uac04\uc740 \ubd80\ud2b8\uc2a4\ud2b8\ub798\ud551\uc744 \ud1b5\ud574 \uc5bb\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "Keys to intuitive physics understanding"}, {"figure_path": "https://arxiv.org/html/2502.11831/x6.png", "caption": "Figure S1: Different surprise measures are better suited for different tasks. Focusing on IntPhys, we find that looking at the average surprise over a video leads to better performance when comparing pairs of videos. A one-sample t-test was performed to see if the relative surprises are greater than zero (left). However, when looking at individual videos\u2019 surprise, choosing the maximum surprise over a video leads to a better separation between possible and impossible videos. A two-sample t-test was performed to see if impossible videos have higher surprise than possible ones. (rigt).", "description": "\uadf8\ub9bc S1\uc740 \uc11c\ub85c \ub2e4\ub978 \ub180\ub77c\uc6c0 \uce21\uc815 \ubc29\uc2dd\uc774 \uc11c\ub85c \ub2e4\ub978 \uc791\uc5c5\uc5d0 \ub354 \uc801\ud569\ud558\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. IntPhys\uc5d0 \uc911\uc810\uc744 \ub450\uba74 \ube44\ub514\uc624\uc758 \ud3c9\uade0 \ub180\ub77c\uc6c0\uc744 \ubcf4\ub294 \uac83\uc774 \ube44\ub514\uc624 \uc30d\uc744 \ube44\uad50\ud560 \ub54c \uc131\ub2a5\uc774 \ub354 \uc6b0\uc218\ud558\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc77c\ud45c\ubcf8 t-\uac80\uc815\uc744 \uc218\ud589\ud558\uc5ec \uc0c1\ub300\uc801\uc778 \ub180\ub77c\uc6c0\uc774 0\ubcf4\ub2e4 \ud070\uc9c0 \ud655\uc778\ud588\uc2b5\ub2c8\ub2e4(\uc67c\ucabd). \uadf8\ub7ec\ub098 \uac1c\ubcc4 \ube44\ub514\uc624\uc758 \ub180\ub77c\uc6c0\uc744 \uc0b4\ud3b4\ubcfc \ub54c \ube44\ub514\uc624\uc758 \ucd5c\ub300 \ub180\ub77c\uc6c0\uc744 \uc120\ud0dd\ud558\uba74 \uac00\ub2a5\ud55c \ube44\ub514\uc624\uc640 \ubd88\uac00\ub2a5\ud55c \ube44\ub514\uc624\ub97c \ub354 \uc798 \uad6c\ubd84\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubd88\uac00\ub2a5\ud55c \ube44\ub514\uc624\uc758 \ub180\ub77c\uc6c0\uc774 \uac00\ub2a5\ud55c \ube44\ub514\uc624\ubcf4\ub2e4 \ub354 \ub192\uc740\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud574 \uc774\ud45c\ubcf8 t-\uac80\uc815\uc744 \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4(\uc624\ub978\ucabd).", "section": "A.7 \ud3c9\uac00 \ud504\ub85c\ud1a0\ucf5c"}, {"figure_path": "https://arxiv.org/html/2502.11831/x7.png", "caption": "Figure S2: Normalized probabilities output by Qwen2-VL-72B. When presented with a pair of videos, we find that the model outputs similar probabilities for possible and impossible videos.", "description": "\uadf8\ub9bc S2\ub294 Qwen2-VL-72B \ubaa8\ub378\uc774 \ud55c \uc30d\uc758 \ube44\ub514\uc624\ub97c \uc81c\uc2dc\ubc1b\uc558\uc744 \ub54c, \uac00\ub2a5\ud55c \ube44\ub514\uc624\uc640 \ubd88\uac00\ub2a5\ud55c \ube44\ub514\uc624\uc5d0 \ub300\ud574 \uc720\uc0ac\ud55c \ud655\ub960\uc744 \ucd9c\ub825\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ubaa8\ub378\uc774 \uac00\ub2a5/\ubd88\uac00\ub2a5 \uc5ec\ubd80\ub97c \uad6c\ubd84\ud558\ub294 \ub370 \uc5b4\ub824\uc6c0\uc744 \uacaa\ub294\ub2e4\ub294 \uac83\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc73c\uba70,  \uac01 \ube44\ub514\uc624 \uc720\ud615\uc758 \ud655\ub960 \ubd84\ud3ec\ub97c \ube44\uad50\ud558\uc5ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \uc138 \uac1c\uc758 \ub2e4\ub978 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b(IntPhys, GRASP, InfLevel-lab)\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \uac01\uac01 \ud45c\uc2dc\ub418\uc5b4 \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \ud3c9\uac00\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.", "section": "A.7 \ud3c9\uac00 \ud504\ub85c\ud1a0\ucf5c"}, {"figure_path": "https://arxiv.org/html/2502.11831/x10.png", "caption": "Figure S3: Models perform suboptimally with a fixed context size. Due to limitations in how long of a video models can process, we find drops in performance when using a single context size across all properties and datasets. Performance remains non-trivial for V-JEPA in this scenario.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc18d\uc131\uacfc \ub370\uc774\ud130\uc14b\uc5d0 \uac78\uccd0 \ub2e8\uc77c \ucee8\ud14d\uc2a4\ud2b8 \ud06c\uae30\ub97c \uc0ac\uc6a9\ud560 \ub54c \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \uc800\ud558\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\uc774 \ucc98\ub9ac\ud560 \uc218 \uc788\ub294 \ube44\ub514\uc624 \uae38\uc774\uc5d0 \uc81c\ud55c\uc774 \uc788\uae30 \ub54c\ubb38\uc5d0 \ubc1c\uc0dd\ud558\ub294 \ud604\uc0c1\uc785\ub2c8\ub2e4.  V-JEPA\uc758 \uacbd\uc6b0 \uc774\ub7ec\ud55c \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c\ub3c4 \uc131\ub2a5\uc774 \uc5ec\uc804\ud788 \uc720\uc758\ubbf8\ud558\uac8c \ub098\ud0c0\ub0a9\ub2c8\ub2e4. \uc989, \ube44\ub514\uc624\uc758 \uc77c\ubd80\ubd84\ub9cc\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc608\uce21\uc744 \uc218\ud589\ud560 \ub54c \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b\uacfc \ubb3c\ub9ac\uc801 \uc18d\uc131\uc5d0 \ub300\ud574, \ub2e8\uc77c \ucee8\ud14d\uc2a4\ud2b8 \ud06c\uae30\uc5d0\uc11c\uc758 \uc131\ub2a5 \uc800\ud558\uac00 \uad00\ucc30\ub418\ub294\ub370, \ud2b9\ud788 V-JEPA \ubaa8\ub378\uc740 \uc774\ub7ec\ud55c \uc0c1\ud669\uc5d0\uc11c\ub3c4 \uc131\ub2a5\uc774 \ud06c\uac8c \ub5a8\uc5b4\uc9c0\uc9c0 \uc54a\uc74c\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "C. \ube44\ub514\uc624\uc758 \uc758\ubbf8 \ubc0f \uc6c0\uc9c1\uc784 \ub2e4\uc591\uc131\uc758 \uc601\ud5a5"}, {"figure_path": "https://arxiv.org/html/2502.11831/x11.png", "caption": "Figure S4: Variation of performance when changing context size for predictions. While models tend to perform better with smaller context sizes, we find the optimal context size to be dependent on the property and dataset. GRASP exhibits the most variation whereas IntPhys and InfLevel-lab are less sensitive in general.", "description": "\uadf8\ub9bc S4\ub294 \ub2e4\uc591\ud55c \ubb3c\ub9ac\uc801 \uc18d\uc131\uacfc \ub370\uc774\ud130 \uc9d1\ud569\uc5d0 \ub300\ud574 \uc608\uce21\uc744 \uc704\ud55c \ubb38\ub9e5 \ud06c\uae30\ub97c \ubcc0\uacbd\ud560 \ub54c \uc131\ub2a5\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c \ubaa8\ub378\uc740 \ub354 \uc791\uc740 \ubb38\ub9e5 \ud06c\uae30\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uc9c0\ub9cc, \ucd5c\uc801\uc758 \ubb38\ub9e5 \ud06c\uae30\ub294 \uc18d\uc131\uacfc \ub370\uc774\ud130 \uc9d1\ud569\uc5d0 \ub530\ub77c \ub2e4\ub985\ub2c8\ub2e4. GRASP\ub294 \uac00\uc7a5 \ud070 \ubcc0\ud654\ub97c \ubcf4\uc774\ub294 \ubc18\uba74, IntPhys\uc640 InfLevel-lab\ub294 \uc0c1\ub300\uc801\uc73c\ub85c \ub35c \ubbfc\uac10\ud569\ub2c8\ub2e4.  \uc989, \uc5b4\ub5a4 \ubb3c\ub9ac\uc801 \ud604\uc0c1\uc744 \uc608\uce21\ud558\ub294 \ub370 \ud544\uc694\ud55c \uacfc\uac70 \uc815\ubcf4\uc758 \uc591\uc740 \ud604\uc0c1\uc758 \uc885\ub958\uc640 \ub370\uc774\ud130 \ud2b9\uc9d5\uc5d0 \ub530\ub77c \ub2e4\ub974\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. GRASP \ub370\uc774\ud130\uc14b\uc758 \uacbd\uc6b0 \ub2e4\ub978 \ub370\uc774\ud130\uc14b\uc5d0 \ube44\ud574 \ucd5c\uc801\uc758 \ubb38\ub9e5 \ud06c\uae30\uac00 \ub354\uc6b1 \ubcc0\ud654\ubb34\uc30d\ud558\uac8c \ub098\ud0c0\ub098\uba70, \uc774\ub294 \ub370\uc774\ud130\uc14b \uc790\uccb4\uc758 \ud2b9\uc131\uacfc \uad00\ub828\uc774 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "C. \ube44\ub514\uc624\uc758 \uc758\ubbf8 \ubc0f \ub3d9\uc791 \ub2e4\uc591\uc131\uc758 \uc601\ud5a5"}, {"figure_path": "https://arxiv.org/html/2502.11831/x12.png", "caption": "Figure S5: Influence of motion and scene diversity. By pretraining V-JEPA-L on subsets of HowTo100M, we investigate how the diversity in motion and scenes affects performance on IntPhys.(left) By subsampling videos, we reduce the diversity in scenes, where we find that the model can still reach good performance with 128h of unique videos. (right) By subsampling frames in videos, we reduce the diversity of motions in each scene. Here we find lower performance than when subsampling videos, but the model still achieves good performance with 2% of the frames (2579h).", "description": "\uadf8\ub9bc S5\ub294 HowTo100M\uc758 \ud558\uc704 \uc9d1\ud569\uc73c\ub85c V-JEPA-L\uc744 \uc0ac\uc804 \ud6c8\ub828\ud558\uc5ec \ub3d9\uc791\uacfc \uc7a5\uba74\uc758 \ub2e4\uc591\uc131\uc774 IntPhys \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc870\uc0ac\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (\uc67c\ucabd) \ube44\ub514\uc624\ub97c \ud558\uc704 \uc0d8\ud50c\ub9c1\ud558\uc5ec \uc7a5\uba74\uc758 \ub2e4\uc591\uc131\uc744 \uc904\uc774\uba74 \ubaa8\ub378\uc740 \uc5ec\uc804\ud788 128\uc2dc\uac04\uc758 \uace0\uc720 \ube44\ub514\uc624\ub85c \uc88b\uc740 \uc131\ub2a5\uc5d0 \ub3c4\ub2ec\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (\uc624\ub978\ucabd) \ube44\ub514\uc624\uc758 \ud504\ub808\uc784\uc744 \ud558\uc704 \uc0d8\ud50c\ub9c1\ud558\uc5ec \uac01 \uc7a5\uba74\uc758 \ub3d9\uc791 \ub2e4\uc591\uc131\uc744 \uc904\uc774\uba74 \ube44\ub514\uc624 \ud558\uc704 \uc0d8\ud50c\ub9c1\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub0ae\uc9c0\ub9cc \ubaa8\ub378\uc740 \uc5ec\uc804\ud788 \ud504\ub808\uc784\uc758 2%(2579\uc2dc\uac04)\ub85c \uc88b\uc740 \uc131\ub2a5\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "C. \ube44\ub514\uc624\uc758 \uc758\ubbf8 \ubc0f \ub3d9\uc791 \ub2e4\uc591\uc131\uc758 \uc601\ud5a5"}, {"figure_path": "https://arxiv.org/html/2502.11831/x13.png", "caption": "Figure S6: Relabeling InfLevel to remove contextualization events. Gravity and solidity both require to remember the properties about the containers shown in a video before the actual experiment. By relabeling the videos such that the prefix video is not necessary, we find a significant increase in performance for both V-JEPA and VideoMAE. However, this relabeling breaks the assumption that the possible and impossible videos have the same difficulty.", "description": "\uadf8\ub9bc S6\uc740 InfLevel \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ubb38\ub9e5\ud654 \uc774\ubca4\ud2b8\ub97c \uc81c\uac70\ud558\uc5ec V-JEPA\uc640 VideoMAE \ubaa8\ub378\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc911\ub825\uacfc \uace0\uccb4\uc131\uacfc \uac19\uc740 \uc18d\uc131\uc740 \uc2e4\ud5d8 \uc804\uc5d0 \ube44\ub514\uc624\uc5d0 \ud45c\uc2dc\ub41c \uc6a9\uae30\uc758 \uc18d\uc131\uc744 \uae30\uc5b5\ud574\uc57c \ud569\ub2c8\ub2e4. \uc811\ub450\uc0ac \ube44\ub514\uc624\uac00 \ud544\uc694\ud558\uc9c0 \uc54a\ub3c4\ub85d \ube44\ub514\uc624\uc5d0 \ub808\uc774\ube14\uc744 \ub2e4\uc2dc \uc9c0\uc815\ud568\uc73c\ub85c\uc368, \ub450 \ubaa8\ub378 \ubaa8\ub450 \uc131\ub2a5\uc774 \ud06c\uac8c \ud5a5\uc0c1\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc774\ub7ec\ud55c \ub808\uc774\ube14 \uc7ac\uc9c0\uc815\uc740 \uac00\ub2a5\ud55c \ube44\ub514\uc624\uc640 \ubd88\uac00\ub2a5\ud55c \ube44\ub514\uc624\uc758 \ub09c\uc774\ub3c4\uac00 \ub3d9\uc77c\ud558\ub2e4\ub294 \uac00\uc815\uc744 \uae68\ub728\ub9bd\ub2c8\ub2e4.", "section": "E. InfLevel \ub370\uc774\ud130\uc14b\uc758 \ubb38\ub9e5\ud654 \uc774\ubca4\ud2b8 \uc911\uc694\uc131"}, {"figure_path": "https://arxiv.org/html/2502.11831/x14.png", "caption": "Figure S7: Complete results for V-JEPA-L. The models (n=5\ud835\udc5b5n=5italic_n = 5) achieve accuracies higher than untrained networks on most properties. Black dots represent the performance of 5 seeds.", "description": "\uadf8\ub9bc S7\uc740 V-JEPA-L \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \ubb3c\ub9ac\uc801 \uc18d\uc131\uc5d0 \ub300\ud574 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  5\uac1c\uc758 V-JEPA-L \ubaa8\ub378(n=5)\uc774 \ub300\ubd80\ubd84\uc758 \uc18d\uc131\uc5d0\uc11c \ud6c8\ub828\ub418\uc9c0 \uc54a\uc740 \ub124\ud2b8\uc6cc\ud06c\ubcf4\ub2e4 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. \uac80\uc740\uc0c9 \uc810\uc740 \uac01 \ubaa8\ub378\uc758 5\ubc88\uc758 \uc2dc\ub3c4\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 V-JEPA-L \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \uc9c1\uad00\uc801 \ubb3c\ub9ac\uc801 \uc18d\uc131\uc744 \uc774\ud574\ud558\ub294 \ub370 \uc788\uc5b4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc900\ub2e4\ub294 \uac83\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "F Per-property performance of methods"}, {"figure_path": "https://arxiv.org/html/2502.11831/x15.png", "caption": "Figure S8: Complete results for V-JEPA-H. The model achieves accuracies higher than untrained networks on most properties. Gray dots represent the performance of the 20 untrained networks. Confidence intervals obtained via bootstrapping.", "description": "\uadf8\ub9bc S8\uc740 V-JEPA-H \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \ubb3c\ub9ac\uc801 \uc18d\uc131\uc5d0 \ub300\ud574 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 V-JEPA-H \ubaa8\ub378\uc774 \ub300\ubd80\ubd84\uc758 \uc18d\uc131\uc5d0\uc11c \ud6c8\ub828\ub418\uc9c0 \uc54a\uc740 \ub124\ud2b8\uc6cc\ud06c\ubcf4\ub2e4 \ub354 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud588\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud68c\uc0c9 \uc810\uc740 20\uac1c\uc758 \ud6c8\ub828\ub418\uc9c0 \uc54a\uc740 \ub124\ud2b8\uc6cc\ud06c\uc758 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0b4\uba70, \uc2e0\ub8b0 \uad6c\uac04\uc740 \ubd80\ud2b8\uc2a4\ud2b8\ub798\ud551\uc744 \ud1b5\ud574 \uc5bb\uc5b4\uc84c\uc2b5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ubaa8\ub378\uc774 \ubb3c\ub9ac\uc801 \uc9c1\uad00\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \uc5bc\ub9c8\ub098 \uc798 \ud559\uc2b5\ud588\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc0c1\uc138\ud55c \uacb0\uacfc\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "F. \uc18d\uc131\ubcc4 \ubc29\ubc95\uc758 \uc131\ub2a5"}, {"figure_path": "https://arxiv.org/html/2502.11831/x16.png", "caption": "Figure S9: Complete results for VideoMAEv2. The model achieves performance on par or slightly higher than untrained networks across properties, apart from solidity and collision. Gray dots represent the performance of the 20 untrained networks. Confidence intervals obtained via bootstrapping.", "description": "\uadf8\ub9bc S9\ub294 VideoMAEv2 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \ubb3c\ub9ac\uc801 \uc18d\uc131\uc5d0 \ub300\ud574 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  VideoMAEv2\ub294 \ud6c8\ub828\ub418\uc9c0 \uc54a\uc740 \ub124\ud2b8\uc6cc\ud06c\uc640 \ube44\uc2b7\ud558\uac70\ub098 \uc57d\uac04 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uc9c0\ub9cc, \uace0\uccb4\uc131(Solidity)\uacfc \ucda9\ub3cc(Collision) \uc18d\uc131\uc5d0\uc11c\ub294 \uc131\ub2a5\uc774 \ub2e4\uc18c \ub5a8\uc5b4\uc9d1\ub2c8\ub2e4. \ud68c\uc0c9 \uc810\uc740 \ud6c8\ub828\ub418\uc9c0 \uc54a\uc740 20\uac1c \ub124\ud2b8\uc6cc\ud06c\uc758 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0b4\uba70, \uc2e0\ub8b0 \uad6c\uac04\uc740 \ubd80\ud2b8\uc2a4\ud2b8\ub798\ud551\uc744 \ud1b5\ud574 \uc5bb\uc5b4\uc84c\uc2b5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ubb3c\ub9ac\uc801 \uc18d\uc131\uc5d0 \ub300\ud55c VideoMAEv2\uc758 \uc131\ub2a5\uc744 \uc790\uc138\ud788 \ubcf4\uc5ec\uc8fc\ub294 \uc2dc\uac01\uc801 \uc790\ub8cc\uc785\ub2c8\ub2e4.", "section": "F. Per-property performance of methods"}]