{"references": [{"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-15", "reason": "This paper is foundational to the study because it establishes the scaling paradigm in modern AI, directly impacting the need for quantization techniques to address computational expense."}, {"fullname_first_author": "Saleh Ashkboos", "paper_title": "Towards end-to-end 4-bit inference on generative large language models", "publication_date": "2023-10-09", "reason": "This paper presents QUIK, a quantization method directly evaluated and compared in the main study, contributing significantly to the results."}, {"fullname_first_author": "Ji Lin", "paper_title": "AWQ: Activation-aware weight quantization for on-device LLM compression and acceleration", "publication_date": "2024-00-00", "reason": "This paper introduces AWQ, another key quantization technique analyzed in the research, directly influencing the comparative analysis of quantization methods."}, {"fullname_first_author": "Vage Egiazarian", "paper_title": "Extreme compression of large language models via additive quantization", "publication_date": "2024-01-06", "reason": "This paper details AQLM, a third significant quantization method examined in the study, contributing substantially to the overall evaluation of different approaches."}, {"fullname_first_author": "Jerry Chee", "paper_title": "QUIP: 2-bit quantization of large language models with guarantees", "publication_date": "2024-00-00", "reason": "This work introduces QUIP#, a fourth crucial quantization technique evaluated within the research, providing a critical component of the comparative study."}]}