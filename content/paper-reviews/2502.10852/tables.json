[{"content": "| Model | Size | Sum F | Sum P | Sum R | MRC F | MRC P | MRC R | MT F | MT P | MT R |\n|---|---|---|---|---|---|---|---|---|---|---|\n| MC2-LLaMA-13B | 13B | 16.1 | 12.3 | 15.5 | 13.2 | 11.7 | 13.1 | 15.1 | 12.2 | 16.8 |\n| mBART-CM | 611M | 8.6 | 11.2 | 15.2 | 7.9 | 6.1 | 5.6 | 11.5 | 7.3 | 9.3 |\n| XLM-SWCM (ours) | 457M | **25.7** | **29.1** | **24.2** | **16.4** | **29.5** | **16.2** | **24.5** | **26.3** | **24.3** |", "caption": "Table 1: \nPerformance metrics of the baseline models, evaluated using three ROUGE-L sub metrics:\nF (F1-score), P (precision),\nand R (recall). Size refers to the number of parameters in each model.", "description": "\ud45c 1\uc740 \uc138 \uac00\uc9c0 ROUGE-L \ud558\uc704 \uc9c0\ud45c(F1 \uc810\uc218, \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ub41c \uae30\uc900 \ubaa8\ub378\uc758 \uc131\ub2a5 \uc9c0\ud45c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378\uc758 \ud06c\uae30\ub294 \uac01 \ubaa8\ub378\uc758 \ud30c\ub77c\ubbf8\ud130 \uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ubcf8 \ud45c\ub294 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc774 \uc800\uc790\uc6d0 \uc5b8\uc5b4\uc5d0 \ub300\ud55c \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \uc791\uc5c5\uc5d0\uc11c \uc5b4\ub5a4 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294\uc9c0 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \ud2b9\ud788,  XLM-SWCM \ubaa8\ub378\uc774 \uae30\uc874\uc758 \ub300\uaddc\ubaa8 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud588\uc744 \ub54c \ud30c\ub77c\ubbf8\ud130 \uc218\ub294 \uc801\uc9c0\ub9cc \uc131\ub2a5\uc740 \uc6b0\uc218\ud558\ub2e4\ub294 \uc810\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc911\uc694\ud55c \uacb0\uacfc\ub97c \ub2f4\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.2.2 \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Model | Zh Sum | Zh MRC | Bo Sum | Bo MRC | Ug Sum | Ug MRC | Mn Sum | Mn MRC | Kk Sum | Kk MRC |\n|---|---|---|---|---|---|---|---|---|---|---|\n| MC2-LLaMA-13B | 47.1 | 43.5 | 9.5 | 6.1 | 3.5 | 2.4 | 3.7 | 2.2 | 2.6 | 3.9 |\n| MC2-LLaMA-13B* | 47.3 | 44.7 | 13.1 | 11.5 | 11.7 | 10.1 | 9.7 | 10.2 | 2.9 | 4.6 |\n| mBART-CM | 32.7 | 25.6 | 6.8 | 2.1 | 2.7 | 2.2 | 3.1 | 1.7 | 0.2 | 0.1 |\n| XLM-SWCM (ours) | 33.1 | 23.5 | 17.1 | 11.1 | 12.5 | 11.1 | 13.5 | 7.2 | 5.6 | 6.9 |", "caption": "Table 2: \nCross-lingual Transfer performance of different models on Text Summarization (Sum) and Machine Reading Comprehension (MRC) tasks, evaluated using ROUGE-L. The best results for each task are highlighted. *\u00a0indicates explicitly prompting MC2-LLaMA-13B with the language to be used in the response during evaluation.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uad50\ucc28 \uc5b8\uc5b4 \uc804\uc774 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud14d\uc2a4\ud2b8 \uc694\uc57d(Sum) \ubc0f \uae30\uacc4 \ub3c5\ud574(MRC) \uc791\uc5c5\uc5d0 \ub300\ud55c ROUGE-L \ud3c9\uac00 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \ucd5c\uace0\uc758 \uacb0\uacfc\uac00 \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  * \ud45c\uc2dc\ub294 \ud3c9\uac00 \uc911 \uc751\ub2f5\uc5d0 \uc0ac\uc6a9\ud560 \uc5b8\uc5b4\ub97c \uba85\uc2dc\uc801\uc73c\ub85c MC2-LLaMA-13B \ubaa8\ub378\uc5d0 \uc9c0\uc2dc\ud588\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc989, \uc800\uc790\ub4e4\uc740 MC2-LLaMA-13B \ubaa8\ub378\uc5d0 \ub300\ud574 \ucd94\uac00\uc801\uc778 \uc870\uac74\uc744 \uc8fc\uc5b4 \uc131\ub2a5\uc744 \ub192\uc774\uace0\uc790 \ud588\uc2b5\ub2c8\ub2e4.", "section": "4.2.2 \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Removing Module | Sum | MRC | MT |\n|---|---|---|---|\n| None (XLM-SWCM) | **25.7** | **16.4** | **24.5** |\n| MT | 25.6 | 15.1 | 20.3 |\n| DAE | 22.4 | 12.2 | 18.7 |\n| WS | 17.1 | 11.7 | 18.2 |\n| MT + DAE | 22.5 | 12.3 | 17.7 |\n| MT + WS | 17.5 | 11.3 | 18.4 |\n| DAE + WS | 15.2 | 11.9 | 17.1 |\n| MT + DAE + WS | 15.9 | 10.8 | 16.5 |", "caption": "Table 3: \nObjective ablation results, evaluated using ROUGE-L.\nThe experiments involve removing different combinations of training components, such as Machine Translation (MT), DAE (Denoising Auto-Encoding), and Weight Sharing (WS).", "description": "\uc774 \ud45c\ub294 ROUGE-L \uc810\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ub41c \ubaa9\ud45c \uc81c\uac70 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\uacc4 \ubc88\uc5ed(MT), DAE(Denoising Auto-Encoding), \uac00\uc911\uce58 \uacf5\uc720(WS)\uc640 \uac19\uc740 \ub2e4\uc591\ud55c \ud6c8\ub828 \uad6c\uc131 \uc694\uc18c\ub97c \uc81c\uac70\ud558\ub294 \uc2e4\ud5d8\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4.  \uac01 \uad6c\uc131 \uc694\uc18c\uc758 \uc81c\uac70\uac00 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc5d0 \uac00\uc7a5 \uc911\uc694\ud55c \uc694\uc18c\ub97c \ud30c\uc545\ud569\ub2c8\ub2e4.", "section": "3.2 Pretraining"}, {"content": "| Model | Sum | MRC | MT |\n|---|---|---|---|\n| Cino-Transformer | 18.9 | 13.5 | 18.7 |\n| XLM-SWCM (ours) | **25.7** | **16.4** | **24.5** |", "caption": "Table 4: \nPerformance metrics of the Ablation of Weight Initialization, evaluated using the ROUGE-L metric.", "description": "\ud45c 4\ub294 \uac00\uc911\uce58 \ucd08\uae30\ud654 \uc81c\uac70 \uc2e4\ud5d8 \uacb0\uacfc\ub97c ROUGE-L \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Cino-Transformer \ubaa8\ub378\uc740 \ub514\ucf54\ub354\uac00 \ubb34\uc791\uc704\ub85c \ucd08\uae30\ud654\ub41c \ubc18\uba74, XLM-SWCM \ubaa8\ub378\uc740 \uc778\ucf54\ub354\uc758 \uac00\uc911\uce58\ub97c \ud65c\uc6a9\ud558\uc5ec \ub514\ucf54\ub354\ub97c \ucd08\uae30\ud654\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub450 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uac00\uc911\uce58 \uacf5\uc720 \uba54\ucee4\ub2c8\uc998\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.2 \uac00\uc911\uce58 \ucd08\uae30\ud654 \uc81c\uac70"}, {"content": "| Model | Sum | MRC | MT |\n|---|---|---|---|\n| BASE-A | 13.7 | 10.3 | 15.7 |\n| BASE-B | 16.3 | 14.1 | 21.1 |\n| XLM-SWCM (ours) | **25.7** | **16.4** | **24.5** |", "caption": "Table 5: \nPerformance metrics of the Ablation of Normal Layers, evaluated using the ROUGE-L metric.\nBASE-A has fewer layers and does not include any normal layers, while BASE-B maintains the same number of layers as XLM-SWCM but uses weight duplication instead of normal layers.", "description": "\ud45c 5\ub294 ROUGE-L \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ub41c \uc815\uaddc\ud654 \uacc4\uce35\uc758 \uc81c\uac70\uc5d0 \ub300\ud55c \uc131\ub2a5 \uc9c0\ud45c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. BASE-A\ub294 \uacc4\uce35 \uc218\uac00 \uc801\uace0 \uc815\uaddc\ud654 \uacc4\uce35\uc774 \ud3ec\ud568\ub418\uc9c0 \uc54a\uc740 \ubc18\uba74, BASE-B\ub294 XLM-SWCM\uacfc \ub3d9\uc77c\ud55c \uacc4\uce35 \uc218\ub97c \uc720\uc9c0\ud558\uc9c0\ub9cc \uc815\uaddc\ud654 \uacc4\uce35 \ub300\uc2e0 \uac00\uc911\uce58 \ubcf5\uc81c\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc815\uaddc\ud654 \uacc4\uce35\uc758 \uc874\uc7ac\uc640 \uac00\uc911\uce58 \ucd08\uae30\ud654 \uc804\ub7b5\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.2 \uad6c\uc870 \uc81c\uac70"}, {"content": "| Language | Data Size | Number of Samples |\n|---|---|---|\n| Tibetan | 2.2 GB | 184,045 |\n| Uyghur | 736 MB | 90,441 |\n| Kazakh | 937 MB | 57,827 |\n| Mongolian | 970 MB | 171,847 |\n| Simplified Chinese | 2.1 GB | 836,075 |", "caption": "Table 6: \nStatistics of our pretraining dataset.", "description": "\ud45c 6\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc0ac\uc804 \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uc758 \ud1b5\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc5b8\uc5b4(\ud2f0\ubca0\ud2b8\uc5b4, \uc704\uad6c\ub974\uc5b4, \uce74\uc790\ud750\uc5b4, \ubabd\uace8\uc5b4, \uac04\uccb4 \uc911\uad6d\uc5b4)\uc5d0 \ub300\ud574 \ub370\uc774\ud130\uc14b \ud06c\uae30(GB \ub610\ub294 MB)\uc640 \uc0d8\ud50c \uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ubaa8\ub378\uc758 \uc0ac\uc804 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc758 \uaddc\ubaa8\uc640 \uc5b8\uc5b4\ubcc4 \ubd84\ud3ec\ub97c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "A Dataset Details"}]