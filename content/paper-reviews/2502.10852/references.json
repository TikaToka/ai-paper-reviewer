{"references": [{"fullname_first_author": "Aakanksha Chowdhery", "paper_title": "Palm: Scaling language modeling with pathways", "publication_date": "2023-XX-XX", "reason": "This paper introduces PaLM, a large language model that significantly advanced the state-of-the-art in multilingual capabilities and serves as a key comparison point for the current work."}, {"fullname_first_author": "Alexis Conneau", "paper_title": "Unsupervised cross-lingual representation learning at scale", "publication_date": "2020-07-05", "reason": "This paper introduced XLM-R, a foundational multilingual language model that the current work builds upon and improves, particularly for low-resource languages."}, {"fullname_first_author": "Yinhan Liu", "paper_title": "Multilingual denoising pre-training for neural machine translation", "publication_date": "2020-XX-XX", "reason": "This paper introduced the mBART model, a significant advancement in multilingual machine translation which is used as a baseline and compared against in the current work."}, {"fullname_first_author": "Linting Xue", "paper_title": "mt5: A massively multilingual pre-trained text-to-text transformer", "publication_date": "2021-06-06", "reason": "This paper introduced the mT5 model, another key multilingual model used for comparison, highlighting advancements in text-to-text capabilities."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-07", "reason": "This paper details LLaMA 2, a large language model that is a significant model in the field, serving as a strong comparative point for evaluating the performance of the newly proposed model."}]}