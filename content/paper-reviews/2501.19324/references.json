{"references": [{"fullname_first_author": "Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for large language models and their capabilities, influencing the development of many subsequent LLMs."}, {"fullname_first_author": "Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-26", "reason": "This paper establishes the scaling laws that govern the performance of LLMs, a crucial concept for understanding the trade-offs between model size and performance."}, {"fullname_first_author": "Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-23", "reason": "This paper presents a new approach to training LLMs that considers compute costs and optimal resource allocation, which is relevant to the efficiency focus of this paper."}, {"fullname_first_author": "Leviathan", "paper_title": "Fast inference from transformers via speculative decoding", "publication_date": "2023-07-01", "reason": "This paper introduces speculative decoding, a technique directly related to and improved upon in this paper, making it a key prior work for the field."}, {"fullname_first_author": "Chen", "paper_title": "Accelerating large language model decoding with speculative sampling", "publication_date": "2023-02-01", "reason": "This paper introduces a speculative sampling method that is closely related to the proposed RSD, and offers insights on the methods and challenges of speculative decoding."}]}