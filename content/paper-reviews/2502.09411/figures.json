[{"figure_path": "https://arxiv.org/html/2502.09411/x2.png", "caption": "Figure 1: \nUsing references broadens the generation capabilities of image generation models. Given a text prompt, ImageRAG dynamically retrieves relevant images and provides them to a base text-to-image model (T2I).\nImageRAG works with different models, such as SDXL (A) or OmniGen (B, C), and different controls, e.g. text (A, B) or personalization (C).", "description": "\uc774 \uadf8\ub9bc\uc740 ImageRAG\uac00 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubaa8\ub378\uc758 \uae30\ub2a5\uc744 \uc5b4\ub5bb\uac8c \ud655\uc7a5\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\uac00 \uc8fc\uc5b4\uc9c0\uba74 ImageRAG\ub294 \uad00\ub828 \uc774\ubbf8\uc9c0\ub97c \ub3d9\uc801\uc73c\ub85c \uac80\uc0c9\ud558\uc5ec \uae30\ubcf8 \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \ubaa8\ub378(T2I)\uc5d0 \uc81c\uacf5\ud569\ub2c8\ub2e4. ImageRAG\ub294 SDXL(A) \ub610\ub294 OmniGen(B, C)\uacfc \uac19\uc740 \ub2e4\uc591\ud55c \ubaa8\ub378 \ubc0f \ud14d\uc2a4\ud2b8(A, B) \ub610\ub294 \uac1c\uc778\ud654(C)\uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uc81c\uc5b4 \ubc29\uc2dd\uacfc \ud568\uaed8 \uc791\ub3d9\ud569\ub2c8\ub2e4.  (A)\ub294 SDXL \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec  \"\uace8\ub4e0 \ub9ac\ud2b8\ub9ac\ubc84\uc640 \uc694\ub78c\" \uc774\ub77c\ub294 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud55c \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uacfc\uc815\uc744, (B)\ub294 OmniGen \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \"\uc720\uce58\uc6d0\uc5d0\uc11c \ub180\uace0 \uc788\ub294 \ub808\ub4dc\ud310\ub2e4 \uc0ac\uc9c4\" \uc774\ub77c\ub294 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud55c \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uacfc\uc815\uc744, (C)\ub294 OmniGen \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec  \"\ud560\ub85c\uc708 \uc758\uc0c1\uc744 \uc785\uace0 \uc788\ub294 \ubaa8\uc2b5\" \uc774\ub77c\ub294 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud55c \uac1c\uc778\ud654\ub41c \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uacfc\uc815\uc5d0\uc11c \uac80\uc0c9\ub41c \ucc38\uc870 \uc774\ubbf8\uc9c0\uac00 \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc5d0 \uc5b4\ub5bb\uac8c \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.09411/x3.png", "caption": "Figure 2: \nHallucinations. When models do not know the meaning of a prompt, they may \u201challucinate\u201d and generate unrelated images (left).\nBy applying our method to retrieve and utilize relevant references (mid), the base models can generate appropriate images (right).", "description": "\uc774 \uadf8\ub9bc\uc740 \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubaa8\ub378\uc774 \ud504\ub86c\ud504\ud2b8\uc758 \uc758\ubbf8\ub97c \uc774\ud574\ud558\uc9c0 \ubabb\ud560 \ub54c \ubc1c\uc0dd\ud558\ub294 '\ud658\uac01' \ud604\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc5d0\ub294 \ubaa8\ub378\uc774 \ud504\ub86c\ud504\ud2b8\uc640 \uad00\ub828\uc5c6\ub294 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\ub294 \uc608\uc2dc\uac00 \ub098\uc640\uc788\uace0, \uac00\uc6b4\ub370\uc5d0\ub294 \uc5f0\uad6c\ud300\uc774 \uc81c\uc548\ud55c \ubc29\ubc95(ImageRAG)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uad00\ub828 \uc774\ubbf8\uc9c0\ub97c \uac80\uc0c9\ud558\uace0 \ud65c\uc6a9\ud558\uc5ec \uc0dd\uc131 \uacfc\uc815\uc744 \uc548\ub0b4\ud558\ub294 \uacfc\uc815\uc774 \ub098\uc640\uc788\uc2b5\ub2c8\ub2e4. \uc624\ub978\ucabd\uc5d0\ub294 ImageRAG\ub97c \uc801\uc6a9\ud558\uc5ec \ud504\ub86c\ud504\ud2b8\uc5d0 \uc801\ud569\ud55c \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud55c \uacb0\uacfc\uac00 \ubcf4\uc5ec\uc9d1\ub2c8\ub2e4.", "section": "2. Hallucinations"}, {"figure_path": "https://arxiv.org/html/2502.09411/x4.png", "caption": "Figure 3: \nTop: a high-level overview of our method. Given a text prompt <\u2062p\u2062>\ud835\udc5d\\mathord{<}p\\mathord{>}< italic_p >, we generate an initial image using a text-to-image (T2I) model. Then, we generate retrieval-captions <\u2062cj\u2062>subscript\ud835\udc50\ud835\udc57\\mathord{<}c_{j}\\mathord{>}< italic_c start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT >, retrieve images from an external database for each caption <\u2062ij\u2062>subscript\ud835\udc56\ud835\udc57\\mathord{<}i_{j}\\mathord{>}< italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT >, and use them as references to the model for better generation.\nBottom: the retrieval-caption generation block.\nWe use a VLM to decide if the initial image matches the given prompt. If not, we ask it to list the missing concepts, and to create a caption that could be used to retrieve appropriate examples for each of these missing concepts.", "description": "\uadf8\ub9bc 3\uc740 ImageRAG \ubc29\ubc95\uc758 \uac1c\ub150\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc704\ucabd \ubd80\ubd84\uc740 ImageRAG\uc758 \uc804\uccb4\uc801\uc778 \ud750\ub984\uc744 \uac04\ub7b5\ud558\uac8c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8(p)\ub97c \uc785\ub825\ubc1b\uc544 \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubaa8\ub378(T2I)\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucd08\uae30 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uadf8\ub7f0 \ub2e4\uc74c, \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\uc640 \ud504\ub86c\ud504\ud2b8\uac00 \uc77c\uce58\ud558\ub294\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud574 Vision-Language Model(VLM)\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc77c\uce58\ud558\uc9c0 \uc54a\ub294 \uacbd\uc6b0, VLM\uc740 \ub204\ub77d\ub41c \uac1c\ub150\ub4e4\uc744 \ud30c\uc545\ud558\uace0, \uac01 \uac1c\ub150\uc5d0 \ub9de\ub294 \uc774\ubbf8\uc9c0\ub97c \uac80\uc0c9\ud560 \uc218 \uc788\ub294 \ucea1\uc158(cj)\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uc774\ub807\uac8c \uc0dd\uc131\ub41c \ucea1\uc158\uc744 \uc774\uc6a9\ud574 \uc678\ubd80 \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc5d0\uc11c \uad00\ub828 \uc774\ubbf8\uc9c0(ij)\ub97c \uac80\uc0c9\ud558\uace0, \uc774 \uc774\ubbf8\uc9c0\ub4e4\uc744 T2I \ubaa8\ub378\uc5d0 \ucd94\uac00\uc801\uc778 \ucc38\uace0 \uc790\ub8cc\ub85c \uc81c\uacf5\ud558\uc5ec \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uacfc\uc815\uc744 \uc548\ub0b4\ud569\ub2c8\ub2e4.  \uc544\ub798\ucabd \ubd80\ubd84\uc740 \uc774\ub7ec\ud55c \ucea1\uc158 \uc0dd\uc131 \uacfc\uc815\uc744 \uc790\uc138\ud788 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ucd08\uae30 \uc774\ubbf8\uc9c0\uc640 \ud504\ub86c\ud504\ud2b8\ub97c VLM\uc5d0 \uc785\ub825\ud558\uc5ec \uc77c\uce58 \uc5ec\ubd80\ub97c \ud310\ub2e8\ud558\uace0, \uc77c\uce58\ud558\uc9c0 \uc54a\uc73c\uba74 \ub204\ub77d\ub41c \uac1c\ub150\ub4e4\uc744 \uc2dd\ubcc4\ud558\uc5ec \uc801\uc808\ud55c \uc774\ubbf8\uc9c0\ub97c \uac80\uc0c9\ud560 \uc218 \uc788\ub294 \ucea1\uc158\uc744 \uc0dd\uc131\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.09411/x5.png", "caption": "Figure 4: Personalized generation example.\nImageRAG can work in parallel with personalization methods and enhance their capabilities. For example, although OmniGen can generate images of a subject based on an image, it struggles to generate some concepts. Using references retrieved by our method, it can generate the required result.", "description": "\uadf8\ub9bc 4\ub294 ImageRAG\uac00 \uac1c\uc778\ud654\ub41c \uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubc29\ubc95\uacfc \ubcd1\ud589\ud558\uc5ec \uc0ac\uc6a9\ub420 \ub54c \uadf8 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  OmniGen\uacfc \uac19\uc740 \ubaa8\ub378\uc740 \uae30\uc874 \uc774\ubbf8\uc9c0\ub97c \uae30\ubc18\uc73c\ub85c \ud2b9\uc815 \uac1d\uccb4\uc758 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc9c0\ub9cc, \ud2b9\uc815 \uac1c\ub150\uc744 \uc0dd\uc131\ud558\ub294 \ub370\ub294 \uc5b4\ub824\uc6c0\uc744 \uacaa\uc2b5\ub2c8\ub2e4.  ImageRAG\ub294 \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 \uad00\ub828 \uc774\ubbf8\uc9c0\ub97c \uac80\uc0c9\ud558\uc5ec \ucd94\uac00 \uc815\ubcf4\ub85c \uc81c\uacf5\ud568\uc73c\ub85c\uc368,  OmniGen\uc774 \uc6d0\ud558\ub294 \uac1c\ub150\uc758 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\ub3c4\ub85d \ub3d5\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c\ub294 \ud2b9\uc815 \uace0\uc591\uc774 \uc0ac\uc9c4\uc744 \uae30\ubc18\uc73c\ub85c,  ImageRAG\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \uc0c1\ud669(\uba38\uadf8\ucef5\uc5d0 \uadf8\ub824\uc9c4 \uace0\uc591\uc774, \ub808\uace0 \uace0\uc591\uc774, \uace0\uc591\uc774\uac00 \uc218\uc5c5\uc744 \ud558\ub294 \uc7a5\uba74 \ub4f1)\uc5d0\uc11c \uace0\uc591\uc774 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\ub294 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. \uad6c\ud604 \uc138\ubd80 \uc815\ubcf4"}, {"figure_path": "https://arxiv.org/html/2502.09411/x6.png", "caption": "Figure 5: Retrieval dataset size vs. CLIP score on ImageNet (left) and Aircraft (right).\nDashed lines represent the scores of the base models.\nEven relatively small, unspecialized retrieval sets can already improve results. More data leads to further increased scores. However, small sets may not contain relevant retrieval examples, and their use may harm results, particularly for stronger models.", "description": "\uadf8\ub9bc 5\ub294 ImageNet\uacfc Aircraft \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uac80\uc0c9 \ub370\uc774\ud130\uc14b \ud06c\uae30\uc640 CLIP \uc810\uc218 \uac04\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc810\uc120\uc740 \uae30\ubcf8 \ubaa8\ub378\uc758 \uc810\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ube44\uad50\uc801 \uc791\uace0 \uc804\ubb38\ud654\ub418\uc9c0 \uc54a\uc740 \uac80\uc0c9 \ub370\uc774\ud130\uc14b\uc774\ub77c\ub3c4 \uacb0\uacfc\ub97c \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub370\uc774\ud130\uac00 \ub9ce\uc744\uc218\ub85d \uc810\uc218\uac00 \ub354 \ub192\uc544\uc9d1\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc791\uc740 \ub370\uc774\ud130\uc14b\uc5d0\ub294 \uad00\ub828 \uac80\uc0c9 \uc608\uc2dc\uac00 \ud3ec\ud568\ub418\uc9c0 \uc54a\uc744 \uc218 \uc788\uc73c\uba70, \ud2b9\ud788 \uac15\ub825\ud55c \ubaa8\ub378\uc758 \uacbd\uc6b0 \uacb0\uacfc\uc5d0 \ud574\ub97c \ub07c\uce60 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.09411/x7.png", "caption": "Figure 6: User study results. Users preference percentage of our method compared to other methods in terms of text alignment, visual quality, and overall preference.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \uc0ac\uc6a9\uc790 \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0ac\uc6a9\uc790\ub4e4\uc740 \uc81c\uc2dc\ub41c \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud55c \ud14d\uc2a4\ud2b8 \uc815\ub82c, \uc2dc\uac01\uc801 \ud488\uc9c8 \ubc0f \uc804\ubc18\uc801\uc778 \uc120\ud638\ub3c4 \uce21\uba74\uc5d0\uc11c ImageRAG \ubc29\ubc95\uc744 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uacfc \ube44\uad50 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. \uac01 \uae30\uc900\uc5d0 \ub530\ub77c ImageRAG \ubc29\ubc95\uc774 \ub2e4\ub978 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \ub354 \ub192\uc740 \uc120\ud638\ub3c4\ub97c \uc5bb\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.4 \uc9c8\uc801 \ube44\uad50"}, {"figure_path": "https://arxiv.org/html/2502.09411/x8.png", "caption": "Figure 7: Qualitative comparisons: rare concept generation. Examples from ImageNet\u00a0(Deng et\u00a0al., 2009), CUB\u00a0(Wah et\u00a0al., 2011) and iNaturalist\u00a0(Van\u00a0Horn et\u00a0al., 2018). The left-most image column is the retrieved reference using ImageRAG for each prompt. OmniGen and SDXL both struggle with the uncommon concepts, sometimes generating similar concepts such as a bull or a cow instead of the dog breed \u201cBoston bull\u201d, while in other times, they generate completely unrelated images, as in the case of \u201cChow\u201d, or \u201cGeococcyx\u201d. When using ImageRAG both models generate the correct concept.", "description": "\uadf8\ub9bc 7\uc740 ImageRAG, OmniGen, SDXL \uc138 \uac00\uc9c0 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub4dc\ubb38 \uac1c\ub150\uc758 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud55c \uacb0\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \uac83\uc785\ub2c8\ub2e4. ImageNet, CUB, iNaturalist \ub370\uc774\ud130\uc14b\uc758 \ub4dc\ubb38 \uac1c\ub150(Chow Chow \uac15\uc544\uc9c0, Boston Bull \uac15\uc544\uc9c0, Geococcyx \ub4f1)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud5d8\ud558\uc600\uc2b5\ub2c8\ub2e4.  \uc67c\ucabd \uc5f4\uc740 ImageRAG\uac00 \uac01 \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud574 \uac80\uc0c9\ud55c \ucc38\uc870 \uc774\ubbf8\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  OmniGen\uacfc SDXL \ubaa8\ub378\uc740 \ub4dc\ubb38 \uac1c\ub150\uc744 \uc0dd\uc131\ud558\ub294 \ub370 \uc5b4\ub824\uc6c0\uc744 \uacaa\uc5b4, Boston Bull\uc758 \uacbd\uc6b0 \uc18c\ub098 \uc816\uc18c\uc640 \uac19\uc740 \uc720\uc0ac\ud55c \uac1c\ub150\uc744 \uc0dd\uc131\ud558\uac70\ub098 Chow Chow\ub098 Geococcyx\ucc98\ub7fc \uc644\uc804\ud788 \uad00\ub828 \uc5c6\ub294 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\ub294 \uacbd\uc6b0\uac00 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc ImageRAG\ub97c \uc0ac\uc6a9\ud558\uba74 \ub450 \ubaa8\ub378 \ubaa8\ub450 \uc815\ud655\ud55c \uac1c\ub150\uc758 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 ImageRAG\uac00 \uc774\ubbf8\uc9c0 \uac80\uc0c9 \uae30\ub2a5\uc744 \ud1b5\ud574 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubaa8\ub378\uc758 \ub4dc\ubb38 \uac1c\ub150 \uc0dd\uc131 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. Qualitative comparisons"}, {"figure_path": "https://arxiv.org/html/2502.09411/x9.png", "caption": "Figure 8: Comparisons between ImageRAG and different methods using retrieval for generation. Prompts and results of all other methods are taken from their papers. The methods we compared to are RDM\u00a0(Blattmann et\u00a0al., 2022), Re-Imagen\u00a0(Chen et\u00a0al., 2022), and KNN-Diffusion\u00a0(Sheynin et\u00a0al., 2022).", "description": "\uadf8\ub9bc 8\uc740 \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc744 \uc704\ud574 \uac80\uc0c9\uc744 \uc0ac\uc6a9\ud558\ub294 ImageRAG\uc640 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc758 \ud504\ub86c\ud504\ud2b8\uc640 \uacb0\uacfc\ub294 \ud574\ub2f9 \ub17c\ubb38\uc5d0\uc11c \uac00\uc838\uc654\uc2b5\ub2c8\ub2e4. \ube44\uad50 \ub300\uc0c1 \ubc29\ubc95\uc740 RDM(Blattmann et al., 2022), Re-Imagen(Chen et al., 2022), KNN-Diffusion(Sheynin et al., 2022)\uc785\ub2c8\ub2e4.  \uac01 \ubc29\ubc95\uc740 \ub2e4\uc591\ud55c \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud574 \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\uba70, ImageRAG\uc758 \uc131\ub2a5\uc744 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc744 \ud1b5\ud574 ImageRAG\uac00 \ubcf5\uc7a1\ud558\uac70\ub098 \ud2b9\uc774\ud55c \uac1c\ub150\uc744 \uc0dd\uc131\ud558\ub294 \ub370 \uc788\uc5b4 \ub2e4\ub978 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \uc5bc\ub9c8\ub098 \ud6a8\uacfc\uc801\uc778\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. Qualitative comparisons"}]