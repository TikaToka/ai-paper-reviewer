{"references": [{"fullname_first_author": "Schulman, J.", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-06", "reason": "This paper introduces the Proximal Policy Optimization (PPO) algorithm, a widely used and effective reinforcement learning algorithm used as a baseline in the paper's experiments."}, {"fullname_first_author": "Lowe, R.", "paper_title": "Multi-agent actor-critic for mixed cooperative-competitive environments", "publication_date": "2017-12-01", "reason": "This paper introduces the Multi-Agent Actor-Critic (MAAC) algorithm, used as a comparison in the paper's experimental section, representing a relevant approach in the multi-agent reinforcement learning field."}, {"fullname_first_author": "Foerster, J.", "paper_title": "Learning to communicate with deep multi-agent reinforcement learning", "publication_date": "2016-12-01", "reason": "This paper is highly relevant as it discusses the challenges of non-stationarity in multi-agent reinforcement learning, which the current paper addresses by proposing a novel hierarchical framework."}, {"fullname_first_author": "Levin, M.", "paper_title": "Technological approach to mind every An experimentally-grounded framework for understanding diverse bodies and minds", "publication_date": "2022-00-00", "reason": "This paper introduces the TAME approach which inspired the current paper's hierarchical multi-agent architecture, providing the biological basis for the proposed framework."}, {"fullname_first_author": "Sutton, R. S.", "paper_title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "publication_date": "1999-00-00", "reason": "This paper introduces the concept of options framework in reinforcement learning, which is highly relevant to the paper's discussion of hierarchical reinforcement learning and temporal abstraction."}]}