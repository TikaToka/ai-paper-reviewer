[{"content": "| # of n-grams | System memory | Solid-state drive |\n|---|---|---|\n| 10<sup>7</sup> | 41.4 GB | 77.3 GB |\n| 10<sup>8</sup> | 413.6 GB | 766.8 GB |\n| 10<sup>9</sup> | (does not fit) | 7665.4 GB |\n| Price (per GB) | \u223c 2 USD | \u223c 0.1 USD |", "caption": "Table 1: Space usage of the f-gram embedding layer \u2131\u2131\\mathcal{F}caligraphic_F, along with cost for memory and NVMe solid-state drives (McCallum, 2024).", "description": "\ud45c 1\uc740 f-gram \uc784\ubca0\ub529 \ub808\uc774\uc5b4 (\u2131)\uc758 \uacf5\uac04 \uc0ac\uc6a9\ub7c9\uacfc \uba54\ubaa8\ub9ac \ubc0f NVMe \uc194\ub9ac\ub4dc \uc2a4\ud14c\uc774\ud2b8 \ub4dc\ub77c\uc774\ube0c \ube44\uc6a9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud06c\uae30\uc758 f-gram(10\u2077, 10\u2078, 10\u2079)\uc5d0 \ub300\ud55c \uc2dc\uc2a4\ud15c \uba54\ubaa8\ub9ac\uc640 SSD \uc800\uc7a5 \uc6a9\ub7c9\uc744 \ubcf4\uc5ec\uc8fc\uba70, \uac01 \uc800\uc7a5 \ub9e4\uccb4\uc5d0 \ub300\ud55c GB\ub2f9 \uac00\uaca9\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc758 \ud06c\uae30 \ubcc0\ud654\uc5d0 \ub530\ub978 \uc800\uc7a5 \uacf5\uac04 \ubc0f \ube44\uc6a9 \ud6a8\uc728\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.", "section": "3.3 Space Usage and Query Latency"}, {"content": "Model size|c4-en|books|common-crawl|pes2o|reddit|stack|wiki|ice|m2de-s2orc|pile|wikitext-103|Average\n---|---|---|---|---|---|---|---|---|---|---|---|---\n1B baseline|16.813|21.570|16.752|11.682|22.612|3.360|14.453|15.281|27.900|10.429|16.053|16.082\n+10M V<sub>f-gram</sub> (0.6B \ud835\udc9c<sub>f-gram</sub>)|16.087|20.963|16.039|11.270|21.797|3.274|13.777|14.979|26.361|10.128|15.371|15.459\n+10M V<sub>f-gram</sub> (1.8B \ud835\udc9c<sub>f-gram</sub>)|15.727|20.429|15.473|11.124|21.388|3.231|13.454|14.709|25.785|9.956|15.104|15.125\n+1B V<sub>f-gram</sub> (0.6B \ud835\udc9c<sub>f-gram</sub>)|15.846|20.593|15.684|11.071|21.411|3.213|13.543|14.702|26.026|9.889|15.077|15.187\n+1B V<sub>f-gram</sub> (1.8B \ud835\udc9c<sub>f-gram</sub>)|15.158|19.680|14.857|10.761|20.757|3.133|12.964|14.220|24.958|9.553|14.354|14.581\n1.3B baseline|15.994|20.157|15.921|11.148|21.634|3.248|13.721|14.651|26.583|9.927|15.143|15.284\n+10M V<sub>f-gram</sub> (0.6B \ud835\udc9c<sub>f-gram</sub>)|15.509|19.816|15.407|10.887|21.022|3.192|13.260|14.372|25.450|9.757|14.616|14.844\n+10M V<sub>f-gram</sub> (1.8B \ud835\udc9c<sub>f-gram</sub>)|15.193|19.587|14.995|10.795|20.735|3.171|13.071|14.272|25.258|9.674|14.438|14.654\n+1B V<sub>f-gram</sub> (0.6B \ud835\udc9c<sub>f-gram</sub>)|15.270|19.510|15.106|10.707|20.763|3.139|13.073|14.177|25.009|9.546|14.397|14.609\n+1B V<sub>f-gram</sub> (1.8B \ud835\udc9c<sub>f-gram</sub>)|14.803|18.996|14.541|10.502|20.296|3.085|12.637|13.971|24.533|9.357|13.971|14.245\n1.9B baseline|15.270|19.017|15.184|10.719|20.752|3.163|13.119|14.095|25.461|9.570|14.229|14.598", "caption": "Table 2: Perplexity (lower is better) on the OLMo evaluation mixture. All models are trained for 200B tokens. Scone consistently improves language modeling performance across all evaluation corpora. With 10M Vf\u2062-\u2062gramsubscript\ud835\udc49f-gramV_{\\mathrm{f\\text{-}gram}}italic_V start_POSTSUBSCRIPT roman_f - roman_gram end_POSTSUBSCRIPT, a 1.3B model matches the performance of the 1.9B baseline. Similarly, with 1B Vf\u2062-\u2062gramsubscript\ud835\udc49f-gramV_{\\mathrm{f\\text{-}gram}}italic_V start_POSTSUBSCRIPT roman_f - roman_gram end_POSTSUBSCRIPT, a 1B model matches the 1.9B baseline.", "description": "\ud45c 2\ub294 OLMo \ud3c9\uac00 \ud63c\ud569\ubb3c\uc5d0 \ub300\ud55c perplexity(\ub0ae\uc744\uc218\ub85d \uc88b\uc74c)\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \ubaa8\ub378\uc740 200B \ud1a0\ud070\uc73c\ub85c \ud559\uc2b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4. SCONE\uc740 \ubaa8\ub4e0 \ud3c9\uac00 \ub9d0\ubb49\uce58\uc5d0\uc11c \uc5b8\uc5b4 \ubaa8\ub378\ub9c1 \uc131\ub2a5\uc744 \uc77c\uad00\ub418\uac8c \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4. 10M\uac1c\uc758 Vf-gram\uc744 \uc0ac\uc6a9\ud558\uba74 1.3B \ubaa8\ub378\uc774 1.9B \uae30\uc900 \ubaa8\ub378\uc758 \uc131\ub2a5\uacfc \uc77c\uce58\ud569\ub2c8\ub2e4. \ub9c8\ucc2c\uac00\uc9c0\ub85c, 1B\uac1c\uc758 Vf-gram\uc744 \uc0ac\uc6a9\ud558\uba74 1B \ubaa8\ub378\uc774 1.9B \uae30\uc900 \ubaa8\ub378\uacfc \uc77c\uce58\ud569\ub2c8\ub2e4.", "section": "4.2 OLMo \ud1a0\ud070\ud654\ub41c \ud559\uc2b5 \ub9d0\ubb49\uce58 \ud655\uc7a5"}, {"content": "| Parameters (million) | d_model | ffw_size | n_layers |\n|---|---|---|---|\n| 128 | 1024 | 4096 | 6 |\n| 204 | 1024 | 4096 | 12 |\n| 491 | 1536 | 6144 | 12 |\n| 759 | 1536 | 6144 | 24 |\n| 589 | 1536 | 6144 | 18 |\n| 1099 | 1536 | 6144 | 36 |", "caption": "Table 3: Baseline model configurations for pre-training on WebText. For constructing the f-gram model (\ud835\udc9cf\u2062-\u2062gramsubscript\ud835\udc9cf-gram\\mathcal{A}_{\\mathrm{f\\text{-}gram}}caligraphic_A start_POSTSUBSCRIPT roman_f - roman_gram end_POSTSUBSCRIPT), we vary the number of layers in the 128M, 491M, and 589M variants and discard the token embedding layer.", "description": "\uc774 \ud45c\ub294 WebText \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud55c \uc0ac\uc804 \ud6c8\ub828\uc744 \uc704\ud55c \uae30\ubcf8 \ubaa8\ub378 \uad6c\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  f-gram \ubaa8\ub378(\ud835\udc9cf\u2062-\u2062gramsubscript\ud835\udc9cf-gram\nmathcal{A}_{\nmathrm{f\n-\ngram}}\n)\uc744 \uad6c\uc131\ud558\uae30 \uc704\ud574 128M, 491M, 589M \ub9e4\uac1c\ubcc0\uc218\ub97c \uac00\uc9c4 \ubaa8\ub378\uc758 \ub808\uc774\uc5b4 \uc218\ub97c \ubcc0\uacbd\ud558\uace0 \ud1a0\ud070 \uc784\ubca0\ub529 \ub808\uc774\uc5b4\ub97c \uc81c\uc678\ud588\uc2b5\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218, d_model, ffw_size, \ub808\uc774\uc5b4 \uc218\ub97c \uba85\uc2dc\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\uc5b4 WebText \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \ubaa8\ub378 \uc0ac\uc804 \ud6c8\ub828 \uad6c\uc131\uc744 \uc790\uc138\ud788 \uc124\uba85\ud569\ub2c8\ub2e4.", "section": "E.1 WebText"}, {"content": "| Parameters (million) | d_model | ffw_size | n_layers |\n|---|---|---|---| \n| 711 | 2048 | 8192 | 12 |\n| 1014 | 2048 | 8192 | 18 |\n| 1316 | 2048 | 8192 | 24 |\n| 1920 | 2048 | 8192 | 36 |", "caption": "Table 4: Baseline model configurations for pre-training on OLMo corpus. For constructing the f-gram model (\ud835\udc9cf\u2062-\u2062gramsubscript\ud835\udc9cf-gram\\mathcal{A}_{\\mathrm{f\\text{-}gram}}caligraphic_A start_POSTSUBSCRIPT roman_f - roman_gram end_POSTSUBSCRIPT), we use the 711M and 1920M configurations and discard the token embedding layers.", "description": "\ud45c 4\ub294 OLMo \ub9d0\ubb49\uce58 \uc0ac\uc804 \ud6c8\ub828\uc744 \uc704\ud55c \uae30\uc900 \ubaa8\ub378 \uad6c\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. f-gram \ubaa8\ub378 (Af-gram)\uc744 \uad6c\uc131\ud558\uae30 \uc704\ud574 711M \ubc0f 1920M \uad6c\uc131\uc744 \uc0ac\uc6a9\ud558\uace0 \ud1a0\ud070 \uc784\ubca0\ub529 \uacc4\uce35\uc740 \uc81c\uc678\ud569\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ubaa8\ub378\uc758 \ud06c\uae30(\ub9e4\uac1c\ubcc0\uc218 \uc218), \uc784\ubca0\ub529 \ucc28\uc6d0(d_model), \ud53c\ub4dc\ud3ec\uc6cc\ub4dc \ub124\ud2b8\uc6cc\ud06c \ud06c\uae30(ffw_size), \ub808\uc774\uc5b4 \uc218(n_layers) \ub4f1\uc758 \uc815\ubcf4\ub97c \ub2f4\uace0 \uc788\uc2b5\ub2c8\ub2e4. OLMo \ub9d0\ubb49\uce58\ub97c \uc0ac\uc6a9\ud55c \uc0ac\uc804 \ud6c8\ub828\uc5d0 \ub300\ud55c \uc138\ubd80 \uc815\ubcf4\ub294 \ubcf8\ubb38\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8\uc801 \ud3c9\uac00"}]