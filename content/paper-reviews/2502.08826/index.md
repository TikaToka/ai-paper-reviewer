---
title: "Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation"
summary: "ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œì— ëŒ€í•œ í¬ê´„ì  ë¶„ì„: ë°ì´í„°ì…‹, í‰ê°€, ë°©ë²•ë¡ , í˜ì‹  ì œì‹œ.  ì‹¤ì œì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ì‹œìŠ¤í…œ ê°œë°œ ìœ„í•œ ê¸°ë°˜ ë§ˆë ¨."
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Multimodal Learning", "Vision-Language Models", "ğŸ¢ Sharif University of Technology",]
showSummary: true
date: 2025-02-12
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2502.08826 {{< /keyword >}}
{{< keyword icon="writer" >}} Mohammad Mahdi Abootorabi et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2025-02-18 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2502.08826" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2502.08826" target="_self" >}}
â†— Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/ask-in-any-modality-a-comprehensive-survey-on" target="_self" >}}
â†— Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

ìµœê·¼ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì •ì  í›ˆë ¨ ë°ì´í„°ì— ì˜ì¡´í•˜ê¸° ë•Œë¬¸ì— í™˜ê°(hallucination) ë° ì˜¤ë˜ëœ ì§€ì‹ê³¼ ê°™ì€ ë¬¸ì œì ì„ ì•ˆê³  ìˆìŠµë‹ˆë‹¤.  **ê²€ì¦ ê°€ëŠ¥í•œ ì¶”ë¡  ë° ì„¸ê³„ ì§€ì‹ ì—…ë°ì´íŠ¸** ë˜í•œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì‹œëœ ì†”ë£¨ì…˜ ì¤‘ í•˜ë‚˜ê°€ ë°”ë¡œ **RAG(Retrieval-Augmented Generation)**ì…ë‹ˆë‹¤. RAGëŠ” ì™¸ë¶€ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì‚¬ì‹¤ì ì´ê³  ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•¨ìœ¼ë¡œì¨ LLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ RAG ì•„í‚¤í…ì²˜ëŠ” ì£¼ë¡œ í…ìŠ¤íŠ¸ ì •ë³´ì— ì§‘ì¤‘ë˜ì–´ ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ì„ í†µí•©í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œ**ì— ëŒ€í•œ í¬ê´„ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.  **ë°ì´í„°ì…‹, í‰ê°€ ì§€í‘œ, ë²¤ì¹˜ë§ˆí¬, ë°©ë²•ë¡ , í˜ì‹ ** ë“± ë‹¤ì–‘í•œ ì¸¡ë©´ì„ ë‹¤ë£¨ë©°, ê²€ìƒ‰, ìœµí•©, ì¦ê°•, ìƒì„± ë“±ì˜ ê¸°ìˆ ì— ëŒ€í•œ ì‹¬ì¸µì ì¸ ê²€í† ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. íŠ¹íˆ, **í›ˆë ¨ ì „ëµ, ê°•ê±´ì„± í–¥ìƒ, ì†ì‹¤ í•¨ìˆ˜** ë“±ì— ëŒ€í•œ ìì„¸í•œ ë…¼ì˜ë¥¼ í†µí•´ ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œì˜ ë°œì „ì„ ì§€ì›í•˜ëŠ” ì—°êµ¬ ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.  **ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ RAG ì‹œë‚˜ë¦¬ì˜¤**ë¥¼ íƒêµ¬í•˜ê³ , **í–¥í›„ ì—°êµ¬ ë°©í–¥ ë° ê°œë°©í˜• ê³¼ì œ**ë¥¼ ë…¼ì˜í•¨ìœ¼ë¡œì¨ **ë”ìš± ê°•ë ¥í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ì‹œìŠ¤í…œ ê°œë°œ**ì— ê¸°ì—¬í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œì€ ì •ì  í›ˆë ¨ ë°ì´í„°ì— ì˜ì¡´í•˜ëŠ” LLMì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë™ì  ì™¸ë¶€ ì •ë³´ë¥¼ í†µí•©í•©ë‹ˆë‹¤. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} ë³¸ ë…¼ë¬¸ì€ ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œì— ëŒ€í•œ êµ¬ì¡°ì ì´ê³  í¬ê´„ì ì¸ ë¶„ì„ì„ ì œê³µí•˜ë©°, ë°ì´í„°ì…‹, í‰ê°€ ì§€í‘œ, ë²¤ì¹˜ë§ˆí¬, ë°©ë²•ë¡ , í˜ì‹  ë“±ì„ ë‹¤ë£¹ë‹ˆë‹¤. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œì˜ í•œê³„ì™€ í–¥í›„ ì—°êµ¬ ë°©í–¥ì„ ì œì‹œí•˜ì—¬, ë”ìš± ê°•ë ¥í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ì‹œìŠ¤í…œ ê°œë°œì„ ìœ„í•œ ê¸°ë°˜ì„ ë§ˆë ¨í•©ë‹ˆë‹¤. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
ë³¸ ë…¼ë¬¸ì€ **ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤)ë¥¼ í†µí•©í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ**ì— ëŒ€í•œ í¬ê´„ì ì¸ ì¡°ì‚¬ë¥¼ ì œê³µí•˜ì—¬, **í˜„ì¬ ì—°êµ¬ ë™í–¥ì„ íŒŒì•…í•˜ê³  í–¥í›„ ì—°êµ¬ ë°©í–¥ì„ ì œì‹œ**í•¨ìœ¼ë¡œì¨ ì—°êµ¬ìë“¤ì—ê²Œ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤. íŠ¹íˆ, **ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œì˜ í•œê³„ì™€ í–¥í›„ ê³¼ì œ**ë¥¼ ëª…í™•íˆ ì œì‹œí•˜ì—¬, **ë”ìš± ì‹ ë¢°í•  ìˆ˜ ìˆê³  íš¨ê³¼ì ì¸ AI ì‹œìŠ¤í…œ ê°œë°œ**ì„ ìœ„í•œ ê¸°ë°˜ì„ ë§ˆë ¨í•©ë‹ˆë‹¤.  **ë°ì´í„°ì…‹, í‰ê°€ ì§€í‘œ, ë²¤ì¹˜ë§ˆí¬, ë°©ë²•ë¡ , í˜ì‹  ë“±** ë‹¤ì–‘í•œ ì¸¡ë©´ì„ ë‹¤ë£¨ì–´ ì—°êµ¬ìë“¤ì´ **ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œì— ëŒ€í•œ ì¢…í•©ì ì¸ ì´í•´**ë¥¼ ë†’ì´ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.

------
#### Visual Insights



![](https://arxiv.org/html/2502.08826/extracted/6211743/MM-RAG-500.png)

> ğŸ”¼ ê·¸ë¦¼ 1ì€ ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë¹„ë””ì˜¤ ë“±)ë¥¼ ì‚¬ìš©í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) íŒŒì´í”„ë¼ì¸ì˜ ê°œìš”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ë¨¼ì € ì‚¬ìš©ì ì§ˆì˜ê°€ ì „ì²˜ë¦¬ë˜ê³ , ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ì˜ ë°ì´í„°ë² ì´ìŠ¤ì™€ í•¨ê»˜ ê³µìœ  ì„ë² ë”© ê³µê°„ì— ì¸ì½”ë”©ë©ë‹ˆë‹¤. ëª¨ë‹¬ë¦¬í‹° ì¤‘ì‹¬ ê²€ìƒ‰, ìœ ì‚¬ë„ ê²€ìƒ‰, ì¬ìˆœìœ„ ì§€ì •ê³¼ ê°™ì€ ê²€ìƒ‰ ì „ëµì„ í†µí•´ ê´€ë ¨ ë¬¸ì„œê°€ ì„ íƒë©ë‹ˆë‹¤.  ìœµí•© ë©”ì»¤ë‹ˆì¦˜ì€ ìŠ¤ì½”ì–´ ìœµí•© ë˜ëŠ” ì–´í…ì…˜ ê¸°ë°˜ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ëª¨ë‹¬ë¦¬í‹°ì˜ ë°ì´í„°ë¥¼ ì •ë ¬í•˜ê³  í†µí•©í•©ë‹ˆë‹¤. ë°˜ë³µì ì¸ ê²€ìƒ‰ê³¼ í”¼ë“œë°± ë©”ì»¤ë‹ˆì¦˜ê³¼ ê°™ì€ ì¦ê°• ê¸°ë²•ì€ ë©€í‹°ëª¨ë‹¬ LLMì„ ìœ„í•œ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë”ìš± ì„¸ë ¨ë˜ê²Œ í•©ë‹ˆë‹¤. ìƒì„± ë‹¨ê³„ì—ì„œëŠ” ì‚¬ê³  ì—°ì‡„ ì¶”ë¡  ë° ì†ŒìŠ¤ ì†ì„± ì§€ì •ê³¼ ê°™ì€ í˜ì‹ ì´ ì ìš©ë˜ì–´ ë” ë‚˜ì€ ê²°ê³¼ë¬¼ì„ ìƒì„±í•˜ê³ , ì •ë ¬ ì†ì‹¤ê³¼ ìƒì„± ì†ì‹¤ì„ ê²°í•©í•œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ë° ìƒì„± êµ¬ì„± ìš”ì†Œë¥¼ ìµœì í™”í•©ë‹ˆë‹¤. ë˜í•œ ë…¸ì´ì¦ˆ ê´€ë¦¬ ê¸°ë²•ì´ ì ìš©ë˜ì–´ í•™ìŠµì˜ ì•ˆì •ì„±ê³¼ ê°•ê±´ì„±ì´ í–¥ìƒë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 1:  Overview of the multimodal retrieval-augmented generation (RAG) pipeline, highlighting the advancements and techniques employed at each stage. The flow begins with query preprocessing, where user queries are refined and then encoded into a shared embedding space alongside a multimodal database. Retrieval strategies, such as modality-centric retrieval, similarity search, and re-ranking, enhance document selection, while fusion mechanisms align and integrate data from multiple modalities using score fusion or attention-based methods. Augmentation techniques such as iterative retrieval with feedback mechanisms, further refine the retrieved documents for the multimodal LLM. The generation stage incorporates innovations like Chain-of-Thought reasoning and source attribution for better outputs, with loss functions combining alignment loss and generation loss to optimize both retrieval and generation components. Noise management techniques are also applied to improve training stability and robustness.
> </details>





{{< table-caption >}}
| Category | Name | Statistics and Description | Modalities | Link |
|---|---|---|---|---|
| Image-Text General | LAION-400M Schuhmann et al. (2021) | 200M imageâ€“text pairs; used for pre-training multimodal models. | Image, Text | https://laion.ai/projects/laion-400-mil-open-dataset/ |
|  | Conceptual-Captions (CC) Sharma et al. (2018) | 15M imageâ€“caption pairs; multilingual Englishâ€“German image descriptions. | Image, Text | https://github.com/google-research-datasets/conceptual-captions |
|  | CIRR Liu et al. (2021) | 36,554 triplets from 21,552 images; focuses on natural image relationships. | Image, Text | https://github.com/Cuberick-Orion/CIRR |
|  | MS-COCO Lin et al. (2014) | 330K images with captions; used for captionâ€“toâ€“image and imageâ€“toâ€“caption generation. | Image, Text | https://cocodataset.org/ |
|  | Flickr30K Young et al. (2014) | 31K images annotated with five English captions per image. | Image, Text | https://shannon.cs.illinois.edu/DenotationGraph/ |
|  | Multi30K Elliott et al. (2016) | 30k German captions from native speakers and humanâ€“translated captions. | Image, Text | https://github.com/multi30k/dataset |
|  | NoCaps Agrawal et al. (2019) | For zeroâ€“shot image captioning evaluation; 15K images. | Image, Text | https://nocaps.org/ |
|  | Laion-5B Schuhmann et al. (2022) | 5B imageâ€“text pairs used as external memory for retrieval. | Image, Text | https://laion.ai/blog/laion-5b/ |
|  | COCO-CN Author and Author (2018) | 20,341 images for cross-lingual tagging and captioning with Chinese sentences. | Image, Text | https://github.com/li-xirong/coco-cn |
|  | CIRCO Baldrati et al. (2023) | 1,020 queries with an average of 4.53 ground truths per query; for composed image retrieval. | Image, Text | https://github.com/miccunifi/CIRCO |
| Video-Text | BDD-X Xu et al. (2018) | 77 hours of driving videos with expert textual explanations; for explainable driving behavior. | Video, Text | https://github.com/JinkyuKimUCB/BDD-X-dataset |
|  | YouCook2 Zhou et al. (2018) | 2,000 cooking videos with aligned descriptions; focused on videoâ€“text tasks. | Video, Text | https://youcook2.eecs.umich.edu/ |
|  | ActivityNet Caba Heilbron et al. (2015) | 20,000 videos with multiple captions; used for video understanding and captioning. | Video, Text | http://activity-net.org/ |
|  | SoccerNet Giancola et al. (2018) | Videos and metadata for 550 soccer games; includes transcribed commentary and key event annotations. | Video, Text | https://www.soccer-net.org/ |
|  | MSR-VTT Xu et al. (2016) | 10,000 videos with 20 captions each; a large video description dataset. | Video, Text | https://ms-multimedia-challenge.com/2016/dataset |
|  | MSVD Chen and Dolan (2011) | 1,970 videos with approximately 40 captions per video. | Video, Text | https://www.cs.utexas.edu/%C2%A0ml/clamp/videoDescription/ |
|  | LSMDC Rohrbach et al. (2015) | 118,081 videoâ€“text pairs from 202 movies; a movie description dataset. | Video, Text | https://sites.google.com/site/describingmovies/ |
|  | DiDemo Anne Hendricks et al. (2017) | 10,000 videos with four concatenated captions per video; with temporal localization of events. | Video, Text | https://github.com/LisaAnne/TemporalLanguageRelease |
|  | Breakfast Kuehne et al. (2014) | 1,712 videos of breakfast preparation; one of the largest fully annotated video datasets. | Video, Text | https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/ |
|  | COIN Tang et al. (2019) | 11,827 instructional YouTube videos across 180 tasks; for comprehensive instructional video analysis. | Video, Text | https://coin-dataset.github.io/ |
|  | MSRVTT-QA Xu et al. (2017) | Video question answering benchmark. | Video, Text | https://github.com/xudejing/video-question-answering |
|  | MSVD-QA Xu et al. (2017) | 1,970 video clips with approximately 50.5K QA pairs; video QA dataset. | Video, Text | https://github.com/xudejing/video-question-answering |
|  | ActivityNet-QA Yu et al. (2019) | 58,000 humanâ€“annotated QA pairs on 5,800 videos; benchmark for video QA models. | Video, Text | https://github.com/MILVLG/activitynet-qa |
|  | EpicKitchens-100 Dima (2020) | 700 videos (100 hours of cooking activities) for online action prediction; egocentric vision dataset. | Video, Text | https://epic-kitchens.github.io/2021/ |
|  | Ego4D Grauman et al. (2022) | 4.3M videoâ€“text pairs for egocentric videos; massiveâ€“scale egocentric video dataset. | Video, Text | https://ego4d-data.org/ |
|  | HowTo100M Miech et al. (2019) | 136M video clips with captions from 1.2M YouTube videos; for learning textâ€“video embeddings. | Video, Text | https://www.di.ens.fr/willow/research/howto100m/ |
|  | CharadesEgo Sigurdsson et al. (2018) | 68,536 activity instances from egoâ€“exo videos; used for evaluation. | Video, Text | https://prior.allenai.org/projects/charades-ego |
|  | ActivityNet Captions Krishna et al. (2017) | 20K videos with 3.7 temporally localized sentences per video; denseâ€“captioning events in videos. | Video, Text | https://cs.stanford.edu/people/ranjaykrishna/densevid/ |
|  | VATEX Wang et al. (2019) | 34,991 videos, each with multiple captions; a multilingual videoâ€“andâ€“language dataset. | Video, Text | https://eric-xw.github.io/vatex-website/ |
|  | Charades Sigurdsson et al. (2016) | 9,848 video clips with textual descriptions; a multimodal research dataset. | Video, Text | https://allenai.org/plato/charades/ |
|  | WebVid Bain et al. (2021) | 10M videoâ€“text pairs (refined to WebVid-Refined-1M). | Video, Text | https://github.com/m-bain/webvid |
|  | Youku-mPLUG Xu et al. (2023) | Chinese dataset with 10M videoâ€“text pairs (refined to Youku-Refined-1M). | Video, Text | https://github.com/X-PLUG/Youku-mPLUG |
| Audio-Text | LibriSpeech Panayotov et al. (2015) | 1,000 hours of read English speech with corresponding text; ASR corpus based on audiobooks. | Audio, Text | https://www.openslr.org/12 |
|  | SpeechBrown Abootorabi and Asgari (2024) | 55K paired speech-text samples; 15 categories covering diverse topics from religion to fiction. | Audio, Text | https://huggingface.co/datasets/llm-lab/SpeechBrown |
|  | AudioCap Kim et al. (2019) | 46K audio clips paired with human-written text captions. | Audio, Text | https://audiocaps.github.io/ |
|  | AudioSet Gemmeke et al. (2017) | 2,084,320 humanâ€“labeled 10â€“second sound clips from YouTube; 632 audio event classes. | Audio, Text | https://research.google.com/audioset/ |
| Medical | MIMIC-CXR Johnson et al. (2019) | 125,417 training pairs of chest Xâ€“rays and reports. | Image, Text | https://physionet.org/content/mimic-cxr/2.0.0/ |
|  | CheXpert Irvin et al. (2019) | 224,316 chest radiographs of 65,240 patients; focused on medical analysis. | Image, Text | https://stanfordmlgroup.github.io/competitions/chexpert/ |
|  | MIMIC-III Johnson et al. (2016) | Health-related data from over 40K patients (text data). | Text | https://mimic.physionet.org/ |
|  | IU-Xray Pavlopoulos et al. (2019) | 7,470 pairs of chest Xâ€“rays and corresponding diagnostic reports. | Image, Text | https://www.kaggle.com/datasets/raddar/chest-xrays-indiana-university |
|  | PubLayNet Zhong et al. (2019) | 100,000 training samples and 2,160 test samples built from PubLayNet (tailored for the medical domain). | Image, Text | https://github.com/ibm-aur-nlp/PubLayNet |
| Fashion | Fashion-IQ Wu et al. (2019) | 77,684 images across three categories; evaluated with Recall@10 and Recall@50. | Image, Text | https://github.com/XiaoxiaoGuo/fashion-iq |
|  | FashionGen Hadi Kiapour et al. (2018) | 260.5K imageâ€“text pairs of fashion images and item descriptions. | Image, Text | https://www.elementai.com/datasets/fashiongen |
|  | VITON-HD Choi et al. (2021) | 83K images for virtual tryâ€“on; highâ€“resolution clothing items. | Image, Text | https://github.com/shadow2496/VITON-HD |
|  | Fashionpedia Author and Author (2023a) | 48,000 fashion images annotated with segmentation masks and fine-grained attributes. | Image, Text | https://fashionpedia.ai/ |
|  | DeepFashion Liu et al. (2016) | Approximately 800K diverse fashion images for pseudo triplet generation. | Image, Text | https://github.com/zalandoresearch/fashion-mnist |
| 3D | ShapeNet Chang et al. (2015) | 7,500 textâ€“3D data pairs; repository for 3D CAD models. | Text, 3D | https://shapenet.org/ |
| Knowledge & QA | VQA Antol et al. (2015) | 400K QA pairs with images for visual question answering. | Image, Text | https://visualqa.org/ |
|  | PAQ Lewis et al. (2021) | 65M textâ€“based QA pairs; a largeâ€“scale dataset. | Text | https://github.com/facebookresearch/PAQ |
|  | ELI5 Fan et al. (2019) | 270K complex and diverse questions augmented with web pages and images. | Text | https://facebookresearch.github.io/ELI5/ |
|  | ViQuAE Biten et al. (2022) | 11.8M passages from Wikipedia covering 2,397 unique entities; knowledgeâ€“intensive QA. | Text | https://github.com/PaulLerner/ViQuAE |
|  | OK-VQA Marino et al. (2019) | 14K questions requiring external knowledge for VQA. | Image, Text | https://okvqa.allenai.org/ |
|  | WebQA Li et al. (2022b) | 46K queries that require reasoning across text and images. | Text, Image | https://webqna.github.io/ |
|  | Infoseek Li et al. (2021) | Fine-grained visual knowledge retrieval using a Wikipediaâ€“based knowledge base (Â 6M passages). | Image, Text | https://open-vision-language.github.io/infoseek/ |
|  | ClueWeb22 Callan et al. (2022) | 10 billion web pages organized into three subsets; a largeâ€“scale web corpus. | Text | https://lemurproject.org/clueweb22/ |
|  | MOCHEG Yao et al. (2023) | 15,601 claims annotated with truthfulness labels and accompanied by textual and image evidence. | Text, Image | https://github.com/VT-NLP/Mocheg |
|  | VQA v2 Goyal et al. (2017b) | 1.1M questions (augmented with VGâ€“QA questions) for fine-tuning VQA models. | Image, Text | https://visualqa.org/ |
|  | A-OKVQA Schwenk et al. (2022) | Benchmark for visual question answering using world knowledge; around 25K questions. | Image, Text | https://github.com/allenai/aokvqa |
|  | XL-HeadTags Shohan et al. (2024) | 415K news headline-article pairs consist of 20 languages across six diverse language families. | Text | https://huggingface.co/datasets/faisaltareque/XL-HeadTags |
|  | SEED-Bench Li et al. (2023a) | 19K multipleâ€“choice questions with accurate human annotations across 12 evaluation dimensions. | Text | https://github.com/AILab-CVC/SEED-Bench |
| Other | ImageNet Deng et al. (2009) | 14,197,122 images for perspective understanding; a hierarchical image database. | Image | http://www.image-net.org/ |
|  | Oxford Flowers102 Nilsback and Zisserman (2008) | 102 flower categories with five examples per category; image classification dataset. | Image | https://www.robots.ox.ac.uk/%C2%A0vgg/data/flowers/102/ |
|  | Stanford Cars Krause et al. (2013) | Images of different car models (five examples per model); for fine-grained categorization. | Image | https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset |
|  | GeoDE Author and Author (2023b) | 61,940 images from 40 classes across 6 world regions; emphasizes geographic diversity in object recognition. | Image | https://github.com/AliRamazani/GeoDE |{{< /table-caption >}}

> ğŸ”¼ ë³¸ ë…¼ë¬¸ì˜ í‘œ 1ì€ ë‹¤ì¤‘ ëª¨ë“œ ê²€ìƒ‰ ì¦ê°• ìƒì„±(Multimodal RAG) ì—°êµ¬ì— ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ë“¤ì„ ê°œê´„ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. í‘œì—ëŠ” ë°ì´í„°ì…‹ì˜ ì¹´í…Œê³ ë¦¬, ì´ë¦„, í†µê³„ ë° ì„¤ëª…, ëª¨ë“œ, ë§í¬ ë“±ì˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê° ë°ì´í„°ì…‹ì˜ ì¹´í…Œê³ ë¦¬ëŠ” ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì¼ë°˜, ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸, ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸, ì˜ë£Œ, íŒ¨ì…˜, 3D, ì§€ì‹ ë° ì§ˆì˜ì‘ë‹µ, ê¸°íƒ€ ë“± ì—¬ëŸ ê°€ì§€ë¡œ ë¶„ë¥˜ë˜ë©°, ê° ë°ì´í„°ì…‹ì˜ íŠ¹ì§•ê³¼ ë‹¤ì–‘í•œ ë‹¤ì¤‘ ëª¨ë“œ RAG ì‘ì—…ì—ì˜ ì ìš© ë²”ìœ„ë¥¼ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.  ë§í¬ ì—´ì—ëŠ” ê° ë°ì´í„°ì…‹ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ ë° ì‘ìš© í”„ë¡œê·¸ë¨ì— ëŒ€í•œ ì‹¬ì¸µì ì¸ íƒêµ¬ë¥¼ ìœ„í•œ ê³µì‹ ì €ì¥ì†Œ ë˜ëŠ” ì¶”ê°€ ë¦¬ì†ŒìŠ¤ë¡œ ì—°ê²°ë˜ëŠ” í•˜ì´í¼ë§í¬ê°€ ì œê³µë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 1: Overview of Popular Datasets in Multimodal RAG Research.
> </details>





### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}