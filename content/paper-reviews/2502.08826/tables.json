[{"content": "| Category | Name | Statistics and Description | Modalities | Link |\n|---|---|---|---|---|\n| Image-Text General | LAION-400M Schuhmann et al. (2021) | 200M image\u2013text pairs; used for pre-training multimodal models. | Image, Text | https://laion.ai/projects/laion-400-mil-open-dataset/ |\n|  | Conceptual-Captions (CC) Sharma et al. (2018) | 15M image\u2013caption pairs; multilingual English\u2013German image descriptions. | Image, Text | https://github.com/google-research-datasets/conceptual-captions |\n|  | CIRR Liu et al. (2021) | 36,554 triplets from 21,552 images; focuses on natural image relationships. | Image, Text | https://github.com/Cuberick-Orion/CIRR |\n|  | MS-COCO Lin et al. (2014) | 330K images with captions; used for caption\u2013to\u2013image and image\u2013to\u2013caption generation. | Image, Text | https://cocodataset.org/ |\n|  | Flickr30K Young et al. (2014) | 31K images annotated with five English captions per image. | Image, Text | https://shannon.cs.illinois.edu/DenotationGraph/ |\n|  | Multi30K Elliott et al. (2016) | 30k German captions from native speakers and human\u2013translated captions. | Image, Text | https://github.com/multi30k/dataset |\n|  | NoCaps Agrawal et al. (2019) | For zero\u2013shot image captioning evaluation; 15K images. | Image, Text | https://nocaps.org/ |\n|  | Laion-5B Schuhmann et al. (2022) | 5B image\u2013text pairs used as external memory for retrieval. | Image, Text | https://laion.ai/blog/laion-5b/ |\n|  | COCO-CN Author and Author (2018) | 20,341 images for cross-lingual tagging and captioning with Chinese sentences. | Image, Text | https://github.com/li-xirong/coco-cn |\n|  | CIRCO Baldrati et al. (2023) | 1,020 queries with an average of 4.53 ground truths per query; for composed image retrieval. | Image, Text | https://github.com/miccunifi/CIRCO |\n| Video-Text | BDD-X Xu et al. (2018) | 77 hours of driving videos with expert textual explanations; for explainable driving behavior. | Video, Text | https://github.com/JinkyuKimUCB/BDD-X-dataset |\n|  | YouCook2 Zhou et al. (2018) | 2,000 cooking videos with aligned descriptions; focused on video\u2013text tasks. | Video, Text | https://youcook2.eecs.umich.edu/ |\n|  | ActivityNet Caba Heilbron et al. (2015) | 20,000 videos with multiple captions; used for video understanding and captioning. | Video, Text | http://activity-net.org/ |\n|  | SoccerNet Giancola et al. (2018) | Videos and metadata for 550 soccer games; includes transcribed commentary and key event annotations. | Video, Text | https://www.soccer-net.org/ |\n|  | MSR-VTT Xu et al. (2016) | 10,000 videos with 20 captions each; a large video description dataset. | Video, Text | https://ms-multimedia-challenge.com/2016/dataset |\n|  | MSVD Chen and Dolan (2011) | 1,970 videos with approximately 40 captions per video. | Video, Text | https://www.cs.utexas.edu/%C2%A0ml/clamp/videoDescription/ |\n|  | LSMDC Rohrbach et al. (2015) | 118,081 video\u2013text pairs from 202 movies; a movie description dataset. | Video, Text | https://sites.google.com/site/describingmovies/ |\n|  | DiDemo Anne Hendricks et al. (2017) | 10,000 videos with four concatenated captions per video; with temporal localization of events. | Video, Text | https://github.com/LisaAnne/TemporalLanguageRelease |\n|  | Breakfast Kuehne et al. (2014) | 1,712 videos of breakfast preparation; one of the largest fully annotated video datasets. | Video, Text | https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/ |\n|  | COIN Tang et al. (2019) | 11,827 instructional YouTube videos across 180 tasks; for comprehensive instructional video analysis. | Video, Text | https://coin-dataset.github.io/ |\n|  | MSRVTT-QA Xu et al. (2017) | Video question answering benchmark. | Video, Text | https://github.com/xudejing/video-question-answering |\n|  | MSVD-QA Xu et al. (2017) | 1,970 video clips with approximately 50.5K QA pairs; video QA dataset. | Video, Text | https://github.com/xudejing/video-question-answering |\n|  | ActivityNet-QA Yu et al. (2019) | 58,000 human\u2013annotated QA pairs on 5,800 videos; benchmark for video QA models. | Video, Text | https://github.com/MILVLG/activitynet-qa |\n|  | EpicKitchens-100 Dima (2020) | 700 videos (100 hours of cooking activities) for online action prediction; egocentric vision dataset. | Video, Text | https://epic-kitchens.github.io/2021/ |\n|  | Ego4D Grauman et al. (2022) | 4.3M video\u2013text pairs for egocentric videos; massive\u2013scale egocentric video dataset. | Video, Text | https://ego4d-data.org/ |\n|  | HowTo100M Miech et al. (2019) | 136M video clips with captions from 1.2M YouTube videos; for learning text\u2013video embeddings. | Video, Text | https://www.di.ens.fr/willow/research/howto100m/ |\n|  | CharadesEgo Sigurdsson et al. (2018) | 68,536 activity instances from ego\u2013exo videos; used for evaluation. | Video, Text | https://prior.allenai.org/projects/charades-ego |\n|  | ActivityNet Captions Krishna et al. (2017) | 20K videos with 3.7 temporally localized sentences per video; dense\u2013captioning events in videos. | Video, Text | https://cs.stanford.edu/people/ranjaykrishna/densevid/ |\n|  | VATEX Wang et al. (2019) | 34,991 videos, each with multiple captions; a multilingual video\u2013and\u2013language dataset. | Video, Text | https://eric-xw.github.io/vatex-website/ |\n|  | Charades Sigurdsson et al. (2016) | 9,848 video clips with textual descriptions; a multimodal research dataset. | Video, Text | https://allenai.org/plato/charades/ |\n|  | WebVid Bain et al. (2021) | 10M video\u2013text pairs (refined to WebVid-Refined-1M). | Video, Text | https://github.com/m-bain/webvid |\n|  | Youku-mPLUG Xu et al. (2023) | Chinese dataset with 10M video\u2013text pairs (refined to Youku-Refined-1M). | Video, Text | https://github.com/X-PLUG/Youku-mPLUG |\n| Audio-Text | LibriSpeech Panayotov et al. (2015) | 1,000 hours of read English speech with corresponding text; ASR corpus based on audiobooks. | Audio, Text | https://www.openslr.org/12 |\n|  | SpeechBrown Abootorabi and Asgari (2024) | 55K paired speech-text samples; 15 categories covering diverse topics from religion to fiction. | Audio, Text | https://huggingface.co/datasets/llm-lab/SpeechBrown |\n|  | AudioCap Kim et al. (2019) | 46K audio clips paired with human-written text captions. | Audio, Text | https://audiocaps.github.io/ |\n|  | AudioSet Gemmeke et al. (2017) | 2,084,320 human\u2013labeled 10\u2013second sound clips from YouTube; 632 audio event classes. | Audio, Text | https://research.google.com/audioset/ |\n| Medical | MIMIC-CXR Johnson et al. (2019) | 125,417 training pairs of chest X\u2013rays and reports. | Image, Text | https://physionet.org/content/mimic-cxr/2.0.0/ |\n|  | CheXpert Irvin et al. (2019) | 224,316 chest radiographs of 65,240 patients; focused on medical analysis. | Image, Text | https://stanfordmlgroup.github.io/competitions/chexpert/ |\n|  | MIMIC-III Johnson et al. (2016) | Health-related data from over 40K patients (text data). | Text | https://mimic.physionet.org/ |\n|  | IU-Xray Pavlopoulos et al. (2019) | 7,470 pairs of chest X\u2013rays and corresponding diagnostic reports. | Image, Text | https://www.kaggle.com/datasets/raddar/chest-xrays-indiana-university |\n|  | PubLayNet Zhong et al. (2019) | 100,000 training samples and 2,160 test samples built from PubLayNet (tailored for the medical domain). | Image, Text | https://github.com/ibm-aur-nlp/PubLayNet |\n| Fashion | Fashion-IQ Wu et al. (2019) | 77,684 images across three categories; evaluated with Recall@10 and Recall@50. | Image, Text | https://github.com/XiaoxiaoGuo/fashion-iq |\n|  | FashionGen Hadi Kiapour et al. (2018) | 260.5K image\u2013text pairs of fashion images and item descriptions. | Image, Text | https://www.elementai.com/datasets/fashiongen |\n|  | VITON-HD Choi et al. (2021) | 83K images for virtual try\u2013on; high\u2013resolution clothing items. | Image, Text | https://github.com/shadow2496/VITON-HD |\n|  | Fashionpedia Author and Author (2023a) | 48,000 fashion images annotated with segmentation masks and fine-grained attributes. | Image, Text | https://fashionpedia.ai/ |\n|  | DeepFashion Liu et al. (2016) | Approximately 800K diverse fashion images for pseudo triplet generation. | Image, Text | https://github.com/zalandoresearch/fashion-mnist |\n| 3D | ShapeNet Chang et al. (2015) | 7,500 text\u20133D data pairs; repository for 3D CAD models. | Text, 3D | https://shapenet.org/ |\n| Knowledge & QA | VQA Antol et al. (2015) | 400K QA pairs with images for visual question answering. | Image, Text | https://visualqa.org/ |\n|  | PAQ Lewis et al. (2021) | 65M text\u2013based QA pairs; a large\u2013scale dataset. | Text | https://github.com/facebookresearch/PAQ |\n|  | ELI5 Fan et al. (2019) | 270K complex and diverse questions augmented with web pages and images. | Text | https://facebookresearch.github.io/ELI5/ |\n|  | ViQuAE Biten et al. (2022) | 11.8M passages from Wikipedia covering 2,397 unique entities; knowledge\u2013intensive QA. | Text | https://github.com/PaulLerner/ViQuAE |\n|  | OK-VQA Marino et al. (2019) | 14K questions requiring external knowledge for VQA. | Image, Text | https://okvqa.allenai.org/ |\n|  | WebQA Li et al. (2022b) | 46K queries that require reasoning across text and images. | Text, Image | https://webqna.github.io/ |\n|  | Infoseek Li et al. (2021) | Fine-grained visual knowledge retrieval using a Wikipedia\u2013based knowledge base (\u00a06M passages). | Image, Text | https://open-vision-language.github.io/infoseek/ |\n|  | ClueWeb22 Callan et al. (2022) | 10 billion web pages organized into three subsets; a large\u2013scale web corpus. | Text | https://lemurproject.org/clueweb22/ |\n|  | MOCHEG Yao et al. (2023) | 15,601 claims annotated with truthfulness labels and accompanied by textual and image evidence. | Text, Image | https://github.com/VT-NLP/Mocheg |\n|  | VQA v2 Goyal et al. (2017b) | 1.1M questions (augmented with VG\u2013QA questions) for fine-tuning VQA models. | Image, Text | https://visualqa.org/ |\n|  | A-OKVQA Schwenk et al. (2022) | Benchmark for visual question answering using world knowledge; around 25K questions. | Image, Text | https://github.com/allenai/aokvqa |\n|  | XL-HeadTags Shohan et al. (2024) | 415K news headline-article pairs consist of 20 languages across six diverse language families. | Text | https://huggingface.co/datasets/faisaltareque/XL-HeadTags |\n|  | SEED-Bench Li et al. (2023a) | 19K multiple\u2013choice questions with accurate human annotations across 12 evaluation dimensions. | Text | https://github.com/AILab-CVC/SEED-Bench |\n| Other | ImageNet Deng et al. (2009) | 14,197,122 images for perspective understanding; a hierarchical image database. | Image | http://www.image-net.org/ |\n|  | Oxford Flowers102 Nilsback and Zisserman (2008) | 102 flower categories with five examples per category; image classification dataset. | Image | https://www.robots.ox.ac.uk/%C2%A0vgg/data/flowers/102/ |\n|  | Stanford Cars Krause et al. (2013) | Images of different car models (five examples per model); for fine-grained categorization. | Image | https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset |\n|  | GeoDE Author and Author (2023b) | 61,940 images from 40 classes across 6 world regions; emphasizes geographic diversity in object recognition. | Image | https://github.com/AliRamazani/GeoDE |", "caption": "Table 1: Overview of Popular Datasets in Multimodal RAG Research.", "description": "\ubcf8 \ub17c\ubb38\uc758 \ud45c 1\uc740 \ub2e4\uc911 \ubaa8\ub4dc \uac80\uc0c9 \uc99d\uac15 \uc0dd\uc131(Multimodal RAG) \uc5f0\uad6c\uc5d0 \ub110\ub9ac \uc0ac\uc6a9\ub418\ub294 \ub370\uc774\ud130\uc14b\ub4e4\uc744 \uac1c\uad04\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ub370\uc774\ud130\uc14b\uc758 \uce74\ud14c\uace0\ub9ac, \uc774\ub984, \ud1b5\uacc4 \ubc0f \uc124\uba85, \ubaa8\ub4dc, \ub9c1\ud06c \ub4f1\uc758 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc758 \uce74\ud14c\uace0\ub9ac\ub294 \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8 \uc77c\ubc18, \ube44\ub514\uc624-\ud14d\uc2a4\ud2b8, \uc624\ub514\uc624-\ud14d\uc2a4\ud2b8, \uc758\ub8cc, \ud328\uc158, 3D, \uc9c0\uc2dd \ubc0f \uc9c8\uc758\uc751\ub2f5, \uae30\ud0c0 \ub4f1 \uc5ec\ub35f \uac00\uc9c0\ub85c \ubd84\ub958\ub418\uba70, \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5\uacfc \ub2e4\uc591\ud55c \ub2e4\uc911 \ubaa8\ub4dc RAG \uc791\uc5c5\uc5d0\uc758 \uc801\uc6a9 \ubc94\uc704\ub97c \uc790\uc138\ud788 \uc124\uba85\ud569\ub2c8\ub2e4.  \ub9c1\ud06c \uc5f4\uc5d0\ub294 \uac01 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ucd94\uac00 \uc815\ubcf4 \ubc0f \uc751\uc6a9 \ud504\ub85c\uadf8\ub7a8\uc5d0 \ub300\ud55c \uc2ec\uce35\uc801\uc778 \ud0d0\uad6c\ub97c \uc704\ud55c \uacf5\uc2dd \uc800\uc7a5\uc18c \ub610\ub294 \ucd94\uac00 \ub9ac\uc18c\uc2a4\ub85c \uc5f0\uacb0\ub418\ub294 \ud558\uc774\ud37c\ub9c1\ud06c\uac00 \uc81c\uacf5\ub429\ub2c8\ub2e4.", "section": "2 Datasets and Benchmarks"}, {"content": "| Category | Name | Statistics and Description | Modalities | Link |\n|---|---|---|---|---|\n| Cross-Modal Understanding | MRAG-Bench [Hu et al. (2024c)] | Evaluates visual retrieval, integration, and robustness to irrelevant visual information. | Images | [MRAG-Bench](https://mragbench.github.io/) |\n|  | M2RAG [Ma et al. (2024c)] | Benchmarks multimodal RAG; evaluates retrieval, multi-hop reasoning, and integration. | Images + Text | [M2RAG](https://arxiv.org/abs/2411.16365) |\n|  | Dyn-VQA [Li et al. (2024b)] | Focuses on dynamic retrieval, multi-hop reasoning, and robustness to changing information. | Images + Text | [Dyn-VQA](https://openreview.net/forum?id=VvDEuyVXkG) |\n|  | MMBench [Liu et al. (2025)] | Covers VQA, captioning, retrieval; evaluates cross-modal understanding across vision, text, and audio. | Images + Text + Audio | [MMBench](https://github.com/open-compass/MMBench) |\n|  | ScienceQA [Saikh et al. (2022)] | Contains 21,208 questions; tests scientific reasoning with text, diagrams, and images. | Images + Diagrams + Text | [ScienceQA](https://scienceqa.github.io/) |\n|  | SK-VQA [Su et al. (2024b)] | Offers 2 million question-answer pairs; focuses on synthetic knowledge, multimodal reasoning, and external knowledge integration. | Images + Text | [SK-VQA](https://arxiv.org/abs/2406.19593) |\n|  | SMMQG [Wu et al. (2024a)] | Includes 1,024 question-answer pairs; focuses on synthetic multimodal data and controlled question generation. | Images + Text | [SMMQG](https://arxiv.org/abs/2407.02233) |\n| Text-Focused | TriviaQA [Joshi et al. (2017)] | Provides 650K question-answer pairs; reading comprehension dataset, adaptable for multimodal RAG. | Text | [TriviaQA](https://nlp.cs.washington.edu/triviaqa/) |\n|  | Natural Questions [Kwiatkowski et al. (2019)] | Contains 307,373 training examples; real-world search queries, adaptable with visual contexts. | Text | [Natural Questions](https://paperswithcode.com/dataset/natural-questions) |", "caption": "Table 2: Overview of Popular Benchmarks in Multimodal RAG Research.", "description": "\ubcf8 \ud45c\ub294 \ub2e4\uc591\ud55c \ubaa8\ub2ec\ub9ac\ud2f0(\ud14d\uc2a4\ud2b8, \uc774\ubbf8\uc9c0, \ube44\ub514\uc624 \ub4f1)\ub97c \ud65c\uc6a9\ud558\ub294 RAG(Retrieval-Augmented Generation) \uc2dc\uc2a4\ud15c\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\ub294 \ubca4\uce58\ub9c8\ud06c\ub4e4\uc758 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubca4\uce58\ub9c8\ud06c\ub294 \uadf8 \ubaa9\uc801, \uc0ac\uc6a9\ub418\ub294 \ubaa8\ub2ec\ub9ac\ud2f0, \ud3c9\uac00 \uc9c0\ud45c \ub4f1\uc758 \uc815\ubcf4\ub97c \ub2f4\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0 \uc81c\uc2dc\ub41c \ubca4\uce58\ub9c8\ud06c\ub4e4\uc740 \ub2e4\uc591\ud55c \uc885\ub958\uc758 \uc9c8\ubb38\uc751\ub2f5, \uc774\ubbf8\uc9c0 \ucea1\uc154\ub2dd, \ube44\ub514\uc624 \uc774\ud574 \ub4f1\uc758 \uacfc\uc81c\ub97c \ub2e4\ub8f9\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uc5f0\uad6c\uc790\ub4e4\uc740 \uc790\uc2e0\uc758 RAG \uc2dc\uc2a4\ud15c\uc758 \uc131\ub2a5\uc744 \ub2e4\uac01\uc801\uc73c\ub85c \ud3c9\uac00\ud558\uace0, \ud5a5\ud6c4 \uc5f0\uad6c \ubc29\ud5a5\uc744 \uc124\uc815\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2 Datasets and Benchmarks"}]