[{"content": "| Node for Norm Calculation | MMLU (5-shot) | ARC-C (5-shot) |\n|---|---|---|\n| Mixtral 8\u00d77B | Phi-3.5-MoE-ins. | Phi-3.5-MoE-ins. |\n| **x**W<sub>g</sub> | 64.23 (42.70) | 50.43 (4.40) |\n| **x**W<sub>p</sub> | 62.06 (42.73) | 53.41 (4.40) |\n| SiLU(**x**W<sub>g</sub>) | 61.71 (43.88) | 58.79 (4.51) |\n| SiLU(**x**W<sub>g</sub>)\u2299**x**W<sub>p</sub> | 66.64 (75.53) | 58.79 (6.27) |\n| Experts\u2019 Final Outputs | 66.66 (76.15) | 58.62 (7.42) |\n| Performance w. Router | 70.35 (24.30) | 62.12 (2.50) |\n|  | 29.43 (33.05) | 28.84 (3.47) |\n|  | 34.60 (33.05) | 40.36 (3.47) |\n|  | 38.03 (34.32) | 47.53 (3.60) |\n|  | 27.89 (52.60) | 35.32 (5.42) |\n|  | 29.69 (69.20) | 36.35 (7.07) |\n|  | 78.20 (14.53) | 67.41 (1.60) |", "caption": "Table 1: \nWe remove routers from pre-trained MoE-LLMs and select experts during inference based on the internal activation norms of specific nodes in the computational graph.\nThe accuracy on two challenging tasks is reported, along with the time cost (in minutes) for 8\u00d7A800-80G GPUs, which is given in parentheses.\nWithout parameter updates, we can largely preserve accuracy under certain nodes, but this rudimentary approach requires significant improvements in efficiency.", "description": "\uc774 \ud45c\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c MoE(Mixture-of-Experts) \uc5b8\uc5b4 \ubaa8\ub378\uc5d0\uc11c \ub77c\uc6b0\ud130\ub97c \uc81c\uac70\ud558\uace0, \uacc4\uc0b0 \uadf8\ub798\ud504 \ub0b4 \ud2b9\uc815 \ub178\ub4dc\uc758 \ub0b4\ubd80 \ud65c\uc131\ud654 \uaddc\ubc94\uc744 \uae30\ubc18\uc73c\ub85c \ucd94\ub860 \uc911 \uc804\ubb38\uac00\ub97c \uc120\ud0dd\ud558\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub450 \uac00\uc9c0 \uc5b4\ub824\uc6b4 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\uc640 8\uac1c\uc758 A800-80G GPU\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \uc2dc\uac04 \ube44\uc6a9(\ubd84)\uc744 \ubcf4\uace0\ud569\ub2c8\ub2e4. \uad04\ud638 \uc548\uc5d0 \uc2dc\uac04 \ube44\uc6a9\uc774 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ud30c\ub77c\ubbf8\ud130 \uc5c5\ub370\uc774\ud2b8 \uc5c6\uc774\ub3c4 \ud2b9\uc815 \ub178\ub4dc\uc5d0\uc11c\ub294 \uc815\ud655\ub3c4\ub97c \uc0c1\ub2f9 \ubd80\ubd84 \uc720\uc9c0\ud560 \uc218 \uc788\uc9c0\ub9cc, \uc774\ub7ec\ud55c \uae30\ubcf8\uc801\uc778 \uc811\uadfc \ubc29\uc2dd\uc740 \ud6a8\uc728\uc131\uc744 \ud06c\uac8c \uac1c\uc120\ud574\uc57c \ud569\ub2c8\ub2e4.", "section": "3. Method"}, {"content": "| Configuration | ARC-E | PIQA | SIQA | WINO | HELLA | MNLI | QNLI | SST2 | AVG. |\n|---|---|---|---|---|---|---|---|---|---|---| \n| 1 Traditional MoE | 39.90 | 58.43 | 35.67 | 52.09 | 27.98 | 33.09 | 49.28 | 49.66 | 43.28 |\n| 2 +  \u2112<sub>aux</sub> | 40.74 | 58.49 | 36.13 | 51.30 | 28.11 | 32.67 | 50.23 | 51.83 | 43.68 |\n| 3 +  \u2112<sub>aux</sub> + Factorized \ud835\udc16<sub>g</sub> | 40.45 | 58.65 | 36.75 | 52.09 | 28.03 | 32.55 | 50.08 | 51.03 | 43.70 |\n| 4 +  \u2112<sub>aux</sub> + Large Router | 41.41 | 57.62 | 36.64 | 52.33 | 28.34 | 33.18 | 49.53 | 50.69 | 43.71 |\n| 5 AoE (d<sub>low</sub>=64) | 39.77 | 58.71 | 35.31 | 52.33 | 28.29 | 32.78 | 50.27 | 52.98 | 43.81 |\n| 6 +  \u2112<sub>aux</sub> | 42.17 | 57.67 | 36.75 | 50.75 | 28.15 | 34.06 | 50.49 | 53.10 | 44.12 |\n| 7 AoE (d<sub>low</sub>=128) | 40.70 | 59.41 | 36.64 | 52.09 | 28.06 | 34.38 | 50.69 | 53.21 | 44.39 |\n| 8 +  \u2112<sub>aux</sub> | 41.33 | 58.65 | 36.80 | 50.75 | 28.40 | 33.71 | 49.55 | 53.10 | 44.04 |\n| 9 AoE (d<sub>low</sub>=256) | 41.08 | 58.81 | 36.44 | 51.70 | 28.23 | 32.24 | 50.54 | 53.90 | 44.12 |\n| 10 +  \u2112<sub>aux</sub> | 41.16 | 58.32 | 36.80 | 53.04 | 28.37 | 32.78 | 50.61 | 54.59 | 44.46 |\n| 11 AoE (d<sub>low</sub>=512) | 40.57 | 57.89 | 36.75 | 50.59 | 28.38 | 32.71 | 49.72 | 53.56 | 43.77 |\n| 12 +  \u2112<sub>aux</sub> | 41.16 | 57.83 | 36.75 | 52.09 | 28.30 | 34.92 | 50.67 | 50.92 | 44.08 |", "caption": "Table 2: \nAblations were performed on 732M-parameter language models (with 247M active parameters).\nEach model was trained on 100 billion tokens.\nThe results, highlighted in color, emphasize superior performance compared to configuration  2, the most common MoE setup.\nBold text indicates that the configuration outperforms the best traditional MoE variant in terms of average performance.", "description": "\ud45c 2\ub294 732M \ud30c\ub77c\ubbf8\ud130 \uc5b8\uc5b4 \ubaa8\ub378(\ud65c\uc131 \ud30c\ub77c\ubbf8\ud130 247M)\uc744 \uc0ac\uc6a9\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc740 1000\uc5b5 \ud1a0\ud070\uc73c\ub85c \ud559\uc2b5\ub418\uc5c8\uc73c\uba70, \uc0c9\uc0c1\uc73c\ub85c \uac15\uc870\ub41c \uacb0\uacfc\ub294 \uac00\uc7a5 \uc77c\ubc18\uc801\uc778 MoE \uc124\uc815\uc778 \uad6c\uc131 2\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uad75\uc740 \ud14d\uc2a4\ud2b8\ub294 \ud574\ub2f9 \uad6c\uc131\uc774 \ud3c9\uade0 \uc131\ub2a5 \uce21\uba74\uc5d0\uc11c \uae30\uc874 MoE \ubcc0\ud615\ubcf4\ub2e4 \uc6b0\uc218\ud568\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378 \uad6c\uc131(\uc804\ud1b5\uc801\uc778 MoE, Load Balancing Loss \ucd94\uac00,  Weight Factorization \ucd94\uac00, Router \ud06c\uae30 \ubcc0\uacbd, \uadf8\ub9ac\uace0 AoE \ubaa8\ub378\uc758 \ub2e4\uc591\ud55c dlow \uac12)\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ud558\ub958 \uc791\uc5c5(ARC-E, PIQA, SIQA, WINO, HELLA, MNLI, QNLI, SST2)\uc758 \uc131\ub2a5\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uad6c\uc131\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec AoE\uc758 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc744 \ud3c9\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Strategy | Model | ARC-E | PIQA | SIQA | WINO | HELLA | MNLI | QNLI | SST2 | AVG. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Top-P | Traditional MoE | 41.08 | 57.96 | 37.46 | 50.36 | 28.25 | 32.79 | 50.39 | 52.64 | 43.87 |\n| [Huang et al., 2024] | AoE | 41.04 | 58.65 | 36.39 | 51.07 | 28.35 | 32.96 | 51.46 | 54.36 | **44.29** |\n| Expert-Choice | Traditional MoE | 40.91 | 59.09 | 37.26 | 50.75 | 28.09 | 32.11 | 50.12 | 52.75 | 43.89 |\n| [Zhou et al., 2022] | AoE | 41.58 | 58.22 | 37.21 | 53.04 | 28.44 | 33.83 | 50.54 | 50.46 | **44.17** |", "caption": "Table 3: \nComparison of traditional MoE and AoE models trained using alternative expert-selection strategies.\nFor the Top\u2011P\ud835\udc43Pitalic_P strategy, the number of activated parameters is input-dependent but nearly the same between the two models, whereas the expert-choice strategy activates 247 out of 732M parameters.", "description": "\ud45c 3\uc740 \uc11c\ub85c \ub2e4\ub978 \uc804\ubb38\uac00 \uc120\ud0dd \uc804\ub7b5\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub41c \uae30\uc874 MoE \ubaa8\ub378\uacfc AoE \ubaa8\ub378\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Top-P \uc804\ub7b5\uc758 \uacbd\uc6b0 \ud65c\uc131\ud654\ub41c \ub9e4\uac1c\ubcc0\uc218\uc758 \uc218\ub294 \uc785\ub825\uc5d0 \ub530\ub77c \ub2e4\ub974\uc9c0\ub9cc \ub450 \ubaa8\ub378 \uac04\uc5d0 \uac70\uc758 \ub3d9\uc77c\ud558\uba70, \uc804\ubb38\uac00 \uc120\ud0dd \uc804\ub7b5\uc758 \uacbd\uc6b0 732M \ub9e4\uac1c\ubcc0\uc218 \uc911 247\uac1c\ub9cc \ud65c\uc131\ud654\ub429\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc804\ubb38\uac00 \uc120\ud0dd \ubc29\ubc95(Top-P, \uc804\ubb38\uac00 \uc120\ud0dd)\uc5d0 \ub530\ub978 MoE\uc640 AoE \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uac01 \uc804\ub7b5\uc758 \ud6a8\uc728\uc131\uacfc \uc801\ud569\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud558\ub958 \uc791\uc5c5(ARC-E, PIQA, SIQA, WINO, HELLA, MNLI, QNLI, SST2)\uc5d0 \ub300\ud55c \uc131\ub2a5 \uc9c0\ud45c\ub97c \uc81c\uc2dc\ud558\uc5ec, \uc5b4\ub5a4 \uc804\ubb38\uac00 \uc120\ud0dd \uc804\ub7b5\uc774 \ud2b9\uc815 \uc791\uc5c5\uc5d0 \ub354 \uc801\ud569\ud55c\uc9c0, \uadf8\ub9ac\uace0 AoE \ubaa8\ub378\uc774 \uae30\uc874 MoE \ubaa8\ub378\uc5d0 \ube44\ud574 \uc5b4\ub5a4 \uac15\uc810\uc744 \ubcf4\uc774\ub294\uc9c0 \uc790\uc138\ud788 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Model | ARC-E | PIQA | SIQA | WINO | HELLA | MNLI | QNLI | SST2 | AVG. |\n|---|---|---|---|---|---|---|---|---|---| \n| Traditional MoE | 53.70 | 65.40 | 39.10 | 51.54 | 35.80 | 32.19 | 49.77 | 57.00 | 48.06 |\n| AoE | 55.98 | 65.61 | 39.87 | 52.57 | 36.77 | 35.39 | 50.05 | 61.93 | **49.80** |", "caption": "Table 4: For 4B-parameter LLMs (with 1.18B active parameters), AoE\u00a0exhibits better downstream performance than MoE models.", "description": "\ud45c 4\ub294 40\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uac00\uc9c4 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(\ud65c\uc131 \ud30c\ub77c\ubbf8\ud130 11\uc5b5 8\ucc9c\ub9cc \uac1c)\uc5d0\uc11c, \uc790\uc728 \uc804\ubb38\uac00 \ubaa8\ub378(AoE)\uc774 \uae30\uc874\uc758 \ud63c\ud569 \uc804\ubb38\uac00 \ubaa8\ub378(MoE)\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud558\ub958 \uc791\uc5c5(ARC-E, PIQA, SIQA, WINO, HELLA, MNLI, QNLI, SST2)\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec AoE\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uba85\ud655\ud788 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \ud65c\uc131 \ud30c\ub77c\ubbf8\ud130 \uc218\uac00 \uc81c\ud55c\uc801\uc778 \uc0c1\ud669\uc5d0\uc11c\ub3c4, AoE\uac00 MoE\ubcf4\ub2e4 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc911\uc694\ud55c \uacb0\uacfc\ud45c\uc785\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"content": "| Configuration | TP. (K/s) / Mem. (GB) |\n|---|---| \n| Traditional MoE | 51.42 / 50.61 |\n| AoE ($d_{low}=64$) | 49.79 / 59.39 |\n| AoE ($d_{low}=128$) | 49.42 / 57.86 |\n| AoE ($d_{low}=256$) | 47.98 / 57.32 |\n| AoE ($d_{low}=512$) | 46.07 / 55.90 |", "caption": "Table 5: Throughput and memory usage comparison among several configurations.\nAuxiliary losses do not impact efficiency.", "description": "\ud45c 5\ub294 \ub2e4\uc591\ud55c \uc124\uc815\uc5d0\uc11c\uc758 \ucc98\ub9ac\ub7c9\uacfc \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. \ubcf4\uc870 \uc190\uc2e4 \ud568\uc218\ub294 \ud6a8\uc728\uc131\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c dlow \uac12\uc744 \uac00\uc9c4 AoE \ubaa8\ub378\uacfc \uae30\uc874 MoE \ubaa8\ub378\uc758 \ucc98\ub9ac\ub7c9(\ucd08\ub2f9 \ud1a0\ud070 \uc218)\uacfc \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9(GB)\uc744 \ube44\uad50\ud558\uc5ec AoE \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Node for Norm Calculation | MMLU (5-shot) | ARC-C (5-shot) |\n|---|---|---|\n| Mixtral 8\u00d77B | Phi-3.5-MoE-ins. | Phi-3.5-MoE-ins. |\n| **x**W<sub>g</sub> | 51.14 | 41.98 |\n| **x**W<sub>p</sub> | 39.79 | 40.19 |\n| SiLU(**x**W<sub>g</sub>) | 47.29 | 45.73 |\n| SiLU(**x**W<sub>g</sub>)\u2299**x**W<sub>p</sub> | 54.37 | 50.09 |\n| Experts\u2019 Final Outputs | 57.84 | 52.73 |\n| Performance w. Router | 70.35 | 62.12 |", "caption": "Table 6: Preliminary study results on pre-trained MoE-LLMs, selecting experts by L1superscript\ud835\udc3f1L^{1}italic_L start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm of internal activation.", "description": "\uc774 \ud45c\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c MoE(Mixture-of-Experts) \uc5b8\uc5b4 \ubaa8\ub378\uc5d0\uc11c \ub0b4\ubd80 \ud65c\uc131\ud654\uc758 L1 \ub178\ub984\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc804\ubb38\uac00\ub97c \uc120\ud0dd\ud558\ub294 \uc608\ube44 \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \ub178\ub4dc\uc5d0\uc11c \uacc4\uc0b0\ub41c L1 \ub178\ub984\uc744 \uae30\ubc18\uc73c\ub85c \ud55c MMLU(Massive Multitask Language Understanding)\uc640 ARC-C(AI2 Reasoning Challenge) \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\uac00 Mixtral 8x7B\uc640 Phi-3.5-MoE-ins \ubaa8\ub378\uc5d0 \ub300\ud574 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ub77c\uc6b0\ud130\ub97c \uc0ac\uc6a9\ud55c \uc131\ub2a5\uacfc \ube44\uad50\ud558\uc5ec, L1 \ub178\ub984 \uae30\ubc18 \uc804\ubb38\uac00 \uc120\ud0dd\uc758 \ud6a8\uc728\uc131 \ubc0f \uc815\ud655\ub3c4\ub97c \ud3c9\uac00\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3.1 An Insight: Experts \u201cKnow\u201d What They Know"}, {"content": "| Node for Norm Calculation | MMLU (5-shot) |  | ARC-C (5-shot) |  | \n|---|---|---|---|---|---| \n| Mixtral 8\u00d77B | Phi-3.5-MoE-ins. | 48.16 | 29.28 | 43.77  | 35.92 | \n|  |  | 50.43 | 34.78 | 49.49 | 40.02 | \n| \ud835\udc31\ud835\udc16<sub>g</sub> |  | 54.30 | 36.38 | 47.95 | 50.85 | \n| \ud835\udc31\ud835\udc16<sub>p</sub> |  | 50.72 | 26.43 | 46.08 | 33.02  | \n| SiLU(\ud835\udc31\ud835\udc16<sub>g</sub>) |  | 51.03 | 23.64 | 53.16 | 30.12 | \n| SiLU(\ud835\udc31\ud835\udc16<sub>g</sub>)\u2299\ud835\udc31\ud835\udc16<sub>p</sub> |  | 70.35 | 78.20 | 62.12 | 67.41 | \n| Experts\u2019 Final Outputs |  |  |  |  |  | \n| Performance w. Router |  |  |  |  |  | ", "caption": "Table 7: Preliminary study results on pre-trained MoE-LLMs, selecting experts by L\u221esuperscript\ud835\udc3fL^{\\infty}italic_L start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT norm of internal activation.", "description": "\ubcf8 \ub17c\ubb38\uc758 \ud45c 7\uc740 \uc0ac\uc804 \ud6c8\ub828\ub41c MoE(Mixture-of-Experts) \uc5b8\uc5b4 \ubaa8\ub378\uc5d0\uc11c \ub0b4\ubd80 \ud65c\uc131\ud654\uc758 L\u221e \ub178\ub984\uc744 \uae30\ubc18\uc73c\ub85c \uc804\ubb38\uac00\ub97c \uc120\ud0dd\ud558\ub294 \uc608\ube44 \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\ub294  MMLU(Massive Multitask Language Understanding)\uc640 ARC-C(AI2 Reasoning Challenge) \ub450 \uac00\uc9c0 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \ub2e4\uc591\ud55c \ub178\ub4dc\uc5d0\uc11c \uacc4\uc0b0\ub41c L\u221e \ub178\ub984\uc744 \uc0ac\uc6a9\ud558\uc5ec \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uac01 \ub178\ub4dc\ubcc4 \uacb0\uacfc\uc640 \ub77c\uc6b0\ud130\ub97c \uc0ac\uc6a9\ud55c \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec L\u221e \ub178\ub984 \uae30\ubc18 \uc804\ubb38\uac00 \uc120\ud0dd \uc804\ub7b5\uc758 \ud6a8\uc728\uc131\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "3.1 An Insight: Experts \u201cKnow\u201d What They Know"}]