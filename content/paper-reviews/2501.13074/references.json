{"references": [{"fullname_first_author": "Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-00-00", "reason": "This paper introduces the Mixture-of-Experts (MoE) layer, a foundational concept that the current research builds upon and improves."}, {"fullname_first_author": "Lepikhin", "paper_title": "{GS}hard: Scaling giant models with conditional computation and automatic sharding", "publication_date": "2021-00-00", "reason": "This paper presents GShard, a technique for scaling large language models which is highly relevant to the efficiency improvements explored in the current research."}, {"fullname_first_author": "Fedus", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-00-00", "reason": "This paper introduces Switch Transformers, another important approach to scaling large language models, offering a valuable comparison point for the proposed method."}, {"fullname_first_author": "Jiang", "paper_title": "Mixtral of experts", "publication_date": "2024-00-00", "reason": "This paper introduces Mixtral, a large language model that uses MoE, providing a strong baseline for the current work to compare against."}, {"fullname_first_author": "Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper introduces Llama, a foundational large language model architecture that serves as a basis for many subsequent models, including those used in the current research."}]}