{"references": [{"fullname_first_author": "Z. Shao", "paper_title": "Pushing the Limits of Mathematical Reasoning in Open Language Models", "publication_date": "2024-02-00", "reason": "This paper is highly relevant due to its focus on mathematical reasoning in LLMs, a key aspect of the current work's schema adherence strategy."}, {"fullname_first_author": "Remi Louf", "paper_title": "Efficient Guided Generation for Large Language Models", "publication_date": "2023-07-00", "reason": "This work is important as it explores efficient methods for controlled text generation, which is directly relevant to the problem of strict schema adherence addressed in the current paper."}, {"fullname_first_author": "DeepSeek-AI", "paper_title": "Deepeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "publication_date": "2025-01-00", "reason": "This paper is crucial as it introduces the DeepSeek R1 framework which forms the basis of the methodology used in the current work, making it a foundational reference."}, {"fullname_first_author": "Nico Erdmann", "paper_title": "AI Maturity Model for GxP Application: A Foundation for AI Validation", "publication_date": "2022-00-00", "reason": "This paper's focus on AI validation within regulated GxP environments is highly relevant to the current work, which addresses the challenge of generating schema-compliant text within the biomanufacturing domain."}, {"fullname_first_author": "Yizhong Wang", "paper_title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "publication_date": "2022-12-00", "reason": "This paper explores self-instruction methods for aligning LLMs which is relevant to the current paper's use of reinforcement learning and fine-tuning for achieving schema adherence."}]}