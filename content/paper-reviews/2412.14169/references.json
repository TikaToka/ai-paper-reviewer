{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduces the foundational autoregressive large language model architecture that inspired the vision generation model in this paper."}, {"fullname_first_author": "Aditya Ramesh", "paper_title": "Zero-shot text-to-image generation", "publication_date": "2021-07-01", "reason": "This is a seminal work that demonstrated the feasibility of high-quality image generation from text prompts, paving the way for similar video generation models."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This paper introduced denoising diffusion probabilistic models, a core technique used in many state-of-the-art image and video generation models, including those compared to in this work."}, {"fullname_first_author": "Huiwen Chang", "paper_title": "Maskgit: Masked generative image transformer", "publication_date": "2022-06-01", "reason": "This paper introduces the masked autoregressive modeling technique used in NOVA for spatial set-by-set prediction, improving efficiency and scalability."}, {"fullname_first_author": "Tianhong Li", "paper_title": "Autoregressive image generation without vector quantization", "publication_date": "2024-06-01", "reason": "This is a highly relevant work that directly inspired NOVA's non-quantized autoregressive approach for video generation, demonstrating high efficiency."}]}