[{"content": "| Method | SC \u2191 | BC \u2191 | MS \u2191 | AQ \u2191 | IQ \u2191 | PC \u2191 |\n|---|---|---|---|---|---|---|\n| I2VGen-XL | 0.83247 | 0.89147 | 0.95706 | **0.44055** | 0.58532 | 0.32665 |\n| Ours | **0.95892** | **0.95202** | **0.98456** | 0.43369 | **0.60423** | **0.37434** |", "caption": "Table 1: Quantitative Evaluation. We evaluate various aspects of our method using the VBench\u00a0[16] and VideoPhy\u00a0[4] benchmarks. The evaluated metrics are as follows: (VBench) SC: subject consistency, BC: background consistency, MS: motion smoothness, AQ: aesthetic quality, IQ: imaging quality; (VideoPhy) PC: physical commonsense. By incorporating 3D knowledge, our video model shows substantial improvement in metrics such as physical commonsense, motion smoothness, and subject/background consistency. This demonstrates that our method generates significantly more temporally consistent and physically plausible videos.", "description": "\ud45c 1\uc740 \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \ub2e4\uc591\ud55c \uce21\uba74\uc744 VBench [16] \ubc0f VideoPhy [4] \ubca4\uce58\ub9c8\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc815\ub7c9\uc801\uc73c\ub85c \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud3c9\uac00 \uc9c0\ud45c\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. (VBench) SC(\uc8fc\uc81c \uc77c\uad00\uc131), BC(\ubc30\uacbd \uc77c\uad00\uc131), MS(\uc6c0\uc9c1\uc784 \ubd80\ub4dc\ub7ec\uc6c0), AQ(\ubbf8\uc801 \ud488\uc9c8), IQ(\uc601\uc0c1 \ud488\uc9c8); (VideoPhy) PC(\ubb3c\ub9ac\uc801 \uc0c1\uc2dd). 3D \uc815\ubcf4\ub97c \ud1b5\ud569\ud568\uc73c\ub85c\uc368, \uc81c\uc548\ub41c \ube44\ub514\uc624 \ubaa8\ub378\uc740 \ubb3c\ub9ac\uc801 \uc0c1\uc2dd, \uc6c0\uc9c1\uc784 \ubd80\ub4dc\ub7ec\uc6c0, \uc8fc\uc81c/\ubc30\uacbd \uc77c\uad00\uc131\uacfc \uac19\uc740 \uc9c0\ud45c\uc5d0\uc11c \uc0c1\ub2f9\ud55c \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uc774 \uc2dc\uac04\uc801\uc73c\ub85c \ub354\uc6b1 \uc77c\uad00\uc131 \uc788\uace0 \ubb3c\ub9ac\uc801\uc73c\ub85c \ud0c0\ub2f9\ud55c \ube44\ub514\uc624\ub97c \uc0dd\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Method | Q1 \u2191 | Q2 \u2193 | Q3 \u2191 | Q4 \u2191 | Q5 \u2191 |\n|---|---|---|---|---|---| \n| I2VGen-XL | 0.138 | 0.862 | 0.135 | 0.132 | 0.137 |\n| Ours | **0.862** | **0.137** | **0.865** | **0.868** | **0.863** |", "caption": "Table 2: User Study Results. Our model demonstrates a significant improvement in physical plausibility, as assessed by human labelers. Here, Q2 asks the user to identify negative artifacts in the videos, while the other questions positively assess physical plausibility.", "description": "\uc774 \ud45c\ub294 \uc0ac\uc6a9\uc790 \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 10\uba85\uc758 \ud3c9\uac00\uc790\uac00 \uc81c\uc2dc\ub41c \ube44\ub514\uc624\ub4e4\uc744 \ube44\uad50 \ud3c9\uac00\ud558\uc5ec \ubb3c\ub9ac\uc801 \uc0ac\uc2e4\uc131\uc5d0 \ub300\ud55c \uc120\ud638\ub3c4\ub97c \ub098\ud0c0\ub0c8\uc2b5\ub2c8\ub2e4. Q2 \uc9c8\ubb38\uc740 \ube44\ub514\uc624 \ub0b4\uc758 \ubd80\uc815\uc801\uc778 \uc778\uacf5\ubb3c(artifact)\uc744 \uc2dd\ubcc4\ud558\ub294 \uac83\uc774\uace0, \ub2e4\ub978 \uc9c8\ubb38\ub4e4\uc740 \ubb3c\ub9ac\uc801 \uc0ac\uc2e4\uc131\uc744 \uae0d\uc815\uc801\uc73c\ub85c \ud3c9\uac00\ud558\ub294 \uc9c8\ubb38\uc785\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c, \uc81c\uc548\ub41c \ubaa8\ub378\uc774 \ubb3c\ub9ac\uc801 \uc0ac\uc2e4\uc131 \uce21\uba74\uc5d0\uc11c \uc0c1\ub2f9\ud55c \uac1c\uc120\uc744 \ubcf4\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "D. \uc0ac\uc6a9\uc790 \uc5f0\uad6c"}]