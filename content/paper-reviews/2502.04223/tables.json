[{"content": "| a) | <img src=\"https://arxiv.org/html/2502.04223/imr.png\" width=\"69\" height=\"156\"> |  |\n| b) | <img src=\"https://arxiv.org/html/2502.04223/im2.png\" width=\"685\" height=\"182\"> |  |", "caption": "Table 1: Summary of the datasets used to train \u00c9CLAIR, including a description of the maximum information available in the annotations of each dataset.", "description": "\ubcf8 \ud45c\ub294 \u00c9CLAIR \ubaa8\ub378 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \uc694\uc57d \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30, \ud30c\uc77c \ud615\uc2dd, \uc5b4\ub178\ud14c\uc774\uc158 \uc720\ud615(\uad6c\uc870\ud654\ub41c \ub370\uc774\ud130, \ubc14\uc6b4\ub529 \ubc15\uc2a4, \ud074\ub798\uc2a4 \uc815\ubcf4 \uc720\ubb34 \ub4f1) \ubc0f \ucd5c\ub300 \uc815\ubcf4\ub7c9\uc744 \ud3ec\ud568\ud558\uc5ec, \u00c9CLAIR \ubaa8\ub378\uc758 \ub2e4\uc591\ud55c \uae30\ub2a5\uc744 \ud3c9\uac00\ud558\ub294\ub370 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5\uc744 \uc790\uc138\ud788 \uc124\uba85\ud569\ub2c8\ub2e4.", "section": "2. \ub370\uc774\ud130\uc14b"}, {"content": "| Dataset | Size | Modality |\n|---|---|---|\n| **arXiv-5M** | 5M | Structured, Boxes, Classes |\n| **SynthTabNet [33]** | 480K | Structured, Boxes, Classes |\n| **README** | 302K | Structured |\n| **DocLayNet [40]** | 56K | Plain, Boxes, Classes |\n| **G1000 [49]** | 324K | Plain |\n| **Human-labeled Common Crawl samples** | 14K | Plain, Boxes, Classes |\n| **Total** | 6.176M |  |", "caption": "Table 2: Evaluation results on DROBS. Reported standard NLTK metrics [41] are character level (Edit-distance) or word level (F1, Precision, Recall, BLEU, METEOR) metrics typically used by the OCR and natural language processing (NLP) communities. We also report Counting F1 and word error rate/word edit distance metrics. \n*MIP-maximal-information prompt\n**Counting F1 score is computed over the set {{\\{{ he1subscripthe1\\texttt{he}_{1}he start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, said1subscriptsaid1\\texttt{said}_{1}said start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, that1subscriptthat1\\texttt{that}_{1}that start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, she1subscriptshe1\\texttt{she}_{1}she start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, said2subscriptsaid2\\texttt{said}_{2}said start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, that2subscriptthat2\\texttt{that}_{2}that start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, they1subscriptthey1\\texttt{they}_{1}they start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, said3subscriptsaid3\\texttt{said}_{3}said start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, that3subscriptthat3\\texttt{that}_{3}that start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, he2subscripthe2\\texttt{he}_{2}he start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, said4subscriptsaid4\\texttt{said}_{4}said start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, something1subscriptsomething1\\texttt{something}_{1}something start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT }}\\}}. This allows to track and penalize words that missed but has more than one occurrence in the document.", "description": "\ud45c 2\ub294 DROBS\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\uc5d0\ub294 OCR \ubc0f \uc790\uc5f0\uc5b4 \ucc98\ub9ac(NLP) \ucee4\ubba4\ub2c8\ud2f0\uc5d0\uc11c \uc77c\ubc18\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \ubb38\uc790 \uc218\uc900(\ud3b8\uc9d1 \uac70\ub9ac) \ub610\ub294 \ub2e8\uc5b4 \uc218\uc900(F1, \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728, BLEU, METEOR) \uc9c0\ud45c\uac00 \ubcf4\uace0\ub429\ub2c8\ub2e4.  *MIP(\ucd5c\ub300 \uc815\ubcf4 \ud504\ub86c\ud504\ud2b8)\ub294 \ubaa8\ub4e0 \uc8fc\uc11d \uc720\ud615\uc744 \ud3ec\ud568\ud558\ub294 \ud504\ub86c\ud504\ud2b8\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  **Counting F1 \uc810\uc218\ub294 \ubb38\uc11c\uc5d0\uc11c \uc5ec\ub7ec \ubc88 \ub098\ud0c0\ub098\ub294 \ub2e8\uc5b4\ub97c \ucd94\uc801\ud558\uace0 \ud328\ub110\ud2f0\ub97c \ubd80\uc5ec\ud558\uae30 \uc704\ud574 he1, said1, that1, she1, said2, that2, they1, said3, that3, he2, said4, something1 \uc9d1\ud569\uc5d0 \ub300\ud574 \uacc4\uc0b0\ub429\ub2c8\ub2e4.", "section": "3.1. \uc77d\uae30 \uc21c\uc11c \ubca4\uce58\ub9c8\ud06c"}, {"content": "| Method | Maskout | Counting F1 \u2191 | WER \u2193 | Edit distance \u2193 | F1 \u2191 | Precision \u2191 | Recall \u2191 | BLEU \u2191 | METEOR \u2191 |\n|---|---|---|---|---|---|---|---|---|---| \n| \u00c9CLAIR-MIP | \u2717 | 0.934 | **0.142** | 0.109 | **0.942** | 0.960 | 0.942 | **0.886** | **0.930** |\n| \u00c9CLAIR-MIP | \u2713 | **0.937** | 0.146 | **0.108** | 0.941 | **0.966** | 0.936 | 0.885 | 0.927 |\n| Kosmos-2.5 (ocr-mode) | \u2713 | 0.919 | 0.195 | 0.114 | 0.937 | 0.932 | **0.950** | 0.862 | 0.927 |\n| Kosmos-2.5 (md-mode) | \u2713 | 0.843 | 0.249 | 0.184 | 0.890 | 0.941 | 0.876 | 0.805 | 0.851 |\n| GOT (ocr-mode) | \u2713 | 0.776 | 0.302 | 0.216 | 0.818 | 0.863 | 0.825 | 0.713 | 0.795 |\n| GOT (md-mode) | \u2713 | 0.825 | 0.259 | 0.157 | 0.879 | 0.908 | 0.875 | 0.760 | 0.852 |", "caption": "Table 3: Accuracy comparison of various methods across different metrics in both English and Chinese (zh). Currently \u00c9CLAIR doesn\u2019t train with additional chinese data or other form of multi-lingual data. The numbers in top row are obtained from GOT\u00a0[52].", "description": "\ubcf8 \ud45c\ub294 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4. \uc601\uc5b4\uc640 \uc911\uad6d\uc5b4(zh) \ub450 \uac00\uc9c0 \uc5b8\uc5b4\uc5d0 \ub300\ud574, \uc5ec\ub7ec \ud3c9\uac00 \uc9c0\ud45c(Edit Distance, BLEU, METEOR \ub4f1)\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc815\ud655\ub3c4\ub97c \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4.  \u00c9CLAIR \ubaa8\ub378\uc740 \ucd94\uac00\uc801\uc778 \uc911\uad6d\uc5b4 \ub370\uc774\ud130\ub098 \ub2e4\uad6d\uc5b4 \ub370\uc774\ud130\ub85c \ud559\uc2b5\ub418\uc9c0 \uc54a\uc558\ub2e4\ub294 \uc810\uc744 \uba85\uc2dc\ud558\uace0 \uc788\uc73c\uba70, GOT [52] \ub17c\ubb38\uc5d0\uc11c \uc5bb\uc740 \uc218\uce58\uc640 \ube44\uad50\ud558\uc5ec \uacb0\uacfc\ub97c \uc81c\uc2dc\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uc9c0\ud45c\ubcc4\ub85c \uc601\uc5b4\uc640 \uc911\uad6d\uc5b4 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ud30c\uc545\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3. Results"}, {"content": "| Method | Size | Edit Distance (\u2193) en | Edit Distance (\u2193) zh | F1-score (\u2191) en | F1-score (\u2191) zh | Precision (\u2191) en | Precision (\u2191) zh | Recall (\u2191) en | Recall (\u2191) zh | BLEU (\u2191) en | BLEU (\u2191) zh | METEOR (\u2191) en | METEOR (\u2191) zh |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| Nougat [6] | 250M | 0.255 | - | 0.745 | - | 0.720 | - | 0.809 | - | 0.665 | - | 0.761 | - |\n| TextMonkey [29] | 7B | 0.265 | - | 0.821 | - | 0.778 | - | 0.906 | - | 0.671 | - | 0.762 | - |\n| DocOwl1.5 [16] | 7B | 0.258 | - | 0.862 | - | 0.835 | - | 0.962 | - | 0.788 | - | 0.858 | - |\n| Vary [50] | 7B | 0.092 | 0.113 | 0.918 | 0.952 | 0.906 | 0.961 | 0.956 | 0.944 | 0.885 | 0.754 | 0.926 | 0.873 |\n| Vary-toy [51] | 1.8B | 0.082 | 0.142 | 0.924 | 0.914 | 0.919 | 0.928 | 0.938 | 0.907 | 0.889 | 0.718 | 0.929 | 0.832 |\n| Qwen-VL-Plus [3] | - | 0.096 | 0.121 | 0.931 | 0.895 | 0.921 | 0.903 | 0.950 | 0.890 | 0.893 | 0.684 | 0.936 | 0.828 |\n| Qwen-VL-Max [3] | 72B+ | 0.057 | 0.091 | 0.964 | 0.931 | 0.955 | 0.917 | 0.977 | 0.946 | 0.942 | 0.756 | 0.971 | 0.885 |\n| Fox [26] | 1.8B | 0.046 | 0.061 | 0.952 | 0.954 | 0.957 | 0.964 | 0.948 | 0.946 | 0.930 | 0.842 | 0.954 | 0.908 |\n| GOT [52] | 580M | 0.035 | 0.038 | **0.972** | 0.980 | **0.971** | 0.982 | 0.973 | 0.978 | 0.947 | 0.878 | 0.958 | 0.939 |\n| **\u00c9CLAIR** | 936M | **0.032** | - | 0.968 | - | 0.962 | - | **0.974** | - | **0.950** | - | **0.980** | - |", "caption": "Table 4: Evaluation of the Nougat-Base model on the Nougat validation set (as reported in \u00a0[6]), and of \u00c9CLAIR pre-trained on the arXiv-5M dataset and validated on the corresponding validation set. Note: We do not aim to provide a direct comparison between Nougat and \u00c9CLAIR here due to the concerns discussed in Section\u00a03.2.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 3.2\uc808\uc5d0\uc11c \ub17c\uc758\ub41c \ubb38\uc81c\uc810\ub4e4 \ub54c\ubb38\uc5d0 Nougat \ubaa8\ub378\uacfc \u00c9CLAIR \ubaa8\ub378\uc744 \uc9c1\uc811 \ube44\uad50\ud558\uc9c0 \uc54a\ub294\ub2e4\ub294 \uc810\uc744 \uba85\uc2dc\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  Nougat \uae30\ubcf8 \ubaa8\ub378\uc758 Nougat \uac80\uc99d \uc138\ud2b8 \ud3c9\uac00 \uacb0\uacfc\uc640 arXiv-5M \ub370\uc774\ud130 \uc138\ud2b8\ub85c \uc0ac\uc804 \ud6c8\ub828\ub418\uace0 \ud574\ub2f9 \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c \uac80\uc99d\ub41c \u00c9CLAIR \ubaa8\ub378\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 '\ubaa8\ub4e0', '\ud14d\uc2a4\ud2b8', '\uc218\ud559', '\ud45c' \ub4f1\uc758 \ubaa8\ub2ec\ub9ac\ud2f0\ubcc4\ub85c  \ud3b8\uc9d1 \uac70\ub9ac, BLEU, METEOR, \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728, F1 \uc810\uc218\uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uc9c0\ud45c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "3. \uacb0\uacfc (Results)"}, {"content": "| Method | Size | Modality | Edit Distance \u2193 | BLEU \u2191 | METEOR \u2191 | Precision \u2191 | Recall \u2191 | F1-score \u2191 |\n|---|---|---|---|---|---|---|---|---|\n| Nougat-Base [6] | 350M | All | 0.071 | 0.891 | 0.930 | 0.935 | 0.928 | 0.931 |\n|  |  | Text | 0.058 | 0.912 | 0.946 | 0.962 | 0.953 | 0.957 |\n|  |  | Math | 0.128 | 0.569 | 0.754 | 0.765 | 0.766 | 0.765 |\n|  |  | Tables | 0.211 | 0.697 | 0.791 | 0.754 | 0.807 | 0.780 |\n| \u00c9CLAIR | 963M | All | 0.026 | 0.952 | 0.998 | 0.970 | 0.970 | 0.970 |\n|  |  | Text | 0.015 | 0.979 | 0.996 | 0.992 | 0.990 | 0.990 |\n|  |  | Math | 0.123 | 0.679 | 0.934 | 0.858 | 0.860 | 0.853 |\n|  |  | Tables | 0.064 | 0.871 | 0.992 | 0.918 | 0.916 | 0.916 |", "caption": "Table 5: Comparison of \u00c9CLAIR on DROBS before and after the fine-tuning stage, and also with and without a repetition-penalty (after the fine-tuning stage).", "description": "\ubcf8 \ud45c\ub294 \ubbf8\uc138 \uc870\uc815 \uc804\uacfc \ud6c4\uc758 DROBS\uc5d0 \ub300\ud55c \u00c9CLAIR\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\uc640 \ubbf8\uc138 \uc870\uc815 \ud6c4 \ubc18\ubcf5 \ud328\ub110\ud2f0 \uc801\uc6a9 \uc720\ubb34\uc5d0 \ub530\ub978 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \u00c9CLAIR \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud558\uae30 \uc804\uacfc \ud6c4, \uadf8\ub9ac\uace0 \ubbf8\uc138 \uc870\uc815 \ud6c4 \ubc18\ubcf5 \ud328\ub110\ud2f0\ub97c \uc801\uc6a9\ud588\uc744 \ub54c\uc640 \uc801\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uc815\ud655\ud558\uac8c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.  \ud2b9\ud788, DROBS \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \uc9c0\ud45c (Counting F1, WER \ub4f1) \ub97c \uc0ac\uc6a9\ud558\uc5ec  \u00c9CLAIR \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.1. \uc77d\uae30 \uc21c\uc11c \ubca4\uce58\ub9c8\ud06c"}, {"content": "| PreTraining | FineTuning | Repetition Penalty=1.1 | Counting F1** \u2191 | WER \u2193 |\n|---|---|---|---|---|\n| \u2713 | \u2717 | \u2717 | 0.663 | 0.264 |\n| \u2713 | \u2713 | \u2717 | 0.925 | 0.151 |\n| \u2713 | \u2713 | \u2713 | 0.934 | 0.142 |", "caption": "Table 6: COCO-mAP (with defaults IoU=0.5:0.95, area=all, maxDets=100) on DocLayNet Benchmark.", "description": "\ud45c 6\uc740 DocLayNet \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c COCO-mAP \ud3c9\uac00 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \u00c9CLAIR \ubaa8\ub378\uc758 \uac1d\uccb4 \ud0d0\uc9c0 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. COCO-mAP\ub294 IoU(Intersection over Union) \uc784\uacc4\uac12 0.5\uc5d0\uc11c 0.95\uae4c\uc9c0, \uac1d\uccb4 \ud06c\uae30\ub294 \ubaa8\ub4e0 \ud06c\uae30(area=all), \ucd5c\ub300 \ud0d0\uc9c0 \uac1c\uc218\ub294 100\uac1c(maxDets=100)\ub85c \uc124\uc815\ud558\uc5ec \uacc4\uc0b0\ub429\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \ud074\ub798\uc2a4(Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title)\uc5d0 \ub300\ud55c COCO-mAP \uc810\uc218\ub97c \ubcf4\uc5ec\uc8fc\uba70, \ubaa8\ub4e0 \ud074\ub798\uc2a4\uc5d0 \ub300\ud55c \uc885\ud569\uc801\uc778 COCO-mAP \uc810\uc218\ub3c4 \ud3ec\ud568\ud569\ub2c8\ub2e4.", "section": "3.3. Document Object Detection"}, {"content": "| Classes | Mask-RCNN [14] | SwinDoc Segmenter [4] | \u00c9CLAIR |\n|---|---|---|---| \n| Caption | 71.5 | 83.5 | **83.5** |\n| Footnote | **71.8** | 67.8 | 66.9 |\n| Formula | 63.4 | 64.2 | **65.7** |\n| List-item | 80.8 | **84.1** | 79.0 |\n| Page-footer | 59.3 | **65.1** | 62.0 |\n| Page-header | 70.0 | **71.3** | 70.7 |\n| Picture | 72.7 | **85.6** | 76.9 |\n| Sec-header | **69.3** | 68.0 | 67.0 |\n| Table | 82.9 | **86.0** | 77.6 |\n| Text | **85.8** | 84.5 | 82.0 |\n| Title | 80.4 | 66.8 | **82.0** |\n| All | 73.5 | **75.2** | 73.9 |", "caption": "Table 7: Comparison of the Nemotron-8B accuracy when trained on data extracted with \u00c9CLAIR or PyMuPDF4LLM\u00a0[19].", "description": "\uc774 \ud45c\ub294 Nemotron-8B \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4.  \u00c9CLAIR\uacfc PyMuPDF4LLM [19] \ub450 \uac00\uc9c0 \ubc29\ubc95\uc73c\ub85c \ucd94\ucd9c\ud55c \ub370\uc774\ud130\ub85c Nemotron-8B \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a8 \ud6c4, MMLU \ubca4\uce58\ub9c8\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc131\ub2a5\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ubc29\ubc95\uc73c\ub85c \ucd94\ucd9c\ud55c \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ub41c \ubaa8\ub378\uc758 MMLU \uc815\ud655\ub3c4\uc640 \ud3c9\uade0 \uc815\ud655\ub3c4\uac00 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4.  \u00c9CLAIR\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucd94\ucd9c\ud55c \ub370\uc774\ud130\ub85c \ud559\uc2b5\ub41c \ubaa8\ub378\uc774 \ub354 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.4 LLM \ubca4\uce58\ub9c8\ud06c"}, {"content": "| Method | Tokens Extracted (B) | MMLU \u2191 | Other Bench Avg \u2191 |\n|---|---|---|---| \n| PyMuPDF4LLM [19] | 43.6 | 37.2 | 55.72 |\n| \u00c9CLAIR | 55.1 | **39.1** | **56.7** |", "caption": "Table 8: Results and speed of multi-token models and competing methods. We report the average speed per image on DROBS test set (s\u2062e\u2062ci\u2062m\u2062g\ud835\udc60\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc5a\ud835\udc54\\frac{sec}{img}divide start_ARG italic_s italic_e italic_c end_ARG start_ARG italic_i italic_m italic_g end_ARG), and speed per 100 tokens (s\u2062e\u2062c100\ud835\udc60\ud835\udc52\ud835\udc50100\\frac{sec}{100}divide start_ARG italic_s italic_e italic_c end_ARG start_ARG 100 end_ARG). These values are obtained from a PyTorch-based inference pipeline on an NVIDIA H-100 GPU.", "description": "\ud45c 8\uc740 \ub2e4\uc911 \ud1a0\ud070 \ubaa8\ub378\uacfc \uacbd\uc7c1 \ubaa8\ub378\uc758 \uacb0\uacfc\uc640 \uc18d\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. DROBS \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0\uc11c \uc774\ubbf8\uc9c0\ub2f9 \ud3c9\uade0 \uc18d\ub3c4(\ucd08/\uc774\ubbf8\uc9c0)\uc640 100\uac1c \ud1a0\ud070\ub2f9 \uc18d\ub3c4(\ucd08/100\ud1a0\ud070)\ub97c \ubcf4\uace0\ud569\ub2c8\ub2e4. \uc774 \uac12\uc740 NVIDIA H-100 GPU\uc5d0\uc11c PyTorch \uae30\ubc18 \ucd94\ub860 \ud30c\uc774\ud504\ub77c\uc778\uc73c\ub85c \uc5bb\uc740 \uac83\uc785\ub2c8\ub2e4.", "section": "3.5. \ub2e4\uc911 \ud1a0\ud070 \ucd94\ub860"}, {"content": "| Method | tkn/step | WER \u2193 | F1 \u2191 | sec/img \u2193 | sec/100 \u2193 |\n|---|---|---|---|---|---| \n| Nougat [6] | 1 | - | - | 4.7 | 0.41 |\n| GOT [52] | 1 | 0.25 | 0.82 | 9.8 | 0.90 |\n| \u00c9CLAIR | 1 | 0.14 | 0.93 | 3.8 | 0.42 |\n| \u00c9CLAIR-2tkn | 2 | 0.13 | 0.94 | 2.5 | 0.31 |\n|  | 1 | **0.12** | **0.95** | 3.8 | 0.42 |\n| \u00c9CLAIR-3tkn | 3 | 0.15 | 0.92 | 1.77 | 0.23 |\n|  | 1 | 0.13 | 0.94 | 3.8 | 0.42 |\n| \u00c9CLAIR-4tkn | 4 | 0.17 | 0.90 | **1.32** | **0.20** |\n|  | 1 | 0.14 | 0.94 | 3.8 | 0.42 |", "caption": "Table S1: The mean precision and mean recall of \u00c9CLAIR for each class and the corresponding mean recall and mean precision of SwinDocSegmenter for the respective recall/precision on the PR-curve evaluated on the DocLayNet evaluation dataset. Computed for I\u2062o\u2062U\u22650.5\ud835\udc3c\ud835\udc5c\ud835\udc480.5IoU\\geq 0.5italic_I italic_o italic_U \u2265 0.5 (corresponding to Fig.\u00a0S4) and for averaged thresholds of I\u2062o\u2062U\u2265{0.5,0.55,\u2026,0.9,0.95}\ud835\udc3c\ud835\udc5c\ud835\udc480.50.55\u20260.90.95IoU\\geq\\{0.5,0.55,...,0.9,0.95\\}italic_I italic_o italic_U \u2265 { 0.5 , 0.55 , \u2026 , 0.9 , 0.95 } (default for COCO metrics). *SDS: SwinDocSegmenter.", "description": "\ud45c S1\uc740 DocLayNet \ud3c9\uac00 \ub370\uc774\ud130\uc14b\uc5d0\uc11c PR \uace1\uc120\uc758 \uac01 \uc7ac\ud604\uc728/\uc815\ubc00\ub3c4\uc5d0 \ub300\ud574 \u00c9CLAIR\uacfc SwinDocSegmenter\uc758 \ud3c9\uade0 \uc7ac\ud604\uc728 \ubc0f \ud3c9\uade0 \uc815\ubc00\ub3c4\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. IoU\u22650.5 \ubc0f IoU\u2265{0.5, 0.55,\u2026,0.9, 0.95}\uc5d0 \ub300\ud574 \uacc4\uc0b0\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  *SDS\ub294 SwinDocSegmenter\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "S5.2. \ub300\uc751\ud558\ub294 \uc815\ubc00\ub3c4/\uc7ac\ud604\uc728 \uc218\uc900 \ube44\uad50"}]