{"references": [{"fullname_first_author": "Peter Anderson", "paper_title": "Greenback bears and fiscal hawks: Finance is a jungle and text embeddings must adapt", "publication_date": "2024-11-01", "reason": "This paper is highly relevant due to its focus on the challenges of applying general-purpose text embedding models to the financial domain and proposes a novel solution."}, {"fullname_first_author": "Elliot Bolton", "paper_title": "BioMedLM: A 2.7B parameter language model trained on biomedical text", "publication_date": "2024-03-15", "reason": "This paper is important because it details the development of a large language model specialized for the biomedical domain, highlighting the growing trend of domain-specific language models."}, {"fullname_first_author": "Jing Chen", "paper_title": "The BQ corpus: A large-scale domain-specific Chinese corpus for sentence semantic equivalence identification", "publication_date": "2018-11-01", "reason": "This paper is highly relevant as it introduces a significant dataset used in the benchmark, showcasing the importance of domain-specific datasets for evaluation."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-01", "reason": "This is a foundational paper in NLP, introducing the BERT model architecture which many subsequent embedding models build upon, including models evaluated in this paper."}, {"fullname_first_author": "Tomas Mikolov", "paper_title": "Efficient estimation of word representations in vector space", "publication_date": "2013-01-15", "reason": "This paper is highly influential because it introduced the Word2Vec model, a seminal work in word embeddings which is a foundation for many of the embedding models discussed in the paper."}]}