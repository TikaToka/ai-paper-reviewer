---
title: "SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models"
summary: "Self-play with refinement boosts instruction-following in LLMs."
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Natural Language Processing", "Large Language Models", "ğŸ¢ Tsinghua University",]
showSummary: true
date: 2024-12-16
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2412.11605 {{< /keyword >}}
{{< keyword icon="writer" >}} Jiale Cheng et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2024-12-17 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2412.11605" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2412.11605" target="_self" >}}
â†— Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/spar-self-play-with-tree-search-refinement-to" target="_self" >}}
â†— Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

**LLMs struggle with complex instructions**, often getting distracted by irrelevant details. Current methods for training LLMs with preferences create comparisons between entirely different responses, which worsens the issue. This introduces variations that are unrelated to actually following the instructions and makes it difficult for models to identify the key factors that lead to correct responses. **Existing preference learning methods do not address this subtle but crucial issue**.  As a result, LLMs fail to accurately reflect subtle nuances within the instructions and their output.  This limitation hinders the effectiveness of preference learning in enhancing instruction-following ability, especially for multi-constraint tasks.

**SPAR, a self-play framework with tree-search refinement, is proposed** to address the limitations of current preference learning techniques. LLMs learn by **playing against themselves**, refining their own imperfect responses. **Tree search** systematically explores possible refinements, while minimizing interference.  This approach helps LLMs focus on the key differences that lead to better instruction following, without getting lost in unrelated details.  Experiments show significant improvements over other self-improvement methods.  Impressively, SPAR even surpasses GPT-4-Turbo on a key benchmark, demonstrating its effectiveness in enhancing instruction-following capability.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Self-play with tree-search refinement significantly improves instruction following in LLMs. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The proposed method, SPAR, outperforms existing techniques and even surpasses GPT-4-Turbo on certain benchmarks. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} SPAR enhances model scalability and shows potential for continuous self-improvement without relying on bootstrapping data or human feedback {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
**SPAR offers a novel approach to improving instruction-following in LLMs, which is crucial for aligning these powerful models with human intent.**  By focusing on refinement and minimizing interference, it enhances both performance and robustness. The demonstrated scalability across different model sizes and the potential for continuous self-improvement make SPAR **a significant contribution to the field**. This work opens up **new possibilities for developing more aligned and reliable LLMs**, impacting various applications. The iterative nature and self-play aspect offer a unique perspective on model training, potentially inspiring further research in autonomous LLM alignment and improvement.

------
#### Visual Insights



![](https://arxiv.org/html/2412.11605/x1.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ ë…ë¦½ì ìœ¼ë¡œ ìƒ˜í”Œë§ëœ ì—¬ëŸ¬ ì‘ë‹µì—ì„œ ë°œìƒí•˜ëŠ” ê°„ì„­ ìš”ì¸(ìŠ¤í† ë¦¬ ë‚´ìš©)ì˜ ì˜ˆì‹œ(ì™¼ìª½)ì™€ ì´ëŸ¬í•œ ìš”ì¸ì„ ë°°ì œí•˜ê³  í•µì‹¬ ì°¨ì´ì (ë§ˆì§€ë§‰ ë¬¸ì¥)ì„ ê°•ì¡°í•˜ì—¬ ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµëœ LLaMA3-8B-Instructì˜ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜¨ ê°œì„ ëœ ì‘ë‹µ ìŒ(ì˜¤ë¥¸ìª½)ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì™¼ìª½ ë¶€ë¶„ì€ ì£¼ì–´ì§„ ì§€ì‹œ(Write a story and end it with 'The devil is in the details.')ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ì´ì•¼ê¸°(<Hansel and Gretel>, <Little Red Riding Hood>)ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ì˜ ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì˜¤ë¥¸ìª½ ë¶€ë¶„ì€ IFEval ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ í‰ê·  ì ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê·¸ë˜í”„ë¡œ, ì„¸ ë²ˆì˜ ë°˜ë³µ í•™ìŠµ í›„ SPARê°€ ì§ì ‘ ìƒ˜í”Œë§ëœ ìŒë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 1: An example of the interfering factors (story content) in independently sampled multiple responses (Left). Refined response pairs exclude these factors, highlight the key difference (ending sentence), and lead to improved performance on iteratively trained LLaMA3-8B-Instruct (Right).
> </details>





{{< table-caption >}}
| Model | IFEval | | | | | FollowBench (SSR) | | | | | |
|---|---|---|---|---|---|---|---|---|---|---|---|
| | P (L) | I (L) | P (S) | I (S) | Avg. | Lv-1 | Lv-2 | Lv-3 | Lv-4 | Lv-5 | Avg. |
|---|---|---|---|---|---|---|---|---|---|---|---|
| **LLaMA3-8B Models** | | | | | | | | | | | |
| LLaMA3-8B-Instruct | 77.6 | 84.5 | 70.6 | 78.9 | 77.9 | 69.4 | 62.2 | 63.1 | 61.9 | 60.9 | 63.5 |
| AutoIF-8Bâ€  | 43.1 | 56.0 | 28.8 | 42.2 | 42.5 | 54.6 | 52.1 | 50.0 | 49.0 | 43.7 | 49.9 |
| SELF | 78.2 | 84.5 | 76.0 | 82.9 | 80.4 | 68.3 | 65.7 | 65.2 | 62.2 | 62.4 | 64.8 |
| Humpback | 72.5 | 80.2 | 70.1 | 78.1 | 75.2 | 66.8 | 66.1 | 67.2 | 60.2 | 62.6 | 64.6 |
| Self-Rewarding | 77.3 | 84.2 | 74.1 | 81.7 | 79.3 | 72.8 | 66.6 | 66.8 | **64.9** | 64.1 | 67.0 |
| Meta-Rewarding | 77.8 | 84.1 | 75.4 | 82.3 | 79.9 | 73.9 | 71.9 | 66.0 | 62.3 | 62.6 | 67.3 |
| SPaR-8B-SFT | 75.4 | 82.5 | 73.4 | 80.6 | 78.0 | 73.9 | 67.4 | 68.1 | 63.1 | 61.3 | 66.8 |
| SPaR-8B-DPO-iter1 | 78.0 | 84.7 | 75.8 | 82.6 | 80.3 | **75.3** | 67.7 | 67.6 | 64.7 | 62.3 | 67.5 |
| SPaR-8B-DPO-iter2 | 78.9 | 85.0 | 77.1 | 83.3 | 81.1 | 73.9 | 71.9 | 69.1 | 64.0 | 62.2 | 68.2 |
| SPaR-8B-DPO-iter3 | **79.9** | **85.4** | **78.0** | **83.7** | **81.8** | 73.0 | **72.3** | **70.0** | 64.1 | **64.7** | **68.8** |
| 
cdashline{1-12}Â Â w/ tree search | 82.4 | 87.5 | 79.5 | 85.3 | 83.7 | 73.9 | 71.7 | 70.3 | 66.8 | 64.1 | 69.4 |
| **GLM-4-9B Models** | | | | | | | | | | | |
| GLM-4-9B-Chat | 71.5 | 79.9 | 68.0 | 77.2 | 74.2 | 80.8 | 75.1 | 67.4 | 64.3 | **65.4** | 70.6 |
| SPaR-9B-SFT | 71.5 | 80.5 | 68.8 | 78.1 | 74.7 | 79.4 | 70.9 | 68.2 | 65.1 | 63.7 | 69.5 |
| SPaR-9B-DPO-iter3 | **77.3** | **84.1** | **73.6** | **81.4** | **79.1** | **82.7** | **76.7** | **67.9** | **68.3** | 64.2 | **72.0** |
| **LLaMA3-70B Models** | | | | | | | | | | | |
| LLaMA3-70B-Instruct | 83.7 | 88.9 | 77.1 | 83.8 | 83.4 | 77.1 | 72.5 | 69.4 | 68.7 | 66.3 | 70.8 |
| AutoIF-70Bâ€  | **85.6** | **90.4** | 80.2 | 86.7 | 85.7 | 71.0 | 67.2 | 66.2 | 64.6 | 63.5 | 66.5 |
| SPaR-70B-DPO-iter3 | **85.6** | 90.2 | **81.3** | **87.3** | **86.1** | **80.3** | **75.7** | **71.4** | **73.7** | **70.5** | **74.3** |{{< /table-caption >}}

> ğŸ”¼ ì´ í‘œëŠ” ì—¬ëŸ¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µ í•™ìŠµì‹œì¼°ì„ ë•Œ ëª…ë ¹ì–´ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë”°ë¥´ëŠ”ì§€ í‰ê°€í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° LLMë§ˆë‹¤ ìµœê³  ì„±ëŠ¥ì„ êµµê²Œ í‘œì‹œí–ˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë…¼ë¬¸ì˜ í‘œ 6ì„ ì°¸ì¡°í•˜ì„¸ìš”. í‰ê°€ ì§€í‘œëŠ” í¬ê²Œ ëª…ë ¹ì–´ ìˆ˜ì¤€(I)ê³¼ í”„ë¡¬í”„íŠ¸ ìˆ˜ì¤€(P)ìœ¼ë¡œ ë‚˜ë‰˜ë©°, ê°ê° ëŠìŠ¨í•œ í‰ê°€(L)ì™€ ì—„ê²©í•œ í‰ê°€(S)ë¡œ ì„¸ë¶„í™”ë©ë‹ˆë‹¤. ë˜í•œ, ì¶”ë¡  ê³¼ì •ì—ì„œ íŠ¸ë¦¬ íƒìƒ‰ ê¸°ë²•ì„ ì‚¬ìš©í–ˆì„ ë•Œì˜ ê²°ê³¼ëŠ” ë…¹ìƒ‰ìœ¼ë¡œ ê°•ì¡° í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 1: Main results of iteratively trained LLMs on instruction-following benchmarks (Cf. Table 6 for full results). P stands for prompt level, and I represents instruction level. L and S denote loose and strict evaluations, respectively. Avg. indicates average results and Lv means level. Results using inference-time tree search are highlighted in green. The highest results for each backbone model is bolded. Scores marked with â€  are sourced directly from the original paper.
> </details>





### In-depth insights


#### Preference Bias
**ì„ í˜¸ í¸í–¥**ì€ ê°•í™” í•™ìŠµì—ì„œ ì¤‘ìš”í•œ ë¬¸ì œì…ë‹ˆë‹¤. ëª¨ë¸ì´ íŠ¹ì • ì‘ë‹µì„ ì„ í˜¸í•˜ê²Œ ë˜ë©´ **ë‹¤ì–‘ì„± ë¶€ì¡±**ê³¼ **í¸í–¥ëœ ì¶œë ¥**ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” í›ˆë ¨ ë°ì´í„°ì˜ **ë¶ˆê· í˜•**, **ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„ì˜ ê²°í•¨**, ë˜ëŠ” **íƒìƒ‰ ì „ëµì˜ ë¶€ì ì ˆì„±**ì—ì„œ ê¸°ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„ í˜¸ í¸í–¥ì„ ì™„í™”í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œëŠ” **ë‹¤ì–‘í•œ ë°ì´í„° ìˆ˜ì§‘**, **ë³´ìƒ í•¨ìˆ˜ ì¬ì„¤ê³„**, **ê·œì¹™ ê¸°ë°˜ ë³´ìƒ ì¶”ê°€**, **ì—­ê°•í™” í•™ìŠµ**, **íƒìƒ‰-í™œìš© ê· í˜• ì¡°ì •** ë“±ì´ ìˆìŠµë‹ˆë‹¤. **SPAR**ì™€ ê°™ì€ ìê¸° ëŒ€ì „ ê¸°ë°˜ í•™ìŠµ ë° íŠ¸ë¦¬ íƒìƒ‰ ê¸°ë°˜ ê°œì„ ì€ **ì„ í˜¸ í¸í–¥ì„ ì¤„ì´ê³ ** **ì§€ì‹œ ë”°ë¥´ê¸° ì„±ëŠ¥ì„ í–¥ìƒ**ì‹œí‚¤ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ìì²´ í‰ê°€ì— ëŒ€í•œ **ê°ê´€ì ì¸ ê²€ì¦** ë˜í•œ ì¤‘ìš”í•˜ë©°, **ì¸ê°„ í”¼ë“œë°±**ì„ í†µí•œ ì§€ì†ì ì¸ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.

#### Self-Play Refinement
**ì…€í”„ í”Œë ˆì´ ê°œì„ **ì€ LLMì˜ ëª…ë ¹ì–´ ì¤€ìˆ˜ ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•œ í•µì‹¬ ì „ëµì…ë‹ˆë‹¤. LLMì´ **ìì‹ ê³¼ ëŒ€ê²°í•˜ë©° í•™ìŠµ**í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, í–‰ìœ„ìì™€ ê°œì„ ì ì—­í• ì„ ë²ˆê°ˆì•„ ìˆ˜í–‰í•©ë‹ˆë‹¤. í–‰ìœ„ìëŠ” ì£¼ì–´ì§„ ëª…ë ¹ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ê³ , ê°œì„ ìëŠ” ì´ ì‘ë‹µì„ í‰ê°€í•˜ê³  ê°œì„ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ **ë°˜ë³µì  ê³¼ì •**ì„ í†µí•´ ëª¨ë¸ì€ **ë¯¸ë¬˜í•œ ì°¨ì´**ë¥¼ í•™ìŠµí•˜ê³  ëª…ë ¹ì–´ ì¤€ìˆ˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. **íŠ¸ë¦¬ ê²€ìƒ‰ ê¸°ë°˜ ê°œì„ **ì€ íš¨ê³¼ì ì¸ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤. ì´ëŠ” ë‹¨ìˆœíˆ ìµœì ì˜ ì‘ë‹µì„ ì°¾ëŠ” ê²ƒë¿ ì•„ë‹ˆë¼, ë‹¤ì–‘í•œ ì‘ë‹µì„ íƒìƒ‰í•˜ì—¬ ëª¨ë¸ì´ **í•µì‹¬ì ì¸ ì°¨ì´**ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ì¦‰, ì…€í”„ í”Œë ˆì´ ê°œì„ ì€ **ì§€ì†ì ì¸ ìê¸° ê°œì„ **ì„ ìœ„í•œ ìœ íš¨í•œ ì „ëµì…ë‹ˆë‹¤.

#### Iterative LLM Training
**ë°˜ë³µì  LLM í›ˆë ¨**ì€ LLMì˜ ì„±ëŠ¥ì„ ì ì§„ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ëŠ” ê°•ë ¥í•œ ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ëª¨ë¸ì˜ ìì²´ ì˜ˆì¸¡, ì™¸ë¶€ í”¼ë“œë°± ë˜ëŠ” ê°•í™” í•™ìŠµì—ì„œ íŒŒìƒëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ì—¬ëŸ¬ í›ˆë ¨ ì£¼ê¸°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ê° ì£¼ê¸°ì—ì„œ ëª¨ë¸ì€ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ í›ˆë ¨ë˜ì–´ ì´ì „ ë°˜ë³µì˜ ê²°í•¨ì„ í•´ê²°í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì´ ë°˜ë³µì  í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ëª¨ë¸ì€ ë³´ë‹¤ ì •í™•í•˜ê³  íš¨ìœ¨ì ì´ë©° ë‹¤ì–‘í•œ ì‘ì—…ì— ì í•©í•˜ê²Œ ë°œì „í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ **ê³¼ì í•©** ë° **ê³„ì‚° ë¹„ìš©**ê³¼ ê°™ì€ ë¬¸ì œëŠ” ì‹ ì¤‘í•˜ê²Œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ, ê° ë°˜ë³µì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì˜ í’ˆì§ˆì´ ìµœì¢… ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°ì´í„° ì„ íƒ ë° ì •ì œ ê³¼ì •ì—ì„œ ì£¼ì˜ë¥¼ ê¸°ìš¸ì—¬ì•¼ í•©ë‹ˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ ë°˜ë³µì  LLM í›ˆë ¨ì€ **ì§€ì†ì ì¸ ìê¸° ê°œì„ **ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ìœ ë§í•œ ë°©ë²•ì´ì§€ë§Œ ì„±ê³µì ì¸ êµ¬í˜„ì„ ìœ„í•´ì„œëŠ” ì„¸ì‹¬í•œ ê³„íšê³¼ ì‹¤í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤.

#### Instruction Following
**ëª…ë ¹ì–´ ì¤€ìˆ˜**ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ, ì£¼ì–´ì§„ ëª…ë ¹ì„ ì •í™•íˆ ì´í•´í•˜ê³  ë”°ë¥´ëŠ” ëŠ¥ë ¥ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ë‹¤ì–‘í•œ ìƒí™©ì— ëŒ€ì‘í•˜ëŠ” LLMì˜ ì„±ëŠ¥ì„ ì¢Œìš°í•˜ëŠ” ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤. ëª…ë ¹ì–´ ì¤€ìˆ˜ ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•´ **ë‹¤ì–‘í•œ í‰ê°€ ë²¤ì¹˜ë§ˆí¬**ê°€ ê°œë°œë˜ì—ˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ ì¸¡ì •í•˜ê³  ê°œì„  ë°©í–¥ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, **ì§€ì†ì ì¸ ìê¸° ê°œì„  ë° ê°•í™” í•™ìŠµ ê¸°ë²•**ì„ í†µí•´ ëª…ë ¹ì–´ ì¤€ìˆ˜ ëŠ¥ë ¥ì„ ë”ìš± ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. **ë¯¸ë¬˜í•œ ì°¨ì´ë¥¼ ì¸ì‹í•˜ê³  ë°˜ì˜í•˜ëŠ” ëŠ¥ë ¥**ì€ ê³ í’ˆì§ˆì˜ ëª…ë ¹ì–´ ì¤€ìˆ˜ë¥¼ ìœ„í•œ ì¤‘ìš”í•œ ìš”ì†Œì´ë©°, ì´ë¥¼ ìœ„í•´ **ìê°€ í•™ìŠµ ë° ê²€ìƒ‰ ê¸°ë°˜ ê°œì„  ì „ëµ** ë“± ë‹¤ì–‘í•œ ë°©ë²•ì´ ì—°êµ¬ë˜ê³  ìˆìŠµë‹ˆë‹¤. ê¶ê·¹ì ìœ¼ë¡œ ëª…ë ¹ì–´ ì¤€ìˆ˜ëŠ” LLMì´ ì¸ê°„ê³¼ ìì—°ìŠ¤ëŸ½ê³  íš¨ìœ¨ì ìœ¼ë¡œ ìƒí˜¸ ì‘ìš©í•˜ëŠ” ë° í•„ìˆ˜ì ì¸ ëŠ¥ë ¥ì…ë‹ˆë‹¤.

#### IFEval Benchmark
**IFEval ë²¤ì¹˜ë§ˆí¬**ëŠ” ì½”ë“œ ê¸°ë°˜ í‰ê°€ë¥¼ ìœ„í•´ íŠ¹ë³„íˆ ê³ ì•ˆëœ 541ê°œì˜ ê²€ì¦ ê°€ëŠ¥í•œ ëª…ë ¹ì–´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í‚¤ì›Œë“œ ë¹ˆë„, ë‹¨ì–´ ìˆ˜ì™€ ê°™ì€ ì‘ì—…ì„ í¬í•¨í•˜ì—¬ 25ê°€ì§€ ìœ í˜•ì˜ ê²€ì¦ ê°€ëŠ¥í•œ ëª…ë ¹ì–´ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ê°ê´€ì ì¸ í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ë¯€ë¡œ ì½”ë“œ ìƒì„± ë° ì´í•´ ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë° ì í•©í•©ë‹ˆë‹¤. **IFEval**ì€ ëª…ë ¹ì–´ë¥¼ ë”°ë¥´ëŠ” ëŠ¥ë ¥ì„ ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ë¯€ë¡œ ë¯¸ë¬˜í•œ ì°¨ì´ì—ë„ ë¯¼ê°í•©ë‹ˆë‹¤. ë”°ë¼ì„œ **IFEval**ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ì€ **ë³µì¡í•œ ëª…ë ¹ì–´ë¥¼ ì •í™•íˆ ë”°ë¥´ëŠ” ë° ëŠ¥ìˆ™**í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2412.11605/x2.png)

> ğŸ”¼ SPaRì€ ì•¡í„°ì™€ ë¦¬íŒŒì´ë„ˆ, ë‘ ê°œì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ìê¸° ê°œì„  í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ê·¸ë¦¼ 2ëŠ” ì´ í”„ë ˆì„ì›Œí¬ì˜ ë°˜ë³µì ì¸ í›ˆë ¨ ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. të²ˆì§¸ ë°˜ë³µì—ì„œ ë¦¬íŒŒì´ë„ˆ RtëŠ” ì•¡í„° Mtê°€ ìƒì„±í•œ ì‘ë‹µì„ í‰ê°€í•˜ì—¬ ë¶€ì •ì ì¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ íŠ¸ë¦¬ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ë¶ˆì™„ì „í•œ ì‘ë‹µì„ ê°œì„ í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ìˆ˜ì§‘ ë° ê°œì„ ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•¡í„°ì™€ ë¦¬íŒŒì´ë„ˆë¥¼ ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•´ ìµœì í™”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°˜ë³µì ì¸ ìê¸° í”Œë ˆì´ ê³¼ì •ì„ í†µí•´ ëª¨ë¸ì€ ì§€ì†ì ìœ¼ë¡œ ìê¸° ê°œì„ ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 2: SPaR iterative training framework. At iteration tğ‘¡titalic_t, the refiner Rtsubscriptğ‘…ğ‘¡R_{t}italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT first judges the generated responses from the actor Mtsubscriptğ‘€ğ‘¡M_{t}italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to collect negative data. Next, a tree-search algorithm is employed to refine these imperfect responses. Finally, using the data from the above steps, we can optimize the actor and refiner for the next iteration, aiming for continuous self-improvement.
> </details>



![](https://arxiv.org/html/2412.11605/extracted/6072122/figures/baseline.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ SPaR-8B ëª¨ë¸ì´ IFEval ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë‹¤ë¥¸ ê¸°ì¤€ì„ ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. xì¶•ì€ í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ê³  yì¶•ì€ IFEvalì˜ í‰ê·  ì ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. SPaR-8BëŠ” ëª¨ë“  ë°˜ë³µì—ì„œ Self-rewarding, Meta-rewarding, SELFì™€ ê°™ì€ ë‹¤ë¥¸ ìê°€ ê°œì„  ë°©ë²•ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤. ë˜í•œ GPT-4-Turboì˜ ì„±ëŠ¥ë„ ëŠ¥ê°€í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 3: Comparison with baseline methods across iterations (Cf. Figure 9 for SPaR-7B). SPaR-8B consistently surpasses all baselines.
> </details>



![](https://arxiv.org/html/2412.11605/extracted/6072122/figures/decoding.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ í•©ì„± ë°ì´í„° ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì™¼ìª½ì€ ë¬¸ì ì‹œí€€ìŠ¤ ìƒì„±, ì˜¤ë¥¸ìª½ì€ ì´ì•¼ê¸° ì‹œì‘/ë ìƒì„± ì‘ì—…ì— ëŒ€í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ë¬¸ì ì‹œí€€ìŠ¤ ìƒì„± ì‘ì—…ì—ì„œ ê°„ì„­ ìŒì€ ëŒ€ë¬¸ì ë¹„ìœ¨(ê°„ì„­ ìš”ì†Œ)ì„ ë¹ ë¥´ê²Œ í•™ìŠµí•˜ì§€ë§Œ ê°œì„  ìŒë³´ë‹¤ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. ì´ì•¼ê¸° ì‹œì‘/ë ìƒì„± ì‘ì—…ì—ì„œ ê°œì„  ìŒì€ ê°„ì„­ ìŒë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë©°, ê°„ì„­ ìŒì€ 0ë‹¨ê³„ì—ì„œ ì›ë˜ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. ì¦‰, ê°œì„  ìŒì„ ì‚¬ìš©í•˜ë©´ ì‘ì—…ì˜ í•µì‹¬ ì°¨ì´ì ì— ì§‘ì¤‘í•˜ì—¬ ì„±ëŠ¥ì´ í–¥ìƒë˜ê³  ê°„ì„­ ìš”ì†Œë¥¼ ìµœì†Œí™”í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 4: Synthetic data experiment results: Character Sequence Generation (left) and Start/End Story Generation (right). For Character Sequence Generation, interfering pairs show rapid learning of the uppercase ratio (interfering factor) but perform worse than refinement pairs. In the Start/End Story Generation task, refinement pairs outperform interfering pairs, which even underperform the original model at step 0.
> </details>



![](https://arxiv.org/html/2412.11605/extracted/6072122/figures/taxonomy.png)

> ğŸ”¼ ì´ í‘œëŠ” í–‰ìœ„ì ëª¨ë¸ì— ëŒ€í•œ ì ˆì œ ì—°êµ¬ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. 'w/o íŠ¸ë¦¬ ê²€ìƒ‰', 'w/o ë°˜ë³µ í›ˆë ¨', 'w/o ê°œì„ 'ì€ ê°ê° íŠ¸ë¦¬ ê²€ìƒ‰, ë°˜ë³µ í›ˆë ¨, ê°œì„  ë°ì´í„° ì—†ì´ SPARë¥¼ í›ˆë ¨ì‹œì¼°ì„ ë•Œì˜ ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŸ¬í•œ ìš”ì†Œë“¤ì„ ì œê±°í•˜ë©´ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 4: Ablation study on the actor.
> </details>



![](https://arxiv.org/html/2412.11605/x3.png)

> ğŸ”¼ ì´ í‘œëŠ” ì¬êµ¬ì„±ê¸°(refiner)ì— ëŒ€í•œ ì ˆì œ ì—°êµ¬(ablation study) ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¬êµ¬ì„±ê¸°ëŠ” tree-search refinement ê³¼ì •ê³¼ ë°˜ë³µì ì¸ í›ˆë ¨ ê³¼ì •ì„ í¬í•¨í•˜ëŠ” SPARì˜ í•µì‹¬ ìš”ì†Œ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. í‘œ 5ëŠ” tree-search refinement ì—†ì´, í˜¹ì€ ë°˜ë³µì ì¸ í›ˆë ¨ ì—†ì´ ì¬êµ¬ì„±ê¸°ë¥¼ í›ˆë ¨í–ˆì„ ë•Œì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ, ìì—°ì–´(Natural) ë° ì ëŒ€ì (Adversarial) ì…ë ¥ì— ëŒ€í•œ ì •í™•ë„(Acc)ì™€ F1 ì ìˆ˜ ëª¨ë‘ ê°ì†Œí•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” tree-search refinementì™€ ë°˜ë³µì ì¸ í›ˆë ¨ì´ ì¬êµ¬ì„±ê¸°ì˜ ì„±ëŠ¥ í–¥ìƒì— ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 5: Ablation study on the refiner.
> </details>



![](https://arxiv.org/html/2412.11605/x4.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ ì¶”ë¡  ì‹œê°„(ì‘ë‹µ ìƒì„± íšŸìˆ˜ë¡œ ì¸¡ì •)ì„ ëŠ˜ë ¤ SPAR-8B-DPO-iter3 ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê·¸ë¦¬ë”” ë””ì½”ë”©(Greedy Decoding), ë² ìŠ¤íŠ¸-ì˜¤ë¸Œ-N ìƒì„±(Best-of-N), ë„ˆë¹„ ìš°ì„  íƒìƒ‰(BFS), ê¹Šì´ ìš°ì„  íƒìƒ‰(DFS) ë“± ë‹¤ì–‘í•œ ë””ì½”ë”© ì „ëµì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤. ì¶”ë¡  ì‹œê°„ì´ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ ëª¨ë“  ë°©ë²•ì˜ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ íŠ¸ë¦¬ íƒìƒ‰ ê¸°ë°˜ ê°œì„ (BFSì™€ DFS)ì€ ë² ìŠ¤íŠ¸-ì˜¤ë¸Œ-N ìƒì„±ë³´ë‹¤ ê²°êµ­ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ëŠ” ê°œì„ ì´ ìƒì„±ë³´ë‹¤ ë” ê°•ë ¥í•˜ë©° ì¶”ë¡  ì‹œê°„ì— ê³„ì‚° ê·œëª¨ë¥¼ ì¡°ì •í•˜ëŠ” ë° ë” ì í•©í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 5: Comparison of decoding strategies. Model performance improves with increased inference times.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Model | Natural | | Adversarial | | | | | | | Average | | |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| | Acc. | F1 | GPTInst | | GPTOut | | Manual | | Neighbor | | Average | | Acc. | F1 |
| | | | Acc. | F1 | Acc. | F1 | Acc. | F1 | Acc. | F1 | Acc. | F1 | | |
| GPT-4o-Mini | 74.5 | 70.5 | 69.2 | 61.6 | 60.9 | 51.4 | 59.8 | 51.9 | 72.8 | 66.4 | 65.7 | 57.8 | 67.4 | 60.4 |
| **_LLaMA3-8B Models_** | | | | | | | | | | | | | | |
| LLaMA3-8B-Instruct | 60.0 | 51.8 | 55.4 | 46.1 | 47.9 | 39.5 | 51.1 | 36.6 | 54.5 | 45.0 | 52.2 | 41.8 | 53.8 | 43.8 |
| SELF | 69.5 | 61.6 | 62.0 | 50.7 | 64.9 | 54.8 | 57.6 | 41.8 | 64.6 | 51.3 | 62.2 | 49.6 | 63.7 | 52.0 |
| Self-Rewarding | **71.0** | **66.3** | 70.1 | **66.7** | 63.8 | 59.5 | 62.0 | 55.7 | 67.5 | 61.7 | 65.9 | 60.9 | 66.9 | 61.9 |
| Meta-Rewarding | 70.5 | **66.3** | 68.5 | 64.6 | 64.9 | **60.2** | 64.1 | 58.3 | **69.0** | **63.1** | 66.6 | 61.6 | 67.4 | 62.5 |
| SPaR-8B-SFT | 68.5 | 60.9 | 67.9 | 62.4 | 59.6 | 50.0 | 63.0 | 54.1 | 68.3 | 59.3 | 64.7 | 56.5 | 65.5 | 57.3 |
| SPaR-8B-RFT-iter1 | 68.5 | 63.2 | 66.8 | 60.6 | 63.8 | 55.3 | 62.0 | 53.3 | 66.8 | 59.0 | 64.9 | 57.1 | 65.6 | 58.3 |
| SPaR-8B-RFT-iter2 | 70.5 | 64.2 | 66.8 | 61.6 | **66.0** | 60.0 | 65.2 | 57.9 | **69.0** | 62.4 | 66.8 | 60.5 | 67.5 | 61.2 |
| SPaR-8B-RFT-iter3 | 70.5 | 65.9 | **70.7** | **66.7** | 63.8 | 57.5 | **68.5** | **63.3** | 68.3 | 62.2 | **67.8** | **62.4** | **68.3** | **63.1** |
| **_GLM-4-9B Models_** | | | | | | | | | | | | | | |
| GLM-4-9B-Chat | **74.5** | **76.5** | 74.5 | **75.9** | 57.4 | **62.3** | 53.3 | 56.6 | 69.8 | **72.0** | 63.7 | **66.7** | 65.9 | **68.6** |
| SPaR-9B-SFT | 70.5 | 65.5 | 72.8 | 70.2 | **59.6** | 55.8 | 64.1 | 53.5 | 71.3 | 67.2 | 66.9 | 61.7 | 67.7 | 62.5 |
| SPaR-9B-RFT-iter3 | 71.0 | 68.8 | **75.5** | 74.6 | 58.5 | 55.2 | **68.5** | **64.2** | **68.7** | 65.9 | **67.8** | 64.9 | **68.4** | 65.7 |
| **_LLaMA3-70B Models_** | | | | | | | | | | | | | | |
| LLaMA3-70B-Instruct | 75.0 | 71.9 | 73.4 | 69.6 | **69.1** | **66.7** | 66.3 | **60.8** | 69.0 | 63.4 | 69.5 | 65.1 | 70.6 | 66.5 |
| SPaR-70B-RFT-iter3 | **78.0** | **74.7** | **78.8** | **76.9** | 64.9 | 61.2 | **67.4** | 59.5 | **72.4** | **68.1** | **70.9** | **66.4** | **72.3** | **68.1** |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” LLMBar ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµëœ LLMì˜ íŒë‹¨ ëŠ¥ë ¥ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. Mistral-7B-Instruct ê²°ê³¼ëŠ” í‘œ 8ì„ ì°¸ì¡°í•˜ì„¸ìš”. ê° ê¸°ë³¸ ëª¨ë¸ì— ëŒ€í•´ ê°€ì¥ ë†’ì€ ì ìˆ˜ëŠ” êµµê²Œ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. í‘œì—ëŠ” ìì—°ì–´ì™€ ì ëŒ€ì  ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•ë„ì™€ F1 ì ìˆ˜ê°€ í‘œì‹œë˜ë©°, ê° ì§ˆë¬¸ ìœ í˜•ì— ëŒ€í•´ GPT ì…ë ¥, GPT ì¶œë ¥, ìˆ˜ë™, ì´ì›ƒ ë“± ë‹¤ì–‘í•œ í‰ê°€ ë°©ì‹ì„ ì‚¬ìš©í•œ ê²°ê³¼ê°€ ì œê³µë©ë‹ˆë‹¤. ë˜í•œ, ê° ëª¨ë¸ì— ëŒ€í•´ ìì—°ì–´ì™€ ì ëŒ€ì  ì§ˆë¬¸ì— ëŒ€í•œ í‰ê·  ì •í™•ë„ì™€ F1 ì ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ í‘œë¥¼ í†µí•´ SPAR í•™ìŠµ ê³¼ì •ì—ì„œ Refinerì˜ íŒë‹¨ ëŠ¥ë ¥ì´ í–¥ìƒë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 2:  Evaluation of judgment capability for iteratively trained LLMs on LLMBar. (Cf. Table 8 for Mistral-7B-Instruct results.) Acc. denotes accuracy. The highest scores for each base model are highlighted in bold.
> </details>

{{< table-caption >}}
| Model | Acc-GPT | Acc-SPaR |
|---|---|---| 
| GPT-4o-Mini | 79.0 | 71.0 |
| SPaR-8B-SFT | 73.5 | 71.0 |
| SPaR-8B-RFT-iter1 | 77.5 | 77.0 |
| SPaR-8B-RFT-iter2 | 74.5 | 76.0 |
| SPaR-8B-RFT-iter3 | 79.0 | 90.5 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ë‹¤ì–‘í•œ í‰ê°€ ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ SPAR í”„ë ˆì„ì›Œí¬ì˜ ê°œì„  ê¸°ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. 'Acc-GPT' ì—´ì€ GPT-40ì„ íŒì‚¬ë¡œ ì‚¬ìš©í•œ ì •í™•ë„ë¥¼ ë‚˜íƒ€ë‚´ê³  'Acc-SPAR' ì—´ì€ SPAR-8B-RFT-iter3ë¥¼ íŒì‚¬ë¡œ ì‚¬ìš©í•œ ì •í™•ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. í‘œì—ì„œ SPAR-8B-RFT-iter3ê°€ ìì²´ í‰ê°€ì—ì„œ GPT-40ë³´ë‹¤ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì•˜ì§€ë§Œ GPT-40 í‰ê°€ì—ì„œëŠ” ê·¸ë ‡ì§€ ì•Šë‹¤ëŠ” ì ì— ìœ ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ìì²´ í‰ê°€ í¸í–¥ì˜ ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 3: Refinement evaluation results. Acc-GPT uses GPT-4o as judge; -SPaR uses SPaR-8B-RFT-iter3.
> </details>

{{< table-caption >}}
| Model | IFEval | | FollowBench (SSR) | 
|---|---|---|---|
| | Prompt(S) | Instruction(S) | Avg. |
| SPaR-8B-DPO-iter3 | 78.0 | 83.7 | 68.8 |
| *w/o* Tree Search | -2.0 | -0.8 | -1.7 |
| *w/o* Iterative Training | -0.9 | -0.2 | -2.0 |
| *w/o* Refinement | -2.6 | -1.6 | -3.1 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” SPaR-7B, SPaR-9B, SPaR-70B ëª¨ë¸ì˜ ëª…ë ¹ì–´ ìˆ˜í–‰ ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ë¥¼ ìì„¸íˆ ë³´ì—¬ì¤ë‹ˆë‹¤. IFEval ë° FollowBench(SSR) ë²¤ì¹˜ë§ˆí¬ì—ì„œ í”„ë¡¬í”„íŠ¸ ë ˆë²¨(P)ê³¼ ëª…ë ¹ì–´ ë ˆë²¨(I) ëª¨ë‘ì— ëŒ€í•œ ì ìˆ˜, ëŠìŠ¨í•œ í‰ê°€(L)ì™€ ì—„ê²©í•œ í‰ê°€(S) ì ìˆ˜, ê·¸ë¦¬ê³  ê° ë ˆë²¨(Lv1~Lv5)ë³„ í‰ê·  ì ìˆ˜ê°€ ì œê³µë©ë‹ˆë‹¤.  ë…¼ë¬¸ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¨ ì ìˆ˜ëŠ” â€ ë¡œ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 6: Full results of SPaR-7B, SPaR-9B, and SPaR-70B on instruction-following benchmarks. P stands for prompt level, and I represents instruction level. L and S denote loose and strict evaluations, respectively. Avg. indicates average results and Lv means level. Scores marked with â€  are sourced directly from the original paper.
> </details>

{{< table-caption >}}
| Model | Natural | Adversarial |
|---|---|---|
| | Acc. | F1 | Acc. | F1 |
| SPaR-8B-RFT-iter3 | 70.5 | 65.9 | 67.8 | 62.4 |
| *w/o* Tree Search | -0.5 | -1.2 | -4.3 | -8.2 |
| *w/o* Iterative Training | -0.5 | -2.5 | -1.7 | -3.5 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” SPaRì´ ëª¨ë¸ì˜ ì¼ë°˜ì ì¸ ëŠ¥ë ¥ì„ ìœ ì§€í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ì¼ë°˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. SPaRì„ í†µí•´ êµìœ¡ëœ ëª¨ë¸ì€ GSM8k, TriviaQA, MMLU ë° HumanEvalê³¼ ê°™ì€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë˜ì§€ ì•Šê³  ì˜¤íˆë ¤ í–¥ìƒë˜ëŠ” ê²½ìš°ë„ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 7: Performance on general benchmarks. SPaR maintains the modelâ€™s general capabilities.
> </details>

{{< table-caption >}}
| Model | IFEval | | | | | FollowBench (SSR) | | | | | |
|---|---|---|---|---|---|---|---|---|---|---|---|
| | **P (L)** | **I (L)** | **P (S)** | **I (S)** | **Avg.** | **Lv-1** | **Lv-2** | **Lv-3** | **Lv-4** | **Lv-5** | **Avg.** |
|---|---|---|---|---|---|---|---|---|---|---|---| 
| *Mistral-7B Models* | | | | | | | | | | | |
| Mistral-7B-Instruct | 55.1 | 64.9 | 49.9 | 60.2 | 57.5 | 65.1 | 61.6 | 61.6 | 56.8 | 57.2 | 60.4 |
| SELF | 71.3 | 79.7 | 68.0 | 76.9 | 74.0 | 71.5 | 64.2 | 60.8 | 58.0 | 57.0 | 62.3 |
| Humpback | 60.4 | 71.0 | 56.6 | 67.6 | 63.9 | 70.7 | 63.9 | 63.8 | 59.8 | 57.9 | 63.2 |
| Self-Rewarding | 64.3 | 73.5 | 61.0 | 70.7 | 67.4 | 70.8 | 64.8 | 62.3 | 61.9 | **58.3** | 63.6 |
| Meta-Rewarding | 65.1 | 74.7 | 61.0 | 71.1 | 68.0 | 73.2 | 64.6 | 64.5 | 60.6 | 57.6 | 64.1 |
| SPaR-7B-SFT | 62.7 | 72.3 | 59.3 | 68.7 | 65.8 | 74.4 | 64.3 | 62.5 | 58.2 | 55.0 | 62.9 |
| SPaR-7B-DPO-iter1 | 68.2 | 76.6 | 64.7 | 73.6 | 70.8 | 73.2 | 64.6 | 63.1 | 60.3 | 56.6 | 63.6 |
| SPaR-7B-DPO-iter2 | 70.0 | 78.1 | 65.8 | 74.2 | 72.0 | 72.2 | **65.7** | 61.4 | **62.4** | 57.5 | 63.8 |
| SPaR-7B-DPO-iter3 | **74.1** | **80.9** | **69.7** | **77.1** | **75.5** | **74.6** | 63.8 | **66.1** | 61.0 | 58.0 | **64.7** |
| *GLM-4-9B Models* | | | | | | | | | | | |
| GLM-4-9B-Chat | 71.5 | 79.9 | 68.0 | 77.2 | 74.2 | 80.8 | 75.1 | 67.4 | 64.3 | **65.4** | 70.6 |
| SPaR-9B-SFT | 71.5 | 80.5 | 68.8 | 78.1 | 74.7 | 79.4 | 70.9 | **68.2** | 65.1 | 63.7 | 69.5 |
| SPaR-9B-DPO-iter1 | 73.8 | 81.2 | 70.6 | 78.5 | 76.0 | 82.6 | 76.0 | 67.9 | 64.9 | 63.6 | 71.0 |
| SPaR-9B-DPO-iter2 | 76.7 | 83.3 | 73.2 | 80.9 | 78.5 | 80.4 | 76.6 | 67.4 | **68.7** | 64.1 | 71.4 |
| SPaR-9B-DPO-iter3 | **77.3** | **84.1** | **73.6** | **81.4** | **79.1** | **82.7** | **76.7** | 67.9 | 68.3 | 64.2 | **72.0** |
| *LLaMA3-70B Models* | | | | | | | | | | | |
| LLaMA3-70B-Instruct | 83.7 | 88.9 | 77.1 | 83.8 | 83.4 | 77.1 | 72.5 | 69.4 | 68.7 | 66.3 | 70.8 |
| AutoIF-70Bâ€  | **85.6** | **90.4** | 80.2 | 86.7 | 85.7 | 71.0 | 67.2 | 66.2 | 64.6 | 63.5 | 66.5 |
| SPaR-70B-DPO-iter1 | 84.5 | 89.2 | 80.2 | 85.7 | 84.9 | 77.6 | 74.0 | 70.2 | 70.6 | 66.9 | 71.9 |
| SPaR-70B-DPO-iter2 | 85.0 | 89.4 | 81.5 | 87.2 | 85.8 | **80.4** | **76.4** | 69.9 | **73.7** | 70.2 | 74.1 |
| SPaR-70B-DPO-iter3 | **85.6** | 90.2 | **81.3** | **87.3** | **86.1** | 80.3 | 75.7 | **71.4** | **73.7** | **70.5** | **74.3** |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” Mistral-7B-Instruct ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ SPaR í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°˜ë³µì ìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì˜ íŒë‹¨ ëŠ¥ë ¥ì„ LLMBar ë²¤ì¹˜ë§ˆí¬ì—ì„œ í‰ê°€í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. SPaRì€ ìì²´ ê°œì„ ì„ ìœ„í•œ ì…€í”„ í”Œë ˆì´ í”„ë ˆì„ì›Œí¬ë¡œ, í…ìŠ¤íŠ¸ì˜ ë¯¸ë¬˜í•œ ì°¨ì´ë¥¼ ê°•ì¡°í•˜ì—¬ ëª…ë ¹ì–´ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ ë”°ë¥´ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. í‘œì—ëŠ” ìì—°ì–´ ë° ì ëŒ€ì  ìƒ˜í”Œ ëª¨ë‘ì— ëŒ€í•œ ì •í™•ë„ì™€ F1 ì ìˆ˜ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ê° ìƒ˜í”Œ ìœ í˜•(GPTInst, GPTOut, Manual, Neighbor)ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì „ì²´ í‰ê·  ì ìˆ˜ë¥¼ ì œê³µí•˜ì—¬ ëª¨ë¸ì˜ ì „ë°˜ì ì¸ íŒë‹¨ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. SPaRì„ í†µí•´ ë°˜ë³µì ìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë‹¤ë¥¸ ê¸°ì¤€ì„ ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ íŒë‹¨ ëŠ¥ë ¥ í–¥ìƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 8: Judgment evalution results on LLMBar for SPaR-7B. Acc. stands for accuracy.
> </details>

{{< table-caption >}}
| Model | GSM8k | TriviaQA | MMLU | HumanEval | Average | 
|---|---|---|---|---|---| 
| **_Mistral-7B Models_** | | | | | | 
| Mistral-7B-Instruct | 42.9 | 72.5 | 57.9 | 32.9 | 51.6 | 
| SPaR-7B-SFT | 56.4 | 72.8 | 56.7 | 44.5 | 57.6 (+6.0) | 
| SPaR-7B-DPO-iter1 | 55.6 | 72.2 | 55.3 | 46.3 | 57.4 (+5.8) | 
| SPaR-7B-DPO-iter2 | 54.4 | 72.1 | 55.8 | 45.1 | 56.9 (+5.3) | 
| SPaR-7B-DPO-iter3 | 58.2 | 71.6 | 55.1 | 46.3 | 57.8 (+6.2) | 
| **_LLaMA3-8B Models_** | | | | | | 
| LLaMA3-8B-Instruct | 75.4 | 75.9 | 63.6 | 55.5 | 67.6 | 
| SPaR-8B-SFT | 75.6 | 76.0 | 64.0 | 61.6 | 69.3 (+1.7) | 
| SPaR-8B-DPO-iter1 | 78.8 | 75.2 | 63.8 | 60.4 | 69.6 (+2.0) | 
| SPaR-8B-DPO-iter2 | 77.0 | 74.9 | 63.1 | 60.4 | 68.9 (+1.3) | 
| SPaR-8B-DPO-iter3 | 77.7 | 75.1 | 63.1 | 60.9 | 69.2 (+1.6) | 
| **_GLM-4-9B Models_** | | | | | | 
| GLM-4-9B-Chat | 80.6 | 69.7 | 71.9 | 74.3 | 74.1 | 
| SPaR-9B-SFT | 82.9 | 69.4 | 71.8 | 73.8 | 74.5 (+0.4) | 
| SPaR-9B-DPO-iter1 | 82.6 | 68.8 | 71.6 | 75.0 | 74.5 (+0.4) | 
| SPaR-9B-DPO-iter2 | 82.8 | 68.9 | 71.8 | 73.8 | 74.3 (+0.2) | 
| SPaR-9B-DPO-iter3 | 83.0 | 69.0 | 72.1 | 73.2 | 74.3 (+0.2) | 
| **_LLaMA3-70B Models_** | | | | | | 
| LLaMA3-70B-Instruct | 92.2 | 87.2 | 80.8 | 79.3 | 84.9 | 
| SPaR-70B-DPO-iter1 | 92.5 | 90.4 | 81.0 | 79.3 | 85.8 (+0.9) | 
| SPaR-70B-DPO-iter2 | 92.9 | 89.5 | 80.4 | 78.7 | 85.4 (+0.5) | 
| SPaR-70B-DPO-iter3 | 93.4 | 86.7 | 80.6 | 79.9 | 85.2 (+0.3) |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” LLMBar ë°ì´í„°ì…‹ì—ì„œ ë‹¤ì–‘í•œ ë””ì½”ë”© ì „ëµì„ ë¹„êµí•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì£¼ìš” ë‚´ìš©ì€ íƒìš•ì  ë””ì½”ë”©ê³¼ ë‹¤ìˆ˜ê²° íˆ¬í‘œë¥¼ ì‚¬ìš©í•œ ì—¬ëŸ¬ ìƒ˜í”Œë§ íšŸìˆ˜ë¥¼ ë¹„êµí•œ ê²ƒì…ë‹ˆë‹¤. ìƒ˜í”Œë§ íšŸìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ìì—°ì–´ ë‹µë³€ì˜ ì •í™•ë„ì™€ F1 ì ìˆ˜ëŠ” ë‹¤ì†Œ í–¥ìƒë˜ëŠ” ë°˜ë©´, ì ëŒ€ì  ë‹µë³€ì—ì„œëŠ” ìƒ˜í”Œë§ íšŸìˆ˜ 5ì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ëŠ” ìƒ˜í”Œë§ íšŸìˆ˜ ì¦ê°€ê°€ ì„±ëŠ¥ í–¥ìƒì— ì¤‘ìš”í•  ìˆ˜ ìˆì§€ë§Œ, ì§€ë‚˜ì¹˜ê²Œ ëŠ˜ë¦¬ë©´ ì˜¤íˆë ¤ ì ëŒ€ì  ë‹µë³€ì— ëŒ€í•œ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 9: Comparison of decoding strategies on LLMBar.
> </details>

{{< table-caption >}}
| Model | Natural | | Adversarial | | | | | | Average | | |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| 
| | Acc. | F1 | GPTInst | | GPTOut | | Manual | | Neighbor | | Average | | | |
| | | | Acc. | F1 | Acc. | F1 | Acc. | F1 | Acc. | F1 | Acc. | F1 | Acc. | F1 |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| 
| Mistral-7B-Instruct | 58.0 | **69.1** | 57.1 | **68.8** | 50.0 | **64.1** | 45.6 | **61.5** | 47.8 | 62.6 | 50.1 | **64.3** | 51.7 | **65.2** |
| SELF | 68.0 | 65.2 | 71.2 | 68.7 | 56.4 | 56.8 | 62.0 | 52.6 | 67.5 | 62.3 | 64.3 | 60.1 | 65.0 | 61.1 | 
| Self-Rewarding | 68.0 | 64.0 | 69.0 | 63.7 | 59.6 | 53.7 | **63.0** | 57.5 | **69.4** | **64.3** | **65.3** | 59.8 | 65.8 | 60.6 |
| Meta-Rewarding | 67.5 | 62.4 | 71.7 | 68.7 | 56.4 | 51.8 | **63.0** | 56.4 | 66.8 | 62.1 | 64.5 | 59.7 | 65.1 | 60.3 |
| SPaR-7B-SFT | 69.5 | 63.9 | 71.7 | 67.5 | 55.3 | 48.8 | 55.4 | 45.3 | **69.4** | 62.3 | 63.0 | 56.1 | 64.3 | 57.6 |
| SPaR-7B-RFT-iter1 | 67.0 | 62.1 | 66.3 | 62.7 | 56.4 | 52.9 | 60.9 | 52.6 | 64.2 | 60.7 | 61.9 | 57.2 | 63.0 | 58.2 |
| SPaR-7B-RFT-iter2 | 68.0 | 64.4 | 68.5 | 64.6 | **60.6** | 57.5 | 62.0 | 52.1 | 64.2 | 60.0 | 63.8 | 58.5 | 64.7 | 59.7 |
| SPaR-7B-RFT-iter3 | **71.0** | 66.7 | **72.3** | 67.5 | 57.4 | 55.6 | 60.9 | 51.4 | 68.3 | 62.6 | 64.7 | 59.2 | **66.0** | 60.7 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ë‹¤ì–‘í•œ ë””ì½”ë”© ì „ëµì„ ë¹„êµí•˜ì—¬ SPaR-8B ëª¨ë¸ì˜ ê°œì„  ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Acc-GPTëŠ” GPT-40ì„ íŒì‚¬ë¡œ ì‚¬ìš©í•œ ì •í™•ë„ì´ê³ , Acc-SPaRëŠ” SPaR-8B-RFT-iter3ë¥¼ íŒì‚¬ë¡œ ì‚¬ìš©í•œ ì •í™•ë„ì…ë‹ˆë‹¤. í‘œì—ì„œ BFSì™€ DFSì™€ ê°™ì€ íŠ¸ë¦¬ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ì´ ë‹¤ë¥¸ ë°©ë²•ë“¤ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë©°, íŠ¹íˆ íƒìš•ì  ë””ì½”ë”©ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì‘ë‹µ ê°œì„  ì‘ì—…ì—ì„œ íŠ¸ë¦¬ íƒìƒ‰ì˜ ì¤‘ìš”ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 10: Comparison of different decoding strategies for refinement task. Acc-GPT stands for the accuracy of using GPT-4o as judge, and Acc-SPaR for the accuracy of using SPaR-8B-RFT-iter3 as judge.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}