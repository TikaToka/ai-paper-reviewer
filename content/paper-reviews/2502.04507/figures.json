[{"figure_path": "https://arxiv.org/html/2502.04507/x1.png", "caption": "Figure 1: (a) Generating a 5s 720P clip in Hunyuan involves processing 115K tokens, making attention the dominant cost. (b) Attention latency comparison: existing methods fail to translate FLOP reduction into wall-clock speedup; STA\u00a0is hardware-efficient and achieves proportional speedup with sparsity.", "description": "\uadf8\ub9bc 1\uc740 \ub450 \uac00\uc9c0 \ud558\uc704 \uadf8\ub9bc\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. (a)\ub294 Hunyuan \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec 5\ucd08 \uae38\uc774\uc758 720P \ube44\ub514\uc624\ub97c \uc0dd\uc131\ud558\ub294 \ub370 11.5\ub9cc \uac1c\uc758 \ud1a0\ud070\uc744 \ucc98\ub9ac\ud574\uc57c \ud558\uba70, \uc774 \uc911 \uc5b4\ud150\uc158 \uc5f0\uc0b0\uc774 \uac00\uc7a5 \ub9ce\uc740 \ube44\uc6a9\uc744 \ucc28\uc9c0\ud55c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (b)\ub294 \uae30\uc874 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uacfc \uc81c\uc548\ub41c STA(Sliding Tile Attention)\uc758 \uc5b4\ud150\uc158 \uc5f0\uc0b0 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. \uae30\uc874 \ubc29\ubc95\ub4e4\uc740 FLOP(Floating Point Operations) \uac10\uc18c\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \uc2e4\uc81c \uc18d\ub3c4 \ud5a5\uc0c1\uc73c\ub85c \uc774\uc5b4\uc9c0\uc9c0 \uc54a\uc558\uc9c0\ub9cc, STA\ub294 \ud558\ub4dc\uc6e8\uc5b4 \ud6a8\uc728\uc131\uc744 \ub192\uc5ec \uc5b4\ud150\uc158 \uc2a4\ud30c\uc2a4\ud2f0(sparsity)\uc5d0 \ube44\ub840\ud558\ub294 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2502.04507/x5.png", "caption": "Figure 2: Visualization of attention locality. The green point means the query point and the magma-colored regions indicate areas of high attention values in response to the query. Instead of attending to the entire image, the query\u2019s attention forms a concentrated local hotspot.", "description": "\uadf8\ub9bc 2\ub294 \uc5b4\ud150\uc158\uc758 \uad6d\uc9c0\uc131(locality)\uc744 \uc2dc\uac01\ud654\ud55c \uac83\uc785\ub2c8\ub2e4. \ub179\uc0c9 \uc810\uc740 \ucffc\ub9ac(\uc9c8\uc758) \uc810\uc744 \ub098\ud0c0\ub0b4\uace0, \ub9c8\uadf8\ub9c8 \uc0c9\uc0c1\uc758 \uc601\uc5ed\uc740 \ud574\ub2f9 \ucffc\ub9ac\uc5d0 \ub300\ud55c \uc751\ub2f5\uc73c\ub85c \ub192\uc740 \uc5b4\ud150\uc158 \uac12\uc744 \uac16\ub294 \uc601\uc5ed\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c \ubcf4\ub4ef\uc774, \ucffc\ub9ac\ub294 \uc804\uccb4 \uc774\ubbf8\uc9c0\uc5d0 \uc5b4\ud150\uc158\uc744 \ub450\ub294 \ub300\uc2e0 \uc9d1\uc911\ub41c \uad6d\uc9c0\uc801 \uc601\uc5ed(hotspot)\uc5d0\ub9cc \uc5b4\ud150\uc158\uc744 \uc9d1\uc911\ud558\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c \ube44\ub514\uc624 \ud655\uc0b0 \ubaa8\ub378\uc5d0\uc11c \uc5b4\ud150\uc158 \uc810\uc218\uac00 \uc8fc\ub85c \uad6d\uc9c0\uc801\uc778 3D \uc708\ub3c4\uc6b0 \ub0b4\uc5d0 \uc9d1\uc911\ub418\uc5b4 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc785\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04507/extracted/6182573/media/Fig6/f6-sta.png", "caption": "Figure 3: Left: Fraction of attention scores within a (12, 24, 24) local window across diffusion steps and 10 different prompts. Most heads show high recall, indicating a local attention pattern.\nRight: Despite the different recall across heads, the standard deviation across prompts remains low.", "description": "\uadf8\ub9bc 3\uc740 \ud6c8\ub828\ub41c \ube44\ub514\uc624 \ud655\uc0b0 \ubaa8\ub378\uc5d0\uc11c \uc5b4\ud150\uc158 \uc810\uc218\uc758 \uad6d\uc9c0\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd \uadf8\ub9bc\uc740 10\uac1c\uc758 \ub2e4\ub978 \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud574 \ud655\uc0b0 \uacfc\uc815 \uc804\ubc18\uc5d0 \uac78\uccd0 (12, 24, 24) \ud06c\uae30\uc758 \uc791\uc740 3\ucc28\uc6d0 \ucc3d \uc548\uc5d0 \uc788\ub294 \uc5b4\ud150\uc158 \uc810\uc218\uc758 \ube44\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub300\ubd80\ubd84\uc758 \ud5e4\ub4dc\uc5d0\uc11c \ub192\uc740 \uc7ac\ud604\uc728\uc744 \ubcf4\uc774\ub294\ub370, \uc774\ub294 \uc5b4\ud150\uc158\uc774 \uc8fc\ub85c \uad6d\uc9c0\uc801\uc778 \uacf5\uac04-\uc2dc\uac04 \uc601\uc5ed\uc5d0 \uc9d1\uc911\ub428\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc624\ub978\ucabd \uadf8\ub9bc\uc740 \ud5e4\ub4dc \uac04 \uc7ac\ud604\uc728\uc758 \ucc28\uc774\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ud504\ub86c\ud504\ud2b8 \uac04 \ud45c\uc900 \ud3b8\ucc28\uac00 \ub0ae\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc774 \ud504\ub86c\ud504\ud2b8\uc5d0 \uad00\uacc4\uc5c6\uc774 \uc77c\uad00\ub41c \uad6d\uc9c0\uc801 \uc5b4\ud150\uc158 \ud328\ud134\uc744 \ub530\ub978\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2502.04507/x6.png", "caption": "Figure 4: The attention map of NATTEN, Tiled NATTEN, and STA. We plot with an image size 24\u00d7\\times\u00d724 and a 12\u00d7\\times\u00d712 local window. The tile size is set to 4\u00d7\\times\u00d74. (a) NATTEN creates many mixed blocks that are very inefficient for Flash Attention computation. (b) Tiled NATTEN increases the number of dense blocks, but the mixed blocks persist. (c) STA\u00a0completely eliminates the mixed block, making the computation extremely friendly for GPU. Note that we mainly show STA\u2019s application in 3D scenarios for video generation in this paper, but for better illustration, we present the 2D scenario in this plot.", "description": "\uadf8\ub9bc 4\ub294 NATTEN, Tiled NATTEN \ubc0f STA\uc758 \uc5b4\ud150\uc158 \ub9f5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ubbf8\uc9c0 \ud06c\uae30\ub294 24x24\uc774\uace0, \uc9c0\uc5ed \uc708\ub3c4\uc6b0 \ud06c\uae30\ub294 12x12\uc774\uba70, \ud0c0\uc77c \ud06c\uae30\ub294 4x4\ub85c \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4. (a) NATTEN\uc740 \ud50c\ub798\uc2dc \uc5b4\ud150\uc158 \uacc4\uc0b0\uc5d0 \ub9e4\uc6b0 \ube44\ud6a8\uc728\uc801\uc778 \ub9ce\uc740 \ud63c\ud569 \ube14\ub85d\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. (b) Tiled NATTEN\uc740 \ubc00\uc9d1 \ube14\ub85d\uc758 \uc218\ub97c \ub298\ub9ac\uc9c0\ub9cc, \ud63c\ud569 \ube14\ub85d\uc740 \uc5ec\uc804\ud788 \ub0a8\uc544 \uc788\uc2b5\ub2c8\ub2e4. (c) STA\ub294 \ud63c\ud569 \ube14\ub85d\uc744 \uc644\uc804\ud788 \uc81c\uac70\ud558\uc5ec GPU \uacc4\uc0b0\uc744 \ub9e4\uc6b0 \ud6a8\uc728\uc801\uc73c\ub85c \ub9cc\ub4ed\ub2c8\ub2e4. \uc774 \ub17c\ubb38\uc5d0\uc11c\ub294 \uc8fc\ub85c \ube44\ub514\uc624 \uc0dd\uc131\uc744 \uc704\ud55c 3D \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c STA\uc758 \uc801\uc6a9\uc744 \ubcf4\uc5ec\uc8fc\uc9c0\ub9cc, \ub354 \ub098\uc740 \uc124\uba85\uc744 \uc704\ud574 2D \uc2dc\ub098\ub9ac\uc624\ub97c \uc774 \uadf8\ub9bc\uc5d0 \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2502.04507/extracted/6182573/media/Fig5/human_eval.png", "caption": "Figure 5: 2D Sliding Tile Attention\u00a0with tile size (2, 2) and window size (6, 6). After attending to all the key tiles, each query tile will generate nine 4x4 dense blocks in the attention map. We showcase 2D STA for better illustration. 3D STA can be inferred similarly.", "description": "\uadf8\ub9bc 5\ub294 2\ucc28\uc6d0 \uc2ac\ub77c\uc774\ub529 \ud0c0\uc77c \uc5b4\ud150\uc158(STA) \uba54\ucee4\ub2c8\uc998\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud0c0\uc77c \ud06c\uae30\ub294 (2, 2), \uc708\ub3c4\uc6b0 \ud06c\uae30\ub294 (6, 6)\uc73c\ub85c \uc124\uc815\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ucffc\ub9ac \ud0c0\uc77c\uc740 \ubaa8\ub4e0 \ud0a4 \ud0c0\uc77c\uc744 \ucc38\uc870\ud55c \ud6c4, \uc5b4\ud150\uc158 \ub9f5\uc5d0 9\uac1c\uc758 4x4 \ud06c\uae30\uc758 \uc870\ubc00\ud55c \ube14\ub85d\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 3\ucc28\uc6d0 STA\ub97c \ubcf4\ub2e4 \uc27d\uac8c \uc774\ud574\ud560 \uc218 \uc788\ub3c4\ub85d 2\ucc28\uc6d0\uc73c\ub85c \ub2e8\uc21c\ud654\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 3\ucc28\uc6d0 STA\ub294 \uc720\uc0ac\ud55c \ubc29\uc2dd\uc73c\ub85c \uc791\ub3d9\ud558\uc9c0\ub9cc, \uacf5\uac04 \ucc28\uc6d0\uc774 \ud558\ub098 \ub354 \ucd94\uac00\ub429\ub2c8\ub2e4.  \uc989, 2\ucc28\uc6d0\uc5d0\uc11c \ud0c0\uc77c\uacfc \uc708\ub3c4\uc6b0\uac00 2\ucc28\uc6d0 \ud3c9\uba74 \uc0c1\uc5d0\uc11c \uc774\ub3d9\ud558\ub294 \uac83\ucc98\ub7fc, 3\ucc28\uc6d0\uc5d0\uc11c\ub294 \ud0c0\uc77c\uacfc \uc708\ub3c4\uc6b0\uac00 \uc2dc\uac04 \ucd95\uc744 \ud3ec\ud568\ud55c 3\ucc28\uc6d0 \uacf5\uac04\uc5d0\uc11c \uc774\ub3d9\ud569\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc744 \ud1b5\ud574 \ucffc\ub9ac\uc640 \ud0a4 \uac04\uc758 \uacf5\uac04\uc801 \uad00\uacc4\uc640 STA\uc758 \ud6a8\uc728\uc801\uc778 \uacc4\uc0b0 \ubc29\uc2dd\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \uc774\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2502.04507/extracted/6182573/media/appendix/reorder.png", "caption": "Figure 6: Qualitative example of 720P 5-second videos. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by \u0394\u0394\\Deltaroman_\u0394-DiT are generally less sharp than those generated by the original HunyuanVideo and \u00a0STA.", "description": "\uadf8\ub9bc 6\uc740 \ud6c8\ub828 \uc804\ud6c4\uc758 \ube44\ub514\uc624 \uc0dd\uc131 \ud488\uc9c8\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uac83\uc785\ub2c8\ub2e4. \ud6c8\ub828 \ud6c4 STA-t-2.43x \ubaa8\ub378\uc740 \ucd9c\ub825 \ubd84\ud3ec\uc5d0 \uc57d\uac04\uc758 \ubcc0\ud654\uac00 \uc788\uc9c0\ub9cc, \uc5ec\uc804\ud788 \ub192\uc740 \ud654\uc9c8\uc758 \ube44\ub514\uc624\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ubc18\uba74, \u0394\u0394\\Deltaroman_\u0394-DiT \ubaa8\ub378\uc740 \uc6d0\ubcf8 HunyuanVideo \ubc0f STA \ubaa8\ub378\uc5d0 \ube44\ud574 \uc0dd\uc131\ub41c \ube44\ub514\uc624\uc758 \uc120\uba85\ub3c4\uac00 \ub5a8\uc5b4\uc9d1\ub2c8\ub2e4. 5\ucd08 \ubd84\ub7c9\uc758 720P \uace0\ud654\uc9c8 \ube44\ub514\uc624\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.04507/extracted/6182573/media/Fig6/f6-swa.png", "caption": "Figure 7: Human evaluation on 200 prompts from the MovieGen Bench\u00a0(Polyak et\u00a0al., 2024). STA\u00a0achieves a 1.36\u00d7 end-to-end speedup while maintaining performance comparable to the original HunyuanVideo. Additionally, STA\u00a0consistently outperforms \u0394\u0394\\Deltaroman_\u0394-DiT across different inference budgets.", "description": "\uc774 \uadf8\ub9bc\uc740 MovieGen Bench(Polyak et al., 2024)\uc758 200\uac1c \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud55c \uc0ac\ub78c\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. STA\ub294 \uc6d0\ubcf8 HunyuanVideo\uc640 \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uc5d4\ub4dc-\ud22c-\uc5d4\ub4dc \uc18d\ub3c4\ub97c 1.36\ubc30 \ud5a5\uc0c1\uc2dc\ucf30\uc2b5\ub2c8\ub2e4. \ub610\ud55c, STA\ub294 \ub2e4\uc591\ud55c \ucd94\ub860 \uc608\uc0b0\uc5d0\uc11c A-DiT\ub97c \uc9c0\uc18d\uc801\uc73c\ub85c \ub2a5\uac00\ud558\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4.  \uadf8\ub9bc\uc740 STA\uc758 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc744 \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc8fc\ub294 \ub2e4\uc591\ud55c \ube44\uad50 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "4.2 Human Evaluations"}, {"figure_path": "https://arxiv.org/html/2502.04507/x7.png", "caption": "Figure 8: Left: Conventional zigzag flattening strategy. Right: STA\u2019 sequence flattening strategy. The plot is given assuming a (9, 9) image with (3, 3) tile size.", "description": "\uadf8\ub9bc 8\uc740 2\ucc28\uc6d0 \uc774\ubbf8\uc9c0\ub97c 1\ucc28\uc6d0 \uc2dc\ud000\uc2a4\ub85c \ubcc0\ud658\ud558\ub294 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc740 \uae30\uc874\uc758 \uc9c0\uadf8\uc7ac\uadf8 \ubc29\uc2dd\uc744, \uc624\ub978\ucabd\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 STA(Sliding Tile Attention) \ubc29\uc2dd\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  STA\ub294 3x3 \ud06c\uae30\uc758 \ud0c0\uc77c\uc744 \uc0ac\uc6a9\ud558\uc5ec 9x9 \uc774\ubbf8\uc9c0\ub97c \ucc98\ub9ac\ud558\ub294 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc9c0\uadf8\uc7ac\uadf8 \ubc29\uc2dd\uc740 \ud53d\uc140\uc744 \uc21c\ucc28\uc801\uc73c\ub85c 1\ucc28\uc6d0 \ubc30\uc5f4\ub85c \ubcc0\ud658\ud558\uc9c0\ub9cc, STA\ub294 3x3 \ud0c0\uc77c \ub2e8\uc704\ub85c \uc774\ubbf8\uc9c0\ub97c \ub098\ub204\uc5b4 \ucc98\ub9ac\ud558\uc5ec \uc5f0\uc18d\uc801\uc778 \ud1a0\ud070 \uc778\ub371\uc2a4\ub97c \uc720\uc9c0\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c STA\uc758 \ud0c0\uc77c \uae30\ubc18 \uc811\uadfc \ubc29\uc2dd\uc740 \uc5f0\uc0b0 \ud6a8\uc728\uc131\uc744 \ub192\uc774\uace0, \ud2b9\ud788 \ud558\ub4dc\uc6e8\uc5b4 \uac00\uc18d\uc5d0 \uc720\ub9ac\ud569\ub2c8\ub2e4.  \uc774\ub294 \uc774\ud6c4 \ub2e8\ub77d\uc5d0\uc11c \uc124\uba85\ud558\ub294 SLIDING TILE ATTENTION \uc54c\uace0\ub9ac\uc998\uc758 \ud575\uc2ec \uac1c\ub150\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \uc124\uba85\ud574\uc90d\ub2c8\ub2e4.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2502.04507/x8.png", "caption": "Figure 9: 2D Sliding Window Attention visualization.", "description": "\uadf8\ub9bc 9\ub294 2\ucc28\uc6d0 \uc2ac\ub77c\uc774\ub529 \uc708\ub3c4\uc6b0 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \uc2dc\uac01\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ucffc\ub9ac \ud1a0\ud070\uc740 \uc911\uc2ec\uc744 \uae30\uc900\uc73c\ub85c \ud2b9\uc815 \ud06c\uae30\uc758 \uc708\ub3c4\uc6b0 \ub0b4\uc5d0 \uc788\ub294 \ud0a4 \ud1a0\ud070\ub4e4\uacfc\ub9cc \uc5b4\ud150\uc158\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4.  \uc708\ub3c4\uc6b0\ub294 \ud1a0\ud070 \ub2e8\uc704\ub85c \uc2ac\ub77c\uc774\ub529\ud558\uba70, \uac01 \uc708\ub3c4\uc6b0\ub9c8\ub2e4 \ucffc\ub9ac\uc640 \ud0a4 \uac04\uc758 \uc5b4\ud150\uc158\uc774 \uacc4\uc0b0\ub429\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc744 \ud1b5\ud574 \uc2ac\ub77c\uc774\ub529 \uc708\ub3c4\uc6b0 \uc5b4\ud150\uc158\uc774 \uc5b4\ub5bb\uac8c \ub3d9\uc791\ud558\ub294\uc9c0, \uadf8\ub9ac\uace0 \ucffc\ub9ac \ud1a0\ud070\uc774 \uc804\uccb4 \uacf5\uac04\uc774 \uc544\ub2cc \uad6d\ubd80\uc801\uc778 \uc601\uc5ed\uc5d0\ub9cc \uc9d1\uc911\ud558\uc5ec \uc5b4\ud150\uc158\uc744 \uacc4\uc0b0\ud558\ub294\uc9c0 \uc9c1\uad00\uc801\uc73c\ub85c \uc774\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2. Sliding Window Attention\uc758 \ube44\ud6a8\uc728\uc131"}, {"figure_path": "https://arxiv.org/html/2502.04507/x9.png", "caption": "Figure 10: Qualitative comparisons. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by \u0394\u0394\\Deltaroman_\u0394-DiT are generally less sharp than those generated by the original HunyuanVideo and \u00a0STA.", "description": "\uadf8\ub9bc 10\uc740 \ud6c8\ub828\ub41c STA(STA-t-2.43x)\uc758 \ucd9c\ub825 \ubd84\ud3ec\uc5d0 \uc57d\uac04\uc758 \ubcc0\ud654\uac00 \uc788\uc9c0\ub9cc, \uace0\ud488\uc9c8\uc758 \ube44\ub514\uc624 \uc0dd\uc131 \ub2a5\ub825\uc740 \uc720\uc9c0\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub300\uc870\uc801\uc73c\ub85c, A-DiT \ubaa8\ub378\uc740 \uc6d0\ubcf8 HunyuanVideo\ub098 STA \ubaa8\ub378\uc5d0 \ube44\ud574 \uc0dd\uc131\ub41c \ube44\ub514\uc624\uc758 \uc120\uba85\ub3c4\uac00 \ub5a8\uc5b4\uc9d1\ub2c8\ub2e4.  \uc989,  STA \uae30\ubc18 \ubaa8\ub378\uc774 \uae30\uc874 \ubaa8\ub378\uc5d0 \ube44\ud574 \ube44\ub514\uc624 \ud654\uc9c8\uc774 \ub354 \uc6b0\uc218\ud568\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud55c \uadf8\ub9bc\uc785\ub2c8\ub2e4. \ub450 \uac00\uc9c0 \ub2e4\ub978 \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud55c \ube44\ub514\uc624 \uc0dd\uc131 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ube44\uad50 \uc774\ubbf8\uc9c0\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.04507/x10.png", "caption": "Figure 11: Qualitative comparisons. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by \u0394\u0394\\Deltaroman_\u0394-DiT are generally less sharp than those generated by the original HunyuanVideo and \u00a0STA.", "description": "\uadf8\ub9bc 11\uc740 \ud6c8\ub828 \uc5c6\uc774 STA\ub97c \uc801\uc6a9\ud55c \ubaa8\ub378(STA-tf-1.36x), \ubbf8\uc138 \uc870\uc815\uc744 \ud1b5\ud574 STA\ub97c \uc801\uc6a9\ud55c \ubaa8\ub378(STA-t-2.43x), \uadf8\ub9ac\uace0 A-DiT \ubaa8\ub378\uc774 \uc0dd\uc131\ud55c \ube44\ub514\uc624\uc640 \uc6d0\ubcf8 HunyuanVideo \ubaa8\ub378\uc774 \uc0dd\uc131\ud55c \ube44\ub514\uc624\ub97c \uc815\uc131\uc801\uc73c\ub85c \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubbf8\uc138 \uc870\uc815\uc744 \uac70\uce5c STA-t-2.43x \ubaa8\ub378\uc740 \ucd9c\ub825 \ubd84\ud3ec\uc5d0 \uc57d\uac04\uc758 \ubcc0\ud654\uac00 \uc788\uc9c0\ub9cc, \uc5ec\uc804\ud788 \ub192\uc740 \ud654\uc9c8\uc758 \ube44\ub514\uc624 \uc0dd\uc131 \ub2a5\ub825\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4. \ubc18\uba74\uc5d0 A-DiT \ubaa8\ub378\uc740 \uc6d0\ubcf8 HunyuanVideo \ubc0f STA \ubaa8\ub378\uc5d0 \ube44\ud574 \uc0dd\uc131\ub41c \ube44\ub514\uc624\uc758 \uc120\uba85\ub3c4\uac00 \ub5a8\uc5b4\uc9c0\ub294 \uacbd\ud5a5\uc744 \ubcf4\uc785\ub2c8\ub2e4.  \uc989, \ubcf8 \uadf8\ub9bc\uc740 \uc81c\uc548\ub41c STA \ubc29\ubc95\uc774 \ube44\ub514\uc624 \uc0dd\uc131 \ud488\uc9c8\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud568\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc815\uc131\uc801 \ube44\uad50 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}]