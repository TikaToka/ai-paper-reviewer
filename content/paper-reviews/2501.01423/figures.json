[{"figure_path": "https://arxiv.org/html/2501.01423/x1.png", "caption": "Figure 1: Optimization dilemma within latent diffusion models. In latent diffusion models, increasing the dimension of the visual tokenizer enhances detail reconstruction but significantly reduces generation quality. (In tokenizer specification, \u201cf\u201d and \u201cd\u201d represent the downsampling rate and dimension, respectively. All results are evaluated on ImageNet 256\u00d7\\times\u00d7256 dataset with a fixed compute budget during diffusion model training.)", "description": "\ubcf8 \uadf8\ub9bc\uc740 \uc7a0\uc7ac \ud655\uc0b0 \ubaa8\ub378\uc5d0\uc11c \ucd5c\uc801\ud654 \ub51c\ub808\ub9c8\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc2dc\uac01\uc801 \ud1a0\ud06c\ub098\uc774\uc800\uc758 \ucc28\uc6d0\uc744 \ub298\ub9ac\uba74 \uc0c1\uc138\ud55c \uc7ac\uad6c\uc131\uc774 \ud5a5\uc0c1\ub418\uc9c0\ub9cc \uc0dd\uc131 \ud488\uc9c8\uc740 \ud06c\uac8c \uc800\ud558\ub429\ub2c8\ub2e4.  \ud1a0\ud06c\ub098\uc774\uc800 \uc0ac\uc591\uc5d0\uc11c 'f'\ub294 \ub2e4\uc6b4\uc0d8\ud50c\ub9c1 \ube44\uc728\uc744, 'd'\ub294 \ucc28\uc6d0\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ubaa8\ub4e0 \uacb0\uacfc\ub294 \uace0\uc815\ub41c \ucef4\ud4e8\ud305 \uc790\uc6d0\uc73c\ub85c Diffusion Model\uc744 \ud559\uc2b5\ud558\ub294 \ub3d9\uc548 ImageNet 256x256 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ud3c9\uac00\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \uc989, \ud1a0\ud06c\ub098\uc774\uc800\uc758 \ucc28\uc6d0\uc744 \ub192\uc774\uba74 \uc7ac\uad6c\ucd95 \uc131\ub2a5\uc740 \uc88b\uc544\uc9c0\uc9c0\ub9cc, \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uc131\ub2a5\uc740 \ub5a8\uc5b4\uc9c0\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4.  \uc774\ub7ec\ud55c \ud604\uc0c1\uc740 \ubaa8\ub378\uc758 \uacc4\uc0b0 \ube44\uc6a9\uacfc \ud559\uc2b5 \uc2dc\uac04 \uc99d\uac00\ub85c \uc774\uc5b4\uc9c0\uae30 \ub54c\ubb38\uc5d0 \ucd5c\uc801\uc758 \ubaa8\ub378\uc744 \ucc3e\uae30 \uc5b4\ub835\ub2e4\ub294 \uc810\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.01423/x2.png", "caption": "Figure 2: Reconstruction-generation frontier of latent diffusion models. VA-VAE improves the feature distribution of high-dimensional latent. Through alignment with vision foundation models, we expand the frontier between reconstruction and generation in latent diffusion models.", "description": "\uc774 \uadf8\ub9bc\uc740 \uc7a0\uc7ac \ud655\uc0b0 \ubaa8\ub378\uc758 \uc7ac\uad6c\uc131-\uc0dd\uc131 \uacbd\uacc4\uba74\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uace0\ucc28\uc6d0 \uc7a0\uc7ac \uacf5\uac04\uc5d0\uc11c VA-VAE(Vision Foundation model Aligned Variational AutoEncoder)\ub294 \ud2b9\uc9d5 \ubd84\ud3ec\ub97c \uac1c\uc120\ud569\ub2c8\ub2e4.  \ube44\uc804 \uae30\ubc18 \ubaa8\ub378\uacfc\uc758 \uc815\ub82c\uc744 \ud1b5\ud574 \uc7ac\uad6c\uc131\uacfc \uc0dd\uc131 \uac04\uc758 \uacbd\uacc4\uba74\uc744 \ud655\uc7a5\ud558\uc5ec, \uace0\ucc28\uc6d0 \uc7a0\uc7ac \uacf5\uac04\uc5d0\uc11c\ub3c4 \uc591\ucabd \uc131\ub2a5\uc744 \ub3d9\uc2dc\uc5d0 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \uae30\uc874 \ubaa8\ub378\ub4e4\uc740 \uc7ac\uad6c\uc131 \uc131\ub2a5\uc744 \ub192\uc774\uba74 \uc0dd\uc131 \uc131\ub2a5\uc774 \ub5a8\uc5b4\uc9c0\ub294 trade-off\ub97c \ubcf4\uc600\uc73c\ub098, VA-VAE\ub294 \uc774\ub7ec\ud55c \uc81c\uc57d\uc744 \uadf9\ubcf5\ud558\uace0 \uc591\ucabd \uc131\ub2a5\uc744 \ubaa8\ub450 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "3. Align VAE with Vision Foundation Models"}, {"figure_path": "https://arxiv.org/html/2501.01423/x3.png", "caption": "Figure 3: The proposed Vision foundation model Aligned VAE (VA-VAE). Vision foundation models are used to guide the training of high-dimensional visual tokenizers, effectively mitigating the optimization dilemma and improve generation performance.", "description": "\uadf8\ub9bc 3\uc740 \uc81c\uc548\ub41c \ube44\uc804 \uae30\ubc18 \ubaa8\ub378 \uc815\ub82c VAE(VA-VAE)\uc758 \uac1c\ub150\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uace0\ucc28\uc6d0 \uc2dc\uac01 \ud1a0\ud070\ud654\uae30\ub97c \ud559\uc2b5\uc2dc\ud0ac \ub54c, \ubbf8\ub9ac \ud559\uc2b5\ub41c \ube44\uc804 \uae30\ubc18 \ubaa8\ub378(\uc608: DINOv2, MAE)\uc744 \ud65c\uc6a9\ud558\uc5ec \uc7a0\uc7ac \uacf5\uac04\uc744 \uc81c\uc5b4\ud568\uc73c\ub85c\uc368 \uc7ac\uad6c\uc131\uacfc \uc0dd\uc131 \uc131\ub2a5 \uac04\uc758 \ucd5c\uc801\ud654 \ub51c\ub808\ub9c8\ub97c \uc644\ud654\ud558\uace0 \uc0dd\uc131 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uace0\ucc28\uc6d0 \uc2dc\uac01 \ud1a0\ud070\ud654\uae30\ub294 \ub192\uc740 \ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \uc555\ucd95\ud558\uace0 \uc7ac\uad6c\uc131\ud560 \uc218 \uc788\uc9c0\ub9cc,  \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uc815\ubcf4 \uc190\uc2e4\uc774 \ubc1c\uc0dd\ud558\uc5ec \uc0dd\uc131 \uc131\ub2a5\uc774 \uc800\ud558\ub418\ub294 \ubb38\uc81c\uc810\uc774 \uc788\uc2b5\ub2c8\ub2e4.  VA-VAE\ub294 \uc0ac\uc804 \ud559\uc2b5\ub41c \ube44\uc804 \uae30\ubc18 \ubaa8\ub378\uc758 \uc9c0\uc2dd\uc744 \ud65c\uc6a9\ud558\uc5ec, \uace0\ucc28\uc6d0 \uc7a0\uc7ac \uacf5\uac04\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud559\uc2b5\ud558\uace0 \uc0dd\uc131 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ucf1c \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \ud574\uacb0\ud569\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 VA-VAE\uc758 \uc544\ud0a4\ud14d\ucc98\uc640 \ube44\uc804 \uae30\ubc18 \ubaa8\ub378\uc744 \ud65c\uc6a9\ud55c \ud559\uc2b5 \uacfc\uc815\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\uba70, \uac01 \uad6c\uc131 \uc694\uc18c\uc758 \uc5ed\ud560\uacfc \uc0c1\ud638 \uc791\uc6a9\uc744 \uba85\ud655\ud788 \uc124\uba85\ud569\ub2c8\ub2e4.", "section": "3. Align VAE with Vision Foundation Models"}, {"figure_path": "https://arxiv.org/html/2501.01423/x4.png", "caption": "(a)", "description": "\uadf8\ub9bc 4(a)\ub294 \uc11c\ub85c \ub2e4\ub978 \ud1a0\ud06c\ub098\uc774\uc800(f16d32, f16d64)\ub97c \uc0ac\uc6a9\ud558\uc5ec LightningDiT-B \ubaa8\ub378\uc744 ImageNet 256 \ud574\uc0c1\ub3c4\uc5d0\uc11c 160 \uc5d0\ud3ed \ub3d9\uc548 \ud559\uc2b5\uc2dc\ud0a8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. VF \uc190\uc2e4(Vision Foundation model alignment Loss)\uc744 \uc801\uc6a9\ud588\uc744 \ub54c, \uc801\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\uc5d0 \ube44\ud574 FID(Fr\u00e9chet Inception Distance) \uac12\uc774 \ud6e8\uc52c \ube60\ub974\uac8c \uac10\uc18c\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 VF \uc190\uc2e4\uc774 \uc218\ub834 \uc18d\ub3c4\ub97c \ucd5c\ub300 2.7\ubc30\uae4c\uc9c0 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  f16d32 \ud1a0\ud06c\ub098\uc774\uc800\uc758 \uacbd\uc6b0 VF loss (DINOv2)\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c FID\uac00 \uc57d 2.54\ubc30 \ube60\ub974\uac8c \uac10\uc18c\ud588\uace0, f16d64 \ud1a0\ud06c\ub098\uc774\uc800\uc758 \uacbd\uc6b0 \uc57d 2.76\ubc30 \ube60\ub974\uac8c \uac10\uc18c\ud588\uc2b5\ub2c8\ub2e4.", "section": "4. VF Loss \uac1c\uc120"}, {"figure_path": "https://arxiv.org/html/2501.01423/x5.png", "caption": "(b)", "description": "\uadf8\ub9bc (b)\ub294 \ub2e4\uc591\ud55c \ud1a0\ud06c\ub098\uc774\uc800(f16d32, f16d64)\ub97c \uc0ac\uc6a9\ud558\uc5ec LightningDiT-B \ubaa8\ub378\uc744 160 \uc5d0\ud3ed \ub3d9\uc548 ImageNet 256 \ud574\uc0c1\ub3c4\uc5d0\uc11c \ud559\uc2b5\uc2dc\ud0a8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. VF \uc190\uc2e4(VF Loss)\uc744 \uc801\uc6a9\ud588\uc744 \ub54c, VF Loss\ub97c \uc801\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\ubcf4\ub2e4 \ud6e8\uc52c \ube60\ub974\uac8c \uc218\ub834\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. f16d64 \ud1a0\ud06c\ub098\uc774\uc800\uc758 \uacbd\uc6b0 VF Loss \uc801\uc6a9 \uc2dc \ucd5c\ub300 2.7\ubc30\uc758 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uace0\ucc28\uc6d0 \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0 VF Loss\uac00 \uc218\ub834 \uc18d\ub3c4\ub97c \ud06c\uac8c \uac1c\uc120\ud55c\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "3.3 \uc801\uc751\uc801 \uac00\uc911\uce58"}, {"figure_path": "https://arxiv.org/html/2501.01423/x6.png", "caption": "(c)", "description": "\uadf8\ub9bc (c)\ub294 \ub2e4\uc591\ud55c \ud1a0\ud06c\ub098\uc774\uc800 \ud06c\uae30(\ucc28\uc6d0)\uc5d0\uc11c VF Loss(Vision Foundation model alignment Loss)\uc758 \ud655\uc7a5\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 DiT(Diffusion Transformer) \ubaa8\ub378\uc758 \ud06c\uae30(\uc2ed\uc5b5 \ub9e4\uac1c\ubcc0\uc218)\ub97c \ub85c\uadf8 \uc2a4\ucf00\uc77c\ub85c \ub098\ud0c0\ub0b4\uace0, y\ucd95\uc740 FID(Fr\u00e9chet Inception Distance) \uc810\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc5ec\ub7ec \ud1a0\ud06c\ub098\uc774\uc800 \ud06c\uae30(f16d16, f16d32, f16d64)\uc5d0 \ub300\ud574, VF Loss\ub97c \uc0ac\uc6a9\ud55c \uacbd\uc6b0\uc640 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0\uc758 FID \uc810\uc218\ub97c \ube44\uad50\ud558\uc5ec, VF Loss\uac00 \uace0\ucc28\uc6d0 \ud1a0\ud06c\ub098\uc774\uc800\uc5d0\uc11c\ub3c4 \ub354 \uc791\uc740 \ubaa8\ub378 \ud06c\uae30\ub85c \ub192\uc740 \uc131\ub2a5\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, VF Loss\ub97c \ud1b5\ud574 \uace0\ucc28\uc6d0 \ud1a0\ud06c\ub098\uc774\uc800\uc758 \ud655\uc7a5\uc131 \ubb38\uc81c\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ud574\uacb0\ud568\uc73c\ub85c\uc368,  \ub354 \uc801\uc740 \ub9e4\uac1c\ubcc0\uc218\ub85c\ub3c4 \uc6b0\uc218\ud55c \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uc131\ub2a5\uc744 \uc5bb\uc744 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788, \ubaa8\ub378 \ud06c\uae30\uac00 \ucee4\uc9d0\uc5d0 \ub530\ub77c VF Loss\uc758 \ud6a8\uacfc\uac00 \ub354\uc6b1 \ub450\ub4dc\ub7ec\uc9c0\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.3 Foundation Models Improve Scalability"}, {"figure_path": "https://arxiv.org/html/2501.01423/x7.png", "caption": "Figure 4: (a)&(b) VF Loss Improves Convergence. We train LightningDiT-B for 160 epochs on ImageNet at 256 resolution using different tokenizers. The VF loss significantly accelerates convergence, with a maximum speedup of up to 2.7 times. (c) VF Loss Improves Scalability. VF loss reduces the need for large parameters in generative models of high-dimensional tokenizer, enabling better scalability.", "description": "\uadf8\ub9bc 4\ub294 VF Loss(Vision Foundation model alignment Loss)\uac00 Latent Diffusion Model\uc758 \ud559\uc2b5 \uc18d\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0a4\uace0 \ubaa8\ub378\uc758 \ud655\uc7a5\uc131\uc744 \uac1c\uc120\ud558\ub294 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\uc640 (b)\ub294 \uc11c\ub85c \ub2e4\ub978 \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc0ac\uc6a9\ud558\uc5ec ImageNet 256 \ud574\uc0c1\ub3c4\uc5d0\uc11c LightningDiT-B \ubaa8\ub378\uc744 160 \uc5d0\ud3ed \ub3d9\uc548 \ud559\uc2b5\uc2dc\ud0a8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. VF Loss\ub97c \uc801\uc6a9\ud55c \uacbd\uc6b0, \ucd5c\ub300 2.7\ubc30\uae4c\uc9c0 \ud559\uc2b5 \uc18d\ub3c4\uac00 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (c)\ub294 \uace0\ucc28\uc6d0 \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc0ac\uc6a9\ud558\ub294 \uc0dd\uc131 \ubaa8\ub378\uc5d0\uc11c VF Loss\uac00 \ud544\uc694\ud55c \ud30c\ub77c\ubbf8\ud130\uc758 \uc218\ub97c \uc904\uc5ec \ud655\uc7a5\uc131\uc744 \uac1c\uc120\ud558\ub294 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Adaptive Weighting"}, {"figure_path": "https://arxiv.org/html/2501.01423/x8.png", "caption": "Figure 5: Visualization Results. We visualize our latent diffusion system with proposed VA-VAE together with LightningDiT-XL trained on ImageNet 256\u00d7256256256256\\times 256256 \u00d7 256 resolution.", "description": "\uc774 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ub41c VA-VAE\uc640 LightningDiT-XL \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec ImageNet 256x256 \ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc885\ub958\uc758 \uc774\ubbf8\uc9c0\ub4e4\uc774 \uc0dd\uc131\ub418\uc5c8\uc73c\uba70, \ubaa8\ub378\uc758 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \ub2a5\ub825\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.  \uac01 \uc774\ubbf8\uc9c0\ub294 \ubaa8\ub378\uc774 \uc5bc\ub9c8\ub098 \ub2e4\uc591\ud558\uace0 \uc0ac\uc2e4\uc801\uc778 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.", "section": "5. Experiments"}]