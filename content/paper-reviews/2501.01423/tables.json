[{"content": "| Training Trick | Training Sample | Epoch | FID-50k \u2193 | \n|---|---|---|---| \n| DiT-XL/2 [29] | 400k \u00d7 256 | 80 | 19.50 | \n| **Training Strategies** |  |  |  | \n| + Rectified Flow [23] | 400k \u00d7 256 | 80 | 17.20 | \n| + *batchsize* \u00d7 4 & *lr* \u00d7 2 | 100k \u00d7 1024 | 80 | 16.59 | \n| + AdamW \u03b2<sub>2</sub>=0.95 [1] | 100k \u00d7 1024 | 80 | 16.61 | \n| + Logit Normal Sampling [7] | 100k \u00d7 1024 | 80 | 13.99 | \n| + Velocity Direction Loss [41] | 100k \u00d7 1024 | 80 | 12.52 | \n| **Architecture Improvements** |  |  |  | \n| + SwiGLU FFN [34] | 100k \u00d7 1024 | 80 | 10.10 | \n| + RMS Norm [44] | 100k \u00d7 1024 | 80 | 9.25 | \n| + Rotary Pos Embed [35] | 100k \u00d7 1024 | 80 | 7.13 | \n| + patch size=1 & VA-VAE (Sec. 3) | 100k \u00d7 1024 | 80 | 4.29 |", "caption": "Table 1: Performance of LightningDiT. With SD-VAE\u00a0[33], LightningDiT\u00a0achieves FID-50k=7.13 on ImageNet class-conditional generation, using 94% fewer training samples compared to the original DiT\u00a0[29].\nWe show that the original DiT can also achieve exceptional performance by leveraging advanced design techniques.", "description": "\ud45c 1\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 LightningDiT \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\uc874 DiT [29] \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec, SD-VAE [33]\ub97c \uc0ac\uc6a9\ud55c LightningDiT\ub294 ImageNet \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uc791\uc5c5\uc5d0\uc11c FID-50k 7.13\uc774\ub77c\ub294 \ucd5c\ucca8\ub2e8 \uc131\ub2a5\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, \uae30\uc874 DiT \ubaa8\ub378 \ub300\ube44 94% \uc801\uc740 \ud6c8\ub828 \uc0d8\ud50c\uc744 \uc0ac\uc6a9\ud558\uba74\uc11c\ub3c4 \uc774\ub7ec\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\uc5ec \ud6c8\ub828 \ud6a8\uc728\uc131\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ucf30\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c, \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 \uace0\uae09 \uc124\uacc4 \uae30\ubc95\ub4e4\uc744 \ud65c\uc6a9\ud558\uc5ec \uae30\uc874 DiT \ubaa8\ub378\ub3c4 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ucd94\uac00\uc801\uc73c\ub85c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "2.2. Diffusion Transformer\uc758 \ube60\ub978 \uc218\ub834"}, {"content": "| Tokenizer | Spec. | Reconstruction Performance |  |  |  | Generation Performance (FID-10K)\u2193 |  |  | \n|---|---|---|---|---|---|---|---|---| \n| **Tokenizer** | **Spec.** | **rFID\u2193** | **PSNR\u2191** | **LPIPS\u2193** | **SSIM\u2191** | **LightningDiT-B** | **LightningDiT-L** | **LightningDiT-XL** | \n| LDM [33] |  | 0.49 | 26.10 | 0.132 | 0.72 | 16.24 | 9.49 | 8.28 | \n| LDM+VF loss (MAE) [15] |  | 0.51 | 26.01 | 0.137 | 0.71 | 16.86 (+0.62) | 10.93 (+1.44) | 9.19 (+0.91) | \n| LDM+VF loss (DINOv2) [28] |  | 0.55 | 25.29 | 0.147 | 0.69 | 15.79 (-0.45) | 10.02 (+0.53) | 8.71 (+0.43) | \n| LDM [33] |  | 0.26 | 28.59 | 0.089 | 0.80 | 22.62 | 12.86 | 10.92 | \n| LDM+VF loss (MAE) [15] |  | 0.28 | 28.33 | 0.091 | 0.80 | 19.89 (-2.73) | 11.51 (-1.35) | 9.92 (-1.00) | \n| LDM+VF loss (DINOv2) [28] |  | 0.28 | 27.96 | 0.096 | 0.79 | 15.82 (-6.80) | 9.82 (-3.04) | 8.22 (-2.70) | \n| LDM [33] |  | 0.17 | 31.03 | 0.055 | 0.88 | 36.83 | 20.73 | 17.24 | \n| LDM+VF loss (MAE) [15] |  | 0.15 | 31.03 | 0.054 | 0.87 | 23.58 (-13.25) | 14.40 (-6.33) | 11.69 (-5.55) | \n| LDM+VF loss (DINOv2) [28] |  | 0.14 | 30.71 | 0.055 | 0.87 | 24.00 (-12.83) | 14.95 (-5.78) | 11.98 (-5.26) | ", "caption": "Table 2: VF loss Improves Generation Performance. The f16d16 tokenizer specification is widely used\u00a0[33, 21]. As dimensionality increases, we observe that (1) higher dimensions improve reconstruction but reduce generation quality, highlighting an optimization dilemma within the latent diffusion framework; (2) VF Loss significantly enhances generative performance in high-dimensional tokenizers with minimal impact on reconstruction.", "description": "\ud45c 2\ub294 VF \uc190\uc2e4\uc774 \uc0dd\uc131 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  f16d16 \ud1a0\ud06c\ub098\uc774\uc800 \uc0ac\uc591\uc740 \ub110\ub9ac \uc0ac\uc6a9\ub429\ub2c8\ub2e4 [33, 21]. \ucc28\uc6d0\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc7ac\uad6c\uc131 \uc131\ub2a5\uc740 \ud5a5\uc0c1\ub418\uc9c0\ub9cc \uc0dd\uc131 \ud488\uc9c8\uc740 \uc800\ud558\ub418\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc7a0\uc7ac \ud655\uc0b0 \ud504\ub808\uc784\uc6cc\ud06c \ub0b4\uc758 \ucd5c\uc801\ud654 \ub51c\ub808\ub9c8\ub97c \uac15\uc870\ud569\ub2c8\ub2e4.  VF \uc190\uc2e4\uc740 \uc7ac\uad6c\uc131 \uc131\ub2a5\uc5d0 \uac70\uc758 \uc601\ud5a5\uc744 \ubbf8\uce58\uc9c0 \uc54a\uc73c\uba74\uc11c \uace0\ucc28\uc6d0 \ud1a0\ud06c\ub098\uc774\uc800\uc758 \uc0dd\uc131 \uc131\ub2a5\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.", "section": "3. VF \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud55c Vision Foundation Model \uc815\ub82c"}, {"content": "| Method | Tokenizer | rFID | gFID | #params | sFID | IS | Pre. | Rec. | gFID | sFID | IS | Pre. | Rec. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **AutoRegressive (AR)** |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| MaskGIT [2] | MaskGiT | 2.28 | 555 | 227M | 6.18 | - | 182.1 | 0.80 | 0.51 | - | - | - | - |\n| LlamaGen [36] | VQGAN\u2020 | 0.59 | 300 | 3.1B | 9.38 | 8.24 | 112.9 | 0.69 | 0.67 | 2.18 | 5.97 | 263.3 | 0.81 | 0.58 |\n| VAR [38] | - | - | 350 | 2.0B | - | - | - | - | - | 1.80 | - | 365.4 | 0.83 | 0.57 |\n| MagViT-v2 [42] | - | - | 1080 | 307M | 3.65 | - | 200.5 | - | - | 1.78 | - | 319.4 | - | - |\n| MAR [21] | LDM\u2020 | 0.53 | 800 | 945M | 2.35 | - | 227.8 | 0.79 | 0.62 | 1.55 | - | 303.7 | 0.81 | 0.62 |\n| **Latent Diffusion Models** |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| MaskDiT [45] | SD-VAE [33] | 0.61 | 1600 | 675M | 5.69 | 10.34 | 177.9 | 0.74 | 0.60 | 2.28 | 5.67 | 276.6 | 0.80 | 0.61 |\n| DiT [29] | SD-VAE [33] | 0.61 | 1400 | 675M | 9.62 | 6.85 | 121.5 | 0.67 | 0.67 | 2.27 | 4.60 | 278.2 | 0.83 | 0.57 |\n| SiT [26] | SD-VAE [33] | 0.61 | 1400 | 675M | 8.61 | 6.32 | 131.7 | 0.68 | 0.67 | 2.06 | 4.50 | 270.3 | 0.82 | 0.59 |\n| FasterDiT [41] | SD-VAE [33] | 0.61 | 400 | 675M | 7.91 | 5.45 | 131.3 | 0.67 | 0.69 | 2.03 | 4.63 | 264.0 | 0.81 | 0.60 |\n| MDT [11] | SD-VAE [33] | 0.61 | 1300 | 675M | 6.23 | 5.23 | 143.0 | 0.71 | 0.65 | 1.79 | 4.57 | 283.0 | 0.81 | 0.61 |\n| MDTv2 [12] | SD-VAE [33] | 0.61 | 1080 | 675M | - | - | - | - | - | 1.58 | 4.52 | 314.7 | 0.79 | 0.65 |\n| REPA [43] |  |  | 800 | 675M | 5.90 | - | - | - | - | 1.42 | 4.70 | 305.7 | 0.80 | 0.65 |\n| LightningDiT | VA-VAE | 0.28 | 64 | 675M | 5.14 | 4.22 | 130.2 | 0.76 | 0.62 | 2.11 | 4.16 | 252.3 | 0.81 | 0.58 |\n|  | VA-VAE | 0.28 | 800 | 675M | 2.17 | 4.36 | 205.6 | 0.77 | 0.65 | 1.35 | 4.15 | 295.3 | 0.79 | 0.65 |", "caption": "Table 3: System-Level Performance on ImageNet 256\u00d7\\times\u00d7256. Our latent diffusion system achieves state-of-the-art performance with rFID=0.28 and FID=1.35. Besides, our LightningDiT together with VA-VAE surpasses DiT\u00a0[29] and SiT\u00a0[26] in FID within only 64 training epochs, demonstrating a 21.8 \u00d7\\times\u00d7 faster convergence.", "description": "\ud45c 3\uc740 ImageNet 256x256 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \ubc29\ubc95\ub4e4\uc744 \uc0ac\uc6a9\ud558\uc5ec \uce21\uc815\ub41c \uc2dc\uc2a4\ud15c \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \uc7a0\uc7ac \ud655\uc0b0 \ubaa8\ub378(LightningDiT + VA-VAE)\uc740 rFID 0.28, FID 1.35\ub77c\ub294 \ucd5c\ucca8\ub2e8 \uc131\ub2a5\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, \uae30\uc874\uc758 DiT [29] \uc640 SiT [26] \ubaa8\ub378\uacfc \ube44\uad50\ud588\uc744 \ub54c, \ub2e8 64\ubc88\uc758 \ud559\uc2b5 \uc5d0\ud3ed\ub9cc\uc73c\ub85c\ub3c4 FID \uc9c0\ud45c\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uc73c\uba70, \uc774\ub294 \uae30\uc874 \ubaa8\ub378\ubcf4\ub2e4 21.8\ubc30 \ube60\ub978 \uc218\ub834 \uc18d\ub3c4\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uc7ac\uad6c\uc131 \uc131\ub2a5(rFID, PSNR, LPIPS, SSIM)\uacfc \uc0dd\uc131 \uc131\ub2a5(gFID, IS, Pre, Rec)\uc744 \ud3ec\ud568\ud558\uc5ec \ub2e4\uc591\ud55c \uc9c0\ud45c\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. CFG(Classifier-Free Guidance)\ub97c \uc0ac\uc6a9\ud55c \uacbd\uc6b0\uc640 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0\uc758 \uc131\ub2a5 \ucc28\uc774\ub3c4 \ube44\uad50 \ubd84\uc11d\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218(params), \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \uc5d0\ud3ed \uc218(Epoches)\uc640 \ub9e4\uac1c\ubcc0\uc218 \uc218(params)\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ud558\uc5ec \ubaa8\ub378\uc758 \ubcf5\uc7a1\ub3c4 \ubc0f \ud559\uc2b5 \ud6a8\uc728\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\ub3c4\ub85d \ub3d5\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. VA-VAE\ub97c \uc0ac\uc6a9\ud55c \ube44\uc804 \uae30\ubc18 \ubaa8\ub378 \uc815\ub82c (Vision Foundation Model Alignment with VA-VAE) \ub610\ub294 4. \ud5a5\uc0c1\ub41c \ud655\uc0b0 \ud2b8\ub79c\uc2a4\ud3ec\uba38(Improved Diffusion Transformer)"}, {"content": "| Model Type | rFID \u2193 | PSNR \u2191 | LPIPS \u2193 | SSIM \u2191 | gFID \u2193 |\n|---|---|---|---|---|---| \n| naive | 0.26 | 28.59 | 0.089 | 0.80 | 22.62 |\n| DINOv2 [28] | 0.28 | 27.96 | 0.096 | 0.79 | 15.82 |\n| MAE [15] | 0.28 | 28.33 | 0.091 | 0.80 | 19.89 |\n| SAM [18] | 0.26 | 28.31 | 0.091 | 0.80 | 19.80 |\n| CLIP [32] | 0.33 | 28.39 | 0.091 | 0.80 | 18.93 |", "caption": "Table 4: Ablation on Foundation Models. We evaluate the impact of different VF losses on generative performance. Our results show that DINOv2 achieves the highest generative performance.", "description": "\uc774 \ud45c\ub294 \ub2e4\uc591\ud55c Vision Foundation Model(VFM)\uc744 \uc0ac\uc6a9\ud558\uc5ec VF Loss(Vision Foundation model alignment Loss)\uc758 \uc0dd\uc131 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc2e4\ud5d8\uc740  f16d32 \ud1a0\ud06c\ub098\uc774\uc800\ub97c 50 \uc5d0\ud3ed \ub3d9\uc548 \ud559\uc2b5\uc2dc\ud0a4\uace0, \uac01\uac01\uc758 VFM\uc744 \uc0ac\uc6a9\ud558\uc5ec LightningDiT-B\ub97c 160 \uc5d0\ud3ed \ub3d9\uc548 \ud559\uc2b5\uc2dc\ucf1c \uc218\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\ub4ef\uc774, DINOv2\uac00 \uac00\uc7a5 \ub192\uc740 \uc0dd\uc131 \uc131\ub2a5(\uac00\uc7a5 \ub0ae\uc740 gFID)\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.  \uc774\ub294 DINOv2\uac00 \ub2e4\ub978 VFM\uc5d0 \ube44\ud574 \ub354 \ub098\uc740 latent space \uc815\ub82c\uc744 \uc81c\uacf5\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "6.2 Vision Foundation Models"}, {"content": "| Loss Type | rFID\u2193 | PSNR\u2191 | LPIPS\u2193 | SSIM\u2191 | gFID\u2193 |\n|---|---|---|---|---|---| \n| NaN | 0.26 | 28.59 | 0.089 | 0.80 | 22.62 |\n| full | 0.28 | 27.96 | 0.096 | 0.79 | 15.82 |\n| -mcos loss | 0.27 | 28.52 | 0.090 | 0.80 | 21.87 |\n| -mdistmat loss | 0.27 | 28.24 | 0.090 | 0.80 | 17.74 |\n| -margin | 0.27 | 28.07 | 0.093 | 0.79 | 17.77 |", "caption": "Table 5: Ablation Study of VF Loss Formulations:\nComparison of different configurations on generative performance metrics using LightningDiT-B.", "description": "\uc774 \ud45c\ub294 LightningDiT-B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec VF \uc190\uc2e4 \uacf5\uc2dd\uc758 \uc5ec\ub7ec \uad6c\uc131\uc5d0 \ub300\ud55c \uc0dd\uc131 \uc131\ub2a5 \uc9c0\ud45c\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, \ub9c8\uc9c4 \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4 \uc190\uc2e4(mcos), \ub9c8\uc9c4 \uac70\ub9ac \ud589\ub82c \uc720\uc0ac\ub3c4 \uc190\uc2e4(mdistmat), \ub9c8\uc9c4 \uac12\uc744 \uc81c\uac70\ud55c \uacbd\uc6b0\uc758 \uc7ac\uad6c\uc131 \ubc0f \uc0dd\uc131 \uc131\ub2a5(rFID, PSNR, LPIPS, SSIM, gFID) \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\uc5b4 \uac01 \uad6c\uc131 \uc694\uc18c\uc758 \ud6a8\uacfc\ub97c \ubd84\uc11d\ud569\ub2c8\ub2e4.  LightningDiT-B \ubaa8\ub378\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c \ud5a5\uc0c1\ub41c DiT \uae30\ubcf8 \ubaa8\ub378\uc785\ub2c8\ub2e4.", "section": "6.3 Ablations on Loss Formulations"}, {"content": "| Tokenizer | VF Loss | density \u2193 | gini coefficient \u2193 | normalized entropy \u2191 | gFID (DiT-B) \u2193 |\n|---|---|---|---|---|---| \n| _f16d32_ | NaN | 0.263 | 0.145 | 0.995 | 22.62 |\n|  | MAE | 0.193 | 0.101 | 0.997 | 19.89 |\n|  | **DINOv2** | **0.178** | **0.096** | **0.998** | **15.82** |\n| _f16d64_ | NaN | 0.296 | 0.166 | 0.994 | 36.83 |\n|  | MAE | 0.256 | 0.143 | 0.995 | **23.58** |\n|  | **DINOv2** | **0.251** | **0.141** | **0.996** | 24.00 |", "caption": "Table 6: Relationship between uniformity and generative performance: We evaluate the uniformity of feature distribution. Results indicate a possible positive correlation between the uniformity of feature distribution and generative performance.", "description": "\ud45c 6\uc740 \ud2b9\uc9d5 \ubd84\ud3ec\uc758 \uade0\uc77c\uc131\uc744 \ud3c9\uac00\ud558\uace0, \uadf8 \uacb0\uacfc\ub97c \ubc14\ud0d5\uc73c\ub85c \ud2b9\uc9d5 \ubd84\ud3ec\uc758 \uade0\uc77c\uc131\uacfc \uc0dd\uc131 \uc131\ub2a5 \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubd84\uc11d\ud55c \ub0b4\uc6a9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e8\uc21c\ud788 \ucea1\uc158\uc5d0 \uc81c\uc2dc\ub41c \ub0b4\uc6a9\ubcf4\ub2e4 \uc790\uc138\ud788 \uc124\uba85\ud558\uba74,  \ub2e4\uc591\ud55c \ud1a0\ud06c\ub098\uc774\uc800(f16d32, f16d64)\uc640 VF Loss \uc801\uc6a9 \uc5ec\ubd80\uc5d0 \ub530\ub978 \uc7a0\uc7ac \uacf5\uac04\uc758 \ubd84\ud3ec \uade0\uc77c\uc131\uc744 t-SNE \uae30\ubc95\uc744 \ud1b5\ud574 \uc2dc\uac01\ud654\ud558\uace0, KDE(Kernel Density Estimation)\uc744 \uc774\uc6a9\ud558\uc5ec \uade0\uc77c\uc131 \uc9c0\ud45c(\ud45c\uc900\ud3b8\ucc28, \uc9c0\ub2c8 \uacc4\uc218)\ub97c \uacc4\uc0b0\ud588\uc2b5\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uc0dd\uc131 \uc131\ub2a5 \uc9c0\ud45c\uc778 gFID\uc640\uc758 \uad00\uacc4\ub97c \ubd84\uc11d\ud558\uc5ec, \uc7a0\uc7ac \uacf5\uac04\uc758 \ubd84\ud3ec\uac00 \uade0\uc77c\ud560\uc218\ub85d \uc0dd\uc131 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub420 \uac00\ub2a5\uc131\uc774 \uc788\uc74c\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "6.4 Discuss on VF loss with Latent Distribution"}]