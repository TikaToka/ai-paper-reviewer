{"references": [{"fullname_first_author": "Marah Abdin", "paper_title": "Phi-4 technical report", "publication_date": "2024-12-00", "reason": "This paper is highly relevant due to its focus on seed-based synthetic data generation for language models, a topic directly compared and contrasted with the proposed MAGA method."}, {"fullname_first_author": "Loubna Ben Allal", "paper_title": "Smollm-corpus", "publication_date": "2024-00-00", "reason": "The SmolLM-corpus serves as the foundational dataset for the MAGA method, making it a crucial reference for understanding the context and baseline performance of the proposed approach."}, {"fullname_first_author": "Guilherme Penedo", "paper_title": "The refined-web dataset for falcon llm: Outperforming curated corpora with web data only", "publication_date": "2023-00-00", "reason": "This paper is important as it provides insights into data curation techniques and benchmarks for high-quality web data, which are relevant to the MAGA method's data expansion strategy."}, {"fullname_first_author": "Jack W Rae", "paper_title": "Scaling language models: Methods, analysis & insights from training gopher", "publication_date": "2021-12-00", "reason": "This work offers insights into scaling laws for language models, providing a theoretical foundation for understanding the relationship between model size, data, and performance, which is directly relevant to the challenges addressed by MAGA."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-00", "reason": "This seminal paper establishes the critical role of scale in LLM performance, highlighting the relationship between model size and data, which is foundational to the MAGA approach's emphasis on data expansion."}]}