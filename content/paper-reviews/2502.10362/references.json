{"references": [{"fullname_first_author": "Andrea Agostinelli", "paper_title": "MusicLM: Generating music from text", "publication_date": "2023-01-11", "reason": "This paper proposes a text-to-music generation model, which is directly relevant to the current paper's focus on cross-modal music information retrieval and generation."}, {"fullname_first_author": "Jisheng Bai", "paper_title": "AudioSetCaps: An enriched audio-caption dataset using automated generation pipeline with large audio and language models", "publication_date": "2024-11-24", "reason": "This paper introduces a new dataset for music information retrieval, enriching existing datasets with more diverse and detailed annotations, which directly addresses the data scarcity challenges discussed in the current work."}, {"fullname_first_author": "Lo\u00efc Barrault", "paper_title": "SeamlessM4T\u2014Massively multilingual & multimodal machine translation", "publication_date": "2023-08-11", "reason": "This paper presents a state-of-the-art multilingual and multimodal machine translation model, which is crucial for enabling cross-lingual generalization in the current paper's MIR framework."}, {"fullname_first_author": "Rohit Girdhar", "paper_title": "ImageBind: One embedding space to bind them all", "publication_date": "2023-06-17", "reason": "This paper introduces a multi-modal framework that unifies multiple modalities into a single embedding space.  This is highly relevant to the current paper's approach of aligning different music modalities with text using contrastive learning."}, {"fullname_first_author": "Shangda Wu", "paper_title": "CLAMP: Contrastive language-music pre-training for cross-modal symbolic music information retrieval", "publication_date": "2023-11-05", "reason": "This paper introduces CLAMP, a previous version of the model presented in the current paper, establishing the foundational work and providing a baseline for comparison."}]}