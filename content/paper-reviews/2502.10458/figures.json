[{"figure_path": "https://arxiv.org/html/2502.10458/x2.png", "caption": "Figure 1: \n(a) Our ThinkDiff reasons over interleaved images (a flying monkey and a flying cat) and text prompts (monkey, cat, and zebra) to generate a logically correct and high-quality image (a flying zebra). The ground truth reasoning answer is provided as a reference for readers. (b) ThinkDiff composes images and texts into a coherent and reasonable image.", "description": "\uadf8\ub9bc 1\uc740 ThinkDiff \ubaa8\ub378\uc758 \ub2e4\uc911 \ubaa8\ub4dc \ub9e5\ub77d \ucd94\ub860 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \ub0a0\uc544\ub2e4\ub2c8\ub294 \uc6d0\uc22d\uc774\uc640 \ub0a0\uc544\ub2e4\ub2c8\ub294 \uace0\uc591\uc774 \uc774\ubbf8\uc9c0\uc640 \uc6d0\uc22d\uc774, \uace0\uc591\uc774, \uc5bc\ub8e9\ub9d0\uc774\ub77c\ub294 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\ub97c ThinkDiff\uac00 \uacb0\ud569\ud558\uc5ec \ub17c\ub9ac\uc801\uc73c\ub85c \uc815\ud655\ud558\uace0 \uace0\ud488\uc9c8\uc758 \uc774\ubbf8\uc9c0(\ub0a0\uc544\ub2e4\ub2c8\ub294 \uc5bc\ub8e9\ub9d0)\ub97c \uc0dd\uc131\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ucc38\uace0\ub85c, \uc2e4\uc81c \ucd94\ub860 \uacb0\uacfc\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. (b)\ub294 ThinkDiff\uac00 \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8\ub97c \uacb0\ud569\ud558\uc5ec \uc77c\uad00\uc131 \uc788\uace0 \ud0c0\ub2f9\ud55c \uc774\ubbf8\uc9c0\ub97c \uad6c\uc131\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.10458/x3.png", "caption": "Figure 2: \n(a) Reconstruction-based diffusion finetuning integrates image features using a diffusion loss, focusing on pixel-level image reconstruction without reasoning.\n(b) ThinkDiff aligns a VLM to an LLM decoder by vision-language training on image-caption datasets. In inference (dotted lines), it transfers multimodal in-context reasoning capabilities from the VLM to a diffusion decoder.", "description": "\uadf8\ub9bc 2\ub294 \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \ud655\uc0b0 \ubaa8\ub378\uc5d0 \ub300\ud55c \ub450 \uac00\uc9c0 \uc811\uadfc \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \uae30\uc874\uc758 \uc7ac\uad6c\uc131 \uae30\ubc18 \ubbf8\uc138 \uc870\uc815 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ubc29\ubc95\uc740 \ud655\uc0b0 \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0 \ud2b9\uc9d5\uc744 \ud1b5\ud569\ud558\uc9c0\ub9cc \ucd94\ub860 \ub2a5\ub825\uc740 \uace0\ub824\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. (b)\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 ThinkDiff \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. ThinkDiff\ub294 \uc774\ubbf8\uc9c0-\ucea1\uc158 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ube44\uc804-\uc5b8\uc5b4 \ud559\uc2b5\uc744 \ud1b5\ud574 VLM(Vision-Language Model)\uc744 LLM(Large Language Model) \ub514\ucf54\ub354\uc5d0 \uc815\ub82c\ud569\ub2c8\ub2e4. \ucd94\ub860 \ub2e8\uacc4\uc5d0\uc11c\ub294 \uc810\uc120\uc73c\ub85c \ud45c\uc2dc\ub41c \ubc14\uc640 \uac19\uc774 VLM\uc758 \ub2e4\uc911 \ubaa8\ub4dc \uc0c1\ud669 \ucd94\ub860 \uae30\ub2a5\uc774 \ud655\uc0b0 \ub514\ucf54\ub354\ub85c \uc804\ub2ec\ub429\ub2c8\ub2e4. ThinkDiff\ub294 \uc7ac\uad6c\uc131 \uae30\ubc18 \ubc29\ubc95\uacfc \ub2ec\ub9ac \ucd94\ub860\uc5d0 \uc911\uc810\uc744 \ub450\uc5b4 \ub2e4\uc911 \ubaa8\ub4dc \uc0c1\ud669 \uc774\ud574 \ubc0f \ucd94\ub860 \uae30\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.10458/x4.png", "caption": "Figure 3: Several diffusion models share a language encoder with encoder-decoder LLMs, allowing aligning with diffusion decoders through aligning with LLM decoders.", "description": "\uc774 \uadf8\ub9bc\uc740 \uc5ec\ub7ec \ud655\uc0b0 \ubaa8\ub378\ub4e4\uc774 \uc778\ucf54\ub354-\ub514\ucf54\ub354 \ud615\ud0dc\uc758 \uac70\ub300 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uacfc \uc5b8\uc5b4 \uc778\ucf54\ub354\ub97c \uacf5\uc720\ud568\uc73c\ub85c\uc368, LLM \ub514\ucf54\ub354\uc640\uc758 \uc815\ub82c\uc744 \ud1b5\ud574 \ud655\uc0b0 \ub514\ucf54\ub354\uc640\uc758 \uc815\ub82c\uc744 \uac04\uc18c\ud654\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, LLM \ub514\ucf54\ub354\uc640\uc758 \uc815\ub82c\uc744 \ub9e4\uac1c \ubcc0\uc218\ub85c \uc0bc\uc544 \ud655\uc0b0 \ubaa8\ub378\uc758 \ub514\ucf54\ub354\ub97c \uc9c1\uc811\uc801\uc73c\ub85c \uc870\uc815\ud558\ub294 \ub300\uc2e0,  LLM\uacfc\uc758 \uc5f0\uacb0\uace0\ub9ac\ub97c \ud1b5\ud574 \uac04\uc811\uc801\uc73c\ub85c \ud655\uc0b0 \ub514\ucf54\ub354\uc758 \uc131\ub2a5\uc744 \uac1c\uc120\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \uc774\ub294 \ubcf5\uc7a1\ud55c \ud559\uc2b5 \uacfc\uc815\uacfc \ubc29\ub300\ud55c \ub370\uc774\ud130\uc14b \uc5c6\uc774\ub3c4 \ud655\uc0b0 \ubaa8\ub378\uc5d0 \ub300\ud55c \uc774\ud574, \ucd94\ub860 \ubc0f \uad6c\uc131 \ub2a5\ub825\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud5a5\uc0c1\uc2dc\ud0a4\ub294 ThinkDiff\uc758 \ud575\uc2ec \uc544\uc774\ub514\uc5b4\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \uc124\uba85\ud569\ub2c8\ub2e4.", "section": "3. \ubc29\ubc95"}, {"figure_path": "https://arxiv.org/html/2502.10458/x5.png", "caption": "Figure 4: (a) In ThinkDiff-LVLM training, the LVLM processes an image and a text to generate text tokens and token features, with some token features randomly masked. Unmasked token features are passed to a trainable aligner network and an LLM decoder, predicting masked text tokens supervised by cross-entropy loss. In inference, the LLM decoder is replaced by a diffusion decoder, enabling in-context reasoning image generation from interleaved images and texts. (b) In ThinkDiff-CLIP training, a CLIP vision model extracts image token features which are then mapped by a trainable aligner network. A part of the image caption is encoded by the LLM encoder and concatenated with image tokens. These combined tokens are passed to the LLM decoder to predict the next part of the caption supervised by cross-entropy loss. In inference, the LLM decoder is replaced by a diffusion encoder, allowing coherent image generation based on multimodal context.", "description": " \uadf8\ub9bc 4\ub294 ThinkDiff\uc758 \ub450 \uac00\uc9c0 \ubcc0\ud615 \ubaa8\ub378\uc778 ThinkDiff-LVLM\uacfc ThinkDiff-CLIP\uc758 \ud559\uc2b5 \ubc0f \ucd94\ub860 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a) ThinkDiff-LVLM\uc740 \ub300\uaddc\ubaa8 \ube44\uc804-\uc5b8\uc5b4 \ubaa8\ub378(LVLM)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8\ub97c \ucc98\ub9ac\ud558\uace0 \ud14d\uc2a4\ud2b8 \ud1a0\ud070\uacfc \ud1a0\ud070 \ud2b9\uc9d5\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc77c\ubd80 \ud1a0\ud070 \ud2b9\uc9d5\uc740 \ubb34\uc791\uc704\ub85c \ub9c8\uc2a4\ud06c \ucc98\ub9ac\ub418\uace0, \ub9c8\uc2a4\ud06c\ub418\uc9c0 \uc54a\uc740 \ud2b9\uc9d5\uc740 \ud559\uc2b5 \uac00\ub2a5\ud55c \uc815\ub82c \ub124\ud2b8\uc6cc\ud06c\uc640 LLM \ub514\ucf54\ub354\uc5d0 \uc804\ub2ec\ub418\uc5b4 \ub9c8\uc2a4\ud06c\ub41c \ud14d\uc2a4\ud2b8 \ud1a0\ud070\uc744 \uc608\uce21\ud569\ub2c8\ub2e4. \uc774 \uacfc\uc815\uc740 \uad50\ucc28 \uc5d4\ud2b8\ub85c\ud53c \uc190\uc2e4\uc744 \ud1b5\ud574 \uac10\ub3c5\ub429\ub2c8\ub2e4. \ucd94\ub860 \uc2dc\uc5d0\ub294 LLM \ub514\ucf54\ub354\uac00 \ud655\uc0b0 \ub514\ucf54\ub354\ub85c \ub300\uccb4\ub418\uc5b4, \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8\uac00 \ud63c\ud569\ub41c \uc785\ub825\uc73c\ub85c\ubd80\ud130 \ubb38\ub9e5 \ub0b4 \ucd94\ub860 \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc774 \uac00\ub2a5\ud574\uc9d1\ub2c8\ub2e4. (b) ThinkDiff-CLIP\uc740 CLIP \ube44\uc804 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0 \ud1a0\ud070 \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud558\uace0, \ud559\uc2b5 \uac00\ub2a5\ud55c \uc815\ub82c \ub124\ud2b8\uc6cc\ud06c\ub97c \ud1b5\ud574 \ub9e4\ud551\ud569\ub2c8\ub2e4. \uc774\ubbf8\uc9c0 \ucea1\uc158\uc758 \uc77c\ubd80\ub294 LLM \uc778\ucf54\ub354\ub85c \uc778\ucf54\ub529\ub418\uc5b4 \uc774\ubbf8\uc9c0 \ud1a0\ud070\uacfc \uc5f0\uacb0\ub418\uace0, \uc774 \uacb0\ud569\ub41c \ud1a0\ud070\uc740 LLM \ub514\ucf54\ub354\uc5d0 \uc804\ub2ec\ub418\uc5b4 \ucea1\uc158\uc758 \ub2e4\uc74c \ubd80\ubd84\uc744 \uc608\uce21\ud569\ub2c8\ub2e4. \uc774 \ub610\ud55c \uad50\ucc28 \uc5d4\ud2b8\ub85c\ud53c \uc190\uc2e4\ub85c \uac10\ub3c5\ub429\ub2c8\ub2e4. \ucd94\ub860 \uc2dc\uc5d0\ub294 LLM \ub514\ucf54\ub354\uac00 \ud655\uc0b0 \uc778\ucf54\ub354\ub85c \ub300\uccb4\ub418\uc5b4 \ub2e4\uc911 \ubaa8\ub4dc \ubb38\ub9e5\uc744 \uae30\ubc18\uc73c\ub85c \uc77c\uad00\ub41c \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc774 \uac00\ub2a5\ud574\uc9d1\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.10458/x6.png", "caption": "Figure 5: 2-shot evaluation results on CoBSAT. The input structure is similar to Figure\u00a01a. Given multimodal inputs, ThinkDiff-LVLM accurately captures both implicit attributes (e.g., wicker material) and explicit attributes (e.g. car), and generates a logically correct image (wicker car). In contrast, methods such as SEED-LLaMA\u00a0(Ge et\u00a0al., 2024), Emu\u00a0(Sun et\u00a0al., 2023) and GILL\u00a0(Koh et\u00a0al., 2024) produce inaccurate and lower-quality images. The ground truth implicit attribute is highlighted in red for readers\u2019 reference. See more results in Appendix Figure\u00a09 and 10.", "description": "\uadf8\ub9bc 5\ub294 CoBSAT \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c ThinkDiff-LVLM\uc758 2-\uc0f7 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc 1a\uc640 \uc720\uc0ac\ud55c \uad6c\uc870\uc758 \ub2e4\uc911 \ubaa8\ub4dc \uc785\ub825\uc774 \uc8fc\uc5b4\uc9c0\uba74 ThinkDiff-LVLM\uc740 '\ub4f1\ub098\ubb34 \uc7ac\uc9c8'\uacfc \uac19\uc740 \uc554\uc2dc\uc801 \uc18d\uc131\uacfc '\uc790\ub3d9\ucc28'\uc640 \uac19\uc740 \uba85\uc2dc\uc801 \uc18d\uc131\uc744 \uc815\ud655\ud558\uac8c \ud3ec\ucc29\ud558\uc5ec \ub17c\ub9ac\uc801\uc73c\ub85c \uc815\ud655\ud55c \uc774\ubbf8\uc9c0(\ub4f1\ub098\ubb34 \uc790\ub3d9\ucc28)\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ubc18\uba74\uc5d0 SEED-LLaMA(Ge et al., 2024), Emu(Sun et al., 2024), GILL(Koh et al., 2024)\uacfc \uac19\uc740 \ubc29\ubc95\ub4e4\uc740 \ubd80\uc815\ud655\ud558\uace0 \ud488\uc9c8\uc774 \ub0ae\uc740 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uc815\ub2f5\uc778 \uc554\uc2dc\uc801 \uc18d\uc131\uc740 \ub3c5\uc790\uc758 \ucc38\uace0\ub97c \uc704\ud574 \ube68\uac04\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ubd80\ub85d \uadf8\ub9bc 9\uc640 10\uc5d0\uc11c \ub354 \ub9ce\uc740 \uacb0\uacfc\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.10458/x7.png", "caption": "Figure 6: Generation results for single image (I) and single image with text prompt (I + T) inputs. Our method effectively integrates semantic details of both image and text modalities to produce coherent images. FLUX excels at replicating the input image but struggles to maintain consistency with additional text prompts. See more results in Figure\u00a011.", "description": "\uadf8\ub9bc 6\uc740 \ub2e8\uc77c \uc774\ubbf8\uc9c0(I)\uc640 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\uac00 \ud3ec\ud568\ub41c \ub2e8\uc77c \uc774\ubbf8\uc9c0(I+T) \uc785\ub825\uc5d0 \ub300\ud55c \uc0dd\uc131 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubcf8 \uc5f0\uad6c\uc758 \ubc29\ubc95\uc740 \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8 \ubaa8\ub2ec\ub9ac\ud2f0 \ubaa8\ub450\uc758 \uc758\ubbf8\uc801 \uc138\ubd80 \uc815\ubcf4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ud1b5\ud569\ud558\uc5ec \uc77c\uad00\ub41c \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. FLUX\ub294 \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \ubcf5\uc81c\ud558\ub294 \ub370 \ub6f0\uc5b4\ub098\uc9c0\ub9cc \ucd94\uac00\uc801\uc778 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\uc640\uc758 \uc77c\uad00\uc131\uc744 \uc720\uc9c0\ud558\ub294 \ub370 \uc5b4\ub824\uc6c0\uc744 \uacaa\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc 11\uc5d0\uc11c \ub354 \ub9ce\uc740 \uacb0\uacfc\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.3. ThinkDiff-CLIP\uc758 \ud3c9\uac00 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.10458/x8.png", "caption": "Figure 7: Training losses (log scale) of ThinkDiff-LVLM comparing different RMSNorm designs. Disabling RMSNorm (w/o RMSNorm) or using the default RMSNorm initialization (RMSNorm w/ Default init.) results in significantly unstable training.", "description": "\uadf8\ub9bc 7\uc740 ThinkDiff-LVLM \ubaa8\ub378 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c RMSNorm(Root Mean Square Normalization)\uc758 \uc124\uacc4 \ubcc0\uacbd\uc5d0 \ub530\ub978 \uc190\uc2e4 \ud568\uc218 \ubcc0\ud654\ub97c \ub85c\uadf8 \uc2a4\ucf00\uc77c\ub85c \ube44\uad50\ud55c \uadf8\ub798\ud504\uc785\ub2c8\ub2e4.  RMSNorm\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uac70\ub098(w/o RMSNorm), \uae30\ubcf8 \ucd08\uae30\ud654\uac12\uc744 \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0(RMSNorm w/ Default init.) \ud559\uc2b5 \uacfc\uc815\uc774 \ub9e4\uc6b0 \ubd88\uc548\uc815\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubc18\uba74, \uc81c\uc548\ub41c RMSNorm \uc124\uacc4\ub97c \uc801\uc6a9\ud588\uc744 \ub54c\ub294 \uc548\uc815\uc801\uc778 \ud559\uc2b5\uc774 \uc774\ub8e8\uc5b4\uc9d0\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.10458/x9.png", "caption": "Figure 8: Results of ThinkDiff-CLIP composing two images. It creatively merge semantic details of both images. See more results in Appendix Figure\u00a012.", "description": "\uadf8\ub9bc 8\uc740 ThinkDiff-CLIP\uc774 \ub450 \uac1c\uc758 \uc774\ubbf8\uc9c0\ub97c \uacb0\ud569\ud558\ub294 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ThinkDiff-CLIP\uc740 \ub450 \uc774\ubbf8\uc9c0\uc758 \uc758\ubbf8\ub860\uc801 \uc138\ubd80 \uc0ac\ud56d\uc744 \ucc3d\uc758\uc801\uc73c\ub85c \uc735\ud569\ud558\uc5ec \uc0c8\ub85c\uc6b4 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uc608\ub97c \ub4e4\uc5b4, \ud310\ub2e4\uc640 \ubc24\ud558\ub298 \uc774\ubbf8\uc9c0\ub97c \uc785\ub825\uc73c\ub85c \ubc1b\uc73c\uba74 \ud310\ub2e4\ub294 \uadf8\ub300\ub85c \uc720\uc9c0\ub418\uc9c0\ub9cc \ubc30\uacbd\uc774 \ubc24\ud558\ub298\ub85c \ubc14\ub00c\uc5b4 \ud310\ub2e4\uac00 \ubc24\ud558\ub298\uc744 \ubc30\uacbd\uc73c\ub85c \ud558\uace0 \uc788\ub294 \uc774\ubbf8\uc9c0\uac00 \uc0dd\uc131\ub429\ub2c8\ub2e4.  \uc774\ub294 \ub2e8\uc21c\ud55c \uc774\ubbf8\uc9c0 \uacb9\uce68\uc774 \uc544\ub2c8\ub77c, \ub450 \uc774\ubbf8\uc9c0\uc758 \uc758\ubbf8\ub97c \uc774\ud574\ud558\uace0 \uc0c8\ub85c\uc6b4 \uc870\ud654\ub85c\uc6b4 \uc774\ubbf8\uc9c0\ub97c \ub9cc\ub4e4\uc5b4\ub0b4\ub294 ThinkDiff-CLIP\uc758 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubd80\ub85d \uadf8\ub9bc 12\uc5d0 \ub354 \ub9ce\uc740 \uacb0\uacfc\ub97c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.3. ThinkDiff-CLIP\uc758 \ud3c9\uac00 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.10458/x10.png", "caption": "Figure 9: More 2-shot reasoning results of ThinkDiff-LVLM on CoBSAT benchmark.", "description": "\uadf8\ub9bc 9\ub294 CoBSAT \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c ThinkDiff-LVLM\uc758 2-\uc0f7 \ucd94\ub860 \uacb0\uacfc\ub97c \ub354 \uc790\uc138\ud788 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ud589\uc740 \ud558\ub098\uc758 \ucd94\ub860 \uacfc\uc81c\ub97c \ub098\ud0c0\ub0b4\uba70, \uc67c\ucabd\uc5d0\uc11c\ubd80\ud130 \uc785\ub825 \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8, ThinkDiff-LVLM\uc774 \uc0dd\uc131\ud55c \uc774\ubbf8\uc9c0, \uadf8\ub9ac\uace0 \ub2e4\ub978 \uc138 \uac1c\uc758 \uae30\uc874 \ubaa8\ub378(SEED-LLAMA, Emu, GILL)\uc774 \uc0dd\uc131\ud55c \uc774\ubbf8\uc9c0\uac00 \uc21c\uc11c\ub300\ub85c \ubcf4\uc5ec\uc9d1\ub2c8\ub2e4.  \uac01 \uacfc\uc81c\uc5d0 \ub300\ud574 ThinkDiff-LVLM\uc774 \uc0dd\uc131\ud55c \uc774\ubbf8\uc9c0\uac00 \ub2e4\ub978 \ubaa8\ub378\ub4e4\ubcf4\ub2e4 \ub17c\ub9ac\uc801\uc73c\ub85c \uc815\ud655\ud558\uace0, \uc2dc\uac01\uc801\uc73c\ub85c \ub354 \ub192\uc740 \ud488\uc9c8\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 ThinkDiff-LVLM\uc774 \ubcf5\uc7a1\ud55c \ub2e4\uc911 \ubaa8\ub4dc \ub9e5\ub77d \ucd94\ub860\uc5d0\uc11c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2502.10458/x11.png", "caption": "Figure 10: More 2-shot reasoning results of ThinkDiff-LVLM on CoBSAT benchmark.", "description": "\uadf8\ub9bc 10\uc740 CoBSAT \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c ThinkDiff-LVLM\uc758 2-\uc0f7 \ucd94\ub860 \uacb0\uacfc\ub97c \ub354 \uc790\uc138\ud788 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ud589\uc740 \ud558\ub098\uc758 \ucd94\ub860 \uacfc\uc81c\ub97c \ub098\ud0c0\ub0b4\uba70, \uc67c\ucabd\uc5d0\uc11c\ubd80\ud130 \uc785\ub825 \uc774\ubbf8\uc9c0, \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8, ThinkDiff-LVLM\uc5d0 \uc758\ud574 \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\uac00 \uc21c\uc11c\ub300\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc720\ucd94 \ubc0f \ubcf5\ud569 \ucd94\ub860 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \uac00\uc9c0 \uc608\uc2dc\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uac01 \uc5f4\uc740 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \uc2dc\uac01\uc801 \ucd94\ub860 \uacfc\uc81c(\uc608: \ubc30\uacbd, \uc0c9\uc0c1, \uc2a4\ud0c0\uc77c, \ub3d9\uc791, \uc9c8\uac10 \ub4f1)\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ubaa8\ub378\uc740 \uc785\ub825 \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\ub97c \uae30\ubc18\uc73c\ub85c \uc77c\uad00\uc131 \uc788\uace0 \ub17c\ub9ac\uc801\uc778 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2502.10458/x12.png", "caption": "Figure 11: Generation results of a single image and a text prompt of ThinkDiff-CLIP.", "description": "\ubcf8 \uadf8\ub9bc\uc740 ThinkDiff-CLIP \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e8\uc77c \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\ub97c \uacb0\ud569\ud558\uc5ec \uc0dd\uc131\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ThinkDiff-CLIP\uc740 \uc774\ubbf8\uc9c0\uc758 \uc758\ubbf8\ub97c \uc798 \uc774\ud574\ud558\uace0 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\uc640 \uc77c\uad00\ub418\uac8c \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  FLUX Ultra \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec ThinkDiff-CLIP\uc774 \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8 \uc815\ubcf4\ub97c \ub354\uc6b1 \ud6a8\uacfc\uc801\uc73c\ub85c \ud1b5\ud569\ud558\uace0 \uc77c\uad00\uc131 \uc788\ub294 \uacb0\uacfc\ubb3c\uc744 \uc0dd\uc131\ud568\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ud589\uc740 \uac19\uc740 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud55c \ub2e4\ub978 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\uc5d0 \ub530\ub978 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ube44\uad50 \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "4.3. ThinkDiff-CLIP\uc758 \ud3c9\uac00 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.10458/x13.png", "caption": "Figure 12: Multiple input image generation results of ThinkDiff-CLIP.", "description": "\uc774 \uadf8\ub9bc\uc740 ThinkDiff-CLIP \ubaa8\ub378\uc774 \uc5ec\ub7ec \uac1c\uc758 \uc774\ubbf8\uc9c0\ub97c \uc785\ub825\uc73c\ub85c \ubc1b\uc544 \uc0dd\uc131\ud55c \uacb0\uacfc\ubb3c\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ThinkDiff-CLIP\uc740 \ub2e4\uc911 \ubaa8\ub4dc \ub9e5\ub77d \ucd94\ub860 \uae30\ub2a5\uc744 \uac16\ucd98 \ud655\uc0b0 \ubaa8\ub378\ub85c, \uc5ec\ub7ec \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8\ub97c \ub17c\ub9ac\uc801\uc73c\ub85c \uc77c\uad00\uc131 \uc788\ub294 \uc774\ubbf8\uc9c0\ub85c \uad6c\uc131\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\ub294 \ud310\ub2e4, \ubcc4\ubc24, \ud30c\ub3c4 \uadf8\ub9bc, \ubc30\ub0ad \ub4f1 \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0\ub4e4\uc774 \uc870\ud569\ub418\uc5b4 \uc0dd\uc131\ub41c \uc0c8\ub85c\uc6b4 \uc774\ubbf8\uc9c0\ub4e4\uc774 \uc5ec\ub7ec \uac1c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 ThinkDiff-CLIP\uc774 \ub2e4\uc911 \ubaa8\ub4dc \uc785\ub825\uc744 \uc774\ud574\ud558\uace0 \ub17c\ub9ac\uc801\uc73c\ub85c \ud1b5\ud569\ud558\uc5ec \uc0c8\ub85c\uc6b4 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.", "section": "4.3. ThinkDiff-CLIP \ud3c9\uac00 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.10458/x14.png", "caption": "Figure 13: Generation results for multiple images (2I) and multiple images with a text prompt (2I + T) of ThinkDiff-CLIP.", "description": "\uadf8\ub9bc 13\uc740 ThinkDiff-CLIP\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5ec\ub7ec \uc774\ubbf8\uc9c0(2I)\uc640 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\uac00 \uc788\ub294 \uc5ec\ub7ec \uc774\ubbf8\uc9c0(2I + T)\uc5d0 \ub300\ud55c \uc0dd\uc131 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 ThinkDiff-CLIP\uc774 \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8 \uc815\ubcf4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ud1b5\ud569\ud558\uc5ec \uc77c\uad00\uc131 \uc788\ub294 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ud589\uc740 \uc785\ub825 \uc774\ubbf8\uc9c0 \uc30d\uacfc \ud574\ub2f9 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8, \uadf8\ub9ac\uace0 ThinkDiff-CLIP\uc5d0 \uc758\ud574 \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ThinkDiff-CLIP\uc774 \uc774\ubbf8\uc9c0\uc758 \uc138\ubd80 \ub0b4\uc6a9\uc744 \uc815\ud655\ud558\uac8c \uc774\ud574\ud558\uace0, \uc8fc\uc5b4\uc9c4 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\ub97c \ubc14\ud0d5\uc73c\ub85c \uc0c8\ub85c\uc6b4 \uc2dc\uac01\uc801 \uc694\uc18c\ub97c \uc0dd\uc131\ud558\uba70, \uc5ec\ub7ec \uc785\ub825 \uc774\ubbf8\uc9c0 \uac04\uc758 \uc5f0\uad00\uc131\uc744 \ud30c\uc545\ud558\uace0 \uc774\ub97c \uc870\ud654\ub86d\uac8c \uacb0\ud569\ud558\ub294 \ub2a5\ub825\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.3. ThinkDiff-CLIP\uc758 \ud3c9\uac00 \uacb0\uacfc"}]