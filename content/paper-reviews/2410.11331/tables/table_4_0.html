<table id='0' style='font-size:18px'><tr><td>Features</td><td>Shakti-LLM Specification</td></tr><tr><td>Model Parameters</td><td>2.5 Billion</td></tr><tr><td>Layers</td><td>16</td></tr><tr><td>Model Dimension</td><td>4096</td></tr><tr><td>FFN Dimension</td><td>4096</td></tr><tr><td>Attention Heads</td><td>32</td></tr><tr><td>Key/Value Heads</td><td>8</td></tr><tr><td>Peak Learning Rate</td><td>3.6e-5</td></tr><tr><td>Activation Function</td><td>SwiGLU</td></tr><tr><td>Vocabulary Size</td><td>128256</td></tr><tr><td>Positional Embeddings</td><td>RoPE (0 = 500,000)</td></tr><tr><td>GPU Consumption (Raw)</td><td>9 GB</td></tr><tr><td>GPU Consumption (Quantized)</td><td>4 GB</td></tr></table>