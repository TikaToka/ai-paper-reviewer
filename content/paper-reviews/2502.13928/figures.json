[{"figure_path": "https://arxiv.org/html/2502.13928/x1.png", "caption": "Figure 1: Improvement over the base-VLM grouped by the test ability domain of benchmarks. Our S-VCO delivers the most significant overall improvement across nearly all domains, with particularly strong gains in reducing visual hallucinations. In vision-centric and general capability domains, S-VCO also achieves considerable performance boosts over the base-VLM, outperforming existing preference tuning methods including DPO and mDPO (discussed in more detail in Section\u00a05.2).", "description": "\uadf8\ub9bc 1\uc740 \ubca4\uce58\ub9c8\ud06c\uc758 \ud14c\uc2a4\ud2b8 \ub2a5\ub825 \uc601\uc5ed\ubcc4\ub85c \uae30\uc900 VLM\uc5d0 \ub300\ud55c \uac1c\uc120 \uc0ac\ud56d\uc744 \uadf8\ub8f9\ud654\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc81c\uc548\ub41c S-VCO\ub294 \uac70\uc758 \ubaa8\ub4e0 \uc601\uc5ed\uc5d0\uc11c \uac00\uc7a5 \ud070 \uc804\ubc18\uc801\uc778 \uac1c\uc120\uc744 \uc81c\uacf5\ud558\uba70, \ud2b9\ud788 \uc2dc\uac01\uc801 \ud658\uac01 \uac10\uc18c\uc5d0 \uc788\uc5b4 \uc0c1\ub2f9\ud55c \uc131\uacfc\ub97c \ubcf4\uc785\ub2c8\ub2e4. \uc2dc\uac01 \uc911\uc2ec \ubc0f \uc77c\ubc18\uc801\uc778 \uae30\ub2a5 \uc601\uc5ed\uc5d0\uc11c S-VCO\ub294 \uae30\uc900 VLM\uc744 \ub2a5\uac00\ud558\ub294 \uc0c1\ub2f9\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud558\uba70, DPO \ubc0f mDPO\ub97c \ud3ec\ud568\ud55c \uae30\uc874\uc758 \uae30\ubcf8 \uc124\uc815 \uc870\uc815 \ubc29\ubc95\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. (5.2\uc808\uc5d0\uc11c \uc790\uc138\ud788 \uc124\uba85)", "section": "5.2 \uc8fc\uc694 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.13928/x2.png", "caption": "Figure 2: Upper Part: A comparison of a VLM\u2019s perplexity (PPL) for generating a text caption when the input is an image matching the text,\nan image contradicting the text,\nor no image input at all. Intuitively, the PPL should be lowest when the image matches the text. However, the current VLM exhibits the lowest PPL without any image input and the highest PPL given the matching image. Lower Part: This counterintuitive pattern holds across 1,00010001,0001 , 000 random examples with these visual counterfactual pairs extracted from CounterCurate dataset (Section\u00a04.1).", "description": "\uc774 \uadf8\ub9bc\uc740 \ud070 \ube44\uc804-\uc5b8\uc5b4 \ubaa8\ub378(VLM)\uc774 \uc774\ubbf8\uc9c0 \ub0b4\uc6a9\uc744 \ubb34\uc2dc\ud558\uace0 \uc5b8\uc5b4 \ubaa8\ub378\uc5d0 \uacfc\ub3c4\ud558\uac8c \uc758\uc874\ud558\ub294 \uacbd\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc704\ucabd \ubd80\ubd84\uc740 \uc77c\uce58\ud558\ub294 \uc774\ubbf8\uc9c0, \uc0c1\ubc18\ub418\ub294 \uc774\ubbf8\uc9c0, \uc774\ubbf8\uc9c0 \uc5c6\uc74c\uc758 \uc138 \uac00\uc9c0 \uc870\uac74\uc5d0\uc11c \ud14d\uc2a4\ud2b8 \ucea1\uc158\uc744 \uc0dd\uc131\ud560 \ub54c VLM\uc758 \ud37c\ud50c\ub809\uc11c\ud2f0(PPL)\ub97c \ube44\uad50\ud569\ub2c8\ub2e4.  \uc9c1\uad00\uc801\uc73c\ub85c\ub294 \uc774\ubbf8\uc9c0\uac00 \ud14d\uc2a4\ud2b8\uc640 \uc77c\uce58\ud560 \ub54c PPL\uc774 \uac00\uc7a5 \ub0ae\uc544\uc57c \ud558\uc9c0\ub9cc, \ud604\uc7ac VLM\uc740 \uc774\ubbf8\uc9c0\uac00 \uc5c6\uc744 \ub54c PPL\uc774 \uac00\uc7a5 \ub0ae\uace0, \uc77c\uce58\ud558\ub294 \uc774\ubbf8\uc9c0\uac00 \uc788\uc744 \ub54c \uac00\uc7a5 \ub192\uc740 PPL\uc744 \ubcf4\uc785\ub2c8\ub2e4. \uc544\ub798\ucabd \ubd80\ubd84\uc740 CounterCurate \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ucd94\ucd9c\ud55c 1,000\uac1c\uc758 \uc2dc\uac01\uc801 \ubc18\ub840 \uc30d\uc5d0 \ub300\ud574 \uc774\ub7ec\ud55c \ubc18\uc9c1\uad00\uc801\uc778 \ud328\ud134\uc774 \uc720\uc9c0\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 VLM\uc774 \uc138\ubd80\uc801\uc778 \uc774\ubbf8\uc9c0 \uc815\ubcf4\ub97c \ubb34\uc2dc\ud558\uace0 \uc5b8\uc5b4 \ubaa8\ub378 \uc0ac\uc804\uc5d0\ub9cc \uc758\uc874\ud55c\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13928/x3.png", "caption": "Figure 3: Upper Part: MVC of visual counterfactual images [b)] in comparison to the image pair data used in prior work [a)] (Section\u00a04). MVC\u2019s image pair differs in meaningful visual details that are also grounded in the associated texts [b)], while corrupting original images with random cropping or adding noise leads to images that are not aligned with the texts directly derived from language preference data [a)]. Lower Part: S-VCO in comparison to existing VLM preference tuning paradigms DPO and visual-conditional PO (Section\u00a03). Unlike prior methods that treat visual supervision as uni-modal preferences, S-VCO considers the contrast of the image-text pair as a whole. It rewards the model for attending to matching images and rejecting contradictory ones (Section\u00a03.1), while using a symmetrical mechanism to switch the role of each image-text pair, thus avoiding shortcut learning (Section\u00a03.2).", "description": "\uadf8\ub9bc 3\uc740 \uae30\uc874 \uc5f0\uad6c\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc774\ubbf8\uc9c0 \uc30d \ub370\uc774\ud130(a)\uc640 \ube44\uad50\ud558\uc5ec \ucd5c\uc18c\ud55c\uc758 \uc2dc\uac01\uc801 \ucc28\uc774\ub97c \uac16\ub294 \uc774\ubbf8\uc9c0 \uc30d \ub370\uc774\ud130(MVC, b)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MVC \uc774\ubbf8\uc9c0 \uc30d\uc740 \uad00\ub828 \ud14d\uc2a4\ud2b8\uc5d0 \uadfc\uac70\ud558\uc5ec \uc758\ubbf8\uc788\ub294 \uc2dc\uac01\uc801 \uc138\ubd80 \uc0ac\ud56d\uc5d0\uc11c \ucc28\uc774\uac00 \uc788\uc9c0\ub9cc, \ubb34\uc791\uc704 \uc790\ub974\uae30\ub098 \ub178\uc774\uc988 \ucd94\uac00\ub97c \ud1b5\ud574 \uc6d0\ubcf8 \uc774\ubbf8\uc9c0\ub97c \uc190\uc0c1\uc2dc\ud0a4\uba74 \uc5b8\uc5b4 \uae30\ubc18 \uc120\ud638\ub3c4 \ub370\uc774\ud130\uc5d0\uc11c \uc9c1\uc811 \ud30c\uc0dd\ub41c \ud14d\uc2a4\ud2b8\uc640 \uc815\ub82c\ub418\uc9c0 \uc54a\uc740 \uc774\ubbf8\uc9c0\uac00 \uc0dd\uc131\ub429\ub2c8\ub2e4(a). \ud558\ub2e8\uc5d0\ub294 \uae30\uc874 VLM \uc120\ud638\ub3c4 \uc870\uc815 \ubc29\ubc95\uc778 DPO\uc640 \uc2dc\uac01\uc801 \uc870\uac74\ubd80 PO\uc640 \ube44\uad50\ud558\uc5ec S-VCO\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\uc874 \ubc29\ubc95\ub4e4\uc774 \uc2dc\uac01\uc801 \uc9c0\ub3c4\ub97c \ub2e8\uc77c \ubaa8\ub4dc \uc120\ud638\ub3c4\ub85c \ucde8\uae09\ud558\ub294 \uac83\uacfc \ub2ec\ub9ac, S-VCO\ub294 \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8 \uc30d\uc758 \ub300\uc870\ub97c \uc804\uccb4\uc801\uc73c\ub85c \uace0\ub824\ud569\ub2c8\ub2e4. S-VCO\ub294 \ubaa8\ub378\uc774 \uc77c\uce58\ud558\ub294 \uc774\ubbf8\uc9c0\uc5d0 \uc8fc\uc758\ub97c \uae30\uc6b8\uc774\uace0 \ubaa8\uc21c\ub418\ub294 \uc774\ubbf8\uc9c0\ub97c \uac70\ubd80\ud558\ub3c4\ub85d(3.1\uc808) \ubcf4\uc0c1\ud558\ub294 \ub3d9\uc2dc\uc5d0 \uac01 \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8 \uc30d\uc758 \uc5ed\ud560\uc744 \ubc14\uafb8\ub294 \ub300\uce6d \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc9c0\ub984\uae38 \ud559\uc2b5\uc744 \ubc29\uc9c0\ud569\ub2c8\ub2e4(3.2\uc808).", "section": "3 S-VCO: Symmetrical Visual Contrastive Optimization"}, {"figure_path": "https://arxiv.org/html/2502.13928/x4.png", "caption": "Figure 4: MVC dataset outline:\na). Types of visual counterfactuals sourced from Zhang et\u00a0al. (2024); Liu et\u00a0al. (2024c);\nb). Our vision-centric filter that keeps only image pairs whose CLIP-similarity >0.7absent0.7>0.7> 0.7 to select hard samples for current VLMs, while ensuring meaningful visual differences with DINOv2-similarity <0.5absent0.5<0.5< 0.5;\nc). Rewriting captions into conversational queries and responses without changing the explicit minimal visual contrasts.", "description": "\uadf8\ub9bc 4\ub294 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 MVC(Minimal Visual Contrasts) \ub370\uc774\ud130\uc14b\uc758 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 Zhang et al. (2024)\uacfc Liu et al. (2024c)\uc758 \uc5f0\uad6c\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc2dc\uac01\uc801 \ubc18\ub840 \uc774\ubbf8\uc9c0 \uc720\ud615\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ubbf8\uc9c0 \ubcc0\ud615\uc740 \uac1d\uccb4 \uad50\uccb4, \uc18d\uc131 \ubcc0\uacbd, \uac1c\uc218 \uc218\uc815, \uc704\uce58 \ubc18\uc804\uc758 \ub124 \uac00\uc9c0 \uc720\ud615\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. (b)\ub294 \uc81c\uc548\ub41c \uc2dc\uac01 \uc911\uc2ec \ud544\ud130\uc758 \uc791\ub3d9 \ubc29\uc2dd\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.  CLIP \uc720\uc0ac\ub3c4\uac00 0.7\ubcf4\ub2e4 \ud06c\uace0 DINOv2 \uc720\uc0ac\ub3c4\uac00 0.5\ubcf4\ub2e4 \uc791\uc740 \uc774\ubbf8\uc9c0 \uc30d\ub9cc\uc744 \uc120\ud0dd\ud558\uc5ec \uae30\uc874 VLM \ubaa8\ub378\uc5d0 \ub3c4\uc804\uc801\uc778 \uc0d8\ud50c\uc744 \uc120\ud0dd\ud558\uba74\uc11c \uc758\ubbf8 \uc788\ub294 \uc2dc\uac01\uc801 \ucc28\uc774\ub97c \uc720\uc9c0\ud569\ub2c8\ub2e4. (c)\ub294 \uc6d0\ub798 \ucea1\uc158\uc744 \ub300\ud654\ud615 \uc9c8\ubb38\uacfc \uc751\ub2f5\uc73c\ub85c \ub2e4\uc2dc \uc791\uc131\ud558\uc5ec \uba85\uc2dc\uc801\uc778 \ucd5c\uc18c \uc2dc\uac01\uc801 \ub300\uc870\ub97c \uc720\uc9c0\ud558\uba74\uc11c VLM \ubbf8\uc138 \uc870\uc815\uc5d0 \uc801\ud569\ud558\ub3c4\ub85d \ub9cc\ub4dc\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 Minimal Visual Contrasts Dataset"}, {"figure_path": "https://arxiv.org/html/2502.13928/x5.png", "caption": "Figure 5: Qualitative examples extracted from various benchmarks comparing base-VLMs (LV-INT or LV-1.5) to the results after finetuning with DPO, mDPO or S-VCO on MVC dataset. Accurate captions of visual information are highlighted.\nOur method S-VCO demonstrates superior understanding of fine-grained visual details (e.g., identifying the absence of a toothbrush) and shows strong resilience to visual hallucinations (e.g., recognizing marker-drawings, fire hydrants, slide-phones). Furthermore, S-VCO excels in more advanced visual reasoning (e.g., interpreting drive-lane conditions & regulations, estimating object sizes & distances), and captures complex scenes with greater detailedness and depth (e.g., identifying weather through the window, recognizing oncoming vehicles in low-light settings).", "description": "\uadf8\ub9bc 5\ub294 MVC \ub370\uc774\ud130\uc14b\uc5d0\uc11c DPO, mDPO \ub610\ub294 S-VCO\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubbf8\uc138 \uc870\uc815\ud55c \ud6c4 \uae30\ubcf8 VLM(LV-INT \ub610\ub294 LV-1.5)\uacfc\uc758 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ucd94\ucd9c\ud55c \uc815\uc131\uc801 \uc608\uc2dc\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc2dc\uac01 \uc815\ubcf4\uc758 \uc815\ud655\ud55c \ucea1\uc158\uc774 \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc81c\uc548\ub41c S-VCO \ubc29\ubc95\uc740 \uc12c\uc138\ud55c \uc2dc\uac01\uc801 \uc138\ubd80 \uc0ac\ud56d(\uc608: \uce6b\uc194\uc758 \ubd80\uc7ac \uc2dd\ubcc4)\uc744 \ub354 \uc798 \uc774\ud574\ud558\uace0 \uc2dc\uac01\uc801 \ud658\uac01 \ud604\uc0c1(\uc608: \ub9c8\ucee4 \uadf8\ub9bc, \uc18c\ud654\uc804, \uc2ac\ub77c\uc774\ub4dc\ud3f0 \uc778\uc2dd)\uc5d0 \ub300\ud55c \uac15\ub825\ud55c \ubcf5\uc6d0\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c S-VCO\ub294 \ub354\uc6b1 \uace0\uae09 \uc2dc\uac01\uc801 \ucd94\ub860(\uc608: \ucc28\uc120 \uc0c1\ud669 \ubc0f \uaddc\uc815 \ud574\uc11d, \ubb3c\uccb4 \ud06c\uae30 \ubc0f \uac70\ub9ac \ucd94\uc815)\uc5d0\uc11c \ub6f0\uc5b4\ub098\uba70, \ucc3d\ubb38\uc744 \ud1b5\ud574 \ub0a0\uc528\ub97c \uc2dd\ubcc4\ud558\uace0 \uc800\uc870\ub3c4 \ud658\uacbd\uc5d0\uc11c \ub2e4\uac00\uc624\ub294 \ucc28\ub7c9\uc744 \uc778\uc2dd\ud558\ub294 \ub4f1 \ub354\uc6b1 \uc0c1\uc138\ud558\uace0 \uae4a\uc774 \uc788\ub294 \ubcf5\uc7a1\ud55c \uc7a5\uba74\uc744 \ud3ec\ucc29\ud569\ub2c8\ub2e4.", "section": "5.2 \uc8fc\uc694 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.13928/x6.png", "caption": "Figure 6: Trend of improvement over the base-VLM as benchmarks become increasingly visually dependent. A metric\u2019s visual dependency is measured as the performance drop of the base-VLM when no image input is provided.\nS-VCO exhibits the most significant trend of improvements with increasing task visual dependency, highlighting how its objective design (Section\u00a03) enhances model\u2019s focus on critical visual details.\nMVC dataset also strengthens existing preference tuning methods (DPO and mDPO), while SFT (Section\u00a05.3) degrades performance on more visually demanding benchmarks.", "description": "\uadf8\ub9bc 6\uc740 \uc2dc\uac01\uc801 \uc758\uc874\ub3c4\uac00 \uc810\uc810 \ub192\uc544\uc9c0\ub294 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uae30\uc900 VLM\uc5d0 \ub300\ud55c \uc131\ub2a5 \ud5a5\uc0c1 \ucd94\uc138\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uce21\uc815\ud56d\ubaa9\uc758 \uc2dc\uac01\uc801 \uc758\uc874\ub3c4\ub294 \uc774\ubbf8\uc9c0 \uc785\ub825\uc774 \uc5c6\uc744 \ub54c \uae30\uc900 VLM\uc758 \uc131\ub2a5 \uc800\ud558\ub85c \uce21\uc815\ub429\ub2c8\ub2e4. S-VCO\ub294 \uc791\uc5c5\uc758 \uc2dc\uac01\uc801 \uc758\uc874\ub3c4\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uac00\uc7a5 \ud070 \uc131\ub2a5 \ud5a5\uc0c1 \ucd94\uc138\ub97c \ubcf4\uc5ec\uc8fc\uba70, \uc774\ub294 3\uc808\uc5d0\uc11c \uc124\uba85\ud55c S-VCO\uc758 \ubaa9\ud45c \uc124\uacc4\uac00 \ubaa8\ub378\uc758 \uc911\uc694\ud55c \uc2dc\uac01\uc801 \uc138\ubd80 \uc0ac\ud56d\uc5d0 \ub300\ud55c \uc9d1\uc911\ub3c4\ub97c \ub192\uc778\ub2e4\ub294 \uac83\uc744 \uac15\uc870\ud569\ub2c8\ub2e4. MVC \ub370\uc774\ud130\uc14b\uc740 \uae30\uc874\uc758 \uc120\ud638\ub3c4 \uc870\uc815 \ubc29\ubc95(DPO \ubc0f mDPO)\uc744 \uac15\ud654\ud558\uc9c0\ub9cc, 5.3\uc808\uc5d0\uc11c \uc124\uba85\ud55c SFT\ub294 \uc2dc\uac01\uc801 \uc694\uad6c \uc0ac\ud56d\uc774 \ub9ce\uc740 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc131\ub2a5\uc774 \uc800\ud558\ub429\ub2c8\ub2e4.", "section": "5.2 \uc8fc\uc694 \uacb0\uacfc"}]