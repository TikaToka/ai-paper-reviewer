{"references": [{"fullname_first_author": "Jonathan Ho", "paper_title": "Classifier-free diffusion guidance", "publication_date": "2021", "reason": "This paper introduces a classifier-free guidance method that improves the quality of generated images in diffusion models, which is relevant to the Show-o Turbo's text-to-image generation."}, {"fullname_first_author": "Yang Song", "paper_title": "Consistency models", "publication_date": "2023", "reason": "This paper introduces the Consistency Model (CM) family for accelerating diffusion models by mapping points on trajectories to the same endpoint, which is the basis of the Show-o Turbo's acceleration strategy."}, {"fullname_first_author": "Jinheng Xie", "paper_title": "Show-o: One single transformer to unify multimodal understanding and generation", "publication_date": "2024", "reason": "This is the paper that Show-o Turbo builds upon.  It introduces the original Show-o model, a unified multimodal model that integrates discrete diffusion modeling of image tokens and autoregressive modeling of text tokens, providing the foundation for Show-o Turbo's improvements."}, {"fullname_first_author": "Siqi Kou", "paper_title": "CLLMs: Consistency large language models", "publication_date": "2024", "reason": "This paper extends consistency distillation to large language models (LLMs), providing inspiration for applying similar strategies to multimodal models like Show-o, resulting in Show-o Turbo."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022", "reason": "This paper introduces Stable Diffusion, a widely used diffusion model for high-resolution image generation, providing crucial background for the image generation component of Show-o and its acceleration in Show-o Turbo."}]}