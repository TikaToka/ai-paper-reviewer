[{"content": "| Model | MSFT CR\u2191 | MSFT SR\u2191 | MSFT AV\u2193 | MSFT MDD\u2193 | JNJ CR\u2191 | JNJ SR\u2191 | JNJ AV\u2193 | JNJ MDD\u2193 | UVV CR\u2191 | UVV SR\u2191 | UVV AV\u2193 | UVV MDD\u2193 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Buy & Hold** | 15.340 | 1.039 | 24.980 | 9.428 | 13.895 | 1.343 | 17.500 | 9.847 | 36.583 | 2.112 | 29.299 | 15.406 |\n| *Financial Domain Models* |  |  |  |  |  |  |  |  |  |  |  |  |\n| **Palmyra-Fin-70B** | 14.697 | 0.897 | 27.518 | 9.428 | 5.748 | 0.450 | 19.317 | 9.367 | 37.875 | 2.039 | 31.200 | 15.967 |\n| *Proprietary Models* |  |  |  |  |  |  |  |  |  |  |  |  |\n| **GPT-o1-preview** | 17.184 | 0.962 | 30.000 | 9.428 | 13.561 | 1.086 | 20.864 | 9.847 | 41.508 | 2.147 | 32.479 | 9.633 |\n| **GPT-4** | 16.654 | 0.932 | 30.022 | 9.428 | 13.712 | 1.103 | 20.894 | 9.860 | 31.791 | 1.640 | 32.567 | 10.434 |\n| **GPT-4o** | 12.461 | 0.924 | 22.653 | 6.647 | 9.099 | 0.875 | 17.471 | 7.169 | 8.043 | 0.496 | 27.241 | 14.889 |\n| *Open-Source Models* |  |  |  |  |  |  |  |  |  |  |  |  |\n| **Qwen2.5-72B-Instruct** | 7.421 | 0.588 | 21.238 | 6.973 | 14.353 | 1.140 | 20.995 | 9.812 | 37.178 | 1.822 | 34.223 | 13.365 |\n| **Llama-3.1-70B-Instruct** | 17.396 | 1.335 | 21.892 | 7.045 | 13.868 | 1.121 | 20.779 | 9.825 | 35.981 | 1.728 | 34.986 | 15.406 |\n| **DeepSeek-67B-Chat** | 13.941 | 0.834 | 28.081 | 7.850 | 14.426 | 1.185 | 20.450 | 9.825 | 29.940 | 1.481 | 33.964 | 15.407 |\n| **Yi-1.5-34B-Chat** | 22.093 | 1.253 | 29.613 | 9.428 | 14.004 | 1.180 | 19.938 | 9.847 | 20.889 | 1.020 | 34.417 | 14.936 |\n| **Qwen2.5-32B-Instruct** | -0.557 | -0.041 | 22.893 | 8.946 | 2.905 | 0.292 | 16.725 | 7.169 | -1.623 | -0.097 | 27.973 | 17.986 |\n| **DeepSeek-V2-Lite (15.7B)** | 11.904 | 0.694 | 28.796 | 16.094 | -7.482 | -0.670 | 18.773 | 17.806 | 33.560 | 1.703 | 33.099 | 12.984 |\n| **Yi-1.5-9B-Chat** | 19.333 | 1.094 | 29.690 | 9.428 | 18.606 | 1.611 | 19.409 | 10.986 | 49.415 | 2.410 | 34.446 | 11.430 |\n| **Llama-3.1-8B-Instruct** | 22.703 | 1.322 | 28.855 | 7.385 | 13.988 | 1.486 | 20.460 | 9.969 | 41.108 | 1.981 | 34.866 | 16.429 |\n| **Qwen-2.5-Instruct-7B** | -10.305 | -0.724 | 23.937 | 23.371 | 21.852 | 0.980 | 37.425 | 9.573 | 11.752 | 0.853 | 22.988 | 15.451 |\n| *FLAG-TRADER* |  |  |  |  |  |  |  |  |  |  |  |  |\n| **SmolLM2-135M-Instruct** | 20.106 | 1.373 | 24.932 | 9.428 | 33.724 | 3.344 | 17.174 | 9.320 | 46.799 | 1.463 | 67.758 | 35.039 |", "caption": "Table 1: Performance of stock trading with different LLMs as backbone model across seven stocks.", "description": "\ud45c 1\uc740 7\uac00\uc9c0 \uc8fc\uc2dd\uc5d0 \ub300\ud574 \ubc31\ubcf8 \ubaa8\ub378\ub85c \uc11c\ub85c \ub2e4\ub978 LLMs\uc744 \uc0ac\uc6a9\ud55c \uc8fc\uc2dd \uac70\ub798 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 LLM\uc758 \ub204\uc801 \uc218\uc775\ub960(CR), \uc0e4\ud504 \uc9c0\uc218(SR), \uc5f0\uac04 \ubcc0\ub3d9\uc131(AV), \ucd5c\ub300 \uc190\uc2e4(MDD)\uc744 \ube44\uad50\ud558\uc5ec \uc8fc\uc2dd \uc2dc\uc7a5\uc5d0\uc11c\uc758 \uac01 \ubaa8\ub378\uc758 \uc131\uacfc\ub97c \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.  \ud45c\ub294 \ub2e4\uc591\ud55c \uaddc\ubaa8\uc640 \uc720\ud615\uc758 LLMs\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec FLAG-TRADER \ud504\ub808\uc784\uc6cc\ud06c\uc758 \ud6a8\uacfc\ub97c \uc785\uc99d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "5 \uc2e4\ud5d8"}, {"content": "| Model | HON CR\u2191 | HON SR\u2191 | HON AV\u2193 | HON MDD\u2193 | TSLA CR\u2191 | TSLA SR\u2191 | TSLA AV\u2193 | TSLA MDD\u2193 | BTC CR\u2191 | BTC SR\u2191 | BTC AV\u2193 | BTC MDD\u2193 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Buy & Hold** | 33.256 | 2.347 | 23.967 | 9.195 | 39.244 | 0.869 | 75.854 | 37.975 | 21.821 | 0.683 | 37.426 | 20.796 |\n| ***Financial Domain Models*** |  |  |  |  |  |  |  |  |  |  |  |  |\n| **Palmyra-Fin-70B** | 20.016 | 1.464 | 22.974 | 6.824 | -6.661 | -0.222 | 50.379 | 25.820 | -20.812 | -1.212 | 20.036 | 27.782 |\n| ***Proprietary Models*** |  |  |  |  |  |  |  |  |  |  |  |  |\n| **GPT-o1-preview** | 13.162 | 0.776 | 28.511 | 11.558 | 34.499 | 0.796 | 72.822 | 35.490 | 34.060 | 1.114 | 35.846 | 17.075 |\n| **GPT-4** | 34.342 | 2.005 | 28.779 | 9.195 | 45.246 | 1.190 | 63.896 | 25.031 | 22.396 | 0.828 | 31.699 | 17.206 |\n| **GPT-4o** | 38.540 | 2.418 | 26.782 | 8.979 | 45.946 | 1.348 | 57.281 | 21.631 | 14.330 | 0.532 | 31.304 | 17.278 |\n| ***Open-Source Models*** |  |  |  |  |  |  |  |  |  |  |  |  |\n| **Qwen2.5-72B-Instruct** | 34.309 | 2.000 | 28.779 | 9.292 | 39.112 | 1.075 | 61.136 | 26.985 | 0.549 | 0.325 | 1.979 | 0.897 |\n| **Llama-3.1-70B-Instruct** | 43.944 | 2.646 | 27.903 | 8.993 | 37.545 | 0.891 | 70.815 | 29.813 | 20.440 | 0.758 | 31.604 | 17.813 |\n| **DeepSeek-67B-Chat** | 32.536 | 1.909 | 28.628 | 10.782 | 35.647 | 0.885 | 67.660 | 33.359 | 28.307 | 0.891 | 37.219 | 17.944 |\n| **Yi-1.5-34B-Chat** | 30.743 | 1.823 | 28.335 | 9.195 | 35.364 | 0.808 | 73.561 | 35.490 | 13.620 | 0.434 | 36.778 | 22.790 |\n| **Qwen2.5-32B-Instruct** | 26.332 | 1.980 | 22.348 | 5.261 | 21.336 | 0.729 | 49.157 | 20.704 | 11.566 | 0.869 | 15.608 | 7.984 |\n| **DeepSeek-V2-Lite (15.7B)** | 16.686 | 0.974 | 28.771 | 16.806 | 31.458 | 0.744 | 68.524 | 35.404 | 4.804 | 0.153 | 36.846 | 20.562 |\n| **Yi-1.5-9B-Chat** | 29.028 | 1.700 | 28.682 | 12.588 | 31.350 | 0.703 | 74.895 | 37.975 | 7.953 | 0.253 | 36.799 | 26.545 |\n| **Llama-3.1-8B-Instruct** | 39.079 | 2.320 | 28.299 | 10.341 | 35.622 | 0.832 | 71.936 | 36.383 | 20.521 | 0.646 | 37.240 | 21.104 |\n| **Qwen-2.5-Instruct-7B** | 4.291 | 0.285 | 24.933 | 14.156 | 41.203 | 0.925 | 74.862 | 37.975 | 19.477 | 0.612 | 37.289 | 20.796 |\n| ***FLAG-TRADER*** |  |  |  |  |  |  |  |  |  |  |  |  |\n| **SmolLM2-135M-Instruct** | 34.342 | 2.429 | 23.913 | 10.872 | 50.394 | 1.362 | 64.004 | 37.975 | 45.511 | 1.734 | 30.903 | 24.440 |", "caption": "Table 2: Performance of stock trading with different LLMs as backbone model across seven stocks.", "description": "\ud45c 2\ub294 7\uac1c\uc758 \uc8fc\uc2dd\uc5d0 \ub300\ud574 \ubc31\ubcf8 \ubaa8\ub378\ub85c \ub2e4\uc591\ud55c LLMs\uc744 \uc0ac\uc6a9\ud55c \uc8fc\uc2dd \uac70\ub798 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 LLM\uc758 \uc131\ub2a5\uc740 \ub204\uc801 \uc218\uc775\ub960(CR), \uc0e4\ud504 \ube44\uc728(SR), \uc5f0\uac04 \ubcc0\ub3d9\uc131(AV), \ucd5c\ub300 \uc190\uc2e4(MDD)\uc758 \ub124 \uac00\uc9c0 \uc9c0\ud45c\ub85c \uce21\uc815\ub429\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uc11c\ub85c \ub2e4\ub978 LLM \uc544\ud0a4\ud14d\ucc98\uac00 \uc8fc\uc2dd \uac70\ub798 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "5 \uc2e4\ud5d8"}, {"content": "| Parameter | Default Value | Description |\n|---|---|---|\n| `total_timesteps` | 13860 | Total number of timesteps |\n| `learning_rate` | 5e-05 | Learning rate of optimizer |\n| `num_envs` | 1 | Number of parallel environments |\n| `num_steps` | 40 | Steps per policy rollout |\n| `anneal_lr` | true | Enable learning rate annealing |\n| `gamma` | 0.95 | Discount factor \u03b3 |\n| `gae_lambda` | 0.98 | Lambda for Generalized Advantage Estimation |\n| `update_epochs` | 1 | Number of update epochs per cycle |\n| `norm_adv` | true | Advantages whitening |\n| `clip_coef` | 0.2 | Surrogate clipping coefficient |\n| `clip_vloss` | true | Clipped loss for value function |\n| `ent_coef` | 0.05 | Coefficient of entropy term |\n| `vf_coef` | 0.5 | Coefficient of value function |\n| `kl_coef` | 0.05 | KL divergence with reference model |\n| `max_grad_norm` | 0.5 | Maximum gradient clipping norm |\n| `target_kl` | null | Target KL divergence threshold |\n| `dropout` | 0.0 | Dropout rate |\n| `llm` | \"SmolLM2-135M-Instruct\" | Model to fine-tune |\n| `train_dtype` | \"float16\" | Training data type |\n| `gradient_accumulation_steps` | 8 | Number of gradient accumulation steps |\n| `minibatch_size` | 32 | Mini-batch size for fine-tuning |\n| `max_episode_steps` | 65 | Maximum number of steps per episode |", "caption": "Table 3: FLAG-Trader with PPO Finetuning Hyperparameters and Settings.", "description": "\ud45c 3\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 FLAG-TRADER \ubaa8\ub378\uc758 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c PPO(Proximal Policy Optimization) \uc54c\uace0\ub9ac\uc998\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\uc640 \uc124\uc815 \uac12\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\uc758 \uc774\ub984, \uae30\ubcf8\uac12, \uadf8\ub9ac\uace0 \ud574\ub2f9 \ud30c\ub77c\ubbf8\ud130\uc758 \uc5ed\ud560\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \ucd1d \ud559\uc2b5 \ub2e8\uacc4 \uc218, \ud559\uc2b5\ub960, \ud658\uacbd \uc218, \uc815\ucc45 \uc5c5\ub370\uc774\ud2b8 \uc8fc\uae30, \ud560\uc778\uc728, \uc77c\ubc18\ud654 \uc774\uc810 \ucd94\uc815(GAE) \ud30c\ub77c\ubbf8\ud130, \uc5d4\ud2b8\ub85c\ud53c \uacc4\uc218, \uac00\uce58 \ud568\uc218 \uacc4\uc218, KL \ubc1c\uc0b0, \ucd5c\ub300 \uae30\uc6b8\uae30 \ud06c\uae30 \uc81c\ud55c, \ubaa9\ud45c KL \ubc1c\uc0b0, \ub4dc\ub86d\uc544\uc6c3 \ube44\uc728, \ubbf8\uc138 \uc870\uc815\ud560 \ubaa8\ub378 \ub4f1\uc758 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 FLAG-TRADER \ubaa8\ub378\uc758 \ud559\uc2b5 \uacfc\uc815\uacfc \uc131\ub2a5\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc8fc\uc694 \uc694\uc18c\ub4e4\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "5 Experiments"}]