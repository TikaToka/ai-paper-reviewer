[{"figure_path": "https://arxiv.org/html/2501.12375/x2.png", "caption": "Figure 1: Left: Our model can generate consistent depth predictions for long videos with rich actions. The demo video shows a 196-second (4690 frames) long take of pair skating, as sourced from\u00a0[14]. Right: Comparison to baselines in terms of accuracy (\u03b41subscript\ud835\udeff1\\delta_{1}italic_\u03b4 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT), consistency, and latency on the Nvidia A100 GPU (denoted with circle size). Consistency is defined as the maximum Temporal Alignment Error (TAE) among all models minus the TAE of each individual model. Our model achieves the best performance in all aspects.", "description": "\uadf8\ub9bc 1\uc740 \ub17c\ubb38\uc758 \uc8fc\uc694 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc740 \ubaa8\ub378\uc774 \ud48d\ubd80\ud55c \ub3d9\uc791\uc744 \ud3ec\ud568\ud558\ub294 \uae34 \ube44\ub514\uc624\uc5d0 \ub300\ud574 \uc77c\uad00\ub41c \uae4a\uc774 \uc608\uce21\uc744 \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc8fc\ub294 196\ucd08(4690 \ud504\ub808\uc784)\uc9dc\ub9ac \ud398\uc5b4 \uc2a4\ucf00\uc774\ud305 \uc601\uc0c1\uc758 \uc608\uc2dc\uc785\ub2c8\ub2e4.  \uc774 \uc601\uc0c1\uc740 [14]\uc5d0\uc11c \uac00\uc838\uc654\uc2b5\ub2c8\ub2e4. \uc624\ub978\ucabd \uadf8\ub798\ud504\ub294 \uc815\ud655\ub3c4(\u03b41), \uc77c\uad00\uc131, Nvidia A100 GPU \uc0c1\uc5d0\uc11c\uc758 \uc9c0\uc5f0 \uc2dc\uac04 \uce21\uba74\uc5d0\uc11c \uc81c\uc548\ub41c \ubaa8\ub378\uacfc \uae30\uc874 \ubaa8\ub378\ub4e4\uc758 \ube44\uad50 \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc6d0\uc758 \ud06c\uae30\ub294 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc77c\uad00\uc131\uc740 \ubaa8\ub4e0 \ubaa8\ub378\uc758 \ucd5c\ub300 \uc2dc\uac04 \uc815\ub82c \uc624\ub958(TAE)\uc5d0\uc11c \uac01 \ubaa8\ub378\uc758 TAE\ub97c \ube80 \uac12\uc73c\ub85c \uc815\uc758\ub429\ub2c8\ub2e4. \uc81c\uc548\ub41c \ubaa8\ub378\uc740 \ubaa8\ub4e0 \uce21\uba74\uc5d0\uc11c \ucd5c\uace0\uc758 \uc131\ub2a5\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.12375/x3.png", "caption": "Figure 2: Overall pipeline and the spatio-temporal head. Left: Our model is composed of a backbone encoder from Depth Anything V2 and a newly proposed spatio-temporal head. We jointly train our model on video data using ground-truth depth labels for supervision and on unlabeled images with pseudo labels generated by a teacher model. During training, only the head is learned. Right: Our spatiotemporal head inserts several temporal layers into the DPT head, while preserving the original structure of DPT head\u00a0[28].", "description": "\uadf8\ub9bc 2\ub294 \uc81c\uc548\ub41c \ubaa8\ub378\uc758 \uc804\uccb4 \ud30c\uc774\ud504\ub77c\uc778\uacfc \uacf5\uac04-\uc2dc\uac04\uc801 \ud5e4\ub4dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc740 Depth Anything V2\uc758 \ubc31\ubcf8 \uc778\ucf54\ub354\uc640 \uc0c8\ub86d\uac8c \uc81c\uc548\ub41c \uacf5\uac04-\uc2dc\uac04\uc801 \ud5e4\ub4dc\ub85c \uad6c\uc131\ub41c \ubaa8\ub378\uc758 \uc804\uccb4 \uad6c\uc870\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\uc740 \uc9c0\uc0c1 \uc9c4\uc2e4 \uc2ec\ub3c4 \ub808\uc774\ube14\uc744 \uc0ac\uc6a9\ud558\uc5ec \ube44\ub514\uc624 \ub370\uc774\ud130\ub97c \uacf5\ub3d9\uc73c\ub85c \ud559\uc2b5\ud558\uace0, \uad50\uc0ac \ubaa8\ub378\uc5d0 \uc758\ud574 \uc0dd\uc131\ub41c \uc758\uc0ac \ub808\uc774\ube14\uc744 \uc0ac\uc6a9\ud558\uc5ec \ube44\ud45c\uc9c0 \uc774\ubbf8\uc9c0\ub97c \ud559\uc2b5\ud569\ub2c8\ub2e4. \ud559\uc2b5 \uc911\uc5d0\ub294 \ud5e4\ub4dc\ub9cc \ud559\uc2b5\ub429\ub2c8\ub2e4. \uc624\ub978\ucabd\uc740 \uc6d0\ub798 DPT \ud5e4\ub4dc [28]\uc758 \uad6c\uc870\ub97c \uc720\uc9c0\ud558\uba74\uc11c \uacf5\uac04-\uc2dc\uac04\uc801 \ud5e4\ub4dc\uac00 \uc5ec\ub7ec \uc2dc\uac04\uc801 \ub808\uc774\uc5b4\ub97c DPT \ud5e4\ub4dc\uc5d0 \uc0bd\uc785\ud558\ub294 \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Video Depth Anything"}, {"figure_path": "https://arxiv.org/html/2501.12375/extracted/6147235/figures/imgs/performance_comparison.png", "caption": "Figure 3: Inference strategy for long videos. N\ud835\udc41Nitalic_N is the video clip lenght consumed by our model. Each inference video clip is built by N\u2212To\u2212Tk\ud835\udc41subscript\ud835\udc47\ud835\udc5csubscript\ud835\udc47\ud835\udc58N-T_{o}-T_{k}italic_N - italic_T start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT - italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT future frames, Tosubscript\ud835\udc47\ud835\udc5cT_{o}italic_T start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT overlapping/adjacent frames, and Tksubscript\ud835\udc47\ud835\udc58T_{k}italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT key frames. The key frames are selected by taking every \u0394ksubscript\u0394\ud835\udc58\\Delta_{k}roman_\u0394 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT-th frame going backward. Then, the new depth predictions will be scale-shift-aligned to the previous frames based on the Tksubscript\ud835\udc47\ud835\udc58T_{k}italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT overlapping frames. We use N=32,To=8,Tk=2,\u0394k=12formulae-sequence\ud835\udc4132formulae-sequencesubscript\ud835\udc47\ud835\udc5c8formulae-sequencesubscript\ud835\udc47\ud835\udc582subscript\u0394\ud835\udc5812N=32,T_{o}=8,T_{k}=2,\\Delta_{k}=12italic_N = 32 , italic_T start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = 8 , italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 2 , roman_\u0394 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 12.", "description": "\uadf8\ub9bc 3\uc740 \ucd08\uc7a5\uc2dc\uac04 \ube44\ub514\uc624\uc5d0 \ub300\ud55c \ucd94\ub860 \uc804\ub7b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\uc774 \ucc98\ub9ac\ud558\ub294 \ube44\ub514\uc624 \ud074\ub9bd\uc758 \uae38\uc774\ub97c N\uc73c\ub85c \ud45c\uc2dc\ud569\ub2c8\ub2e4. \uac01 \ucd94\ub860 \ube44\ub514\uc624 \ud074\ub9bd\uc740 N-To-Tk\uac1c\uc758 \ubbf8\ub798 \ud504\ub808\uc784, To\uac1c\uc758 \uacb9\uce58\ub294/\uc778\uc811\ud55c \ud504\ub808\uc784, Tk\uac1c\uc758 \ud0a4 \ud504\ub808\uc784\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \ud0a4 \ud504\ub808\uc784\uc740 \uc774\uc804 \ud504\ub808\uc784\uc5d0\uc11c \u0394k \uac04\uaca9\uc73c\ub85c \uc5ed\uc21c\uc73c\ub85c \uc120\ud0dd\ub429\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c \uc0c8\ub85c\uc6b4 \uae4a\uc774 \uc608\uce21\uc740 Tk\uac1c\uc758 \uacb9\uce58\ub294 \ud504\ub808\uc784\uc744 \uae30\ubc18\uc73c\ub85c \uc774\uc804 \ud504\ub808\uc784\uc5d0 \uc2a4\ucf00\uc77c-\uc26c\ud504\ud2b8 \ubc29\uc2dd\uc73c\ub85c \uc815\ub82c\ub429\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c\ub294 N=32, To=8, Tk=2, \u0394k=12\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.", "section": "3. Video Depth Anything"}, {"figure_path": "https://arxiv.org/html/2501.12375/x4.png", "caption": "Figure 4: Video depth estimation accuracy for different frame length. We compare our model (VDA-L) with DepthCrafter\u00a0[13] and DepthAnyVideo\u00a0[40] from 110 to 500 frames on Bonn\u00a0[24], Scannet\u00a0[7], and NYUv2\u00a0[22].", "description": "\uadf8\ub9bc 4\ub294 \ub2e4\uc591\ud55c \ud504\ub808\uc784 \uae38\uc774\uc5d0 \ub530\ub978 \ube44\ub514\uc624 \uae4a\uc774 \ucd94\uc815 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Bonn [24], Scannet [7], NYUv2 [22] \ub370\uc774\ud130\uc14b\uc5d0\uc11c 110\uc5d0\uc11c 500\ud504\ub808\uc784\uae4c\uc9c0, \uc81c\uc548\ub41c \ubaa8\ub378(VDA-L)\uc744 DepthCrafter [13]\uc640 DepthAnyVideo [40]\uc640 \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \ud504\ub808\uc784 \uae38\uc774 \ubcc0\ud654\uc5d0 \ub530\ub978 \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5 \ubcc0\ud654\uc640 \uc0c1\ub300\uc801 \uc131\ub2a5 \ucc28\uc774\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50\ud558\uace0, \uc81c\uc548 \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uacfc \uc815\ud655\uc131\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2501.12375/x5.png", "caption": "Figure 5: Qualitative comparison for real-world long video depth estimation. We compare our model with DAv2-L\u00a0[42] and DepthCrafter\u00a0[13] on 500-frame videos from Scannet\u00a0[7] and Bonn\u00a0[24].", "description": "\uadf8\ub9bc 5\ub294 \uc2e4\uc81c \ud658\uacbd\uc758 \uae34 \ube44\ub514\uc624\uc5d0 \ub300\ud55c \uae4a\uc774 \ucd94\uc815 \uacb0\uacfc\ub97c \uc815\uc131\uc801\uc73c\ub85c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4.  \ubcf8 \ub17c\ubb38\uc758 \ubaa8\ub378\uacfc DAv2-L [42], DepthCrafter [13] \uc138 \uac00\uc9c0 \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c Scannet [7]\uacfc Bonn [24] \ub370\uc774\ud130\uc14b\uc758 500\ud504\ub808\uc784 \ube44\ub514\uc624\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uae4a\uc774 \ucd94\uc815 \uacb0\uacfc\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\uc5b4 \uc131\ub2a5 \ucc28\uc774\ub97c \uba85\ud655\ud558\uac8c \ub4dc\ub7ec\ub0c5\ub2c8\ub2e4.  \ud2b9\ud788, \ubcf8 \ub17c\ubb38\uc758 \ubaa8\ub378\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \ub354\uc6b1 \uc77c\uad00\ub418\uace0 \uc815\ud655\ud55c \uae4a\uc774 \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2. Zero-shot Depth Estimation"}, {"figure_path": "https://arxiv.org/html/2501.12375/x6.png", "caption": "Figure 6: Qualitative comparison for in-the-wild short video depth estimation. We compare with Depth-Anything-V2\u00a0[42], DepthCrafter\u00a0[13] and DepthAnyVideo\u00a0[40] on videos with less than 100 frames from DAVIS\u00a0[26]. Red boxes show incorrect depth estimation while blue boxes show inconsistent depth estimation.", "description": "\uadf8\ub9bc 6\uc740 \ub2e4\uc591\ud55c \ud658\uacbd\uc5d0\uc11c \ucd2c\uc601\ub41c \uc9e7\uc740 \ube44\ub514\uc624\uc5d0 \ub300\ud55c \uc2ec\ub3c4 \ucd94\uc815 \uacb0\uacfc\ub97c \uc815\uc131\uc801\uc73c\ub85c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. Depth-Anything-V2, DepthCrafter, DepthAnyVideo \uc138 \uac00\uc9c0 \ubc29\ubc95\uacfc \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \uacb0\uacfc\ub97c DAVIS \ub370\uc774\ud130\uc14b\uc758 100\ud504\ub808\uc784 \ubbf8\ub9cc \ube44\ub514\uc624\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\uad50\ud558\uc600\uc2b5\ub2c8\ub2e4. \ube68\uac04\uc0c9 \ubc15\uc2a4\ub294 \uc798\ubabb\ub41c \uc2ec\ub3c4 \ucd94\uc815\uc744, \ud30c\ub780\uc0c9 \ubc15\uc2a4\ub294 \uc77c\uad00\uc131 \uc5c6\ub294 \uc2ec\ub3c4 \ucd94\uc815\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc81c\uc548\ub41c \ubc29\ubc95\uc740 \uae30\uc874 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \ub354 \uc815\ud655\ud558\uace0 \uc77c\uad00\ub41c \uc2ec\ub3c4 \ucd94\uc815 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Video Depth Anything"}, {"figure_path": "https://arxiv.org/html/2501.12375/x7.png", "caption": "Figure 7: Qualitative comparisons of different inference strategies. We compare overlap alignment (OA) with our proposed overlap interpolation and key-frame referencing (OI + KR) on a self-captured video with 7320 frames.", "description": "\uadf8\ub9bc 7\uc740 \uc11c\ub85c \ub2e4\ub978 \ucd94\ub860 \uc804\ub7b5\uc758 \uc815\uc131\uc801 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 7320\ud504\ub808\uc784\uc758 \uc790\uccb4 \ucd2c\uc601 \ube44\ub514\uc624\uc5d0\uc11c \uc81c\uc548\ub41c \uc911\ucca9 \ubcf4\uac04 \ubc0f \ud0a4\ud504\ub808\uc784 \ucc38\uc870(OI+KR) \ubc29\ubc95\uacfc \uc911\ucca9 \uc815\ub82c(OA) \ubc29\ubc95\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. OA\ub294 \uc774\uc804\uc5d0 \uc608\uce21\ub41c \uae4a\uc774 \uc815\ubcf4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud604\uc7ac \uc138\uadf8\uba3c\ud2b8\uc758 \uae4a\uc774\ub97c \ubcf4\uc815\ud558\ub294 \ubc18\uba74, OI+KR\uc740 \uc774\uc804 \uc138\uadf8\uba3c\ud2b8\uc758 \ud0a4\ud504\ub808\uc784\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubd80\ub4dc\ub7ec\uc6b4 \uc804\ud658\uc744 \ubcf4\uc7a5\ud569\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 OI+KR\uc774 \uc7a5\uc2dc\uac04 \ube44\ub514\uc624\uc5d0\uc11c \ub354\uc6b1 \uc77c\uad00\ub418\uace0 \ubd80\ub4dc\ub7ec\uc6b4 \uae4a\uc774 \uc608\uce21\uc744 \uc81c\uacf5\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.3 \uc288\ud37c \ub871 \uc2dc\ud000\uc2a4\ub97c \uc704\ud55c \ucd94\ub860 \uc804\ub7b5"}, {"figure_path": "https://arxiv.org/html/2501.12375/x8.png", "caption": "Figure 8: Qualitative comparison for static image depth estimation. We compare our model with Depth-Anything-V2\u00a0[42], DepthCrafter\u00a0[13], and Depth Any Video\u00a0[40] on static image depth estimation. Our model demonstrates visualization results comparable to those of Depth-Anything-V2\u00a0[42].", "description": "\uadf8\ub9bc 8\uc740 \uc815\uc9c0 \uc601\uc0c1\uc758 \uae4a\uc774 \ucd94\uc815\uc5d0 \ub300\ud55c \uc815\uc131\uc801 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubcf8 \ub17c\ubb38\uc758 \ubaa8\ub378\uc744 Depth-Anything-V2 [42], DepthCrafter [13], Depth Any Video [40]\uc640 \ube44\uad50\ud558\uc5ec \uc815\uc9c0 \uc601\uc0c1\uc5d0 \ub300\ud55c \uae4a\uc774 \ucd94\uc815 \uacb0\uacfc\ub97c \uc2dc\uac01\ud654\ud588\uc2b5\ub2c8\ub2e4.  \ube44\uad50 \uacb0\uacfc, \ubcf8 \ub17c\ubb38\uc758 \ubaa8\ub378\uc740 Depth-Anything-V2 [42]\uc640 \ube44\uc2b7\ud55c \uc218\uc900\uc758 \uc2dc\uac01\uc801 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uc720\ud615\uc758 \uc774\ubbf8\uc9c0\ub4e4\uc5d0 \ub300\ud55c \uae4a\uc774 \ucd94\uc815 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \uc608\uc2dc\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ube44\uad50\ud558\uc5ec, \ubcf8 \ub17c\ubb38\uc758 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\ub2e4 \uba85\ud655\ud558\uac8c \uc774\ud574\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2501.12375/x9.png", "caption": "Figure 9: Qualitative comparison for real-world long video depth estimation. We compare with Depth-Anything-V2\u00a0[42] and DepthCrafter\u00a0[13] on 500-frames videos from Scannet\u00a0[7] and Bonn\u00a0[24] . We show changes in color and depth over time at the vertical red line in videos. White boxes show inconsistent estimation. Blue boxes show our algorithm has higher accuracy.", "description": "\uadf8\ub9bc 9\ub294 \uc2e4\uc81c \ud658\uacbd\uc758 \uae34 \ube44\ub514\uc624\uc5d0 \ub300\ud55c \uae4a\uc774 \ucd94\uc815 \uacb0\uacfc\ub97c \uc815\uc131\uc801\uc73c\ub85c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. Depth-Anything-V2 [42]\uc640 DepthCrafter [13]\uc758 \uacb0\uacfc\uc640 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c Scannet [7] \ubc0f Bonn [24] \ub370\uc774\ud130\uc14b\uc758 500\ud504\ub808\uc784 \ube44\ub514\uc624\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\uad50\ud569\ub2c8\ub2e4. \ube44\ub514\uc624\uc758 \uc218\uc9c1 \ube68\uac04\uc0c9 \uc120\uc744 \ub530\ub77c \uc2dc\uac04\uc5d0 \ub530\ub978 \uc0c9\uc0c1\uacfc \uae4a\uc774\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud770\uc0c9 \uc0c1\uc790\ub294 \uc77c\uad00\uc131 \uc5c6\ub294 \ucd94\uc815 \uacb0\uacfc\ub97c, \ud30c\ub780\uc0c9 \uc0c1\uc790\ub294 \uc81c\uc548\ub41c \uc54c\uace0\ub9ac\uc998\uc758 \uc815\ud655\ub3c4\uac00 \ub354 \ub192\uc740 \ubd80\ubd84\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ubcf8 \ub17c\ubb38\uc758 \ubaa8\ub378\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\ubcf4\ub2e4 \uc2dc\uac04\uc801 \uc77c\uad00\uc131\uc774 \ub354 \ub192\uace0 \uc815\ud655\ub3c4\uac00 \ub354 \ub192\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2. Zero-shot Depth Estimation"}, {"figure_path": "https://arxiv.org/html/2501.12375/x10.png", "caption": "Figure 10: Temporal layer. The feature shape is adjusted for temporal attention.", "description": "\uadf8\ub9bc 10\uc740 Video Depth Anything \ubaa8\ub378\uc758 \uacf5\uac04-\uc2dc\uac04\uc801 \ud5e4\ub4dc(STH) \ub0b4\ubd80 \uad6c\uc870\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \uc2dc\uac04\uc801 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uc801\uc6a9\ud558\uae30 \uc704\ud574 \ud2b9\uc9d5 \ub9f5\uc758 \ud615\ud0dc\ub97c \uc870\uc815\ud558\ub294 \uacfc\uc815\uc744 \uc0c1\uc138\ud788 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc785\ub825 \ud2b9\uc9d5 \ub9f5\uc740 \uc2dc\uac04\uc801 \uc5b4\ud150\uc158 \uacc4\uc0b0\uc744 \uc704\ud574 (B x Hf x Wf) x N x C \ud615\ud0dc\ub85c \ubcc0\ud658\ub418\uace0, \uac01 \uc2dc\uac04\uc801 \uc5b4\ud150\uc158 \ub808\uc774\uc5b4\ub97c \uac70\uce5c \ud6c4\uc5d0\ub294 (B x N) x C x Hf x Wf \ud615\ud0dc\ub85c \ub2e4\uc2dc \ubcc0\ud658\ub429\ub2c8\ub2e4. \uc5ec\uae30\uc11c B\ub294 \ubc30\uce58 \ud06c\uae30, N\uc740 \ud504\ub808\uc784 \uc218, Hf\uc640 Wf\ub294 \ud2b9\uc9d5 \ub9f5\uc758 \ub192\uc774\uc640 \ub108\ube44, C\ub294 \ucc44\ub110 \uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacfc\uc815\uc744 \ud1b5\ud574 \ubaa8\ub378\uc740 \ube44\ub514\uc624 \ud504\ub808\uc784 \uac04\uc758 \uc2dc\uac04\uc801 \uad00\uacc4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ud559\uc2b5\ud558\uace0, \uc77c\uad00\uc131 \uc788\ub294 \uae4a\uc774 \uc608\uce21\uc744 \uc218\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.1 Architecture"}, {"figure_path": "https://arxiv.org/html/2501.12375/x11.png", "caption": "Figure 11: 3D Video Conversion. A video from the DAVIS dataset\u00a0[26] is transformed into a 3D video using our model.", "description": "\uc774 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc758 3D \ube44\ub514\uc624 \ubcc0\ud658 \uc139\uc158\uc5d0 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, DAVIS \ub370\uc774\ud130\uc14b [26]\uc758 \ube44\ub514\uc624\ub97c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec 3D \ube44\ub514\uc624\ub85c \ubcc0\ud658\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac04\ub2e8\ud788 \ub9d0\ud574, 2\ucc28\uc6d0 \ube44\ub514\uc624\ub97c \uc785\ub825\ubc1b\uc544 \uae4a\uc774 \uc815\ubcf4\ub97c \ucd94\uc815\ud558\uc5ec 3\ucc28\uc6d0 \ube44\ub514\uc624\ub85c \ubcc0\ud658\ud558\ub294 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378\uc774 2D \ube44\ub514\uc624\uc5d0 \uae4a\uc774 \uc815\ubcf4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ucd94\uac00\ud558\uc5ec 3D \ud6a8\uacfc\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Video Depth Anything"}]