{"references": [{"fullname_first_author": "Reiner Birkl", "paper_title": "MiDaS v3: a model zoo for robust monocular relative depth estimation", "publication_date": "2023-07-14", "reason": "This paper introduces MiDaS v3, a significant advancement in monocular depth estimation that serves as a foundation for the current work."}, {"fullname_first_author": "Lihe Yang", "paper_title": "Depth anything: Unleashing the power of large-scale unlabeled data", "publication_date": "2024-00-00", "reason": "This paper introduces Depth Anything, a foundational model for depth estimation that the current work builds upon and improves."}, {"fullname_first_author": "Lihe Yang", "paper_title": "Depth anything v2", "publication_date": "2024-00-00", "reason": "This paper presents Depth Anything V2, which is directly used as the encoder in the proposed model, representing a crucial component of the architecture."}, {"fullname_first_author": "Wenbo Hu", "paper_title": "DepthCrafter: Generating consistent long depth sequences for open-world videos", "publication_date": "2024-09-24", "reason": "This paper proposes DepthCrafter, a state-of-the-art method for video depth estimation that is compared against the proposed method, highlighting its significance in the field."}, {"fullname_first_author": "Jiahao Shao", "paper_title": "Learning temporally consistent video depth from video diffusion priors", "publication_date": "2024-06-24", "reason": "This paper introduces ChronoDepth, another state-of-the-art method for video depth estimation that is compared against in the evaluation, showcasing its importance as a benchmark."}]}