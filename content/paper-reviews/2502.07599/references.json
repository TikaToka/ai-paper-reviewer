{"references": [{"fullname_first_author": "Christiano, P. F.", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-00-00", "reason": "It is a foundational paper in RLHF, a method closely related to DPO and the focus of this paper."}, {"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "It is another highly influential paper on RLHF that provides context and background for DPO."}, {"fullname_first_author": "Rafailov, R.", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-00-00", "reason": "This paper introduces DPO, the method this paper aims to improve, making it the most central reference."}, {"fullname_first_author": "Razin, N.", "paper_title": "Unintentional unalignment: Likelihood displacement in direct preference optimization", "publication_date": "2024-00-00", "reason": "It identifies and analyzes the \"likelihood displacement\" problem that this paper addresses, providing critical context."}, {"fullname_first_author": "Bradley, R. A.", "paper_title": "Rank analysis of incomplete block designs: I. the method of paired comparisons", "publication_date": "1952-00-00", "reason": "It introduces the Bradley-Terry model which is used in DPO and is theoretically analyzed in this paper."}]}