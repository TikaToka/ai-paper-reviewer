[{"content": "| $f(\\lambda)$ strategy | Llama 3-8B | Qwen 2-7B |\n|---|---|---|\n| **SFT** | 5.64 | 5.88 |\n| **DPO** | 6.513 | 6.875 |\n| `fixed` 0.5 | 6.118 | 6.150 |\n| `fixed` 0.55 | 6.269 | 6.369 |\n| `fixed` 0.6 | 6.169 | 6.331 |\n| `fixed` 0.65 | 6.314 | 6.494 |\n| `fixed` 0.7 | 6.500 | 6.581 |\n| `fixed` 0.75 | 6.444 | 6.700 |\n| `fixed` 0.8 | **6.731** | 6.869 |\n| `fixed` 0.85 | 6.644 | 6.775 |\n| `fixed` 0.9 | **6.738** | 6.725 |\n| `fixed` 0.95 | 6.444 | 6.875 |\n| `increase_linear` 0.75 | **6.588** | 6.775 |\n| `increase_linear` 0.85 | 6.425 | 6.806 |\n| `increase_linear` 0.95 | 6.519 | **7.044** |\n| `decrease_linear` 0.75 | **6.613** | 6.742 |\n| `decrease_linear` 0.85 | 6.481 | **6.906** |\n| `decrease_linear` 0.95 | **6.606** | **6.944** |", "caption": "Table 1: MT-Bench (Zheng et\u00a0al., 2023) results for Llama 3-8B and Qwen 2-7B trained on UltraFeedback, where GPT-4o is the judge model. The results better than DPO is bolded. Results for fixed, linear_increase, and linear_decrease are included.", "description": "\ud45c 1\uc740 UltraFeedback \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\ub41c Llama 3-8B \ubc0f Qwen 2-7B \ubaa8\ub378\uc5d0 \ub300\ud55c MT-Bench(Zheng et al., 2023) \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  GPT-4\uac00 \ud3c9\uac00 \ubaa8\ub378\ub85c \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uae30\uc900 DPO \ubc29\ubc95\uacfc \ube44\uad50\ud558\uc5ec DPO-Shift\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc138 \uac00\uc9c0 \uc804\ub7b5(fixed, linear_increase, linear_decrease)\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. DPO\ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc88b\uc740 \uacb0\uacfc\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.2 Downstream Performance"}]