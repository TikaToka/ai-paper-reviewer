{"references": [{"fullname_first_author": "Aida Amini", "paper_title": "MathQA: Towards interpretable math word problem solving with operation-based formalisms", "publication_date": "2019-05-13", "reason": "This paper introduces a benchmark dataset, MathQA, crucial for evaluating mathematical reasoning capabilities of LLMs, directly relevant to the paper's focus on long-context understanding and complex reasoning tasks."}, {"fullname_first_author": "Peter Clark", "paper_title": "Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge", "publication_date": "2018-03-05", "reason": "The ARC challenge dataset is a key benchmark used in the paper for evaluating reasoning abilities, providing a standard for comparison of different LLMs' performance in complex reasoning."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-14", "reason": "This work explores training LLMs to verify solutions to math problems, a relevant approach to the paper's investigation of enhancing LLM reasoning capabilities through a chain-of-clarifications approach."}, {"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention-2: Faster attention with better parallelism and work partitioning", "publication_date": "2023-07-08", "reason": "FlashAttention-2 is a crucial optimization technique used in the paper to handle the long context lengths efficiently during inference, a key component of the proposed AgenticLU framework."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The LLaMA 3 herd of models", "publication_date": "2024-07-21", "reason": "The LLaMA 3 models are the foundation upon which the AgenticLU framework is built, serving as the base LLMs whose long-context understanding capabilities are enhanced; this paper is thus foundational to the experimental setup."}]}