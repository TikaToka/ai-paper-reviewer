[{"content": "| Data | # |\n|---|---| \n| **Num of Traces** | 107,550 |\n| **Avg Context Length** | 67,812 |\n| **Avg Chosen Response Length** | 165 |\n| **Avg Rejected Response Length** | 164 |\n| **Total Generation Tokens** | 17M |", "caption": "Table 1: Statistics of the generated traces dataset used in finetuning derived from NarrativeQA. We left out 11.9K traces for validation.", "description": "\uc774 \ud45c\ub294 NarrativeQA \ub370\uc774\ud130\uc14b\uc744 \uae30\ubc18\uc73c\ub85c \uc0dd\uc131\ub41c \ucd94\uc801(trace) \ub370\uc774\ud130\uc14b\uc758 \ud1b5\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  AgenticLU \ubaa8\ub378\uc758 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30, \ubb38\ub9e5 \uae38\uc774, \uc751\ub2f5 \uae38\uc774 \ub4f1\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.  \ucd1d 107,550\uac1c\uc758 \ucd94\uc801\uc774 \uc0dd\uc131\ub418\uc5c8\uace0, \uac80\uc99d\uc744 \uc704\ud574 11,900\uac1c\uc758 \ucd94\uc801\uc740 \uc81c\uc678\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud3c9\uade0 \ubb38\ub9e5 \uae38\uc774\ub294 67,812 \ud1a0\ud070, \ud3c9\uade0 \uc120\ud0dd \uc751\ub2f5 \uae38\uc774\ub294 165 \ud1a0\ud070, \ud3c9\uade0 \uae30\uac01 \uc751\ub2f5 \uae38\uc774\ub294 164 \ud1a0\ud070\uc785\ub2c8\ub2e4.  \ucd1d \ud1a0\ud070 \uc218\ub294 17M\uc5d0 \ub2ec\ud569\ub2c8\ub2e4.", "section": "5 Data Generation & Model Training"}, {"content": "| Model | Short Avg | HotpotQA | Natural Questions | TriviaQA | PopQA | NarrativeQA | InfiniQA | InfiniChoice | Long Avg |\n|---|---|---|---|---|---|---|---|---|---|\n| Llama3.1-8B | 62.3 | 40.0 | 56.1 | 80.6 | 56.1 | 38.0 | 48.0 | 55.0 | 53.4 |\n| AgenticLU (<math alttext=\"\u03b4\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.3.1.1.1.m1.1\"><semantics id=\"S6.T2.3.1.1.1.m1.1a\"><mi id=\"S6.T2.3.1.1.1.m1.1.1\" xref=\"S6.T2.3.1.1.1.m1.1.1.cmml\">\u03b4</mi><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.3.1.1.1.m1.1b\"><ci id=\"S6.T2.3.1.1.1.m1.1.1.cmml\" xref=\"S6.T2.3.1.1.1.m1.1.1\">\ud835\udeff</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.3.1.1.1.m1.1c\">\u03b4</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T2.3.1.1.1.m1.1d\">italic_\u03b4</annotation></semantics></math>) | -0.6 | +31.1 | +21.7 | +7.7 | +9.4 | +18.0 | +2.0 | +13.0 | +14.7 |", "caption": "Table 2: Performance difference of AgenticLU and its base, Llama3.1-8B-Instruct (\u03b4=\ud835\udeffabsent\\delta=italic_\u03b4 =AgenticLU-8B minus Llama3.1-8B), on long context (the 128K tasks) and short-context benchmarks (6 regular tasks including ARC, GSM8K, and MMLU), the details of the short-context performance can be found in\u00a0appendix\u00a0B. Scores represent accuracy, with AgenticLU demonstrating significantly improved performance across long-context tasks with minimal effect on regular task performance.", "description": "\ud45c 2\ub294 AgenticLU\uc640 \uae30\ubcf8 \ubaa8\ub378\uc778 Llama3.1-8B-Instruct\uc758 \uc131\ub2a5 \ucc28\uc774(AgenticLU-8B\uc5d0\uc11c Llama3.1-8B\ub97c \ube80 \uac12)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae34 \ubb38\ub9e5(128K \ud1a0\ud070 \uc791\uc5c5)\uacfc \uc9e7\uc740 \ubb38\ub9e5 \ubca4\uce58\ub9c8\ud06c(ARC, GSM8K, MMLU\ub97c \ud3ec\ud568\ud55c 6\uac00\uc9c0 \uc77c\ubc18 \uc791\uc5c5) \ubaa8\ub450\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ud3ec\ud568\ud558\uba70, \uc9e7\uc740 \ubb38\ub9e5 \uc131\ub2a5\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ubd80\ub85d B\uc5d0 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4. \uc810\uc218\ub294 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0b4\uba70, AgenticLU\ub294 \uae34 \ubb38\ub9e5 \uc791\uc5c5\uc5d0\uc11c \uc131\ub2a5\uc774 \ud06c\uac8c \ud5a5\uc0c1\ub418\uc5c8\uace0 \uc77c\ubc18 \uc791\uc5c5\uc5d0\ub294 \uac70\uc758 \uc601\ud5a5\uc744 \ubbf8\uce58\uc9c0 \uc54a\uc558\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6 \ud3c9\uac00"}, {"content": "| Model | HotpotQA | NaturalQ | PopQA | TriviaQA | Avg |\n|---|---|---|---|---|---| \n| Llama-3.1-8B | 40.0 | 56.1 | 56.1 | 80.6 | 58.2 |\n| AgenticLU-8B | 71.1 | 77.8 | 65.5 | 88.3 | 75.7 |\n| (w/ 2 rounds) | 71.1 | 76.7 | 67.2 | 91.7 | 76.7 |\n| (w/ 3 rounds) | 75.5 | 78.8 | 68.3 | 91.1 | 78.4 |", "caption": "Table 3: We evaluate the performance of adding additional self-clarification and contextual grounding rounds at inference time. The gain from self-clarification is close to optimal at the initial round.", "description": "\uc774 \ud45c\ub294 \ucd94\ub860 \uc2dc \ucd94\uac00\uc801\uc778 \uc790\uae30 \uc124\uba85 \ubc0f \ubb38\ub9e5\uc801 \uadfc\uac70 \ucd94\uac00 \ub77c\uc6b4\ub4dc\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc790\uae30 \uc124\uba85\uc758 \uc774\uc810\uc740 \uccab \ubc88\uc9f8 \ub77c\uc6b4\ub4dc\uc5d0\uc11c \ucd5c\uc801\uc5d0 \uac00\uae5d\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "7 Analyses & Ablation Studies"}, {"content": "| Model | HotpotQA | NaturalQ | PopQA | TriviaQA | Avg |\n|---|---|---|---|---|---| \n| Llama-3.1-8B | 40.0 | 56.1 | 56.1 | 80.6 | 58.2 |\n| AgenticLU-8B | 71.1 | 77.8 | 65.5 | 88.3 | 75.7 |\n| (w/o Clarification) | 57.8 | 56.7 | 55.5 | 78.3 | 62.1 |\n| (w/o Pointback) | 53.3 | 59.4 | 52.7 | 83.3 | 62.2 |", "caption": "Table 4: We test the agentic workflow with AgenticLU-8B when taking out the self-clarification steps and the contextual grounding (pointback) step. The tasks are with 128K context length.", "description": "\uc774 \ud45c\ub294 AgenticLU-8B \ubaa8\ub378\uc5d0\uc11c \uc790\uae30 \uc124\uba85 \ub2e8\uacc4\uc640 \ubb38\ub9e5\uc801 \uadfc\uac70 \ub2e8\uacc4\ub97c \uc81c\uac70\ud588\uc744 \ub54c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  128K \uae38\uc774\uc758 \ubb38\ub9e5\uc744 \uc0ac\uc6a9\ud558\ub294 \ub124 \uac00\uc9c0 \uacfc\uc81c(HotpotQA, Natural Questions, PopQA, TriviaQA)\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uacfc\uc81c\uc5d0 \ub300\ud574 AgenticLU-8B\uc758 \uae30\ubcf8 \uc131\ub2a5\uacfc \uc790\uae30 \uc124\uba85 \uc81c\uac70, \ubb38\ub9e5\uc801 \uadfc\uac70 \uc81c\uac70 \uc2dc\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uac01 \ub2e8\uacc4\uc758 \uc911\uc694\uc131\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uc790\uae30 \uc124\uba85\uacfc \ubb38\ub9e5\uc801 \uadfc\uac70\uac00 \uc7a5\ubb38 \ub9e5\ub77d \uc774\ud574\uc5d0 \uc5bc\ub9c8\ub098 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud558\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "7 Analyses & Ablation Studies"}, {"content": "| Metric | Baseline | AgenticLU |\n|---|---|---|\n| Runtime Overhead | 100% | 101.93% |\n| Avg Tokens Generated in One Round | 76.28 | 1205.38 |", "caption": "Table 5: Performance Overhead Comparison between direct answering baseline and AgenticLU.", "description": "\ud45c 5\ub294 \uae30\uc874\uc758 \uc9c1\uc811 \ub2f5\ubcc0 \ubc29\uc2dd(Baseline)\uacfc \uc81c\uc548\ud558\ub294 AgenticLU \ubaa8\ub378\uc758 \uc131\ub2a5 \uc624\ubc84\ud5e4\ub4dc\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  \uc9c1\uc811 \ub2f5\ubcc0 \ubc29\uc2dd\uc740 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub2f5\ubcc0\uc744 \ubc14\ub85c \uc0dd\uc131\ud558\ub294 \ubc29\uc2dd\uc774\uace0, AgenticLU\ub294 \uc790\uae30 \uc9c8\ubb38\uacfc \ub2f5\ubcc0\uc744 \ud1b5\ud574 \ucd94\ub860 \uacfc\uc815\uc744 \uac70\uce58\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.  \ub530\ub77c\uc11c,  \uc2e4\ud589 \uc2dc\uac04 \uc624\ubc84\ud5e4\ub4dc(Runtime Overhead) \uc640 \uac01 \ub77c\uc6b4\ub4dc\uc5d0\uc11c \uc0dd\uc131\ub418\ub294 \ud1a0\ud070 \uc218(Avg Tokens Generated in One Round)\ub97c \ube44\uad50\ud558\uc5ec AgenticLU \ubc29\uc2dd\uc758 \ud6a8\uc728\uc131\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "6.4 Performance on Short-Context Tasks"}, {"content": "| Hyperparameter | Value |\n|---|---| \n| Learning Rate | 5e-7 |\n| Learning Rate Schedule | Cosine Annealing |\n| Optimizer | Adam |\n| \\(\\beta_{1}\\) | 0.9 |\n| \\(\\beta_{2}\\) | 0.95 |\n| Training dtype | bf16 |\n| Batch Size | 128 |\n| Max Length | 131,072 |", "caption": "Table 6: Hyperparameters for SFT.", "description": "\ud45c 6\ub294 \ubcf8 \ub17c\ubb38\uc758 5\uc7a5(Data Generation & Model Training)\uc5d0\uc11c \uc124\uba85\ud558\ub294 Self-Taught Agentic Long-Context Understanding(AgenticLU) \ubaa8\ub378 \ud559\uc2b5\uc744 \uc704\ud55c \ucd08 \ub9e4\uac1c\ubcc0\uc218(Hyperparameter)\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Supervised Fine-Tuning(SFT) \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ud559\uc2b5\ub960(Learning Rate), \ucd5c\uc801\ud654\uae30(Optimizer), \ubc30\uce58 \ud06c\uae30(Batch Size), \ucd5c\ub300 \uae38\uc774(Max Length) \ub4f1\uc758 \uc138\ubd80 \uc124\uc815 \uac12\ub4e4\uc744 \uba85\uc2dc\uc801\uc73c\ub85c \uc81c\uc2dc\ud558\uc5ec, AgenticLU \ubaa8\ub378\uc758 \ud559\uc2b5 \uacfc\uc815\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "5 Data Generation & Model Training"}, {"content": "| Hyperparameter | Value |\n|---|---| \n| Learning Rate | 5e-7 |\n| Learning Rate Schedule | Cosine Annealing |\n| Optimizer | Adam |\n| \u03b2\u2081 | 0.9 |\n| \u03b2\u2082 | 0.95 |\n| Training dtype | bf16 |\n| Batch Size | 128 |\n| \u03b2 | 0.1 |\n| Max Length | 131,072 |", "caption": "Table 7: Hyperparameters for DPO.", "description": "\ud45c 7\uc740 Direct Preference Optimization(DPO)\uc5d0 \uc0ac\uc6a9\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud559\uc2b5\ub960, \ucd5c\uc801\ud654\uae30, \ubc30\uce58 \ud06c\uae30, \ucd5c\ub300 \uae38\uc774 \ub4f1 DPO \ubaa8\ub378 \ud559\uc2b5\uc5d0 \uc601\ud5a5\uc744 \uc8fc\ub294 \uc8fc\uc694 \uc124\uc815\uac12\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 DPO \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uad6c\uccb4\uc801\uc778 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815\uc744 \ud1b5\ud574 AgenticLU \ubaa8\ub378\uc758 \uc131\ub2a5 \uac1c\uc120\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ub3d5\uc2b5\ub2c8\ub2e4.", "section": "5.2 CoC \uacbd\ub85c \uc99d\ub958"}, {"content": "| Model | ARC Easy | ARC Challenge | GSM8k | MathQA | MMLU | MMLU Pro | Avg |\n|---|---|---|---|---|---|---|---| \n| Llama3.1-8B | 84.80 | 59.64 | 80.13 | 42.88 | 68.72 | 37.71 | 62.31 |\n| AgenticLU-8B | 83.96 | 58.36 | 80.51 | 41.74 | 68.38 | 37.51 | 61.74 |", "caption": "Table 8: Performance comparison of AgenticLU and Llama3.1-8B-Instruct on short-context benchmarks. Scores represent accuracy percentages, with AgenticLU demonstrating matching results across tasks.", "description": "\ud45c 8\uc740 AgenticLU\uc640 Llama3.1-8B-Instruct \ubaa8\ub378\uc758 \ub2e8\uae30 \ubb38\ub9e5 \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4.  AgenticLU\ub294 \ub2e4\uc591\ud55c \ub2e8\uae30 \ubb38\ub9e5 \uc791\uc5c5\uc5d0\uc11c Llama3.1-8B-Instruct \ubaa8\ub378\uacfc \uac70\uc758 \ub3d9\uc77c\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 ARC Easy, ARC Challenge, GSM8K, MathQA, MMLU, MMLU Pro \ub4f1 \uc5ec\ub7ec \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4 \ube44\uc728(%)\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  AgenticLU\uac00 \uc7a5\uae30 \ubb38\ub9e5 \uc791\uc5c5\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd98 \ubaa8\ub378\uc784\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0, \ub2e8\uae30 \ubb38\ub9e5 \uc791\uc5c5\uc5d0\uc11c\ub3c4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \uc720\uc9c0\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6.4 Performance on Short-Context Tasks"}, {"content": "|               | 8K   | 16K   | 32K   | 64K   | 128K  |\n|---------------|------|-------|-------|-------|-------|\n| **HotpotQA** |       |       |       |       |       |\n| Llama3.1-8B   | 63.3 | 56.7  | 61.1  | 47.8  | 40.0  |\n| Llama3.1-8B+step-by-step | 60.0 | 66.7  | 56.7  | 58.9  | 56.7  |\n| Llama3.1-8B+plan&solve | 71.1 | 66.7  | 72.2  | 62.2  | 50.0  |\n| Llama3.1-8B+fact&reflect | 58.9 | 58.9  | 62.2  | 61.1  | 48.9  |\n| ProLong-8B    | 62.2 | 65.6  | 57.8  | 53.3  | 58.9  |\n| Llama3.1-8B+LongRAG | 61.1 | 58.9  | 73.3  | 56.7  | 57.8  |\n| AgenticLU-8B  | 81.1 | 75.6  | 78.9  | 75.6  | 71.1  |", "caption": "Table 9: Long-context performance on HotpotQA.", "description": "\ud45c 9\ub294 HotpotQA \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774(8K, 16K, 32K, 64K, 128K \ud1a0\ud070)\uc5d0 \ub530\ub978 \uc5ec\ub7ec \uc7a5\uae30 \ucee8\ud14d\uc2a4\ud2b8 \uc774\ud574 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Llama 3.1-8B \uae30\ubcf8 \ubaa8\ub378\uacfc  \uc5ec\ub7ec \ud504\ub86c\ud504\ud305 \uae30\ubc95(\ub2e8\uacc4\ubcc4, \uacc4\ud68d \ubc0f \ud574\uacb0, \uc0ac\uc2e4 \ubc0f \ubc18\uc131)\uc744 \uc0ac\uc6a9\ud55c \ubaa8\ub378,  \ucd5c\ucca8\ub2e8 ProLong-8B \ubaa8\ub378, \uadf8\ub9ac\uace0 \uc81c\uc548\ub41c AgenticLU-8B \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec AgenticLU-8B \ubaa8\ub378\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6.1 Tasks and Metrics"}, {"content": "| Model | 8K | 16K | 32K | 64K | 128K |\n|---|---|---|---|---|---| \n| Llama3.1-8B | 71.7 | 69.4 | 70.6 | 73.9 | 56.1 |\n| Llama3.1-8B+step-by-step | 66.7 | 66.1 | 58.9 | 55.6 | 38.9 |\n| Llama3.1-8B+plan&solve | 67.8 | 71.7 | 66.7 | 62.2 | 50.6 |\n| Llama3.1-8B+fact&reflect | 63.3 | 63.3 | 61.7 | 59.4 | 40.0 |\n| ProLong-8B | 83.3 | 82.2 | 83.9 | 90.0 | 77.8 |\n| Llama3.1-8B+LongRAG | 65.6 | 76.1 | 79.4 | 77.2 | 73.9 |\n| AgenticLU-8B | 91.7 | 91.1 | 85.0 | 85.0 | 77.8 |", "caption": "Table 10: Long-context performance on Nature Questions.", "description": "\ud45c 10\uc740 Nature Questions \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774(8K, 16K, 32K, 64K, 128K \ud1a0\ud070)\uc5d0\uc11c \uc5ec\ub7ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Llama3.1-8B \uae30\ubcf8 \ubaa8\ub378\uacfc \ud568\uaed8, Chain-of-Thought, Plan-and-Solve, Fact-and-Reflect \ud504\ub86c\ud504\ud305 \uae30\ubc95\uc744 \uc801\uc6a9\ud55c \ubaa8\ub378\uacfc LongRAG, ProLong-8B \ubc0f AgenticLU-8B \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uae34 \ucee8\ud14d\uc2a4\ud2b8\ub97c \uc774\ud574\ud558\ub294 \ubaa8\ub378\uc758 \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \ud2b9\ud788, \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc815\ud655\ub3c4 \ubcc0\ud654\ub97c \uc0b4\ud3b4\ubcf4\uace0,  AgenticLU-8B \ubaa8\ub378\uc758 \ud6a8\uacfc\ub97c \ub2e4\ub978 \ubaa8\ub378\uacfc \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "6.1 Tasks and Metrics"}, {"content": "| Model | 8K | 16K | 32K | 64K | 128K |\n|---|---|---|---|---|---| \n| Llama3.1-8B | 82.8 | 86.7 | 85.6 | 81.1 | 80.6 |\n| Llama3.1-8B+step-by-step | 84.4 | 86.1 | 90.0 | 82.2 | 57.2 |\n| Llama3.1-8B+plan&solve | 78.9 | 88.3 | 89.4 | 87.2 | 86.7 |\n| Llama3.1-8B+fact&reflect | 87.8 | 83.9 | 84.4 | 86.7 | 84.4 |\n| ProLong-8B | 71.1 | 88.3 | 78.9 | 82.8 | 78.3 |\n| Llama3.1-8B+LongRAG | 77.2 | 79.4 | 83.9 | 83.9 | 83.3 |\n| AgenticLU-8B | 88.3 | 92.2 | 91.1 | 93.3 | 88.3 |", "caption": "Table 11: Long-context performance on TriviaQA.", "description": "\ud45c 11\uc740 TriviaQA \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud55c \uc7a5\ubb38 \ub9e5\ub77d \uc774\ud574 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\uc591\ud55c \ubaa8\ub378 (Llama3.1-8B, Llama3.1-8B\uc5d0 \uc5ec\ub7ec \ud504\ub86c\ud504\ud2b8 \uae30\ubc95 \uc801\uc6a9, ProLong-8B, AgenticLU-8B)\uc758 8K, 16K, 32K, 64K, 128K \ud1a0\ud070 \uae38\uc774\uc5d0 \ub530\ub978 \uc815\ud655\ub3c4\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uac01 \ubaa8\ub378\uc758 \uc7a5\ubb38 \ub9e5\ub77d \ucc98\ub9ac \ub2a5\ub825\uacfc \uadf8 \ud55c\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, AgenticLU-8B \ubaa8\ub378\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 128K \ud1a0\ud070 \uae38\uc774\uc5d0\uc11c\ub3c4 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "6.1 Tasks and Metrics"}, {"content": "| Model | 8K | 16K | 32K | 64K | 128K |\n|---|---|---|---|---|---| \n| Llama3.1-8B | 61.1 | 62.8 | 57.2 | 58.3 | 56.1 |\n| Llama3.1-8B+step-by-step | 61.7 | 58.9 | 55.0 | 58.9 | 60.6 |\n| Llama3.1-8B+plan&solve | 62.2 | 63.3 | 58.9 | 55.0 | 61.1 |\n| Llama3.1-8B+fact&reflect | 65.0 | 64.4 | 58.9 | 53.3 | 65.0 |\n| ProLong-8B | 67.8 | 68.3 | 70.0 | 64.4 | 65.6 |\n| Llama3.1-8B+LongRAG | 47.8 | 54.4 | 54.4 | 57.2 | 50.6 |\n| AgenticLU-8B | 82.2 | 82.2 | 78.3 | 76.7 | 65.6 |", "caption": "Table 12: Long-context performance on PopQA.", "description": "\ud45c 12\ub294 PopQA \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud55c \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uc7a5\ubb38 \ub9e5\ub77d \uc774\ud574 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 8K, 16K, 32K, 64K, 128K \ud1a0\ud070 \uae38\uc774\uc758 \ub9e5\ub77d\uc744 \uc0ac\uc6a9\ud55c \uc2e4\ud5d8 \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, Llama3.1-8B \uae30\ubcf8 \ubaa8\ub378\uacfc \uc5ec\ub7ec \ud504\ub86c\ud504\ud2b8 \uae30\ubc95(\ub2e8\uacc4\ubcc4, \uacc4\ud68d \ubc0f \ud574\uacb0, \uc0ac\uc2e4 \ubc0f \ubc18\uc131, LongRAG) \ubc0f \ucd5c\ucca8\ub2e8 ProLong-8B \ubaa8\ub378\uacfc\uc758 \ube44\uad50 \ubd84\uc11d\uc744 \ud1b5\ud574 AgenticLU-8B \ubaa8\ub378\uc758 \uc131\ub2a5 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ub9e5\ub77d \uae38\uc774\uc5d0 \ub530\ub77c \ube44\uad50\ud558\uc5ec \uc7a5\ubb38 \ub9e5\ub77d \uc774\ud574\uc5d0 \ub300\ud55c \ud6a8\uacfc\ub97c \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "6.1 Tasks and Metrics"}, {"content": "| Model | 8K | 16K | 32K | 64K | 128K |\n|---|---|---|---|---|---| \n| Llama3.1-8B | 15.0 | 19.0 | 27.0 | 35.0 | 38.0 |\n| Llama3.1-8B+step-by-step | 23.0 | 30.0 | 36.0 | 51.0 | 43.0 |\n| Llama3.1-8B+plan&solve | 22.0 | 25.0 | 38.0 | 41.0 | 39.0 |\n| Llama3.1-8B+fact&reflect | 18.0 | 35.0 | 37.0 | 42.0 | 46.0 |\n| ProLong-8B | 18.0 | 27.0 | 28.0 | 38.0 | 42.0 |\n| Llama3.1-8B+LongRAG | 23.3 | 23.3 | 50.0 | 50.0 | 46.7 |\n| AgenticLU-8B | 27.0 | 35.0 | 41.0 | 49.0 | 56.0 |", "caption": "Table 13: Long-context performance on NarrativeQA.", "description": "\ud45c 13\uc740 NarrativeQA \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774(8K, 16K, 32K, 64K, 128K \ud1a0\ud070)\uc5d0\uc11c \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uc7a5\ubb38 \uc774\ud574 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  AgenticLU \ubaa8\ub378\uc744 \ud3ec\ud568\ud558\uc5ec Llama3.1-8B \uae30\ubcf8 \ubaa8\ub378\uacfc \uc5ec\ub7ec \ud504\ub86c\ud504\ud2b8 \ubc29\uc2dd(\ub2e8\uacc4\ubcc4, \uacc4\ud68d \ubc0f \ud574\uacb0, \uc0ac\uc2e4 \ubc0f \ubc18\uc131, LongRAG) \ubc0f \ucd5c\ucca8\ub2e8 ProLong-8B \ubaa8\ub378\uacfc \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uac01 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc7a5\ubb38 \ucee8\ud14d\uc2a4\ud2b8 \uc774\ud574 \uacfc\uc81c\uc5d0\uc11c AgenticLU\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "6.1 Tasks and Metrics"}, {"content": "| Model | 8K | 16K | 32K | 64K | 128K |\n|---|---|---|---|---|---| \n| Llama3.1-8B | 17.0 | 31.0 | 36.0 | 40.0 | 48.0 |\n| Llama3.1-8B+step-by-step | 21.0 | 36.0 | 36.0 | 45.0 | 43.0 |\n| Llama3.1-8B+plan&solve | 17.0 | 26.0 | 32.0 | 41.0 | 40.0 |\n| Llama3.1-8B+fact&reflect | 19.0 | 30.0 | 40.0 | 42.0 | 37.0 |\n| ProLong-8B | 16.0 | 31.0 | 29.0 | 31.0 | 45.0 |\n| Llama3.1-8B+LongRAG | 16.7 | 23.3 | 36.7 | 43.3 | 36.7 |\n| AgenticLU-8B | 25.0 | 39.0 | 42.0 | 47.0 | 50.0 |", "caption": "Table 14: Long-context performance on InfbenchQA.", "description": "\ud45c 14\ub294 \ub17c\ubb38\uc758 6.1\uc808 \"\uc791\uc5c5 \ubc0f \uce21\uc815\ud56d\ubaa9\" \uc5d0\uc11c \ub2e4\ub8e8\ub294 \uae34 \ubb38\ub9e5 \uc774\ud574 \ub2a5\ub825 \ud3c9\uac00\ub97c \uc704\ud55c \ubca4\uce58\ub9c8\ud06c\uc778 InfbenchQA\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378(Llama3.1-8B, Llama3.1-8B\ub97c \uae30\ubc18\uc73c\ub85c \ud55c \uc5ec\ub7ec \ud504\ub86c\ud504\ud305 \uae30\ubc95 \uc801\uc6a9 \ubaa8\ub378, ProLong-8B, AgenticLU-8B)\uc758 InfbenchQA \uc815\ud655\ub3c4\ub97c \ubb38\ub9e5 \uae38\uc774(8K, 16K, 32K, 64K, 128K \ud1a0\ud070)\ubcc4\ub85c \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  AgenticLU-8B \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \uc0c1\ub2f9\ud788 \uc6b0\uc218\ud568\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.", "section": "6 \ud3c9\uac00"}, {"content": "| Model | 8K | 16K | 32K | 64K | 128K |\n|---|---|---|---|---|---| \n| Llama3.1-8B | 9.0 | 12.0 | 24.0 | 39.0 | 55.0 |\n| Llama3.1-8B+step-by-step | 15.0 | 13.0 | 41.0 | 41.0 | 44.0 |\n| Llama3.1-8B+plan&solve | 27.0 | 15.0 | 48.0 | 55.0 | 58.0 |\n| Llama3.1-8B+fact&reflect | 20.0 | 14.0 | 38.0 | 51.0 | 56.0 |\n| ProLong-8B | 22.0 | 27.0 | 37.0 | 48.0 | 58.0 |\n| Llama3.1-8B+LongRAG | 16.7 | 30.0 | 43.3 | 53.3 | 63.3 |\n| AgenticLU-8B | 45.0 | 46.0 | 47.0 | 64.0 | 68.0 |", "caption": "Table 15: Long-context performance on InfbenchChoice.", "description": "\ud45c 15\ub294 InfiniteBench Multiple-Choice \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774(8K, 16K, 32K, 64K, 128K \ud1a0\ud070)\uc5d0 \ub530\ub978 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  AgenticLU \ubaa8\ub378\uc744 \ud3ec\ud568\ud558\uc5ec \uc5ec\ub7ec \uae30\uc900 \ubaa8\ub378(Llama3.1-8B, Llama3.1-8B+step-by-step, Llama3.1-8B+plan&solve, Llama3.1-8B+fact&reflect, ProLong-8B, Llama3.1-8B+LongRAG)\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uae34 \ucee8\ud14d\uc2a4\ud2b8 \uc774\ud574 \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ud1b5\ud574 \uae34 \ucee8\ud14d\uc2a4\ud2b8 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub2f5\ubcc0 \uc815\ud655\ub3c4\uc640 \ud6a8\uc728\uc131\uc744 \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "6.1 Tasks and Metrics"}]