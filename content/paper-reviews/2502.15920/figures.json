[{"figure_path": "https://arxiv.org/html/2502.15920/x1.png", "caption": "Figure 1: Overview of the AgenticLU pipeline: The model iteratively refines its understanding of long-context inputs through an agentic workflow. At each step, it raises self-clarifications, retrieves relevant context via the pointback mechanism, and updates its reasoning trace. The framework integrates CoC Path Construction to generate diverse reasoning paths, followed by two-stage fine-tuning (SFT and DPO) to enhance long-context understanding.", "description": "\uadf8\ub9bc 1\uc740 AgenticLU \ud30c\uc774\ud504\ub77c\uc778\uc758 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  AgenticLU\ub294 \uc5d0\uc774\uc804\ud2b8 \uae30\ubc18 \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub97c \ud1b5\ud574 \ubaa8\ub378\uc774 \uc7a5\ubb38 \ucee8\ud14d\uc2a4\ud2b8 \uc785\ub825\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ubc18\ubcf5\uc801\uc73c\ub85c \uac1c\uc120\ud558\ub294 \ud504\ub808\uc784\uc6cc\ud06c\uc785\ub2c8\ub2e4. \uac01 \ub2e8\uacc4\uc5d0\uc11c \ubaa8\ub378\uc740 \uc790\uccb4\uc801\uc73c\ub85c \uc9c8\ubb38\uc744 \uba85\ud655\ud788 \ud558\uace0, \ud3ec\uc778\ud2b8\ubc31 \uba54\ucee4\ub2c8\uc998\uc744 \ud1b5\ud574 \uad00\ub828 \ucee8\ud14d\uc2a4\ud2b8\ub97c \uac80\uc0c9\ud558\uba70, \ucd94\ub860 \uacfc\uc815\uc744 \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4. \uc774 \ud504\ub808\uc784\uc6cc\ud06c\ub294 \ub2e4\uc591\ud55c \ucd94\ub860 \uacbd\ub85c\ub97c \uc0dd\uc131\ud558\uae30 \uc704\ud574 CoC \uacbd\ub85c \uad6c\uc131\uc744 \ud1b5\ud569\ud558\uace0, \uc7a5\ubb38 \ucee8\ud14d\uc2a4\ud2b8 \uc774\ud574\ub3c4\ub97c \ub192\uc774\uae30 \uc704\ud574 2\ub2e8\uacc4 \ubbf8\uc138 \uc870\uc815(SFT \ubc0f DPO)\uc744 \uac70\uce69\ub2c8\ub2e4.  \uc989, \ubaa8\ub378\uc740 \uc790\uccb4\uc801\uc73c\ub85c \uc9c8\ubb38\uc744 \uc0dd\uc131\ud558\uace0 \ub2f5\ubcc0\ud558\uba70, \uad00\ub828 \uc815\ubcf4\ub97c \ucc3e\uc544 \uc774\ud574\ub3c4\ub97c \ub192\uc5ec\ub098\uac00\ub294 \uacfc\uc815\uc744 \uac70\uce58\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  CoC(Chain-of-Clarifications) \uacbd\ub85c \uc0dd\uc131\uc744 \ud1b5\ud574 \uc5ec\ub7ec \uac00\uc9c0 \ucd94\ub860 \uacbd\ub85c\ub97c \ub9cc\ub4e4\uace0, SFT(Supervised Fine-Tuning)\uc640 DPO(Direct Preference Optimization)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud568\uc73c\ub85c\uc368 \uc7a5\ubb38 \ucee8\ud14d\uc2a4\ud2b8 \uc774\ud574 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.15920/x2.png", "caption": "Figure 2: Effective context size is smaller than nominal context size.\nPerformance of Llama3.1-8B-Instruct (advertised 128K-token context) on the HotPotQA dataset\ndrops sharply as input length increases (8K, 16K, 32K, 64K, 128K), illustrating the\ngap between nominal and effective context capacities.", "description": "\uadf8\ub9bc 2\ub294 \ubaa8\ub378\uc758 \uba85\ubaa9\uc0c1\uc758 context \uae38\uc774\uc640 \uc2e4\uc81c\ub85c \ud6a8\uacfc\uc801\uc73c\ub85c \ud65c\uc6a9 \uac00\ub2a5\ud55c context \uae38\uc774 \uac04\uc758 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Llama3.1-8B-Instruct \ubaa8\ub378\uc740 \ucd5c\ub300 128K \ud1a0\ud070\uc758 context\ub97c \ucc98\ub9ac\ud560 \uc218 \uc788\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc9c0\ub9cc, HotPotQA \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud55c \uc2e4\ud5d8 \uacb0\uacfc, \uc785\ub825 \uae38\uc774\uac00 8K, 16K, 32K, 64K, 128K\ub85c \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc131\ub2a5\uc774 \uae09\uaca9\ud788 \uc800\ud558\ub418\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc774 \uba85\ubaa9\uc0c1\uc73c\ub85c \uc9c0\uc6d0\ud558\ub294 context \uae38\uc774\ubcf4\ub2e4 \ud6e8\uc52c \uc9e7\uc740 \uae38\uc774\uc758 context\ub9cc\uc744 \uc2e4\uc81c\ub85c \ud6a8\uacfc\uc801\uc73c\ub85c \ucc98\ub9ac\ud55c\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ubaa8\ub378\uc758 \uba85\ubaa9\uc0c1 context \uc6a9\ub7c9\uacfc \uc2e4\uc81c \ud6a8\uacfc\uc801\uc778 context \uc6a9\ub7c9 \uac04\uc758 \ucc28\uc774\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3 The Context Size Gap"}, {"figure_path": "https://arxiv.org/html/2502.15920/x3.png", "caption": "Figure 3: Main results on 7 long-context tasks across context lengths from 8K to 128K. Our AgenticLU-8B (dotted orange) achieves significant improvements on all tasks over our base model Llama3.1-8B (solid orange). We also compare with the prompting methods (Step-by-Step, Plan-and-Solve, Fact-and-Reflect, LongRAG) and the state-of-the-art ProLong-8B model. AgenticLU-8B consistently maintains strong performance across most tasks and context lengths.", "description": "\uadf8\ub9bc 3\uc740 8KB\uc5d0\uc11c 128KB\uae4c\uc9c0 \ub2e4\uc591\ud55c \ubb38\ub9e5 \uae38\uc774\uc5d0 \uac78\uccd0 7\uac00\uc9c0 \uc7a5\ubb38\ub9e5 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc8fc\uc694 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc810\uc120 \uc8fc\ud669\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub41c AgenticLU-8B \ubaa8\ub378\uc740 \uae30\ubcf8 \ubaa8\ub378\uc778 Llama3.1-8B (\uc2e4\uc120 \uc8fc\ud669\uc0c9) \ubaa8\ub378\uc5d0 \ube44\ud574 \ubaa8\ub4e0 \uc791\uc5c5\uc5d0\uc11c \uc131\ub2a5\uc774 \ud06c\uac8c \ud5a5\uc0c1\ub418\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c, \ub2e8\uacc4\ubcc4, \uacc4\ud68d \ubc0f \ud574\uacb0, \uc0ac\uc2e4 \ubc0f \ubc18\uc131, LongRAG \ub4f1\uc758 \ud504\ub86c\ud504\ud305 \ubc29\ubc95\uacfc \ucd5c\ucca8\ub2e8 ProLong-8B \ubaa8\ub378\uacfc \ube44\uad50 \ubd84\uc11d\ud558\uc5ec AgenticLU-8B\uac00 \ub300\ubd80\ubd84\uc758 \uc791\uc5c5\uacfc \ubb38\ub9e5 \uae38\uc774\uc5d0\uc11c \uc77c\uad00\ub418\uac8c \uac15\ub825\ud55c \uc131\ub2a5\uc744 \uc720\uc9c0\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6 Evaluation"}]