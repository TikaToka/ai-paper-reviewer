{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-08", "reason": "This paper is a technical report describing GPT-4, a highly influential large language model which is frequently used as a benchmark in this research."}, {"fullname_first_author": "Stanislaw Antol", "paper_title": "VQA: Visual Question Answering", "publication_date": "2015-00-00", "reason": "This paper introduced the VQA dataset, a seminal benchmark that has greatly influenced the field of visual question answering and multimodal large language models."}, {"fullname_first_author": "Drew A Hudson", "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "publication_date": "2019-00-00", "reason": "This paper introduced the GQA dataset, another influential benchmark for visual reasoning that has been used to evaluate various MLLMs and serves as a precursor to more complex benchmarks."}, {"fullname_first_author": "Jeffrey P Bigham", "paper_title": "VizWiz: Nearly real-time answers to visual questions", "publication_date": "2010-00-00", "reason": "This paper introduced the VizWiz dataset, an important early benchmark for visual question answering that focuses on real-world scenarios, contributing to advancements in this field."}, {"fullname_first_author": "Yuan Liu", "paper_title": "MMBench: Is your multi-modal model an all-around player?", "publication_date": "2025-00-00", "reason": "This paper introduced MMBench, a widely adopted benchmark for evaluating the performance of MLLMs across various dimensions, and is central to the research in this paper."}]}