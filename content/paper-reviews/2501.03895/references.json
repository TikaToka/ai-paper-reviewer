{"references": [{"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A versatile vision-language model for understanding, localization, text reading, and beyond", "publication_date": "2023-08-12", "reason": "This paper introduces Qwen-VL, a strong competitor model which is used as a baseline for comparison in this paper's experiments."}, {"fullname_first_author": "Daniel Bolya", "paper_title": "Token merging: Your ViT but faster", "publication_date": "2023-10-09", "reason": "This paper proposes an efficient token merging method which is compared with the proposed method in this paper."}, {"fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing GPT-4 with 90% ChatGPT quality", "publication_date": "2023-03-30", "reason": "This paper introduces the Vicuna model, which is used as the LLM backbone in this paper's proposed model."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-00-00", "reason": "This paper introduces the vision transformer model, which is a core component of many LLMs and is used as a vision encoder in this paper's proposed model."}, {"fullname_first_author": "Haotian Liu", "paper_title": "LLaVA: A large language and vision assistant", "publication_date": "2023-00-00", "reason": "This paper introduces LLaVA, the model that the proposed model in this paper improves upon."}]}