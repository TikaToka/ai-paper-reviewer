[{"figure_path": "https://arxiv.org/html/2502.04296/x1.png", "caption": "Figure 1: \nAction-Video Dynamics Model from Heterogeneous Robot Interactions. HMA utilizes heterogeneous datasets comprising over 3 million trajectories (videos) from 40 distinct embodiments to pre-train a full dynamics model with next-set-of-token predictions using masked autoregression. After pre-training, the resulting action-video dynamics model is versatile, supporting applications such as video simulation, policy evaluation, synthetic data generation, and direct adoption as an imitation policy.", "description": "\uadf8\ub9bc 1\uc740 \uc774\uc885 \ub85c\ubd07 \uc0c1\ud638\uc791\uc6a9\uc73c\ub85c\ubd80\ud130 \uc5bb\uc5b4\uc9c4 \ub3d9\uc791-\ube44\ub514\uc624 \uc5ed\ud559 \ubaa8\ub378\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. HMA\ub294 40\uac00\uc9c0\uc758 \uc11c\ub85c \ub2e4\ub978 \uad6c\ud604(embodiments)\uc73c\ub85c\ubd80\ud130 3\ubc31\ub9cc \uac1c\uac00 \ub118\ub294 \uada4\uc801(\ube44\ub514\uc624)\uc744 \ud3ec\ud568\ud558\ub294 \uc774\uc885 \ub370\uc774\ud130\uc14b\uc744 \ud65c\uc6a9\ud558\uc5ec \ub9c8\uc2a4\ud06c \uc790\uae30\ud68c\uadc0(masked autoregression)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc74c \ud1a0\ud070 \uc608\uce21\uc744 \ud1b5\ud574 \uc644\uc804\ud55c \uc5ed\ud559 \ubaa8\ub378\uc744 \uc0ac\uc804 \ud6c8\ub828\ud569\ub2c8\ub2e4. \uc0ac\uc804 \ud6c8\ub828 \ud6c4 \uc0dd\uc131\ub41c \ub3d9\uc791-\ube44\ub514\uc624 \uc5ed\ud559 \ubaa8\ub378\uc740 \ub2e4\uc591\ud55c \uc6a9\ub3c4\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, \ube44\ub514\uc624 \uc2dc\ubbac\ub808\uc774\uc158, \uc815\ucc45 \ud3c9\uac00, \ud569\uc131 \ub370\uc774\ud130 \uc0dd\uc131 \ubc0f \ubaa8\ubc29 \uc815\ucc45\uc73c\ub85c\uc758 \uc9c1\uc811 \ucc44\ud0dd \ub4f1\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04296/x2.png", "caption": "Figure 2: Dynamics Model. Masked autoregression in the dynamics model generalizes multiple problem settings including policy learning, forward and passive dynamics, and full dynamics.", "description": "\uadf8\ub9bc 2\ub294 \uc5ed\ub3d9\uc801\uc778 \ubaa8\ub378\uc5d0\uc11c \ub9c8\uc2a4\ud06c \uc790\uae30\ud68c\uadc0\uc758 \uac1c\ub150\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ubaa8\ub378\uc740 \uc815\ucc45 \ud559\uc2b5, \uc21c\ubc29\ud5a5 \ubc0f \uc218\ub3d9 \uc5ed\ud559, \uadf8\ub9ac\uace0 \uc804\uccb4 \uc5ed\ud559\uc744 \ud3ec\ud568\ud55c \ub2e4\uc591\ud55c \ubb38\uc81c \uc124\uc815\uc744 \uc77c\ubc18\ud654\ud569\ub2c8\ub2e4.  \uc989, \ub9c8\uc2a4\ud06c \uc790\uae30\ud68c\uadc0\ub294 \uacfc\uac70 \uad00\ucc30\uacfc \ud589\ub3d9\uc758 \uc2dc\ud000\uc2a4\ub97c \uae30\ubc18\uc73c\ub85c \ubbf8\ub798\uc758 \uad00\ucc30\uacfc \ud589\ub3d9\uc744 \uc608\uce21\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \uac01\uac01\uc758 \uc5ed\ub3d9\uc801 \uc124\uc815(\uc815\ucc45, \uc21c\ubc29\ud5a5, \uc218\ub3d9, \uc804\uccb4)\uc5d0 \ub300\ud55c \ub9c8\uc2a4\ud06c \uc790\uae30\ud68c\uadc0\uc758 \uc801\uc6a9 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub2e4\uc774\uc5b4\uadf8\ub7a8\uc744 \ud1b5\ud574 \uc774 \ubaa8\ub378\uc758 \uc720\uc5f0\uc131\uacfc \ub2e4\uc591\ud55c \ub85c\ubcf4\ud2f1\uc2a4 \ubb38\uc81c\uc5d0 \ub300\ud55c \uc801\uc6a9 \uac00\ub2a5\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "3. Heterogeneous Masked Autoregression"}, {"figure_path": "https://arxiv.org/html/2502.04296/x3.png", "caption": "Figure 3: Network Architecture. The HMA model architecture maps low-level video and action sequences across different embodiments into a shared latent space. For actions, embodiment projectors are activated based on the training sample. The spatial-temporal Transformer produces the output video and action tokens for future frames.", "description": "\uadf8\ub9bc 3\uc740 HMA \ubaa8\ub378\uc758 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ub85c\ubd07(embodiment)\uc5d0\uc11c \uc5bb\uc740 \uc800\uc218\uc900\uc758 \ube44\ub514\uc624 \ubc0f \uc561\uc158 \uc2dc\ud000\uc2a4\ub4e4\uc744 \uacf5\uc720\ub41c \uc7a0\uc7ac \uacf5\uac04(latent space)\uc73c\ub85c \ub9e4\ud551\ud558\ub294 \uad6c\uc870\uc785\ub2c8\ub2e4.  \uc561\uc158\uc758 \uacbd\uc6b0, \ud559\uc2b5 \ub370\uc774\ud130\uc5d0 \ub530\ub77c \uac01 \ub85c\ubd07\uc5d0 \ud574\ub2f9\ud558\ub294 \uc784\ubca0\ub529 \ud504\ub85c\uc81d\ud130(embodiment projector)\uac00 \ud65c\uc131\ud654\ub429\ub2c8\ub2e4.  \uacf5\uac04-\uc2dc\uac04 \ud2b8\ub79c\uc2a4\ud3ec\uba38(spatial-temporal Transformer)\ub294 \ubbf8\ub798 \ud504\ub808\uc784\uc5d0 \ub300\ud55c \ube44\ub514\uc624 \ubc0f \uc561\uc158 \ud1a0\ud070\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uc989, \ub2e4\uc591\ud55c \ub85c\ubd07\ub4e4\uc758 \uc6c0\uc9c1\uc784\uacfc \uadf8\uc5d0 \ub530\ub978 \uc601\uc0c1 \ub370\uc774\ud130\ub97c \ud558\ub098\uc758 \ud1b5\ud569\ub41c \ubaa8\ub378\ub85c \ucc98\ub9ac\ud558\uc5ec, \ub85c\ubd07\uc758 \uc885\ub958\uc5d0 \uc0c1\uad00\uc5c6\uc774 \ubbf8\ub798 \uc608\uce21\uc744 \uc218\ud589\ud560 \uc218 \uc788\ub3c4\ub85d \uc124\uacc4\ub41c \ub124\ud2b8\uc6cc\ud06c \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Heterogeneous Masked Autoregression"}, {"figure_path": "https://arxiv.org/html/2502.04296/x4.png", "caption": "Figure 4: Pre-trained Video Model Generation. We show that a single unified HMA model can generate realistic (left 3 columns) and diverse (right 3 columns) videos across multiple embodiment datasets with heterogeneous action spaces. Each group shows three generated frames from a single sequence.", "description": "\uadf8\ub9bc 4\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \uc774\uc9c8\uc801\uc778 \ub9c8\uc2a4\ud06c \uc790\uae30\ud68c\uadc0(HMA) \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \ub85c\ubd07 \ud658\uacbd\uacfc \uc791\uc5c5\uc5d0\uc11c \uc218\uc9d1\ub41c \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub418\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e8\uc77c HMA \ubaa8\ub378\uc774 \uc5ec\ub7ec \ub85c\ubd07 \uc784\ubca0\ub514\uba3c\ud2b8(\ub2e4\uc591\ud55c \uc561\uc158 \uacf5\uac04\uc744 \uac00\uc9d0)\uc5d0 \uac78\uccd0 \uc0ac\uc2e4\uc801\uc774\uace0 \ub2e4\uc591\ud55c \ube44\ub514\uc624\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uc67c\ucabd \uc138 \uc5f4\uc740 \uc0ac\uc2e4\uc801\uc778 \ube44\ub514\uc624 \ud504\ub808\uc784\uc744, \uc624\ub978\ucabd \uc138 \uc5f4\uc740 \ub2e4\uc591\ud55c \ube44\ub514\uc624 \ud504\ub808\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uadf8\ub8f9\uc740 \ub2e8\uc77c \uc2dc\ud000\uc2a4\uc5d0\uc11c \uc0dd\uc131\ub41c \uc138 \uac1c\uc758 \ud504\ub808\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub294 HMA \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "4. \uc0ac\uc804 \ud6c8\ub828 \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2502.04296/x5.png", "caption": "Figure 5: Ablation on Pre-training Settings and Architecture. Under the pre-training setting with VQ tokens, we ablate the video generation performance (visual fidelity measured by perplexity and controllability measured by controllability). (a) We find action-conditioned models outperform passive video models. (b) We compare different action conditioning architectures in the masked autoregression framework. The purple color denotes the best model that we use by default.", "description": "\uadf8\ub9bc 5\ub294 \uc0ac\uc804 \ud6c8\ub828 \uc124\uc815 \ubc0f \uc544\ud0a4\ud14d\ucc98\uc5d0 \ub300\ud55c ablation \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. VQ \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud55c \uc0ac\uc804 \ud6c8\ub828 \uc124\uc815 \ud558\uc5d0\uc11c, perplexity(\ud654\uc9c8)\uc640 \uc81c\uc5b4 \uac00\ub2a5\uc131(controllability)\uc73c\ub85c \uce21\uc815\ub418\ub294 \ube44\ub514\uc624 \uc0dd\uc131 \uc131\ub2a5\uc744 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4. (a)\uc5d0\uc11c\ub294 action-conditioned \ubaa8\ub378\uc774 passive video \ubaa8\ub378\ubcf4\ub2e4 \uc6b0\uc218\ud568\uc744 \ubcf4\uc5ec\uc8fc\uace0, (b)\uc5d0\uc11c\ub294 masked autoregression \ud504\ub808\uc784\uc6cc\ud06c \ub0b4\uc5d0\uc11c \ub2e4\uc591\ud55c action conditioning \uc544\ud0a4\ud14d\ucc98\ub97c \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. \ubcf4\ub77c\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub41c \ubaa8\ub378\uc774 \uae30\ubcf8\uc801\uc73c\ub85c \uc0ac\uc6a9\ub41c \ucd5c\uc801\uc758 \ubaa8\ub378\uc785\ub2c8\ub2e4.", "section": "3. Heterogeneous Masked Autoregression"}, {"figure_path": "https://arxiv.org/html/2502.04296/x6.png", "caption": "Figure 6: Experiments on Scaling Behaviors of HMA. We observe positive trends in the scaling performance of heterogeneous video models across axes including the number of datasets, number of trajectories, and model sizes. The evaluation metrics on fidelity (perplexity) and controllability (\u0394\u0394\\Deltaroman_\u0394PSNR) are averaged across validation datasets.", "description": "\uadf8\ub9bc 6\uc740 HMA \ubaa8\ub378\uc758 \ud655\uc7a5\uc131 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub370\uc774\ud130\uc14b \uc218, \ud559\uc2b5 \ub370\uc774\ud130 \uc218(\uada4\uc801 \uc218), \ubaa8\ub378 \ud06c\uae30 \ub4f1 \uc138 \uac00\uc9c0 \ucd95\uc5d0 \ub530\ub77c \ubaa8\ub378 \uc131\ub2a5\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4.  \ud3c9\uac00 \uc9c0\ud45c\ub294 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4(Perplexity)\uc640 \uc81c\uc5b4 \uac00\ub2a5\uc131(\u0394PSNR)\uc73c\ub85c, \uac80\uc99d \ub370\uc774\ud130\uc14b \uc804\uccb4\uc5d0 \ub300\ud55c \ud3c9\uade0\uac12\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uc2e4\ud5d8 \uacb0\uacfc, \uc138 \uac00\uc9c0 \ucd95 \ubaa8\ub450\uc5d0\uc11c \ubaa8\ub378 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\ub294 \uc591\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ud655\uc778\ud588\uc2b5\ub2c8\ub2e4. \uc989, \ub370\uc774\ud130\uc14b\uacfc \ud559\uc2b5 \ub370\uc774\ud130\uac00 \ub9ce\uc744\uc218\ub85d, \uadf8\ub9ac\uace0 \ubaa8\ub378\uc774 \ud074\uc218\ub85d \ubaa8\ub378 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 HMA \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \ub85c\ubd07 \ud658\uacbd\uacfc \uc791\uc5c5\uc5d0 \uc77c\ubc18\ud654\ub420 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.2. HMA\uc758 \ud655\uc7a5\uc131 \ud3c9\uac00"}, {"figure_path": "https://arxiv.org/html/2502.04296/x7.png", "caption": "Figure 7: Qualitative Comparisons Between Tokenizers and Models. Despite longer convergence time, diffusion-based methods (Eq.\u00a03) on soft tokens generate better visual quality than on VQ tokens (Eq.\u00a02), qualitatively and measured by PSNR.", "description": "\uadf8\ub9bc 7\uc740 \uc11c\ub85c \ub2e4\ub978 \ud1a0\ud070\ud654 \ubc29\uc2dd(VQ \ud1a0\ud070\uacfc \uc18c\ud504\ud2b8 \ud1a0\ud070)\uacfc \ubaa8\ub378(\ud655\uc0b0 \uae30\ubc18 \ubaa8\ub378)\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c \uc0dd\uc131\ub41c \ube44\ub514\uc624\uc758 \ud654\uc9c8\uc744 \uc815\uc131\uc801 \ubc0f \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uc18c\ud504\ud2b8 \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud55c \ud655\uc0b0 \uae30\ubc18 \ubaa8\ub378(\uc2dd 3)\uc740 VQ \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud55c \ubaa8\ub378(\uc2dd 2)\ubcf4\ub2e4 PSNR(Peak Signal-to-Noise Ratio) \uce21\uc815 \uacb0\uacfc \ud654\uc9c8\uc774 \ub354 \uc6b0\uc218\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud558\uc9c0\ub9cc \uc18c\ud504\ud2b8 \ud1a0\ud070 \uae30\ubc18 \ubaa8\ub378\uc740 \uc218\ub834 \uc2dc\uac04\uc774 \ub354 \uc624\ub798 \uac78\ub9b0\ub2e4\ub294 \uc810\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \uc989, \uc18c\ud504\ud2b8 \ud1a0\ud070\uc740 \ub354 \ub192\uc740 \ud654\uc9c8\uc744 \uc81c\uacf5\ud558\uc9c0\ub9cc, \uadf8\ub9cc\ud07c \ud559\uc2b5 \uc2dc\uac04\uc774 \ub354 \uc624\ub798 \uac78\ub9b0\ub2e4\ub294 \ud2b8\ub808\uc774\ub4dc\uc624\ud504 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4.", "section": "5.1 \uc2e4\uc2dc\uac04 \uc2dc\ubbac\ub808\uc774\uc158\uc744 \uc704\ud55c HMA"}, {"figure_path": "https://arxiv.org/html/2502.04296/x8.png", "caption": "Figure 8: Video Controllability. HMA can follow user action inputs to generate physically plausible object permanence (top row) and block pushing interactions (bottom row). These video predictions are both at out-of-distribution settings and at a much longer horizon than training (over 100 frames).", "description": "\uadf8\ub9bc 8\uc740 HMA \ubaa8\ub378\uc758 \ube44\ub514\uc624 \uc81c\uc5b4 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0ac\uc6a9\uc790\uc758 \ud589\ub3d9 \uc785\ub825\uc5d0 \ub530\ub77c HMA\ub294 \ubb3c\uccb4\uc758 \uc601\uc18d\uc131(\uc704\ucabd \ud589)\uacfc \ube14\ub85d \ubc00\uae30 \uc0c1\ud638 \uc791\uc6a9(\uc544\ub798\ucabd \ud589)\uc744 \ubb3c\ub9ac\uc801\uc73c\ub85c \uadf8\ub7f4\ub4ef\ud558\uac8c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc608\uce21\ub4e4\uc740 \ud559\uc2b5\ubcf4\ub2e4 \ud6e8\uc52c \uae34 \uc2dc\uac04(100\ud504\ub808\uc784 \uc774\uc0c1) \ub3d9\uc548 \ubd84\ud3ec \uc678 \uc124\uc815\uc5d0\uc11c \uc774\ub8e8\uc5b4\uc9d1\ub2c8\ub2e4. \uc989, \ubaa8\ub378\uc774 \ud559\uc2b5\uc5d0\uc11c \ubcf4\uc9c0 \ubabb\ud55c \uc0c1\ud669\uacfc \ud6e8\uc52c \uae34 \uc2dc\ud000\uc2a4\uc5d0\uc11c\ub3c4 \ud604\uc2e4\uc801\uc778 \ub3d9\uc791\uc744 \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "5. Post-Training Applications"}, {"figure_path": "https://arxiv.org/html/2502.04296/x9.png", "caption": "Figure 9: Policy Evaluation with HMA. By learning the action-video dynamics over both successful and failed examples, HMA can be used to evaluate policies, similar to a traditional simulator [46]. The autoregressive horizon at inference time is 10 times more than the training time horizon.", "description": "\uadf8\ub9bc 9\ub294 HMA\ub97c \uc774\uc6a9\ud55c \uc815\ucc45 \ud3c9\uac00 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc131\uacf5 \ubc0f \uc2e4\ud328 \uc0ac\ub840 \ubaa8\ub450\ub97c \ud3ec\ud568\ud55c \ud589\ub3d9-\ube44\ub514\uc624 \uc5ed\ud559\uc744 \ud559\uc2b5\ud568\uc73c\ub85c\uc368, HMA\ub294 \uae30\uc874 \uc2dc\ubbac\ub808\uc774\ud130\uc640 \uc720\uc0ac\ud558\uac8c \uc815\ucc45\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, \ucd94\ub860 \uc2dc \uc790\ub3d9 \ud68c\uadc0 \uc9c0\ud3c9\uc120\uc740 \ud559\uc2b5 \uc2dc \uc790\ub3d9 \ud68c\uadc0 \uc9c0\ud3c9\uc120\ubcf4\ub2e4 10\ubc30 \ub354 \uae41\ub2c8\ub2e4. \uc774\ub294 HMA\uac00 \ud559\uc2b5 \ub370\uc774\ud130\ubcf4\ub2e4 \ud6e8\uc52c \ub354 \uae34 \uc2dc\uac04\ub300\uc5d0 \uac78\uce5c \uc815\ucc45\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud560 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\ub294 \uc131\uacf5\uc801\uc778 \uc0c1\ud638 \uc791\uc6a9\uacfc \uc2e4\ud328\ud55c \uc0c1\ud638 \uc791\uc6a9\uc758 \uc608\uac00 \ud568\uaed8 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. \uc815\ucc45 \ud3c9\uac00"}]