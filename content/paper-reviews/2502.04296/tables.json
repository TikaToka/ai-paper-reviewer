[{"content": "| Model | Method | Parameters (M) | FPS \u2191 |\n|---|---|---|---| \n| IRASim-XL | DiT | 679 | 0.28 |\n| IRASim-XL, amortized | DiT | 679 | 0.58 |\n| HMA-Base | MaskGIT | 44 | 22.72 |\n| HMA-XL | MaskGIT | 679 | 4.38 |\n| HMA-Base | MAR | 96 | 4.44 |\n| HMA-XL | MAR | 741 | 2.01 |", "caption": "Table 1: Inference Speed. We measure the per-frame inference speed across 16 frames for various model sizes. The Base model has a model size of around 30M and the XL model has a similar model size as IRASim-XL. The models all use 32-block transformers where the base model has dimensions 256 and the XL models have dimensions 768. Our fastest model of the same size is more than 15\u00d7\\times\u00d7 faster than [60] because HMA does not pass through the full Transformer multiple times (with diffusion modeling) to generate each frame. MAR incurs more parameters than MaskGIT [8] because of the diffusion heads [27]. The amortized result for [60] comes from averaging over multiple frames. The speeds are all measured on the same hardware setup with RTX-4080 GPU.", "description": "\ud45c 1\uc740 \ub2e4\uc591\ud55c \ubaa8\ub378 \ud06c\uae30\uc5d0 \ub300\ud574 16\ud504\ub808\uc784\uc5d0 \uac78\uccd0 \ud504\ub808\uc784\ub2f9 \ucd94\ub860 \uc18d\ub3c4\ub97c \uce21\uc815\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\ubcf8 \ubaa8\ub378\uc758 \ud06c\uae30\ub294 \uc57d 3\ucc9c\ub9cc \ub9e4\uac1c\ubcc0\uc218\uc774\uace0, XL \ubaa8\ub378\uc758 \ud06c\uae30\ub294 IRASim-XL\uacfc \ube44\uc2b7\ud569\ub2c8\ub2e4. \ubaa8\ub4e0 \ubaa8\ub378\uc740 32\uac1c\uc758 \ube14\ub85d\uc73c\ub85c \uad6c\uc131\ub41c \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub97c \uc0ac\uc6a9\ud558\uba70, \uae30\ubcf8 \ubaa8\ub378\uc740 256\ucc28\uc6d0, XL \ubaa8\ub378\uc740 768\ucc28\uc6d0\uc744 \uac16\uc2b5\ub2c8\ub2e4. \ub3d9\uc77c\ud55c \ud06c\uae30\uc758 \ubaa8\ub378 \uc911 HMA\uac00 \uac00\uc7a5 \ube60\ub978\ub370, \uadf8 \uc774\uc720\ub294 HMA\uac00 \uac01 \ud504\ub808\uc784\uc744 \uc0dd\uc131\ud558\uae30 \uc704\ud574 \uc804\uccb4 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub97c \uc5ec\ub7ec \ubc88 \ud1b5\uacfc\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4(\ud655\uc0b0 \ubaa8\ub378\ub9c1 \uc0ac\uc6a9). MAR\uc740 \ud655\uc0b0 \ud5e4\ub4dc\ub85c \uc778\ud574 MaskGIT\ubcf4\ub2e4 \ub354 \ub9ce\uc740 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. [60]\uc758 \ud3c9\uade0\ud654\ub41c \uacb0\uacfc\ub294 \uc5ec\ub7ec \ud504\ub808\uc784\uc5d0 \ub300\ud55c \ud3c9\uade0\uac12\uc785\ub2c8\ub2e4. \ubaa8\ub4e0 \uc18d\ub3c4\ub294 RTX-4080 GPU\ub97c \uc0ac\uc6a9\ud558\ub294 \ub3d9\uc77c\ud55c \ud558\ub4dc\uc6e8\uc5b4 \uc124\uc815\uc5d0\uc11c \uce21\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "5. Post-Training Applications"}, {"content": "| Metric | IRASim | HMA |\n|---|---|---|\n| PSNR \u2191 | 25.41 | 28.19 |\n| SSIM \u2191 | 0.82 | 0.83 |\n| \u0394PSNR \u2191 | 5.78 | 6.06 |\n| LPIPS \u2193 | 0.08 | 0.07 |\n| FID \u2193 | 23.22 | 33.56 |\n| FVD \u2193 | 152.20 | 111.52 |", "caption": "Table 2: Comparison with IRASim. In Language Table Benchmark [31], we show that a pre-trained HMA-based model (diffusion) is able to achieve better visual qualities and controllability than IRASim while maintaining faster speed and requiring less compute. The results are computed over 200 held-out trajectories.", "description": "\ud45c 2\ub294 \uc81c\uc548\ub41c HMA \ubaa8\ub378(\ud655\uc0b0 \uae30\ubc18)\uacfc \uae30\uc874 IRASim \ubaa8\ub378\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Language Table Benchmark [31] \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec 200\uac1c\uc758 \ud640\ub4dc\uc544\uc6c3 \ud14c\uc2a4\ud2b8 \uc2dc\ud000\uc2a4\uc5d0 \ub300\ud574 \uc2dc\uac01\uc801 \ucda9\uc2e4\ub3c4(PSNR, SSIM, LPIPS), \uc81c\uc5b4 \uac00\ub2a5\uc131(\u0394PSNR), \uadf8\ub9ac\uace0 \uc18d\ub3c4(FPS)\ub97c \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4.  HMA \ubaa8\ub378\uc740 IRASim\ubcf4\ub2e4 \ub354 \ub098\uc740 \uc2dc\uac01\uc801 \ucda9\uc2e4\ub3c4\uc640 \uc81c\uc5b4 \uac00\ub2a5\uc131\uc744 \ub2ec\uc131\ud588\uc73c\uba70, \ub3d9\uc2dc\uc5d0 \ub354 \ube60\ub978 \uc18d\ub3c4\uc640 \uc801\uc740 \uacc4\uc0b0\ub7c9\uc73c\ub85c \ub3d9\uc791\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 HMA \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "5. Post-Training Applications"}, {"content": "|Method|PSNR \u2191|Perplexity \u2193|\u0394 PSNR \u2191|LPIPS \u2193|\n|---|---|---|---|---|\n|HMA|21.01|305.87|0.01|0.19|\n|HMA<sup>+</sup>|22.04|189.83|0.06|0.17|", "caption": "Table 3: Real World Finetuning. HMA + denotes finetuned model based on pre-trained checkpoints while HMA trains from scratch on the finetuning data. This experiment uses the discrete loss baseline.", "description": "\ubcf8 \ud45c\ub294 \uc2e4\uc81c \ud658\uacbd\uc5d0\uc11c \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \"HMA+\"\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uae30\ubc18\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\uc774\uace0, \"HMA\"\ub294 \ubbf8\uc138 \uc870\uc815 \ub370\uc774\ud130\ub85c\ubd80\ud130 \ucc98\uc74c\ubd80\ud130 \ud559\uc2b5\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4. \uc774 \uc2e4\ud5d8\uc5d0\uc11c\ub294 \uc774\uc0b0 \uc190\uc2e4(discrete loss) \uae30\uc900\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 PSNR, Perplexity, \u0394PSNR, LPIPS \uc9c0\ud45c\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \uc9c0\ud45c\ub294 \ube44\ub514\uc624 \uc0dd\uc131 \ubaa8\ub378\uc758 \ud654\uc9c8\uacfc \uc81c\uc5b4 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. PSNR\uacfc SSIM\uc740 \uc774\ubbf8\uc9c0 \ud488\uc9c8 \ud3c9\uac00 \uc9c0\ud45c\uc774\uba70, Perplexity\ub294 \ubaa8\ub378\uc774 \uc5bc\ub9c8\ub098 \uc798 \ub370\uc774\ud130\ub97c \uc608\uce21\ud558\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 \uc9c0\ud45c\uc774\uace0, \u0394PSNR\uc740 \ubaa8\ub378\uc758 \uc81c\uc5b4 \uac00\ub2a5\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uc9c0\ud45c\uc774\uba70, LPIPS\ub294 \uc778\uac04\uc758 \uc9c0\uac01\uacfc \uc77c\uce58\ud558\ub294 \uc774\ubbf8\uc9c0 \uc720\uc0ac\uc131 \uc9c0\ud45c\uc785\ub2c8\ub2e4.", "section": "5. Post-Training Applications"}, {"content": "| Method | PSNR \u2191 | Perplexity \u2193 | PSNR* \u2191 | Perplexity* \u2193 |\n|---|---|---|---|---|\n| HMA | 24.17 | 20.69 | 19.19 | 1193.70 |\n| HMA<sup>+</sup> | 25.11 | 11.82 | 20.20 | 103.01 |", "caption": "Table 4: Simulation Transfer Learning. We show that pre-trained HMA can help with fine-tuning using cross-entropy losses and diffusion losses jointly. where HMA + denotes the finetuned model based on pre-trained checkpoints.", "description": "\ubcf8 \ud45c\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c HMA \ubaa8\ub378\uc744 \uc774\uc6a9\ud558\uc5ec \uad50\ucc28 \uc5d4\ud2b8\ub85c\ud53c \uc190\uc2e4\uacfc \ud655\uc0b0 \uc190\uc2e4\uc744 \ud568\uaed8 \uc0ac\uc6a9\ud558\ub294 \ubbf8\uc138 \uc870\uc815 \uacfc\uc815\uc5d0\uc11c \uc0ac\uc804 \ud6c8\ub828\ub41c \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uae30\ubc18\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378(HMA+)\uc774 \uc5b4\ub5bb\uac8c \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ub3c4\uc6c0\uc774 \ub418\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\uc774 \uae30\uc874 \ubaa8\ub378 \ub300\ube44 \uc2dc\ubbac\ub808\uc774\uc158 \uc804\uc774 \ud559\uc2b5\uc5d0\uc11c PSNR(\ud53c\ud06c \uc2e0\ud638 \ub300 \uc7a1\uc74c\ube44)\uc740 \ub192\uc774\uace0 Perplexity(\ud37c\ud50c\ub809\uc11c\ud2f0)\ub294 \ub0ae\ucd94\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  PSNR* \ubc0f Perplexity* \uc9c0\ud45c\ub294 \uc791\uc740 \ud589\ub3d9 \ubcc0\ud654\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \ubbfc\uac10\ub3c4\ub97c \uce21\uc815\ud558\ub294 \uc9c0\ud45c\uc774\uba70, \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\uc758 \uac15\uac74\uc131\uacfc \uc548\uc815\uc131\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. Post-Training Applications"}, {"content": "| Policy Evaluator | 1 | 2 | 3 | 4 |\n|---|---|---|---|---|\n| Ground Truth Simulator | 0.38 | 0.52 | 0.70 | 1.00 |\n| HMA Simulator | 0.43 | 0.56 | 0.66 | 0.73 |", "caption": "Table 5: Policy Evaluation Results Across 4 Different Policies. We observed positive correlations of the evaluation results for 4 different policies bewteen the ground truth and learned simulators. The Pearson ratio between evaluations is 0.95.", "description": "\uc774 \ud45c\ub294 \ub124 \uac00\uc9c0 \ub2e4\ub978 \uc815\ucc45\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc815\ucc45\uc740 \uc2e4\uc81c \uc2dc\ubbac\ub808\uc774\ud130\uc640 \ud559\uc2b5\ub41c \uc2dc\ubbac\ub808\uc774\ud130 \ubaa8\ub450\uc5d0\uc11c \ud3c9\uac00\ub418\uc5c8\uc73c\uba70, \ub450 \uc2dc\ubbac\ub808\uc774\ud130 \uac04 \ud3c9\uac00 \uacb0\uacfc\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4 (\ud53c\uc5b4\uc2a8 \uc0c1\uad00 \uacc4\uc218 0.95).  \uc774\ub294 \ud559\uc2b5\ub41c \uc2dc\ubbac\ub808\uc774\ud130\uac00 \uc2e4\uc81c \uc2dc\ubbac\ub808\uc774\ud130\uc640 \uc720\uc0ac\ud558\uac8c \uc815\ucc45\uc744 \ud3c9\uac00\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \uc815\ucc45\uc5d0 \ub300\ud55c 1~4\ubc88 \uc2dc\ubbac\ub808\uc774\uc158 \uacb0\uacfc\uac00 \ub098\ud0c0\ub098 \uc788\uc73c\uba70, \uc2e4\uc81c \uc2dc\ubbac\ub808\uc774\ud130\uc640 \ud559\uc2b5\ub41c \uc2dc\ubbac\ub808\uc774\ud130 \ubaa8\ub450\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. Post-Training Applications"}, {"content": "|             | +0   | +10  | +50  | +90  | original |\n| :---------- | :---- | :---- | :---- | :---- | :------- |\n| Success in [32] | 82%  | 90%  | 96%  | 100% | 100%     |\n| Validation Loss in [31] | 1.72 | 1.16 | 1.09 | 0.88 | 0.87     |", "caption": "Table 6: Synthetic Data for Policy Learning. We evaluate the quality of generated synthetic data by adding different numbers of generated video trajectories in [32] and [31], from 10 to 100, to a fixed subset (10 trajectories) of the original data (100 trajectories). We then conduct policy training and evaluation and report the Robomimic success rates (top row) and Language Table validation losses (bottom row).", "description": "\uc774 \ud45c\ub294 \ud569\uc131 \ub370\uc774\ud130\ub97c \uc774\uc6a9\ud55c \uc815\ucc45 \ud559\uc2b5\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\uc874\uc758 100\uac1c\uc758 \uc2e4\uc81c \ub370\uc774\ud130\uc5d0 10\uac1c\uc5d0\uc11c 100\uac1c\uae4c\uc9c0\uc758 \ud569\uc131 \ub370\uc774\ud130\ub97c \ucd94\uac00\ud558\uc5ec \uc815\ucc45 \ud559\uc2b5\uc744 \uc9c4\ud589\ud558\uace0, Robomimic \uc131\uacf5\ub960\uacfc Language Table \uac80\uc99d \uc190\uc2e4\uc744 \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4.  \ud569\uc131 \ub370\uc774\ud130\uc758 \ube44\uc728\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc815\ucc45 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub294 HMA \ubaa8\ub378\uc774 \ud604\uc2e4\uc801\uc778 \ub3d9\uc791\uc744 \uc798 \ubaa8\ubc29\ud558\ub294 \uace0\ud488\uc9c8\uc758 \ud569\uc131 \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "5.4 \ud569\uc131 \ub370\uc774\ud130 \uc0dd\uc131"}]