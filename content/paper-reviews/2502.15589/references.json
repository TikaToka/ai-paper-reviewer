{"references": [{"fullname_first_author": "DeepSeek-AI", "paper_title": "DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning", "publication_date": "2025-01-12", "reason": "This paper introduces the R1-Distill model used to initialize training in the current work, and it is a key method for enhancing reasoning abilities in LLMs, directly impacting the current research."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-11-28", "reason": "This paper introduces Chain-of-Thought (CoT) prompting, a crucial technique for improving reasoning capabilities in LLMs that has significantly influenced the development of slow-thinking modes and is directly related to this paper's work."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-04", "reason": "This foundational paper introduced the Transformer architecture, which is the basis for most current LLMs, making it a critical foundation for the current research on improving LLM efficiency."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The Llama 3 herd of models", "publication_date": "2024-07-21", "reason": "This paper introduces the Llama 3 series of LLMs, one of the two model series used in the experiments, and it demonstrates notable advancements in large language models, making it a significant component of the comparative analysis."}, {"fullname_first_author": "Jintian Zhang", "paper_title": "OneGen: Efficient one-pass unified generation and retrieval for LLMs", "publication_date": "2024-07-09", "reason": "This paper, also authored by some of the current paper's authors, proposes a method for improving LLM efficiency, which has directly informed the design and development of the LightThinker model proposed in the current research."}]}