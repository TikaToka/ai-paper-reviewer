[{"figure_path": "https://arxiv.org/html/2412.11279/x2.png", "caption": "Figure 1: Face swapping results of VividFace\u00a0 at 512\u00d7512512512512\\times 512512 \u00d7 512 resolution. Our method produces high-fidelity and vivid outputs that accurately follow both pose and expression changes.", "description": "VividFace\uc758 512x512 \ud574\uc0c1\ub3c4 \uc5bc\uad74 \ubc14\uafb8\uae30 \uacb0\uacfc. \uc81c\uc2dc\ub41c \uc608\uc2dc\uc5d0\uc11c VividFace\ub294 \uc6d0\ubcf8 \uc5bc\uad74\uc758 \ud3ec\uc988\uc640 \ud45c\uc815 \ubcc0\ud654\ub97c \uc815\ud655\ud558\uac8c \ub530\ub974\ub294 \uace0\ud488\uc9c8\uc758 \uc0dd\uc0dd\ud55c \uacb0\uacfc\ubb3c\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uccab \ubc88\uc9f8 \ud589\uacfc \ub450 \ubc88\uc9f8 \ud589\uc740 \uac01\uac01 \uc5ec\uc131\uacfc \ub0a8\uc131\uc758 \uc5bc\uad74 \ubc14\uafb8\uae30 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uba70, VividFace\uac00 \ub2e4\uc591\ud55c \uc131\ubcc4\uc758 \uc5bc\uad74\uc5d0 \ub300\ud574\uc11c\ub3c4 \ud6a8\uacfc\uc801\uc73c\ub85c \uc791\ub3d9\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c, \ub2e4\uc591\ud55c \ud3ec\uc988\uc640 \ud45c\uc815 \ubcc0\ud654\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \uc77c\uad00\ub41c \uacb0\uacfc\ub97c \uc0dd\uc131\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.11279/x3.png", "caption": "Figure 2: Overview of the proposed framework. During training, our framework randomly chooses static images or video sequences as the training data. In addition to the noise ztsubscript\ud835\udc67\ud835\udc61z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, three other types of inputs are integrated to guide the generation process: (1) a face region mask, which controls the generation of facial imagery; (2) a 3D reconstructed face, which helps guide the pose and expression, especially in cases of large pose variations; and (3) masked source images, which supply background information. These inputs are processed through the Backbone Network, which performs the denoising operation. Within the Backbone Network, we employ cross-attention and temporal attention mechanisms. The temporal attention module ensures temporal continuity and consistency across frames. Our face encoder extracts identity and texture features from the target face, as well as pose and expression details from the source face, and uses these features in cross-attention to produce realistic and high-fidelity results.", "description": "VividFace \ud504\ub808\uc784\uc6cc\ud06c\ub294 \uc774\ubbf8\uc9c0-\ube44\ub514\uc624 \ud558\uc774\ube0c\ub9ac\ub4dc \ud559\uc2b5 \uc804\ub7b5\uc744 \uc0ac\uc6a9\ud558\uc5ec \ube44\ub514\uc624 \uc5bc\uad74 \uad50\uccb4\ub97c \uc218\ud589\ud569\ub2c8\ub2e4. \ud559\uc2b5 \uc911 \ud504\ub808\uc784\uc6cc\ud06c\ub294 \uc815\uc801 \uc774\ubbf8\uc9c0 \ub610\ub294 \ube44\ub514\uc624 \uc2dc\ud000\uc2a4\ub97c \ubb34\uc791\uc704\ub85c \uc120\ud0dd\ud569\ub2c8\ub2e4. \uc0dd\uc131 \ud504\ub85c\uc138\uc2a4\ub97c \uc548\ub0b4\ud558\uae30 \uc704\ud574 \ub178\uc774\uc988 $z_t$ \uc678\uc5d0\ub3c4 \uc138 \uac00\uc9c0 \uc720\ud615\uc758 \uc785\ub825\uc774 \ud1b5\ud569\ub429\ub2c8\ub2e4. (1) \uc5bc\uad74 \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc744 \uc81c\uc5b4\ud558\ub294 \uc5bc\uad74 \uc601\uc5ed \ub9c8\uc2a4\ud06c, (2) \ud2b9\ud788 \ud070 \ud3ec\uc988 \ubcc0\ud654\uc758 \uacbd\uc6b0 \ud3ec\uc988 \ubc0f \ud45c\uc815\uc744 \uc548\ub0b4\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub418\ub294 3D \uc7ac\uad6c\uc131\ub41c \uc5bc\uad74, (3) \ubc30\uacbd \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\ub294 \ub9c8\uc2a4\ud06c\ub41c \uc18c\uc2a4 \uc774\ubbf8\uc9c0. \uc774\ub7ec\ud55c \uc785\ub825\uc740 Backbone Network\ub97c \ud1b5\ud574 \ucc98\ub9ac\ub418\uc5b4 denoising \uc791\uc5c5\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. Backbone Network \ub0b4\uc5d0\uc11c \uad50\ucc28 \uc8fc\uc758 \ubc0f \uc2dc\uac04\uc801 \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc2dc\uac04\uc801 \uc8fc\uc758 \ubaa8\ub4c8\uc740 \ud504\ub808\uc784 \uc804\uccb4\uc5d0\uc11c \uc2dc\uac04\uc801 \uc5f0\uc18d\uc131\uacfc \uc77c\uad00\uc131\uc744 \ubcf4\uc7a5\ud569\ub2c8\ub2e4. \uc5bc\uad74 \uc778\ucf54\ub354\ub294 \ub300\uc0c1 \uc5bc\uad74\uc5d0\uc11c ID \ubc0f \ud14d\uc2a4\ucc98 \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud558\uace0 \uc18c\uc2a4 \uc5bc\uad74\uc5d0\uc11c \ud3ec\uc988 \ubc0f \ud45c\uc815 \uc138\ubd80 \uc815\ubcf4\ub97c \ucd94\ucd9c\ud558\uc5ec \uad50\ucc28 \uc8fc\uc758\uc5d0 \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc2e4\uc801\uc774\uace0 \ucda9\uc2e4\ub3c4 \ub192\uc740 \uacb0\uacfc\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.11279/x4.png", "caption": "Figure 3: Overview of the proposed VidFaceVAE, capable of simultaneous encoding and decoding of both image and video data. Certain modules are specifically designed for video inputs, and image inputs bypass these modules as needed.", "description": "\uc81c\uc548\ub41c VidFaceVAE\uc758 \uac1c\uc694\ub294 \uc774\ubbf8\uc9c0\uc640 \ube44\ub514\uc624 \ub370\uc774\ud130 \ubaa8\ub450\uc758 \ub3d9\uc2dc \uc778\ucf54\ub529 \ubc0f \ub514\ucf54\ub529\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. \ud2b9\uc815 \ubaa8\ub4c8\uc740 \ube44\ub514\uc624 \uc785\ub825\uc6a9\uc73c\ub85c \ud2b9\ubcc4\ud788 \uc124\uacc4\ub418\uc5c8\uc73c\uba70 \uc774\ubbf8\uc9c0 \uc785\ub825\uc740 \ud544\uc694\uc5d0 \ub530\ub77c \uc774\ub7ec\ud55c \ubaa8\ub4c8\uc744 \uc6b0\ud68c\ud569\ub2c8\ub2e4. VidFaceVAE\ub294 (2+1)D \ube14\ub85d\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 2D \uacf5\uac04 \ubc0f 1D \uc2dc\uac04\uc801 \ucee8\ubcfc\ub8e8\uc158\uc744 \uacb0\ud569\ud558\uc5ec \uc758\uc0ac 3D \uc5f0\uc0b0\uc790\ub97c \ud615\uc131\ud569\ub2c8\ub2e4. \uc774\ubbf8\uc9c0 \uc785\ub825\uc758 \uacbd\uc6b0 STFM(Spatial Temporal Fusion Module)\uc740 2D ResBlock\uc758 \uacb0\uacfc\ub97c \uc9c1\uc811 \ucd9c\ub825\ud558\uc5ec Temporal ResBlock\uc744 \uc6b0\ud68c\ud569\ub2c8\ub2e4. \ube44\ub514\uc624 \uc785\ub825\uc758 \uacbd\uc6b0 STFM\uc740 \ud559\uc2b5 \uac00\ub2a5\ud55c \uacc4\uc218 \u03b2\ub97c \uc0ac\uc6a9\ud558\uc5ec 2D \ubc0f \uc2dc\uac04 \ube14\ub85d\uc758 \ucd9c\ub825\uc744 \uacb0\ud569\ud569\ub2c8\ub2e4. \uc2dc\uac04\uc801 \ub2e4\uc6b4\uc0d8\ud50c\ub9c1 \ubaa8\ub4c8\uc740 \uc774\ubbf8\uc9c0 \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud574\uc57c \ud558\ubbc0\ub85c VAE \ud504\ub808\uc784\uc6cc\ud06c\uc5d0 \ud3ec\ud568\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. VidFaceVAE\ub294 \ub450 \uac00\uc9c0 \uc8fc\uc694 \uc774\uc810\uc774 \uc788\ub294 (2+1)D \uad6c\uc870\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. (1) \uacf5\uac04 \ubc0f \uc2dc\uac04 \ucee8\ubcfc\ub8e8\uc158\uc744 \ubd84\ub9ac\ud558\uc5ec \uc804\uccb4 3D \ucee8\ubcfc\ub8e8\uc158\ubcf4\ub2e4 \uacc4\uc0b0 \ube44\uc6a9\uc744 \uc904\uc785\ub2c8\ub2e4. (2) \uc0ac\uc804 \ud6c8\ub828\ub41c 2D VAE \ub9e4\uac1c\ubcc0\uc218\uc640 SD \uc0ac\uc804 \ud6c8\ub828\ub41c \uac00\uc911\uce58\ub97c \uc7ac\uc0ac\uc6a9\ud558\uc5ec \uc218\ub834 \uc18d\ub3c4\ub97c \ub192\uc774\uace0 \ucd5c\uc885 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4. OD-VAE\uc640 \ub2ec\ub9ac \uc2dc\uac04 \ubaa8\ub4c8\uc740 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uac74\ub108\ub6f0\uace0 \ubc31\ubcf8 \ub124\ud2b8\uc6cc\ud06c\ub294 \ubcc0\ud658\uae30\ub97c \uae30\ubc18\uc73c\ub85c \ud558\uc9c0 \uc54a\uc73c\ubbc0\ub85c 3D-Causal-CNN\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc778\uacfc \ucee8\ubcfc\ub8e8\uc158\uc740 \ubaa8\ub378 \uc6a9\ub7c9\uc744 \uc81c\ud55c\ud558\uba70 \uc778\uacfc \ucee8\ubcfc\ub8e8\uc158\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc815\uc801 \uc774\ubbf8\uc9c0\ub97c \ucc98\ub9ac\ud574\ub3c4 \ube44\ub514\uc624\uc640 \uc774\ubbf8\uc9c0 \ubaa8\ub450\uc758 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.11279/x5.png", "caption": "Figure 4: Visualization of our occlusion data augmentation, which improves the stability and consistency of the generated videos.", "description": "Figure 4\ub294 VividFace\uc758 \ud3d0\uc0c9 \ub370\uc774\ud130 \uc99d\uac15 \uae30\ubc95\uc744 \uc2dc\uac01\ud654\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uae30\ubc95\uc740 \uc0dd\uc131\ub41c \ube44\ub514\uc624\uc758 \uc548\uc815\uc131\uacfc \uc77c\uad00\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. Figure 4\ub294 \uc6d0\ubcf8 \ube44\ub514\uc624 \ud504\ub808\uc784\uacfc \ud3d0\uc0c9 \ub370\uc774\ud130 \uc99d\uac15 \uae30\ubc95\uc774 \uc801\uc6a9\ub41c \ud504\ub808\uc784\uc744 \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud3d0\uc0c9 \ub370\uc774\ud130 \uc99d\uac15 \uae30\ubc95\uc740 \ub2e4\uc591\ud55c \uc885\ub958\uc758 \ud3d0\uc0c9 \uac1d\uccb4(\uc608: \uc7a5\ub09c\uac10, \uc190 \ub4f1)\ub97c \ucd94\uac00\ud558\uace0 \uc2dc\uac04\uc801 \ud328\ud134\uc744 \ub3d9\uc801\uc73c\ub85c \ubcc0\ud654\uc2dc\ucf1c \ub300\uc0c1 \uc774\ubbf8\uc9c0\uc758 \uc5bc\uad74\uc744 \ubd80\ubd84\uc801\uc73c\ub85c \uac00\ub9bd\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378\uc740 \uac00\ub824\uc9d0, \ud070 \ud3ec\uc988 \ubcc0\ud654, \uc870\uba85 \ubcc0\ud654 \ub4f1 \uc2e4\uc81c \ube44\ub514\uc624\uc5d0\uc11c \ubc1c\uc0dd\ud560 \uc218 \uc788\ub294 \ub2e4\uc591\ud55c \uc5b4\ub824\uc6b4 \uc0c1\ud669\uc5d0 \ub300\ud55c \uacac\uace0\uc131\uc744 \ub192\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c, VividFace\ub294 \uc2dc\uac04\uc801 \uc65c\uace1, \uae5c\ube61\uc784, \uc5bc\uad74 \uc65c\uace1\uacfc \uac19\uc740 \ubb38\uc81c\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \uc644\ud654\ud558\uace0 \uace0\ud488\uc9c8\uc758 \ube44\ub514\uc624 \uc5bc\uad74 \uc2a4\uc651 \uacb0\uacfc\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.11279/x6.png", "caption": "Figure 5: Visualization of our AIDT dataset. For video facial data, we present only the target and decoupling faces, as the source faces can be derived from any other frame within the same video clip.", "description": "AIDT \ub370\uc774\ud130\uc14b\uc740 \uc18c\uc2a4 \uc5bc\uad74, \ud0c0\uac9f \uc5bc\uad74, GAN \uc0dd\uc131 \ub514\ucee4\ud50c\ub9c1 \uc5bc\uad74\uc758 \uc138 \uac00\uc9c0 \uc5bc\uad74 \uc774\ubbf8\uc9c0\ub85c \uad6c\uc131\ub41c \ud2b8\ub9ac\ud50c\ub81b \ub370\uc774\ud130\uc785\ub2c8\ub2e4. \uc18c\uc2a4\uc640 \ud0c0\uac9f \uc5bc\uad74\uc740 \ub3d9\uc77c \uc778\ubb3c\uc774\uc9c0\ub9cc \ud3ec\uc988\uc640 \ud45c\uc815\uc774 \ub2e4\ub985\ub2c8\ub2e4. GAN \uc0dd\uc131 \ub514\ucee4\ud50c\ub9c1 \uc5bc\uad74\uc740 \ud0c0\uac9f \uc5bc\uad74\uacfc \ud3ec\uc988\uc640 \ud45c\uc815\uc740 \uac19\uc9c0\ub9cc \ub2e4\ub978 \uc0ac\ub78c\uc758 \uc5bc\uad74\uc785\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uad6c\uc131\uc744 \ud1b5\ud574 \uc5bc\uad74 \uad50\ud658 \ubaa8\ub378\uc774 ID \ud2b9\uc9d5\uacfc \ud3ec\uc988 \ud2b9\uc9d5\uc744 \ubd84\ub9ac\ud558\uc5ec \ud559\uc2b5\ud558\uace0, \uc18c\uc2a4\uc640 \ud0c0\uac9f\uc774 \ub2e4\ub978 \uc0ac\ub78c\uc77c \ub54c\uc758 \uc77c\ubc18\ud654 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c \ube44\ub514\uc624 \ub370\uc774\ud130\uc758 \uacbd\uc6b0, \uac19\uc740 \ube44\ub514\uc624 \ud074\ub9bd \ub0b4\uc758 \ub2e4\ub978 \ud504\ub808\uc784\uc5d0\uc11c \uc18c\uc2a4 \uc5bc\uad74\uc744 \uc5bb\uc744 \uc218 \uc788\uae30 \ub54c\ubb38\uc5d0 \ud0c0\uac9f \uc5bc\uad74\uacfc \ub514\ucee4\ud50c\ub9c1 \uc5bc\uad74\ub9cc \ud45c\uc2dc\ub429\ub2c8\ub2e4.", "section": "4. AIDT Dataset"}, {"figure_path": "https://arxiv.org/html/2412.11279/x7.png", "caption": "Figure 6: Qualitative comparison at 512\u00d7512512512512\\times 512512 \u00d7 512 resolution. Our method generates high-fidelity results and handles challenging cases effectively, such as large poses (b) and occlusions (c). Corresponding videos are provided in the supplementary material.It is best viewed at a larger scale for optimal evaluation.", "description": "Figure 6\uc740 VividFace\uac00 \uc0dd\uc131\ud55c \uc5bc\uad74 \uad50\uccb4 \uacb0\uacfc\ub97c \ub2e4\ub978 \ubc29\ubc95\ub4e4\uacfc 512x512 \ud574\uc0c1\ub3c4\uc5d0\uc11c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. (a)\uc640 (d)\ub294 \uc77c\ubc18\uc801\uc778 \uc0c1\ud669\uc5d0\uc11c, (b)\ub294 \ud070 \ud3ec\uc988 \ubcc0\ud654\uac00 \uc788\ub294 \uacbd\uc6b0, (c)\ub294 \uc5bc\uad74\uc758 \uc77c\ubd80\uac00 \uac00\ub824\uc9c4 \uacbd\uc6b0\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. VividFace\ub294 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc5d0 \ube44\ud574 \ub192\uc740 \ud488\uc9c8\uc758 \uacb0\uacfc\ub97c \uc0dd\uc131\ud558\uace0, \ud070 \ud3ec\uc988 \ubcc0\ud654\ub098 \uac00\ub824\uc9d0\uacfc \uac19\uc740 \uc5b4\ub824\uc6b4 \uc0c1\ud669\uc5d0\uc11c\ub3c4 \uc548\uc815\uc801\uc73c\ub85c \uc5bc\uad74 \uad50\uccb4\ub97c \uc218\ud589\ud569\ub2c8\ub2e4. \ucd94\uac00\uc801\uc73c\ub85c, \ubcf4\ucda9 \uc790\ub8cc\uc5d0 \ud574\ub2f9\ud558\ub294 \ube44\ub514\uc624\ub4e4\uc774 \uc81c\uacf5\ub429\ub2c8\ub2e4.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.11279/x8.png", "caption": "Figure 7: Ablation on the different combinations of texture weights and attribute weights.", "description": "\uc774 \uadf8\ub9bc\uc740 \uc5bc\uad74 \uad50\uccb4 \ud504\ub808\uc784\uc6cc\ud06c\uc5d0\uc11c \ud14d\uc2a4\ucc98 \uac00\uc911\uce58\uc640 \uc18d\uc131 \uac00\uc911\uce58\uc758 \ub2e4\uc591\ud55c \uc870\ud569\uc5d0 \ub300\ud55c ablation study \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud14d\uc2a4\ucc98 \uac00\uc911\uce58\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c ID \uc720\uc0ac\uc131\uc774 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \uad00\ucc30\ud560 \uc218 \uc788\uc9c0\ub9cc, \ub108\ubb34 \ub192\uac8c \uc124\uc815\ud558\uba74 \ub300\uc0c1\uc758 \uc18d\uc131(\ud3ec\uc988 \ubc0f \ud45c\uc815) \ubcf4\uc874\uc774 \uc190\uc2e4\ub429\ub2c8\ub2e4. \ubc18\ub300\ub85c \uc18d\uc131 \uac00\uc911\uce58\uac00 \uc99d\uac00\ud558\uba74 \ub300\uc0c1\uc758 \uc18d\uc131\uc774 \ub354 \uc798 \ubcf4\uc874\ub418\uc9c0\ub9cc ID \uc720\uc0ac\uc131\uc740 \uac10\uc18c\ud569\ub2c8\ub2e4.", "section": "5.4. Analysis"}]