[{"content": "| Model | # Emb. P. | Loss\u2193 | Downstream\u2191 |\n|---|---|---|---|\n| OLMoE-1.3B | 51M | 2.554 | 0.510 |\n| +OE-12.8M | 13.1B | 2.472 (-0.082) | 0.524 (+0.014) |\n| OLMoE-7B | 102M | 2.305 | 0.601 |\n| +OE-12.8M | 26.3B | 2.229 (-0.076) | 0.608 (+0.007) |", "caption": "Table 1: Performance of Over-Encoding on MoE architecture with 500B tokens\u2019 training. The column \u2018Emb. P.\u2019 represents \u2018Embedding Parameters\u2019. \u2018Downstream\u2019 stands for the average of MMLU-Var, Hellaswag, ARC-Challenge, ARC-Easy, and PIQA. For \u2018+OE\u2019 rows, we provide metric difference with blue labels.", "description": "\ubcf8 \ud45c\ub294 5000\uc5b5 \ud1a0\ud070\uc758 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec MoE(Mixture of Experts) \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c \uacfc\ub3c4\ud55c \uc778\ucf54\ub529(Over-Encoding) \uae30\ubc95\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'Emb. P.' \uc5f4\uc740 Embedding Parameter\uc758 \uc218\ub97c \ub098\ud0c0\ub0b4\uba70, 'Downstream' \uc5f4\uc740 MMLU-Var, Hellaswag, ARC-Challenge, ARC-Easy, PIQA \ub2e4\uc12f \uac00\uc9c0 \ud558\uc704 \uc791\uc5c5\uc758 \ud3c9\uade0 \uc131\ub2a5 \uc810\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  '+OE' \ud589\uc740 \uacfc\ub3c4\ud55c \uc778\ucf54\ub529\uc744 \uc801\uc6a9\ud588\uc744 \ub54c \uae30\uc900 \ubaa8\ub378 \ub300\ube44 \uc131\ub2a5 \ubcc0\ud654\ub97c \ud30c\ub780\uc0c9\uc73c\ub85c \ud45c\uc2dc\ud569\ub2c8\ub2e4.  \uc989, \uacfc\ub3c4\ud55c \uc778\ucf54\ub529 \uc801\uc6a9\uc73c\ub85c \uc778\ud55c \uc131\ub2a5 \ud5a5\uc0c1 \ub610\ub294 \uc800\ud558\ub97c \uc218\uce58\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "Id|Model|Train Loss \u2193|Train PPL \u2193|Eval Loss \u2193|Eval PPL \u2193|Downstream MMLU-V \u2191|Downstream HS \u2191|Downstream ARC-C \u2191|Downstream ARC-E \u2191|Downstream PIQA \u2191\n---|---|---|---|---|---|---|---|---|---|---\n1|OLMoE-1.3B|2.554|12.864|2.924|18.625|0.327|0.553|0.325|0.622|0.727\n2|+\ud835\udd3c<sup>3.2M\u00d7d</sup>(x<sup>(-2)</sup>)|2.511|12.319|2.887|17.944|0.340|0.569|0.351|0.656|0.734\n3|+\ud835\udd3c<sup>6.4M\u00d7d/2</sup>(x<sup>(-2)</sup>)|2.507|12.268|2.882|17.851|0.330|0.573|0.341|0.648|0.731\n4|+\ud835\udd3c<sup>3.2M\u00d7d|2</sup>(x<sup>(-2)</sup>)|2.503|12.221|2.877|17.754|0.337|0.575|0.345|0.651|0.740\n5|+\ud835\udd3c<sup>3.2M\u00d7d|4</sup>(x<sup>(-2)</sup>)|2.503|12.226|2.876|17.736|0.328|0.575|0.337|0.653|0.734\n6|+\u2211<sub>i\u2208{2,3}</sub>\ud835\udd3c<sub>i</sub><sup>3.2M\u00d7d/2|2</sup>(x<sup>(-i)</sup>)|2.495|12.127|2.870|17.638|0.340|0.578|0.330|0.636|0.738\n7|+\ud835\udd3c<sup>12.8M\u00d7d</sup>(x<sup>(-2)</sup>)|2.493|12.100|2.881|17.832|0.334|0.569|0.343|0.643|0.730\n8|+\u2211<sub>i\u2208{2,3}</sub>\ud835\udd3c<sub>i</sub><sup>12.8M\u00d7d/2|2</sup>(x<sup>(-i)</sup>)|2.472|11.854|2.862|17.494|0.342|0.577|0.329|0.645|0.728", "caption": "Table 2: Ablation study on different input vocabulary designs. The downstream tasks follow the eval settings in OLMoE, where MMLU-V stands for MMLU-Var, HS for Hellaswag, ARC-C for ARC-Challenge and ARC-E for ARC-Easy. All models are trained with 500B tokens.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \uc785\ub825 \uc5b4\ud718 \uc124\uacc4\uc5d0 \ub300\ud55c \ucd94\uac00 \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  OLMoE \ud3c9\uac00 \uc124\uc815\uc744 \ub530\ub77c MMLU-Var, Hellaswag, ARC-Challenge, ARC-Easy \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc758 \ud3c9\uade0 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \ubaa8\ub378\uc740 500B \ud1a0\ud070\uc73c\ub85c \ud559\uc2b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\uc11c\ub294 \uc785\ub825 \uc784\ubca0\ub529\uc5d0 \uc0ac\uc6a9\ub41c \ud1a0\ud070\uc758 n-gram \uad6c\uc131, \uc784\ubca0\ub529 \ucc28\uc6d0 \ubc0f \uc784\ubca0\ub529 \ub9e4\ud2b8\ub9ad\uc2a4\uc758 \ubd84\ud560 \uc218\uc5d0 \ub530\ub978 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uad6c\uc131\uc5d0 \ub530\ub978 \uc190\uc2e4, perplexity, \uadf8\ub9ac\uace0 \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5 \uc131\ub2a5 \uc9c0\ud45c(MMLU-V, HS, ARC-C, ARC-E, PIQA)\uac00 \uc81c\uc2dc\ub418\uc5b4, \uc5b4\ub5a4 \uc785\ub825 \uc5b4\ud718 \uc124\uacc4\uac00 \uac00\uc7a5 \ud6a8\uacfc\uc801\uc778\uc9c0 \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| 1-Gram | 2-Gram | 3-Gram | k | Loss\u2193 | PPL\u2193 |\n|---|---|---|---|---|---| \n| \u2713 | \u2717 | \u2717 | - | 2.714 | 15.094 |\n| \u2717 | \u2713 | \u2717 | 1 | 2.785 | 16.205 |\n| \u2713 | \u2713 | \u2717 | 1 | **2.678** | **14.555** |\n| \u2713 | \u2713 | \u2717 | 4 | 2.670 | 14.447 |\n| \u2713 | \u2717 | \u2713 | 4 | 2.684 | 14.642 |\n| \u2713 | \u2713 | \u2713 | 4 | **2.667** | **14.394** |", "caption": "Table 3: Ablation study for the hierarchical design of over-encoding. The symbol \u2018\u2713\u2019 on the n\ud835\udc5bnitalic_n-gram column denotes n\ud835\udc5bnitalic_n-gram token x(\u2212n)superscript\ud835\udc65\ud835\udc5bx^{(-n)}italic_x start_POSTSUPERSCRIPT ( - italic_n ) end_POSTSUPERSCRIPT is adopted. The experiments are conducted with m=3.2\u2062M\ud835\udc5a3.2Mm=3.2\\mathrm{M}italic_m = 3.2 roman_M, and the metrics are reported after training on 50B tokens.", "description": "\ud45c 3\uc740 \uacc4\uce35\uc801 \uacfc\uc789 \uc778\ucf54\ub529 \uc124\uacc4\uc5d0 \ub300\ud55c \ucd94\uac00 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\ub294 1-gram, 2-gram, 3-gram \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud55c \uc5ec\ub7ec \uac00\uc9c0 \uc2e4\ud5d8 \uc124\uc815\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.  '\u2713' \uae30\ud638\ub294 \ud574\ub2f9 n-gram \ud1a0\ud070 (x(-n))\uc774 \uc0ac\uc6a9\ub418\uc5c8\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ubaa8\ub4e0 \uc2e4\ud5d8\uc740 m=3.2M\uc73c\ub85c \uc9c4\ud589\ub418\uc5c8\uc73c\uba70, 50B \ud1a0\ud070\uc73c\ub85c \ud559\uc2b5\ud55c \ud6c4 \ud3c9\uac00 \uc9c0\ud45c\ub97c \ubcf4\uace0\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c n-gram \ud1a0\ud070 \uc870\ud569\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\uc5ec, \uacc4\uce35\uc801 \uc811\uadfc \ubc29\uc2dd\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Insights from Synthetic Experiments"}, {"content": "| Model | Loss \u2193 | PPL \u2193 | Eval Loss \u2193 | Eval PPL \u2193 |\n|---|---|---|---|---|\n| baseline | 2.714 | 15.094 | 3.094 | 22.060 |\n| +\ud835\udd3c<sup>64V\u00d7d</sup> | 2.702 | 14.892 | 3.077 | 21.710 |\n| +\ud835\udd3c<sup>3.2M\u00d7d</sup> | **2.678** | **14.555** | **3.054** | **21.202** |", "caption": "Table 4: Ablation study on hashing conflicts. Note the experiments are kept roughly the same vocabulary size, i.e. 64\u2062V\u22483.218\u2062M64\ud835\udc493.218M64V\\approx 3.218\\mathrm{M}64 italic_V \u2248 3.218 roman_M. The metrics are reported after training with 50B tokens.", "description": "\ud45c 4\ub294 \ud574\uc2f1 \ucda9\ub3cc\uc5d0 \ub300\ud55c \ucd94\uac00 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc758 \uc2e4\ud5d8\uc5d0\uc11c\ub294 \uc5b4\ud718 \ud06c\uae30\uac00 \uac70\uc758 \ub3d9\uc77c\ud558\uac8c \uc720\uc9c0\ub418\ub3c4\ub85d(64V \u2248 3.218M) \uc124\uc815\ub418\uc5c8\uc73c\uba70, \uac01 \ubaa8\ub378\uc740 500\uc5b5 \ud1a0\ud070\uc73c\ub85c \ud559\uc2b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ud559\uc2b5 \uc190\uc2e4, \ud3c9\uac00 \uc190\uc2e4, \uadf8\ub9ac\uace0 \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5(MMLU-Var, Hellaswag, ARC-Challenge, ARC-Easy, PIQA)\uc5d0 \ub300\ud55c \uc131\ub2a5 \uc9c0\ud45c\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub97c \ud1b5\ud574 \ud574\uc2f1 \ucda9\ub3cc\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uacfc \ub2e4\uc591\ud55c \uc5b4\ud718 \uad6c\uc131 \ubc29\uc2dd\uc758 \ud6a8\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.3 Over-Tokenized Transformer"}, {"content": "| Model | Loss \u2193 | Eval Loss \u2193 | Downstream \u2191 |\n|---|---|---|---|\n| baseline | 2.554 | 2.924 | 0.510 |\n| +MTP | 2.556 +0.002 | 2.925 +0.001 | 0.508 -0.002 |\n| +MTP-DS | 2.555 +0.001 | 2.926 +0.002 | 0.511 +0.001 |\n| OE-12.8M | 2.472 | 2.862 | 0.524 |\n| OT-12.8M | 2.481 +0.009 | 2.869 +0.007 | 0.537 +0.013 |", "caption": "Table 5: MTP Experiments on OLMoE-1.3B. The loss refers to the next one token prediction loss for MTP methods. Metric difference that improves baseline are marked blue while degrations are marked red.", "description": "\ud45c 5\ub294 OLMoE-1.3B \ubaa8\ub378\uc5d0 \ub300\ud55c \ub2e4\uc911 \ud1a0\ud070 \uc608\uce21(MTP) \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'Loss' \uc5f4\uc740 MTP \ubc29\ubc95\uc5d0 \ub300\ud55c \ub2e4\uc74c \ud1a0\ud070 \uc608\uce21 \uc190\uc2e4\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uae30\uc900 \ubaa8\ub378(baseline)\uc5d0 \ube44\ud574 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uac00\uc838\uc628 \uc9c0\ud45c \ucc28\uc774\ub294 \ud30c\ub780\uc0c9\uc73c\ub85c, \uc131\ub2a5 \uc800\ud558\ub97c \uac00\uc838\uc628 \uc9c0\ud45c \ucc28\uc774\ub294 \ube68\uac04\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc989, \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c MTP \ubc29\ubc95\uc744 \uc801\uc6a9\ud588\uc744 \ub54c\uc758 \uc190\uc2e4 \ubc0f \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5 \uc131\ub2a5 \ubcc0\ud654\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "4.3 Over-Tokenized Transformer"}, {"content": "| Model | Loss \u2193 | Eval Loss \u2193 | MMLU-Var \u2191 | Hellaswag \u2191 | Arc-Challenge \u2191 | Arc-Easy \u2191 | PIQA \u2191 |\n|---|---|---|---|---|---|---|---| \n| OLMoE-1.3B | 2.554 | 2.924 | 0.327 | **0.553** | 0.325 | 0.622 | 0.727 |\n| +OD \u03bb\u2082=0.1 | 2.549 | 2.920 | 0.325 | **0.553** | **0.331** | 0.610 | 0.721 |\n| +OD \u03bb\u2082=0.2 | 2.549 | **2.918** | 0.327 | 0.551 | 0.323 | **0.633** | **0.728** |\n| +OD \u03bb\u2082=0.5 | 2.549 | **2.918** | 0.325 | **0.553** | 0.308 | 0.619 | 0.727 |\n| +OD \u03bb\u2082=1.0 | 2.555 | 2.923 | **0.328** | 0.550 | 0.320 | 0.629 | 0.722 |\n| OLMoE-7B | 2.306 | **2.670** | 0.385 | **0.695** | **0.414** | **0.740** | 0.775 |\n| +OD \u03bb\u2082=0.1 | **2.304** | 2.672 | **0.387** | 0.691 | 0.409 | 0.724 | **0.776** |", "caption": "Table 6: Ablation study on loss weights for OD. The column downstream represents the average score of MMLU-Var, Hellaswag, ARC-Challenge, ARC-Easy and PIQA.", "description": "\ud45c 6\uc740 \uacfc\uc18c\ub514\ucf54\ub529(OD)\uc5d0\uc11c \uc190\uc2e4 \uac00\uc911\uce58\uc5d0 \ub300\ud55c \ucd94\uac00 \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc190\uc2e4 \uac00\uc911\uce58(\u03bb2)\ub97c \uc0ac\uc6a9\ud558\uc5ec OLMOE-1.3B \ubc0f OLMOE-7B \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ucf30\uace0, \ud3c9\uac00 \uc190\uc2e4, MMLU-Var, Hellaswag, ARC-Challenge, ARC-Easy \ubc0f PIQA\uc758 \ud3c9\uade0 \uc810\uc218\ub97c \ud3ec\ud568\ud55c \uc5ec\ub7ec \uc9c0\ud45c\ub97c \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ucd5c\uc801\uc758 \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\uae30 \uc704\ud55c \uc190\uc2e4 \uac00\uc911\uce58\uc758 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.3 \uacfc\uc18c \ud1a0\ud070\ud654 \ubcc0\ud658\uae30"}]