[{"figure_path": "https://arxiv.org/html/2501.16975/x1.png", "caption": "Figure 1: Scaling trend for Over-Encoded models and baselines on OLMo2. We plot the loss with 400B tokens\u2019 training. For over-encoding, input vocabulary size is extended from 0.1 to 1.2 and 12.8 million (12\u00d712\\times12 \u00d7 and 128\u00d7128\\times128 \u00d7 larger than baseline), referred to as OE-1.2M and OE-12.8M. We observe OE-12.8M with 400M parameters matches the baseline with 1B parameters.", "description": "\uadf8\ub9bc 1\uc740 OLMo2 \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c \uacfc\ub3c4\ud558\uac8c \ud1a0\ud070\ud654\ub41c(Over-Encoded) \ubaa8\ub378\uacfc \uae30\uc900 \ubaa8\ub378\uc758 \uc131\ub2a5 \ucd94\uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 4000\uc5b5 \uac1c\uc758 \ud1a0\ud070\uc744 \ud559\uc2b5 \ub370\uc774\ud130\ub85c \uc0ac\uc6a9\ud588\uc73c\uba70, \uacfc\ub3c4\ud55c \ud1a0\ud070\ud654\ub294 \uc785\ub825 \uc5b4\ud718 \ud06c\uae30\ub97c \uae30\uc900 \ubaa8\ub378 \ub300\ube44 12\ubc30(OE-1.2M) \ubc0f 128\ubc30(OE-12.8M)\ub85c \ud655\uc7a5\ud55c \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uc67c\ucabd \uadf8\ub9bc\uc740 \ubaa8\ub378 \ud30c\ub77c\ubbf8\ud130 \uc218\uc5d0 \ub530\ub978 \uc190\uc2e4\uc744, \uc624\ub978\ucabd \uadf8\ub9bc\uc740 \uc5b4\ud718 \ud06c\uae30\uc5d0 \ub530\ub978 \uc190\uc2e4\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud2b9\ud788, 4\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uac00\uc9c4 OE-12.8M \ubaa8\ub378\uc774 10\uc5b5 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uac00\uc9c4 \uae30\uc900 \ubaa8\ub378\uacfc \uc720\uc0ac\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc785\ub825 \uc5b4\ud718 \ud06c\uae30 \ud655\uc7a5\uc774 \ubaa8\ub378 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ud6a8\uacfc\uc801\uc784\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.16975/x2.png", "caption": "Figure 2: Performance comparison for models trained on CFG data. The left panel compares 1-gram and 3-gram tokenizers, showing that 3-gram improves larger (85M parameters) models but harms smaller (2.4M parameters) ones. The right panel examines 3-gram usage in encoders and decoders, revealing consistent gains with 3-gram encoders regardless of model size, while 3-gram decoders degrade performance in smaller models.", "description": "\uadf8\ub9bc 2\ub294 CFG \ub370\uc774\ud130\ub85c \ud559\uc2b5\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd \ud328\ub110\uc740 1-gram\uacfc 3-gram \ud1a0\ud06c\ub098\uc774\uc800\ub97c \ube44\uad50\ud558\uc5ec, 3-gram \ud1a0\ud06c\ub098\uc774\uc800\uac00 \ud070 \ubaa8\ub378(85M \ud30c\ub77c\ubbf8\ud130)\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uc9c0\ub9cc \uc791\uc740 \ubaa8\ub378(2.4M \ud30c\ub77c\ubbf8\ud130)\uc758 \uc131\ub2a5\uc740 \uc800\ud558\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc624\ub978\ucabd \ud328\ub110\uc740 \uc778\ucf54\ub354\uc640 \ub514\ucf54\ub354\uc5d0\uc11c 3-gram \uc0ac\uc6a9\uc744 \ubd84\uc11d\ud558\uc5ec, \ubaa8\ub378 \ud06c\uae30\uc5d0 \uad00\uacc4\uc5c6\uc774 3-gram \uc778\ucf54\ub354\ub294 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uac00\uc838\uc624\uc9c0\ub9cc 3-gram \ub514\ucf54\ub354\ub294 \uc791\uc740 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc800\ud558\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \ud1a0\ud06c\ub098\uc774\uc800\uc758 \uadf8\ub7a8 \uc218\ub97c \ub298\ub9ac\ub294 \uac83\uc774 \ud56d\uc0c1 \uc88b\uc740 \uac83\uc740 \uc544\ub2c8\uba70, \ubaa8\ub378\uc758 \ud06c\uae30\uc5d0 \ub530\ub77c \uc801\uc808\ud55c \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc120\ud0dd\ud574\uc57c \ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3.1. \ud569\uc131 \uc2e4\ud5d8 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2501.16975/x3.png", "caption": "Figure 3: Illustration of 2-gram encoding/decoding GPT. Note that 2-gram decoding only preserves the predicted next 1 token though next 2 is predicted, which keeps inference cost identical to the vanilla model.", "description": "\uc774 \uadf8\ub9bc\uc740 2-gram \uc778\ucf54\ub529 \ubc0f \ub514\ucf54\ub529\uc744 \uc0ac\uc6a9\ud558\ub294 GPT \ubaa8\ub378\uc758 \uc791\ub3d9 \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\uc874\uc758 GPT \ubaa8\ub378\uacfc \ub2ec\ub9ac, \uc785\ub825 \ud1a0\ud070\uc744 2-gram \ub2e8\uc704\ub85c \uc778\ucf54\ub529\ud558\uc5ec \ubaa8\ub378\uc774 \ub354\uc6b1 \uae34 \ubb38\ub9e5\uc744 \uace0\ub824\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.  \ub514\ucf54\ub529 \uacfc\uc815\uc5d0\uc11c\ub294 \ub2e4\uc74c \ub450 \uac1c\uc758 \ud1a0\ud070\uc744 \uc608\uce21\ud558\uc9c0\ub9cc, \uc2e4\uc81c\ub85c\ub294 \ub2e4\uc74c \ud55c \uac1c\uc758 \ud1a0\ud070\ub9cc \uc720\uc9c0\ud558\uc5ec \ucd94\ub860 \ube44\uc6a9\uc744 \uae30\uc874 \ubaa8\ub378\uacfc \ub3d9\uc77c\ud558\uac8c \uc720\uc9c0\ud569\ub2c8\ub2e4.  \uc774\ub294 2-gram \uc778\ucf54\ub529\uc744 \ud1b5\ud574 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub3d9\uc2dc\uc5d0, \ucd94\ub860 \uc18d\ub3c4 \uc800\ud558\ub97c \ubc29\uc9c0\ud558\ub294 \ud6a8\uc728\uc801\uc778 \ubc29\ubc95\uc784\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.16975/x4.png", "caption": "Figure 4: Training curves for OE-12.8M and baseline model on OLMo2-1B. The metrics are smoothed via exponential moving average with weight 0.99 for loss and 0.9 for downstream tasks. We observe significant convergence acceleration for the OE model: 5.7\u00d75.7\\times5.7 \u00d7 on loss, 3.2\u00d73.2\\times3.2 \u00d7 on MMLU-Var, 3.0\u00d73.0\\times3.0 \u00d7 on Hellaswag, 2.6\u00d72.6\\times2.6 \u00d7 on ARC-Challenge, 3.1\u00d73.1\\times3.1 \u00d7 on ARC-Easy and 3.9\u00d73.9\\times3.9 \u00d7 on PIQA.", "description": "\uadf8\ub9bc 4\ub294 OLMo2-1B \ubaa8\ub378\uc5d0 \ub300\ud574 OE-12.8M \ubaa8\ub378\uacfc \uae30\uc900 \ubaa8\ub378\uc758 \ud559\uc2b5 \uace1\uc120\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc190\uc2e4\uc5d0 \ub300\ud574\uc11c\ub294 0.99\uc758 \uac00\uc911\uce58\ub97c \uc0ac\uc6a9\ud55c \uc9c0\uc218 \uc774\ub3d9 \ud3c9\uade0\uc744 \ud1b5\ud574, \ud558\uc704 \uc791\uc5c5\uc5d0 \ub300\ud574\uc11c\ub294 0.9\uc758 \uac00\uc911\uce58\ub97c \uc0ac\uc6a9\ud55c \uc9c0\uc218 \uc774\ub3d9 \ud3c9\uade0\uc744 \ud1b5\ud574 \uba54\ud2b8\ub9ad\uc744 \ubd80\ub4dc\ub7fd\uac8c \ucc98\ub9ac\ud588\uc2b5\ub2c8\ub2e4. OE \ubaa8\ub378\uc740 \uc190\uc2e4\uc774 5.7\ubc30, MMLU-Var\uac00 3.2\ubc30, Hellaswag\uac00 3.0\ubc30, ARC-Challenge\uac00 2.6\ubc30, ARC-Easy\uac00 3.1\ubc30, PIQA\uac00 3.9\ubc30 \ud5a5\uc0c1\ub418\ub294 \ub4f1 \uc218\ub834 \uc18d\ub3c4\uac00 \ud06c\uac8c \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.16975/x5.png", "caption": "Figure 5: Log-linear relationship is observed between vocabulary size m\ud835\udc5amitalic_m and training loss \u2112\u2112\\mathcal{L}caligraphic_L, i.e. \u2112=2.6754\u22120.0256\u00d7log10\u2061m\u21122.67540.0256subscript10\ud835\udc5a\\mathcal{L}=2.6754-0.0256\\times\\log_{10}{m}caligraphic_L = 2.6754 - 0.0256 \u00d7 roman_log start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT italic_m. The values are collected with 500B tokens\u2019 training on OLMoE-1.3B models.", "description": "\uadf8\ub9bc 5\ub294 OLMoE-1.3B \ubaa8\ub378\uc744 5000\uc5b5 \ud1a0\ud070\uc73c\ub85c \ud559\uc2b5\uc2dc\ud0a8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc785\ub825 \uc5b4\ud718 \uc0ac\uc804\uc758 \ud06c\uae30(m)\uc640 \ud559\uc2b5 \uc190\uc2e4(L) \uc0ac\uc774\uc758 \uad00\uacc4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ub85c\uadf8-\uc120\ud615 \uad00\uacc4\uac00 \uad00\ucc30\ub418\ub294\ub370, \uc774\ub294 \uc5b4\ud718 \uc0ac\uc804 \ud06c\uae30\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ud559\uc2b5 \uc190\uc2e4\uc774 \uc120\ud615\uc801\uc73c\ub85c \uac10\uc18c\ud568\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uc218\uc2dd L = 2.6754 - 0.0256 * log\u2081\u2080(m)\uc740 \uc774\ub7ec\ud55c \uad00\uacc4\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \uc785\ub825 \uc5b4\ud718 \uc0ac\uc804 \ud06c\uae30\uc758 \ud655\uc7a5\uc774 \ubaa8\ub378 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc911\uc694\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.16975/extracted/6161742/figures/cfg_example.png", "caption": "Figure 6: Left Panel: CFG rules used in our experiments; Right Panel: an example of the generated sequences using the rules. This figure is taken from (Allen-Zhu & Li, 2024).", "description": "\uc774 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc758 3.1\uc808 \"\ud569\uc131 \uc2e4\ud5d8 \ub370\uc774\ud130\uc5d0\uc11c\uc758 \ud1b5\ucc30\"\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ubb38\ub9e5 \uc790\uc720 \ubb38\ubc95(CFG)\uc5d0 \ub300\ud55c \uc124\uba85\uc785\ub2c8\ub2e4. \uc67c\ucabd \ud328\ub110\uc740 \uc2e4\ud5d8\uc5d0 \uc0ac\uc6a9\ub41c CFG \uaddc\uce59\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc624\ub978\ucabd \ud328\ub110\uc740 \uc774 \uaddc\uce59\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ub41c \uc2dc\ud000\uc2a4\uc758 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 Allen-Zhu & Li (2024) \ub17c\ubb38\uc5d0\uc11c \uac00\uc838\uc654\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc740 CFG \uaddc\uce59\uc758 \uad6c\uc870\uc640 \uc774\ub97c \ud1b5\ud574 \uc0dd\uc131\ub418\ub294 \uc2dc\ud000\uc2a4\uc758 \ud2b9\uc9d5\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90c\uc73c\ub85c\uc368, \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ud569\uc131 \ub370\uc774\ud130\uc758 \ud2b9\uc9d5\uc744 \uc774\ud574\ud558\ub294\ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "3.1 \ud569\uc131 \uc2e4\ud5d8 \ub370\uc774\ud130\uc5d0\uc11c\uc758 \ud1b5\ucc30"}, {"figure_path": "https://arxiv.org/html/2501.16975/x6.png", "caption": "Figure 9: All metrics for OLMo2-1B, comparing OE-12.8M and baseline.", "description": "\uadf8\ub9bc 9\ub294 OLMo2-1B \ubaa8\ub378\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ud3c9\uac00 \uc9c0\ud45c\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  OE-12.8M(Over-Encoding 12.8M) \ubaa8\ub378\uacfc \uae30\uc900 \ubaa8\ub378(baseline)\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec, \uacfc\ub3c4\ud55c \ud1a0\ud070\ud654(Over-Tokenization) \uae30\ubc95\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ub2e4\uac01\uc801\uc73c\ub85c \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c\ub294, perplexity, \uc190\uc2e4 \ud568\uc218 \uac12(loss)\uacfc \uc5ec\ub7ec \ud558\ub958 \uc791\uc5c5(downstream task)\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uacfc\ub3c4\ud55c \ud1a0\ud070\ud654 \uae30\ubc95\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc9c0\ud45c\ub294 \ud559\uc2b5 \uacfc\uc815 \uc804\ubc18\uc5d0 \uac78\uccd0 \ucd94\uc801\ub418\uc5b4, \uc2dc\uac04\uc5d0 \ub530\ub978 \uc131\ub2a5 \ubcc0\ud654\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.16975/x7.png", "caption": "Figure 11: All metrics for OLMoE-1.3B, comparing OE-12.8M and baseline.", "description": "\uadf8\ub9bc 11\uc740 OLMoE-1.3B \ubaa8\ub378\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ud3c9\uac00 \uc9c0\ud45c\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. OE-12.8M (Over-Encoding \ubaa8\ub378, 1280\ub9cc \uac1c\uc758 \uc784\ubca0\ub529 \ud30c\ub77c\ubbf8\ud130)\uacfc \uae30\uc900 \ubaa8\ub378(baseline)\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec, \uac01 \uc9c0\ud45c(\uc608: \uc190\uc2e4, \ud37c\ud50c\ub809\uc11c\ud2f0, \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5 \uc131\ub2a5 \ub4f1)\uc5d0 \ub300\ud55c \ucc28\uc774\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \uacfc\ub3c4\ud55c \ud1a0\ud070\ud654(Over-Tokenization) \uae30\ubc95\uc774 \ubaa8\ub378\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc885\ud569\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2501.16975/x8.png", "caption": "Figure 12: All metrics for OLMoE-7B, comparing OE-12.8M and baseline.", "description": "\uadf8\ub9bc 12\ub294 OLMoE-7B \ubaa8\ub378\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ud3c9\uac00 \uc9c0\ud45c\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. OE-12.8M(Over-Encoding \ubaa8\ub378, \uc785\ub825 \uc5b4\ud718 \ud06c\uae30 1280\ub9cc)\uacfc \uae30\uc900 \ubaa8\ub378(baseline)\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uac01 \uc9c0\ud45c\ubcc4 \ucd94\uc138(training loss, perplexity, downstream task \uc131\ub2a5 \ub4f1)\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uacfc\ub3c4\ud55c \ud1a0\ud070\ud654(Over-Tokenization) \uae30\ubc95\uc774 OLMoE-7B \ubaa8\ub378\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud558\uace0 \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.16975/x9.png", "caption": "Figure 13: All metrics for OLMoE-1.3B, comparing OT-12.8 and OE-12.8M.", "description": "\uadf8\ub9bc 13\uc740 OLMoE-1.3B \ubaa8\ub378\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ud3c9\uac00 \uc9c0\ud45c\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  OT-12.8M (Over-Tokenized Transformer with 12.8M embedding parameters)\uacfc OE-12.8M (Over-Encoded Transformer with 12.8M embedding parameters)\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \ub450 \ubaa8\ub378\uc758 \ucc28\uc774\uc810\uacfc \uac01 \ubaa8\ub378\uc758 \uac15\uc810\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub2e4\uc591\ud55c \uc9c0\ud45c\ub4e4\uc758 \ucd94\uc774\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uac01 \uc9c0\ud45c\ub294 \ubaa8\ub378 \ud559\uc2b5 \uc9c4\ud589\uc5d0 \ub530\ub978 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\ub85c \ud45c\ud604\ub418\uc5b4, \ubaa8\ub378 \uc131\ub2a5 \ud5a5\uc0c1 \ubc0f \ud559\uc2b5 \uc548\uc815\uc131 \uce21\uba74\uc5d0\uc11c OT-12.8M\uacfc OE-12.8M\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uc790\uc138\ud788 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud558\ub958 \uc791\uc5c5(downstream tasks)\uc758 \uc131\ub2a5\uc744 \ud3ec\ud568\ud558\uc5ec \ubaa8\ub378\uc758 \uc804\ubc18\uc801\uc778 \uc131\ub2a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}]