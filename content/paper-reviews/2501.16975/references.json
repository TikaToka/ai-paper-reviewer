{"references": [{"fullname_first_author": "A. Pagnoni", "paper_title": "Byte Latent Transformer: Patches scale better than tokens", "publication_date": "2024-12-00", "reason": "This paper introduces a novel tokenizer design that improves performance and efficiency in large language models, which is directly relevant to the core idea of the main paper."}, {"fullname_first_author": "T. Tao", "paper_title": "Scaling laws with vocabulary: Larger models deserve larger vocabularies", "publication_date": "2024-07-00", "reason": "This paper empirically studies the relationship between vocabulary size and model performance, providing theoretical support for the main paper's focus on vocabulary scaling."}, {"fullname_first_author": "F. Gloeckle", "paper_title": "Better & faster large language models via multi-token prediction", "publication_date": "2024-00-00", "reason": "This paper proposes a multi-token prediction method that improves the efficiency and performance of large language models, which is directly related to the over-decoding technique in the main paper."}, {"fullname_first_author": "J. Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-00", "reason": "This paper introduces the concept of scaling laws in neural language models, which forms the basis for understanding the scaling behavior of over-tokenized transformers."}, {"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduces the transformer architecture, which is the foundation upon which many large language models, including the over-tokenized transformers in this paper, are built."}]}