[{"figure_path": "https://arxiv.org/html/2502.01456/x1.png", "caption": "Figure 1: Overall math performance. Eurus-2-7B-PRIME excels at competition-level mathematics benchmarks, outperforming advanced math models and larger models. Notably, PRIME brings substantial performance gain (+16.7%) over Eurus-2-7B-SFT.", "description": "\uadf8\ub9bc 1\uc740 \uba87\uba87 \uacbd\uc7c1 \uc218\uc900 \uc218\ud559 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c Eurus-2-7B-PRIME\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Eurus-2-7B-PRIME\uc740 \uae30\uc874\uc758 \ucd5c\ucca8\ub2e8 \uc218\ud559 \ubaa8\ub378 \ubc0f \ub300\uaddc\ubaa8 \ubaa8\ub378\ub4e4\uc744 \ub2a5\uac00\ud558\ub294 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uba70, \ud2b9\ud788 Eurus-2-7B-SFT \ub300\ube44 16.7%\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc785\ub2c8\ub2e4. \uc774\ub294 PRIME\uc774 \ubcf5\uc7a1\ud55c \ub2e4\ub2e8\uacc4 \ucd94\ub860\uc774 \ud544\uc694\ud55c \uacfc\uc81c\uc5d0\uc11c\ub3c4 \ud6a8\uacfc\uc801\uc73c\ub85c \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uadf8\ub798\ud504\ub294 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c(AIME 2024, AMC, Minerva Math, OlympiadBench, MATH-500)\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2502.01456/x2.png", "caption": "Figure 2: Illustration of PRIME. PRIME follows that (1) initialize policy model and the Implicit PRM both with the reference model; (2) sample multiple responses for each prompt and filter with output accuracy; (3) obtain implicit process rewards by the Implicit PRM and update it using cross-entropy (CE) loss; (4) compute advantage and policy loss then update the policy model.", "description": "\uadf8\ub9bc 2\ub294 PRIME\uc758 \uc791\ub3d9 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  PRIME\uc740 (1) \uc815\ucc45 \ubaa8\ub378\uacfc \uc554\uc2dc\uc801 PRM\uc744 \ubaa8\ub450 \ucc38\uc870 \ubaa8\ub378\ub85c \ucd08\uae30\ud654\ud558\uace0, (2) \uac01 \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud574 \uc5ec\ub7ec \uc751\ub2f5\uc744 \uc0d8\ud50c\ub9c1\ud558\uace0 \ucd9c\ub825 \uc815\ud655\ub3c4 \ud544\ud130\ub97c \uc801\uc6a9\ud558\uba70, (3) \uc554\uc2dc\uc801 PRM\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc554\uc2dc\uc801 \uacfc\uc815 \ubcf4\uc0c1\uc744 \uc5bb\uace0 \uad50\ucc28 \uc5d4\ud2b8\ub85c\ud53c(CE) \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5c5\ub370\uc774\ud2b8\ud558\uace0, (4) \uc774\uc810\uacfc \uc815\ucc45 \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uc5ec \uc815\ucc45 \ubaa8\ub378\uc744 \uc5c5\ub370\uc774\ud2b8\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \ub3d9\uc791\ud569\ub2c8\ub2e4.", "section": "3 PRIME"}, {"figure_path": "https://arxiv.org/html/2502.01456/x3.png", "caption": "Figure 3: Impact of online prompt filtering on training rewards.", "description": "\uadf8\ub9bc 3\uc740 \uc628\ub77c\uc778 \ud504\ub86c\ud504\ud2b8 \ud544\ud130\ub9c1\uc774 \ud6c8\ub828 \ubcf4\uc0c1\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc628\ub77c\uc778 \ud504\ub86c\ud504\ud2b8 \ud544\ud130\ub9c1\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0\uc640 \ube44\uad50\ud558\uc5ec, \ud544\ud130\ub9c1\uc744 \uc801\uc6a9\ud588\uc744 \ub54c \ubcf4\uc0c1\uc758 \ubd84\uc0b0\uc774 \ud06c\uac8c \uac10\uc18c\ud558\uace0 \ubcf4\uc0c1\uc758 \uc548\uc815\uc131\uc774 \uc99d\uac00\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ud6c8\ub828 \uacfc\uc815\uc5d0\uc11c \ub354 \uc548\uc815\uc801\uc774\uace0 \ud6a8\uc728\uc801\uc778 \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3.3 \uae30\ud0c0 \uae30\uc220"}, {"figure_path": "https://arxiv.org/html/2502.01456/x4.png", "caption": "(a) Outcome training rewards (10-step moving).", "description": "\uc774 \uadf8\ub9bc\uc740 10\ub2e8\uacc4 \uc774\ub3d9 \ud3c9\uade0\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5d0\ud53c\uc18c\ub4dc \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c\uc758 \ub204\uc801 \ubcf4\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  PRIME \uc54c\uace0\ub9ac\uc998\uacfc \uae30\uc900 \uc54c\uace0\ub9ac\uc998(RLOO w/ OV Only)\uc758 \ud559\uc2b5 \uace1\uc120\uc744 \ube44\uad50\ud558\uc5ec PRIME\uc758 \ud559\uc2b5 \ud6a8\uc728\uc131\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  PRIME\uc774 RLOO w/ OV Only \ubcf4\ub2e4 \ud6e8\uc52c \ube60\ub974\uac8c \uc218\ub834\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.4 DENSE REWARDS V.S. SPARSE REWARDS"}, {"figure_path": "https://arxiv.org/html/2502.01456/x5.png", "caption": "(b) Test accuracy across different gradient steps.", "description": "\uadf8\ub9bc (b)\ub294 \ub2e4\uc591\ud55c \uacbd\uc0ac\ub3c4 \ub2e8\uacc4\uc5d0\uc11c\uc758 \ud14c\uc2a4\ud2b8 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \uac15\ud654 \ud559\uc2b5 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\ub294 \ub3d9\uc548 \uc5ec\ub7ec \ub2e8\uacc4\uc5d0\uc11c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  x\ucd95\uc740 \uacbd\uc0ac\ub3c4 \ub2e8\uacc4(gradient step)\uc758 \uc218\ub97c, y\ucd95\uc740 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \ud6c8\ub828 \uacfc\uc815\uc5d0\uc11c \ubaa8\ub378\uc758 \uc131\ub2a5 \ubcc0\ud654 \ucd94\uc774\ub97c \ud30c\uc545\ud558\uace0, \ucd5c\uc801\uc758 \ud6c8\ub828 \ub2e8\uacc4\ub97c \ud655\uc778\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "4.4 DENSE REWARDS V.S. SPARSE REWARDS"}, {"figure_path": "https://arxiv.org/html/2502.01456/x6.png", "caption": "Figure 4: \nThe effect of dense reward. We compare PRIME and RLOO with outcome verifier (OV). Dense rewards in PRIME lead to 2.5\u00d72.5\\times2.5 \u00d7 sample efficiency and 6.9%percent6.96.9\\%6.9 % performance improvement. PRIME also substantially outperforms RLOO on downstream tasks.", "description": "\uadf8\ub9bc 4\ub294 PRIME\uacfc RLOO(Outcome Verifier \uc0ac\uc6a9)\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \ubc00\uc9d1 \ubcf4\uc0c1(dense reward)\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. PRIME\uc740 RLOO\uc5d0 \ube44\ud574 \uc0d8\ud50c \ud6a8\uc728\uc131\uc774 2.5\ubc30 \ud5a5\uc0c1\ub418\uc5c8\uace0, \uc131\ub2a5\uc774 6.9% \ud5a5\uc0c1\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  PRIME\uc740 \ud6c4\uc18d \uc791\uc5c5(downstream tasks)\uc5d0\uc11c\ub3c4 RLOO\ubcf4\ub2e4 \ud6e8\uc52c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4.  \uc989, \ubc00\uc9d1 \ubcf4\uc0c1\uc744 \uc0ac\uc6a9\ud55c PRIME\uc774 \ud76c\uc18c \ubcf4\uc0c1(sparse reward)\uc744 \uc0ac\uc6a9\ud55c RLOO\ubcf4\ub2e4 \ud6e8\uc52c \ud6a8\uc728\uc801\uc774\uace0 \uc131\ub2a5\uc774 \ub6f0\uc5b4\ub098\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "4.4 DENSE REWARDS V.S. SPARSE REWARDS"}, {"figure_path": "https://arxiv.org/html/2502.01456/x7.png", "caption": "(a) Outcome training rewards (10-step moving).", "description": "\uc774 \uadf8\ub9bc\uc740 10\ub2e8\uacc4 \uc774\ub3d9 \ud3c9\uade0\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828 \ubcf4\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, \uac01 \uc9c0\uc810\uc740 \uc774\uc804 10\ub2e8\uacc4\uc758 \ubcf4\uc0c1 \ud3c9\uade0\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub294 \ubcf4\uc0c1 \uc2e0\ud638\uc758 \ubcc0\ub3d9\uc131\uc744 \uc904\uc774\uace0 \ubcf4\ub2e4 \uc548\uc815\uc801\uc778 \ucd94\uc138\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  \uadf8\ub9bc\uc740 \ud6c8\ub828 \uacfc\uc815\uc5d0\uc11c \ubcf4\uc0c1\uc774 \uc5b4\ub5bb\uac8c \ubcc0\ud654\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\uc5b4, \ubaa8\ub378\uc774 \uc2dc\uac04\uc774 \uc9c0\ub0a8\uc5d0 \ub530\ub77c \ud559\uc2b5\ud558\ub294 \ubc29\uc2dd\uc744 \uc774\ud574\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.  \ubcf4\uc0c1\uc774 \ub192\uc744\uc218\ub85d \ubaa8\ub378 \uc131\ub2a5\uc774 \uc88b\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "4.4 DENSE REWARDS V.S. SPARSE REWARDS"}, {"figure_path": "https://arxiv.org/html/2502.01456/x8.png", "caption": "(b) Test accuracy across different gradient steps.", "description": "\uadf8\ub9bc (b)\ub294 \uc5ec\ub7ec \ucd5c\uc801\ud654 \ub2e8\uacc4\uc5d0 \ub530\ub978 \ud14c\uc2a4\ud2b8 \uc815\ud655\ub3c4 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ucd5c\uc801\ud654 \ub2e8\uacc4\uc5d0\uc11c \ubaa8\ub378 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uc5ec, \ud559\uc2b5 \uacfc\uc815\uc758 \ud6a8\uc728\uc131 \ubc0f \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ud655\uc778\ud569\ub2c8\ub2e4.  PRIME \ubaa8\ub378\uc774 \ub2e4\ub978 \ucd5c\uc801\ud654 \ubc29\ubc95\ubcf4\ub2e4 \uc5bc\ub9c8\ub098 \ube60\ub974\uac8c \uc218\ub834\ud558\uace0 \ub192\uc740 \uc815\ud655\ub3c4\uc5d0 \ub3c4\ub2ec\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc9c0\ud45c\uc785\ub2c8\ub2e4.", "section": "4.4 DENSE REWARDS V.S. SPARSE REWARDS"}, {"figure_path": "https://arxiv.org/html/2502.01456/extracted/6173597/figures/images/policy_ref.png", "caption": "Figure 5: Comparison of different PRMs. Online PRM initialized from SFT model achieved the best results. Surprisingly, using PRMs trained on extra rollouts\nhurts the performance in both online and offline settings.", "description": "\uadf8\ub9bc 5\ub294 \ub2e4\uc591\ud55c \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ub41c \ubcf4\uc0c1 \ubaa8\ub378(PRM)\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  SFT(Supervised Fine-Tuning) \ubaa8\ub378\ub85c \ucd08\uae30\ud654\ub41c \uc628\ub77c\uc778 PRM\uc774 \uac00\uc7a5 \uc88b\uc740 \uacb0\uacfc\ub97c \ubcf4\uc600\uc2b5\ub2c8\ub2e4. \ub180\ub78d\uac8c\ub3c4, \ucd94\uac00\uc801\uc778 \ub864\uc544\uc6c3 \ub370\uc774\ud130\ub85c \uc0ac\uc804 \ud6c8\ub828\ub41c PRM\uc744 \uc0ac\uc6a9\ud558\uba74 \uc628\ub77c\uc778 \ubc0f \uc624\ud504\ub77c\uc778 \uc124\uc815 \ubaa8\ub450\uc5d0\uc11c \uc131\ub2a5\uc774 \uc800\ud558\ub418\ub294 \uac83\uc744 \ud655\uc778\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ucd94\uac00 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud55c \uc0ac\uc804 \ud6c8\ub828\uc774 \uc2e4\uc81c RL \ud658\uacbd\uc5d0 \uc801\ud569\ud558\uc9c0 \uc54a\uc740 \uacfc\uc801\ud569\ub41c \ubaa8\ub378\uc744 \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4. \uc628\ub77c\uc778 \ud559\uc2b5\uc744 \ud1b5\ud574 PRM\uc744 \uc9c0\uc18d\uc801\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8\ud558\ub294 \uac83\uc774 \uacfc\uc801\ud569\uc744 \ubc29\uc9c0\ud558\uace0 \ucd5c\uc801\uc758 \uc131\ub2a5\uc744 \uc720\uc9c0\ud558\ub294 \ub370 \uc911\uc694\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.01456/extracted/6173597/figures/images/sfr_ref.png", "caption": "Figure 6: Impact of PRM online update. The offline PRM is gradully been overoptimized while online PRMs achieve higher accuracy throughout training.", "description": "\uadf8\ub9bc 6\uc740 PRIME \ubaa8\ub378\uc758 \ud575\uc2ec \uad6c\uc131 \uc694\uc18c\uc778 \uc554\ubb35\uc801 \ud504\ub85c\uc138\uc2a4 \ubcf4\uc0c1 \ubaa8\ub378(Implicit PRM)\uc758 \uc628\ub77c\uc778 \uc5c5\ub370\uc774\ud2b8\uac00 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc628\ub77c\uc778\uc73c\ub85c PRM\uc744 \uc5c5\ub370\uc774\ud2b8\ud558\uba74 \uc624\ubc84\uc635\ud2f0\ub9c8\uc774\uc81c\uc774\uc158(\uacfc\uc801\ud569)\uc744 \ubc29\uc9c0\ud558\uace0 \ud6c8\ub828 \uc804 \uacfc\uc815\uc5d0\uc11c \ub192\uc740 \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubc18\uba74 \uc624\ud504\ub77c\uc778\uc73c\ub85c PRM\uc744 \ud6c8\ub828\uc2dc\ud0a4\uba74 \ucc98\uc74c\uc5d0\ub294 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ubcf4\uc774\uc9c0\ub9cc, \ud6c8\ub828\uc774 \uc9c4\ud589\ub420\uc218\ub85d \uc815\ud655\ub3c4\uac00 \uc810\ucc28 \uac10\uc18c\ud558\ub294 \uacfc\uc801\ud569 \ud604\uc0c1\uc774 \ubc1c\uc0dd\ud569\ub2c8\ub2e4. \uc774\ub294 \uc628\ub77c\uc778 \uc5c5\ub370\uc774\ud2b8\ub97c \ud1b5\ud574 PRM\uc774 \uc815\ucc45 \ubaa8\ub378\uc758 \ubcc0\ud654\uc5d0 \uc801\uc751\ud558\uc5ec \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3 PRIME"}, {"figure_path": "https://arxiv.org/html/2502.01456/x9.png", "caption": "(a) Policy ref: We use the policy logprob as \u03c0refsubscript\ud835\udf0bref\\pi_{\\text{ref}}italic_\u03c0 start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT for PRM.", "description": "\uc774 \uadf8\ub9bc\uc740 PRIME \ubaa8\ub378\uc758 \ub450 \uac00\uc9c0 \ub2e4\ub978 \ucc38\uc870 \uc815\ucc45 \uad6c\ud604 \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \ud604\uc7ac \uc815\ucc45\uc758 \ub85c\uadf8 \ud655\ub960\uc744 PRM\uc758 \ucc38\uc870 \ubaa8\ub378 \u03c0ref\ub85c \uc0ac\uc6a9\ud558\ub294 \ubc18\uba74, (b)\ub294 \ucd08\uae30 SFT \ubaa8\ub378\uc744 \ucc38\uc870 \ubaa8\ub378\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.  (a)\uc758 \uacbd\uc6b0,  PRM\uc740 \ud604\uc7ac \uc815\ucc45\uc758 \ub85c\uadf8 \ud655\ub960\uc744 \uae30\ubc18\uc73c\ub85c \ud6c8\ub828\ub418\uace0, (b)\uc758 \uacbd\uc6b0 \ucd08\uae30 SFT \ubaa8\ub378\uc744 \uae30\uc900\uc73c\ub85c KL \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uc5ec PRM\uc744 \ud6c8\ub828\ud569\ub2c8\ub2e4. \ub450 \uac00\uc9c0 \ubc29\uc2dd\uc758 \ucc28\uc774\uc810\uacfc \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "5.2 \ucc38\uc870 \ubaa8\ub378 \uc120\ud0dd\uc758 \uc720\uc5f0\uc131"}, {"figure_path": "https://arxiv.org/html/2502.01456/x10.png", "caption": "(b) SFT ref: We retain the initial policy to provide \u03c0refsubscript\ud835\udf0bref\\pi_{\\text{ref}}italic_\u03c0 start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT for PRM and KL.", "description": "\uadf8\ub9bc 7(b)\ub294 'SFT ref' \ubc29\ubc95\uc5d0 \ub300\ud55c \uc124\uba85\uc785\ub2c8\ub2e4.  \uc774 \ubc29\ubc95\uc740 PRM(Process Reward Model)\uacfc KL(Kullback-Leibler divergence) \uacc4\uc0b0\uc744 \uc704\ud574 \ucd08\uae30 \uc815\ucc45(initial policy)\uc744 \u03c0ref (\ud30c\uc774 \uc368\ube0c\uc2a4\ud06c\ub9bd\ud2b8 ref)\ub85c \uc720\uc9c0\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989,  \ubaa8\ub378 \ud559\uc2b5 \uacfc\uc815\uc758 \ucc98\uc74c \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc815\ucc45\uc744 \ucc38\uc870\ud558\uc5ec PRM\uc744 \ucd08\uae30\ud654\ud558\uace0,  KL divergence\ub97c \uacc4\uc0b0\ud558\ub294 \ub370 \uc0ac\uc6a9\ud55c\ub2e4\ub294 \uc758\ubbf8\uc785\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc758 \ud559\uc2b5 \uacfc\uc815 \uc911 \ubc1c\uc0dd\ud560 \uc218 \uc788\ub294 \ubd84\ud3ec \ubcc0\ud654(distribution shift)\uc5d0 \ub300\ud55c \uc548\uc815\uc131\uc744 \ud655\ubcf4\ud558\uae30 \uc704\ud55c \uc804\ub7b5\uc785\ub2c8\ub2e4.", "section": "5.2 \ucc38\uc870 \ubaa8\ub378 \uc120\ud0dd\uc758 \uc720\uc5f0\uc131"}, {"figure_path": "https://arxiv.org/html/2502.01456/x11.png", "caption": "Figure 7: Comparison of different reference policy implementations. One uses the running policy\u2019s old logprobs as reference (policy ref) while the other uses the initial SFT model as the reference model (SFT ref).", "description": "\uadf8\ub9bc 7\uc740 \ub450 \uac00\uc9c0 \ub2e4\ub978 \ucc38\uc870 \uc815\ucc45 \uad6c\ud604 \ubc29\uc2dd\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. \ud558\ub098\ub294 \uc2e4\ud589 \uc911\uc778 \uc815\ucc45\uc758 \uc774\uc804 logprob\uc744 \ucc38\uc870\ub85c \uc0ac\uc6a9\ud558\ub294 \ubc29\uc2dd(policy ref)\uc774\uace0, \ub2e4\ub978 \ud558\ub098\ub294 \ucd08\uae30 SFT \ubaa8\ub378\uc744 \ucc38\uc870 \ubaa8\ub378\ub85c \uc0ac\uc6a9\ud558\ub294 \ubc29\uc2dd(SFT ref)\uc785\ub2c8\ub2e4.  policy ref\ub294 \ud604\uc7ac \uc815\ucc45\uc758 \ud655\ub960 \ubd84\ud3ec\ub97c \ucc38\uc870\ud558\uc5ec \ubcf4\ub2e4 \ub3d9\uc801\uc778 \uc5c5\ub370\uc774\ud2b8\ub97c \uc218\ud589\ud558\uc9c0\ub9cc,  SFT ref\ub294 \ucd08\uae30 \ubaa8\ub378\uc744 \uae30\uc900\uc73c\ub85c \ud558\uc5ec \ub354 \uc548\uc815\uc801\uc778 \ud559\uc2b5\uc744 \uc720\ub3c4\ud569\ub2c8\ub2e4. \ub450 \ubc29\uc2dd\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ube44\uad50\ud558\uc5ec \ucd5c\uc801\uc758 \ucc38\uc870 \uc815\ucc45 \uc120\ud0dd\uc5d0 \ub300\ud55c \ud1b5\ucc30\ub825\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "5.2 Reference Model Choice Is Flexible"}, {"figure_path": "https://arxiv.org/html/2502.01456/x12.png", "caption": "Figure 8: Different reference model for PRM. We compare two reference model selection strategies for PRIME. Using the policy model as reference and using the initial SFT model as reference. Their rewards are similar.", "description": "\uadf8\ub9bc 8\uc740 PRIME \uc54c\uace0\ub9ac\uc998\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \uc554\uc2dc\uc801 PRM(Process Reward Model)\uc758 \uae30\uc900 \ubaa8\ub378 \uc120\ud0dd \uc804\ub7b5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub450 \uac00\uc9c0 \uc804\ub7b5\uc740, (1) \ud604\uc7ac \uc815\ucc45 \ubaa8\ub378\uc758 \ub85c\uadf8 \ud655\ub960\uc744 \uae30\uc900 \ubaa8\ub378\ub85c \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uacfc (2) \ucd08\uae30 SFT(Supervised Fine-Tuning) \ubaa8\ub378\uc744 \uae30\uc900 \ubaa8\ub378\ub85c \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub450 \uac00\uc9c0 \uc804\ub7b5\uc758 \ubcf4\uc0c1(Reward)\uc774 \ube44\uc2b7\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \uc815\ucc45 \ubaa8\ub378 \ub610\ub294 \ucd08\uae30 SFT \ubaa8\ub378 \uc911 \uc5b4\ub290 \uac83\uc744 \uae30\uc900 \ubaa8\ub378\ub85c \uc0ac\uc6a9\ud558\ub354\ub77c\ub3c4 PRIME\uc758 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc740 \ud06c\uc9c0 \uc54a\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "5.2 REFERENCE MODEL CHOICE IS FLEXIBLE"}, {"figure_path": "https://arxiv.org/html/2502.01456/x13.png", "caption": "(a) PRM classification accuracy on training samples.", "description": "\uc774 \uadf8\ub9bc\uc740 \ud559\uc2b5 \uc0d8\ud50c\uc5d0 \ub300\ud55c PRM(Process Reward Model)\uc758 \ubd84\ub958 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e8\uc21c\ud788 \uc815\ud655\ub3c4\ub9cc \ub098\ud0c0\ub0b4\ub294 \uac83\uc774 \uc544\ub2c8\ub77c,  \uc2e4\uc81c \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c PRM\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc2dc\uacc4\uc5f4 \uadf8\ub798\ud504\uc77c \uac00\ub2a5\uc131\uc774 \ub192\uc2b5\ub2c8\ub2e4.  x\ucd95\uc740 \ud559\uc2b5 \ub2e8\uacc4 \ub610\ub294 \uc2dc\uac04\uc744, y\ucd95\uc740 PRM\uc758 \ubd84\ub958 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0bc \uac83\uc785\ub2c8\ub2e4.  \uc774 \uadf8\ub798\ud504\ub97c \ud1b5\ud574 PRM\uc774 \ud559\uc2b5 \ub370\uc774\ud130\uc5d0 \uc5bc\ub9c8\ub098 \uc798 \uc801\uc751\ud558\uace0 \uc788\ub294\uc9c0, \uadf8\ub9ac\uace0 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uc131\ub2a5\uc774 \uc5b4\ub5bb\uac8c \ubcc0\ud654\ud558\ub294\uc9c0\ub97c \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, \ub2e4\ub978 \ud559\uc2b5 \ubc29\ubc95(\uc608: \ub2e8\uc21c \uc804\ubc29 \ub610\ub294 \uc774\uc911 \uc804\ubc29)\uacfc \ube44\uad50\ud558\uc5ec PRM\uc758 \ud6a8\uc728\uc131\uacfc \uc548\uc815\uc131\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.3 SINGLE-FORWARD VS. DOUBLE-FORWARD"}, {"figure_path": "https://arxiv.org/html/2502.01456/x14.png", "caption": "(b) Training outcome rewards.", "description": "\uadf8\ub9bc (b)\ub294 \uc774 \ub17c\ubb38\uc758 5.3\uc808 \"\ub2e8\uc77c \uc804\ub2ec \ub300 \ub2e8\uc77c \uc804\ub2ec\"\uc5d0\uc11c \ub2e4\uc591\ud55c \uac15\ud654 \ud559\uc2b5 \uc54c\uace0\ub9ac\uc998\uc5d0 \ub300\ud55c \ud6c8\ub828 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \ud6c8\ub828 \uacfc\uc815\uc5d0\uc11c \uc5bb\uc5b4\uc9c0\ub294 \ub204\uc801 \ubcf4\uc0c1\uc758 \ubcc0\ud654\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ub2e8\uc77c \uc804\ub2ec \ubc29\ubc95\uacfc \uc774\uc911 \uc804\ub2ec \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uac01 \ubc29\ubc95\uc758 \ud6a8\uc728\uc131\uacfc \uc548\uc815\uc131\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  \uc774\uc911 \uc804\ub2ec \ubc29\ubc95\uc740 \ub2e8\uc77c \uc804\ub2ec \ubc29\ubc95\ubcf4\ub2e4 \ud6c8\ub828 \uacfc\uc815\uc5d0\uc11c\uc758 \ubcf4\uc0c1 \uac12\uc774 \ub354 \ub192\uace0 \uc548\uc815\uc801\uc778 \uacbd\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.3 \ub2e8\uc77c \uc804\ub2ec \ub300 \uc774\uc911 \uc804\ub2ec"}, {"figure_path": "https://arxiv.org/html/2502.01456/x15.png", "caption": "Figure 9: Single and double forward. While double forward methods obtain higher accuracy after online update, the two variants achieve similar rewards during training.", "description": "\uadf8\ub9bc 9\ub294 PRIME \ubaa8\ub378\uc758 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \ub2e8\uc77c \uc804\ub2ec(single forward) \ubc29\uc2dd\uacfc \uc774\uc911 \uc804\ub2ec(double forward) \ubc29\uc2dd\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\uc911 \uc804\ub2ec \ubc29\uc2dd\uc740 \uc628\ub77c\uc778 \uc5c5\ub370\uc774\ud2b8 \uc774\ud6c4 \ub354 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud558\uc9c0\ub9cc, \ub450 \ubc29\uc2dd \ubaa8\ub450 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uc720\uc0ac\ud55c \ubcf4\uc0c1(rewards)\uc744 \uc5bb\ub294\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, \uc774\uc911 \uc804\ub2ec \ubc29\uc2dd\uc774 \uc815\ud655\ub3c4 \uce21\uba74\uc5d0\uc11c\ub294 \uc6b0\uc218\ud558\uc9c0\ub9cc, \ud559\uc2b5 \ud6a8\uc728\uc131 \uce21\uba74\uc5d0\uc11c\ub294 \ub2e8\uc77c \uc804\ub2ec \ubc29\uc2dd\uacfc \ud070 \ucc28\uc774\uac00 \uc5c6\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  \ub450 \ubc29\uc2dd\uc758 \ucc28\uc774\ub294 \uc628\ub77c\uc778 \uc5c5\ub370\uc774\ud2b8 \uc774\ud6c4\uc5d0 \ub098\ud0c0\ub098\ubbc0\ub85c, \ud559\uc2b5 \ucd08\uae30 \ub2e8\uacc4\uc5d0\uc11c\ub294 \ub450 \ubc29\uc2dd \uac04\uc758 \uc131\ub2a5 \ucc28\uc774\uac00 \ubbf8\ubbf8\ud569\ub2c8\ub2e4.", "section": "5.3 SINGLE-FORWARD VS. DOUBLE-FORWARD"}, {"figure_path": "https://arxiv.org/html/2502.01456/x16.png", "caption": "Figure 10: PRIME also benefits REINFORCE, GRPO, and PPO, achieving similar improvement as RLOO.", "description": "\uadf8\ub9bc 10\uc740 \ub2e4\uc591\ud55c \uac15\ud654 \ud559\uc2b5 \uc54c\uace0\ub9ac\uc998(REINFORCE, GRPO, PPO)\uc5d0 PRIME \uae30\ubc95\uc744 \uc801\uc6a9\ud588\uc744 \ub54c\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. PRIME\uc744 \uc801\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\uc640 \ube44\uad50\ud558\uc5ec, \uc138 \uac00\uc9c0 \uc54c\uace0\ub9ac\uc998 \ubaa8\ub450 RLOO\uc640 \uc720\uc0ac\ud55c \uc218\uc900\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 PRIME \uae30\ubc95\uc774 \ud2b9\uc815 \uc54c\uace0\ub9ac\uc998\uc5d0 \uad6d\ud55c\ub418\uc9c0 \uc54a\uace0 \ub2e4\uc591\ud55c \uac15\ud654 \ud559\uc2b5 \uc54c\uace0\ub9ac\uc998\uc5d0 \uc801\uc6a9 \uac00\ub2a5\ud558\uba70 \ud3ed\ub113\uc740 \ud65c\uc6a9\uc131\uc744 \uac00\uc9d0\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "5.4 PRIME WITH OTHER RL ALGORITHMS"}, {"figure_path": "https://arxiv.org/html/2502.01456/x17.png", "caption": "Figure 11: Comparison of value models and reward models. We show that value models, either the original PPO one or Implicit PRM, is substaintially worse than reward models.", "description": "\uadf8\ub9bc 11\uc740 PPO\uc758 \uae30\uc874 \uac12 \ubaa8\ub378 \ub610\ub294 \uc554\ubb35\uc801 PRM(Process Reward Model)\uacfc \ubcf4\uc0c1 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uc2e4\ud5d8 \uacb0\uacfc, \uae30\uc874 \uac12 \ubaa8\ub378\uc774\ub098 \uc554\ubb35\uc801 PRM\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0 \ubcf4\uc0c1 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ud604\uc800\ud788 \ub5a8\uc5b4\uc9c0\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uac15\ud654 \ud559\uc2b5\uc5d0\uc11c \ubcf4\uc0c1 \ubaa8\ub378\uc774 \uac12 \ubaa8\ub378\ubcf4\ub2e4 \ud6e8\uc52c \ud6a8\uacfc\uc801\uc784\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "5.5 \uac12 \ub610\ub294 \ubcf4\uc0c1: \uc554\ubb35\uc801 PRM \uc0ac\uc6a9 \ubc29\ubc95"}]