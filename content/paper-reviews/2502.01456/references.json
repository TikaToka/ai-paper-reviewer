{"references": [{"fullname_first_author": "Arash Ahmadian", "paper_title": "Back to basics: Revisiting reinforce style optimization for learning from human preferences in LLMs", "publication_date": "2024-02-14", "reason": "This paper provides a general theoretical paradigm to understand learning from human preferences, a crucial aspect of reinforcement learning for LLMs which is heavily discussed in the target paper."}, {"fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "publication_date": "2024-10-23", "reason": "This paper offers a general theoretical framework for understanding learning from human preferences, which is highly relevant to the challenges of reward sparsity and reward hacking addressed in the target paper."}, {"fullname_first_author": "Changyu Chen", "paper_title": "Bootstrapping language models with DPO implicit rewards", "publication_date": "2024-06-09", "reason": "This paper introduces a method for using implicit rewards in language models, which is directly relevant to the PRIME method proposed in the target paper."}, {"fullname_first_author": "Huayu Chen", "paper_title": "Noise contrastive alignment of language models with explicit rewards", "publication_date": "2024-02-05", "reason": "This paper explores the use of explicit rewards in language models, providing a contrasting approach to the implicit reward method used in the target paper."}, {"fullname_first_author": "Ganqu Cui", "paper_title": "Ultrafeedback: Boosting language models with scaled AI feedback", "publication_date": "2024-XX-XX", "reason": "This paper is cited as a concurrent work that shares similar conclusions and thus is impeded from incorporating PRMs into their large-scale RL training.  This highlights the significance of the target paper's solution."}]}