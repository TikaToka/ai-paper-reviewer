[{"content": "| Model/Dataset | PubMedQA |  | MedMCQA |  | MedQA |  | MMLU |  |\n|---|---|---|---|---|---|---|---|---|\n|  | En | Ar | En | Ar | En | Ar | En | Ar |\n| Qwen2.5-3B-Instruct (Yang et al. 2024) | 29.2 | 61.2 | 49.2 | 35.5 | 48.8 | 41.7 | 68.0 | 28.0 |\n| Qwen2.5-7B-Instruct (Yang et al. 2024) | 45.2 | 74.4 | 56.8 | 39.5 | 60.2 | 53.9 | 76.7 | 34.9 |\n| Pangea-7B (Yue et al. 2024) | 57.0 | 61.0 | 50.2 | 37.5 | 53.0 | 49.6 | 68.3 | 32.4 |\n| Mistral-7B-Instruct_v0.3 (Jiang et al. 2023) | 45.8 | 46.6 | 46.3 | 28.0 | 49.3 | 33.8 | 65.1 | 21.6 |\n| Llama3.1-8B-Instruct (Dubey et al. 2024) | 76.2 | 73.2 | 58.4 | 35.8 | 62.0 | 29.5 | 73.4 | 46.4 |\n| Silma-9B-Instruct-v1.0 (Silma-AI 2024) | 75.6 | 64.0 | 54.9 | 38.9 | 61.6 | 54.7 | 76.1 | 31.5 |\n| Llama-3.1-70B-Instruct (Dubey et al. 2024) | 73.6 | 79.4 | 71.8 | 52.2 | 78.2 | 56.6 | 87.6 | 70.0 |\n| Qwen2.5-72B-Instruct (Yang et al. 2024) | 63.2 | 76.6 | 68.4 | 56.9 | 76.1 | 76.1 | 87.4 | 76.1 |\n| Med42-Llama3.1-70B (Christophe et al. 2024) | 77.6 | 75.0 | 72.4 | 49.3 | 80.4 | 53.5 | 86.8 | 67.7 |\n| Meditron3-70B (Chen et al. 2023) | 80.6 | 75.8 | 70.9 | 51.2 | 79.3 | 72.0 | 87.0 | 56.6 |\n| BiMedix(Bilingual) (Pieri et al. 2024b) | 77.2 | 78.4 | 61.6 | 49.1 | 65.2 | 47.3 | 73.2 | 56.9 |", "caption": "Table 1: Accuracy of publicly available models on different Medical QA benchmarks. Even though Llama3.1 models are performing better in English, Qwen2.5 models show a stronger performance in Arabic.", "description": "\ud45c 1\uc740 \uc5ec\ub7ec \uacf5\uac1c\uc801\uc73c\ub85c \uc0ac\uc6a9 \uac00\ub2a5\ud55c \uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uc774 \ub2e4\uc591\ud55c \uc758\ub8cc \uc9c8\uc758\uc751\ub2f5(QA) \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub2ec\uc131\ud55c \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Llama 3.1 \ubaa8\ub378\uc774 \uc601\uc5b4 \uae30\ubc18 \uacfc\uc81c\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \ubc18\uba74, Qwen 2.5 \ubaa8\ub378\uc740 \uc544\ub78d\uc5b4 \uae30\ubc18 \uacfc\uc81c\uc5d0\uc11c \ub354 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubaa8\ub378\uc758 \uc5b8\uc5b4 \ubc0f \uacfc\uc81c\ubcc4 \uc131\ub2a5 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc8fc\uc5b4, \uc758\ub8cc \ubd84\uc57c\uc5d0\uc11c \ub2e4\uad6d\uc5b4 \uc9c0\uc6d0 \uc5b8\uc5b4 \ubaa8\ub378 \uac1c\ubc1c\uc758 \uc5b4\ub824\uc6c0\uc744 \uac15\uc870\ud569\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc601\uc5b4 \ubc0f \uc544\ub78d\uc5b4 \uc131\ub2a5\uc744 \uc5ec\ub7ec \uc758\ub8cc QA \ub370\uc774\ud130\uc14b(PubMedQA, MedMCQA, MedQA, MMLU)\uc5d0 \ub300\ud574 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \ubaa8\ub378\uc758 \ub2e4\uad6d\uc5b4 \ucc98\ub9ac \ub2a5\ub825\uacfc \uc758\ub8cc \uc9c0\uc2dd \uc218\uc900\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "\uacb0\uacfc"}, {"content": "| Translation Model | PubMedQA | MedMCQA | MedQA | MMLU |\n|---|---|---|---|---|\n| LlamaX (Lu et al. 2024) | 74.6 | 53.1 | 55.8 | 59.5 |\n| Helsinki (Helsinki-NLP 2024) | 72.0 | 48.9 | 40.8 | 56.6 |\n| Flores 101 (seyoungsong 2024) | 72.0 | 36.6 | 31.2 | 34.0 |\n| Llama3.1-70B-Instruct (Dubey et al. 2024) | 75.8 | 54.8 | 70.5 | 70.7 |\n| Qwen2.5-72B-Instruct (Yang et al. 2024) | **75.9** | **55.2** | **71.3** | **71.5** |", "caption": "Table 2: Performance comparison of various translation models on Arabic medical benchmarks, translated into English and evaluated using Llama3.1-70B-Instruct for accuracy (%).", "description": "\ud45c 2\ub294 \uc5ec\ub7ec \ubc88\uc5ed \ubaa8\ub378\uc774 \uc544\ub78d\uc5b4 \uc758\ud559 \ubca4\uce58\ub9c8\ud06c\ub97c \uc601\uc5b4\ub85c \ubc88\uc5ed\ud55c \ud6c4 Llama3.1-70B-Instruct\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc815\ud655\ub3c4\ub97c \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubc88\uc5ed \ubaa8\ub378\uc758 \uc544\ub78d\uc5b4 \uc758\ud559 \ubca4\uce58\ub9c8\ud06c \ubc88\uc5ed \uc131\ub2a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50\ud558\uc5ec \uc5b4\ub5a4 \ubaa8\ub378\uc774 \uc544\ub78d\uc5b4 \uc758\ud559 \ud14d\uc2a4\ud2b8\ub97c \uc601\uc5b4\ub85c \uac00\uc7a5 \uc815\ud655\ud558\uac8c \ubc88\uc5ed\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 \uc544\ub78d\uc5b4 \uc758\ub8cc \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\ub294 \ub2e4\uad6d\uc5b4 LLM \uac1c\ubc1c\uc5d0 \uc788\uc5b4\uc11c \ubc88\uc5ed \ubaa8\ub378 \uc120\ud0dd\uc758 \uc911\uc694\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "Arabic Evaluation Datasets"}, {"content": "| Dataset | Original | Description | Final | # of Tokens |\n|---|---|---|---|---|\n| 1. AHQAD | Arabic | 100K sampled based on completeness and paraphrased with Qwen-72B-Instruct | Arabic | 8.33 M |\n| 2. Translated MED42 Dataset | English | 500K sampled randomly, cleaned and translated with Qwen-72B-Instruct | Arabic | 230.69 M |\n| 3. CIDAR | Arabic | 10K Instruction-Output good quality dataset | Arabic | 1.34 M |\n| 4. Med42 Dataset | English | Full English FT dataset | English | 464.97 M |\n| 5. Synthetic Open-Ended | English | ~200K sampled based on 1-5 rating and translated with Qwen-72B-Instruct | Arabic | 240 M |\n| **Total # of Arabic Tokens** |  |  |  | **480.36 M** |\n| **Total # of English Tokens** |  |  |  | **469.97 M** |", "caption": "Table 3: Dataset Overview with Language Distribution", "description": "\ud45c 3\uc740 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \uac1c\uc694\uc640 \uac01 \uc5b8\uc5b4\ubcc4 \ub370\uc774\ud130 \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  AHQAD, \ubc88\uc5ed\ub41c MED42 \ub370\uc774\ud130\uc14b, CIDAR, Med42 \ub370\uc774\ud130\uc14b, \uadf8\ub9ac\uace0 \ud569\uc131 \uac1c\ubc29\ud615 \uc9c8\ubb38 \ub2f5\ubcc0 \ub370\uc774\ud130\uc14b \ub4f1 \ub2e4\uc12f \uac00\uc9c0 \ub370\uc774\ud130\uc14b\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc6d0\ub798 \uc5b8\uc5b4, \uc124\uba85, \uadf8\ub9ac\uace0 \ud1a0\ud070 \uc218\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc544\ub78d\uc5b4\uc640 \uc601\uc5b4 \ub370\uc774\ud130\uc758 \ud1a0\ud070 \uc218\ub97c \ud569\uc0b0\ud558\uc5ec \uac01 \uc5b8\uc5b4\uc758 \ucd1d \ud1a0\ud070 \uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc544\ub78d\uc5b4 \uc758\ub8cc \ub370\uc774\ud130\uc14b\uc744 \uc5b4\ub5bb\uac8c \uc900\ube44\ud558\uace0 \uc0ac\uc6a9\ud588\ub294\uc9c0 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "Arabic Evaluation Datasets"}, {"content": "| 100K sampled based on completeness and | paraphrased with Qwen-72B-Instruct |\n", "caption": "Table 4: Accuracy of Finetuned Llama3.1-8b and 70b models with Different Arabic-English Dataset Ratios on medical QA benchmarks. Fine-tuning Llama 3.1 models on varying Arabic-English dataset ratios yields inconsistent results across medical QA tasks. Even large instruct models show limited improvement on Arabic benchmarks after fine-tuning.", "description": "\ud45c 4\ub294 \ub2e4\uc591\ud55c \uc544\ub78d\uc5b4-\uc601\uc5b4 \ub370\uc774\ud130 \ube44\uc728\ub85c \ubbf8\uc138 \uc870\uc815\ub41c Llama 3.1-8B \ubc0f 70B \ubaa8\ub378\uc758 \uc758\ub8cc QA \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uc544\ub78d\uc5b4-\uc601\uc5b4 \ub370\uc774\ud130 \ube44\uc728\ub85c Llama 3.1 \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud558\uba74 \uc758\ub8cc QA \uc791\uc5c5 \uc804\ubc18\uc5d0 \uac78\uccd0 \uc77c\uad00\uc131 \uc5c6\ub294 \uacb0\uacfc\uac00 \ub098\ud0c0\ub0a9\ub2c8\ub2e4. \ub300\uaddc\ubaa8 instruction \ubaa8\ub378\uc870\ucc28\ub3c4 \ubbf8\uc138 \uc870\uc815 \ud6c4 \uc544\ub78d\uc5b4 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc81c\ud55c\uc801\uc785\ub2c8\ub2e4.", "section": "Language Specific Finetuning"}]