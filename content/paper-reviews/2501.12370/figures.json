[{"figure_path": "https://arxiv.org/html/2501.12370/x1.png", "caption": "(a) IsoFLOP surface over sparsity and total parameters", "description": "\uadf8\ub9bc 1(a)\ub294 Mixture-of-Experts (MoE) \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc0ac\uc804 \ud6c8\ub828 \uc190\uc2e4, \ucd1d \ub9e4\uac1c\ubcc0\uc218 \uc218, \uadf8\ub9ac\uace0 \uc2a4\ud30c\uc2a4\uc131 \uac04\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 3\ucc28\uc6d0 \ud45c\uba74 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4.  IsoFLOP \ud45c\uba74\uc774\ub77c\uace0 \ubd88\ub9ac\ub294 \uc774 \uadf8\ub798\ud504\ub294 \uace0\uc815\ub41c \ucd1d \uacc4\uc0b0\ub7c9(FLOPs) \ud558\uc5d0\uc11c \uc2a4\ud30c\uc2a4\uc131\uc774 \ubcc0\ud568\uc5d0 \ub530\ub77c \ubaa8\ub378 \ud06c\uae30\uac00 \uc190\uc2e4\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ucd1d \ub9e4\uac1c\ubcc0\uc218 \uc218\ub294 \ubaa8\ub378\uc758 \uc6a9\ub7c9\uc744 \ub098\ud0c0\ub0b4\uba70, \uc2a4\ud30c\uc2a4\uc131\uc740 \ube44\ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218\uc758 \ube44\uc728\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ud2b9\uc815 \uacc4\uc0b0\ub7c9 \uc81c\uc57d \uc870\uac74 \ud558\uc5d0\uc11c \ucd5c\uc801\uc758 \uc2a4\ud30c\uc2a4\uc131 \uc218\uc900\uc744 \ucc3e\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub418\ub294 \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "2 MoEs\uc5d0\uc11c\uc758 \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218\uc640 \uc2a4\ud30c\uc2a4\uc131 \uac04\uc758 \uc0c1\ud638 \uc791\uc6a9"}, {"figure_path": "https://arxiv.org/html/2501.12370/x2.png", "caption": "(b) IsoFLOP surface over sparsity and active parameters", "description": "\uc774 \uadf8\ub9bc\uc740 \uace0\uc815\ub41c \ucef4\ud4e8\ud305 \uc608\uc0b0 \ud558\uc5d0\uc11c, \ud76c\uc18c \ud63c\ud569 \uc804\ubb38\uac00(MoE) \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \ub300\ud55c \ub9e4\uac1c\ubcc0\uc218 \uc218\uc640 \ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218 \uc218\uc758 \uc0c1\ud638 \uc791\uc6a9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378 \ud06c\uae30(\ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218)\uc640 \ud76c\uc18c\uc131 \uc218\uc900\uc744 \ubcc0\ud654\uc2dc\ud0a4\uba74\uc11c \uc190\uc2e4\uc744 \uc2dc\uac01\ud654\ud558\uc5ec, \ucd5c\uc801\uc758 \ud76c\uc18c\uc131 \uc218\uc900\uc744 \ucc3e\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  \uc989, \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \uc720\uc9c0\ud558\uba74\uc11c \ubaa8\ub378 \uc131\ub2a5\uc744 \ucd5c\ub300\ud654\ud558\ub294 \ud76c\uc18c\uc131 \uc218\uc900\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uba74\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "2 MoEs\uc5d0\uc11c\uc758 \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218\uc640 \ud76c\uc18c\uc131 \uac04\uc758 \uc0c1\ud638 \uc791\uc6a9"}, {"figure_path": "https://arxiv.org/html/2501.12370/x3.png", "caption": "Figure 1: IsoFLOP surface over observed pretraining loss \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L, model size (in terms of total \ud835\udc0d\ud835\udc0d\\mathbf{N}bold_N and active parameters \ud835\udc0d\ud835\udc1asubscript\ud835\udc0d\ud835\udc1a\\mathbf{N_{a}}bold_N start_POSTSUBSCRIPT bold_a end_POSTSUBSCRIPT), and sparsity \ud835\udc12\ud835\udc12\\mathbf{S}bold_S. We fit a polynomial function mapping \ud835\udc0d\ud835\udc0d\\mathbf{N}bold_N (or \ud835\udc0d\ud835\udc1asubscript\ud835\udc0d\ud835\udc1a\\mathbf{N_{a}}bold_N start_POSTSUBSCRIPT bold_a end_POSTSUBSCRIPT), \ud835\udc12\ud835\udc12\\mathbf{S}bold_S, and their interaction to \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L, using empirical data. Both fits achieve an MSE loss of 0.0001 on a held-out set. These results indicate that for a fixed compute budget, increasing model sparsity leads to a reduction in pretraining loss. When considering optimal model size, we observe opposite trends for total parameters (\ud835\udc0d\ud835\udc0d\\mathbf{N}bold_N) (Figure a) versus active parameters (\ud835\udc0d\ud835\udc1asubscript\ud835\udc0d\ud835\udc1a\\mathbf{N_{a}}bold_N start_POSTSUBSCRIPT bold_a end_POSTSUBSCRIPT) (Figure b). (Figure\u00a08 includes results Section\u00a0D.1 for different total compute budgets C\ud835\udc36Citalic_C.)", "description": "\uadf8\ub9bc 1\uc740 \uc0ac\uc804 \ud559\uc2b5 \uc190\uc2e4(L), \ucd1d \ub9e4\uac1c\ubcc0\uc218(N) \ubc0f \ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218(Na)\uc640 \uc2a4\ud30c\uc2a4\uc131(S)\uc5d0 \ub300\ud55c \ub4f1\ub7c9 FLOP \ud45c\uba74\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc2e4\ud5d8 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec N(\ub610\ub294 Na), S \ubc0f \uc0c1\ud638 \uc791\uc6a9\uc744 L\uc5d0 \ub9e4\ud551\ud558\ub294 \ub2e4\ud56d\uc2dd \ud568\uc218\ub97c \uc801\ud569\ud588\uc2b5\ub2c8\ub2e4. \ub450 \uc801\ud569 \ubaa8\ub450 \ud640\ub4dc\uc544\uc6c3 \uc9d1\ud569\uc5d0\uc11c 0.0001\uc758 MSE \uc190\uc2e4\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacb0\uacfc\ub294 \uace0\uc815\ub41c \uacc4\uc0b0 \uc608\uc0b0\uc758 \uacbd\uc6b0 \ubaa8\ub378 \uc2a4\ud30c\uc2a4\uc131\uc744 \ub192\uc774\uba74 \uc0ac\uc804 \ud559\uc2b5 \uc190\uc2e4\uc774 \uac10\uc18c\ud55c\ub2e4\ub294 \uac83\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ucd5c\uc801 \ubaa8\ub378 \ud06c\uae30\ub97c \uace0\ub824\ud560 \ub54c \ucd1d \ub9e4\uac1c\ubcc0\uc218(N)(\uadf8\ub9bc a)\uc640 \ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218(Na)(\uadf8\ub9bc b)\uc5d0 \ub300\ud574 \ubc18\ub300\ub418\ub294 \ucd94\uc138\ub97c \uad00\ucc30\ud588\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc 8\uc5d0\ub294 \ub2e4\uc591\ud55c \ucd1d \uacc4\uc0b0 \uc608\uc0b0(C)\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \ub098\uc640\uc788\uc2b5\ub2c8\ub2e4.", "section": "2 MoEs\uc5d0\uc11c \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218\uc640 \uc2a4\ud30c\uc2a4\uc131 \uac04\uc758 \uc0c1\ud638 \uc791\uc6a9"}, {"figure_path": "https://arxiv.org/html/2501.12370/x4.png", "caption": "Figure 2: IsoFLOP slices along Sparsity and Model Size (C=1\u2062e\u206220\ud835\udc361\ud835\udc5220C=1e20italic_C = 1 italic_e 20).\nWe use fitted isoFLOP surfaces (Section\u00a02) to analyze how sparsity \ud835\udc12\ud835\udc12\\mathbf{S}bold_S and model size \ud835\udc0d\ud835\udc0d\\mathbf{N}bold_N impact the loss \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L for a fixed compute budget. We identify optimal points by (a) fixing \ud835\udc0d\ud835\udc0d\\mathbf{N}bold_N and varying \ud835\udc12\ud835\udc12\\mathbf{S}bold_S, (b) fixing \ud835\udc12\ud835\udc12\\mathbf{S}bold_S and varying \ud835\udc0d\ud835\udc0d\\mathbf{N}bold_N and (c) fixing \ud835\udc12\ud835\udc12\\mathbf{S}bold_S and varying active parameters \ud835\udc0d\ud835\udc1asubscript\ud835\udc0d\ud835\udc1a\\mathbf{N_{a}}bold_N start_POSTSUBSCRIPT bold_a end_POSTSUBSCRIPT. Observe that (a) the optimal sparsity S\ud835\udc46Sitalic_S increases with increasing model size N\ud835\udc41Nitalic_N and converges to 1 while (b) and (c) show that the optimal model size N\ud835\udc41Nitalic_N and active parameter count Nasubscript\ud835\udc41\ud835\udc4eN_{a}italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT increase and decrease respectively with increasing sparsity levels. (see Figure\u00a09 in Section\u00a0D.1 for other total\ntraining compute budgets.)", "description": "\uadf8\ub9bc 2\ub294 \uace0\uc815\ub41c \ucef4\ud4e8\ud305 \uc608\uc0b0(C=1e20) \ud558\uc5d0\uc11c, sparsity (S)\uc640 \ubaa8\ub378 \ud06c\uae30 (N)\uac00 \uc190\uc2e4(L)\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud55c \uac83\uc785\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 (a) N\uc744 \uace0\uc815\ud558\uace0 S\ub97c \ubcc0\ud654\uc2dc\ucf1c \ucd5c\uc801\uc758 S\ub97c \ucc3e\uace0, (b) S\ub97c \uace0\uc815\ud558\uace0 N\uc744 \ubcc0\ud654\uc2dc\ucf1c \ucd5c\uc801\uc758 N\uc744 \ucc3e\uace0, (c) S\ub97c \uace0\uc815\ud558\uace0 \ud65c\uc131 \ud30c\ub77c\ubbf8\ud130(Na)\ub97c \ubcc0\ud654\uc2dc\ucf1c \ucd5c\uc801\uc758 Na\ub97c \ucc3e\ub294 \uc138 \uac00\uc9c0 \ubc29\ubc95\uc73c\ub85c \ucd5c\uc801\uc810\uc744 \ud655\uc778\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c \ubcf4\uc5ec\uc8fc\ub4ef\uc774, (a) \ucd5c\uc801 sparsity S\ub294 \ubaa8\ub378 \ud06c\uae30 N\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc99d\uac00\ud558\uba70 1\uc5d0 \uc218\ub834\ud558\uace0, (b)\uc640 (c)\uc5d0\uc11c\ub294 \ucd5c\uc801 \ubaa8\ub378 \ud06c\uae30 N\uacfc \ud65c\uc131 \ud30c\ub77c\ubbf8\ud130 \uc218 Na\uac00 sparsity S\uc758 \uc99d\uac00\uc5d0 \ub530\ub77c \uac01\uac01 \uc99d\uac00 \ubc0f \uac10\uc18c\ud558\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubd80\ub85d D.1\uc758 \uadf8\ub9bc 9\uc5d0\uc11c\ub294 \ub2e4\ub978 \ucd1d \ud6c8\ub828 \ucef4\ud4e8\ud305 \uc608\uc0b0\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2 MoEs\uc5d0\uc11c \ubaa8\ub378 \ud30c\ub77c\ubbf8\ud130\uc640 \uc2a4\ud30c\uc2a4\uc131 \uac04\uc758 \uc0c1\ud638\uc791\uc6a9"}, {"figure_path": "https://arxiv.org/html/2501.12370/x5.png", "caption": "Figure 3: \nEffect of compute budget on model size, number of active parameters and loss with sparsity. Across all compute budgets, we observe that (a) the optimal model size N\ud835\udc41Nitalic_N increases with sparsity, (b) the optimal number of active parameters Nasubscript\ud835\udc41\ud835\udc4eN_{a}italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT decreases with sparsity, and (c) the loss L\ud835\udc3fLitalic_L decreases with sparsity.", "description": "\uadf8\ub9bc 3\uc740 \ubaa8\ub378 \ud06c\uae30, \ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218 \uc218, \uadf8\ub9ac\uace0 \uc2a4\ud30c\uc2a4\uc131\uc5d0 \ub530\ub978 \uc190\uc2e4\uc5d0 \ub300\ud55c \ucef4\ud4e8\ud305 \uc608\uc0b0\uc758 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \ucef4\ud4e8\ud305 \uc608\uc0b0\uc5d0\uc11c (a) \ucd5c\uc801 \ubaa8\ub378 \ud06c\uae30 N\uc740 \uc2a4\ud30c\uc2a4\uc131\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc99d\uac00\ud558\uace0, (b) \ucd5c\uc801 \ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218 \uc218 Na\ub294 \uc2a4\ud30c\uc2a4\uc131\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uac10\uc18c\ud558\uba70, (c) \uc190\uc2e4 L\uc740 \uc2a4\ud30c\uc2a4\uc131\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uac10\uc18c\ud569\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ud6c8\ub828 \ucef4\ud4e8\ud305 \uc608\uc0b0\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ubaa8\ub378 \ud06c\uae30\uc640 \uc2a4\ud30c\uc2a4\uc131 \uac04\uc758 \uc0c1\ud638 \uc791\uc6a9\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ucd94\uac00 \ubd84\uc11d\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3 Impact of Training Compute Budget on the Interaction between Model Parameters and Sparsity"}, {"figure_path": "https://arxiv.org/html/2501.12370/x6.png", "caption": "Figure 4: Effect of training budget C\ud835\udc36Citalic_C and total parameters N\ud835\udc41Nitalic_N on MoE sparsity. Optimal MoE sparsity S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT changes with respect to the total number of parameters N\ud835\udc41Nitalic_N and the training budget C\ud835\udc36Citalic_C. The x\ud835\udc65xitalic_x-axis represents the total parameters N\ud835\udc41Nitalic_N on a logarithmic scale, and the y\ud835\udc66yitalic_y-axis shows the optimal MoE sparsity S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT.", "description": "\uc774 \uadf8\ub9bc\uc740 \uc8fc\uc5b4\uc9c4 \ud6c8\ub828 \ucef4\ud4e8\ud305 \uc608\uc0b0 \ud558\uc5d0\uc11c \ucd5c\uc801\uc758 MoE(Mixture-of-Experts) \uc2a4\ud30c\uc2a4(sparsity) \uc218\uc900\uc774 \ubaa8\ub378\uc758 \ud30c\ub77c\ubbf8\ud130 \uc218\uc640 \ud6c8\ub828 \ucef4\ud4e8\ud305 \uc608\uc0b0\uc5d0 \ub530\ub77c \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. x\ucd95\uc740 \ub85c\uadf8 \uc2a4\ucf00\uc77c\ub85c \ud45c\ud604\ub41c \ucd1d \ud30c\ub77c\ubbf8\ud130 \uc218\ub97c \ub098\ud0c0\ub0b4\uace0, y\ucd95\uc740 \ucd5c\uc801\uc758 MoE \uc2a4\ud30c\uc2a4 \uc218\uc900\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud6c8\ub828 \ucef4\ud4e8\ud305 \uc608\uc0b0(C)\uc5d0 \ub300\ud55c \ucd5c\uc801 \uc2a4\ud30c\uc2a4 \uc218\uc900(S*)\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \uc120\uc774 \uadf8\ub824\uc838 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ud6c8\ub828 \ucef4\ud4e8\ud305 \uc608\uc0b0\uacfc \ubaa8\ub378 \ud06c\uae30\uac00 MoE \uc2a4\ud30c\uc2a4\uc131\uacfc \ucd5c\uc801\ud654\ub41c \ubaa8\ub378 \uc131\ub2a5 \uac04\uc758 \uc0c1\ud638 \uc791\uc6a9\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3 Impact of Training Compute Budget on the Interaction between Model Parameters and Sparsity"}, {"figure_path": "https://arxiv.org/html/2501.12370/x7.png", "caption": "Figure 5: \nEffect of sparsity on downstream vs upstream performance. Downstream error shows a tight relationship with pretraining (\u201cupstream\u201d) loss across downstream tasks across all sparsity levels.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \uc0ac\uc804 \ud559\uc2b5 \uc190\uc2e4(\uc5c5\uc2a4\ud2b8\ub9bc)\uacfc \ub2e4\uc591\ud55c \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc758 \uc131\ub2a5 \uac04\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc5d0\uc11c \uc0ac\uc804 \ud559\uc2b5 \uc190\uc2e4\uc774 \uac10\uc18c\ud568\uc5d0 \ub530\ub77c \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc758 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\ub294 \ubc00\uc811\ud55c \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ud76c\uc18c\uc131 \uc218\uc900\uc5d0 \uad00\uacc4\uc5c6\uc774 \uc77c\uad00\ub418\uac8c \ub098\ud0c0\ub098\uba70, \uc0ac\uc804 \ud559\uc2b5 \uc131\ub2a5\uc774 \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5 \uc131\ub2a5\uc5d0 \uc0c1\ub2f9\ud55c \uc601\ud5a5\uc744 \ubbf8\uce68\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub7ec\ub098, \uc77d\uae30 \uc774\ud574 \uc791\uc5c5\uc758 \uacbd\uc6b0, \ub354 \ud76c\uc18c\ud55c \ubaa8\ub378\uc774 \ubc00\uc9d1 \ubaa8\ub378\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub5a8\uc5b4\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ub354 \uc801\uc740 \uacc4\uc0b0 \ube44\uc6a9\uc73c\ub85c \uc778\ud574 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc73c\uba70, \uc774\uc5d0 \ub300\ud55c \ucd94\uac00\uc801\uc778 \ubd84\uc11d\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.", "section": "4 MoE \ud76c\uc18c\uc131\uc758 \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5 \uc131\ub2a5\uc5d0 \ub300\ud55c \uc601\ud5a5"}, {"figure_path": "https://arxiv.org/html/2501.12370/x8.png", "caption": "(a) Fit on data used to estimate coefficients.", "description": "\uc774 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc758 5\uc7a5 \"\uc2a4\ud30c\uc2a4\uc131\uc744 \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc5d0 \ud1b5\ud569\ud558\uae30\"\uc5d0\uc11c \uc81c\uc2dc\ub41c \ub9e4\uac1c\ubcc0\uc218 \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc744 \ub370\uc774\ud130\uc5d0 \uc801\ud569\uc2dc\ud0a4\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  (a)\ub294 \uacc4\uc218\ub97c \ucd94\uc815\ud558\ub294 \ub370 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc801\ud569\ub3c4\ub97c, (b)\ub294 \ubcf4\uc720\ub41c \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \ub300\ud55c \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc758 \uc720\ud6a8\uc131 \uac80\uc0ac \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uadf8\ub9bc (a)\ub294 \ucd94\uc815\ub41c \ub9e4\uac1c\ubcc0\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ub41c \uc190\uc2e4\uacfc \uc2e4\uc81c \uad00\ucc30\ub41c \uc190\uc2e4 \uac04\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc0b0\uc810\ub3c4\uc785\ub2c8\ub2e4. \uadf8\ub9bc (b)\ub294 \ub3c5\ub9bd\uc801\uc778 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \uc608\uce21 \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4.  \uc774\ub294 \uc81c\uc548\ub41c \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc758 \uc815\ud655\uc131\uacfc \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc911\uc694\ud55c \ubd80\ubd84\uc785\ub2c8\ub2e4.", "section": "\uc2a4\ud30c\uc2a4\uc131\uc744 \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc5d0 \ud1b5\ud569\ud558\uae30"}, {"figure_path": "https://arxiv.org/html/2501.12370/x9.png", "caption": "(b) Validating scaling law on held-out dataset.", "description": "\uadf8\ub9bc 6(b)\ub294 \uc81c\uc2dc\ub41c \ub9e4\uac1c\ubcc0\uc218\ud654\ub41c \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc758 \uc608\uce21 \uc131\ub2a5\uc744 \uac80\uc99d\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub41c \ud640\ub4dc\uc544\uc6c3 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc774 \ud6c8\ub828 \uc5f0\uc0b0\ub7c9\uc774 \ucd5c\uc801\ud654\ub41c \ubaa8\ub378\uc5d0 \ub300\ud574 \uc5bc\ub9c8\ub098 \uc798 \uc77c\ubc18\ud654\ub418\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  x\ucd95\uc740 \uc2e4\uc81c \uc190\uc2e4\uc744, y\ucd95\uc740 \uc608\uce21 \uc190\uc2e4\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uac01 \uc810\uc740 \ud2b9\uc815 \ubaa8\ub378 \ud06c\uae30\uc640 \ub370\uc774\ud130\uc14b \ud06c\uae30\ub97c \ub098\ud0c0\ub0b4\uba70, \uc9c1\uc120 y=x\ub294 \uc644\ubcbd\ud55c \uc608\uce21\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc810\ub4e4\uc774 \uc9c1\uc120\uc5d0\uc11c \uc5bc\ub9c8\ub098 \ubc97\uc5b4\ub098 \uc788\ub294\uc9c0 \ud655\uc778\ud558\uba74 \ubaa8\ub378\uc758 \uc608\uce21 \uc815\ud655\ub3c4\ub97c \ud3c9\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5 \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc5d0 \uc2a4\ud30c\uc2a4\uc131 \ud1b5\ud569\ud558\uae30"}, {"figure_path": "https://arxiv.org/html/2501.12370/x10.png", "caption": "Figure 6: Scaling law fit on data obtained from training compute-optimal models. Figure\u00a06(a) shows the fit on the data used to estimate the coefficients for equation\u00a06, while Figure\u00a06(b) validates these coefficients on a held-out dataset. All data points with S=0.98\ud835\udc460.98S=0.98italic_S = 0.98 were excluded from the fitting process for out-of-sample validation.", "description": "\uadf8\ub9bc 6\uc740 \uacc4\uc0b0\ub7c9 \ucd5c\uc801\ud654 \ubaa8\ub378 \ud559\uc2b5\uc5d0\uc11c \uc5bb\uc740 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59 \uc801\ud569\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \ub4f1\uc2dd 6\uc758 \uacc4\uc218\ub97c \ucd94\uc815\ud558\ub294 \ub370 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc801\ud569\uc744, (b)\ub294 \uc0d8\ud50c \uc678 \uac80\uc99d\uc744 \uc704\ud574 S=0.98\uc778 \ubaa8\ub4e0 \ub370\uc774\ud130 \uc9c0\uc810\uc744 \uc81c\uc678\ud558\uace0 \ubcf4\ub958 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \ub300\ud55c \uacc4\uc218\uc758 \uc720\ud6a8\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5 \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc5d0 \uc2a4\ud30c\uc2a4\uc131 \ud1b5\ud569"}, {"figure_path": "https://arxiv.org/html/2501.12370/x11.png", "caption": "Figure 7: Accuracy of 6\u2062Na\u2062D6subscript\ud835\udc41\ud835\udc4e\ud835\udc376N_{a}D6 italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT italic_D FLOPs Estimator for MoEs.\nRatio of the MoE FLOPs estimator (Equation\u00a09) to the 6\u2062Na\u2062D6subscript\ud835\udc41\ud835\udc4e\ud835\udc376N_{a}D6 italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT italic_D estimator as a function of the total number of parameters, for a fixed context length of D=2048\ud835\udc372048D=2048italic_D = 2048, used in our experiments.", "description": "\uadf8\ub9bc 7\uc740 Mixture-of-Experts(MoE) \ubaa8\ub378\uc5d0 \ub300\ud55c FLOPs \ucd94\uc815 \ubc29\uc2dd\uc758 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ubcf8 \ub17c\ubb38\uc758 \ub4f1\uc2dd (9)\ub97c \uc0ac\uc6a9\ud558\uc5ec \uacc4\uc0b0\ub41c MoE FLOPs \ucd94\uc815\uac12\uacfc 6NaD \ucd94\uc815\uac12\uc758 \ube44\uc728\uc744 \ubaa8\ub378\uc758 \ucd1d \ud30c\ub77c\ubbf8\ud130 \uc218\uc758 \ud568\uc218\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uace0\uc815\ub41c context \uae38\uc774 D=2048\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud5d8\uc5d0\uc11c \uc5bb\uc740 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc744 \ud1b5\ud574 MoE \ubaa8\ub378\uc5d0 \ub300\ud55c 6NaD \ucd94\uc815 \ubc29\uc2dd\uc758 \uc815\ud655\ub3c4\ub97c \ud30c\ub77c\ubbf8\ud130 \uc218\uc5d0 \ub530\ub77c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "C Estimating Mixture-of-Expert (MoE) FLOPs"}, {"figure_path": "https://arxiv.org/html/2501.12370/x12.png", "caption": "Figure 8: \nIsoFLOP surfaces over total parameters N\ud835\udc41Nitalic_N, MoE sparsity S\ud835\udc46Sitalic_S, and pretraining loss L\ud835\udc3fLitalic_L for different compute budgets.\nThe rows correspond to IsoFLOP surface fitted using models trained with a budget of 3e19, 6e19, 1e20, 3e20, and 1e21.\nThe subplots on the left visualize IsoFLOP surfaces mapping total parameters N\ud835\udc41Nitalic_N and sparsity level S\ud835\udc46Sitalic_S to pretraining loss L\ud835\udc3fLitalic_L.\nThe subplots on the right correlate the ground-truth pretraining loss with the estimated pretraining loss on held-out data.\nTaken together, these results show that isoFLOP surfaces are accurate proxies for understanding how model size and MoE sparsity jointly impact pretraining loss.", "description": "\uadf8\ub9bc 8\uc740 \ub2e4\uc591\ud55c \uc5f0\uc0b0 \uc608\uc0b0(compute budget) \ud558\uc5d0\uc11c \ucd1d \ud30c\ub77c\ubbf8\ud130 \uc218(N), MoE \uc2a4\ud30c\uc2a4(S), \uadf8\ub9ac\uace0 \uc0ac\uc804 \ud6c8\ub828 \uc190\uc2e4(L)\uc5d0 \ub300\ud55c IsoFLOP \ud45c\uba74\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ud589\uc740 3e19, 6e19, 1e20, 3e20, 1e21\uc758 \uc608\uc0b0\uc73c\ub85c \ud6c8\ub828\ub41c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc801\ud569\ub41c IsoFLOP \ud45c\uba74\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4. \uc67c\ucabd \ud558\uc704 \uadf8\ub9bc\uc740 \ucd1d \ud30c\ub77c\ubbf8\ud130 \uc218(N)\uc640 \uc2a4\ud30c\uc2a4 \uc218\uc900(S)\uc744 \uc0ac\uc804 \ud6c8\ub828 \uc190\uc2e4(L)\uc5d0 \ub9e4\ud551\ud558\ub294 IsoFLOP \ud45c\uba74\uc744 \uc2dc\uac01\ud654\ud569\ub2c8\ub2e4. \uc624\ub978\ucabd \ud558\uc704 \uadf8\ub9bc\uc740 \ubcf4\ub958\ub41c \ub370\uc774\ud130\uc5d0 \ub300\ud55c \ucd94\uc815 \uc0ac\uc804 \ud6c8\ub828 \uc190\uc2e4\uacfc \uc2e4\uc81c \uc0ac\uc804 \ud6c8\ub828 \uc190\uc2e4\uc758 \uc0c1\uad00 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc885\ud569\uc801\uc73c\ub85c \uc774\ub7ec\ud55c \uacb0\uacfc\ub294 IsoFLOP \ud45c\uba74\uc774 \ubaa8\ub378 \ud06c\uae30\uc640 MoE \uc2a4\ud30c\uc2a4\uac00 \uc0ac\uc804 \ud6c8\ub828 \uc190\uc2e4\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc774\ud574\ud558\ub294 \ub370 \uc815\ud655\ud55c \uadfc\uc0ac\uce58\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2 The Interplay between Model Parameters and Sparsity in MoEs"}, {"figure_path": "https://arxiv.org/html/2501.12370/x13.png", "caption": "Figure 9: \nOptimal MoE configurations predictably change with training compute budget.\nEach row corresponds to an analysis of how optimal MoE sparsity S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT, total parameters N\u2217superscript\ud835\udc41N^{*}italic_N start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT, and active parameters Na\u2217subscriptsuperscript\ud835\udc41\ud835\udc4eN^{*}_{a}italic_N start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT change for a given training budget.\nThe subplots on the left show that (a) increasing the training budget increases the model size N\ud835\udc41Nitalic_N (denoted with black dots) with the minimum pretraining loss and (b) for models smaller than a threshold (which increases with training budget), dense models (i.e., 0%percent00\\%0 % sparsity) fare better than sparse MoEs.\nThe subplots in the second and third panel show that (a) increasing MoE sparsity increases the optimal total parameters N\u2217superscript\ud835\udc41N^{*}italic_N start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT and decreases the optimal active parameters Na\u2217subscriptsuperscript\ud835\udc41\ud835\udc4eN^{*}_{a}italic_N start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT. In both cases, for a fixed sparsity level, increasing the budget shifts increases the optimal total and active parameters.", "description": "\uadf8\ub9bc 9\ub294 \ud559\uc2b5 \ucef4\ud4e8\ud305 \uc608\uc0b0\uc5d0 \ub530\ub77c \ucd5c\uc801\uc758 MoE \uad6c\uc131\uc774 \uc608\uce21 \uac00\ub2a5\ud558\uac8c \ubcc0\ud654\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ud589\uc740 \uc8fc\uc5b4\uc9c4 \ud559\uc2b5 \ucef4\ud4e8\ud305 \uc608\uc0b0\uc5d0 \ub300\ud574 \ucd5c\uc801\uc758 MoE \uc2a4\ud30c\uc2a4\uc131 (S*), \ucd1d \ub9e4\uac1c\ubcc0\uc218 (N*), \ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218 (Na*)\uac00 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0 \ubd84\uc11d\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. \uc67c\ucabd \ud558\uc704 \uadf8\ub9bc\uc740 (a) \ud559\uc2b5 \ucef4\ud4e8\ud305 \uc608\uc0b0\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ucd5c\uc18c \uc0ac\uc804 \ud559\uc2b5 \uc190\uc2e4\uc744 \uac00\uc9c4 \ubaa8\ub378 \ud06c\uae30(N)\uac00 \uc99d\uac00\ud558\uace0 (b) \ud559\uc2b5 \ucef4\ud4e8\ud305 \uc608\uc0b0\ubcf4\ub2e4 \uc791\uc740 \ubaa8\ub378\uc758 \uacbd\uc6b0\uc5d0\ub294(\uc608\uc0b0\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc99d\uac00), \uc2a4\ud30c\uc2a4 MoE\ubcf4\ub2e4 \uc870\ubc00\ud55c \ubaa8\ub378(\uc989, \uc2a4\ud30c\uc2a4\uc131 0%)\uc774 \ub354 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub450 \ubc88\uc9f8\uc640 \uc138 \ubc88\uc9f8 \ud328\ub110\uc758 \ud558\uc704 \uadf8\ub9bc\uc740 (a) MoE \uc2a4\ud30c\uc2a4\uc131\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ucd5c\uc801\uc758 \ucd1d \ub9e4\uac1c\ubcc0\uc218(N*)\uac00 \uc99d\uac00\ud558\uace0 \ucd5c\uc801\uc758 \ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218(Na*)\uac00 \uac10\uc18c\ud558\uba70 (b) \uace0\uc815\ub41c \uc2a4\ud30c\uc2a4\uc131 \uc218\uc900\uc5d0\uc11c \uc608\uc0b0\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ucd5c\uc801\uc758 \ucd1d \ub9e4\uac1c\ubcc0\uc218\uc640 \ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218\uac00 \ubaa8\ub450 \uc99d\uac00\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3 Impact of Training Compute Budget on the Interaction between Model Parameters and Sparsity"}, {"figure_path": "https://arxiv.org/html/2501.12370/x14.png", "caption": "Figure 10: \nDownstream task performance vs. upstream pre-training loss.\nEach subplot shows the relationship between upstream pre-training loss (x-axis) and downstream task performance (y-axis) for a specific task.\nSimilar to our results in\u00a0Section\u00a04, we find that the MoE sparsity level does not change the relationship between upstream pre-training loss and downstream task performance.", "description": "\uadf8\ub9bc 10\uc740 \uc5c5\uc2a4\ud2b8\ub9bc \uc0ac\uc804 \ud6c8\ub828 \uc190\uc2e4(x\ucd95)\uacfc \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5 \uc131\ub2a5(y\ucd95) \uac04\uc758 \uad00\uacc4\ub97c \ud2b9\uc815 \uc791\uc5c5\uc5d0 \ub300\ud574 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ud558\uc704 \uadf8\ub9bc\uc740 \ud2b9\uc815 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc5c5\uc2a4\ud2b8\ub9bc \uc0ac\uc804 \ud6c8\ub828 \uc190\uc2e4\uacfc \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5 \uc131\ub2a5 \uac04\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc139\uc158 4\uc758 \uacb0\uacfc\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c, MoE \uc2a4\ud30c\uc2a4\uc131 \uc218\uc900\uc740 \uc5c5\uc2a4\ud2b8\ub9bc \uc0ac\uc804 \ud6c8\ub828 \uc190\uc2e4\uacfc \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5 \uc131\ub2a5 \uac04\uc758 \uad00\uacc4\ub97c \ubcc0\uacbd\ud558\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc5d0\uc11c \uc0ac\uc804 \ud6c8\ub828 \uc190\uc2e4\uacfc \uc131\ub2a5 \uac04\uc758 \uad00\uacc4\uac00 MoE \uc2a4\ud30c\uc2a4\uc131 \uc218\uc900\uc5d0 \ub530\ub77c \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 Effect of MoE Sparsity on Downstream Task Performance"}, {"figure_path": "https://arxiv.org/html/2501.12370/x15.png", "caption": "Figure 11: \nEffect of MoE sparsity on pretraining loss across different training compute budgets.\nAs sparsity increases, the validation loss decreases for all compute budgets, with larger budgets (darker lines) achieving lower losses at each sparsity level.\nThis trend is consistent with the findings from\u00a0Section\u00a03, demonstrating that increasing sparsity reduces the optimal pretraining loss across all compute budgets.", "description": "\uadf8\ub9bc 11\uc740 \ub2e4\uc591\ud55c \ud6c8\ub828 \uc5f0\uc0b0\ub7c9 \uc608\uc0b0\uc5d0\uc11c MoE(Mixture-of-Experts)\uc758 sparsity\uac00 pretraining loss\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. sparsity\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uac80\uc99d loss\ub294 \ubaa8\ub4e0 \uc5f0\uc0b0\ub7c9 \uc608\uc0b0\uc5d0\uc11c \uac10\uc18c\ud558\uba70, \ub354 \ud070 \uc608\uc0b0(\ub354 \uc5b4\ub450\uc6b4 \uc120)\uc77c\uc218\ub85d \uac01 sparsity \uc218\uc900\uc5d0\uc11c \ub354 \ub0ae\uc740 loss\ub97c \ub2ec\uc131\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacbd\ud5a5\uc740 3\uc7a5\uc758 \uacb0\uacfc\uc640 \uc77c\uce58\ud558\uba70, sparsity\ub97c \uc99d\uac00\uc2dc\ud0a4\uba74 \ubaa8\ub4e0 \ud6c8\ub828 \uc5f0\uc0b0\ub7c9 \uc608\uc0b0\uc5d0\uc11c \ucd5c\uc801\uc758 pretraining loss\uac00 \uac10\uc18c\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3 Impact of Training Compute Budget on the Interaction between Model Parameters and Sparsity"}, {"figure_path": "https://arxiv.org/html/2501.12370/x16.png", "caption": "Figure 12: \nEffect of MoE sparsity on optimal total and active parameters across different training compute budgets.\nEach row shows the change in total and active parameters as a function of sparsity level for fixed training budgets.\nIncreasing sparsity leads to an increase in the optimal total parameters while reducing the optimal active parameters, consistent with our findings in\u00a0Section\u00a02 (Figure\u00a02).\nLarger training compute budgets result in higher optimal (total and active) parameters across all sparsity levels.", "description": "\uadf8\ub9bc 12\ub294 \uace0\uc815\ub41c \ud6c8\ub828 \ucef4\ud4e8\ud305 \uc608\uc0b0 \ud558\uc5d0\uc11c \ub2e4\uc591\ud55c \ud6c8\ub828 \ucef4\ud4e8\ud305 \uc608\uc0b0\uc5d0 \uac78\uccd0 \ucd5c\uc801\uc758 \ucd1d \ub9e4\uac1c\ubcc0\uc218\uc640 \ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218\uc5d0 \ub300\ud55c MoE \uc2a4\ud30c\uc2a4\ud2f0\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ud589\uc740 \uace0\uc815\ub41c \ud6c8\ub828 \uc608\uc0b0\uc5d0 \ub300\ud574 \uc2a4\ud30c\uc2a4\ud2f0 \uc218\uc900\uc758 \ud568\uc218\ub85c \ucd1d \ub9e4\uac1c\ubcc0\uc218\uc640 \ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc2a4\ud30c\uc2a4\ud2f0\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ucd5c\uc801\uc758 \ucd1d \ub9e4\uac1c\ubcc0\uc218\ub294 \uc99d\uac00\ud558\uace0 \ucd5c\uc801\uc758 \ud65c\uc131 \ub9e4\uac1c\ubcc0\uc218\ub294 \uac10\uc18c\ud558\ub294\ub370, \uc774\ub294 2\uc808(\uadf8\ub9bc 2)\uc758 \uacb0\uacfc\uc640 \uc77c\uce58\ud569\ub2c8\ub2e4. \ub354 \ud070 \ud6c8\ub828 \ucef4\ud4e8\ud305 \uc608\uc0b0\uc740 \ubaa8\ub4e0 \uc2a4\ud30c\uc2a4\ud2f0 \uc218\uc900\uc5d0\uc11c \ub354 \ub192\uc740 \ucd5c\uc801\uc758 (\ucd1d \ubc0f \ud65c\uc131) \ub9e4\uac1c\ubcc0\uc218\ub97c \ucd08\ub798\ud569\ub2c8\ub2e4.", "section": "D \ucd94\uac00 \ubd84\uc11d"}]