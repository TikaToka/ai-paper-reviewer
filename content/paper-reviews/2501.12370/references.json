{"references": [{"fullname_first_author": "J. Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-01", "reason": "This paper introduces scaling laws for neural language models, providing a foundational framework for understanding and predicting the performance of LLMs."}, {"fullname_first_author": "J. Hoffmann", "paper_title": "An empirical analysis of compute-optimal large language model training", "publication_date": "2022-01-01", "reason": "This paper provides insights into the optimal balance between model size and data size when the training compute budget is fixed, offering valuable insights for efficient LLM training."}, {"fullname_first_author": "A. Clark", "paper_title": "Unified scaling laws for routed language models", "publication_date": "2022-01-01", "reason": "This paper introduces unified scaling laws for routed language models, addressing the interplay between parameters and FLOPs per example and the impact of sparsity on scaling laws for MoEs."}, {"fullname_first_author": "W. Fedus", "paper_title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "publication_date": "2022-01-01", "reason": "This paper introduces Switch Transformers, a novel architecture that scales to trillion-parameter models with simple and efficient sparsity, demonstrating the effectiveness of sparse MoEs."}, {"fullname_first_author": "J. Ludziejewski", "paper_title": "Scaling laws for fine-grained mixture of experts", "publication_date": "2024-01-01", "reason": "This paper investigates the role of variables like the number and granularity of experts in MoEs, providing insights into the interaction between FLOPs per example and total parameters in MoEs."}]}