{"references": [{"fullname_first_author": "Patrick S. H. Lewis", "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks", "publication_date": "2020-12-06", "reason": "This paper is foundational to the field of Retrieval-Augmented Generation (RAG), a core concept explored and extended by the current research."}, {"fullname_first_author": "Ori Ram", "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks", "publication_date": "2023-12-06", "reason": "This work provides a comprehensive overview of RAG techniques, highlighting their significance and challenges, directly influencing the current study's approach."}, {"fullname_first_author": "Rohan Anil", "paper_title": "Gemini: A family of highly capable multimodal models", "publication_date": "2023-12-06", "reason": "The Gemini multimodal models are leveraged in the current research, making this paper vital to understanding the foundation model used for VideoRAG."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-01", "reason": "BERT's contextualized embeddings are essential to several methods compared against VideoRAG, highlighting BERT's importance to the broader research context."}, {"fullname_first_author": "Yi Wang", "paper_title": "InternVideo2: Scaling foundation models for multimodal video understanding", "publication_date": "2024-09-29", "reason": "InternVideo2, the specific Large Video Language Model (LVLM) used in this research, significantly shapes the VideoRAG framework's capabilities, thus highlighting this paper's importance."}]}