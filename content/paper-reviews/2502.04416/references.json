{"references": [{"fullname_first_author": "S. Zhang", "paper_title": "Opt: Open pre-trained transformer language models", "publication_date": "2022-05-01", "reason": "This paper introduces OPT, a large language model that serves as a foundational model for many of the techniques discussed in the target paper."}, {"fullname_first_author": "H. Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "Llama 2 is a significant model that is directly used and modified in many of the experiments performed in the target paper."}, {"fullname_first_author": "Z. Liu", "paper_title": "Deja vu: Contextual sparsity for efficient LLMs at inference time", "publication_date": "2023-07-01", "reason": "This paper introduces the concept of contextual sparsity, a key idea that is leveraged and expanded in the target paper's methodology."}, {"fullname_first_author": "Z. Zhang", "paper_title": "Moefication: Transformer feed-forward layers are mixtures of experts", "publication_date": "2021-10-01", "reason": "This paper presents MoEfication, a foundational method for converting dense transformer models into Mixture-of-Experts architectures, a central theme of the target paper."}, {"fullname_first_author": "T. Zhu", "paper_title": "Llama-moe: Building mixture-of-experts from llama with continual pre-training", "publication_date": "2024-01-01", "reason": "This paper directly tackles the problem of constructing MoE models from existing dense LLMs, which is the core focus of the target paper, and serves as a primary comparison point."}]}