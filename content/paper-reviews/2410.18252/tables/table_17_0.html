<table id='0' style='font-size:16px'><tr><td>Hyperparameter</td><td>Value</td></tr><tr><td>Model</td><td>Meta-Llama-3.1-8B</td></tr><tr><td>Max Sequence Length</td><td>4,096</td></tr><tr><td>Batch Size (effective)</td><td>128</td></tr><tr><td>Learning Rate</td><td>5.0 x 10-6</td></tr><tr><td>Learning Rate Schedule</td><td>Linear</td></tr><tr><td>Learning Rate Warmup Ratio</td><td>0.03</td></tr><tr><td>Learning Rate Weight Decay</td><td>0.0</td></tr><tr><td>Number of Epochs</td><td>2</td></tr></table>