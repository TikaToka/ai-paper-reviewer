{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is All You Need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation of many modern large language models and is fundamental to this research."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "publication_date": "2020-07-01", "reason": "This paper introduced the T5 model, a powerful text-to-text transformer that is used as a benchmark in this work, highlighting its significance."}, {"fullname_first_author": "Ofir Press", "paper_title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation", "publication_date": "2021-08-12", "reason": "This paper addressed the challenge of long sequence processing, a key issue that TAPE tackles and is a crucial related work."}, {"fullname_first_author": "Sean McLeish", "paper_title": "Transformers can do arithmetic with the right embeddings", "publication_date": "2024-05-17", "reason": "This paper directly inspired the research by demonstrating the limitations of existing positional embeddings in handling arithmetic tasks, motivating the development of TAPE."}, {"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "publication_date": "2022-12-01", "reason": "This paper introduced FlashAttention, an efficient attention mechanism that is used for optimizing TAPE's computational performance and is a direct technical contribution."}]}