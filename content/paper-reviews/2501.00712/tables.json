[{"content": "| Metric (%) | QAS | QAS | CNLI | CNLI | NQA | QuAL | QMS | SumS | GovR |\n|---|---|---|---|---|---|---|---|---|---|---|\n|  | F1 | EM | F1 | EM | F1 | EM | Rgm | Rgm | Rgm |\n| Median length | 5472 | 2148 | 57829 | 7171 | 14197 | 9046 | 8841 |  |  |\n| RoPE (Kitaev et al., 2020) | 8.39 | 65.00 | 1.77 | 0.04 | 6.34 | 5.63 | 9.71 |  |  |\n| ALiBi (Press et al., 2021a) | 8.25 | 69.62 | 4.11 | 0.0 | 9.92 | 9.78 | 18.81 |  |  |\n| RandPE (Ruoss et al., 2023) | 13.44 | 62.01 | 4.63 | 0.38 | 8.43 | 8.31 | 8.93 |  |  |\n| FIRE (Li et al., 2023) | 3.41 | 71.26 | 0.48 | 1.25 | 8.78 | 7.42 | 11.03 |  |  |\n| xPos (Sun et al., 2022) | 9.02 | 71.75 | 4.83 | 0.24 | 10.73 | 9.38 | 16.38 |  |  |\n| TAPE (ours) | 11.52 | 72.80 | 6.79 | 11.60 | 12.42 | 10.34 | 15.18 |  |  |", "caption": "Table 1: Performance comparison on seven datasets from SCROLLS benchmark.", "description": "\ubcf8 \ub17c\ubb38\uc758 \ud45c 1\uc740 SCROLLS \ubca4\uce58\ub9c8\ud06c\uc758 7\uac00\uc9c0 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \uc704\uce58 \uc778\ucf54\ub529 \ubc29\ubc95\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, QAS, CNLI, NQA, QuAL, QMS, SumS, GovR \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec F1 \uc810\uc218, EM \uc810\uc218, ROUGE \uc810\uc218 \ub4f1\uc758 \ub2e4\uc591\ud55c \uc9c0\ud45c\ub97c \ud1b5\ud574 ROPE, ALiBi, RandPE, FIRE, xPos \ub4f1\uc758 \uae30\uc874 \ubc29\ubc95\ub4e4\uacfc \uc81c\uc548\ub41c TAPE \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \ud45c\ub294 \uac01 \ubc29\ubc95\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\uc5b4, TAPE\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\ub2e4 \uba85\ud655\ud788 \uc124\uba85\ud558\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "4 EXPERIMENTS"}, {"content": "| Method | Proof-pile 1024 | Proof-pile 2048 | Proof-pile 4096 | Proof-pile 8192 | PG19 1024 | PG19 2048 | PG19 4096 | PG19 8192 |\n|---|---|---|---|---|---|---|---|---|\n| LoRA | 3.828 | 3.369 | 3.064 | 2.867 | 9.791 | 9.098 | 8.572 | 8.199 |\n| LongLoRA | 3.918 | 3.455 | 3.153 | 2.956 | 9.989 | 9.376 | 8.948 | 8.645 |\n| Theta Scaling | 3.864 | 3.415 | 3.121 | 2.934 | 9.257 | 8.640 | 8.241 | 7.999 |\n| TAPE | 3.641 | 3.196 | 2.901 | 2.708 | 8.226 | 7.642 | 7.278 | 7.063 |", "caption": "Table 2: Evaluation on perplexity across different context lengths.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \ubb38\ub9e5 \uae38\uc774\uc5d0 \ub530\ub978 perplexity \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uae38\uc774\uc758 \uc2dc\ud000\uc2a4\uc5d0 \ub300\ud574 \uc5ec\ub7ec \ubaa8\ub378\uc758 perplexity \uac12\uc744 \ube44\uad50\ud558\uc5ec \ubaa8\ub378\uc758 \uae34 \ubb38\ub9e5 \ucc98\ub9ac \uc131\ub2a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c\ub294  Proof-pile \uacfc PG19 \ub370\uc774\ud130\uc14b\uc758 1024, 2048, 4096, 8192 \ud1a0\ud070 \uae38\uc774\uc5d0 \ub300\ud55c perplexity \uc218\uce58\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.3 Context Window Extension by Parameter-Efficient Tuning"}, {"content": "| Method | TAPE | RoPE | FIRE | T5's relative bias |\n|---|---|---|---|---|\n| FLOPS (G) | 365.65 | 321.10 | 331.97 | 321.10 |\n| MACs (G) | 180.69 | 160.46 | 165.69 | 160.46 |\n| Params. (M) | 155.33 | 154.89 | 154.90 | 154.90 |", "caption": "Table 3: Comparison of FLOPS, MACs, and the number of parameters for models with different position embeddings.", "description": "\ubcf8 \ud45c\ub294 \uc11c\ub85c \ub2e4\ub978 \uc704\uce58 \uc778\ucf54\ub529 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\ub294 \ubaa8\ub378\ub4e4\uc758 FLOPS, MACs \ubc0f \ud30c\ub77c\ubbf8\ud130 \uc218\ub97c \ube44\uad50\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  TAPE, ROPE, FIRE \ubc0f T5\uc758 \uc0c1\ub300\uc801 \ud3b8\ud5a5 \ub4f1 \ub2e4\uc591\ud55c \uc704\uce58 \uc778\ucf54\ub529 \uae30\ubc95\uc744 \uc801\uc6a9\ud55c \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \uacc4\uc0b0\ub7c9 \uce21\uba74\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 TAPE \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uc744 \uae30\uc874 \ubc29\ubc95\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.4 \ud6a8\uc728\uc131 \ubd84\uc11d"}, {"content": "| Method | TAPE w/ Fusion | TAPE w/o Fusion | RoPE | FIRE | T5's relative bias |\n|---|---|---|---|---|---| \n| Time (\u00d710\u207b\u2074) | 2.56 | 5.63 | 2.08 | 5.56 | 6.90 |\n| Throughput | 3910 | 1775 | 4810 | 1799 | 1449 |\n| Flash Attention | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 |", "caption": "Table 4: System measurement.\nWe report execution time per step in the Time row and iteration per second in the Throughput row. The values are averaged over 100 inference steps.", "description": "\ud45c 4\ub294 \uc2dc\uc2a4\ud15c \uc131\ub2a5 \uce21\uc815 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  '\uc2dc\uac04' \ud589\uc740 100\ud68c\uc758 \ucd94\ub860 \ub2e8\uacc4\uc5d0 \uac78\uce5c \ud3c9\uade0 \uc2e4\ud589 \uc2dc\uac04\uc744, '\ucc98\ub9ac\ub7c9' \ud589\uc740 \ucd08\ub2f9 \ubc18\ubcf5 \ud69f\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 TAPE, RoPE, FIRE \ubc0f T5\uc758 \uc0c1\ub300\uc801 \ud3b8\ud5a5\uc744 \uac00\uc9c4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "4.4 \ud6a8\uc728\uc131 \ubd84\uc11d"}, {"content": "| Method | Setting | \n|---|---| \n| Arithmetic (\u00a74.1) | Sequence length: 80 | \n|  | Batch size: 512 | \n|  | Number of iterations: 20k | \n|  | Attention dropout prob.: 0.0 | \n|  | Optimizer: AdamW | \n|  | Learning rate: 1e-4 | \n| C4 Pre-training (\u00a74.2) | Sequence length: 1024 | \n|  | Batch size: 512 | \n|  | Number of iterations: 10k | \n|  | Attention dropout prob.: 0.0 | \n|  | Optimizer: AdamW | \n|  | Learning rate: 1e-4 | \n| SCROLLS (\u00a74.2) | Sequence length: 1024 | \n|  | Batch size: 64 | \n|  | Number of iterations: 1k | \n|  | Attention dropout prob.: 0.0 | \n|  | Optimizer: AdamW | \n|  | Learning rate: 1e-5 | \n| Context Extension (\u00a74.3) | Sequence length: 8096 | \n|  | Batch size: 64 | \n|  | Number of iterations: 1k | \n|  | Attention dropout prob.: 0.0 | \n|  | Optimizer: AdamW | \n|  | Learning rate: 2e-5 | ", "caption": "Table 5: Training recipe for language model pre-training and fine-tuning in experiments.", "description": "\ud45c 5\ub294 \ub17c\ubb38\uc758 \uc2e4\ud5d8\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc5b8\uc5b4 \ubaa8\ub378 \uc0ac\uc804 \ud559\uc2b5 \ubc0f \ubbf8\uc138 \uc870\uc815\uc5d0 \ub300\ud55c \uad50\uc721 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \uc2e4\ud5d8 \uc124\uc815\uc5d0 \ub300\ud55c \uc2dc\ud000\uc2a4 \uae38\uc774, \ubc30\uce58 \ud06c\uae30, \ubc18\ubcf5 \ud69f\uc218, \uc5b4\ud150\uc158 \ub4dc\ub86d\uc544\uc6c3 \ud655\ub960, \ucd5c\uc801\ud654\uae30, \ud559\uc2b5\ub960 \ub4f1\uc758 \uc138\ubd80 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc0ac\uc804 \ud559\uc2b5 \ubc0f \ubbf8\uc138 \uc870\uc815 \ubaa8\ub450\uc5d0 \ub300\ud574 \uc138 \uac00\uc9c0 \uc2e4\ud5d8 \uc124\uc815\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub17c\ubb38\uc758 \uc2e4\ud5d8 \ubd80\ubd84\uc744 \uc774\ud574\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8"}, {"content": "| Architecture | Perplexity |\n|---|---|---|---|---|\n| **Attention** | **Feed Forward** | **128** | **256** | **512** | **1024** |\n| \u2717 | \u2717 | 139.2 | 92.8 | 69.3 | 57.2 |\n| \u2717 | \u2713 | 143.3 | 95.0 | 70.7 | 58.4 |\n| \u2713 | \u2717 | 142.7 | 94.3 | 70.1 | 57.6 |\n| \u2713 | \u2713 | 132.0 | 86.6 | 63.9 | 52.2 |\n| **Rotation Equivariance** | **Tensorial Embedding** |  |  |  |  |\n| \u2717 | \u2717 | 140.7 | 92.1 | 68.2 | 56.2 |\n| \u2713 | \u2717 | 138.4 | 91.3 | 67.8 | 55.7 |\n| \u2717 | \u2713 | 132.9 | 87.8 | 65.4 | 54.1 |\n| \u2713 | \u2713 | 132.0 | 86.6 | 63.9 | 52.2 |", "caption": "Table 6: Ablation study on TAPE architecture. We evalute pre-trained models\u2019 perplexity across varying sequence lengths on the GitHub test set.", "description": "\ud45c 6\uc740 TAPE \uc544\ud0a4\ud14d\ucc98\uc5d0 \ub300\ud55c ablation \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc2dc\ud000\uc2a4 \uae38\uc774\uc5d0 \ub300\ud574 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 perplexity\ub97c GitHub \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0\uc11c \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, \uc5b4\ud150\uc158 \ub808\uc774\uc5b4\uc640 MLP \ub808\uc774\uc5b4\uc758 \uc544\ud0a4\ud14d\ucc98 \ub514\uc790\uc778, \ud68c\uc804 \ub4f1\ubcc0\uc131(rotation equivariance), \ud150\uc11c \uc784\ubca0\ub529(tensorial embedding)\uc758 \uc138 \uac00\uc9c0 \uce21\uba74\uc5d0 \ub300\ud55c ablation \uc2e4\ud5d8\uc744 \uc218\ud589\ud558\uc5ec \uac01 \uc694\uc18c\uac00 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 ablation \uc124\uc815\uc5d0 \ub300\ud55c perplexity \uac12\uc774 \ub2e4\uc591\ud55c \uc2dc\ud000\uc2a4 \uae38\uc774\uc5d0 \ub300\ud574 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uc774\ub97c \ud1b5\ud574 TAPE \uc544\ud0a4\ud14d\ucc98\uc758 \uac01 \uad6c\uc131 \uc694\uc18c\uc758 \uc911\uc694\uc131\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. OUR APPROACH"}, {"content": "| **TAPE** | **Perplexity** |  |  |  |  |\n|---|---|---|---|---|---| \n| **Added Params. (M)** | **$I$** | **128** | **256** | **512** | **1024** |\n| 0.11 | 12 | 133.2 | 87.9 | 65.2 | 53.6 |\n| 0.22 | 24 | 133.0 | 86.1 | 63.2 | 51.8 |\n| 0.44 | 48 | 132.0 | 86.6 | 63.9 | 52.2 |\n| 0.88 | 96 | 133.2 | 87.5 | 64.5 | 52.7 |\n| 1.76 | 192 | 133.0 | 87.3 | 64.5 | 53.0 |", "caption": "Table 7: Ablation study on TAPE hyperparameter I\ud835\udc3cIitalic_I. We evalute pre-trained models\u2019 perplexity across varying sequence lengths on the GitHub test set.", "description": "\ud45c 7\uc740 TAPE\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 I\uc758 \uc601\ud5a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uae38\uc774\uc758 \uc2dc\ud000\uc2a4\uc5d0 \ub300\ud574 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 perplexity\ub97c GitHub \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0\uc11c \uce21\uc815\ud558\uc600\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 I\uc758 \uac12\uc744 \ubcc0\uacbd\ud588\uc744 \ub54c perplexity\uc5d0 \uc5b4\ub5a4 \uc601\ud5a5\uc774 \uc788\ub294\uc9c0 \ubcf4\uc5ec\uc90c\uc73c\ub85c\uc368 \ucd5c\uc801\uc758 I \uac12\uc744 \ucc3e\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  \uacb0\uacfc\uc801\uc73c\ub85c, I\uc758 \uac12\uc740 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc774 \uc81c\ud55c\uc801\uc774\uc9c0\ub9cc 2H \uc5d0\uc11c 4H \uc0ac\uc774\uc758 \uac12\uc774 \uc131\ub2a5\uc5d0 \ub354 \ub098\uc740 \uacb0\uacfc\ub97c \uac00\uc838\uc624\ub294 \uacbd\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3 OUR APPROACH"}, {"content": "| Atten. Diff. (<math>\\times 10^{-2}</math>) | Add Tokens |  |  |  | Shift IDs |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---| \n|  | Layer 1 | Layer 2 | Layer 4 | Layer 8 | Layer 1 | Layer 2 | Layer 4 | Layer 8 |\n|---|---|---|---|---|---|---|---|---|---|\n| RoPE | 8.93 | 8.51 | 12.29 | 11.46 | 0.01 | 0.02 | 0.02 | 0.03 |\n| TAPE | 9.08 | 11.24 | 12.23 | 13.78 | 0.01 | 0.02 | 0.04 | 0.04 |\n| w/o EQ | 11.30 | 11.38 | 13.32 | 14.55 | 0.01 | 0.24 | 0.37 | 0.51 |", "caption": "Table 8: Comparison of RoPE, TAPE, and TAPE without equivariance (w/o EQ) under positional shifts. The table shows differences in attention weights (top) and positional embedding dot products (bottom) across layers for two shift methods: adding three [BOS] tokens (\u201cAdd Tokens\u201d) and starting position IDs at 3 (\u201cShift IDs\u201d).", "description": "\ud45c 8\uc740 \uc704\uce58 \uc774\ub3d9\uc5d0 \ub530\ub978 RoPE, TAPE \ubc0f \ub3d9\ubcc0\ud658 \uc5c6\ub294 TAPE\uc758 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc704\uce58 \uc774\ub3d9 \ubc29\ubc95\uc740 \ub450 \uac00\uc9c0\ub85c, \uc138 \uac1c\uc758 [BOS] \ud1a0\ud070 \ucd94\uac00 \ubc0f \uc2dc\uc791 \uc704\uce58 ID\ub97c 3\uc73c\ub85c \uc124\uc815\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub450 \uac00\uc9c0 \uc704\uce58 \uc774\ub3d9 \ubc29\ubc95\uc5d0 \ub300\ud574 \uac01 \uce35\uc5d0\uc11c \uc5b4\ud150\uc158 \uac00\uc911\uce58(\uc704\ucabd)\uc640 \uc704\uce58 \uc784\ubca0\ub529 \ub0b4\uc801(\uc544\ub798\ucabd)\uc758 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.4 \ud6a8\uc728\uc131 \ubd84\uc11d"}, {"content": "| PE Dot Prod. | Diff. (%) | Add Tokens |  |  |  |  | Shift IDs |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  |  | Layer 1 | Layer 2 | Layer 4 | Layer 8 | Layer 1 | Layer 2 | Layer 4 | Layer 8 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| RoPE | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 |\n| TAPE | 0.03 | 0.37 | 2.75 | 6.62 | 0.03 | 0.02 | 0.03 | 0.04 |\n| w/o EQ | 0.03 | 2.29 | 3.34 | 6.37 | 0.03 | 0.54 | 0.44 | 0.86 |", "caption": "Table 9: Accuracy in Percentage Across Methods and Benchmarks", "description": "\ud45c 9\ub294 \ub2e4\uc591\ud55c \ubc29\ubc95\ub4e4\uc744 \uc0ac\uc6a9\ud558\uc5ec \uce21\uc815\ub41c \uc5ec\ub7ec \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \ubc31\ubd84\uc728\ub85c \ub098\ud0c0\ub0b8 \uac83\uc785\ub2c8\ub2e4.  MMLU(Massive Multitask Language Understanding) \ubc0f ARC(AI2 Reasoning Challenge) \ubca4\uce58\ub9c8\ud06c\uc758 \ud558\uc704 \ubca4\uce58\ub9c8\ud06c(\uc778\ubb38\ud559, \uc0ac\ud68c\uacfc\ud559, STEM, \uae30\ud0c0, ARC Challenge, ARC Easy) \ubcc4\ub85c LoRA, LongLoRA, ThetaScaling, TAPE\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 TAPE \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\ub978 \uae30\uc874 \ubc29\ubc95\ub4e4\uacfc \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "4.3 Context Window Extension by Parameter-Efficient Tuning"}, {"content": "| Method | Humanities | Social Sciences | STEM | Other | ARC Challenge | ARC Easy |\n|---|---|---|---|---|---|---|\n| LoRA | 39.09 \u00b1 0.69 | 46.47 \u00b1 0.88 | 33.65 \u00b1 0.83 | 45.83 \u00b1 0.89 | 45.31 \u00b1 1.45 | 74.28 \u00b1 0.90 |\n| LongLoRA | 37.53 \u00b1 0.69 | 43.55 \u00b1 0.88 | 32.54 \u00b1 0.83 | 43.84 \u00b1 0.88 | 45.31 \u00b1 1.45 | 74.16 \u00b1 0.90 |\n| ThetaScaling | 37.45 \u00b1 0.69 | 43.16 \u00b1 0.88 | 33.05 \u00b1 0.83 | 44.64 \u00b1 0.88 | 45.65 \u00b1 1.46 | 74.24 \u00b1 0.90 |\n| TAPE | 37.96 \u00b1 0.69 | 45.40 \u00b1 0.88 | 33.27 \u00b1 0.83 | 45.06 \u00b1 0.88 | 46.25 \u00b1 1.46 | 74.16 \u00b1 0.90 |", "caption": "Table 10: Comparing answers of different methods on example questions in QuALITY.", "description": "\ubcf8 \ud45c\ub294 QuALITY \ub370\uc774\ud130\uc14b\uc758 \ub450 \uc9c8\ubb38\uc5d0 \ub300\ud574 \ub2e4\uc591\ud55c positional encoding \ubc29\ubc95(TAPE, RoPE, xPos, RandPE, ALiBi)\uc758 \ub2f5\ubcc0\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubc29\ubc95\uc758 \uc815\ub2f5 \uc5ec\ubd80(EM)\ub97c \ud45c\uc2dc\ud558\uc5ec, TAPE\uc758 \uc131\ub2a5 \uc6b0\uc218\uc131\uacfc \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc758 \ud55c\uacc4\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788, TAPE\ub294 \uc815\ub2f5 \ub610\ub294 \uc815\ub2f5\uacfc \uc720\uc0ac\ud55c \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud558\uc9c0\ub9cc, \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc740 \ubd80\uc815\ud655\ud558\uac70\ub098 \ube44\ubb38\ub9e5\uc801\uc778 \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud558\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 ARITHMETIC LEARNING"}, {"content": "| Method | Question A | EM | Question B | EM |\n|---|---|---|---|---|\n| Ground Truth | The secret service budget was small | \u2713 | Only the private quarters or the office restroom | \u2713 |\n| TAPE | The secret service budget was small | \u2713 | Only the private quarters | \u2717 |\n| xPos | They were all they were waiting for | \u2717 | Only a tiny part of the right of the right to leave foreverish | \u2717 |\n| RandPE | Their human opinion was trusted by others who have trust the services of their people | \u2717 | Only a handsome man | \u2717 |\n| RoPE | Their orless them together with their repories did not only they didn\u2019s never done was never done was never done\u2026 (repeating) | \u2717 | The/O only the full-College All of the full-College All of the full-College\u2026 (repeating) | \u2717 |\n| ALiBi | Jimmy Carter is the president\u2019s de facto president | \u2717 | Jimmy Carter is the president\u2019s de facto president | \u2717 |", "caption": "Table 11: Example Questions in QuALITY", "description": "\ud45c 11\uc740 \ub17c\ubb38\uc758 QuALITY \uc139\uc158\uc5d0\uc11c \uc608\uc2dc\ub85c \uc81c\uc2dc\ub41c \uc9c8\ubb38\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc9c8\ubb38\uc740 QuALITY \ub370\uc774\ud130\uc14b\uc5d0 \uc18d\ud558\uba70, \uae34 \ubcf8\ubb38\uc744 \ubc14\ud0d5\uc73c\ub85c \ub2f5\uc744 \uc720\ucd94\ud574\uc57c \ud558\ub294 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \ud45c\ub294 \uac01 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub9e5\ub77d \uc815\ubcf4\uc640 \uc815\ub2f5 \ud6c4\ubcf4\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uc73c\uba70, \uc774\ub97c \ud1b5\ud574  \ubaa8\ub378\uc774 \uae34 \ubb38\ub9e5\uc744 \uc774\ud574\ud558\uace0 \uc815\ub2f5\uc744 \ub3c4\ucd9c\ud558\ub294 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "4.1 ARITHMETIC LEARNING"}]