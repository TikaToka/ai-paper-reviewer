[{"content": "| model | qa1 | qa2 | qa3 | qa4 | qa5 | qa6 | qa7 | qa8 | qa9 | qa10 | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **0K** |  |  |  |  |  |  |  |  |  |  |  |\n| Llama-3.2-1.2B | 54.0 | 25.0 | 29.0 | 62.0 | 59.0 | 49.0 | 14.0 | 52.0 | 41.0 | 22.0 | 40.7 |\n| vanilla-Llama-1.7B | 86.0 | 57.0 | 46.0 | 59.0 | 85.0 | 83.0 | 95.0 | 79.0 | 83.0 | 77.0 | 75.0 |\n| RMT-1.7B | 85.0 | 49.0 | 49.0 | 81.0 | 95.0 | 84.0 | 82.0 | 78.0 | 85.0 | 76.0 | 76.4 |\n| LM2-1.7B | **99.0** | **89.0** | **70.0** | **88.0** | **98.0** | **95.0** | **96.0** | **97.0** | **99.0** | **94.0** | **92.5** |\n| **1K** |  |  |  |  |  |  |  |  |  |  |  |\n| Llama-3.2-1.2B | 48.0 | 22.0 | 24.0 | 55.0 | 69.0 | 49.0 | 9.0 | 31.0 | 55.0 | 33.0 | 39.5 |\n| Llama-3.2-1.2B-RAG | 51.0 | 14.0 | 19.0 | 59.0 | 80.0 | 49.0 | 10.0 | 38.0 | 40.0 | 46.0 | 40.6 |\n| vanilla-Llama-1.7B | 31.0 | 21.0 | 44.0 | 43.0 | 71.0 | 60.0 | 71.0 | 40.0 | 67.0 | 58.0 | 50.6 |\n| RMT-1.7B | 35.0 | 26.0 | 29.0 | 33.0 | 61.0 | 50.0 | 83.0 | 41.0 | 68.0 | 53.0 | 47.9 |\n| LM2-1.7B | **85.0** | **59.0** | **72.0** | **68.0** | **91.0** | **84.0** | **96.0** | **69.0** | **82.0** | **77.0** | **78.3** |\n| **2K** |  |  |  |  |  |  |  |  |  |  |  |\n| Llama-3.2-1.2B | 44.0 | 18.0 | 19.0 | **50.0** | 64.0 | 52.0 | 18.0 | 24.0 | 55.0 | 42.0 | 38.6 |\n| Llama-3.2-1.2B-RAG | 52.0 | 11.0 | 12.0 | 49.0 | 75.0 | 48.0 | 5.0 | 33.0 | 50.0 | 43.0 | 37.8 |\n| vanilla-Llama-1.7B | 25.0 | 22.0 | 37.0 | 34.0 | 58.0 | 60.0 | 65.0 | 38.0 | 66.0 | 58.0 | 46.3 |\n| RMT-1.7B | 44.0 | 21.0 | 43.0 | 41.0 | 79.0 | 47.0 | 78.0 | 41.0 | 69.0 | 51.0 | 51.4 |\n| LM2-1.7B | **58.0** | **43.0** | **64.0** | 43.0 | **87.0** | **73.0** | **93.0** | **53.0** | **75.0** | **69.0** | **65.8** |\n| **4K** |  |  |  |  |  |  |  |  |  |  |  |\n| Llama-3.2-1.2B | 37.0 | 16.0 | 25.0 | 56.0 | 56.0 | 50.0 | 14.0 | 27.0 | 55.0 | 32.0 | 36.8 |\n| Llama-3.2-1.2B-RAG | 47.0 | 3.0 | 16.0 | **58.0** | 68.0 | 58.0 | 3.0 | 36.0 | 45.0 | 39.0 | 37.3 |\n| vanilla-Llama-1.7B | 21.0 | 18.0 | 38.0 | 28.0 | 55.0 | 61.0 | 64.0 | 35.0 | 49.0 | 53.0 | 42.2 |\n| RMT-1.7B | 24.0 | 20.0 | 22.0 | 24.0 | 28.0 | 46.0 | 75.0 | 35.0 | **65.0** | 45.0 | 38.4 |\n| LM2-1.7B | **46.0** | **37.0** | **48.0** | 34.0 | **78.0** | **66.0** | **93.0** | **45.0** | 62.0 | **50.0** | **55.9** |\n| **AVG. Length \u22658K** |  |  |  |  |  |  |  |  |  |  |  |\n| Llama-3.2-1.2B | 19.0 | 8.0 | 17.8 | 27.3 | 36.5 | 49 | 21.3 | 12.8 | 48.0 | 41.8 | 28.2 |\n| Llama-3.2-1.2B-RAG | 29.3 | 1.0 | 5.0 | **55.8** | **72.0** | **49.8** | 4.8 | 22.8 | 46.3 | 36.8 | 32.3 |\n| vanilla-Llama-1.7B | 11.3 | 15.0 | 21.3 | 14.5 | 31.0 | 44.0 | 63.0 | 33.5 | 42.0 | 36.3 | 31.2 |\n| RMT-1.7B | 17.5 | 14.5 | 20.5 | 22.5 | 20.3 | 47.0 | 73.3 | 34.5 | 62.5 | **43.0** | 35.5 |\n| LM2-1.7B | **23.8** | **15.0** | **24.5** | 24.0 | 38.8 | 47.3 | **92.8** | **37.0** | **53.8** | 42.0 | **39.9** |", "caption": "Table 1: Performance on the BABILong dataset: All models are evaluated on various context lengths ranging from 0K, 1K, 2K, and 4K to an aggregated average length of \u22658\u2062Kabsent8\ud835\udc3e\\geq 8K\u2265 8 italic_K. Qa stands for various subsets.\nDue to page limits, we aggregate the results for 8K, 16K, 32K, 64K, and 128K into a single metric, with detailed results provided in Appendix\u00a0B.", "description": "\ud45c 1\uc740 BABILong \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 0K, 1K, 2K, 4K \uae38\uc774\uc758 \ubb38\ub9e5\uc744 \uc0ac\uc6a9\ud55c \ud3c9\uac00 \uacb0\uacfc\uc640 8K \uc774\uc0c1 \uae38\uc774\uc758 \ubb38\ub9e5\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'qa'\ub294 \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5 \uc9d1\ud569\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud398\uc774\uc9c0 \uc81c\ud55c\uc73c\ub85c \uc778\ud574 8K, 16K, 32K, 64K, 128K \uae38\uc774\uc758 \ubb38\ub9e5\uc5d0 \ub300\ud55c \uacb0\uacfc\ub294 \ub2e8\uc77c \uc9c0\ud45c\ub85c \uc9d1\uacc4\ub418\uc5c8\uc73c\uba70, \uc790\uc138\ud55c \uacb0\uacfc\ub294 \ubd80\ub85d B\uc5d0 \uc218\ub85d\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ubcf8 \ud45c\ub294 \ub2e4\uc591\ud55c \uae38\uc774\uc758 \ubb38\ub9e5\uc5d0\uc11c \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \ubaa8\ub378\uc758 \uc7a5\ub2e8\uc810\uacfc \uba54\ubaa8\ub9ac \ud6a8\uc728\uc131\uc744 \ud30c\uc545\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4.1 \uba54\ubaa8\ub9ac \uc791\uc5c5 \uc131\ub2a5"}, {"content": "|                   | vanilla | Llama | RMT | LM2 |\n|---|---|---|---|---|\n| **Subject** <br> **Category** |  |  |  |  |\n| STEM | 27.2 | 25.7 | **28.1** |  |\n| Humanities | 28.7 | 26.7 | **32.2** |  |\n| Social Sciences | 29.2 | 27.0 | **31.6** |  |\n| Others | 27.7 | 27.1 | **28.0** |  |\n| **Difficulty** <br> **Level** |  |  |  |  |\n| High School | 28.8 | 26.5 | **30.4** |  |\n| College | 27.7 | 27.1 | **29.0** |  |\n| Professional | 27.5 | 26.6 | **27.6** |  |\n| General <br> Knowledge | 27.2 | 25.6 | **28.5** |  |\n| Average | 28.0 | 26.5 | **29.4** |  |", "caption": "Table 2: Performance on MMLU dataset. For better visualization, the dataset is categorized on two criteria - subject and difficulty.", "description": "\ud45c 2\ub294 MMLU \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c LM2 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc2dc\uac01\ud654\ub97c \uc704\ud574 \ub370\uc774\ud130\uc14b\uc740 \ub450 \uac00\uc9c0 \uae30\uc900, \uc989 \uacfc\ubaa9(STEM, \uc778\ubb38\ud559, \uc0ac\ud68c\uacfc\ud559, \uae30\ud0c0)\uacfc \ub09c\uc774\ub3c4(\uace0\ub4f1\ud559\uad50, \ub300\ud559\uad50, \uc804\ubb38\uac00, \uc77c\ubc18 \uc0c1\uc2dd)\ub85c \ubd84\ub958\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \uacfc\ubaa9\uacfc \ub09c\uc774\ub3c4\uc5d0 \ub530\ub978 LM2, Vanilla Llama, RMT \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 LM2 \ubaa8\ub378\uc774 \uc77c\ubc18\uc801\uc778 \uc5b8\uc5b4 \ubaa8\ub378\ubcf4\ub2e4 \ub2e4\uc591\ud55c \uacfc\ubaa9\uacfc \ub09c\uc774\ub3c4\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.2 \uc77c\ubc18 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5"}, {"content": "| Model | \n|---|---| \n| vanilla | \n| Llama | ", "caption": "Table 3: Detailed performance of BABILong benchmark", "description": "\ud45c 3\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c BABILong \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uc131\ub2a5 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ubb38\ub9e5 \uae38\uc774(0K, 1K, 2K, 4K, 8K, 16K, 32K, 64K, 128K \ud1a0\ud070)\uc5d0 \uac78\uccd0 Llama-3.2-1.2B, Llama-3.2-1.2B-RAG, vanilla-Llama-1.7B, RMT-1.7B, LM2-1.7B \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758  10\uac00\uc9c0 \ud558\uc704 \uc791\uc5c5(qa1~qa10)\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \uc218\uce58\ub85c \uc81c\uc2dc\ud558\uc5ec, \ub2e4\uc591\ud55c \ubb38\ub9e5 \uae38\uc774\uc640 \uc791\uc5c5 \uc720\ud615\uc5d0 \ub530\ub978 \uac01 \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \uc0c1\uc138\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uac01 \ubaa8\ub378\uc758 \uc7a5\ub2e8\uc810\uacfc \ud2b9\ud788 LM2 \ubaa8\ub378\uc758 \uc6b0\uc218\uc131\uc744 \uba85\ud655\ud558\uac8c \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.1 \uba54\ubaa8\ub9ac \uc791\uc5c5\uc5d0 \ub300\ud55c \uc131\ub2a5"}]