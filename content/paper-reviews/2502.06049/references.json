{"references": [{"fullname_first_author": "Joshua Ainslie", "paper_title": "ETC: encoding long and structured data in transformers", "publication_date": "2020-04-08", "reason": "This paper proposes a novel method for encoding long and structured data in transformers, which is highly relevant to the current research on large memory models."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper demonstrates that large language models can achieve strong performance on various tasks with limited training data, which is a key motivation for the current research on efficient memory augmentation."}, {"fullname_first_author": "Aydar Bulatov", "paper_title": "Recurrent memory transformer", "publication_date": "2022-07-06", "reason": "This paper introduces a recurrent memory transformer architecture that improves performance on long-context reasoning tasks by maintaining a recurrent prompt to track long-term information, providing a direct baseline for the current work."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2021-01-01", "reason": "This paper introduces a large-scale multitask language understanding benchmark (MMLU) that is used to evaluate the generalization capabilities of the model, providing a standard benchmark for the evaluation of the model."}, {"fullname_first_author": "Yuri Kuratov", "paper_title": "Babilong: Testing the limits of llms with long context reasoning-in-a-haystack", "publication_date": "2024-01-01", "reason": "This paper introduces a challenging benchmark dataset for evaluating the performance of large language models on long-context reasoning tasks, which is directly used for evaluating the performance of LM2."}]}