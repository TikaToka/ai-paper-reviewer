{"references": [{"fullname_first_author": "Steven Bills", "paper_title": "Language models can explain neurons in language models", "publication_date": "2023-05-14", "reason": "This paper introduces an automated interpretability pipeline using LLMs to explain neurons, a foundational concept for the current work."}, {"fullname_first_author": "Mor Geva", "paper_title": "Transformer feed-forward layers are key-value memories", "publication_date": "2021-11-19", "reason": "This paper provides insights into the representation mechanism of LLMs, which influences the understanding of features and their descriptions."}, {"fullname_first_author": "Atticus Geiger", "paper_title": "Causal abstraction: A theoretical foundation for mechanistic interpretability", "publication_date": "2024-01-03", "reason": "This paper introduces a theoretical framework for mechanistic interpretability, providing a basis for evaluating and improving feature description methods."}, {"fullname_first_author": "Gon\u00e7alo Paulo", "paper_title": "Automatically interpreting millions of features in large language models", "publication_date": "2024-10-13", "reason": "This paper addresses the challenge of scaling automated interpretability, which is directly relevant to the efficiency concerns of the proposed methods."}, {"fullname_first_author": "Adly Templeton", "paper_title": "Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet", "publication_date": "2024-08-01", "reason": "This paper proposes methods for enhancing automated interpretability, focusing on output-centric approaches and contributing to the comparative analysis of methods."}]}