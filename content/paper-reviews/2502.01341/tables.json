[{"content": "| Model | DocVQA VAL | InfoVQA VAL | DeepForm TEST | KLC TEST | WTQ TEST | TabFact TEST | ChartQA TEST | TextVQA VAL | TableVQA TEST | Avg. Score |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **Closed-Source VLMs** |  |  |  |  |  |  |  |  |  |  |\n| (*Opaque Training Data*) |  |  |  |  |  |  |  |  |  |  |\n| Claude-3.5 Sonnet | 88.48 | 59.05 | 31.41 | 24.82 | 47.13 | 53.48 | 51.84 | **71.42** | **81.27** | 56.54 |\n| GeminiPro-1.5 | 91.23 | **73.94** | 32.16 | 24.07 | **50.29** | 71.22 | 34.68 | 68.16 | 80.43 | 58.46 |\n| GPT-4o 20240806 | **92.80** | 66.37 | **38.39** | **29.92** | 46.63 | **81.10** | **85.70** | 70.46 | 72.87 | **64.91** |\n| **Open-Source Instruct VLMs** |  |  |  |  |  |  |  |  |  |  |\n| (*Semi-Opaque Training Data*) |  |  |  |  |  |  |  |  |  |  |\n| Janus-1.3B [Wu et al., 2024a] | 30.15 | 17.09 | 0.62 | 15.06 | 9.30 | 51.34 | 57.20 | 51.97 | 18.67 | 27.93 |\n| Qwen2-VL-2B [Wang et al., 2024] | **89.16** | **64.11** | 32.38 | 25.18 | 38.20 | 57.21 | 73.40 | 79.90 | 43.07 | **55.84** |\n| InternVL-2.5-2B [Chen et al., 2024b] | 87.70 | 61.85 | 13.14 | 16.58 | 36.33 | 57.26 | 74.96 | 76.85 | 42.20 | 51.87 |\n| DeepSeek-VL2-Tiny-3.4B [Wu et al., 2024b] | 88.57 | 63.88 | 25.11 | 19.04 | 35.07 | 52.15 | 80.92 | **80.48** | 56.30 | 55.72 |\n| Phi3.5-Vision-4B [Abdin et al., 2024] | 86.00 | 56.20 | 10.47 | 7.49 | 17.18 | 30.43 | **82.16** | 73.12 | **70.70** | 48.19 |\n| Qwen2-VL-7B [Wang et al., 2024] | **93.83** | **76.12** | 34.55 | 23.37 | **52.52** | 74.68 | **83.16** | **84.48** | **53.97** | **64.08** |\n| LLaVA-NeXT-7B [Xu et al., 2024] | 63.51 | 30.90 | 1.30 | 5.35 | 20.06 | 52.83 | 52.12 | 65.10 | 32.87 | 36.00 |\n| DocOwl1.5-8B [Hu et al., 2024] | 80.73 | 49.94 | **68.84** | **37.99** | 38.87 | **79.67** | 68.56 | 68.91 | 52.60 | 60.68 |\n| InternVL-2.5-8B [Chen et al., 2024b] | 91.98 | 75.36 | 34.55 | 22.31 | 50.33 | 74.75 | 82.84 | 79.00 | 52.10 | 62.58 |\n| Ovis-1.6-Gemma2-9B [Lu et al., 2024] | 88.84 | 73.97 | 45.16 | 23.91 | 50.72 | 76.66 | 81.40 | 77.73 | 48.33 | 62.96 |\n| Llama3.2-11B [Grattafiori et al., 2024] | 82.71 | 36.62 | 1.78 | 3.47 | 23.03 | 58.33 | 23.80 | 54.28 | 22.40 | 34.04 |\n| Pixtral-12B [Agrawal et al., 2024] | 87.67 | 49.45 | 27.37 | 24.07 | 45.18 | 73.53 | 71.80 | 76.09 | 67.13 | 58.03 |\n| **Document Understanding Instructed Models** |  |  |  |  |  |  |  |  |  |  |\n| (*Instruction Tuned on BigDocs-7.5M + DocDownStream [Rodriguez et al., 2024a; Hu et al., 2024]*) |  |  |  |  |  |  |  |  |  |  |\n| Qwen2-VL-2B (base+) [Qwen et al., 2025] | 57.23 | 31.88 | 49.31 | 34.39 | 31.61 | 64.75 | 68.60 | **61.01** | 47.53 | 49.59 |\n| **AlignVLM-Llama-3.2-1B (ours)** | 72.42 | 38.16 | 60.47 | 33.71 | 28.66 | 71.31 | 65.44 | 48.81 | 50.29 | 52.14 |\n| **AlignVLM-Llama-3.2-3B (ours)** | **79.63** | **44.53** | **63.49** | **35.25** | **38.59** | **78.51** | **71.88** | 57.38 | **60.10** | **58.81** |\n| DocOwl1.5-8B (base+) [Hu et al., 2024] | 78.70 | 47.62 | 64.39 | 36.93 | 35.69 | 72.65 | 65.80 | 67.30 | 49.03 | 57.56 |\n| Llama3.2-11B (base+) [Grattafiori et al., 2024] | 78.99 | 44.27 | **67.05** | **37.22** | 40.18 | 78.04 | 71.40 | **68.46** | 56.73 | 60.26 |\n| **AlignVLM-Llama-3.1-8B (ours)** | **81.18** | **53.75** | 63.25 | 35.50 | **45.31** | **83.04** | **75.00** | 64.60 | **64.33** | **62.88** |", "caption": "Table 1: Main Results on General Document Benchmarks. We compare AlignVLM (ours) with state-of-the-art (SOTA) open and closed-source instructed models, and with base models that we trained using the process described in Section\u00a03.3. AlignVLM models outperform all Base VLM models trained in the same data regime. Our models also perform competitively across document benchmarks even compared with SOTA models, in which the data regime is more targeted and optimized. Color coding for comparison: closed-source models, open-source models below 7B parameters, open-source models between 7-12B parameters.", "description": "\ud45c 1\uc740 \uc77c\ubc18\uc801\uc778 \ubb38\uc11c \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc8fc\uc694 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\uc5d0\uc11c\ub294 ALIGNVLM \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ucd5c\ucca8\ub2e8(SOTA) \uc624\ud508 \ubc0f \ud074\ub85c\uc988 \uc18c\uc2a4 \uc9c0\uc2dc \ubaa8\ub378 \ubc0f 3.3\uc808\uc5d0 \uc124\uba85\ub41c \uc808\ucc28\uc5d0 \ub530\ub77c \ud559\uc2b5\ub41c \uae30\ubcf8 \ubaa8\ub378\uacfc \ube44\uad50\ud569\ub2c8\ub2e4. ALIGNVLM \ubaa8\ub378\uc740 \ub3d9\uc77c\ud55c \ub370\uc774\ud130 \uccb4\uc81c\ub85c \ud559\uc2b5\ub41c \ubaa8\ub4e0 \uae30\ubcf8 VLM \ubaa8\ub378\ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc6b0\uc218\ud569\ub2c8\ub2e4. \ub610\ud55c, \ubcf8 \uc5f0\uad6c\uc758 \ubaa8\ub378\uc740 \ub370\uc774\ud130 \uccb4\uc81c\uac00 \ub354\uc6b1 \ubaa9\ud45c \uc9c0\ud5a5\uc801\uc774\uace0 \ucd5c\uc801\ud654\ub41c \ucd5c\ucca8\ub2e8 \ubaa8\ub378\uacfc \ube44\uad50\ud588\uc744 \ub54c\uc5d0\ub3c4 \uc5ec\ub7ec \ubb38\uc11c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0c9\uc0c1 \ucf54\ub4dc\ub294 \ud074\ub85c\uc988 \uc18c\uc2a4 \ubaa8\ub378, 7B \ubbf8\ub9cc \ub9e4\uac1c\ubcc0\uc218\uc758 \uc624\ud508 \uc18c\uc2a4 \ubaa8\ub378, 7-12B \ub9e4\uac1c\ubcc0\uc218\uc758 \uc624\ud508 \uc18c\uc2a4 \ubaa8\ub378\uc744 \uad6c\ubd84\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "5. \uacb0\uacfc"}, {"content": "| Model | DocVQA VAL | InfoVQA VAL | DeepForm TEST | KLC TEST | WTQ TEST | TabFact TEST | ChartQA TEST | TextVQA VAL | TableVQA TEST | Avg. Score |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Llama-3.2-3B-**MLP** | 71.46 | 37.56 | 62.07 | 33.36 | 28.94 | 73.22 | 66.48 | 53.56 | 50.96 | 53.06 |\n| Llama-3.2-3B-**Perciever R.** | 69.08 | 34.13 | 57.08 | 31.75 | 27.95 | 71.93 | 65.16 | 51.33 | 47.76 | 50.68 |\n| Llama-3.2-3B-**Ovis** | 74.68 | 42.11 | 58.02 | 33.50 | 33.13 | 76.67 | 67.92 | 52.60 | 53.93 | 54.72 |\n| Llama-3.2-3B-**Align** (ours) | **79.63** | **44.53** | **63.49** | **35.25** | **38.59** | **78.51** | **71.88** | **57.38** | **60.10** | **58.81** |", "caption": "Table 2: Impact of Connector Designs on VLM Performance: We present the results of experiments evaluating different connector designs for conditioning LLMs on visual features. Our proposed Align connector is compared against a basic Multi-Layer Perceptron (MLP), the Perceiver Resampler, and Ovis. The results demonstrate that Align consistently outperforms these alternatives across all benchmarks.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \ube44\uc804-\uc5b8\uc5b4 \ubaa8\ub378(VLM) \ucee4\ub125\ud130 \uc124\uacc4\uc758 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc81c\uc548\ub41c Align \ucee4\ub125\ud130\ub294 \uae30\ubcf8 \ub2e4\uce35 \ud37c\uc149\ud2b8\ub860(MLP), Perceiver Resampler \ubc0f Ovis\uc640 \ube44\uad50\ub429\ub2c8\ub2e4. \uc2e4\ud5d8 \uacb0\uacfc\ub294 Align \ucee4\ub125\ud130\uac00 \ubaa8\ub4e0 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub2e4\ub978 \ub300\uc548\ubcf4\ub2e4 \uc77c\uad00\ub418\uac8c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.2. \ucee4\ub125\ud130 \uc124\uacc4\uc758 VLM \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5"}, {"content": "| GT: | (appears in written history in) |\n|---|---| \n| MLP: | (<span style=\"color:#FF0000;\">census</span> in written history in)<span style=\"color:#FF0000;\">\u2717</span> |\n| Align | (appears in written history in)<span style=\"color:#00FF00;\">\u2713</span> |", "caption": "Table 3: Robustness to Noise. Comparison of Avg. Scores with and without Gaussian noise (\u03c3=3\ud835\udf0e3\\sigma=3italic_\u03c3 = 3), including performance drop (\u0394\u0394\\Deltaroman_\u0394).", "description": "\ud45c 3\uc740 \uac00\uc6b0\uc2dc\uc548 \ub178\uc774\uc988\uac00 \ucd94\uac00\ub418\uc5c8\uc744 \ub54c\uc640 \ucd94\uac00\ub418\uc9c0 \uc54a\uc558\uc744 \ub54c\uc758 \ud3c9\uade0 \uc810\uc218\ub97c \ube44\uad50\ud558\uc5ec \ubaa8\ub378\uc758 \uacac\uace0\uc131\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\uc6b0\uc2dc\uc548 \ub178\uc774\uc988\uc758 \ud45c\uc900\ud3b8\ucc28\ub294 3\uc73c\ub85c \uc124\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ub178\uc774\uc988\uac00 \uc5c6\ub294 \uacbd\uc6b0\uc640 \ub178\uc774\uc988\uac00 \uc788\ub294 \uacbd\uc6b0\uc758 \ud3c9\uade0 \uc810\uc218\uc640 \uc131\ub2a5 \uc800\ud558 \uc815\ub3c4(\u0394)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \ub178\uc774\uc988\uc5d0 \ub300\ud55c \uac15\uac74\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.5. Robustness to Noise Analysis"}, {"content": "| GT: | (the system used for assigning) |\n|---|---| \n| MLP: | (the system used for <font color=\"red\">accounting</font>) <font color=\"red\">\u2717</font> |\n| Align | (the system used for assigning) <font color=\"green\">\u2713</font> |", "caption": "Table 4: Detailed hyperparameters for each training stage across different LLM backbones.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 \uc2e4\ud5d8 \uc124\uc815\uc5d0 \ub300\ud55c \uc138\ubd80 \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.  \uc138 \uac00\uc9c0 \ub2e4\ub978 \ud06c\uae30\uc758 LLM \ubc31\ubcf8(Llama 3.2-1B, Llama 3.2-3B, Llama 3.1-8B)\uc5d0 \ub300\ud574 \uc138 \uac00\uc9c0 \ud6c8\ub828 \ub2e8\uacc4(Stage 1, Stage 2, Stage 3)\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ub2e8\uacc4\ub294 \ub2e4\ub978 \ub370\uc774\ud130\uc14b\uacfc \ud6c8\ub828 \ubaa9\ud45c\ub97c \uac00\uc9c0\uace0 \uc788\uc73c\uba70, \uc774\uc5d0 \ub530\ub77c \ud6c8\ub828 \uac00\ub2a5\ud55c \ud30c\ub77c\ubbf8\ud130, \ubc30\uce58 \ud06c\uae30, \ud14d\uc2a4\ud2b8 \ucd5c\ub300 \uae38\uc774, \uc5d0\ud3ed \uc218, \ud559\uc2b5\ub960 \ub4f1\uc774 \ub2e4\ub985\ub2c8\ub2e4.  \uc774 \ud45c\ub97c \ud1b5\ud574 \ub3c5\uc790\uac00 \ub17c\ubb38\uc5d0\uc11c \uc218\ud589\ub41c \uc2e4\ud5d8\uc758 \uc7ac\ud604\uc131\uc744 \ub192\uc77c \uc218 \uc788\ub3c4\ub85d \uad6c\uccb4\uc801\uc778 \uc2e4\ud5d8 \uc124\uc815\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "A.1. Experimental Setup"}]