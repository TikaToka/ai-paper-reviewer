{"references": [{"fullname_first_author": "Alayrac, J.-B.", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-04-26", "reason": "This paper introduces Flamingo, a visual language model that is frequently compared against in the current paper, establishing it as a key benchmark in the field."}, {"fullname_first_author": "Liu, H.", "paper_title": "LLaVA-NeXT: Improved reasoning, ocr, and world knowledge", "publication_date": "2024-01-30", "reason": "This paper introduces LLaVA-NeXT, a model which is frequently cited and compared against in the current paper, highlighting its significance in the multimodal document understanding field."}, {"fullname_first_author": "Lu, S.", "paper_title": "Ovis: Structural embedding alignment for multimodal large language model", "publication_date": "2024-05-21", "reason": "This paper introduces Ovis, a model that is frequently compared against in the current paper, which proposes a novel approach for multimodal understanding that is analyzed and contrasted in the current paper."}, {"fullname_first_author": "Rodriguez, J. A.", "paper_title": "BigDocs: An open and permissively-licensed dataset for training multimodal models on document and code tasks", "publication_date": "2024-12-03", "reason": "This paper introduces a large multimodal dataset, BigDocs, that is used in this current paper. It is crucial as it provides the training data for the experiments."}, {"fullname_first_author": "Zhang, T.", "paper_title": "VCR: Visual caption restoration", "publication_date": "2024-06-12", "reason": "This paper introduces the VCR benchmark, which is used in this current paper for evaluating the ability of vision-language models to handle pixel-level visual and textual cues."}]}