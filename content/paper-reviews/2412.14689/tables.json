[{"content": "|                     | ArXiv | Books2 | Books3 | Math  | Enron | EuroParl | FreeLaw | GitHub | PG-19  | HackerNews | NIH   | Avg   |\n| :------------------ | :---- | :----- | :----- | :---- | :---- | :------ | :------ | :----- | :---- | :-------- | :---- | :---- |\n| Human data          | 22.26 | 25.39  | 22.87  | 10.84 | 23.50 | 30.73   | 12.04   | 4.15   | 16.88 | 32.54     | 23.53 |       |\n| 25% Synthetic Data | 21.86 | 26.32  | 23.87  | 11.05 | 24.85 | 35.02   | 12.84   | 4.35   | 17.99 | 33.80     | 23.76 |       |\n| 50% Synthetic Data | 22.50 | 28.01  | 25.75  | 10.84 | 26.56 | 41.99   | 14.02   | 4.67   | 19.70 | 36.12     | 24.61 |       |\n| 75% Synthetic Data | 24.35 | 31.19  | 28.98  | 11.81 | 30.30 | 56.32   | 16.03   | 5.30   | 22.75 | 40.44     | 26.19 |       |\n| Synthetic Data     | 35.60 | 43.72  | 47.72  | 17.25 | 66.97 | 129.75  | 29.62   | 12.00  | 50.14 | 87.95     | 39.48 |       |\n|                     | OpenSubts | OWT2  | Phil  | Pile-CC | PubMed-A | PubMed-C | StackEx | Ubuntu | USPTO  | Wikipedia | Youtube | Avg   |\n| Human data          | 28.08   | 25.77 | 33.56 | 26.78  | 18.97   | 15.49   | 10.81  | 20.86 | 19.32  | 24.31     | 21.54 | 21.37 |\n| 25% Synthetic Data | 29.25   | 26.94 | 34.63 | 27.83  | 19.55   | 15.38   | 11.03  | 22.32 | 19.58  | 25.88     | 22.63 | 22.31 |\n| 50% Synthetic Data | 31.00   | 28.76 | 37.48 | 29.36  | 20.51   | 15.89   | 11.54  | 23.53 | 20.51  | 27.57     | 24.91 | 23.90 |\n| 75% Synthetic Data | 34.18   | 32.04 | 42.39 | 32.17  | 22.33   | 16.92   | 12.55  | 26.54 | 22.21  | 30.68     | 28.98 | 27.03 |\n| Synthetic Data     | 57.83   | 53.94 | 78.18 | 54.69  | 34.82   | 23.87   | 20.47  | 51.78 | 37.24  | 46.12     | 65.49 | 49.30 |", "caption": "Table 1: PPL evaluation results for GPT-2 Small (124M) pre-trained on data mixture. The PPL increases as the proportion of synthetic data grows, providing further confirmation of Figure\u00a02.", "description": "\ud45c 1\uc740 GPT-2 Small(124M) \ubaa8\ub378\uc744 \ub2e4\uc591\ud55c \ube44\uc728\uc758 \ud569\uc131 \ub370\uc774\ud130\uc640 \uc2e4\uc81c \ub370\uc774\ud130\ub97c \ud63c\ud569\ud558\uc5ec \uc0ac\uc804 \ud6c8\ub828\uc2dc\ud0a8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ud569\uc131 \ub370\uc774\ud130\uc758 \ube44\uc728\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c perplexity(PPL) \uac12\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\uba70, \uc774\ub294 \uadf8\ub9bc 2\uc758 \uacb0\uacfc\ub97c \ub4b7\ubc1b\uce68\ud569\ub2c8\ub2e4.  \uc989, \ubaa8\ub378 \uc131\ub2a5\uc774 \ud569\uc131 \ub370\uc774\ud130 \ube44\uc728\uc774 \ub192\uc544\uc9c8\uc218\ub85d \uc800\ud558\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b(ArXiv, Books2, Books3, Math, Enron, EuroParl, FreeLaw, GitHub, PG-19, HackerNews, NIH, OpenSubtitles, OWT2, Phil, Pile-CC, PubMed-A, PubMed-C, StackExchange, Ubuntu, USPTO, Wikipedia, Youtube)\uc5d0 \ub300\ud55c PPL \uac12\uc744 \ube44\uad50\ud558\uc5ec \ud569\uc131 \ub370\uc774\ud130\uc758 \uc601\ud5a5\uc744 \uc790\uc138\ud788 \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2 Non-iterative Model Collapse"}, {"content": "| Models | MQP | ChemProt | PubMedQA | RCT | USMLE | Average |\n|---|---|---|---|---|---|---|\n| OLMo-1B | 52.59 | 17.2 | 51.40 | 32.70 | 28.90 | 36.63 |\n| CPT | 52.29 | 21.00 | 58.50 | 34.90 | 27.49 | 38.83 |\n| \u0394 ToEdit | 54.59 | 22.40 | 65.00 | 34.50 | 27.96 | 40.89 |\n| LLama-3-8B | 66.80 | 28.59 | 60.8 | 73.85 | 40.61 | 54.13 |\n| CPT | 72.29 | 29.4 | 69.1 | 72.65 | 36.76 | 56.04 |\n| \u0394 ToEdit | 76.39 | 30.2 | 65.3 | 73.30 | 37.23 | 56.48 |\n|  |  |  |  |  |  |  |\n| Models | HeadLine | FPB | FiQA-SA | ConvFinQA | NER | Average |\n|---|---|---|---|---|---|---|\n| OLMo-1B | 69.00 | 47.03 | 48.05 | 4.83 | 62.19 | 46.22 |\n| CPT | 70.31 | 49.78 | 40.36 | 18.72 | 60.44 | 47.92 |\n| \u0394 ToEdit | 71.77 | 51.39 | 46.06 | 18.85 | 62.97 | 50.21 |\n| LLama-3-8B | 81.28 | 63.58 | 81.60 | 52.88 | 72.53 | 70.37 |\n| CPT | 85.68 | 54.22 | 81.88 | 67.78 | 67.43 | 71.40 |\n| \u0394 ToEdit | 83.83 | 61.61 | 80.82 | 67.31 | 67.62 | 72.24 |\n|  |  |  |  |  |  |  |\n| Models | ARC-c | GPQA | GSM8K | MATH | MMLU | Average |\n|---|---|---|---|---|---|---|\n| OLMo-1B | 28.67 | 24.23 | 1.67 | 0.00 | 26.56 | 16.23 |\n| CPT | 28.41 | 24.03 | 1.52 | 0.10 | 27.23 | 16.26 |\n| \u0394 ToEdit | 28.92 | 28.12 | 2.20 | 0.10 | 23.63 | 16.59 |", "caption": "Table 2: Performance on domain-specific tasks for continual pre-training models. CPT indicates continual pre-training. \u0394\u0394\\Deltaroman_\u0394 denotes training with our edited data. Our method demonstrates consistent improvements across three domains on both OLMo-1B and Llama-3-8B.", "description": "\ud45c 2\ub294 \uc9c0\uc18d\uc801 \uc0ac\uc804 \ud6c8\ub828 \ubaa8\ub378\uc5d0 \ub300\ud55c \ub3c4\uba54\uc778\ubcc4 \uacfc\uc81c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. CPT\ub294 \uc9c0\uc18d\uc801 \uc0ac\uc804 \ud6c8\ub828\uc744 \ub098\ud0c0\ub0b4\uace0, \u0394\u0394\n\u0394\ub294 \uc800\ud76c\uac00 \ud3b8\uc9d1\ud55c \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud55c \ud6c8\ub828\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 OLMo-1B\uc640 Llama-3-8B \ubaa8\ub450\uc5d0\uc11c \uc138 \uac00\uc9c0 \ub3c4\uba54\uc778\uc5d0 \uac78\uccd0 \uc77c\uad00\ub41c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c \ud1a0\ud070 \ud3b8\uc9d1 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ub41c \ubc18\ud569\uc131 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud55c \uc9c0\uc18d\uc801 \uc0ac\uc804 \ud6c8\ub828\uc774 OLMo-1B \uc640 Llama-3-8B \ubaa8\ub378 \ubaa8\ub450\uc5d0\uc11c \uc0dd\uc758\ud559, \uae08\uc735, \uc218\ud559 \uc138 \uac00\uc9c0 \ub3c4\uba54\uc778\uc758 \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5\uc5d0\uc11c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uac00\uc838\uc654\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2 NON-ITERATIVE MODEL COLLAPSE"}, {"content": "|           | PIQA | BoolQ | OBQA | ARC-c | ARC-e | HellaSwag | SIQA | Winogrande | Average |\n| :--------- | :----: | :----: | :----: | :----: | :----: | :-------: | :----: | :--------: | :------: |\n| OLMo-1B (PT) | 53.97 | 38.26 | 12.20 | 17.23 | 28.36 |  26.02   | 34.80 |   51.14   |  32.75  |\n| <div style=\"text-align:left;\">\u0394 ToEdit</div> | 54.13 | 38.65 | 12.80 | 18.43 | 27.48 |  25.94   | 34.95 |   52.49   |  33.11  |", "caption": "Table 3: General performance of the pre-trained base models. PT indicates we pre-train OLMo-1B from scratch. Experimental results demonstrate that our method can also enhance the effectiveness of pre-training.", "description": "\ud45c 3\uc740 \uc0ac\uc804 \ud6c8\ub828\ub41c \uae30\ubcf8 \ubaa8\ub378\ub4e4\uc758 \uc77c\ubc18\uc801\uc778 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. PT\ub294 OLMo-1B\ub97c \ucc98\uc74c\ubd80\ud130 \uc0ac\uc804 \ud6c8\ub828\uc2dc\ucf30\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc2e4\ud5d8 \uacb0\uacfc\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uc774 \uc0ac\uc804 \ud6c8\ub828\uc758 \ud6a8\uacfc\ub97c \ub192\uc77c \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud558\ub958 \uc791\uc5c5(\ud558\ub958 \uacfc\uc81c)\uc5d0\uc11c OLMo-1B\uc640 ToEdit(\uc81c\uc548\ub41c \ubc29\ubc95 \uc801\uc6a9) \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec, ToEdit\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\uc5c8\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ud558\ub958 \uc791\uc5c5\uc758 \uc131\ub2a5\uc740 \ud574\ub2f9 \uc791\uc5c5\uc5d0 \ub9de\ub294 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uce21\uc815\ub429\ub2c8\ub2e4.  \ud45c\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\ub4ef\uc774,  \ub300\ubd80\ubd84\uc758 \ud558\ub958 \uc791\uc5c5\uc5d0\uc11c ToEdit\uc744 \uc801\uc6a9\ud55c \ubaa8\ub378\uc774 \uae30\ubcf8 OLMo-1B \ubaa8\ub378\ubcf4\ub2e4 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2.1 \uc0ac\uc804 \ud6c8\ub828 \ub370\uc774\ud130 \ud63c\ud569"}, {"content": "|       | Models       | PIQA  | BoolQ | HellaSwag | SIQA  | Winogrande | Average |\n| :---- | :----------- | :---- | :---- | :-------- | :---- | :--------- | :------ |\n| Instruction Tuning |             |       |       |           |       |           |        |\n| _Natural Instructions_ | Llama-3-8B | 79.82 | 87.06 | 58.32     | 46.83 | 74.66      | 69.34  |\n| \u0394 ToEdit             |             | 80.58 | 87.80 | 58.27     | 46.93 | 74.90      | 69.70  |\n| _CoT_                 | Llama-3-8B | 79.87 | 81.28 | 59.72     | 49.69 | 74.51      | 69.01  |\n| \u0394 ToEdit             |             | 80.25 | 81.16 | 59.74     | 50.56 | 74.59      | 69.26  |\n| _FLAN v2_             | Llama-3-8B | 80.79 | 84.04 | 59.98     | 51.43 | 74.66      | 70.18  |\n| \u0394 ToEdit             |             | 80.69 | 85.20 | 59.99     | 52.00 | 75.37      | 70.65  |\n| _Open Assistant 1_   | Llama-3-8B | 79.65 | 83.18 | 60.51     | 48.52 | 74.11      | 69.19  |\n| \u0394 ToEdit             |             | 79.98 | 83.91 | 60.34     | 48.31 | 74.66      | 69.44  |", "caption": "Table 4: Performance of the SFT models. We fine-tune LLaMA-3-8B using instruction tuning and code reasoning tasks, comparing performance with the edited version produced by our method. The experimental results indicate that our approach can enhance the data for instruction-tuning and code reasoning tasks.", "description": "\ud45c 4\ub294 \uc9c0\uc2dc \uc870\uc815 \ubc0f \ucf54\ub4dc \ucd94\ub860 \uc791\uc5c5\uc744 \uc0ac\uc6a9\ud558\uc5ec LLaMA-3-8B\ub97c \ubbf8\uc138 \uc870\uc815\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc81c\uc548\ub41c \ubc29\ubc95\uc73c\ub85c \uc0dd\uc131\ub41c \ud3b8\uc9d1\ub41c \ubc84\uc804\uacfc\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \ubcf8 \uc5f0\uad6c\uc758 \uc811\uadfc \ubc29\uc2dd\uc774 \uc9c0\uc2dc \uc870\uc815 \ubc0f \ucf54\ub4dc \ucd94\ub860 \uc791\uc5c5\uc5d0 \uc0ac\uc6a9\ub418\ub294 \ub370\uc774\ud130\ub97c \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \uc9c0\uc2dc \uc870\uc815 \ubc0f \ucf54\ub4dc \ucd94\ub860 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc131\ub2a5 \uc9c0\ud45c\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \ud3b8\uc9d1\ub41c \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \uc131\ub2a5 \ud5a5\uc0c1 \uc815\ub3c4\ub97c \uc218\uce58\uc801\uc73c\ub85c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "4 EXPERIMENTS"}, {"content": "|   | Models | ARC-c | GPQA | GSM8K | MMLU | Average |\n|---|---|---|---|---|---|---|\n| *Code Reasoning* |  |  |  |  |  |  |\n| *OSS-Instruct-75K* | Llama-3-8B | 51.28 | 27.46 | 49.58 | 62.14 | 45.76 |\n|  | \u0394 ToEdit | 51.79 | 28.79 | 49.36 | 62.04 | 46.13 |\n| *Evol-Instruct-110K* | Llama-3-8B | 52.90 | 27.90 | 50.87 | 62.40 | 46.62 |\n|  | \u0394 ToEdit | 52.22 | 29.69 | 50.87 | 62.60 | 46.92 |", "caption": "Table 5: \nResults of different sampling strategies.", "description": "\ud45c 5\ub294 \ub2e4\uc591\ud55c \uc0d8\ud50c\ub9c1 \uc804\ub7b5\uc5d0 \ub530\ub978 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 top-k, top-p, rejection \uc138 \uac00\uc9c0 \uc0d8\ud50c\ub9c1 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5b8\uc5b4 \ubaa8\ub378 \ud559\uc2b5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.  \uac01 \uc0d8\ud50c\ub9c1 \ubc29\ubc95\uc758 \uc7a5\ub2e8\uc810(\uacc4\uc0b0 \ud6a8\uc728\uc131, \uc131\ub2a5)\uc744 \uc81c\uc2dc\ud558\uba70, \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ud1b5\ud574 top-k \uc0d8\ud50c\ub9c1 \ubc29\ubc95\uc758 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5 \uce21\uba74\uc5d0\uc11c\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2.3 \uc81c\uc548\ub41c \uc804\ub7b5"}, {"content": "| Sampling Strategy | PubMedQA | MedMCQA | MedQA (4 options) |\n|---|---|---|---| \n| Top-k | 64.5 | 26.13 | 24.82 |\n| Top-p | 63.8 | 27.11 | 25.61 |\n| Reject Sampling | 64.5 | 28.90 | 28.20 |", "caption": "Table 6: \nAblation study on sampling size k\ud835\udc58kitalic_k for top-k.", "description": "\ubcf8 \ud45c\ub294 top-k \uc0d8\ud50c\ub9c1\uc5d0\uc11c \uc0d8\ud50c \ud06c\uae30 k\uc5d0 \ub300\ud55c \ucd94\uac00 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c k \uac12 (\uc608: k=8, k=64)\uc5d0 \ub300\ud574 \uc138 \uac00\uc9c0 \ud558\uc704 \uc791\uc5c5(PubMedQA, MedMCQA, MedQA(4 options))\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \ucd5c\uc801\uc758 \uc0d8\ud50c \ud06c\uae30\ub97c \uacb0\uc815\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub418\ub294 \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.  \ud45c\uc758 \uacb0\uacfc\ub294 \uacc4\uc0b0 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5 \uac04\uc758 \uade0\ud615\uc744 \uace0\ub824\ud558\uc5ec \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 k\ub97c \uc120\ud0dd\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc9c0\uce68\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "4.4 ABLATION STUDIES"}, {"content": "| Sampling Size (<math>k</math>) | PubMedQA | MedMCQA | MedQA (4 options) |\n|---|---|---|---|\n| <math>k=8</math> | 64.5 | 26.13 | 24.82 |\n| <math>k=64</math> | 63.8 | 28.14 | 27.34 |", "caption": "Table 7: \nPerformance impact of different resampled token condition (p\ud835\udc5dpitalic_p) in Biomedicine domain.", "description": "\ud45c 7\uc740 \uc0dd\uc758\ud559 \ubd84\uc57c\uc5d0\uc11c \uc7ac\ud45c\ubcf8\ud654\ub41c \ud1a0\ud070 \uc870\uac74(p)\uc758 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c p \uac12\uc5d0 \ub530\ub978 \uc5ec\ub7ec \uc124\uc815\uc5d0\uc11c\uc758 \uc131\ub2a5 \ubcc0\ub3d9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  p \uac12\uc774 \ud074\uc218\ub85d \uc7ac\ud45c\ubcf8\ud654\ub418\ub294 \ud1a0\ud070 \uc218\uac00 \uc801\uc5b4\uc9c0\uace0, p \uac12\uc774 \uc791\uc744\uc218\ub85d \uc7ac\ud45c\ubcf8\ud654\ub418\ub294 \ud1a0\ud070 \uc218\uac00 \ub9ce\uc544\uc9d1\ub2c8\ub2e4. \uc131\ub2a5\uacfc \ub370\uc774\ud130 \ubd84\ud3ec \uc720\uc9c0\ub97c \uace0\ub824\ud558\uc5ec \uc784\uacc4\uac12\uc73c\ub85c p=0.99\ub97c \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4.", "section": "2.2 \ud569\uc131 \ub370\uc774\ud130\ub294 \uc65c \uc5b8\uc5b4 \ubaa8\ub378 \uc0ac\uc804 \ud6c8\ub828\uc5d0 \uc2e4\ud328\ud558\ub294\uac00?"}, {"content": "|   | PubMedQA | MQP | RCT | USMLE | ChemProt | Avg |\n|---|---|---|---|---|---|---|\n| $p \\geq 0.99$ | 64.5 | 55.73 | 30.95 | 27.65 | 14.6 | 38.69 |\n| $p \\geq 0.999$ | 63.6 | 55.4 | 29.09 | 28.12 | 16.2 | 38.48 |\n| $p \\leq 0.1$ | 62.4 | 51.47 | 25.6 | 29.14 | 10.0 | 35.72 |\n| $p \\leq 0.01$ | 65.4 | 54.91 | 28.19 | 27.80 | 11.0 | 37.46 |\n| $p \\leq 0.001$ | 64.2 | 56.39 | 35.0 | 27.80 | 12.4 | 39.16 |", "caption": "Table 8: \nToken distribution across different probability ranges in BioMed dataset.", "description": "\ud45c 8\uc740 BioMed \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ud1a0\ud070\uc758 \ud655\ub960 \ubd84\ud3ec\ub97c \ub2e4\uc591\ud55c \ud655\ub960 \ubc94\uc704\ubcc4\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubc94\uc704(\uc608: 0.0-0.1, 0.1-0.2 \ub4f1)\uc5d0 \uc18d\ud558\ub294 \ud1a0\ud070\uc758 \ube44\uc728\uacfc \uac1c\uc218\ub97c \ub098\ud0c0\ub0b4\uc5b4, BioMed \ub370\uc774\ud130\uc14b \ub0b4 \ud1a0\ud070\uc758 \ud655\ub960 \ubd84\ud3ec \ud2b9\uc9d5\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ud569\uc131 \ub370\uc774\ud130\uc758 \uae34 \uaf2c\ub9ac(long tail) \ubd80\uc871 \ubc0f \ud2b9\uc815 \uc601\uc5ed\uc5d0 \ub300\ud55c \uacfc\ub3c4\ud55c \uc9d1\uc911 \ud604\uc0c1\uacfc \uac19\uc740 \ubb38\uc81c\uc810\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "2.2 WHY DOES SYNTHETIC DATA FAIL IN LANGUAGE MODEL PRE-TRAINING?"}, {"content": "| Probability Range | Percentage | Token Count |\n|---|---|---|\n| 0.0-0.1 | 34.7% | 388,626,330 |\n| 0.1-0.2 | 8.1% | 90,716,809 |\n| 0.2-0.3 | 5.4% | 60,477,872 |\n| 0.3-0.4 | 4.4% | 49,278,266 |\n| 0.4-0.5 | 3.8% | 42,558,503 |\n| 0.5-0.6 | 3.6% | 40,318,546 |\n| 0.6-0.7 | 3.7% | 41,438,924 |\n| 0.7-0.8 | 4.0% | 44,798,424 |\n| 0.8-0.9 | 5.2% | 58,238,944 |\n| 0.9-1.0 | 27.1% | 303,543,988 |", "caption": "Table 9: Percentage of tokens requiring edits in the Natural-Instructions dataset. The total number of tokens is 4,671,834. and \u201cGen\u201d is short for \u201cGeneration\u201d.", "description": "\ud45c 9\ub294 Natural Instructions \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ud1a0\ud070 \uc218\uc815\uc774 \ud544\uc694\ud55c \ube44\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ucd1d \ud1a0\ud070 \uc218\ub294 4,671,834\uac1c\uc774\uba70, 'Gen'\uc740 'Generation'(\uc138\ub300)\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ud1a0\ud070 \uc218\uc815\uc774 \ud544\uc694\ud55c \ube44\uc728\uc774 \uc138\ub300\uac00 \uac70\ub4ed\ub420\uc218\ub85d \uac10\uc18c\ud558\ub294 \uacbd\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, \ucd08\uae30 \uc138\ub300(Gen 1)\uc5d0\uc11c\ub294 \uc218\uc815\uc774 \ud544\uc694\ud55c \ud1a0\ud070\uc758 \ube44\uc728\uc774 \ub192\uc9c0\ub9cc, \uc138\ub300\uac00 \uc9c4\ud589\ub420\uc218\ub85d \uadf8 \ube44\uc728\uc774 \uc810\ucc28 \uac10\uc18c\ud569\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc774 \ud559\uc2b5\uc744 \uac70\ub4ed\ud558\uba74\uc11c \ub370\uc774\ud130 \ubd84\ud3ec\uc758 \ubcc0\ud654\uc5d0 \uc801\uc751\ud558\uace0, \uc218\uc815\uc774 \ud544\uc694\ud55c \ud1a0\ud070\uc758 \uc218\uac00 \uc904\uc5b4\ub4e0\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ud558\ub294 \ud1a0\ud070 \ud3b8\uc9d1 \ubc29\ubc95\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc911\uc694\ud55c \uc99d\uac70\uc785\ub2c8\ub2e4.", "section": "3.3 TEST ERROR UNDER DATA EDITING"}, {"content": "| **Tokens (p&gt;0.99)** | **Gen 1 (source)** | **Gen 2** | **Gen 3** |\n|---|---|---|---|\n| 584,103 | 12.5% | 11.76% | 11.08% |", "caption": "Table 10: \nComparison of human and synthetic data performance across downstream tasks in\u00a0(Maini et\u00a0al., 2024), based on training with GPT-2.", "description": "\ud45c 10\uc740 GPT-2 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\uc2dc\ud0a8 \ud6c4, \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5(Maini et al., 2024)\uc5d0\uc11c \uc0ac\ub78c\uc774 \uc791\uc131\ud55c \ub370\uc774\ud130\uc640 \ud569\uc131 \ub370\uc774\ud130\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc0ac\ub78c\uc774 \uc791\uc131\ud55c \ub370\uc774\ud130, 25%, 50%, 75%\uc758 \ud569\uc131 \ub370\uc774\ud130\ub97c \uc11e\uc740 \ub370\uc774\ud130, \uadf8\ub9ac\uace0 \uc21c\uc218 \ud569\uc131 \ub370\uc774\ud130\ub85c \ud6c8\ub828\uc2dc\ud0a8 GPT-2 \ubaa8\ub378\uc758 TruthfulQA, LogiQA, Winogrande, PIQA, ARC-E, BoolQ, OBQA \uc791\uc5c5\uc5d0 \ub300\ud55c \uc131\ub2a5(\uc815\ud655\ub3c4)\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ud569\uc131 \ub370\uc774\ud130\uc758 \ube44\uc728\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ubaa8\ub378 \uc131\ub2a5\uc774 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2. NON-ITERATIVE MODEL COLLAPSE"}, {"content": "|                   | TruthfulQA | LogiQA | Wino. | PIQA | ARC-E | BoolQ | OBQA | Avg |\n| :----------------- | :----------: | :-----: | :----: | :---: | :----: | :----: | :---: | :--: |\n| Human Data         |    32.68     |  23.03  |  51.3  | 64.42 |  44.4  | 60.98 |  15   | 41.69 |\n| 25% Synthetic Data |    27.91     |  21.37  | 50.12  | 63.93 | 43.94 | 62.29 | 15.4  | 40.71 |\n| 50% Synthetic Data |    30.84     |  22.58  | 52.41  | 63.33 | 44.02 | 62.14 |  16   | 41.62 |\n| 75% Synthetic Data |    29.5      |  22.65  |  49.8  | 63.44 | 44.53 | 61.56 | 17.2  | 41.24 |\n| Synthetic Data     |    28.89     |  22.58  | 49.72  |  63   |  46.3  | 54.53 | 16.8  | 40.26 |", "caption": "Table 11: \nComparison of human and synthetic data performance across downstream tasks in\u00a0(Maini et\u00a0al., 2024), based on training with OLMo-237M. \u00b1 indicates the standard error.", "description": "\ud45c 11\uc740 OLMo-237M \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ud588\uc744 \ub54c, \uc778\uac04\uc774 \uc791\uc131\ud55c \ub370\uc774\ud130\uc640 \ud569\uc131 \ub370\uc774\ud130\uc758 \uc131\ub2a5\uc744 \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc5d0\uc11c \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5(TruthfulQA, LogiQA, Wino, PIQA, ARC-E, OBQA, Avg)\uc5d0 \ub300\ud55c \uac01 \ub370\uc774\ud130 \uc720\ud615\uc758 \uc131\ub2a5\uacfc \ud45c\uc900 \uc624\ucc28\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ud569\uc131 \ub370\uc774\ud130 \uc0ac\uc6a9\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  Maini et al.(2024)\uc758 \uc5f0\uad6c \uacb0\uacfc\ub97c \ubc14\ud0d5\uc73c\ub85c \ud558\uc600\uc2b5\ub2c8\ub2e4.", "section": "2.1 \ub370\uc774\ud130 \ud63c\ud569\uc5d0 \ub300\ud55c \uc0ac\uc804 \ud6c8\ub828"}, {"content": "|               | TruthfulQA | LogiQA | Wino. | PIQA  | ARC-E | OBQA  | Avg   |\n| :------------ | :----------: | :-----: | :----: | :----: | :----: | :----: | :----: |\n| Human Data    | 26.81 \u00b1 1.550 | 21.06 \u00b1 1.028 | 52.01 \u00b1 1.404 | 56.69 \u00b1 1.156 | 31.73 \u00b1 0.9550 | 13.80 \u00b1 1.543 | 33.68 |\n| 25% Synthetic Data | 26.44 \u00b1 1.543 | 21.25 \u00b1 1.032 | 52.64 \u00b1 1.403 | 57.02 \u00b1 1.155 | 31.78 \u00b1 0.9552 | 12.40 \u00b1 1.475 | 33.59 |\n| 50% Synthetic Data | 25.95 \u00b1 1.534 | 20.04 \u00b1 1.099 | 52.25 \u00b1 1.408 | 56.64 \u00b1 1.126 | 31.82 \u00b1 0.9557 | 12.80 \u00b1 1.495 | 33.25 |\n| 75% Synthetic Data | 25.34 \u00b1 1.522 | 20.87 \u00b1 1.025 | 50.43 \u00b1 1.405 | 55.60 \u00b1 1.159 | 32.74 \u00b1 0.9629 | 12.00 \u00b1 1.454 | 32.83 |\n| Synthetic Data | 23.01 \u00b1 1.473 | 20.29 \u00b1 1.014 | 49.33 \u00b1 1.405 | 55.93 \u00b1 1.158 | 33.33 \u00b1 0.9673 | 14.20 \u00b1 1.562 | 32.68 |", "caption": "Table 12: PPL results of GPT-2 124M pretraining on mixture of human and synthetic data.", "description": "\ubcf8 \ud45c\ub294 GPT-2 124M \ubaa8\ub378\uc744 \uc778\uac04\uc774 \uc791\uc131\ud55c \ub370\uc774\ud130\uc640 \ud569\uc131 \ub370\uc774\ud130\uc758 \ud63c\ud569\ubb3c\ub85c \uc0ac\uc804 \ud6c8\ub828\ud588\uc744 \ub54c\uc758 \ud37c\ud50c\ub809\uc11c\ud2f0(PPL) \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ube44\uc728\uc758 \ud569\uc131 \ub370\uc774\ud130(0%, 25%, 50%, 75%, 100%)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ucf30\uace0, \ud6c8\ub828 \ub370\uc774\ud130 \ud06c\uae30(\ud1a0\ud070 \uc218)\uc640 \uc5d0\ud3ec\ud06c \uc218\uc5d0 \ub530\ub978 PPL \uac12\uc744 \uc5ec\ub7ec \uac1c\uc758 \uac80\uc99d \uc138\ud2b8(Wikitext-103, RedPajama, Falcon-RefinedWeb, c4-en, mc4-en)\uc5d0 \ub300\ud574 \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ud569\uc131 \ub370\uc774\ud130 \ube44\uc728\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2.1 \ub370\uc774\ud130 \ud63c\ud569\uc5d0 \ub300\ud55c \uc0ac\uc804 \ud6c8\ub828"}, {"content": "| Synthetic Data Ratio | 25% | 25% | 25% | 25% | 25% | 50% | 50% | 50% | 50% | 50% | 75% | 75% | 75% | 75% | 75% |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Tokens Size | 8.4B | 16.8B | 25.2B | 33.6B | 42B | 8.4B | 16.8B | 25.2B | 33.6B | 42B | 8.4B | 16.8B | 25.2B | 33.6B | 42B |\n| Epochs | 1 | 2 | 3 | 4 | 5 | 1 | 2 | 3 | 4 | 5 | 1 | 2 | 3 | 4 | 5 |\n| Wikitext-103 | 45.97 | 39.87 | 37.65 | 36.91 | 36.32 | 50.29 | 43.15 | 40.46 | 39.43 | 38.65 | 58.66 | 48.75 | 45.20 | 43.42 | 42.95 |\n| RedPajama | 42.28 | 37.62 | 35.72 | 34.66 | 34.24 | 46.89 | 41.42 | 39.37 | 38.21 | 37.72 | 55.72 | 49.26 | 46.27 | 44.81 | 44.30 |\n| Falcon-RefinedWeb | 56.40 | 50.62 | 48.26 | 47.13 | 46.66 | 61.06 | 54.34 | 51.72 | 50.39 | 49.87 | 69.32 | 61.50 | 58.28 | 56.77 | 56.19 |\n| c4-en | 48.15 | 43.14 | 40.98 | 39.91 | 39.41 | 51.79 | 46.06 | 43.90 | 42.73 | 42.23 | 58.60 | 52.22 | 49.26 | 47.87 | 47.27 |\n| mc4-en | 62.46 | 56.80 | 54.35 | 53.06 | 52.71 | 70.43 | 62.48 | 59.61 | 57.66 | 57.07 | 80.37 | 71.77 | 67.90 | 65.31 | 64.82 |", "caption": "Table 13: PPL results of OLMo-237M pretraining on mixture of human and synthetic data.", "description": "\ud45c 13\uc740 \uc778\uac04\uc774 \uc791\uc131\ud55c \ub370\uc774\ud130\uc640 \ud569\uc131 \ub370\uc774\ud130\ub97c \uc11e\uc5b4 OLMo-237M \uc5b8\uc5b4 \ubaa8\ub378\uc744 \uc0ac\uc804 \ud6c8\ub828\uc2dc\ud0a8 \uacb0\uacfc\uc785\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ube44\uc728\uc758 \ud569\uc131 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 PPL(Perplexity) \uc9c0\ud45c\ub97c \ud1b5\ud574 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud569\uc131 \ub370\uc774\ud130\uc758 \ube44\uc728\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c PPL \uac12\uc774 \ubcc0\ud654\ud558\ub294 \uc591\uc0c1\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \uac80\uc99d \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c PPL \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \ud569\uc131 \ub370\uc774\ud130\uc758 \uc0ac\uc6a9\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \uc5b4\ub5a4 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \uc790\uc138\ud788 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "2. NON-ITERATIVE MODEL COLLAPSE"}, {"content": "| Synthetic Data Ratio | 0% | 25% | 50% | 75% | 100% | DSIR (1M) | DSIR (10M) | Edu Classifier (1M) | Edu Classifier (10M) | PPL Filter (1M) | PPL Filter (10M) | Density Sampling (1M) | Density Sampling (10M) |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| Unique Tokens | 8.4B | 8.4B | 8.4B | 8.4B | 8.4B | 0.6B | 8.4B | 0.75B | 7.4B | 0.97B | 9B | 0.6B | 7.1B |\n| Training Tokens | 8.4B | 8.4B | 8.4B | 8.4B | 8.4B | 8.4B | 8.4B | 10.5B | 7.4B | 13.68B | 9B | 8.9B | 7.1B |\n| Epochs | 1 | 1 | 1 | 1 | 1 | 14 | 1 | 14 | 1 | 14 | 1 | 14 | 1 |\n| Wikitext-103 | 187.36 | 185.5 | 260.08 | 367.46 | 1605.73 | 1309.53 | 1757.03 | 1111.29 | 1612.95 | 738.36 | 1193.25 | 1188.40 | 1753.89 |\n| RedPajama | 175.38 | 183.93 | 236.33 | 301.09 | 907.91 | 649.36 | 916.51 | 811.14 | 1104.75 | 376.36 | 645.82 | 789.67 | 896.18 |\n| Falcon-RefinedWeb | 165.17 | 166.69 | 199.68 | 245.15 | 523.93 | 573.61 | 510.96 | 522.97 | 612.72 | 344.82 | 449.86 | 501.99 | 560.92 |\n| c4-en | 123.88 | 127.68 | 147.69 | 174.48 | 410.19 | 457.96 | 404.63 | 415.88 | 487.97 | 286.95 | 367.44 | 414.55 | 457.71 |\n| mc4-en | 208.91 | 208.94 | 263.35 | 324.91 | 800.40 | 861.01 | 823.12 | 769.86 | 955.70 | 476.81 | 662.00 | 740.75 | 844.53 |\n| M2D2-Wiki | 88.24 | 87.34 | 107.77 | 114.19 | 189.06 | 234.45 | 183.17 | 161.58 | 206.45 | 130.43 | 162.08 | 167.20 | 205.50 |\n| M2D2-S2ORC | 86.15 | 81.53 | 97.61 | 100.64 | 204.22 | 170.78 | 496.40 | 145.27 | 201.52 | 117.44 | 163.38 | 131.22 | 192.97 |", "caption": "Table 14: Comparison of different synthetic data methods.", "description": "\ud45c 14\ub294 \uc11c\ub85c \ub2e4\ub978 \uc138 \uac00\uc9c0 \uc778\uacf5 \ub370\uc774\ud130 \uc0dd\uc131 \ubc29\ubc95, \uc989 \uc21c\uc218 \ud569\uc131 \ub370\uc774\ud130(Cosmopedia), \uc900\ud569\uc131 \ub370\uc774\ud130(Rephrasing the Web), \uadf8\ub9ac\uace0 \ud1a0\ud070 \ud3b8\uc9d1 \uae30\ubc18 \uc900\ud569\uc131 \ub370\uc774\ud130(ToEdit)\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4. \uac01 \ubc29\ubc95\uc740 \ub370\uc774\ud130 \uc0dd\uc131 \ubc29\uc2dd, \ub370\uc774\ud130 \uc720\ud615, \uadf8\ub9ac\uace0 \uc2e4\ud5d8 \uacb0\uacfc(\ubaa8\ub378 \ubd95\uad34 \ubc1c\uc0dd \uc5ec\ubd80 \ub610\ub294 \uc131\ub2a5 \ud5a5\uc0c1 \uc5ec\ubd80)\ub97c \uc81c\uc2dc\ud558\uc5ec \uc11c\ub85c \ub2e4\ub978 \uc778\uacf5 \ub370\uc774\ud130 \uc0dd\uc131 \ubc29\ubc95\uc758 \ud2b9\uc9d5\uacfc \ud6a8\uacfc\ub97c \ube44\uad50\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2.2 \ud569\uc131 \ub370\uc774\ud130\ub294 \uc65c \uc5b8\uc5b4 \ubaa8\ub378 \uc0ac\uc804 \ud6c8\ub828\uc5d0 \uc2e4\ud328\ud558\ub294\uac00?"}, {"content": "| Method | Data Type | Approach | Result |\n|---|---|---|---| \n| Cosmopedia (Ben Allal et al., 2024) | Pure synthetic | Using a prompt to induce data from LLMs. | Reveal non-iterative model collapse. |\n| Rephrasing the Web (Maini et al., 2024) | Semi-synthetic | Using a prompt and source content to guide LLMs to reformat source content. | Improve training performance. |\n| ToEdit (Ours) | Semi-synthetic | Using the distribution of source content estimated by LLMs (single forward pass) to replace tokens. | Improve training performance. |", "caption": "Table 15: PPL results of GPT-2 124M pretraining on pure Human or Synthetic data.", "description": "\ud45c 15\ub294 \uc21c\uc218 \uc778\uac04 \ub370\uc774\ud130 \ub610\ub294 \ud569\uc131 \ub370\uc774\ud130\ub85c GPT-2 124M\uc744 \uc0ac\uc804 \ud6c8\ub828\uc2dc\ud0a8 \uacb0\uacfc\uc758 \ud37c\ud50c\ub809\uc11c\ud2f0(PPL)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud1a0\ud070 \ud06c\uae30(8.4B, 16.8B, 25.2B, 33.6B, 42B)\uc640 \uc5d0\ud3ed(1, 2, 3, 4, 5)\uc5d0 \ub530\ub978 \ud37c\ud50c\ub809\uc11c\ud2f0 \uac12\uc744 Wikitext-103, RedPajama, Falcon-RefinedWeb, c4-en, mc4-en \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uc778\uac04 \ub370\uc774\ud130\uc640 \ud569\uc131 \ub370\uc774\ud130\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ud569\uc131 \ub370\uc774\ud130\ub9cc\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc790\uc138\ud788 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "2. NON-ITERATIVE MODEL COLLAPSE"}, {"content": "| Data Type | Human Data (Dolma) |  |  |  |  | Synthetic Data (Cosmopedia) |  |  |  |  | \n|---|---|---|---|---|---|---|---|---|---|---|\n| Tokens Size | 8.4B | 16.8B | 25.2B | 33.6B | 42B | 8.4B | 16.8B | 25.2B | 33.6B | 42B |\n| Epochs | 1 | 2 | 3 | 4 | 5 | 1 | 2 | 3 | 4 | 5 |\n| Wikitext-103 | 43.62 | 38.57 | 36.11 | 34.89 | 34.55 | 169.38 | 147.73 | 135.23 | 131.78 | 128.05 |\n| RedPajama | 40.18 | 35.84 | 33.97 | 32.74 | 32.34 | 116.37 | 103.25 | 99.27 | 96.81 | 96.03 |\n| Falcon-RefinedWeb | 54.85 | 49.10 | 46.93 | 45.43 | 44.90 | 146.97 | 132.60 | 127.68 | 124.32 | 122.69 |\n| c4-en | 45.87 | 41.00 | 39.10 | 37.95 | 37.56 | 128.25 | 114.41 | 109.73 | 107.53 | 106.55 |\n| mc4-en | 61.00 | 54.44 | 52.11 | 50.38 | 49.74 | 171.44 | 153.70 | 150.28 | 145.44 | 144.99 |", "caption": "Table 16: Dolma dataset statistics (v1.6), quoted from source\u00a0(Soldaini et\u00a0al., 2024).", "description": "\ud45c 16\uc740 Soldaini \uc678\uc758 2024\ub144 \ub17c\ubb38\uc5d0\uc11c \uc778\uc6a9\ud55c Dolma \ub370\uc774\ud130\uc14b(v1.6)\uc758 \ud1b5\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  UTF-8 \ubc14\uc774\ud2b8(GB), \ubb38\uc11c \uc218(\ubc31\ub9cc), \uc720\ub2c8\ucf54\ub4dc \ub2e8\uc5b4 \uc218(\uc2ed\uc5b5), Llama \ud1a0\ud070 \uc218(\uc2ed\uc5b5)\uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uce21\uba74\uc5d0\uc11c \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\uc640 \uad6c\uc131\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uac01 \uc22b\uc790\ub294 \ub370\uc774\ud130\uc14b\uc5d0 \ud3ec\ud568\ub41c Common Crawl \uc6f9 \ud398\uc774\uc9c0, Stack Overflow \ucf54\ub4dc, Wikipedia, Wikibooks, Reddit \uc18c\uc15c \ubbf8\ub514\uc5b4, PubMed \uae30\uc0ac, \uadf8\ub9ac\uace0 Project Gutenberg \ub3c4\uc11c\uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uc18c\uc2a4\uc758 \ub370\uc774\ud130 \uc591\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 Dolma \ub370\uc774\ud130\uc14b\uc758 \uaddc\ubaa8\uc640 \ub2e4\uc591\ud55c \uc18c\uc2a4\ub85c\ubd80\ud130\uc758 \ub370\uc774\ud130 \ubd84\ud3ec\ub97c \uc774\ud574\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4.2 \ub370\uc774\ud130\uc14b \ubc0f \ubaa8\ub378"}]