{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report on GPT-4, a highly influential large language model, which is directly relevant to the topic of the current paper on synthesizing text data without model collapse."}, {"fullname_first_author": "Ilia Shumailov", "paper_title": "AI models collapse when trained on recursively generated data", "publication_date": "2024-00-00", "reason": "This paper is highly relevant as it formally introduces the concept of model collapse, a central concern of the current paper."}, {"fullname_first_author": "Elvis Dohmatob", "paper_title": "Model collapse demystified: The case of regression", "publication_date": "2024-02-07", "reason": "This paper provides theoretical foundations for understanding model collapse, which is crucial for addressing the problem of synthetic data in training language models."}, {"fullname_first_author": "Matthias Gerstgrasser", "paper_title": "Is model collapse inevitable? Breaking the curse of recursion by accumulating real and synthetic data", "publication_date": "2024-04-01", "reason": "This paper explores methods to mitigate model collapse by combining real and synthetic data, offering a potential solution that is relevant to the problem explored in the current paper."}, {"fullname_first_author": "Luca Soldaini", "paper_title": "Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research", "publication_date": "2024-02-00", "reason": "This paper introduces a large, high-quality dataset (Dolma) used in the experiments of the current paper, making it a crucial resource for the empirical validation of the proposed method."}]}