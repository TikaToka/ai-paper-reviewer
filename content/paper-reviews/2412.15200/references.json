{"references": [{"fullname_first_author": "Antonio Alliegro", "paper_title": "Polydiff: Generating 3d polygonal meshes with diffusion models", "publication_date": "2023-12-00", "reason": "This paper is highly relevant due to its focus on generating 3D polygonal meshes using diffusion models, a technique directly related to the core methodology of the current paper."}, {"fullname_first_author": "Armen Avetisyan", "paper_title": "Scene-script: Reconstructing scenes with an autoregressive structured language model", "publication_date": "2024-03-00", "reason": "This work is significant for its exploration of autoregressive structured language models in scene reconstruction, providing a valuable comparative approach to the current paper's methodology."}, {"fullname_first_author": "Tim Brooks", "paper_title": "Video generation models as world simulators", "publication_date": "2024-00-00", "reason": "This is a key reference because of its exploration of video generation models as world simulators, which offers a relevant parallel to the current paper's approach to high-quality 3D asset creation."}, {"fullname_first_author": "Junsong Chen", "paper_title": "Pixart-a: Fast training of diffusion transformer for photorealistic text-to-image synthesis", "publication_date": "2023-10-00", "reason": "This paper is important due to its focus on fast training of diffusion transformers for photorealistic text-to-image synthesis, a technique relevant to the efficient training methods explored in the current paper."}, {"fullname_first_author": "Yen-Chi Cheng", "paper_title": "Sdfusion: Multimodal 3d shape completion, reconstruction, and generation", "publication_date": "2023-00-00", "reason": "This reference is crucial due to its exploration of multimodal 3D shape completion, reconstruction, and generation, which provides a comparative context for the current paper's image-to-3D generation approach."}]}