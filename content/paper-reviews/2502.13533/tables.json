[{"content": "| Method | OpenHermes MathQA | OpenHermes GSM8K | OpenOrca MathQA | OpenOrca GSM8K | Redu. Ratio |\n|---|---|---|---|---|---| \n| 13B w/o FT | 32.60 | 24.26 | 32.93 | 23.35 | 1.00\u00d7 |\n| 7B LoRA | 29.61 | 22.82 | 30.95 | 13.87 | 1.93\u00d7 |\n| 13B LoRAM-Rand | 33.77 | 27.22 | 32.83 | 25.93 | 2.17\u00d7 |\n| 13B LoRAM-Stru | 33.80 | 24.64 | 33.07 | 24.49 | 2.17\u00d7 |\n| 13B LoRAM-Semi | 31.76 | 36.92 | 33.07 | 27.29 | 1.95\u00d7 |\n| 13B LoRAM-Unst | 30.12 | 31.92 | 32.70 | 26.61 | 2.16\u00d7 |\n| 70B w/o FT | 39.53 | 52.01 | 39.53 | 52.01 | 1.00\u00d7 |\n| 13B LoRA | 32.03 | 36.69 | 33.63 | 25.70 | 5.30\u00d7 |\n| 70B QLoRAM-Rand | 39.66 | 57.62 | 39.40 | 55.72 | 12.84\u00d7 |\n| 70B QLoRAM-Stru | 39.77 | 57.16 | 39.73 | 54.44 | 12.84\u00d7 |", "caption": "Table 1: Accuracy (%) of the MathQA (1-shot) & GSM8K (8-shots) in the mathematical domain under LLaMA-2.\n\u25b2 indicates the theoretical parameters reduction of non-structured pruning. However, these parameters are filled with zeros in actual training, so the memory footprint is not reduced.", "description": "\ubcf8 \ud45c\ub294 LLaMA-2 \uc5b8\uc5b4 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc218\ud589\ub41c \uc218\ud559\uc801 \ucd94\ub860 \uc791\uc5c5\uc5d0 \ub300\ud55c MathQA (1-shot) \ubc0f GSM8K (8-shots) \ub370\uc774\ud130\uc14b\uc758 \uc815\ud655\ub3c4(%)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \u25b2 \uae30\ud638\ub294 \ube44\uad6c\uc870\uc801 \uac00\uc9c0\uce58\uae30(pruning)\uc758 \uc774\ub860\uc801 \ub9e4\uac1c\ubcc0\uc218 \uac10\uc18c\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc2e4\uc81c \ud6c8\ub828 \uacfc\uc815\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \ub9e4\uac1c\ubcc0\uc218\ub4e4\uc774 0\uc73c\ub85c \ucc44\uc6cc\uc9c0\uae30 \ub54c\ubb38\uc5d0 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc740 \uc904\uc5b4\ub4e4\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.  \uc989, \uc774 \ud45c\ub294 \uac00\uc9c0\uce58\uae30 \uc804\ub7b5\uc5d0 \ub530\ub978 \ubaa8\ub378 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc694\uc57d\ud55c \uac83\uc73c\ub85c, \ud2b9\ud788 \uba54\ubaa8\ub9ac \ud6a8\uc728\uacfc\uc758 \uad00\ub828\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "3.3 \ud558\uc704 \uc791\uc5c5 \uc131\ub2a5"}, {"content": "| Method | OpenHermes | OpenOrca | Parameter\n|---|---|---|---|\n| Mean \u00b1 Std | Mean \u00b1 Std |  Redu. Ratio |\n| 13B w/o FT | 64.28 \u00b1 1.30 | 64.28 \u00b1 1.30 | 1.00 \u00d7 |\n| 7B LoRA | 61.51 \u00b1 1.29 | 61.42 \u00b1 1.30 | 1.93 \u00d7 |\n| 13B LoRAM-Rand | 64.64 \u00b1 1.29 | 64.49 \u00b1 1.30 | 2.17 \u00d7 |\n| 13B LoRAM-Stru | 64.42 \u00b1 1.29 | 64.32 \u00b1 1.29 | 2.17 \u00d7 |\n| 13B LoRAM-Semi | 64.38 \u00b1 1.29 | 64.73 \u00b1 1.30 | 1.95 \u00d7 |\n| 13B LoRAM-Unst | 64.12 \u00b1 1.29 | 64.68 \u00b1 1.29 | 2.16 \u00d7 |\n| 70B w/o FT | 68.69 \u00b1 1.27 | 68.69 \u00b1 1.27 | 1.00 \u00d7 |\n| 13B LoRA | 65.05 \u00b1 1.29 | 65.40 \u00b1 1.29 | 5.30 \u00d7 |\n| 70B QLoRAM-Rand | 68.99 \u00b1 1.27 | 68.46 \u00b1 1.27 | 12.84 \u00d7 |\n| 70B QLoRAM-Stru | 69.10 \u00b1 1.27 | 68.94 \u00b1 1.27 | 12.84 \u00d7 |", "caption": "Table 2: Average accuracy (%) of the CSR in the common sense reasoning domain (1-shot) under the LLaMA-2. Baseline results for each subtask of CSR are detailed in\u00a0Appendix\u00a0E.", "description": "\ubcf8 \ud45c\ub294 LLaMA-2 \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c \ud55c \uacf5\ud1b5 \uc0c1\uc2dd \ucd94\ub860(CSR) \uc791\uc5c5\uc5d0\uc11c 1-shot \uc124\uc815 \ud558\uc5d0 \uac01 \ud558\uc704 \uc791\uc5c5\ubcc4 \ud3c9\uade0 \uc815\ud655\ub3c4(%)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c LoRAM \ubcc0\ud615 \ubc0f \uae30\uc900 \ubaa8\ub378(\uc6d0\ubcf8 LLaMA-2, LoRA \ubbf8\uc138 \uc870\uc815\ub41c \uc18c\ud615 \ubaa8\ub378)\uc758 \uc131\ub2a5\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \ubaa8\ub378\uc758 \ud30c\ub77c\ubbf8\ud130 \uac10\uc18c \ube44\uc728\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ud558\uc704 \uc791\uc5c5\uc758 \uae30\uc900 \uc131\ub2a5 \uacb0\uacfc\ub294 \ubd80\ub85d E\uc5d0 \uc790\uc138\ud788 \uc124\uba85\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 LoRAM\uc774 \uba54\ubaa8\ub9ac \ud6a8\uc728\uc131\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uacf5\ud1b5 \uc0c1\uc2dd \ucd94\ub860 \uc791\uc5c5\uc5d0\uc11c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud558\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.3 \ud558\uc704 \uc791\uc5c5 \uc131\ub2a5"}, {"content": "| Method | OpenHermes Pass@1 | OpenHermes Pass@10 | OpenOrca Pass@1 | OpenOrca Pass@10 | Parameter\n---|---|---|---|---|---| \n| 13B w/o FT | 17.68 | 35.37 | 17.68 | 35.37 | 1.00\u00d7 | \n| 7B LoRA | 15.24 | 28.04 | 15.85 | 26.21 | 1.93\u00d7 | \n| 13B LoRAM-Rand | 19.51 | 33.54 | 19.51 | 32.32 | 2.17\u00d7 | \n| 13B LoRAM-Stru | 17.68 | 35.37 | 17.07 | 31.71 | 2.17\u00d7 | \n| 13B LoRAM-Semi | 20.12 | 35.37 | 18.29 | 39.63 | 1.95\u00d7 | \n| 13B LoRAM-Unst | 22.56 | 34.15 | 18.29 | 37.20 | 2.16\u00d7 | \n| 70B w/o FT | 31.71 | 58.54 | 31.71 | 58.54 | 1.00\u00d7 | \n| 13B LoRA | 18.29 | 35.98 | 18.29 | 39.02 | 5.30\u00d7 | \n| 70B QLoRAM-Rand | 29.27 | 57.32 | 31.71 | 56.71 | 12.84\u00d7 | \n| 70B QLoRAM-Stru | 32.32 | 58.54 | 32.32 | 59.15 | 12.84\u00d7 |", "caption": "Table 3: Pass@1(%) and Pass@10(%) of HumanEval in the code generation domain under LLaMA-2. The best results for all baselines are reported, selected from temperature settings in {0.0, 0.2, 0.4, 0.6, 0.8} with toppsubscripttopp\\textsc{top}_{\\textsc{p}}top start_POSTSUBSCRIPT p end_POSTSUBSCRIPT fixed at 0.95.", "description": "\ubcf8 \ud45c\ub294 LLaMA-2 \uc5b8\uc5b4 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucf54\ub4dc \uc0dd\uc131 \uc791\uc5c5\uc5d0 \ub300\ud55c HumanEval \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Pass@1\uacfc Pass@10 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uba70, temperature\ub294 0.0\uc5d0\uc11c 0.8\uae4c\uc9c0 \ub2e4\uc591\ud558\uac8c \uc124\uc815\ud558\uace0 topp\ub294 0.95\ub85c \uace0\uc815\ud558\uc5ec \uac00\uc7a5 \uc88b\uc740 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uae30\uc900 \ubaa8\ub378 (\uc608: \ud6c8\ub828\ub418\uc9c0 \uc54a\uc740 \ubaa8\ub378, LoRA\ub85c \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378)\uacfc \uc5ec\ub7ec \uac00\uc9c0 LORAM \ubcc0\ud615 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec LORAM\uc758 \ud6a8\uc728\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.3 \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5 \uc131\ub2a5"}, {"content": "| Method | #Orig. Params | Pruning Ratio | #Pruned Params | Reduction | HBM |\n|---|---|---|---|---|---| \n| LoRAM-Semi | 13015864320 | 0.50 | 6738415616 | 1.93x | 12.55 |\n| LoRAM-Unst | 13015864320 | 0.55 | 6037628912 | 2.16x | 11.25 |\n| LoRAM-Rand & Stru | 13015864320 | 0.65 | 6005662720 | 2.17x | 11.19 |", "caption": "Table 4: LoRAM configures on LLaMA-2-13B. Comparison of different pruning methods in terms of parameter reduction ratio (Reduction) and HBM footprint (GB) of pruned parameters (HBM), ignoring low-rank matrix overhead.", "description": "\ud45c 4\ub294 LLaMA-2-13B \ubaa8\ub378\uc5d0 \ub300\ud55c LoRAM \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uac00\uc9c0\uce58\uae30 \ubc29\ubc95\uc758 \ub9e4\uac1c\ubcc0\uc218 \uac10\uc18c\uc728 (Reduction)\uacfc \uac00\uc9c0\uce58\uae30\ub41c \ub9e4\uac1c\ubcc0\uc218\uc758 HBM \uacf5\uac04 \uc0ac\uc6a9\ub7c9 (GB)\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. \uc800\ucc28\uc6d0 \ud589\ub82c \uc624\ubc84\ud5e4\ub4dc\ub294 \uace0\ub824\ud558\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.  \uc989, \uc774 \ud45c\ub294 \uc5ec\ub7ec \uac00\uc9c0\uce58\uae30 \uae30\ubc95(LORAM-Semi, LORAM-Unst, LORAM-Rand & Stru)\uc744 \uc0ac\uc6a9\ud558\uc5ec LLaMA-2-13B \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc5bc\ub9c8\ub098 \ud6a8\uc728\uc801\uc73c\ub85c \uc904\uc77c \uc218 \uc788\ub294\uc9c0, \uadf8\ub9ac\uace0 \uadf8\uc5d0 \ub530\ub978 HBM \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2 MEMORY-EFFICIENT LORA TRAINING \u2013 LORAM"}, {"content": "| Method | #Orig. Params | Pruning Ratio | #Pruned Params | Reduction | HBM |\n|---|---|---|---|---|---| \n| LoRAM-Rand & Stru | 68976648192 | 0.65 | 28099436544 | 2.45x | 52.34 |\n| LoRAM-Rand & Stru | 68976648192 | 0.75 | 21488738304 | 3.21x | 40.03 |\n| LoRAM-Rand & Stru | 68976648192 | 0.85 | 16272924672 | 4.24x | 30.31 |\n| LoRAM-Rand & Stru | 68976648192 | 0.95 | 9662226432 | 7.14x | 18.00 |\n| LoRAM-Rand & Stru | 70553706496 | 0.85 | 17849982976 | 3.95x | 33.25 |", "caption": "Table 5: LoRAM configures on LLaMA-2-70B and LLaMA-3.1-70B with different pruning ratios.", "description": "\ud45c 5\ub294 LLaMA-2-70B\uc640 LLaMA-3.1-70B \ubaa8\ub378\uc5d0 \ub300\ud574 \ub2e4\uc591\ud55c \uac00\uc9c0\uce58\uae30 \ube44\uc728\uc744 \uc801\uc6a9\ud55c LoRAM \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ud589\uc740 \ub2e4\ub978 \uac00\uc9c0\uce58\uae30 \ube44\uc728(Pruning Ratio)\uc744 \uc0ac\uc6a9\ud558\ub294 LoRAM \uc124\uc815\uc744 \ub098\ud0c0\ub0b4\uba70, \uc6d0\ub798 \ub9e4\uac1c\ubcc0\uc218 \uc218(Orig. Params), \uac00\uc9c0\uce58\uae30\ub41c \ub9e4\uac1c\ubcc0\uc218 \uc218(Pruned Params), \ub9e4\uac1c\ubcc0\uc218 \uac10\uc18c \ube44\uc728(Reduction), \uadf8\ub9ac\uace0 \uac00\uc9c0\uce58\uae30\ub41c \ub9e4\uac1c\ubcc0\uc218\ub97c \uc800\uc7a5\ud558\ub294 \ub370 \ud544\uc694\ud55c HBM(High Bandwidth Memory) \uc6a9\ub7c9(GB)\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uac00\uc9c0\uce58\uae30 \uc804\ub7b5\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c LoRAM\uc758 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc5d0 \ub300\ud55c \ud1b5\ucc30\ub825\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3.2 Fine-tuning Convergence"}, {"content": "| Method | #Orig. Params | Pruning Ratio | #Pruned Params | Reduction | HBM |\n|---|---|---|---|---|---| \n| QLoRAM-Rand & Stru | 68976648192 | 0.65 | 7024859136 | 9.82x | 13.08 |\n| QLoRAM-Rand & Stru | 68976648192 | 0.75 | 5372184576 | 12.84x | 10.01 |\n| QLoRAM-Rand & Stru | 68976648192 | 0.85 | 4068231168 | 16.95x | 7.58 |\n| QLoRAM-Rand & Stru | 68976648192 | 0.95 | 2415556608 | 28.56x | 4.50 |\n| QLoRAM-Rand & Stru | 70553706496 | 0.85 | 4462495744 | 15.81x | 8.31 |", "caption": "Table 6: QLoRAM configures on LLaMA-2-70B and LLaMA-3.1-70B with , demonstrating more aggressive parameter compression.", "description": "\ud45c 6\uc740 QLoRAM(QLORA\uc640 LORAM\uc744 \uacb0\ud569\ud55c \ubc29\ubc95)\uc744 LLaMA-2-70B\uc640 LLaMA-3.1-70B \ubaa8\ub378\uc5d0 \uc801\uc6a9\ud588\uc744 \ub54c\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uac00\uc9c0\uce58\uae30 \ube44\uc728(Pruning Ratio)\uc5d0\uc11c \ub9e4\uac1c\ubcc0\uc218 \uac10\uc18c\uc728(Reduction)\uacfc \uc0ac\uc6a9\ub41c HBM(High Bandwidth Memory) \uc6a9\ub7c9\uc744 \ubcf4\uc5ec\uc8fc\uc5b4,  \ub9e4\uac1c\ubcc0\uc218\ub97c \ub354\uc6b1 \uacf5\uaca9\uc801\uc73c\ub85c \uc555\ucd95\ud588\uc744 \ub54c\uc758 \ud6a8\uacfc\ub97c \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uc6d0\ubcf8 \ubaa8\ub378\uc758 \ud06c\uae30 \ub300\ube44 \ub9e4\uac1c\ubcc0\uc218 \ud06c\uae30 \uac10\uc18c\uc728\uc774 9.82\ubc30\uc5d0\uc11c \ucd5c\ub300 28.56\ubc30\uae4c\uc9c0 \ub2ec\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub294 QLoRA\uc758 \uc591\uc790\ud654 \uae30\ubc95\uacfc LORAM\uc758 \uac00\uc9c0\uce58\uae30 \uae30\ubc95\uc744 \ubcd1\ud589\ud558\uc5ec \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \ud06c\uac8c \uc904\uc774\ub294 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\ub294 \uac01 \uac00\uc9c0\uce58\uae30 \ube44\uc728\uc5d0 \ub530\ub978 \ub9e4\uac1c\ubcc0\uc218 \uac1c\uc218\uc640 HBM \uc0ac\uc6a9\ub7c9\uc744 \ubcf4\uc5ec\uc8fc\uc5b4,  \uba54\ubaa8\ub9ac \ud6a8\uc728\uc801\uc778 \ubaa8\ub378 \ud559\uc2b5\uc744 \uc704\ud55c \ucd5c\uc801\uc758 \uac00\uc9c0\uce58\uae30 \ube44\uc728\uc744 \uacb0\uc815\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3.2 Fine-tuning Convergence"}, {"content": "| LLaMA-3.1 | GSM8K | Parameter Reduction Ratio |\n|---|---|---|\n| 8B w/o Fine-Tuning | 55.27 | 8.79\u00d7 |\n| 8B LoRA (OpenHermes 400) | 55.80 | 8.79\u00d7 |\n| 70B w/o Fine-Tuning | 75.28 | 1.00\u00d7 |\n| 70B QLoRAM-Stru 400 (OpenHermes 400) | 80.36 | 15.81\u00d7 |\n| 70B QLoRAM-Stru 400 (GSM8K 100) | 77.18 | 15.81\u00d7 |\n| 70B QLoRAM-Stru 400 (GSM8K 200) | 79.15 | 15.81\u00d7 |\n| 70B LoRA (OpenHermes 400) | 80.74 | 1.00\u00d7 |", "caption": "Table 7: Evaluation of LoRAM on the GSM8K dataset for domain-specific fine-tuning. Results show accuracy (%) and parameter reduction ratios for different configurations.", "description": "\ud45c 7\uc740 \ud2b9\uc815 \uc601\uc5ed \ubbf8\uc138 \uc870\uc815\uc744 \uc704\ud574 GSM8K \ub370\uc774\ud130\uc14b\uc5d0\uc11c LoRAM\uc758 \uc131\ub2a5 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc124\uc815\uc5d0 \ub530\ub978 \uc815\ud655\ub3c4(%)\uc640 \ud30c\ub77c\ubbf8\ud130 \uac10\uc18c \ube44\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubcf8 \uc2e4\ud5d8\uc740 LoRAM\uc774 \uc77c\ubc18\uc801\uc778 \uc9c0\uc2dc\uc5b4 \ubbf8\uc138 \uc870\uc815\uc744 \ub118\uc5b4 \ud2b9\uc815 \ub3c4\uba54\uc778 \uc791\uc5c5\uc5d0\uc11c\ub3c4 \uac15\ub825\ud568\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uae30\ubcf8 \ubaa8\ub378(\ubbf8\uc138 \uc870\uc815 \uc5c6\uc74c)\uacfc LoRA\ub85c \ubbf8\uc138 \uc870\uc815\ub41c \uc18c\uaddc\ubaa8 \ubaa8\ub378\uacfc LoRAM\uc758 \uacb0\uacfc\ub97c \ube44\uad50\ud558\uc5ec LoRAM\uc758 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.3 \ud558\uc704 \uc791\uc5c5 \uc131\ub2a5"}, {"content": "| Model | #Model Params | Reduction Ratio | Memory | Latency | Throughput |\n|---|---|---|---|---|---| \n| **LLaMA-2** | **#Model Params** | **Reduction Ratio** | **Memory** | **Latency** | **Throughput** |\n| 7B LoRA | 6.73B | 1.93\u00d7 | 30,517 | **134.27** | **7.626** |\n| 13B LoRA | 13.02B | 1.00\u00d7 | 51,661 | 206.07 | 4.969 |\n| 13B LoRAM-Stru | **6.01B** | **2.17\u00d7** | **29,799** | 147.86 | 6.925 |", "caption": "Table 8: Comparison of peak memory (MiB), latency (s), and throughput (samples/s) during the online training phase for LoRAM and LoRA models. Results are based on a workload of 1024 samples (batch size 128, micro-batch size 4, sequence length 512).", "description": "\ud45c 8\uc740 LoRAM\uacfc LoRA \ubaa8\ub378\uc758 \uc628\ub77c\uc778 \ud559\uc2b5 \ub2e8\uacc4\uc5d0\uc11c \ud53c\ud06c \uba54\ubaa8\ub9ac(MiB), \ub300\uae30 \uc2dc\uac04(\ucd08), \ucc98\ub9ac\ub7c9(\uc0d8\ud50c/\ucd08)\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  \uacb0\uacfc\ub294 1024\uac1c\uc758 \uc0d8\ud50c(\ubc30\uce58 \ud06c\uae30 128, \ub9c8\uc774\ud06c\ub85c \ubc30\uce58 \ud06c\uae30 4, \uc2dc\ud000\uc2a4 \uae38\uc774 512) \uc791\uc5c5\ub7c9\uc744 \uae30\ubc18\uc73c\ub85c \ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 LoRAM\uc774 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uacfc \ub300\uae30 \uc2dc\uac04 \uce21\uba74\uc5d0\uc11c LoRA\uc640 \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\uba74\uc11c\ub3c4 \ucc98\ub9ac\ub7c9 \uce21\uba74\uc5d0\uc11c \uc57d\uac04\uc758 \uc774\uc810\uc744 \uc81c\uacf5\ud55c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.2 Fine-tuning Convergence"}]