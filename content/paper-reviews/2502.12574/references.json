{"references": [{"fullname_first_author": "Agrawal, A.", "paper_title": "Taming throughput-latency tradeoff in LLM inference with sarathi-serve", "publication_date": "2024-00-00", "reason": "This paper proposes SarathiServe, a system that addresses the throughput-latency tradeoff in LLM inference, which is directly relevant to the challenges addressed by HEADINFER."}, {"fullname_first_author": "Aminabadi, R. Y.", "paper_title": "Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale", "publication_date": "2022-00-00", "reason": "This paper introduces Deepspeed-inference, a system that enables efficient inference of transformer models at unprecedented scale, which is a crucial component and related work of HEADINFER."}, {"fullname_first_author": "Bai, Y.", "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding", "publication_date": "2023-08-00", "reason": "This paper introduces LongBench, a benchmark specifically designed for evaluating long-context understanding in LLMs, which is directly used for evaluating the performance of HEADINFER."}, {"fullname_first_author": "Dao, T.", "paper_title": "Flashattention: Fast and memory-efficient exact attention with IO-awareness", "publication_date": "2022-00-00", "reason": "This paper introduces FlashAttention, a fast and memory-efficient attention mechanism, which is used as a building block within HEADINFER and is central to its performance."}, {"fullname_first_author": "Sheng, Y.", "paper_title": "Flexgen: High-throughput generative inference of large language models with a single GPU", "publication_date": "2023-00-00", "reason": "This paper introduces FlexGen, a system that achieves high-throughput generative inference of LLMs using a single GPU, which is a related work and addresses similar challenges as HEADINFER."}]}