<br><table id='2' style='font-size:16px'><tr><td>入: Regularization</td><td>Inputs: attentions: List of attention matrices, 日(L, H)= reg_threshold_weights, T: Sequence length, loss weightage, Y: Hyper-parameter for Tolerance margin</td></tr><tr><td>Output: 1:</td><td>Ltotal: Total loss including entropy regularization</td></tr><tr><td></td><td>Lentropy ← 0</td></tr><tr><td></td><td></td></tr><tr><td>2: Emax</td><td>← log(T) ▷ Theoretical maximum value of entropy</td></tr><tr><td>3:</td><td>Tolmargin ← YEmax ▷ Tolerance margin is set as a small fraction of Emax for each layer 1 in layers do</td></tr><tr><td>4: 5:</td><td>Llayer ← 0</td></tr><tr><td>6:</td><td>A(t) ← attentions[2] ▷ Attention matrix with learnable temperature for each query position</td></tr><tr><td></td><td>1 ET=1 �T=1 Aij (t) log(Aij (t)) ▷ Compute entropy, averaged over query length E(t) ← - T</td></tr><tr><td>7:</td><td></td></tr><tr><td>8: 9:</td><td>for each head h in heads do E(l,h) ← Slice(E(t), ん) ▷ Entropy for head h</td></tr><tr><td></td><td>�(l,h) ← Slice(O(L, H), ん) ▷ Learnable threshold weight head h</td></tr><tr><td>10: 11:</td><td>S(l,h) ← E(l,h) (t) - �(l,h) Emax ▷ Deviation from head-specific threshold</td></tr><tr><td>12:</td><td>penalty(l,h) ← (8(l,h))21 (|8(l,h)| > Tolmargin) ▷ Penalize iff deviation exceeds Tolerance</td></tr><tr><td>13:</td><td>Llayer ← Llayer + penalty (l,h)</td></tr><tr><td>14:</td><td>end for Llayer</td></tr><tr><td>15:</td><td>Llayer ← num heads</td></tr><tr><td>16:</td><td>▷ Average over heads Lentropy Lentropy + Llayer</td></tr><tr><td>17:</td><td>end for Lentropy Lentropy</td></tr><tr><td>18:</td><td>Average over layers ▷</td></tr><tr><td>19:</td><td>len(attentions)</td></tr><tr><td></td><td>Ltotal ← LCE + XLentropy</td></tr><tr><td>20:</td><td>return Ltotal</td></tr></table>