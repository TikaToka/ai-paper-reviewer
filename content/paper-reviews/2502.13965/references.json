{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-MM-DD", "reason": "This paper is foundational to the field of large language models (LLMs), introducing the concept of few-shot learning, which is central to many LLM applications and the focus of this paper."}, {"fullname_first_author": "Amey Agrawal", "paper_title": "Sarathi: Efficient LLM inference by piggybacking decodes with chunked prefills", "publication_date": "2023-MM-DD", "reason": "This paper proposes efficient LLM inference optimizations that are directly relevant to the challenges and solutions presented in the current paper."}, {"fullname_first_author": "Woosuk Kwon", "paper_title": "PagedAttention: Efficient memory management for large language model serving with pagedattention", "publication_date": "2023-MM-DD", "reason": "This paper introduces the vLLM system, which is used as a baseline in the current paper's evaluation; understanding vLLM is crucial to interpreting the performance gains of Autellix."}, {"fullname_first_author": "Shunyu Yao", "paper_title": "React: Synergizing reasoning and acting in language models", "publication_date": "2023-MM-DD", "reason": "This paper introduces the ReAct framework, a key agentic program used in the experiments, making it highly relevant to the overall context of this paper."}, {"fullname_first_author": "Andy Zhou", "paper_title": "Language agent tree search unifies reasoning acting and planning in language models", "publication_date": "2024-MM-DD", "reason": "This paper introduces the Monte Carlo Tree Search (MCTS) technique, another important agentic program used in the current paper's experiments, thus providing essential context."}]}