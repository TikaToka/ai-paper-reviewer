{"references": [{"fullname_first_author": "Team LLaMA", "paper_title": "The Llama 3 Herd of Models", "publication_date": "2024-07-21", "reason": "This paper introduces Llama 3, the base large language model used in the study, and its various configurations, making it foundational to the research."}, {"fullname_first_author": "Gagan Bhatia", "paper_title": "FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models", "publication_date": "2024-02-10", "reason": "This paper introduces FinTral, a state-of-the-art financial LLM, which serves as a key comparison point for evaluating the performance of Llama-Fin."}, {"fullname_first_author": "Zixuan Ke", "paper_title": "Bridging the preference gap between retrievers and LLMs", "publication_date": "2024-01-06", "reason": "This paper addresses the challenge of aligning LLMs with human preferences, a critical aspect of the proposed preference alignment method in FINDAP."}, {"fullname_first_author": "Richard Yuanzhe Pang", "paper_title": "Iterative Reasoning Preference Optimization", "publication_date": "2024-00-00", "reason": "This paper introduces iterative reasoning preference optimization, a crucial technique used in the preference alignment stage of FINDAP to improve reasoning capabilities."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "publication_date": "2023-00-00", "reason": "This paper introduces direct preference optimization (DPO), the core algorithm employed in the preference alignment stage of FINDAP, which is central to its training process."}]}