{"references": [{"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-00-00", "reason": "This paper introduces the MAE model, which is used for initializing the model in Orient Anything, improving its synthetic-to-real transfer ability."}, {"fullname_first_author": "Maxime Oquab", "paper_title": "DINOv2: Learning robust visual features without supervision", "publication_date": "2023-00-00", "reason": "The DINOv2 model is used for initializing the visual encoder in Orient Anything, significantly improving synthetic-to-real transfer."}, {"fullname_first_author": "Garrick Brazil", "paper_title": "Omni3D: A large benchmark and model for 3d object detection in the wild", "publication_date": "2023-00-00", "reason": "This paper introduces the Omni3D dataset and Cube R-CNN model, which is used as a baseline in evaluating the performance of Orient Anything."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This paper introduces the CLIP model, which is used as a baseline for comparing the performance of Orient Anything in orientation understanding."}, {"fullname_first_author": "Aaron Hurst", "paper_title": "GPT-40 system card", "publication_date": "2024-00-00", "reason": "This paper introduces the GPT-40 model, which is used as a baseline for comparing the performance of Orient Anything in orientation understanding and spatial reasoning."}]}