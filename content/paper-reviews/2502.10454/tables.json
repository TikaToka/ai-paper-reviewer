[{"content": "| Models | Judgement F1 (macro) | Rationale Reasoning Examples (%) | Rationale Reasoning Strict (%) | Rationale Reasoning Loose (%) |\n|---|---|---|---|---|\n| **Open source models** |  |  |  |  |\n| size = 7B | Deepseek-Math-7B-rl | 32.2 | 65.9 | 18.9 | 20.6 |\n|  | Eurus-2-7B-PRIME | 37.5 | 64.8 | 28.5 | 32.0 |\n|  | NuminaMath-7B-TIR | 30.4 | 54.1 | 13.0 | 13.7 |\n|  | InternLM2-Math-Plus-7B | 33.9 | 36.6 | 9.0 | 9.5 |\n|  | Abel-7B-002 | 34.4 | 66.1 | 16.0 | 17.9 |\n|  | WizardMath-7B-v1.1 | 27.9 | 43.2 | 6.4 | 7.2 |\n|  | Mathstral-7B-v0.1 | 28.2 | 38.9 | 7.5 | 7.9 |\n|  | MetaMath-Mistral-7B | 31.0 | 26.5 | 0.4 | 0.7 |\n|  | Xwin-Math-7B-V1.0 | 28.1 | 31.3 | 1.2 | 1.7 |\n|  | rho-math-7b-interpreter-v0.1 | 22.3 | 18.3 | 1.9 | 2.1 |\n|  | MAmmoTH2-7B-Plus | 32.3 | 54.2 | 10.7 | 12.1 |\n|  | Qwen2.5-Math-7B-Instruct | **38.3** | **74.2** | **30.2** | **33.2** |\n| 7B<size <70B | Abel-13B-001 | 22.4 | 24.4 | 0.8 | 0.8 |\n|  | Xwin-Math-13B-V1.0 | 30.2 | 31.3 | 1.2 | 1.7 |\n|  | InternLM2-Math-Plus-20B | 18.4 | 28.8 | 8.4 | 9.5 |\n|  | MAmmoTH2-8x7B-Plus | 28.8 | 51.4 | 14.1 | 15.5 |\n|  | QwQ-32B-Preview | **39.9** | **70.0** | **38.6** | **43.8** |\n| size >=70B | InternLM2-Math-Plus-Mixtral8x22B | 37.3 | 63.2 | 21.5 | 23.1 |\n|  | Xwin-Math-70B-V1.0 | 25.5 | 25.2 | 1.4 | 1.7 |\n|  | Abel-70B-001 | 31.0 | 48.4 | 5.3 | 6.1 |\n|  | WizardMath-70B-v1.0 | 24.2 | 52.9 | 6.3 | 7.4 |\n|  | Qwen2.5-Math-72B-Instruct | **41.8** | **76.6** | **38.9** | **41.6** |\n| **Commercial models** |  |  |  |  |\n|  | GPT-4o | 59.0 | 44.7 | 19.7 | 21.3 |\n|  | OpenAI o1-preview | **60.1** | 55.8 | **39.8** | **40.9** |\n|  | Qwen-max | 58.9 | **61.8** | 30.4 | 33.9 |", "caption": "Table 1: Main evaluation results of various mainstream mathematical LLMs with the default CoT prompts on CounterMATH. The Examples, Strict, and Loose represent the three of our designed example-related evaluation metrics.", "description": "\ud45c 1\uc740 \ub2e4\uc591\ud55c \uc8fc\ub958 \uc218\ud559\uc801 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc744 CounterMATH\uc5d0 \uae30\ubcf8 CoT \ud504\ub86c\ud504\ud2b8\uc640 \ud568\uaed8 \ud3c9\uac00\ud55c \uc8fc\uc694 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Examples, Strict, Loose\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc0c8\ub86d\uac8c \uc81c\uc548\ud55c \uc138 \uac00\uc9c0 \uc608\uc2dc \uad00\ub828 \ud3c9\uac00 \uc9c0\ud45c\uc785\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uac01 \ubaa8\ub378\uc758 \uc815\ub2f5\ub960(F1 \uc810\uc218), \uc608\uc2dc \uc0ac\uc6a9 \ube44\uc728, \uc5c4\uaca9\ud55c \uc77c\uce58\uc728, \ub290\uc2a8\ud55c \uc77c\uce58\uc728\uc744 \ubcf4\uc5ec\uc8fc\uc5b4 \ubaa8\ub378\uc758 \uc218\ud559\uc801 \ucd94\ub860 \ub2a5\ub825, \ud2b9\ud788 \ubc18\ub840 \uae30\ubc18 \ucd94\ub860 \ub2a5\ub825\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.", "section": "4. \ubca4\uce58\ub9c8\ud06c \uc124\uc815"}, {"content": "| Models | F1 (macro) | Examples(%) | Strict(%) | Loose(%) |\n|---|---|---|---|---|\n| **Base models** |  |  |  |  |\n| Qwen2.5-Math-7B-Instruct | 38.3 | 74.2 | 30.2 | 33.2 |\n| Qwen2.5-Math-7B-Instruct + Hint prompt | 39.4 | 79.0 | **33.1** | **36.4** |\n| **Our training model** |  |  |  |  |\n| Qwen2.5-Math-7B-Instruct-SFT | 39.7 | 75.2 | 31.4 | 34.7 |\n| Qwen2.5-Math-7B-Instruct-SFT + Hint prompt | **41.1** | **79.4** | 31.1 | 34.7 |", "caption": "Table 2: The evaluation results on our CounterMATH.", "description": "\ud45c 2\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud55c CounterMATH \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 F1 \uc2a4\ucf54\uc5b4(\uac70\uc2dc\uc801), \uc608\uc2dc \uc0ac\uc6a9 \ube44\uc728, \uc5c4\uaca9\ud55c \uc815\ub82c, \ub290\uc2a8\ud55c \uc815\ub82c \ub4f1 \ub2e4\uc591\ud55c \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. \uae30\ubcf8 \ubaa8\ub378\uacfc \ud78c\ud2b8 \ud504\ub86c\ud504\ud2b8\ub97c \ucd94\uac00\ud55c \ubaa8\ub378, \uadf8\ub9ac\uace0 \uc81c\uc548\ub41c \ubc29\ubc95\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ub41c \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uc5b4,  \uce74\uc6b4\ud130\uc608\uc2dc \uae30\ubc18 \ucd94\ub860 \ub2a5\ub825 \ud5a5\uc0c1 \ud6a8\uacfc\ub97c \uce21\uc815\ud569\ub2c8\ub2e4.", "section": "6. Evaluation Results"}, {"content": "| Models | GSM8K | MATH |\n|---|---|---|\n| GPT-4o-2024-08-06 | 92.9 | 81.1 |\n| Qwen2.5-math-7B-Instruct | 95.1 | 80.5 |\n| Qwen2.5-math-72B-Instruct | 95.4 | 84.9 |\n| Qwen2.5-math-7B-Instruct<br>+Countermath-SFT | 95.6 | 87.9 |", "caption": "Table 3: The Out-of-distribution Evaluation Results.", "description": "\ud45c 3\uc740 \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc5d0 \ud3ec\ud568\ub418\uc9c0 \uc54a\uc740, \uc989 \ubaa8\ub378\uc774 \uc811\ud574\ubcf4\uc9c0 \ubabb\ud55c \uc0c8\ub85c\uc6b4 \uc720\ud615\uc758 \uc218\ud559 \ubb38\uc81c\ub4e4(Out-of-distribution data)\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, MATH\uc640 GSM8K\ub77c\ub294 \ub450 \uac1c\uc758 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \uce21\uc815\ud558\uc600\uc2b5\ub2c8\ub2e4.  \uc774\ub294 COUNTERMATH \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubbf8\uc138 \uc870\uc815(finetuning)\ud55c \ubaa8\ub378\uc774 \uc5bc\ub9c8\ub098 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \uc218\ud559 \ubb38\uc81c\uc5d0 \ub300\ud574\uc11c\ub3c4 \ud6a8\uacfc\uc801\uc73c\ub85c \uc801\uc6a9\ub420 \uc218 \uc788\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc911\uc694\ud55c \uc9c0\ud45c\uc785\ub2c8\ub2e4.", "section": "6. Evaluation Results"}, {"content": "| Model             | Methods                  |\n|-----------------|--------------------------|\n| Qwen2.5-math-7B-Instruct | +Countermath-SFT        |", "caption": "Table 4: Summary of open-weight baseline models. CP stands for Continue Pretrain. SFT stands for Supervised Fine-Tuning. GRPO refers to a variant of PPO, which replaces the value network with the group average (Shao et\u00a0al., 2024). PoT (Chen et\u00a0al., 2023) and TIR (Gou et\u00a0al., 2024) stand for Program-of-Thought and Tool-Integrated Reasoning, respectively. PRIME stands for using ORM as PRM by DPO-like rewards (Cui et\u00a0al., 2025). SLM stands for Selective Language Modeling (Lin et\u00a0al., 2024b). \u2217 only stands for the same model architecture.", "description": "\ud45c 4\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc624\ud508 \uc18c\uc2a4 \uae30\ubc18 \ubaa8\ub378\ub4e4\uc758 \uc694\uc57d \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \ud06c\uae30(Scale), \uae30\ubc18 \ubaa8\ub378(Base Models), \ud559\uc2b5 \ub370\uc774\ud130(Training Data), \uadf8\ub9ac\uace0 \ud559\uc2b5 \ubc29\ubc95(Training Paradigms)\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud559\uc2b5 \ubc29\ubc95\uc5d0 \ub300\ud55c \uc57d\uc5b4\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4: CP(Continue Pretrain), SFT(Supervised Fine-Tuning), GRPO(\uadf8\ub8f9 \ud3c9\uade0\uc744 \uc0ac\uc6a9\ud55c PPO \ubcc0\ud615), PoT(Program-of-Thought), TIR(Tool-Integrated Reasoning), PRIME(DPO \uc720\uc0ac \ubcf4\uc0c1\uc744 \uc0ac\uc6a9\ud55c ORM \uae30\ubc18 PRM), SLM(Selective Language Modeling). \ub9c8\uc9c0\ub9c9 \uc5f4\uc758 \u2217 \ud45c\uc2dc\ub294 \ub3d9\uc77c\ud55c \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4. Benchmark Settings"}]