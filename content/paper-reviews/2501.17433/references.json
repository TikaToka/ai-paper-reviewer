{"references": [{"fullname_first_author": "Qi", "paper_title": "Fine-tuning aligned language models compromises safety, even when users do not intend to!", "publication_date": "2023-10-03", "reason": "This paper is foundational to the current work by establishing the vulnerability of fine-tuned LLMs to harmful attacks, motivating the need for guardrail moderation."}, {"fullname_first_author": "Rosati", "paper_title": "Representation noising effectively prevents harmful fine-tuning on LLMs", "publication_date": "2024-05-14", "reason": "This paper introduces a defense mechanism against harmful fine-tuning attacks, which is directly relevant to the proposed Virus attack that attempts to bypass this defense."}, {"fullname_first_author": "Huang", "paper_title": "Booster: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation", "publication_date": "2024-09-01", "reason": "This paper proposes a defense mechanism against harmful fine-tuning, offering a comparison point for the effectiveness of the proposed Virus attack."}, {"fullname_first_author": "Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "As the method used for fine-tuning in the experiments, this paper's method is essential to the reproducibility and validity of the results."}, {"fullname_first_author": "Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-15", "reason": "This paper introduces the GCG optimizer, which is used in the Virus attack for data optimization; understanding this method is critical to understanding the proposed approach."}]}