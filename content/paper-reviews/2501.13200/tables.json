[{"content": "| Parameter | MAPF (all models) | SRMT LMAPF |\n|---|---|---|\n| Optimizer | Adam | Adam |\n| Learning rate | 0.00013 | 0.00022 |\n| LR Scheduler | Adaptive KL | Constant |\n| \u03b3 (discount factor) | 0.9716 | 0.9756 |\n| Recurrence rollout | 8 | - |\n| Clip ratio | 0.2 | 0.2 |\n| Batch size | 16384 | 16384 |\n| Optimization epochs | 1 | 1 |\n| Entropy coefficient | 0.0156 | 0.023 |\n| Value loss coefficient | 0.5 | 0.5 |\n| GAE<sub>\u03bb</sub> | 0.95 | 0.95 |\n| MLP hidden size | 16 | 512 |\n| ResNet residual blocks | 1 | 8 |\n| ResNet filters | 8 | 64 |\n| Attention hidden size | 16 | 512 |\n| Attention heads | 4 | 8 |\n| GRU hidden size | 16 | - |\n| Activation function | ReLU | ReLU |\n| Network Initialization | orthogonal | orthogonal |\n| Rollout workers | 4 | 8 |\n| Envs per worker | 4 | 4 |\n| Training steps | 2e+07 | 1e+09 |\n| Episode length | 512 | 512 |\n| Observation patch | 5x5 | 11x11 |\n| Number of agents | 2 | 64 |", "caption": "Table 1: Models configuration and training hyperparameters.", "description": "\ud45c 1\uc740 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uc124\uc815\uacfc \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  MAPF(Multi-Agent Pathfinding) \uc791\uc5c5\uacfc SRMT(Shared Recurrent Memory Transformer) \ubc0f LMAPF(Lifelong Multi-Agent Pathfinding) \uc791\uc5c5\uc5d0 \ub300\ud574 \ucd5c\uc801\ud654\uae30, \ud559\uc2b5\ub960, \uac10\uc1e0\uc728, \ubc30\uce58 \ud06c\uae30, \ud0d0\uc0c9 \uacf5\uac04 \ud06c\uae30 \ub4f1\uc758 \uc138\ubd80\uc801\uc778 \uc124\uc815 \uac12\ub4e4\uc744 \ube44\uad50\ud558\uc5ec \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uc774\ub294 \uac01 \ubaa8\ub378\uc758 \ud559\uc2b5 \uacfc\uc815\uacfc \uc131\ub2a5\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc911\uc694\ud55c \uc694\uc18c\ub4e4\uc744 \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788 SRMT \ubaa8\ub378\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \uc790\uc138\ud788 \uc81c\uc2dc\ud558\uc5ec, \ubcf8 \uc5f0\uad6c\uc758 \uc7ac\ud604\uc131\uc744 \ub192\uc774\ub294 \ub370 \uae30\uc5ec\ud569\ub2c8\ub2e4.", "section": "A.1 \ud559\uc2b5 \uc138\ubd80 \uc0ac\ud56d"}, {"content": "| Type | On goal | Move towards goal | Else |\n|---|---|---|---|\n| Directional | +1 | +0.005 | 0 |\n| Sparse | +1 | 0 | 0 |\n| Dense | +1 | -0.01 | -0.01 |\n| Directional Negative | +1 | -0.005 | -0.01 |\n| Moving Negative | +1 | -0.01 | -0.01 for moving, -0.005 for holding |", "caption": "Table 2: Tested reward functions. We list the reward values for achieving the goal, moving on the path toward the goal, or taking other actions (moving not in the direction of the goal and staying in the same position).", "description": "\ud45c 2\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ubcf4\uc0c1 \ud568\uc218\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa9\ud45c \uc9c0\uc810\uc5d0 \ub3c4\ub2ec\ud588\uc744 \ub54c, \ubaa9\ud45c \uc9c0\uc810\uc744 \ud5a5\ud574 \uc774\ub3d9\ud560 \ub54c, \uadf8\ub9ac\uace0 \ubaa9\ud45c \uc9c0\uc810\uacfc\ub294 \ubc18\ub300 \ubc29\ud5a5\uc73c\ub85c \uc774\ub3d9\ud558\uac70\ub098 \uc81c\uc790\ub9ac\uc5d0 \uba38\ubb3c\ub800\uc744 \ub54c \uac01\uac01\uc758 \ubcf4\uc0c1 \uac12\uc744 \ub098\uc5f4\ud569\ub2c8\ub2e4.  \ubcf4\uc0c1 \ud568\uc218\uc758 \uc885\ub958\uc5d0\ub294 Directional, Sparse, Dense, Directional Negative, Moving Negative \ub4f1\uc774 \uc788\uc73c\uba70, \uac01 \ud568\uc218\ub294 \ubaa9\ud45c \ub2ec\uc131 \ubc0f \uc774\ub3d9 \ubc29\ud5a5\uc5d0 \ub530\ub978 \ubcf4\uc0c1 \uac12\uc744 \ub2e4\ub974\uac8c \uc124\uc815\ud558\uc5ec \uc5d0\uc774\uc804\ud2b8\uc758 \ud589\ub3d9\uc744 \uc720\ub3c4\ud569\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uac01 \ubcf4\uc0c1 \ud568\uc218\uac00 \uc5d0\uc774\uc804\ud2b8\uc758 \ud559\uc2b5 \ubc0f \ud589\ub3d9\uc5d0 \uc5b4\ub5bb\uac8c \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \uc774\ud574\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4.1 Bottleneck\uc5d0\uc11c\uc758 \uace0\uc804\uc801 MAPF"}]