{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-02", "reason": "This paper is foundational for RLHF, a core technique used in training and aligning language models, and is directly relevant to reward models."}, {"fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-04-05", "reason": "This paper introduces a key approach to training helpful and harmless language models using RLHF, which is directly related to reward model development and evaluation."}, {"fullname_first_author": "Percy Liang", "paper_title": "Holistic evaluation of language models", "publication_date": "2022-11-09", "reason": "This paper introduces the concept of holistic evaluation for language models, a crucial framework for comprehensive benchmarking, which is adopted and extended in this work for multimodal reward models."}, {"fullname_first_author": "Tony Lee", "paper_title": "VHELM: A holistic evaluation of vision language models", "publication_date": "2024-10-07", "reason": "This paper presents a holistic evaluation framework specifically for vision-language models (VLMs), providing a benchmark against which the proposed Multimodal RewardBench can be compared."}, {"fullname_first_author": "Nathan Lambert", "paper_title": "RewardBench: Evaluating reward models for language modeling", "publication_date": "2024-03-13", "reason": "This paper introduces a benchmark for evaluating reward models for text-based language models, which is extended by this work to address the multimodal setting."}]}