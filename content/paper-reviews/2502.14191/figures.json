[{"figure_path": "https://arxiv.org/html/2502.14191/x1.png", "caption": "Figure 1: Illustration of Multimodal RewardBench. We build a human-annotated benchmark that consists of (multimodal prompt, chosen response, rejected response) triplets (left). Using this benchmark, we evaluate the accuracy of various reward models or judges for vision-language models (right).\nSee \u00a7A for real examples from our benchmark.", "description": "\uadf8\ub9bc 1\uc740 Multimodal RewardBench\uc758 \uac1c\ub150\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc5d0\ub294 \ub2e4\uc591\ud55c \uc2dc\uac01 \uc5b8\uc5b4 \ubaa8\ub378(VLM)\uc758 \ucd9c\ub825\ubb3c\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574, \uc0ac\ub78c\uc774 \uc8fc\uc11d\uc744 \ub2e8 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc758 \uc608\uc2dc\uac00 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\uc14b\uc740 \ub2e4\uc591\ud55c \ubaa8\ub4dc\ub97c \uac00\uc9c4 \ud504\ub86c\ud504\ud2b8(prompt), \uc120\ud0dd\ub41c \uc751\ub2f5(chosen response), \uae30\uac01\ub41c \uc751\ub2f5(rejected response) \uc138 \uac00\uc9c0 \uc694\uc18c\uc758 \uc138 \uc30d(triplets)\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \uc624\ub978\ucabd\uc5d0\ub294 \uc774\ub807\uac8c \ub9cc\ub4e4\uc5b4\uc9c4 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \ubcf4\uc0c1 \ubaa8\ub378 \ub610\ub294 VLM \ud310\uc815 \ubaa8\ub378(judge)\uc758 \uc815\ud655\ub3c4\ub97c \ud3c9\uac00\ud558\ub294 \uacfc\uc815\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "1 Introduction"}]