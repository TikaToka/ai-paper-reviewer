{"references": [{"fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-05-14", "reason": "This paper introduced the concept of few-shot learning in large language models, a key concept for the self-adaptive LLMs discussed in the current paper."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-27", "reason": "This paper established scaling laws for neural language models, which is foundational to understanding the resource requirements and performance trade-offs inherent in self-adaptive LLMs."}, {"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-21", "reason": "This paper introduced LoRA, a parameter-efficient fine-tuning method for LLMs, which the current paper improves upon with its SVF method."}, {"fullname_first_author": "Chen Tianlong", "paper_title": "Mixture-of-experts in the era of LLMs: A new odyssey", "publication_date": "2024-07-01", "reason": "This paper reviews the state-of-the-art in Mixture-of-Experts (MoE) models, a related approach to self-adaptive LLMs that shares similar goals of efficiently handling diverse tasks."}, {"fullname_first_author": "Hanqing Wang", "paper_title": "Milora: Harnessing minor singular components for parameter-efficient LLM finetuning", "publication_date": "2024-06-20", "reason": "This paper explores parameter-efficient fine-tuning using singular value decomposition, a technique closely related to the SVF method proposed in the current paper."}]}