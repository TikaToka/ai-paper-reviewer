{"references": [{"fullname_first_author": "Azar, M. G.", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "publication_date": "2024-00-00", "reason": "This paper provides a general theoretical framework for understanding learning from human preferences, which is highly relevant to the study of test-time preference optimization."}, {"fullname_first_author": "Bai, Y.", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-00-00", "reason": "This paper is highly influential in the field due to its introduction of reinforcement learning from human feedback (RLHF), a key technique for aligning LLMs with human preferences."}, {"fullname_first_author": "Dubey, A.", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-21", "reason": "This paper introduces the Llama 3 family of models, which serve as the foundation for the experiments in the current paper on test-time preference optimization."}, {"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper is highly influential as it presents the reinforcement learning from human feedback (RLHF) method, which is fundamental for aligning LLMs."}, {"fullname_first_author": "Rafailov, R.", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-00-00", "reason": "This paper introduces Direct Preference Optimization (DPO), a method directly optimizing the model parameters based on reward signals, providing a foundation for contrast with the test-time approach"}]}