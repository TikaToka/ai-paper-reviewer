[{"figure_path": "https://arxiv.org/html/2501.12895/x1.png", "caption": "Figure 1: \nTraining-time preference optimization (e.g., RLHF and DPO) compared with test-time preference optimization (TPO), where the model aligns with human preferences during test-time with the model parameters fixed.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \ud559\uc2b5 \uc2dc\uac04 \uc120\ud638\ub3c4 \ucd5c\uc801\ud654(RLHF \ubc0f DPO)\uc640 \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \uc120\ud638\ub3c4 \ucd5c\uc801\ud654(TPO)\ub97c \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. RLHF \ubc0f DPO\ub294 \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\uc5ec \uc778\uac04\uc758 \uc120\ud638\ub3c4\uc5d0 \ub9de\ucd94\ub294 \ubc18\uba74, TPO\ub294 \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218\ub97c \uace0\uc815\ud55c \uc0c1\ud0dc\uc5d0\uc11c \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \ub3d9\uc548 \uc778\uac04\uc758 \uc120\ud638\ub3c4\uc5d0 \ub9de\ucdb0 \ubaa8\ub378\uc744 \uc870\uc815\ud569\ub2c8\ub2e4. \uc989, TPO\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc744 \uc7ac\ud6c8\ub828\ud558\uc9c0 \uc54a\uace0\ub3c4 \uc2e4\uc2dc\uac04\uc73c\ub85c \uc778\uac04\uc758 \uc120\ud638\ub3c4\uc5d0 \ub9de\ucdb0 \uacb0\uacfc\ub97c \uc870\uc815\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.12895/x4.png", "caption": "Figure 2: Framework of test-time preference optimization (TPO), shown here on a real example from AlpacaEval 2.\nConcretely, the model samples responses and scores them with a reward model (Left), interprets reward model feedback of chosen response v3subscript\ud835\udc633v_{3}italic_v start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and rejected response v1subscript\ud835\udc631v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT (Middle),\nand provides critiques, generates improvement suggestions (Right),\nand then updates new responses for the next iteration.\nIn analogy to traditional gradient-based methods, TPO performs gradient descent (loss calculation, gradient computation and variable optimization) in textual form to tailor model outputs based on numerical feedback from the reward model.", "description": "\uadf8\ub9bc 2\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 Test-time Preference Optimization (TPO)\uc758 \ud504\ub808\uc784\uc6cc\ud06c\ub97c AlpacaEval 2\uc758 \uc2e4\uc81c \uc608\uc2dc\ub97c \ud1b5\ud574 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc5d0\ub294 \ubaa8\ub378\uc774 \uc751\ub2f5\uc744 \uc0dd\uc131\ud558\uace0 \ubcf4\uc0c1 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc810\uc218\ub97c \ub9e4\uae30\ub294 \uacfc\uc815\uc774, \uac00\uc6b4\ub370\uc5d0\ub294 \uc120\ud0dd\ub41c \uc751\ub2f5(v3)\uacfc \uae30\uac01\ub41c \uc751\ub2f5(v1)\uc5d0 \ub300\ud55c \ubcf4\uc0c1 \ubaa8\ub378\uc758 \ud53c\ub4dc\ubc31\uc744 \ubaa8\ub378\uc774 \ud574\uc11d\ud558\ub294 \uacfc\uc815\uc774, \uc624\ub978\ucabd\uc5d0\ub294 \ubaa8\ub378\uc774 \ube44\ud310\uc744 \uc81c\uc2dc\ud558\uace0 \uac1c\uc120 \uc81c\uc548\uc744 \uc0dd\uc131\ud558\uc5ec \ub2e4\uc74c \ubc18\ubcf5\uc744 \uc704\ud55c \uc0c8\ub85c\uc6b4 \uc751\ub2f5\uc744 \uc0dd\uc131\ud558\ub294 \uacfc\uc815\uc774 \uac01\uac01 \ub098\ud0c0\ub098 \uc788\uc2b5\ub2c8\ub2e4. \uae30\uc874\uc758 \uae30\uc6b8\uae30 \uae30\ubc18 \ubc29\ubc95\uacfc \uc720\uc0ac\ud558\uac8c TPO\ub294 \ud14d\uc2a4\ud2b8 \ud615\ud0dc\uc758 \uc190\uc2e4 \uacc4\uc0b0, \uae30\uc6b8\uae30 \uacc4\uc0b0 \ubc0f \ubcc0\uc218 \ucd5c\uc801\ud654\ub97c \uc218\ud589\ud558\uc5ec \ubcf4\uc0c1 \ubaa8\ub378\uc758 \uc218\uce58\uc801 \ud53c\ub4dc\ubc31\uc5d0 \ub530\ub77c \ubaa8\ub378 \ucd9c\ub825\uc744 \uc870\uc815\ud569\ub2c8\ub2e4.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2501.12895/x5.png", "caption": "Figure 3: Test-time training curve for the (unaligned) SFT model and (aligned) instruct models.\nThe colored lines represent the test-time training performance (reward model score) w.r.t. training steps (i.e., number of TPO iterations), while the dashed horizontal lines indicate scores for models without test-time training.", "description": "\uadf8\ub9bc 3\uc740 \uc815\ub82c\ub418\uc9c0 \uc54a\uc740 SFT \ubaa8\ub378\uacfc \uc815\ub82c\ub41c Instruct \ubaa8\ub378\uc5d0 \ub300\ud55c \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \ud6c8\ub828 \uace1\uc120\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0c9\uc0c1\uc774 \uc788\ub294 \uc120\uc740 TPO \ubc18\ubcf5 \ud69f\uc218(\uc989, \ud6c8\ub828 \ub2e8\uacc4)\uc5d0 \ub530\ub978 \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \ud6c8\ub828 \uc131\ub2a5(\ubcf4\uc0c1 \ubaa8\ub378 \uc810\uc218)\uc744 \ub098\ud0c0\ub0b4\uace0, \uc810\uc120\uc740 \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \ud6c8\ub828 \uc5c6\uc774 \ubaa8\ub378\uc758 \uc810\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \uadf8\ub798\ud504\ub294 TPO\uac00 \ud6c8\ub828\ub418\uc9c0 \uc54a\uc740 \ubaa8\ub378\uacfc \ud6c8\ub828\ub41c \ubaa8\ub378 \ubaa8\ub450\uc5d0\uc11c \ubcf4\uc0c1 \ubaa8\ub378 \uc810\uc218\ub97c \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \uba87 \ubc88\uc758 TPO \ub2e8\uacc4\ub9cc \uac70\uccd0\ub3c4 \ucd08\uae30\uc5d0\ub294 \uc815\ub82c\ub418\uc9c0 \uc54a\uc740 \ubaa8\ub378\uc774 \uc815\ub82c\ub41c \ubaa8\ub378\ubcf4\ub2e4 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4.", "section": "6. \uc2e4\ud5d8 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2501.12895/x6.png", "caption": "Figure 4: Inference stability of models with and without TPO.", "description": "\uadf8\ub9bc 4\ub294 TPO \uc801\uc6a9 \uc5ec\ubd80\uc5d0 \ub530\ub978 \ubaa8\ub378\uc758 \ucd94\ub860 \uc548\uc815\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. TPO\ub97c \uc801\uc6a9\ud55c \ubaa8\ub378\uc740 TPO\ub97c \uc801\uc6a9\ud558\uc9c0 \uc54a\uc740 \ubaa8\ub378\uc5d0 \ube44\ud574 \ucd94\ub860 \uacb0\uacfc\uc758 \ud45c\uc900 \ud3b8\ucc28\uac00 \ub354 \ub0ae\uc544, \ucd94\ub860 \uc548\uc815\uc131\uc774 \ud5a5\uc0c1\ub418\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 TPO\uac00 \ubaa8\ub378\uc758 \ucd9c\ub825 \ubd84\ud3ec\ub97c \uc778\uac04\uc758 \uc120\ud638\ub3c4\uc5d0 \ub354 \uc798 \ub9de\ucd94\ub3c4\ub85d \uc7ac\ubd84\ubc30\ud558\uc5ec, \ub354 \ub192\uc740 \ud488\uc9c8\uc758 \uc751\ub2f5\uc744 \uc0dd\uc131\ud558\uace0 \uc608\uce21\ud560 \uc218 \uc5c6\ub294 \uacb0\uacfc\ubb3c\uc744 \uc904\uc774\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \ud2b9\ud788, TPO\ub97c \uc801\uc6a9\ud55c \uc815\ub82c\ub418\uc9c0 \uc54a\uc740 \ubaa8\ub378\uc740 \uc815\ub82c\ub41c \ubaa8\ub378\ubcf4\ub2e4 \ub354 \ub098\uc740 \uc548\uc815\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6.2. Benchmark Performance"}, {"figure_path": "https://arxiv.org/html/2501.12895/x7.png", "caption": "Figure 5: Test-time training curves on the HH-RLHF dataset, evaluated under varying sampling widths (i.e., the number of responses sampled per iteration).", "description": "\ubcf8 \uadf8\ub9bc\uc740 HH-RLHF \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \uc0d8\ud50c\ub9c1 \ub108\ube44(\uc989, \uac01 \ubc18\ubcf5\uc5d0\uc11c \uc0d8\ud50c\ub9c1\ub41c \uc751\ub2f5 \uc218)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ub41c \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \ud6c8\ub828 \uace1\uc120\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc0d8\ud50c\ub9c1 \ub108\ube44\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ubcf4\uc0c1 \ubaa8\ub378 \uc810\uc218\uac00 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\uac00 \uc5ec\ub7ec \uac1c \ud45c\uc2dc\ub429\ub2c8\ub2e4. \uc774\ub294 \ud14c\uc2a4\ud2b8 \uc2dc\uac04\uc5d0 \uc120\ud638\ub3c4 \uc815\ub82c\uc774 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2dc\uac01\uc801 \uc99d\uac70\uc785\ub2c8\ub2e4. \uac01 \uace1\uc120\uc740 \ub2e4\ub978 \uc0d8\ud50c\ub9c1 \ub108\ube44\ub97c \ub098\ud0c0\ub0b4\uba70, \uc0d8\ud50c\ub9c1 \ub108\ube44\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ubcf4\uc0c1 \ubaa8\ub378 \uc810\uc218\uac00 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 TPO \uc54c\uace0\ub9ac\uc998\uc758 \ud6a8\uc728\uc131\uacfc \ud655\uc7a5\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "6.1 Test-time Training"}, {"figure_path": "https://arxiv.org/html/2501.12895/x8.png", "caption": "Figure 6: Win-rates of TPO against Best-of-N sampling (BoN).", "description": "\uadf8\ub9bc 6\uc740 TPO\uc640 \ubca0\uc2a4\ud2b8 \uc624\ube0c N (BoN) \uc0d8\ud50c\ub9c1\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  TPO\ub294 \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \ucd5c\uc801\ud654 \ubc29\ubc95\uc73c\ub85c, \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc218\uc815\ud558\uc9c0 \uc54a\uace0 \ucd94\ub860 \ub2e8\uacc4\uc5d0\uc11c \ubaa8\ub378 \ucd9c\ub825\uc744 \uc778\uac04\uc758 \uc120\ud638\ub3c4\uc5d0 \ub9de\ucdb0 \uc870\uc815\ud558\ub294 \uae30\ubc95\uc785\ub2c8\ub2e4. \ubc18\uba74 BoN \uc0d8\ud50c\ub9c1\uc740 \uc5ec\ub7ec \ud6c4\ubcf4 \uc751\ub2f5\uc744 \uc0dd\uc131\ud558\uace0, \ubcf4\uc0c1 \ubaa8\ub378(reward model)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac00\uc7a5 \uc88b\uc740 \uc751\ub2f5\uc744 \uc120\ud0dd\ud558\ub294 \uae30\ubc95\uc785\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc5d0\uc11c\ub294 AlpacaEval 2, Arena-Hard, HH-RLHF \uc138 \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc5d0\uc11c TPO\uc640 BoN\uc758 \uc2b9\ub960\uc744 \ube44\uad50\ud558\uc5ec TPO\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 TPO\ub294 BoN\ubcf4\ub2e4 \uc801\uc740 \uc0d8\ud50c \uc218\ub85c\ub3c4 \ub354 \ub192\uc740 \uc2b9\ub960\uc744 \ub2ec\uc131\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.  \uc138\ub85c\ucd95\uc740 GPT-4\uac00 \ud3c9\uac00\ud55c \uc2b9\ub960\uc744 \ub098\ud0c0\ub0b4\uace0, \uac00\ub85c\ucd95\uc740 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "6.2 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5"}, {"figure_path": "https://arxiv.org/html/2501.12895/x9.png", "caption": "Figure 7: Test-time training curve of Llama-3.1-8B-Instruct (red line) on the HH-RLHF dataset.", "description": "\uadf8\ub9bc 7\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c HH-RLHF \ub370\uc774\ud130\uc14b\uc744 \uae30\ubc18\uc73c\ub85c Llama-3.1-8B-Instruct \ubaa8\ub378\uc5d0 \ub300\ud55c \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \ud6c8\ub828 \uace1\uc120\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \ud6c8\ub828 \uacfc\uc815\uc5d0\uc11c \ubaa8\ub378\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uace1\uc120\uc744 \ud1b5\ud574 TPO(Test-time Preference Optimization)\uc758 \ud6a8\uacfc\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \ube68\uac04\uc0c9 \uc120\uc740 Llama-3.1-8B-Instruct \ubaa8\ub378\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \uadf8\ub798\ud504\ub294 TPO\ub97c \uc801\uc6a9\ud588\uc744 \ub54c \ubaa8\ub378\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \uadf8\ub798\ud504 \uc911 \ud558\ub098\uc774\uba70, \ubaa8\ub378\uc774 \ud14c\uc2a4\ud2b8 \uc2dc\uac04\uc5d0 \uc120\ud638\ub3c4\uc5d0 \ub9de\ucdb0 \uc5bc\ub9c8\ub098 \uc798 \uc870\uc815\ub418\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6.1. Test-time Training"}, {"figure_path": "https://arxiv.org/html/2501.12895/x10.png", "caption": "Figure 8: Test-time training curves of the Llama-3.1-70B-SFT using FsfairX-LLaMA3-RM-v0.1.", "description": "\uadf8\ub9bc 8\uc740 Llama-3.1-70B-SFT \ubaa8\ub378\uc5d0 \ub300\ud55c \ud14c\uc2a4\ud2b8 \uc2dc\uac04 \ud6c8\ub828 \uace1\uc120\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uace1\uc120\uc740 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b(AlpacaEval 2, Arena-Hard, HH-RLHF, BeaverTails, XSTest, MATH-500)\uc5d0 \uac78\uccd0 TPO \ubc18\ubcf5 \ud69f\uc218\uc5d0 \ub530\ub978 \ubcf4\uc0c1 \ubaa8\ub378 \uc810\uc218\uc758 \ubcc0\ud654\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574, TPO\ub97c \uc801\uc6a9\ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0\uc758 \uae30\uc900 \uc810\uc218(\uc810\uc120)\uc640 \ube44\uad50\ud558\uc5ec, TPO \ubc18\ubcf5 \ud69f\uc218\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ubcf4\uc0c1 \ubaa8\ub378 \uc810\uc218\uac00 \uc5b4\ub5bb\uac8c \ubcc0\ud654\ud558\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 TPO\uac00 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc810\uc9c4\uc801\uc73c\ub85c \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "6.1 Test-time Training"}]