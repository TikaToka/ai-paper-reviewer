[{"content": "| Model | AlpacaEval 2 LC(%) | AlpacaEval 2 WR(%) | Arena-Hard | HH-RLHF | BeaverTails | XSTest | MATH-500 |\n|---|---|---|---|---|---|---|---| \n| Llama-3.1-70B-DPO | 32.3 | 23.1 | 50.4 | -2.8 | -6.7 | 89.8 | 63.4 |\n| Llama-3.1-70B-Instruct | 36.9 | 34.9 | 59.0 | -0.5 | -6.4 | 88.7 | 66.4 |\n| Llama-3.1-70B-SFT | 27.8 | 16.8 | 44.1 | -4.1 | -7.2 | 87.8 | 61.8 |\n| w/ TPO (D2-N5) \u2020 | 33.2 | 39.5 | 70.5 | 0.1 | -4.1 | 89.8 | 70.0 |\n| w/ TPO (D2-N5) \u22c6 | 33.0 | 40.5 | 69.7 | -0.6 | -4.8 | 90.4 | 71.2 |\n| w/ TPO (D5-N20) \u22c6 | 37.8 | 55.7 | 77.5 | 0.4 | -4.1 | 89.6 | 71.8 |", "caption": "Table 1: Benchmark performance of the unaligned model (Llama-3.1-70B-SFT) with TPO, compared against training-time aligned baselines (Llama-3.1-70B-DPO and Llama-3.1-70B-Instruct).\nThe bold and underlined numbers indicate the best and second-best performances, respectively.\nBy default, the maximum number of iterations D\ud835\udc37Ditalic_D is set to 2, and the number of samples N\ud835\udc41Nitalic_N is set to 5.\nTo showcase the potential of TPO, we present an ultra setting, in which the number of iterations is increased to 5 and the number of samples to 20.\n\u22c6\u22c6\\star\u22c6 denotes the models optimized with TPO using the reward model FsfairX-LLaMA3-RM-v0.1, while \u2020\u2020\\dagger\u2020 denotes Llama-3.1-Tulu-3-8B-RM.", "description": "\ud45c 1\uc740 TPO\ub97c \uc801\uc6a9\ud55c \uc815\ub82c\ub418\uc9c0 \uc54a\uc740 \ubaa8\ub378(Llama-3.1-70B-SFT)\uc758 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5\uc744 \ud559\uc2b5 \uc2dc\uac04 \uae30\uc900 \uc815\ub82c\ub41c \uae30\uc900 \ubaa8\ub378(Llama-3.1-70B-DPO \ubc0f Llama-3.1-70B-Instruct)\uacfc \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac00\uc7a5 \ub192\uc740 \uc131\ub2a5\uacfc \ub450 \ubc88\uc9f8\ub85c \ub192\uc740 \uc131\ub2a5\uc740 \uad75\uc740 \ubc11\uc904\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc73c\ub85c \ubc18\ubcf5 \ud69f\uc218 D\ub294 2\ub85c, \uc0d8\ud50c \uc218 N\uc740 5\ub85c \uc124\uc815\ub429\ub2c8\ub2e4. TPO\uc758 \uc7a0\uc7ac\ub825\uc744 \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud574 \ubc18\ubcf5 \ud69f\uc218\ub97c 5\ub85c, \uc0d8\ud50c \uc218\ub97c 20\uc73c\ub85c \ub298\ub9b0 \ucd08\uace0\uc131\ub2a5 \uc124\uc815 \uacb0\uacfc\ub3c4 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \ubcc4\ud45c(\u22c6)\ub294 \ubcf4\uc0c1 \ubaa8\ub378 FsfairX-LLaMA3-RM-v0.1\uc744 \uc0ac\uc6a9\ud558\uc5ec TPO\ub85c \ucd5c\uc801\ud654\ub41c \ubaa8\ub378\uc744 \ub098\ud0c0\ub0b4\uace0, \u2020\ub294 Llama-3.1-Tulu-3-8B-RM\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "6. Experimental Results"}, {"content": "| Model | AlpacaEval 2 LC(%) | AlpacaEval 2 WR(%) | Arena-Hard | HH-RLHF | BeaverTails | XSTest | MATH-500 |\n|---|---|---|---|---|---|---|---| \n| Llama-3.1-70B-Instruct | 36.9 | 34.9 | 59.0 | -0.5 | -6.4 | 88.7 | 66.4 |\n| w/ TPO (D2-N5) | 39.1 | 48.5 | 69.5 | 1.3 | -3.6 | 89.6 | 71.6 |\n| Mistral-Small-Instruct-2409 | 45.7 | 38.5 | 53.8 | -0.4 | -5.2 | 87.1 | 57.6 |\n| w/ TPO (D2-N5) | 53.4 | 60.5 | 72.2 | 1.1 | -3.4 | 90.7 | 62.2 |", "caption": "Table 2: Benchmark performance of the aligned models (Llama-3.1-70B-Instruct and Mistral-Small-Instruct-2409) with TPO.\nThe bold numbers indicate the best performance.\nThe maximum number of iterations D\ud835\udc37Ditalic_D is set to 2, and the number of samples N\ud835\udc41Nitalic_N is set to 5.\nThe reward model used for TPO is FsfairX-LLaMA3-RM-v0.1.", "description": "\ud45c 2\ub294 TPO\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubbf8\uc138 \uc870\uc815\ub41c Llama-3.1-70B-Instruct \ubc0f Mistral-Small-Instruct-2409 \ubaa8\ub378\uc758 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\uc7a5 \ub192\uc740 \uc131\ub2a5\uc744 \ub2ec\uc131\ud55c \uacb0\uacfc\ub294 \uad75\uc740 \uc22b\uc790\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4. TPO \ubc18\ubcf5 \ud69f\uc218(D)\ub294 2\ub85c, \uc0d8\ud50c \uc218(N)\ub294 5\ub85c \uc124\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. TPO\uc5d0 \uc0ac\uc6a9\ub41c \ubcf4\uc0c1 \ubaa8\ub378\uc740 FsfairX-LLaMA3-RM-v0.1\uc785\ub2c8\ub2e4.", "section": "6.2 \ubca4\uce58\ub9c8\ud06c \uc131\ub2a5"}, {"content": "| AlpacaEval 2 | Arena-Hard | HH-RLHF | BeaverTails | XSTest | MATH-500 |\n|---|---|---|---|---|---| \n| 805 | 500 | 500 | 700 | 450 | 500 |", "caption": "Table 3: Data statistics of benchmark datasets.", "description": "\ubcf8 \ub17c\ubb38\uc758 \ud45c 3\uc740 \ub2e4\uc591\ud55c \uae30\uc900 \ub370\uc774\ud130\uc14b\uc758 \ud1b5\uacc4\uc801 \uc815\ubcf4\ub97c \uc694\uc57d\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub370\uc774\ud130\uc14b \uc774\ub984\uacfc \uac01 \ub370\uc774\ud130\uc14b\uc5d0 \ud3ec\ud568\ub41c \ub370\uc774\ud130 \uc218\ub97c \ubcf4\uc5ec\uc8fc\uc5b4, \uc774\ud6c4 \uc2e4\ud5d8\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc758 \uaddc\ubaa8\uc640 \ubd84\ud3ec\uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ub3d5\uc2b5\ub2c8\ub2e4.  AlpacaEval 2, Arena-Hard, HH-RLHF, BeaverTails, XSTest, MATH-500 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\ub97c \uc218\uce58\uc801\uc73c\ub85c \uc81c\uc2dc\ud558\uc5ec \ud6c4\uc18d \uc5f0\uad6c \ubd84\uc11d\uc5d0 \ud544\uc694\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "5. Experimental Setup"}]