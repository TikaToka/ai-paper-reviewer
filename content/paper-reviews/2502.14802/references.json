{"references": [{"fullname_first_author": "AI@Meta", "paper_title": "Llama 3 model card", "publication_date": "2024-00-00", "reason": "This paper provides the model card for Llama 3, a large language model used extensively in the experiments and baselines of the current work."}, {"fullname_first_author": "Guti\u00e9rrez", "paper_title": "Hipporag: Neurobiologically inspired long-term memory for large language models", "publication_date": "2024-00-00", "reason": "This paper introduces HippoRAG, the foundation upon which the current work, HippoRAG 2, is built; HippoRAG 2 improves upon this prior work."}, {"fullname_first_author": "Lee", "paper_title": "NV-Embed: Improved techniques for training LLMs as generalist embedding models", "publication_date": "2025-00-00", "reason": "This paper introduces NV-Embed-v2, a strong baseline embedding model that is comprehensively compared against in the current work"}, {"fullname_first_author": "Sarthi", "paper_title": "RAPTOR: recursive abstractive processing for tree-organized retrieval", "publication_date": "2024-00-00", "reason": "This paper introduces RAPTOR, a structure-augmented RAG method that is compared against as a baseline in the paper"}, {"fullname_first_author": "Edge", "paper_title": "From local to global: A graph rag approach to query-focused summarization", "publication_date": "2024-00-00", "reason": "This paper introduces GraphRAG, another structure-augmented RAG method that is compared against as a baseline in the paper"}]}