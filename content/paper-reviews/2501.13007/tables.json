[{"content": "| Input | Prompt |\n|---|---|", "caption": "Table 1: Statistics of the datasets before and after filtering.\nAMC-related datasets shrink significantly because most AMC tasks are multiple-choice.", "description": "\ubcf8 \ud45c\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \ud1b5\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud544\ud130\ub9c1 \uc804\uacfc \ud6c4\uc758 \ub370\uc774\ud130\uc14b \ud06c\uae30 \ubc0f AMC \uad00\ub828 \ub370\uc774\ud130\uc14b\uc758 \uac10\uc18c \ud604\ud669\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. AMC \uad00\ub828 \ub370\uc774\ud130\uc14b\uc774 \ud06c\uac8c \uac10\uc18c\ud55c \uc774\uc720\ub294 \ub300\ubd80\ubd84\uc758 AMC \uacfc\uc81c\uac00 \uac1d\uad00\uc2dd\uc774\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub370\uc774\ud130\uc14b \uc774\ub984, \ud544\ud130\ub9c1 \uc804 \ud06c\uae30, \ud544\ud130\ub9c1 \ud6c4 \ud06c\uae30\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc138 \uc5f4\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub370\uc774\ud130 \uc804\ucc98\ub9ac \uacfc\uc815\uc5d0\uc11c \uc5b4\ub5a4 \ub370\uc774\ud130\uc14b\uc774 \uc5b4\ub5bb\uac8c \uc601\ud5a5\uc744 \ubc1b\uc558\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4 PAIRWISE-443K dataset collection"}, {"content": "| Pairwise | Comparison |\n|---|---|", "caption": "Table 2: Different reward models\u2019 best-of-N sampling performance on MATH-500 and Olympiad Bench with three different LLMs: Llama-3.1-8B-Inst, Qwen-2.5-7B-Inst, and Llama-3.1-70B-Inst. The results are reported in terms of accuracy.\nThe pass@1 accuracy of these three LLMs are 42.0, 73.6, and 59.2 on MATH-500, and 12.3, 35.7, and 25.9 on Olympiad Bench, respectively.\n@16, @32, and @64 denote the accuracy with Best-of-16, Best-of-32, and Best-of-64 sampling, respectively.\nThe best results are in bold, and the second-best results are underlined.", "description": "\ud45c 2\ub294 \uc138 \uac00\uc9c0 \ub2e4\ub978 LLMs (Llama-3.1-8B-Inst, Qwen-2.5-7B-Inst, Llama-3.1-70B-Inst)\uc744 \uc0ac\uc6a9\ud558\uc5ec MATH-500 \ubc0f Olympiad Bench \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \ubcf4\uc0c1 \ubaa8\ub378\uc758 Best-of-N \uc0d8\ud50c\ub9c1 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uacb0\uacfc\ub294 \uc815\ud655\ub3c4(accuracy)\ub85c \ubcf4\uace0\ub418\uba70, \uac01 LLM\uc758 Pass@1 \uc815\ud655\ub3c4\ub294 MATH-500\uc5d0\uc11c \uac01\uac01 42.0, 73.6, 59.2\uc774\uace0, Olympiad Bench\uc5d0\uc11c\ub294 \uac01\uac01 12.3, 35.7, 25.9\uc785\ub2c8\ub2e4. @16, @32, @64\ub294 \uac01\uac01 Best-of-16, Best-of-32, Best-of-64 \uc0d8\ud50c\ub9c1\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ucd5c\uace0 \uacb0\uacfc\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uace0, \ub450 \ubc88\uc9f8\ub85c \uc88b\uc740 \uacb0\uacfc\ub294 \ubc11\uc904\uc774 \uadf8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. Results"}, {"content": "| Dataset | Original Count | Filtered Count |\n|---|---|---|\n| AMC/AIME | 4,070 | 289 |\n| AoPS Forum | 30,192 | 9,017 |\n| Chinese K-12 | 276,554 | 63,779 |\n| GSM8K | 7,342 | 6,539 |\n| Math | 7,477 | 5,988 |\n| Olympiads | 150,563 | 52,766 |\n| ORCA Math | 153,314 | 149,550 |\n| Synthetic AMC | 62,108 | 94 |\n| Synthetic Math | 167,874 | 136,921 |\n| **Total** | **859,494** | **425,943** |", "caption": "Table 3: \nComparison of the Pairwise RM and LLM-as-a-Judge on the MATH-500 and Olympiad datasets on correctness verification task.\nCandidates are generated by Qwen-2.5-7B-Instruct.\nAccuracy is reported.", "description": "\uc774 \ud45c\ub294 Pairwise RM\uacfc LLM-as-a-Judge \ubaa8\ub378\uc758 \uc815\ud655\uc131 \uac80\uc99d \uc131\ub2a5\uc744 MATH-500 \ubc0f Olympiad \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Qwen-2.5-7B-Instruct \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c4\ubcf4 \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud588\uc73c\uba70, \uc815\ud655\ub3c4\ub97c \uce21\uc815\ud558\uc5ec \ube44\uad50\ud558\uc600\uc2b5\ub2c8\ub2e4.", "section": "6.1 Comparison on Correctness Verification"}, {"content": "| Type | Reward Model | Llama-3.1-8B-Inst@16 | Llama-3.1-8B-Inst@32 | Llama-3.1-8B-Inst@64 | Qwen-2.5-7B-Inst@16 | Qwen-2.5-7B-Inst@32 | Qwen-2.5-7B-Inst@64 | Llama-3.1-70B-Inst@16 | Llama-3.1-70B-Inst@32 | Llama-3.1-70B-Inst@64 | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| *MATH-500* |  |  |  |  |  |  |  |  |  |  |  |\n| **ORM** | ArmoRM-Llama3-8B | 51.6 | 49.2 | 49.8 | 77.6 | 77.4 | 76.4 | 64.8 | 64.8 | 65.8 | 64.2 |\n|  | SkyworkRM-Llama3.1-8B | 51.4 | 51.0 | 51.0 | 77.6 | 76.4 | 78.0 | 66.4 | 66.6 | 67.4 | 65.1 |\n|  | EurusRM-7B | 55.2 | 53.4 | 53.4 | 76.6 | 77.0 | 77.4 | 68.0 | 66.6 | 67.6 | 66.1 |\n| **PRM** | Math-Shepherd-7B | 49.5 | 50.1 | 49.2 | 74.7 | 75.3 | 75.9 | 63.5 | 62.8 | 63.6 | 62.7 |\n|  | RLHFlow-8B-Mistral-Data | 51.0 | 51.0 | 50.2 | 75.4 | 76.2 | 76.6 | 64.0 | 63.0 | 64.8 | 63.6 |\n|  | RLHFlow-8B-DS-Data | 55.2 | 57.0 | 56.2 | 75.8 | 76.0 | 76.2 | 66.2 | 66.4 | 65.4 | 66.0 |\n|  | RLHFlow-8B-LLaMA-Data | 55.5 | 56.8 | 56.0 | 76.0 | 76.3 | 76.5 | 66.7 | 67.0 | 66.0 | 66.3 |\n|  | Majority Voting | 57.0 | 58.8 | 58.8 | 77.4 | 77.6 | 78.0 | 70.2 | 72.8 | 73.6 | 69.4 |\n|  | Pairwise RM & Knockout | 61.0 | 64.6 | 65.6 | 80.2 | 79.8 | 80.4 | 72.2 | 75.6 | 77.4 | 73.0 |\n| *Olympiad Bench* |  |  |  |  |  |  |  |  |  |  |  |\n| **ORM** | ArmoRM-Llama3-8B | 16.1 | 15.9 | 16.7 | 39.3 | 40.1 | 40.4 | 29.2 | 29.8 | 30.1 | 28.7 |\n|  | SkyworkRM-Llama3.1-8B | 19.9 | 20.0 | 18.7 | 39.9 | 40.0 | 41.0 | 29.8 | 30.4 | 29.8 | 29.4 |\n|  | EurusRM-7B | 20.4 | 19.6 | 20.1 | 37.9 | 39.4 | 39.1 | 30.1 | 30.7 | 32.4 | 30.0 |\n| **PRM** | Math-Shepherd-7B | 15.2 | 13.7 | 13.1 | 34.8 | 34.5 | 35.1 | 25.3 | 26.0 | 24.1 | 24.6 |\n|  | RLHFlow-8B-Mistral-Data | 16.4 | 14.5 | 14.5 | 36.1 | 35.9 | 36.3 | 26.7 | 27.1 | 25.2 | 25.9 |\n|  | RLHFlow-8B-DS-Data | 18.5 | 19.6 | 19.3 | 35.4 | 34.8 | 34.2 | 28.9 | 29.5 | 30.1 | 27.8 |\n|  | RLHFlow-8B-LLaMA-Data | 18.7 | 20.0 | 19.7 | 35.8 | 35.2 | 34.7 | 29.1 | 29.4 | 30.3 | 28.1 |\n|  | Majority Voting | 20.3 | 22.4 | 23.3 | 40.0 | 40.7 | 39.9 | 35.6 | 35.9 | 36.7 | 32.8 |\n|  | Pairwise RM & Knockout | 22.7 | 24.9 | 25.5 | 41.9 | 40.2 | 41.2 | 33.9 | 36.7 | 37.8 | 33.9 |", "caption": "Table 4: \nPrompt Template for Pairwise RM, the {question}, {response_a}, and {response_b} are placeholders for the math question, response A, and response B, respectively.", "description": "\ud45c 4\ub294 Pairwise RM\uc744 \uc704\ud55c \ud504\ub86c\ud504\ud2b8 \ud15c\ud50c\ub9bf\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  {question}, {response_a}, {response_b}\ub294 \uac01\uac01 \uc218\ud559 \ubb38\uc81c, \uc751\ub2f5 A, \uc751\ub2f5 B\ub97c \ub098\ud0c0\ub0b4\ub294 \uc790\ub9ac \ud45c\uc2dc\uc790\uc785\ub2c8\ub2e4. \uc774 \ud15c\ud50c\ub9bf\uc740 \ub450 \uc751\ub2f5\uc758 \uc815\ud655\uc131\uc744 \ud3c9\uac00\ud558\uace0 \uac01 \uc751\ub2f5\uc758 \uc815\ud655\uc131\uc744 \ub2e8\uacc4\ubcc4\ub85c \uac80\uc99d\ud558\ub294 \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4. \ub2e8\uacc4\ubcc4 \uac80\uc99d \ud6c4 \uac01 \uc751\ub2f5\uc5d0 \ub300\ud55c \ucd5c\uc885 \uc815\ud655\uc131 \ud310\ub2e8\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3 Pairwise Reward Model and Knockout"}, {"content": "| Model | MATH | Olympiad | Avg. |\n|---|---|---|---| \n| LLM-as-a-Judge | 67.7 | 56.9 | 62.3 |\n| Pairwise RM | 70.4 | 64.2 | 67.3 |", "caption": "Table 5: Filtering criteria applied to the dataset to remove low-quality, proof-based, or multiple-choice problems.", "description": "\uc774 \ud45c\ub294 \ub0ae\uc740 \ud488\uc9c8, \uc99d\uba85 \uae30\ubc18 \ub610\ub294 \uc120\ud0dd\ud615 \ubb38\uc81c\ub4e4\uc744 \uc81c\uac70\ud558\uae30 \uc704\ud574 \ub370\uc774\ud130\uc14b\uc5d0 \uc801\uc6a9\ub41c \ud544\ud130\ub9c1 \uae30\uc900\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ud544\ud130\ub9c1 \uc720\ud615(\uc798\ubabb\ub41c \ud488\uc9c8\uc758 \ubb38\uc81c, \ub2f5\ubcc0\uc5d0\uc11c \ub4f1\uc2dd, \ub2e4\uc911 \uc9c8\ubb38, \uc608/\uc544\ub2c8\uc624 \uc9c8\ubb38, \ud14d\uc2a4\ud2b8 \ub2f5\ubcc0, \uc99d\uba85 \ubb38\uc81c, \uc120\ud0dd\ud615 \uc9c8\ubb38)\uc5d0 \ub300\ud55c \uad6c\uccb4\uc801\uc778 \uae30\uc900\ub4e4\uc774 \uba85\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub7ec\ud55c \uae30\uc900\ub4e4\uc744 \uc801\uc6a9\ud568\uc73c\ub85c\uc368, \ub370\uc774\ud130\uc14b\uc758 \ud488\uc9c8\uc744 \ub192\uc774\uace0 \ubd84\uc11d\uc758 \uc815\ud655\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \uae30\uc5ec\ud569\ub2c8\ub2e4.", "section": "4 PAIRWISE-443K dataset collection"}]