[{"figure_path": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/overview2.jpg", "caption": "Figure 1: \nWe compare the scientific method when applied to genomics and deep learning model interpretation.\nGrad-CAM (Selvaraju et\u00a0al., 2017) leverages saliency maps to produce hypothetical explanations for model predictions, but provides no natural way to experimentally validate the hypothesis.\nIn comparison, our proposed use of sparse autoencoders (SAEs) for vision models naturally enables experimental validation via feature suppression. See Section\u00a05 for more examples of experimental validation.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \uc720\uc804\uccb4\ud559\uacfc \ub525\ub7ec\ub2dd \ubaa8\ub378 \ud574\uc11d\uc5d0 \uacfc\ud559\uc801 \ubc29\ubc95\ub860\uc744 \uc801\uc6a9\ud55c \uac83\uc744 \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Grad-CAM\uc740 \ubaa8\ub378 \uc608\uce21\uc5d0 \ub300\ud55c \uac00\uc124\uc744 \uc138\uc6b0\ub294 \ub370 \uc720\uc6a9\ud558\uc9c0\ub9cc, \uc2e4\ud5d8\uc801\uc73c\ub85c \uac80\uc99d\ud560 \ubc29\ubc95\uc774 \uc5c6\uc2b5\ub2c8\ub2e4. \ubc18\uba74\uc5d0 \uc81c\uc548\ub41c \ud76c\uc18c \uc624\ud1a0\uc778\ucf54\ub354(SAE) \uae30\ubc18 \ubc29\ubc95\uc740 \ud2b9\uc9d5 \uc5b5\uc81c\ub97c \ud1b5\ud574 \uac00\uc124\uc744 \uc2e4\ud5d8\uc801\uc73c\ub85c \uac80\uc99d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 5\uc808\uc744 \ucc38\uace0\ud558\uc138\uc694.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.06755/x1.png", "caption": "Figure 2: Sparse autoencoders (SAEs) trained on pre-trained ViT activations discover a wide spread of features across both visual patterns and semantic structures. We show eight different features from an SAE trained on ImageNet-1K activations from a CLIP-trained ViT-B/16.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \uc0ac\uc804 \ud6c8\ub828\ub41c \ube44\uc804 \ud2b8\ub79c\uc2a4\ud3ec\uba38(ViT)\uc758 \ud65c\uc131\ud654 \uacb0\uacfc\ub97c \uc774\uc6a9\ud574 \ud6c8\ub828\ub41c \ud76c\uc18c \uc624\ud1a0\uc778\ucf54\ub354(SAE)\uac00 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \ud328\ud134\uacfc \uc758\ubbf8 \uad6c\uc870\uc5d0 \uac78\uccd0 \uad11\ubc94\uc704\ud55c \ud2b9\uc9d5\ub4e4\uc744 \uc5b4\ub5bb\uac8c \ubc1c\uacac\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ImageNet-1K \ub370\uc774\ud130\uc14b\uc73c\ub85c \uc0ac\uc804 \ud6c8\ub828\ub41c CLIP \uae30\ubc18 ViT-B/16 \ubaa8\ub378\uc758 \ud65c\uc131\ud654 \uacb0\uacfc\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub41c SAE\ub85c\ubd80\ud130 \ucd94\ucd9c\ub41c 8\uac00\uc9c0 \uc11c\ub85c \ub2e4\ub978 \ud2b9\uc9d5\ub4e4\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uac01 \ud2b9\uc9d5\uc740 \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0\ub4e4\uc5d0\uc11c \ud2b9\uc9d5\uc774 \ud65c\uc131\ud654\ub418\ub294 \ud328\ud134\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \uc774\ubbf8\uc9c0\ub4e4\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 SAE\uac00 \ub2e8\uc21c\ud55c \uc2dc\uac01\uc801 \ud328\ud134 \ubfd0\ub9cc \uc544\ub2c8\ub77c, \ub354 \ucd94\uc0c1\uc801\uc778 \uc758\ubbf8\ub860\uc801 \uac1c\ub150\uae4c\uc9c0\ub3c4 \ud6a8\uacfc\uc801\uc73c\ub85c \ud559\uc2b5\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 SAE-Enabled Analysis of Vision Models"}, {"figure_path": "https://arxiv.org/html/2502.06755/x2.png", "caption": "Figure 3: Given a picture and a set of highlighted patches, we find exemplar images by (1) getting ViT activations for each patch, (2) computing a sparse representation for each highlighted patch (Eqs.\u00a01 and\u00a02), (3) summing over sparse representations, (4) choosing the top k\ud835\udc58kitalic_k features by activation magnitude and (5) finding existing images that maximize these features.", "description": "\uc774 \uadf8\ub9bc\uc740 \uc0ac\uc6a9\uc790\uac00 \uc774\ubbf8\uc9c0\uc5d0\uc11c \ud2b9\uc815 \uc601\uc5ed(\ud328\uce58)\uc744 \uc120\ud0dd\ud558\uba74, \ud574\ub2f9 \uc601\uc5ed\uc758 \uc2dc\uac01\uc801 \ud2b9\uc9d5\uc744 \ub098\ud0c0\ub0b4\ub294 \ub300\ud45c \uc774\ubbf8\uc9c0\ub4e4\uc744 \ucc3e\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uba3c\uc800, ViT(Vision Transformer)\ub97c \uc774\uc6a9\ud558\uc5ec \uc120\ud0dd\ub41c \ud328\uce58\ub4e4\uc758 \ud65c\uc131\ud654 \ubca1\ud130\ub97c \ucd94\ucd9c\ud569\ub2c8\ub2e4.  \ub2e4\uc74c\uc73c\ub85c, (\uc2dd 1, 2)\uc5d0 \ub530\ub77c \uac01 \ud328\uce58\uc5d0 \ub300\ud55c \ud76c\uc18c \ud45c\ud604(sparse representation)\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4. \uc774\ub807\uac8c \uc5bb\uc740 \uc5ec\ub7ec \ud328\uce58\ub4e4\uc758 \ud76c\uc18c \ud45c\ud604\ub4e4\uc744 \ub354\ud55c \ud6c4, \ud65c\uc131\ud654 \ud06c\uae30\uac00 \uac00\uc7a5 \ud070 \uc0c1\uc704 k\uac1c\uc758 \ud2b9\uc9d5\uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c, \uc120\ud0dd\ub41c \ud2b9\uc9d5\ub4e4\uc744 \ucd5c\ub300\ud654\ud558\ub294 \uae30\uc874 \uc774\ubbf8\uc9c0\ub4e4\uc744 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ucc3e\uc544\uc11c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uc0ac\uc6a9\uc790\ub294 \uc120\ud0dd\ud55c \uc601\uc5ed\uc758 \uc2dc\uac01\uc801 \ud2b9\uc9d5\uc774 \ubaa8\ub378 \ub0b4\ubd80\uc5d0\uc11c \uc5b4\ub5bb\uac8c \ud45c\ud604\ub418\ub294\uc9c0 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.2 \uac00\uc124: SAE\uac00 \uc0dd\uc131\ud55c \uc124\uba85"}, {"figure_path": "https://arxiv.org/html/2502.06755/x3.png", "caption": "Figure 4: \nCLIP learns robust cultural visual features.\nTop Left (a): A \u201cBrazil\u201d feature (CLIP-24K/6909) responds to distinctive Brazilian imagery including Rio de Janeiro\u2019s urban landscape, the national flag, and the iconic sidewalk tile pattern of Copacabana Beach\nTop Right (b): CLIP-24K/6909 does not respond to other South American symbols like Machu Picchu or the Argentinian flag.\nBottom Left (c): We search DINOv2\u2019s SAE for a similar \u201cBrazil\u201d feature and find that DINOv2-24K/9823 fires on Brazilian imagery.\nBottom Right (d): However, maximally activating ImageNet-1K examples for DINOv2-24K/9823 are of lamps, convincing us that DINOv2-24K/9823 does not reliably detect Brazilian cultural symbols.", "description": "\uadf8\ub9bc 4\ub294 CLIP\uacfc DINOv2 \ubaa8\ub378\uc774 \ud559\uc2b5\ud55c \uc2dc\uac01\uc801 \ud2b9\uc9d5\uc758 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. CLIP\uc740 \ube0c\ub77c\uc9c8\uc758 \ub3c4\uc2dc \ud48d\uacbd, \uad6d\uae30, \ucf54\ud30c\uce74\ubc14\ub098 \ud574\ubcc0\uc758 \ubcf4\ub3c4 \ud0c0\uc77c \ud328\ud134 \ub4f1 \ube0c\ub77c\uc9c8 \ubb38\ud654\uc640 \uad00\ub828\ub41c \uc774\ubbf8\uc9c0\uc5d0 \uac15\ud558\uac8c \ubc18\uc751\ud558\ub294 \u2018\ube0c\ub77c\uc9c8\u2019 \ud2b9\uc9d5(CLIP-24K/6909)\uc744 \ud559\uc2b5\ud588\uc2b5\ub2c8\ub2e4. \ubc18\uba74 \ub9c8\ucd94\ud53d\ucd94\ub098 \uc544\ub974\ud5e8\ud2f0\ub098 \uad6d\uae30\uc640 \uac19\uc740 \ub2e4\ub978 \ub0a8\ubbf8 \uc774\ubbf8\uc9c0\uc5d0\ub294 \ubc18\uc751\ud558\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4. DINOv2\uc758 SAE\uc5d0\uc11c \ube44\uc2b7\ud55c \u2018\ube0c\ub77c\uc9c8\u2019 \ud2b9\uc9d5\uc744 \ucc3e\uc544\ubcf4\uba74 DINOv2-24K/9823\uc774 \ube0c\ub77c\uc9c8 \uc774\ubbf8\uc9c0\uc5d0 \ubc18\uc751\ud558\uc9c0\ub9cc, ImageNet-1K \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc774 \ud2b9\uc9d5\uc744 \ucd5c\ub300\ub85c \ud65c\uc131\ud654\ud558\ub294 \uc774\ubbf8\uc9c0\ub294 \ub7a8\ud504 \uc774\ubbf8\uc9c0\uc600\uc2b5\ub2c8\ub2e4. \uc774\ub294 DINOv2-24K/9823\uc774 \ube0c\ub77c\uc9c8 \ubb38\ud654\uc801 \uc0c1\uc9d5\uc744 \uc548\uc815\uc801\uc73c\ub85c \uac10\uc9c0\ud558\uc9c0 \ubabb\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  \uc989, CLIP\uc740 \ubb38\ud654\uc801 \uc2dc\uac01\uc801 \ud2b9\uc9d5\uc744 \uc798 \ud559\uc2b5\ud558\uc9c0\ub9cc, DINOv2\ub294 \uadf8\ub807\uc9c0 \ubabb\ud558\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 Language Supervision Enables Cultural Understanding"}, {"figure_path": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/classification-extra1.jpg", "caption": "Figure 5: \nCLIP learns unified representations of abstract concepts that persist across visual styles.\nHighlighted patches indicate feature activation strength.\nUpper Left: We find a CLIP SAE feature (CLIP-24K/20652) that consistently activates on \u201caccidents\u201d or \u201ccrashes\u201d: car accidents, plane crashes, cartoon depictions of crashes and generally damaged metal.\nUpper Right: Two exemplar images from ImageNet-1K for feature CLIP-24K/20652.\nLower Left: We probe an SAE trained on DINOv2 activations. DINOv2-24K/9762 is the closest feature, but does not reliably fire on all the examples.\nLower Right: Two exemplar images from ImageNet-1K for feature DINOv2-24K/9762 clarifies that it does not match the semantic concept of \u201ccrash.\u201d", "description": "\uadf8\ub9bc 5\ub294 CLIP\uc774 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc2a4\ud0c0\uc77c\uc5d0\uc11c \uc9c0\uc18d\ub418\ub294 \ucd94\uc0c1\uc801 \uac1c\ub150\uc758 \ud1b5\ud569\ub41c \ud45c\ud604\uc744 \ud559\uc2b5\ud55c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac15\uc870 \ud45c\uc2dc\ub41c \ud328\uce58\ub294 \ud2b9\uc9d5 \ud65c\uc131\ud654 \uac15\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc67c\ucabd \uc0c1\ub2e8\uc5d0\ub294 \"\uc0ac\uace0\" \ub610\ub294 \"\ucda9\ub3cc\"\uc5d0 \uc77c\uad00\ub418\uac8c \ud65c\uc131\ud654\ub418\ub294 CLIP SAE \ud2b9\uc9d5(CLIP-24K/20652)\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\ub294 \uc790\ub3d9\ucc28 \uc0ac\uace0, \ube44\ud589\uae30 \ucd94\ub77d, \ub9cc\ud654\uc5d0\uc11c \ubb18\uc0ac\ub41c \ucda9\ub3cc, \uadf8\ub9ac\uace0 \uc77c\ubc18\uc801\uc73c\ub85c \uc190\uc0c1\ub41c \uae08\uc18d \ub4f1\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4. \uc624\ub978\ucabd \uc0c1\ub2e8\uc5d0\ub294 CLIP-24K/20652 \ud2b9\uc9d5\uc5d0 \ub300\ud55c ImageNet-1K\uc758 \ub450 \uac00\uc9c0 \uc608\uc2dc \uc774\ubbf8\uc9c0\uac00 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4. \uc67c\ucabd \ud558\ub2e8\uc5d0\ub294 DINOv2 \ud65c\uc131\ud654\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub41c SAE\ub97c \uc870\uc0ac\ud569\ub2c8\ub2e4. DINOv2-24K/9762\uac00 \uac00\uc7a5 \uac00\uae4c\uc6b4 \ud2b9\uc9d5\uc774\uc9c0\ub9cc, \ubaa8\ub4e0 \uc608\uc2dc\uc5d0\uc11c \uc548\uc815\uc801\uc73c\ub85c \uc791\ub3d9\ud558\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4. \uc624\ub978\ucabd \ud558\ub2e8\uc5d0\ub294 DINOv2-24K/9762 \ud2b9\uc9d5\uc5d0 \ub300\ud55c ImageNet-1K\uc758 \ub450 \uac00\uc9c0 \uc608\uc2dc \uc774\ubbf8\uc9c0\uac00 \ub098\uc640 \uc788\uc73c\uba70, \uc774\ub294 \"\ucda9\ub3cc\"\uc774\ub77c\ub294 \uc758\ubbf8\uc801 \uac1c\ub150\uacfc \uc77c\uce58\ud558\uc9c0 \uc54a\uc74c\uc744 \uba85\ud655\ud788 \ud569\ub2c8\ub2e4.", "section": "4 Language Supervision Enables Cultural Understanding"}, {"figure_path": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/classification-extra2.jpg", "caption": "Figure 6: \nDemonstrating the scientific method for understanding vision model behavior using sparse autoencoders (SAEs).\nLeft: We observe that CLIP predicts \u201cBlue Jay.\u201d\nUpper Middle: We select the bird\u2019s wing in the input image; the SAE proposes a hypothesis that the most salient feature is \u201cblue feathers\u201d via exemplar images.\nLower Middle: We validate this hypothesis through controlled intervention by suppressing the identified \u201cblue feathers\u201d feature in the model\u2019s activation space.\nRight: we observe a change in behavior: the predicted class shifts away from \u201cBlue Jay\u201d towards \u201cClark Nutcracker\u201d, a similar bird besides the lack of blue plumage.\nThis three-step process of observation, hypothesis formation, and experimental validation enables systematic investigation of how vision models process visual information.", "description": "\uadf8\ub9bc 6\uc740 \ud76c\uc18c \uc624\ud1a0\uc778\ucf54\ub354(SAE)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\uc804 \ubaa8\ub378\uc758 \ub3d9\uc791\uc744 \uc774\ud574\ud558\uae30 \uc704\ud55c \uacfc\ud559\uc801 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc5d0\ub294 CLIP\uc774 '\ud478\ub978 \uc5b4\uce58'\ub97c \uc608\uce21\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\uc6b4\ub370 \uc704\ucabd\uc5d0\ub294 \uc785\ub825 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uc0c8\uc758 \ub0a0\uac1c\ub97c \uc120\ud0dd\ud558\uace0 SAE\uac00 \uc608\uc2dc \uc774\ubbf8\uc9c0\ub97c \ud1b5\ud574 \uac00\uc7a5 \uc911\uc694\ud55c \ud2b9\uc9d5\uc774 '\ud478\ub978 \uae43\ud138'\uc774\ub77c\ub294 \uac00\uc124\uc744 \uc81c\uc2dc\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\uc6b4\ub370 \uc544\ub798\ucabd\uc5d0\uc11c\ub294 \uc2dd\ubcc4\ub41c '\ud478\ub978 \uae43\ud138' \ud2b9\uc9d5\uc744 \ubaa8\ub378\uc758 \ud65c\uc131\ud654 \uacf5\uac04\uc5d0\uc11c \uc5b5\uc81c\ud558\uc5ec \uc774 \uac00\uc124\uc744 \uac80\uc99d\ud558\ub294 \uc81c\uc5b4\ub41c \uac1c\uc785\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc624\ub978\ucabd\uc5d0\ub294 '\ud478\ub978 \uc5b4\uce58'\uc5d0\uc11c '\ud074\ub77d \uc7a3\uae4c\ub9c8\uadc0'(\ud478\ub978 \uae43\ud138\uc774 \uc5c6\ub294 \uc720\uc0ac\ud55c \uc885)\ub85c \uc608\uce21 \ud074\ub798\uc2a4\uac00 \uc774\ub3d9\ud558\ub294 \ud589\ub3d9 \ubcc0\ud654\ub97c \uad00\ucc30\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uad00\ucc30, \uac00\uc124 \ud615\uc131 \ubc0f \uc2e4\ud5d8\uc801 \uac80\uc99d\uc758 3\ub2e8\uacc4 \uacfc\uc815\uc744 \ud1b5\ud574 \ube44\uc804 \ubaa8\ub378\uc774 \uc2dc\uac01 \uc815\ubcf4\ub97c \ucc98\ub9ac\ud558\ub294 \ubc29\uc2dd\uc744 \uccb4\uacc4\uc801\uc73c\ub85c \uc870\uc0ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5 Validating Hypotheses of Vision Model Behavior"}, {"figure_path": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/classification-extra3.jpg", "caption": "Figure 7: Far Left: We train a linear head to predict semantic segmentation classes for each patch.\nMiddle Left: We choose all sand-filled patches in the input image to inspect.\nMiddle Right: Our SAE proposes exemplar images for the maximally activating sparse dimensions, as in Section\u00a05.1, suggesting that DINOv2 is learning a sand feature.\nFar Right: We suppress the sand feature in not just the selected patches, but all patches. We modify all activation vectors and pass them to DINOv2\u2019s final transformer layer followed by our trained linear segmentation head. We see that the head predicts \u201cearth, ground\u201d and \u201cwater\u201d for the former sand patches. Both classes are good second choices if \u201csand\u201d is unavailable. Notably, other patches are not meaningfully affected, demonstrating the pseudo-orthogonality of the SAE\u2019s learned feature vectors.", "description": "\uadf8\ub9bc 7\uc740 DINOv2 \ubaa8\ub378\uc758 \ubaa8\ub798 \ud2b9\uc9d5(sand feature)\uc744 \uc81c\uac70\ud588\uc744 \ub54c\uc758 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uba3c\uc800, \uac01 \ud328\uce58\uc5d0 \ub300\ud55c \uc758\ubbf8\ub860\uc801 \ubd84\ud560 \ud074\ub798\uc2a4(semantic segmentation classes)\ub97c \uc608\uce21\ud558\ub294 \uc120\ud615 \ubd84\ub958\uae30\ub97c \ud559\uc2b5\uc2dc\ud0b5\ub2c8\ub2e4. \uadf8\ub9bc\uc758 \uc67c\ucabd\uc5d0\uc11c \ub450 \ubc88\uc9f8 \ubd80\ubd84\uc740 \uc785\ub825 \uc774\ubbf8\uc9c0\uc5d0\uc11c \ubaa8\ub798\ub85c \ucc44\uc6cc\uc9c4 \ubaa8\ub4e0 \ud328\uce58\ub97c \uc120\ud0dd\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\uc6b4\ub370 \uc624\ub978\ucabd \ubd80\ubd84\uc740 \ucd5c\ub300 \ud65c\uc131\ud654\ub41c \ud76c\uc18c \ucc28\uc6d0(maximally activating sparse dimensions)\uc5d0 \ub300\ud55c \uc608\uc2dc \uc774\ubbf8\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 SAE\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 DINOv2\uac00 \ubaa8\ub798 \ud2b9\uc9d5\uc744 \ud559\uc2b5\ud558\uace0 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc758 \uc624\ub978\ucabd \ubd80\ubd84\uc740 \uc120\ud0dd\ub41c \ud328\uce58\ubfd0\ub9cc \uc544\ub2c8\ub77c \ubaa8\ub4e0 \ud328\uce58\uc5d0\uc11c \ubaa8\ub798 \ud2b9\uc9d5\uc744 \uc81c\uac70\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \ud65c\uc131\ud654 \ubca1\ud130\ub97c \uc218\uc815\ud558\uc5ec DINOv2\uc758 \ucd5c\uc885 \ubcc0\uc555\uae30 \uacc4\uce35(final transformer layer)\uacfc \ud559\uc2b5\ub41c \uc120\ud615 \ubd84\ud560 \ud5e4\ub4dc\uc5d0 \uc804\ub2ec\ud569\ub2c8\ub2e4. \uadf8 \uacb0\uacfc \uc774\uc804 \ubaa8\ub798 \ud328\uce58\uc5d0 \ub300\ud574 '\ub545(earth)', '\uc9c0\uba74(ground)', '\ubb3c(water)'\uc744 \uc608\uce21\ud558\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. '\ubaa8\ub798'\uac00 \uc5c6\uc744 \uacbd\uc6b0 \ub450 \ubc88\uc9f8\ub85c \uc801\uc808\ud55c \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4. \ud2b9\ud788 \ub2e4\ub978 \ud328\uce58\ub294 \uc758\ubbf8 \uc788\uac8c \uc601\ud5a5\uc744 \ubc1b\uc9c0 \uc54a\uc544 SAE\uc758 \ud559\uc2b5\ub41c \ud2b9\uc9d5 \ubca1\ud130\uc758 \uc758\uc0ac \uc9c1\uad50\uc131(pseudo-orthogonality)\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.2 \uc758\ubbf8\ub860\uc801 \ubd84\ud560(Semantic Segmentation)"}, {"figure_path": "https://arxiv.org/html/2502.06755/x4.png", "caption": "Figure B1: \nAdditional examples of cultural features learned by CLIP.\nTop: CLIP-24K/7622 responds to symbolism from the United States of America, including a portrait of George Washington, but not to a portrait of King Louis XIV of France.\nBottom: CLIP-24K/13871 activates strongly on the Brandenburg Gate and other German symbols, but not on visually similar flags like the Belgian flag.", "description": "\uadf8\ub9bc B1\uc740 CLIP\uc774 \ud559\uc2b5\ud55c \ubb38\ud654\uc801 \ud2b9\uc9d5\uc758 \ucd94\uac00\uc801\uc778 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc717\ubd80\ubd84\uc740 CLIP-24K/7622\uac00 \uc870\uc9c0 \uc6cc\uc2f1\ud134 \ucd08\uc0c1\ud654\ub97c \ud3ec\ud568\ud55c \ubbf8\uad6d \uad00\ub828 \uc0c1\uc9d5\ubb3c\uc5d0 \ubc18\uc751\ud558\uc9c0\ub9cc, \ud504\ub791\uc2a4 \ub8e8\uc774 14\uc138 \ucd08\uc0c1\ud654\uc5d0\ub294 \ubc18\uc751\ud558\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc544\ub7ab\ubd80\ubd84\uc740 CLIP-24K/13871\uc774 \ube0c\ub780\ub374\ubd80\ub974\ud06c \ubb38\uacfc \uae30\ud0c0 \ub3c5\uc77c \uc0c1\uc9d5\ubb3c\uc5d0 \uac15\ud558\uac8c \ubc18\uc751\ud558\uc9c0\ub9cc, \ubca8\uae30\uc5d0 \uad6d\uae30\uc640 \uac19\uc740 \uc2dc\uac01\uc801\uc73c\ub85c \uc720\uc0ac\ud55c \uad6d\uae30\uc5d0 \ub300\ud574\uc11c\ub294 \ubc18\uc751\ud558\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 CLIP\uc774 \ub2e8\uc21c\ud788 \uc2dc\uac01\uc801 \uc720\uc0ac\uc131\uc774 \uc544\ub2cc \ubb38\ud654\uc801 \ub9e5\ub77d\uc744 \uace0\ub824\ud558\uc5ec \ud2b9\uc9d5\uc744 \ud559\uc2b5\ud55c\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.1 \uc5b8\uc5b4\uc801 \uac10\ub3c5\uc740 \ubb38\ud654\uc801 \uc774\ud574\ub97c \uac00\ub2a5\ud558\uac8c \ud55c\ub2e4"}, {"figure_path": "https://arxiv.org/html/2502.06755/x5.png", "caption": "Figure C2: Tropical Kingbirds have a distinctive yellow chest. When we suppress a \u201cyellow feathers\u201d feature, our linear classifier predicts Gray Kingbird, a similar species but with a gray chest. This example is available at https://osu-nlp-group.github.io/SAE-V/demos/classification?example=5099", "description": "\uc774 \uadf8\ub9bc\uc740 \uc5f4\ub300 \ubc8c\uc0c8\ub958\uc758 \ud2b9\uc9d5\uc778 \ub178\ub780\uc0c9 \uac00\uc2b4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \"\ub178\ub780 \uae43\ud138\" \ud2b9\uc9d5\uc744 \uc81c\uac70\ud588\uc744 \ub54c, \uc120\ud615 \ubd84\ub958\uae30\ub294 \ud68c\uc0c9 \uac00\uc2b4\uc744 \uac00\uc9c4 \ube44\uc2b7\ud55c \uc885\uc778 \ud68c\uc0c9 \ubc8c\uc0c8\ub958\ub97c \uc608\uce21\ud569\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc774 \uc2dc\uac01\uc801 \ud2b9\uc9d5\uc744 \uc5b4\ub5bb\uac8c \ucc98\ub9ac\ud558\uace0 \ud574\uc11d\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.  \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 https://osu-nlp-group.github.io/SAE-V/demos/classification?example=5099 \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.1 \uc774\ubbf8\uc9c0 \ubd84\ub958"}, {"figure_path": "https://arxiv.org/html/2502.06755/x6.png", "caption": "Figure C3: Canada Warblers have a distinctive black necklace on the chest. CLIP-24K/20376 fires on similar patterns; when we suppress this feature, the linear classifier predicts Wilson Warbler, a similar species without the distinctive black necklace. This example is available at https://osu-nlp-group.github.io/SAE-V/demos/classification?example=1129", "description": "\uadf8\ub9bc C3\ub294 \uce90\ub098\ub2e4 \uc6cc\ube14\ub7ec(Canada Warbler)\uac00 \uac00\uc2b4\uc5d0 \ub3c5\ud2b9\ud55c \uac80\uc740\uc0c9 \ubaa9\uac78\uc774 \ubb34\ub2ac\ub97c \uac00\uc9c0\uace0 \uc788\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. CLIP-24K/20376\uc740 \uc720\uc0ac\ud55c \ud328\ud134\uc5d0 \ubc18\uc751\ud558\ub294\ub370, \uc774 \ud2b9\uc9d5\uc744 \uc5b5\uc81c\ud558\uba74 \uc120\ud615 \ubd84\ub958\uae30\ub294 \ub3c5\ud2b9\ud55c \uac80\uc740\uc0c9 \ubaa9\uac78\uc774 \ubb34\ub2ac\uac00 \uc5c6\ub294 \uc720\uc0ac\ud55c \uc885\uc778 \uc70c\uc2a8 \uc6cc\ube14\ub7ec(Wilson Warbler)\ub97c \uc608\uce21\ud569\ub2c8\ub2e4. \uc774 \uc608\uc2dc\ub294 https://osu-nlp-group.github.io/SAE-V/demos/classification?example=1129 \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.1 \uc774\ubbf8\uc9c0 \ubd84\ub958"}, {"figure_path": "https://arxiv.org/html/2502.06755/x7.png", "caption": "Figure C4: Purple finches have bright red coloration on the head and neck area; when we suppress CLIP-24K/10273, which appears to be a \u201cred feathers\u201d feature, our classifier predicts Field Sparrow, which has similar wing banding but no red coloration.\nThis example is available at https://osu-nlp-group.github.io/SAE-V/demos/classification?example=4139", "description": "\uadf8\ub9bc C4\ub294 \ubd89\uc740\uc0c9 \uae43\ud138\uc774 \ud2b9\uc9d5\uc778 \uc790\uc8fc\uc0c9 \ud540\uce58\uc758 \uc774\ubbf8\uc9c0\uc640, \ud574\ub2f9 \ud2b9\uc9d5\uc744 \uc81c\uac70\ud588\uc744 \ub54c\uc758 \ubd84\ub958 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  CLIP-24K/10273\uc774\ub77c\ub294 \ud2b9\uc9d5 \ubca1\ud130\ub294 \ubd89\uc740\uc0c9 \uae43\ud138\uc744 \ub098\ud0c0\ub0b4\ub294 \uac83\uc73c\ub85c \ucd94\uc815\ub418\uba70, \uc774\ub97c \uc5b5\uc81c\ud588\uc744 \ub54c \ubaa8\ub378\uc740 \ube44\uc2b7\ud55c \ub0a0\uac1c \ubb34\ub2ac\ub97c \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc \ubd89\uc740\uc0c9 \uae43\ud138\uc774 \uc5c6\ub294 \ub4e4\uc0c8\uc778 \ub4e4\ucc38\uc0c8\ub97c \uc608\uce21\ud569\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc774 \ubd89\uc740\uc0c9 \uae43\ud138\uc774\ub77c\ub294 \uc2dc\uac01\uc801 \ud2b9\uc9d5\uc744 \uc774\uc6a9\ud558\uc5ec \uc790\uc8fc\uc0c9 \ud540\uce58\ub97c \uc2dd\ubcc4\ud55c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\uc785\ub2c8\ub2e4.  \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 https://osu-nlp-group.github.io/SAE-V/demos/classification?example=4139 \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.1 \uc774\ubbf8\uc9c0 \ubd84\ub958"}]