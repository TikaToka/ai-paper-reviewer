{"references": [{"fullname_first_author": "Bau", "paper_title": "Network dissection: Quantifying interpretability of deep visual representations", "publication_date": "2017-00-00", "reason": "This paper is foundational for the field of network interpretability, and its methodology is directly relevant to the current research."}, {"fullname_first_author": "Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "CLIP is a major advancement in vision and language models, and is used extensively as a baseline model for this research."}, {"fullname_first_author": "Makhzani", "paper_title": "K-sparse autoencoders", "publication_date": "2013-00-00", "reason": "Sparse autoencoders are the core methodological contribution of this paper, and this is the foundational paper on the subject."}, {"fullname_first_author": "Oquab", "paper_title": "Dinov2: Learning robust visual features without supervision", "publication_date": "2023-00-00", "reason": "DINOv2 is a state-of-the-art self-supervised vision model used as a comparison model, with results having major implications for the analysis presented in this paper."}, {"fullname_first_author": "Templeton", "paper_title": "Scaling monosemanticity: Extracting interpretable features from Claude 3 sonnet", "publication_date": "2024-00-00", "reason": "This paper provides crucial methodological advances used in this research on the interpretability of vision models, specifically on the training and use of sparse autoencoders."}]}