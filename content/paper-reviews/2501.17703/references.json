{"references": [{"fullname_first_author": "Achiam, J.", "paper_title": "Gpt-4 technical report", "publication_date": "2023-03-08", "reason": "This paper introduces GPT-4, a powerful language model used extensively in the current research for data generation and evaluation."}, {"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This is a foundational paper on supervised fine-tuning (SFT), a core technique compared against in this paper's proposed CFT method."}, {"fullname_first_author": "Wang, Y.", "paper_title": "Self-Instruct: Aligning language models with self-generated instructions", "publication_date": "2023-07-01", "reason": "This paper introduces the Self-Instruct method, a key data generation technique used as a baseline for comparison with the paper's CFT approach."}, {"fullname_first_author": "Yue, X.", "paper_title": "MAMmoth2: Scaling instructions from the web", "publication_date": "2024-12-01", "reason": "This paper provides the WebInstruct dataset, a crucial resource used for training and evaluation in this paper, establishing a strong connection and relevance."}, {"fullname_first_author": "Yang, A.", "paper_title": "Qwen2. 5 technical report", "publication_date": "2024-12-01", "reason": "This paper introduces the Qwen2.5 language model, which serves as one of the main base models for the experiments in this paper, making it an essential reference for understanding the results."}]}