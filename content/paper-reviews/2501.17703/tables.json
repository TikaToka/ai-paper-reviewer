[{"content": "| Dataset | Size | Source or Seed | Discipline |\n|---|---|---|---|\n| **Supervised Fine-Tuning Data** |  |  |  |\n| WizardMath | 96K | GSM8K, MATH | Math |\n| MathInstruct | 260K | GSM8K, MATH, etc | Math |\n| MetaMathQA | 395K | GSM8K, MATH | Math |\n| XwinMath | 1.4M | GSM8K, MATH | Math |\n| OrcaMath | 200K | GSM8K | Math |\n| NuminaMath | 860K | GSM8K, MATH, AIME | Math |\n| AceMath | 1.6M | GSM8K, MATH, AIME | Math |\n| OpenMath-2 | 14M | GSM8K, MATH | Math |\n| **Critique Fine-Tuning Data (Ours)** |  |  |  |\n| CFT | 50K | WebInstruct | STEM |\n| CFT-tiny | 4K | WebInstruct | STEM |", "caption": "Table 1: The comparison of MAmmoTH3 vs other SFT and RL models including WizardMath\u00a0(Luo et\u00a0al., 2023), MathInstruct\u00a0(Yue et\u00a0al., 2024a), MetaMathQA\u00a0(Yu et\u00a0al., 2024), XWinMath\u00a0(Li et\u00a0al., 2024a), OrcaMath\u00a0(Mitra et\u00a0al., 2024), NuminaMath\u00a0(Li et\u00a0al., 2024b), AceMath\u00a0(Liu et\u00a0al., 2024), OpenMathInsstruct-2\u00a0(Toshniwal et\u00a0al., 2024) and Qwen2.5-Math\u00a0(Yang et\u00a0al., 2024c).", "description": "\ud45c 1\uc740 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ubc29\ubc95(CFT)\uc758 \ud6a8\uacfc\ub97c \uac80\uc99d\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc744 \uae30\uc874\uc758 SFT \ubc0f \uac15\ud654\ud559\uc2b5(RL) \uae30\ubc18 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  Mammoth3\ub97c \ud3ec\ud568\ud558\uc5ec WizardMath, MathInstruct, MetaMathQA, XWinMath, OrcaMath, NuminaMath, AceMath, OpenMathInstruc-2, Qwen2.5-Math \ub4f1 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \ub370\uc774\ud130\uc14b \ud06c\uae30, \ub370\uc774\ud130 \uc18c\uc2a4, \uadf8\ub9ac\uace0 \ud574\ub2f9 \ubaa8\ub378\uc774 \uc8fc\ub85c \ub2e4\ub8e8\ub294 \ud559\ubb38 \ubd84\uc57c(\uc8fc\ub85c \uc218\ud559)\ub97c \ube44\uad50\ud558\uc5ec CFT \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5(\ub2e4\uc591\ud55c \uc8fc\uc81c\ub97c \ub2e4\ub8e8\uace0 \ub370\uc774\ud130 \ud06c\uae30\uac00 \uc0c1\ub300\uc801\uc73c\ub85c \uc791\uc74c)\uc744 \ubd80\uac01\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2. Method & Dataset"}, {"content": "| Model | Method | MATH | Minerva-Math | GSM8K | OlympiadBench | AIME24 | AMC23 | AVG |\n|---|---|---|---|---|---|---|---|---|\n| DeepSeek-Math-7B | Base | 33.8 | 9.2 | 64.3 | 4.5 | 0.0 | 10.0 | 20.3 |\n|  | WebInstruct-SFT | 26.3 | 12.1 | 34.7 | 6.2 | 0.0 | 17.5 | 16.1 |\n|  | WebInstruct-verified-SFT | 35.8 | 10.7 | 67.5 | 9.3 | 0.0 | 7.5 | 21.8 |\n|  | WebInstruct-GPT4o-SFT | 31.7 | 11.8 | 70.9 | 8.9 | 3.3 | 17.5 | 24.0 |\n|  | WebInstruct-CFT | 42.2 | 12.5 | 74.5 | 12.4 | 3.3 | 20.0 | 27.5 |\n|  | \u0394 = CFT- SFT<sub>best</sub> | 6.4 | 0.4 | 3.6 | 3.1 | 0.0 | 2.5 | 3.5 |\n| Qwen2.5-7B | Base | 49.8 | 15.1 | 85.4 | 26.3 | 10.0 | 37.5 | 37.4 |\n|  | WebInstruct-SFT | 30.8 | 6.6 | 59.5 | 5.8 | 3.3 | 15.0 | 20.2 |\n|  | WebInstruct-verified-SFT | 61.5 | 16.2 | 70.8 | 30.1 | 13.3 | 37.5 | 38.2 |\n|  | WebInstruct-GPT4o-SFT | 45.5 | 18.4 | 77.4 | 19.7 | 10.0 | 50.0 | 36.8 |\n|  | WebInstruct-CFT | 71.1 | 27.9 | 88.8 | 35.7 | 13.3 | 55.0 | 48.6 |\n|  | \u0394 = CFT- SFT<sub>best</sub> | 9.6 | 9.5 | 11.4 | 5.6 | 0.0 | 5.0 | 10.4 |\n| Qwen2.5-Math-7B | Base | 55.4 | 13.6 | 91.6 | 16.1 | 10.0 | 40.0 | 37.8 |\n|  | WebInstruct-SFT | 59.0 | 13.2 | 77.4 | 19.9 | 3.3 | 37.5 | 35.1 |\n|  | WebInstruct-verified-SFT | 62.0 | 12.5 | 78.8 | 22.1 | 16.7 | 50.0 | 40.4 |\n|  | WebInstruct-GPT4o-SFT | 73.2 | 25.7 | 90.0 | 37.6 | 13.3 | 62.5 | 50.4 |\n|  | WebInstruct-CFT | 79.4 | 36.8 | 90.9 | 41.6 | 20.0 | 67.5 | 56.0 |\n|  | \u0394 = CFT- SFT<sub>best</sub> | 6.2 | 11.1 | 0.9 | 4.0 | 3.3 | 5.0 | 5.7 |", "caption": "Table 2: Performance comparison of SFT and CFT on different base models. All the experiments are trained with WebInstruct subset. We select the checkpoint with highest validation score and report their results.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \uae30\ubcf8 \ubaa8\ub378\uc5d0 \ub300\ud55c SFT(Supervised Fine-Tuning)\uc640 CFT(Critique Fine-Tuning)\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub4e0 \uc2e4\ud5d8\uc740 WebInstruct \ud558\uc704 \uc9d1\ud569\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc9c4\ud589\ub418\uc5c8\uc73c\uba70, \uac80\uc99d \uc810\uc218\uac00 \uac00\uc7a5 \ub192\uc740 \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uc120\ud0dd\ud558\uc5ec \uacb0\uacfc\ub97c \ubcf4\uace0\ud569\ub2c8\ub2e4.  MATH, Minerva-Math, GSM8K, OlympiadBench, AIME24, AMC23\uc758 \uc5ec\uc12f \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \uce21\uc815\ud558\uc5ec \ud3c9\uade0 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. DeepSeek-Math-7B, Qwen2.5-7B, Qwen2.5-Math-7B \uc138 \uac00\uc9c0 \ubaa8\ub378\uc5d0 \ub300\ud574 SFT\uc640 CFT\ub97c \uc801\uc6a9\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. \uc2e4\ud5d8"}, {"content": "| Model | #Data | MATH | GPQA | TheoremQA | MMLU-Pro | OlympiadBench | AIME24 | AMC23 | AVG |\n|---|---|---|---|---|---|---|---|---|---| \n| Frontier Models |  |  |  |  |  |  |  |  |  |\n| GPT-4o (2024-08-06) | - | 81.1 | 51.6 | 54.7 | 74.7 | 43.3 | 9.3 | 47.5 | 51.7 |\n| GPT-o1-mini | - | 90.0 | 60.0 | 57.2 | 80.3 | 65.3 | 56.7 | 95.0 | 72.1 |\n| Other Open-sourced Reasoning LLMs |  |  |  |  |  |  |  |  |  |\n| Deepseek-Math-7B-Instruct | - | 44.3 | 31.8 | 23.7 | 35.3 | 13.6 | 3.3 | 15.0 | 23.9 |\n| Mathstral-7B-v0.1 | - | 56.6 | 32.2 | 28.4 | 42.5 | 21.5 | 6.7 | 42.4 | 32.9 |\n| NuminaMath-7B-CoT | - | 55.2 | 30.6 | 28.6 | 38.6 | 19.9 | 6.7 | 30.0 | 29.9 |\n| Llama-3.1-8B-Instruct | - | 51.9 | 30.4 | 30.3 | 48.3 | 14.4 | 6.7 | 30.0 | 30.3 |\n| Llama-3.1-70B-Instruct | - | 65.7 | 42.2 | 51.3 | 62.8 | 14.4 | 16.7 | 30.0 | 40.4 |\n| NuminaMath-72B-CoT | - | 68.0 | 35.3 | 24.9 | 55.0 | 35.0 | 3.3 | 52.5 | 39.1 |\n| Qwen2.5-Math-72B-Instruct | - | 85.9 | 49.0 | 50.3 | 60.3 | 49.0 | 30.0 | 70.0 | 56.4 |\n| Initialized from Qwen2.5-Math-7B-Base |  |  |  |  |  |  |  |  |  |\n| Qwen2.5-Math-Base | 0 | 55.4 | 31.0 | 37.4 | 39.3 | 16.1 | 10.0 | 40.0 | 32.7 |\n| Eurus-2-SFT | 230K | 62.4 | 32.1 | 38.0 | 44.2 | 29.8 | 3.3 | 30.1 | 34.3 |\n| rStar-Math@Greedy | 747K | 78.4 | - | - | - | 47.1 | 26.7 | 47.5 | - |\n| AceMath-Qwen2.5-Math | 2.3M | 83.1 | 26.1 | 24.6 | 48.1 | 42.2 | 16.7 | 60.0 | 43.0 |\n| Qwen2.5-Math-7B-Instruct | 2.5M | 83.6 | 31.1 | 37.0 | 39.5 | 41.6 | 16.7 | 62.5 | 44.6 |\n| Qwen2.5-Math-7B-CFT | 50K | 79.4 | 39.4 | 40.4 | 47.5 | 41.6 | 20.0 | 67.5 | 48.0 |", "caption": "Table 3: Performance comparison of our models vs. other reasoning-specialized models. #Data means the total training set size, but we select the checkpoint with highest validation score.", "description": "\ud45c 3\uc740 \uc81c\uc2dc\ub41c \ubaa8\ub378\uacfc \ub2e4\ub978 \ucd94\ub860 \uc804\ubb38 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub370\uc774\ud130 \uc218\ub294 \uc804\uccb4 \ud559\uc2b5 \ub370\uc774\ud130 \ud06c\uae30\ub97c \ub098\ud0c0\ub0b4\uc9c0\ub9cc, \ucd5c\uace0 \uac80\uc99d \uc810\uc218\ub97c \uac00\uc9c4 \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uc120\ud0dd\ud558\uc5ec \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \ubaa8\ub378\uc758 \uc131\ub2a5\uc740 MATH, GPQA, TheoremQA, MMLU-Pro, OlympiadBench, AIME24, AMC23 \ub4f1 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc810\uc218\ub85c \uce21\uc815\ub429\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc81c\uc2dc\ub41c CFT \ubaa8\ub378\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\ubcf4\ub2e4 \uc801\uc740 \ud559\uc2b5 \ub370\uc774\ud130\ub85c\ub3c4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub370 \uc911\uc810\uc744 \ub461\ub2c8\ub2e4.", "section": "3.3. More Results (CFT Models vs. Existing Models)"}, {"content": "| Model | #Data | GPQA | TheoremQA | AMC23 |\n|---|---|---|---|---|\n| Qwen2.5-32B-Instruct | - | 49.5 | 44.6 | 72.5 |\n| Sky-T1-32B-Preview | 17K | 49.5 | 48.9 | 67.5 |\n| Qwen2.5-32B-Instruct-CFT | 4K | 52.5 | 48.1 | 77.5 |\n| \u0394 (CFT - Sky-T1) | - | 3.0 | -0.8 | 10.0 |", "caption": "Table 4: Performance Comparison of 32B Models across Mathematical Reasoning Benchmarks", "description": "\ud45c 4\ub294 \uc218\ud559\uc801 \ucd94\ub860 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c 32B \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4.  Qwen2.5-32B-Instruct-CFT\uc640 Sky-T1-32B-Preview \ub450 \ubaa8\ub378\uc758 GPQA, TheoremQA, AMC23 \uc138 \uac00\uc9c0 \uc9c0\ud45c\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788, \ub370\uc774\ud130 \ud6a8\uc728\uc131 \uce21\uba74\uc5d0\uc11c Qwen2.5-32B-Instruct-CFT\uac00 Sky-T1-32B-Preview\ubcf4\ub2e4 \ud6e8\uc52c \uc6b0\uc218\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, \ud6e8\uc52c \uc801\uc740 \ud6c8\ub828 \ub370\uc774\ud130\ub85c\ub3c4 \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.", "section": "3. \uc2e4\ud5d8"}, {"content": "| Task | MetaMathQA |  | NuminaMath |  | WebInstruct |  |\n|---|---|---|---|---|---|---|\n|  | SFT | CFT | SFT | CFT | SFT | CFT |\n| MATH | 57.5 | 74.4 | 70.8 | 74.2 | 59.0 | 79.4 |\n| Minerva-Math | 23.9 | 39.3 | 28.3 | 30.5 | 13.2 | 36.8 |\n| GSM8K | 79.5 | 85.7 | 88.3 | 89.1 | 77.4 | 90.9 |\n| OlympiadBench | 20.0 | 36.4 | 36.3 | 37.2 | 19.9 | 41.6 |\n| AIME24 | 6.7 | 23.3 | 10.0 | 23.3 | 3.3 | 20.0 |\n| AMC23 | 37.5 | 57.5 | 50.0 | 62.5 | 37.5 | 67.5 |\n| AVG | 37.5 | 52.8 | 47.3 | 52.8 | 35.1 | 56.0 |", "caption": "Table 5: Performance comparison of SFT and CFT with different training datasets on Qwen2.5-Math-7B.", "description": "\ud45c 5\ub294 \uc11c\ub85c \ub2e4\ub978 \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec Qwen2.5-Math-7B \ubaa8\ub378\uc5d0 \ub300\ud574 SFT(Supervised Fine-Tuning)\uc640 CFT(Critique Fine-Tuning)\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b(MetaMathQA, NuminaMath, WebInstruct)\uc5d0\uc11c \ubaa8\ub378 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uc5ec SFT\uc640 CFT\uc758 \ud6a8\uacfc\ub97c \ub2e4\uc591\ud55c \uc0c1\ud669\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  MATH, GSM8K, AIME24, AMC23 \ub4f1 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc131\ub2a5\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. Experiments"}, {"content": "| Task | Base | Self-generated | Reference |\n|---|---|---|---|\n| MATH | 55.4 | 78.2 | 79.4 |\n| Minerva-Math | 13.6 | 29.4 | 36.8 |\n| GSM8K | 91.6 | 92.4 | 90.9 |\n| OlympiadBench | 16.1 | 42.5 | 41.6 |\n| AIME24 | 10.0 | 16.7 | 20.0 |\n| AMC23 | 40.0 | 67.5 | 67.5 |\n| AVG | 37.8 | 54.5 | 56.0 |", "caption": "Table 6: Performance comparison between self-generated (by Qwen2.5-Math-7B) and reference solutions (from WebInstruct) for CFT training.", "description": "\uc774 \ud45c\ub294 CFT(Critique Fine-Tuning) \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \uc194\ub8e8\uc158\uc758 \ucd9c\ucc98\uc5d0 \ub530\ub978 \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Qwen2.5-Math-7B \ubaa8\ub378\uc774 \uc790\uccb4\uc801\uc73c\ub85c \uc0dd\uc131\ud55c \uc194\ub8e8\uc158\uacfc WebInstruct \ub370\uc774\ud130\uc14b\uc758 \ucc38\uc870 \uc194\ub8e8\uc158\uc744 \uc0ac\uc6a9\ud55c CFT \ud559\uc2b5 \uacb0\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uac01 \uc194\ub8e8\uc158 \uc720\ud615\uc758 \uc131\ub2a5 \ucc28\uc774\uc640 \ud6a8\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.  MATH, Minerva-Math, GSM8K, OlympiadBench, AIME24, AMC23 \ub4f1 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc131\ub2a5 \uacb0\uacfc\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.", "section": "3.4. Ablation Studies"}, {"content": "| Task | SFT-verified | GPT-4o-mini | GPT-4o-1120 |\n|---|---|---|---| \n| MATH | 62.0 | 73.9 | 79.4 |\n| Minerva-Math | 12.5 | 32.7 | 36.8 |\n| GSM8K | 78.8 | 84.5 | 90.9 |\n| OlympiadBench | 22.1 | 35.1 | 41.6 |\n| AIME24 | 16.7 | 20.0 | 20.0 |\n| AMC23 | 50.0 | 62.5 | 67.5 |\n| AVG | 40.4 | 51.5 | 56.0 |", "caption": "Table 7: Performance comparison of CFT using different teacher critique models on Qwen2.5-Math-7B.", "description": "\ud45c 7\uc740 Qwen2.5-Math-7B \ubaa8\ub378\uc5d0 \ub300\ud574 \uc11c\ub85c \ub2e4\ub978 \uad50\uc0ac \ube44\ud3c9 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud55c CFT\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ube44\ud3c9 \ubaa8\ub378(\uc608: GPT-40-mini, GPT-40-1120)\uc744 \uc0ac\uc6a9\ud558\uc5ec CFT\ub97c \ud6c8\ub828\ud55c \uacb0\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \ube44\ud3c9 \ubaa8\ub378\uc758 \uc9c8\uc774 CFT \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c \uc791\uc5c5(\uc608: MATH, Minerva-Math, GSM8K, OlympiadBench, AIME24, AMC23)\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50\ud558\uc5ec, \uc5b4\ub5a4 \ube44\ud3c9 \ubaa8\ub378\uc774 \uac00\uc7a5 \ud6a8\uacfc\uc801\uc778\uc9c0, \uadf8\ub9ac\uace0 \ube44\ud3c9 \ubaa8\ub378\uc758 \uc9c8 \ud5a5\uc0c1\uc774 CFT \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uc5b4\ub5bb\uac8c \uae30\uc5ec\ud558\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.4. Ablation Studies"}, {"content": "| Method | Temperature | MATH | Minerva-Math |\n|---|---|---|---| \n| Direct inference | 0.0 | 79.4 | 36.8 |\n|  | 0.1 | 78.8 | 35.9 |\n|  | 0.3 | 77.5 | 34.7 |\n|  | 0.6 | 75.2 | 33.1 |\n| Single-pass self-critique | 0.1 | 77.2 | 33.7 |\n|  | 0.3 | 76.1 | 32.2 |\n|  | 0.6 | 73.5 | 31.3 |\n| Two-stage self-critique | 0.1 | 77.9 | 35.2 |\n|  | 0.3 | 75.8 | 32.4 |\n|  | 0.6 | 74.6 | 31.5 |", "caption": "Table 8: Performance comparison of different inference methods across various temperature settings.", "description": "\ud45c 8\uc740 \ub2e4\uc591\ud55c \uc628\ub3c4 \uc124\uc815\uc5d0\uc11c \uc5ec\ub7ec \ucd94\ub860 \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  \uc9c1\uc811 \ucd94\ub860, \ub2e8\uc77c \ud328\uc2a4 \uc790\uae30 \ube44\ud310, \uadf8\ub9ac\uace0 2\ub2e8\uacc4 \uc790\uae30 \ube44\ud310 \ubc29\ubc95\uc758 \uc138 \uac00\uc9c0 \ucd94\ub860 \ubc29\ubc95\uc5d0 \ub300\ud55c MATH \ubc0f Minerva-Math \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c 0.0, 0.1, 0.3, 0.6\uc758 \ub124 \uac00\uc9c0 \uc628\ub3c4\uc5d0\uc11c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc790\uae30 \ube44\ud310 \ubc29\ubc95\uc774 \uc9c1\uc811 \ucd94\ub860\uc5d0 \ube44\ud574 \uc131\ub2a5\uc774 \ub5a8\uc5b4\uc9c0\uace0, \uc628\ub3c4\uac00 \ub192\uc544\uc9c8\uc218\ub85d \uc131\ub2a5\uc774 \uc800\ud558\ub418\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Limitations"}]