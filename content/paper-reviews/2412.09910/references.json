{"references": [{"fullname_first_author": "Nicholas Carlini", "paper_title": "Towards evaluating the robustness of neural networks", "publication_date": "2017-01-01", "reason": "This paper introduced the Carlini-Wagner (C-W) attack, a strong white-box adversarial attack that is commonly used to evaluate the robustness of neural networks."}, {"fullname_first_author": "Ian J. Goodfellow", "paper_title": "Explaining and harnessing adversarial examples", "publication_date": "2014-12-20", "reason": "This work introduced the Fast Gradient Sign Method (FGSM) attack, a fundamental adversarial attack method that is widely used for its simplicity and efficiency."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper introduced CLIP, a powerful vision-language model that is used in the proposed method for text encoding and image classification."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper introduced Latent Diffusion Models (LDMs), a class of powerful generative models that are used in the proposed method for generating adversarial examples."}, {"fullname_first_author": "Aleksander Madry", "paper_title": "Towards deep learning models resistant to adversarial attacks", "publication_date": "2017-01-01", "reason": "This work introduced Projected Gradient Descent (PGD), a strong white-box adversarial attack method used extensively in this paper for evaluations and comparisons."}]}