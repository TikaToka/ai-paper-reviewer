{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduced CLIP, a foundational model for contrastive language-image pre-training that SigLIP builds upon."}, {"fullname_first_author": "M. Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "publication_date": "2021-06-01", "reason": "This paper introduced important self-supervised techniques (self-distillation and masked prediction) that improve dense feature extraction, incorporated into SigLIP 2."}, {"fullname_first_author": "X. Chen", "paper_title": "PaLI: A jointly-scaled multilingual language-image model", "publication_date": "2022-09-01", "reason": "This paper introduced PaLI, a multilingual vision-language model that influenced SigLIP 2's multilingual capabilities and improved fairness."}, {"fullname_first_author": "B. Wan", "paper_title": "LocCa: Visual pretraining with location-aware captioners", "publication_date": "2024-01-01", "reason": "This paper introduced the LocCa captioning-based pretraining objective that significantly improves localization in SigLIP 2."}, {"fullname_first_author": "M. F. Naeem", "paper_title": "SILC: Improving vision language pretraining with self-distillation", "publication_date": "2024-01-01", "reason": "This paper introduced self-supervised losses (SILC) incorporated into SigLIP 2 that enhance the quality of dense features and zero-shot capabilities."}]}