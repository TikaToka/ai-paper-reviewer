{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-00", "reason": "This paper introduces Llama 2, a foundational large language model used extensively in the experiments and analysis within the current research."}, {"fullname_first_author": "Andy Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-00", "reason": "This paper provides a benchmark (AdvBench) of adversarial attacks used to evaluate the safety and robustness of LLMs, a key component of the current work."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper details crucial safety alignment techniques (RLHF) used in training many of the LLMs studied, forming a foundational element of the safety analysis."}, {"fullname_first_author": "Guangxuan Xiao", "paper_title": "SmoothQuant: Accurate post-training quantization for large language models", "publication_date": "2023-00-00", "reason": "This paper presents SmoothQuant, a key activation quantization technique analyzed for its impact on LLM safety and efficiency within the current study."}, {"fullname_first_author": "James Liu", "paper_title": "Training-free activation sparsity in large language models", "publication_date": "2024-08-00", "reason": "This paper introduces TEAL, a significant activation sparsification method evaluated for its effects on LLM safety within the current research."}]}