[{"content": "| Method | Cls | Cap | Cap |\n|---|---|---|---|\n| **Method** | **Cls** | **Cap** | **Cap** |\n| **GPT-4** | **GPT-4** | **S-BERT** |  |\n| PointLLM-7B | 53.00 | 44.85 | 47.47 |\n| - Encoder | 35.50 | 33.37 | 41.19 |\n| + 2-layer T.E. | 42.50 | 41.35 | 44.25 |\n| **+ 3-layer T.E.** | **47.31** | **43.86** | **45.89** |\n| + 4-layer T.E. | 45.00 | 42.99 | 44.51 |", "caption": "Table 1: Token Embedding. We evaluate the performance on the Objaverse benchmark and adopt PointLLM-7B as the baseline model. \u2019Cls\u2019 and \u2019Cap\u2019 represent classification and captioning tasks, respectively. S-BERT refers to the Sentence-BERT. T.E. stands for our designed token embedding module.", "description": "\ud45c 1\uc740 \uc81c\uc548\ub41c \ud1a0\ud070 \uc784\ubca0\ub529 \ubaa8\ub4c8\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 Objaverse \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c PointLLM-7B \ubaa8\ub378\uc744 \uae30\uc900\uc73c\ub85c \uc2e4\ud5d8\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \"Cls\"\uc640 \"Cap\"\uc740 \uac01\uac01 \ubd84\ub958\uc640 \ucea1\uc158 \uc0dd\uc131 \uc791\uc5c5\uc744 \ub098\ud0c0\ub0b4\uba70, S-BERT\ub294 \ubb38\uc7a5 BERT\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.  T.E.\ub294 \uc5f0\uad6c\ud300\uc5d0\uc11c \uc0c8\ub86d\uac8c \uc124\uacc4\ud55c \ud1a0\ud070 \uc784\ubca0\ub529 \ubaa8\ub4c8\uc744 \uac00\ub9ac\ud0b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud1a0\ud070 \uc784\ubca0\ub529 \ubc29\ubc95(\uae30\uc900 \ubaa8\ub378\uacfc 2, 3, 4 \ub808\uc774\uc5b4\uc758 T.E.\ub97c \uc0ac\uc6a9\ud55c \uacbd\uc6b0)\uc758 \ubd84\ub958 \ubc0f \ucea1\uc158 \uc0dd\uc131 \uc131\ub2a5\uc744 GPT-4 \uc810\uc218\uc640 S-BERT \uc810\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "2.1. Overall Architecture"}, {"content": "| Method | LR | Cls | Cap | Cap |\n|---|---|---|---|---|\n| **Method** | **LR** | **Cls** | **Cap** | **Cap** |\n|  |  | **GPT-4** | **GPT-4** | **S-BERT** |\n| PointLLM-7B | 2e-3 | 53.00 | 44.85 | 47.47 |\n| + 2 learnable layers | 2e-3 | 41.06 | 42.23 | 45.92 |\n|  | 4e-4 | 45.5 | 44.72 | 47.35 |\n| + 4 learnable layers | 2e-3 | 44.85 | 41.53 | 46.77 |\n|  | 4e-4 | 49.11 | 45.39 | 47.71 |\n| + 8 learnable layers | 2e-3 | 43.76 | 39.71 | 42.38 |\n|  | 4e-4 | 48.00 | 44.49 | 47.21 |", "caption": "Table 2: Further 3D Encoding. We set the LLM early layers to be learnable. LR represents the learning rate during the pre-training stage, with the original learning rate set to 2e-3.", "description": "\ud45c 2\ub294 LLM\uc758 \ucd08\uae30 \ub808\uc774\uc5b4\ub97c \ud559\uc2b5 \uac00\ub2a5\ud558\ub3c4\ub85d \uc124\uc815\ud55c \ucd94\uac00\uc801\uc778 3D \uc778\ucf54\ub529 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc6d0\ub798 \ud559\uc2b5\ub960\uc744 2e-3\uc73c\ub85c \uc124\uc815\ud588\uace0, \ud45c\uc5d0\ub294 \ud559\uc2b5\ub960(LR)\uc744 \ubcc0\uacbd\ud558\uba74\uc11c 2\uac1c, 4\uac1c, 8\uac1c\uc758 \ud559\uc2b5 \uac00\ub2a5\ud55c \ub808\uc774\uc5b4\ub97c \ucd94\uac00\ud588\uc744 \ub54c\uc758 \uacb0\uacfc(GPT-4, S-BERT \uae30\uc900)\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uc124\uc815\uc5d0 \ub530\ub978 \ubd84\ub958 \ubc0f \ucea1\uc158 \uc0dd\uc131 \uc131\ub2a5 \ubcc0\ud654\ub97c \ube44\uad50\ud558\uc5ec 3D \uc778\ucf54\ub529 \uc804\ub7b5\uc758 \ud6a8\uacfc\ub97c \ubd84\uc11d\ud558\uae30 \uc704\ud55c \ud45c\uc785\ub2c8\ub2e4.", "section": "2. Investigation of Encoder-free 3D LMM"}, {"content": "| Method | Cls | Cap | Cap |\n|---|---|---|---|\n| **Method** | **Cls** | **Cap** | **Cap** |\n| **GPT-4** | **GPT-4** | **S-BERT** |  |\n| PointLLM-7B | 53.00 | 44.85 | 47.47 |\n| Masked Modeling Loss<sub>patch</sub><sup>\u03a8</sup> | 48.50 | 45.34 | 46.36 |\n| Masked Modeling Loss<sub>patch</sub><sup>\u03a6</sup> | 50.00 | 46.80 | 47.29 |\n| Masked Modeling Loss<sub>feat</sub><sup>\u03a8</sup> | 50.00 | 45.80 | 46.29 |\n| Masked Modeling Loss<sub>feat</sub><sup>\u03a6</sup> | 49.50 | 47.35 | 47.93 |\n| Reconstruction Loss<sub>patch</sub> | 49.50 | 46.96 | 47.33 |\n| Reconstruction Loss<sub>feat</sub> | 48.50 | 45.95 | 47.18 |\n| Contrastive Loss | 43.50 | 42.91 | 44.77 |\n| Knowledge Distillation Loss | 49.50 | 45.43 | 47.09 |\n| Hybrid Semantic Loss<sub>patch</sub> | 50.50 | 46.84 | 47.59 |\n| Hybrid Semantic Loss<sub>feat</sub> | 52.00 | 48.51 | 48.06 |\n| **+ Position Embedding** | **53.00** | **48.85** | **48.00** |", "caption": "Table 3: LLM-embedded Semantic Encoding. In the pre-training stage, we explore the effects of various self-supervised learning losses targeting point tokens. \u03a8\u03a8\\Psiroman_\u03a8 represents a mask ratio of 60%, while \u03a6\u03a6\\Phiroman_\u03a6 represents a mask ratio of 30%.\nThe subscript patch and feat represent the loss target.\nFor Hybrid Semantic Loss, the subscript patch and feat represent the masked modeling target, while the reconstruction target is the corresponding feat and patch.", "description": "\ubcf8 \ud45c\ub294 \ub17c\ubb38\uc758 2.2\uc808 \"LLM-embedded Semantic Encoding\"\uc5d0\uc11c \ub2e4\uc591\ud55c \uc790\uae30 \uc9c0\ub3c4 \ud559\uc2b5 \uc190\uc2e4 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec 3D \ud3ec\uc778\ud2b8 \ud1a0\ud070\uc744 \ud559\uc2b5\uc2dc\ud0a8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788, \ub9c8\uc2a4\ud06c \ube44\uc728(mask ratio)\uc774 60%\uc778 \uacbd\uc6b0\uc640 30%\uc778 \uacbd\uc6b0\uc758 \uacb0\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  Hybrid Semantic Loss\uc758 \uacbd\uc6b0, patch\uc640 feat\ub294 \uac01\uac01 \ub9c8\uc2a4\ud06c \ubaa8\ub378\ub9c1 \uc190\uc2e4(masked modeling loss)\uacfc \uc7ac\uad6c\uc131 \uc190\uc2e4(reconstruction loss)\uc758 \ub300\uc0c1\uc744 \ub098\ud0c0\ub0b4\uba70, \uc7ac\uad6c\uc131 \uc190\uc2e4\uc758 \uacbd\uc6b0 feat\uc640 patch \ubaa8\ub450 \ub300\uc0c1\uc774 \ub429\ub2c8\ub2e4.  \uac01 \uc190\uc2e4 \ud568\uc218\uc5d0 \ub530\ub978 GPT-4 \uc810\uc218\uc640 S-BERT \uc810\uc218\ub97c \ube44\uad50\ud558\uc5ec \uc5b4\ub5a4 \uc790\uae30 \uc9c0\ub3c4 \ud559\uc2b5 \ubc29\ubc95\uc774 3D LLM\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uac00\uc7a5 \ud6a8\uacfc\uc801\uc778\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2. LLM-embedded Semantic Encoding"}, {"content": "| Method | Cls | Cap | Cap |\n|---|---|---|---|\n| **Method** | **Cls** | **Cap** | **Cap** |\n| **GPT-4** | **GPT-4** | **S-BERT** |  |\n| PointLLM-7B | 53.00 | 44.85 | 47.47 |\n| l=1 | 52.50 | 48.86 | 48.14 |\n| l=2 | 50.00 | 46.76 | 47.95 |\n| l=3 | 48.00 | 45.51 | 46.85 |\n| H=2 | 53.50 | 49.13 | 48.33 |\n| H=4 | 52.50 | 48.39 | 47.75 |\n| H=8 | 51.00 | 48.95 | 47.97 |\n| **+ Self-Attn.** | **55.00** | **50.92** | **48.61** |", "caption": "Table 4: Hierarchical Geometry Aggregation. In the instruction tuning stage, we conduct the experiments of Hierarchical Geometry Aggregation strategy.\nl\ud835\udc59litalic_l represents the number of aggregation and propagation operations.\nH\ud835\udc3bHitalic_H refers to the LLM layers between l\ud835\udc59litalic_l aggregation and l\ud835\udc59litalic_l propagation operations.\n+ Self-Attn. represents the incorporation of the gated self-attention in the aggregation.", "description": "\ud45c 4\ub294 \ub17c\ubb38\uc758 \uc9c0\uc2dc \uc870\uc815 \ub2e8\uacc4\uc5d0\uc11c \uacc4\uce35\uc801 \uae30\ud558\ud559\uc801 \uc9d1\uacc4 \uc804\ub7b5\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  l\uc740 \uc9d1\uacc4 \ubc0f \uc804\ud30c \uc5f0\uc0b0\uc758 \ud69f\uc218\ub97c \ub098\ud0c0\ub0b4\uace0, H\ub294 l\ubc88\uc758 \uc9d1\uacc4\uc640 l\ubc88\uc758 \uc804\ud30c \uc5f0\uc0b0 \uc0ac\uc774\uc5d0 \uc788\ub294 LLM \uacc4\uce35\uc758 \uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. '+ Self-Attn.'\uc740 \uac8c\uc774\ud2b8 \uc81c\uc5b4 \uc790\uae30 \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uc744 \uc9d1\uacc4 \uacfc\uc815\uc5d0 \ud1b5\ud569\ud588\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815(l\uacfc H\uc758 \uc870\ud569)\uc5d0 \ub530\ub978 \ubaa8\ub378 \uc131\ub2a5(GPT-4, Sentence-BERT, SimCSE \uae30\uc900)\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \ucd5c\uc801\uc758 \uacc4\uce35\uc801 \uae30\ud558\ud559\uc801 \uc9d1\uacc4 \uc804\ub7b5\uc744 \ucc3e\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "2.3 \uacc4\uce35\uc801 \uae30\ud558\ud559\uc801 \uc9d1\uacc4"}, {"content": "| Model | GPT-4 | Sentence-BERT | SimCSE | BLEU-1 | ROUGE-L | METEOR | GPT-4 | GPT-4 | QA |\n|---|---|---|---|---|---|---|---|---|---|\n| InstructBLIP-7B (Dai et al., 2023) | 45.34 | 47.41 | 48.48 | 4.27 | 8.28 | 12.99 | 43.50 |  \u2013 |  |\n| InstructBLIP-13B (Dai et al., 2023) | 44.97 | 45.90 | 48.86 | 4.65 | 8.85 | 13.23 | 34.25 |  \u2013 |  |\n| LLaVA-7B (Liu et al., 2024) | 46.71 | 45.61 | 47.10 | 3.64 | 7.70 | 12.14 | 50.00 |  \u2013 |  |\n| LLaVA-13B (Liu et al., 2024) | 38.28 | 46.37 | 45.90 | 4.02 | 8.15 | 12.58 | 51.75 | 47.90 |  |\n| 3D-LLM (Hong et al., 2023) | 33.42 | 44.48 | 43.68 | 16.91 | 19.48 | 19.73 | 45.25 |  \u2013 |  |\n| PointLLM-7B (Xu et al., 2023) | 44.85 | 47.47 | 48.55 | 3.87 | 7.30 | 11.92 | 53.00 | 41.20 |  |\n| PointLLM-13B (Xu et al., 2023) | 48.15 | 47.91 | 49.12 | 3.83 | 7.23 | 12.26 | 54.00 | 46.60 |  |\n| ShapeLLM-7B (Qi et al., 2024) | 46.92 | 48.20 | 49.23 |  \u2013 |  \u2013 |  \u2013 | 54.50 | 47.40 |  |\n| ShapeLLM-13B (Qi et al., 2024) | 48.94 | 48.52 | 49.98 |  \u2013 |  \u2013 |  \u2013 | 54.00 | 53.10 |  |\n| Enel-7B | 50.92 | 48.61 | 49.31 | 3.88 | 7.20 | 12.50 | 55.00 | 42.70 |  |", "caption": "Table 5: Comparison of different models on various 3D understanding tasks.\nA primary focus is placed on GPT-4 evaluation, along with data-driven metrics (Sentence-BERT and SimCSE).", "description": "\ud45c 5\ub294 \ub2e4\uc591\ud55c 3D \uc774\ud574 \uacfc\uc81c\uc5d0\uc11c \uc5ec\ub7ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. GPT-4 \ud3c9\uac00\ub97c \uc911\uc2ec\uc73c\ub85c \ud558\ub418, Sentence-BERT \ubc0f SimCSE \uc640 \uac19\uc740 \ub370\uc774\ud130 \uc911\uc2ec \uc9c0\ud45c\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ub300\ud55c \ubcf4\ub2e4 \ud3ec\uad04\uc801\uc778 \ud3c9\uac00\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.  \ub2e4\uc591\ud55c 3D \uacfc\uc81c(\ubd84\ub958, \ucea1\uc158 \uc0dd\uc131, \uc9c8\uc758\uc751\ub2f5)\uc5d0 \ub300\ud55c \uac01 \ubaa8\ub378\uc758 GPT-4 \uc810\uc218, Sentence-BERT \uc810\uc218, SimCSE \uc810\uc218, \uadf8\ub9ac\uace0 \ucd94\uac00\uc801\uc778 \uc9c0\ud45c(BLEU-1, ROUGE-L, METEOR)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uac01 \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uace0, 3D \uc774\ud574 \ubd84\uc57c\uc5d0\uc11c\uc758 \ucd5c\ucca8\ub2e8 \uae30\uc220 \ub3d9\ud5a5\uc744 \ud30c\uc545\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3. \uacb0\uacfc \ubc0f \uc2dc\uac01\ud654"}, {"content": "| Model | GPT-4 | Sentence-BERT | SimCSE | BLEU-1 | ROUGE-L | METEOR | Cls | GPT-4 |\n|---|---|---|---|---|---|---|---|---|\n| **Enel-7B** | 50.92 | 48.61 | 49.31 | 3.88 | 7.20 | 12.50 | 55.00 |\n| \u2013 Hybrid Semantic Loss | 47.19 | 48.07 | 48.31 | 3.46 | 7.41 | 11.84 | 50.61 |\n| Hybrid Semantic Loss<sub>patch</sub><sup>\u03a6</sup> | 49.05 | 48.82 | 49.20 | 4.01 | 7.25 | 12.38 | 52.20 |\n| Hybrid Semantic Loss<sub>patch</sub><sup>\u03a8</sup> | 48.96 | 48.38 | 49.00 | 3.66 | 6.97 | 11.98 | 52.00 |\n| Hybrid Semantic Loss<sub>feat</sub><sup>\u03a8</sup> | 49.63 | 48.00 | 48.62 | 3.78 | 6.88 | 12.33 | 51.50 |\n| \u2013 gate mechanism | 49.26 | 48.41 | 48.93 | 3.71 | 7.12 | 12.47 | 53.50 |\n| l=2,H=2,O=0 | 48.81 | 48.10 | 48.57 | 3.70 | 6.99 | 12.01 | 51.50 |\n| l=2,H=4,O=0 | 49.02 | 48.47 | 48.61 | 3.65 | 7.10 | 12.31 | 52.00 |\n| l=2,H=2,O=2 | 48.96 | 47.96 | 48.89 | 3.80 | 7.05 | 12.55 | 52.00 |\n| l=2,H=4,O=2 | 49.58 | 48.70 | 48.84 | 3.84 | 7.56 | 12.76 | 53.00 |", "caption": "Table 6: Ablation Experiments.\nWe begin the ablation experiments by changing the single configuration of the module from Enel.\n\u03a8\u03a8\\Psiroman_\u03a8 represents a mask ratio of 60%, while \u03a6\u03a6\\Phiroman_\u03a6 represents a mask ratio of 30%. For Hybrid Semantic Loss, the subscript p\u2062a\u2062t\u2062c\u2062h\ud835\udc5d\ud835\udc4e\ud835\udc61\ud835\udc50\u210epatchitalic_p italic_a italic_t italic_c italic_h and f\u2062e\u2062a\u2062t\ud835\udc53\ud835\udc52\ud835\udc4e\ud835\udc61featitalic_f italic_e italic_a italic_t represent the masked modeling target, while the reconstruction target is the corresponding f\u2062e\u2062a\u2062t\ud835\udc53\ud835\udc52\ud835\udc4e\ud835\udc61featitalic_f italic_e italic_a italic_t and p\u2062a\u2062t\u2062c\u2062h\ud835\udc5d\ud835\udc4e\ud835\udc61\ud835\udc50\u210epatchitalic_p italic_a italic_t italic_c italic_h.\nl\ud835\udc59litalic_l represents the number of aggregation and propagation operations.\nH\ud835\udc3bHitalic_H refers to the LLM layers between l\ud835\udc59litalic_l aggregation and l\ud835\udc59litalic_l propagation operations.\nO\ud835\udc42Oitalic_O refers to the LLM layer between two individual aggregation or propagation operations.", "description": "\ud45c 6\uc740 ENEL \ubaa8\ub378\uc758 \uad6c\uc131 \uc694\uc18c\ub97c \ud558\ub098\uc529 \ubcc0\uacbd\ud574 \uac00\uba70 ablation \uc2e4\ud5d8\uc744 \uc9c4\ud589\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  mask \ube44\uc728(\u03a8\ub294 60%, \u03a6\ub294 30%), Hybrid Semantic Loss\uc758 \uc801\uc6a9 \ubc29\uc2dd (patch\uc640 feat), aggregation \ubc0f propagation \uc5f0\uc0b0 \ud69f\uc218(l), aggregation\uacfc propagation \uc5f0\uc0b0 \uc0ac\uc774\uc758 LLM \ub808\uc774\uc5b4 \uc218(H), \uadf8\ub9ac\uace0 \uac1c\ubcc4 aggregation \ub610\ub294 propagation \uc5f0\uc0b0 \uc0ac\uc774\uc758 LLM \ub808\uc774\uc5b4 \uc218(O) \ub4f1\uc744 \ubcc0\uacbd\ud558\uba70 \uc2e4\ud5d8\ud558\uc600\uc2b5\ub2c8\ub2e4.  \uac01 \uc124\uc815\uc5d0 \ub530\ub978 GPT-4 \uae30\ubc18 \ubd84\ub958 \ubc0f \ucea1\uc158 \uc0dd\uc131 \uc131\ub2a5\uc758 \ubcc0\ud654\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ub300\ud55c \uac01 \uc694\uc18c\uc758 \uc601\ud5a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "2. Investigation of Encoder-free 3D LMM"}]