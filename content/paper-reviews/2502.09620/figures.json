[{"figure_path": "https://arxiv.org/html/2502.09620/extracted/6201996/intro3.png", "caption": "Figure 1: Issues of encoder-based 3D LMMs.\n(a) Point Cloud Resolution Limitation.\nDuring training, the point cloud size (P.T. size) and point token size (P.T. size) are fixed at 8192 and 512, respectively.\nAnd we adjust these two sizes during inference, point cloud size from 2K to 16K and the corresponding point token size from 128 to 2048.\nWe evaluate them on the captioning task of the Objaverse benchmark using GPT-4 scores as the evaluation metric.\n(b) Embedding Semantic Discrepancy.\nWe visualize the attention scores of the average text token to the point tokens, where red indicates higher values.\nThe point tokens in the encoder-free architecture exhibit stronger textual semantic relevance needed for the LLM.", "description": "\uadf8\ub9bc 1\uc740 encoder \uae30\ubc18 3D LMM\uc758 \ubb38\uc81c\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 Point Cloud Resolution Limitation\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc73c\ub85c, \ud559\uc2b5 \uc2dc\uc5d0\ub294 point cloud \ud06c\uae30(P.T. size)\uc640 point token \ud06c\uae30\uac00 \uac01\uac01 8192\uc640 512\ub85c \uace0\uc815\ub418\uc5b4 \uc788\uc9c0\ub9cc, \ucd94\ub860 \uc2dc\uc5d0\ub294 point cloud \ud06c\uae30\ub294 2K\uc5d0\uc11c 16K\ub85c, point token \ud06c\uae30\ub294 128\uc5d0\uc11c 2048\ub85c \uc870\uc815\ub429\ub2c8\ub2e4. Objaverse \ubca4\uce58\ub9c8\ud06c\uc758 captioning \uc791\uc5c5\uc5d0\uc11c GPT-4 \uc810\uc218\ub97c \ud3c9\uac00 \uc9c0\ud45c\ub85c \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. (b)\ub294 Embedding Semantic Discrepancy\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc73c\ub85c, \ud3c9\uade0 text token\uc758 point token\uc5d0 \ub300\ud55c attention \uc810\uc218\ub97c \uc2dc\uac01\ud654\ud55c \uac83\uc785\ub2c8\ub2e4. \ube68\uac04\uc0c9\uc740 \ub192\uc740 \uac12\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. encoder\uac00 \uc5c6\ub294 \uad6c\uc870\uc758 point token\uc740 LLM\uc5d0 \ud544\uc694\ud55c \ud14d\uc2a4\ud2b8 \uc758\ubbf8\uc801 \uad00\ub828\uc131\uc774 \ub354 \uac15\ud569\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.09620/x1.png", "caption": "Figure 2: Overall Pipeline of Enel.\nThe training is divided into two stages: the pre-training stage and the instruction tuning stage. In the first stage, we set the first K\ud835\udc3eKitalic_K layers to be learnable and apply the proposed Hybrid Semantic Loss to embed high-level semantics into the LLM. In the second stage, we adopt the Hierarchical Geometric Aggregation strategy to capture local structures of point clouds.", "description": "\uadf8\ub9bc 2\ub294 ENEL\uc758 \uc804\uccb4 \ud30c\uc774\ud504\ub77c\uc778\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud6c8\ub828\uc740 \ub450 \ub2e8\uacc4, \uc989 \uc0ac\uc804 \ud6c8\ub828 \ub2e8\uacc4\uc640 \uc9c0\uc2dc \ud29c\ub2dd \ub2e8\uacc4\ub85c \ub098\ub269\ub2c8\ub2e4. \uccab \ubc88\uc9f8 \ub2e8\uacc4\uc5d0\uc11c\ub294 \ucc98\uc74c K\uac1c\uc758 \ub808\uc774\uc5b4\ub97c \ud559\uc2b5 \uac00\ub2a5\ud558\ub3c4\ub85d \uc124\uc815\ud558\uace0 \uc81c\uc548\ub41c \ud558\uc774\ube0c\ub9ac\ub4dc \uc758\ubbf8\ub860\uc801 \uc190\uc2e4\uc744 \uc801\uc6a9\ud558\uc5ec \uace0\ucc28\uc6d0 \uc758\ubbf8\ub97c LLM\uc5d0 \ud3ec\ud568\uc2dc\ud0b5\ub2c8\ub2e4. \ub450 \ubc88\uc9f8 \ub2e8\uacc4\uc5d0\uc11c\ub294 \uacc4\uce35\uc801 \uae30\ud558\ud559\uc801 \uc9d1\uacc4 \uc804\ub7b5\uc744 \ucc44\ud0dd\ud558\uc5ec \uc810 \uad6c\ub984\uc758 \uc9c0\uc5ed\uc801 \uad6c\uc870\ub97c \ud3ec\ucc29\ud569\ub2c8\ub2e4.", "section": "2. ENEL \ubaa8\ub378\uc758 \uac1c\uc694"}, {"figure_path": "https://arxiv.org/html/2502.09620/x2.png", "caption": "Figure 3: Point Cloud Self-Supervised Learning Losses.\nIn the pre-training stage, we explore common self-supervised learning losses for the encoder-free 3D LMM: (a) Masked Modeling Loss, (b) Reconstruction Loss, (c) Contrastive Loss, and (d) Knowledge Distillation Loss.\nThe (e) represents our proposed Hybrid Semantic Loss, specifically designed for the encoder-free architecture.", "description": "\uadf8\ub9bc 3\uc740 \uc778\ucf54\ub354\uac00 \uc5c6\ub294 3D LMM\uc744 \uc704\ud55c \uc790\uae30 \uc9c0\ub3c4 \ud559\uc2b5 \uc190\uc2e4 \ud568\uc218\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  (a)\ub294 \ub9c8\uc2a4\ud06c \ubaa8\ub378\ub9c1 \uc190\uc2e4, (b)\ub294 \uc7ac\uad6c\uc131 \uc190\uc2e4, (c)\ub294 \ub300\uc870 \uc190\uc2e4, (d)\ub294 \uc9c0\uc2dd \uc99d\ub958 \uc190\uc2e4\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. (e)\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ud558\uc774\ube0c\ub9ac\ub4dc \uc758\ubbf8\ub860\uc801 \uc190\uc2e4 \ud568\uc218\ub85c, \uc778\ucf54\ub354\uac00 \uc5c6\ub294 \uc544\ud0a4\ud14d\ucc98\ub97c \uc704\ud574 \ud2b9\ubcc4\ud788 \uace0\uc548\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc790\uae30 \uc9c0\ub3c4 \ud559\uc2b5 \uae30\ubc95\uc744 \ube44\uad50\ud558\uc5ec \uc778\ucf54\ub354 \uc5c6\ub294 3D LMM\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud558\ub294 \ucd5c\uc801\uc758 \ubc29\ubc95\uc744 \ucc3e\ub294 \uacfc\uc815\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2.2 LLM-embedded Semantic Encoding"}, {"figure_path": "https://arxiv.org/html/2502.09620/x3.png", "caption": "Figure 4: Hierarchical Geometry Aggregation Strategy.\nIn the instruction tuning stage, we apply aggregation and propagation operations to the point tokens to capture the local structural details.", "description": "\uadf8\ub9bc 4\ub294 ENEL \ubaa8\ub378\uc758 Instruction Tuning \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \uacc4\uce35\uc801 \uae30\ud558\ud559\uc801 \uc9d1\uacc4(Hierarchical Geometry Aggregation) \uc804\ub7b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uc804\ub7b5\uc740 3D \ud3ec\uc778\ud2b8 \ud074\ub77c\uc6b0\ub4dc\uc758 \uad6d\ubd80\uc801\uc778 \uad6c\uc870\uc801 \uc138\ubd80 \uc815\ubcf4\ub97c \ud3ec\ucc29\ud558\uae30 \uc704\ud574 \ud3ec\uc778\ud2b8 \ud1a0\ud070\uc5d0 \uc9d1\uacc4 \ubc0f \uc804\ud30c \uc5f0\uc0b0\uc744 \uc801\uc6a9\ud558\ub294 \ubc29\ubc95\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, \uadf8\ub9bc\uc5d0\uc11c\ub294 LLM\uc758 \ucd08\uae30 \ub808\uc774\uc5b4\uc5d0\uc11c FPS(Farthest Point Sampling)\uc640 k-NN(k-Nearest Neighbor)\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud3ec\uc778\ud2b8 \ud1a0\ud070\uc744 \uc9d1\uacc4\ud558\uace0, \uac8c\uc774\ud2b8\ub41c \uc790\uae30 \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uc744 \ud1b5\ud574 \uad6d\ubd80\uc801 \uae30\ud558\ud559\uc801 \uad6c\uc870\ub97c \ucea1\ucc98\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ud6c4, \uc5ec\ub7ec \ubc88\uc758 \uc9d1\uacc4 \ubc0f \uc804\ud30c \uc5f0\uc0b0\uc744 \ud1b5\ud574 \ub2e4\uc591\ud55c \uc218\uc900\uc758 \uad6d\ubd80 \uc815\ubcf4\ub97c \ucd94\ucd9c\ud558\uc5ec LLM\uc758 \uc0c1\uc704 \ub808\uc774\uc5b4\uc5d0 \uc804\ub2ec\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 LLM\uc740 \uc804\uc5ed\uc801 \uc815\ubcf4\uc640 \uad6d\ubd80\uc801 \uc138\ubd80 \uc815\ubcf4 \ubaa8\ub450\ub97c \ud65c\uc6a9\ud558\uc5ec 3D \ud3ec\uc778\ud2b8 \ud074\ub77c\uc6b0\ub4dc\ub97c \ubcf4\ub2e4 \uc815\ud655\ud558\uac8c \uc774\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2.3 Hierarchical Geometry Aggregation"}, {"figure_path": "https://arxiv.org/html/2502.09620/x4.png", "caption": "Figure 5: Difference in Semantic Encoding.\nBy visualizing the attention scores of the average text token to the point tokens on the Objaverse dataset, we compare the semantic encoding potential of encoder-based and encoder-free architectures, where red indicates higher values.\nAnd (a) represents chairs, (b) represents airplanes, and (c) represents lamps.", "description": "\uadf8\ub9bc 5\ub294 \uc778\ucf54\ub354 \uae30\ubc18 \ubc0f \uc778\ucf54\ub354 \uc5c6\ub294 \uc544\ud0a4\ud14d\ucc98\uc758 \uc758\ubbf8\ub860\uc801 \uc778\ucf54\ub529 \uc7a0\uc7ac\ub825\uc744 \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Objaverse \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ud3c9\uade0 \ud14d\uc2a4\ud2b8 \ud1a0\ud070\uc758 \uc5b4\ud150\uc158 \uc810\uc218\ub97c \uc2dc\uac01\ud654\ud558\uc5ec, \uc778\ucf54\ub354 \uae30\ubc18 \ubaa8\ub378\uacfc \uc778\ucf54\ub354 \uc5c6\ub294 \ubaa8\ub378\uc5d0\uc11c 3D \uc810 \uad6c\ub984\uc758 \uc758\ubbf8\ub860\uc801 \ud45c\ud604\uc758 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube68\uac04\uc0c9\uc740 \ub354 \ub192\uc740 \uc5b4\ud150\uc158 \uc810\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. (a)\ub294 \uc758\uc790, (b)\ub294 \ube44\ud589\uae30, (c)\ub294 \ub7a8\ud504\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc778\ucf54\ub354 \uc5c6\ub294 \ubaa8\ub378\uc740 \ud14d\uc2a4\ud2b8\uc640 3D \ud615\uc0c1 \uc0ac\uc774\uc758 \ub354 \uac15\ub825\ud55c \uc758\ubbf8\ub860\uc801 \uc5f0\uad00\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. \uacb0\uacfc \ubc0f \uc2dc\uac01\ud654"}, {"figure_path": "https://arxiv.org/html/2502.09620/extracted/6201996/loss2.png", "caption": "Figure 6: Variants of Point Cloud Self-Supervised Learning Losses.\n(a) The Variant of Masked Modeling Loss, (b) The Variant of Reconstruction Loss, (c) The Variant of Hybrid Semantic Loss.", "description": "\uadf8\ub9bc 6\uc740 Point Cloud Self-Supervised Learning Loss\uc758 \uc138 \uac00\uc9c0 \ubcc0\ud615\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 Masked Modeling Loss\uc758 \ubcc0\ud615\uc73c\ub85c, \uc785\ub825 \ud3ec\uc778\ud2b8 \ud074\ub77c\uc6b0\ub4dc\uc758 \uc77c\ubd80\ub97c \ub9c8\uc2a4\ud0b9\ud558\uace0 LLM\uc774 \ub9c8\uc2a4\ud0b9\ub41c \ubd80\ubd84\uc744 \uc608\uce21\ud558\uac8c \ud569\ub2c8\ub2e4. (b)\ub294 Reconstruction Loss\uc758 \ubcc0\ud615\uc73c\ub85c, LLM\uc774 \uc0dd\uc131\ud55c \ud3ec\uc778\ud2b8 \ud074\ub77c\uc6b0\ub4dc\ub97c \uc6d0\ub798 \ud3ec\uc778\ud2b8 \ud074\ub77c\uc6b0\ub4dc\uc640 \ube44\uad50\ud558\uc5ec \uc7ac\uad6c\uc131 \uc624\ucc28\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. (c)\ub294 Hybrid Semantic Loss\uc758 \ubcc0\ud615\uc73c\ub85c, Masked Modeling Loss\uc640 Reconstruction Loss\ub97c \uacb0\ud569\ud558\uc5ec \uace0\ucc28\uc6d0 \uc758\ubbf8 \uc815\ubcf4\uc640 \uc800\ucc28\uc6d0 \uacf5\uac04 \uc815\ubcf4\ub97c \ub3d9\uc2dc\uc5d0 \ud559\uc2b5\ud569\ub2c8\ub2e4.  \uac01 \ubcc0\ud615\uc740 LLM\uc774 3D \ud3ec\uc778\ud2b8 \ud074\ub77c\uc6b0\ub4dc\uc758 \uc758\ubbf8\uc801 \ubc0f \uae30\ud558\ud559\uc801 \ud2b9\uc9d5\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud559\uc2b5\ud558\ub294 \ub370 \uae30\uc5ec\ud569\ub2c8\ub2e4.", "section": "2.2 LLM-embedded Semantic Encoding"}, {"figure_path": "https://arxiv.org/html/2502.09620/extracted/6201996/output3.png", "caption": "Figure 7: Enel Output Examples.\nWe demonstrate that Enel provides precise and diverse responses when addressing different problems.", "description": "\uadf8\ub9bc 7\uc740 ENEL \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \uc720\ud615\uc758 3D \ubaa8\ub378\uc5d0 \ub300\ud55c \uc9c8\ubb38\uc5d0 \ub300\ud574 \uc815\ud655\ud558\uace0 \ub2e4\uc591\ud55c \uc751\ub2f5\uc744 \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \uc608\uc2dc\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\ub294 3D \ubaa8\ub378 \uc774\ubbf8\uc9c0\uc640 \ud568\uaed8, \uac01 \ubaa8\ub378\uc5d0 \ub300\ud55c \uc9c8\ubb38\uacfc ENEL\uc774 \uc0dd\uc131\ud55c \uc751\ub2f5\uc774 \ud568\uaed8 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc608\uc2dc \uc9c8\ubb38\ub4e4\uc740 3D \ubaa8\ub378\uc758 \uc138\ubd80\uc801\uc778 \uc124\uba85, \ud2b9\uc9d5 \ud30c\uc545, \uc218\ub7c9 \uc9c8\ubb38 \ub4f1 \ub2e4\uc591\ud55c \uc720\ud615\uc744 \ud3ec\ud568\ud558\uba70, ENEL\uc740 \uac01 \uc9c8\ubb38\uc5d0 \ub300\ud574 \uad6c\uccb4\uc801\uc774\uace0 \ubb38\ub9e5\uc5d0 \ub9de\ub294 \ub2f5\ubcc0\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 ENEL \ubaa8\ub378\uc758 \uac15\ub825\ud55c 3D \uc774\ud574 \ub2a5\ub825\uacfc \ub2e4\uc591\ud55c \uc9c8\ubb38 \uc720\ud615\uc5d0 \ub300\ud55c \uc801\uc751\ub825\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. \uacb0\uacfc \ubc0f \uc2dc\uac01\ud654"}]