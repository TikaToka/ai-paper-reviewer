{"references": [{"fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference", "publication_date": "2024-07-29", "reason": "This paper introduces Chatbot Arena, a popular platform for evaluating large language models (LLMs) using human preferences, and its dataset is used for performance evaluation in the current work."}, {"fullname_first_author": "Yann Dubois", "paper_title": "Length-Controlled AlpacaEval: A Simple Debiasing of Automatic Evaluators", "publication_date": "2024-00-00", "reason": "This work presents AlpacaEval, another popular framework for evaluating LLMs, focusing on pairwise comparisons and debiasing automatic evaluators."}, {"fullname_first_author": "Guglielmo Faggioli", "paper_title": "Who Determines What Is Relevant? Humans or AI? Why Not Both?", "publication_date": "2024-04-01", "reason": "This paper discusses the importance of combining human and AI feedback in modern evaluation protocols for LLMs, which is the focus of Evalica."}, {"fullname_first_author": "Tianle Li", "paper_title": "From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline", "publication_date": "2024-07-29", "reason": "This work describes Arena-Hard, which builds upon Chatbot Arena and introduces an improved evaluation protocol with stricter criteria, and its implementation of Bradley-Terry model is used for performance comparisons with Evalica."}, {"fullname_first_author": "Dmitry Ustalov", "paper_title": "Learning from Crowds with Crowd-Kit", "publication_date": "2024-09-20", "reason": "This paper presents Crowd-Kit, a production-grade toolkit for quality control in crowdsourcing developed by the author of Evalica, and the experience gained in its development informed the design and development of Evalica."}]}