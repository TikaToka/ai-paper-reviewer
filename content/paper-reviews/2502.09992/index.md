---
title: "Large Language Diffusion Models"
summary: "LLaDA: ëŒ€ê·œëª¨ ì–¸ì–´ í™•ì‚° ëª¨ë¸ì´ ìê¸°íšŒê·€ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ë‹¤!"
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Natural Language Processing", "Large Language Models", "ğŸ¢ Ant Group",]
showSummary: true
date: 2025-02-14
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2502.09992 {{< /keyword >}}
{{< keyword icon="writer" >}} Shen Nie et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2025-02-17 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2502.09992" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2502.09992" target="_self" >}}
â†— Hugging Face
{{< /button >}}




### TL;DR


{{< lead >}}

ê¸°ì¡´ì˜ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì£¼ë¡œ ìê¸°íšŒê·€ ëª¨ë¸(ARM)ì— ê¸°ë°˜ì„ ë‘ê³  ìˆìŠµë‹ˆë‹¤.  ARMì€ í…ìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, í™•ì¥ì„±ê³¼ ì—­ì¶”ë¡  ëŠ¥ë ¥ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ìƒˆë¡œìš´ **í™•ì‚° ëª¨ë¸ ê¸°ë°˜ì˜ LLMì¸ LLaDA**ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.



LLaDAëŠ” **ë°ì´í„° ë§ˆìŠ¤í‚¹ ë° ì—­ë°©í–¥ ì˜ˆì¸¡** ê³¼ì •ì„ í†µí•´ í™•ë¥ ì  ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.  **ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜**ì„ í†µí•´ LLaDAëŠ” **ê¸°ì¡´ ARM ê¸°ë°˜ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” í™•ì¥ì„±ê³¼ ì„±ëŠ¥**ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. íŠ¹íˆ, ë¬¸ë§¥ í•™ìŠµ ë° ì§€ì‹œ ë”°ë¥´ê¸° ëŠ¥ë ¥ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ì—­ì¶”ë¡  ê³¼ì œì—ì„œë„ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ë›°ì–´ë‚œ ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.  ë³¸ ì—°êµ¬ëŠ” **LLM ì—°êµ¬ì— ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„**ì„ ì œì‹œí•˜ë©°, **ARMì— ëŒ€í•œ ì˜ì¡´ë„ë¥¼ ë‚®ì¶”ê³  LLMì˜ ì ì¬ë ¥ì„ ë”ìš± í™•ì¥**í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} LLaDAëŠ” **ê¸°ì¡´ ìê¸°íšŒê·€ ëª¨ë¸ ê¸°ë°˜ LLMì˜ í•œê³„ë¥¼ ê·¹ë³µ**í•˜ëŠ” **ìƒˆë¡œìš´ í™•ì‚° ëª¨ë¸ ê¸°ë°˜ LLM**ì…ë‹ˆë‹¤. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} LLaDAëŠ” **ë›°ì–´ë‚œ í™•ì¥ì„±ê³¼ ì„±ëŠ¥**ì„ ë³´ì´ë©°, **ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ê²°ê³¼**ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} ë³¸ ì—°êµ¬ëŠ” **LLM ì—°êµ¬ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„**ì„ ì œì‹œí•˜ë©°, í–¥í›„ **LLM ê°œë°œ ë° ì‘ìš© ë¶„ì•¼ í™•ì¥**ì— ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
ë³¸ ë…¼ë¬¸ì€ **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ê¸°ì¡´ íŒ¨ëŸ¬ë‹¤ì„ì„ ë›°ì–´ë„˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹**ì„ ì œì‹œí•˜ì—¬, ì—°êµ¬ìë“¤ì—ê²Œ **LLMì˜ í™•ì¥ì„±ê³¼ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±**ì„ ì œì‹œí•©ë‹ˆë‹¤.  **í™•ì‚° ëª¨ë¸ ê¸°ë°˜ì˜ LLMì¸ LLaDA**ì˜ ìš°ìˆ˜í•œ ì„±ëŠ¥ê³¼ í™•ì¥ì„±ì€ ê¸°ì¡´ì˜ ìê¸°íšŒê·€ ëª¨ë¸ ê¸°ë°˜ì˜ LLMê³¼ ë¹„êµí•˜ì—¬ **ìƒˆë¡œìš´ ì—°êµ¬ ë°©í–¥**ì„ ì œì‹œí•˜ë©°, í–¥í›„ **LLM ì—°êµ¬ì˜ ë°œì „ì— ì¤‘ìš”í•œ ì˜í–¥**ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. íŠ¹íˆ, **ë¹„ìê¸°íšŒê·€ì  íŠ¹ì„±**ì€ ê¸°ì¡´ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  **ìƒˆë¡œìš´ ê¸°ëŠ¥**ì„ ì œê³µí•˜ì—¬, ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì—ì„œì˜ LLM í™œìš© ê°€ëŠ¥ì„±ì„ ë”ìš± í™•ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

------
#### Visual Insights



![](https://arxiv.org/html/2502.09992/x1.png)

> ğŸ”¼ ê·¸ë¦¼ 1ì€ LLaDAì˜ ì œë¡œìƒ· ë° í“¨ìƒ· ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  ì—°êµ¬ì§„ì€ LLaDAë¥¼ 80ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¡œ ì²˜ìŒë¶€í„° í•™ìŠµì‹œì¼œ ê°•ë ¥í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë“¤ê³¼ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ìì²´ì ìœ¼ë¡œ êµ¬ì¶•í•œ ê¸°ì¤€ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.  LLaDAëŠ” ë¬¸ë§¥ ë‚´ í•™ìŠµ ë° ì§€ì‹œ ë”°ë¥´ê¸° ëŠ¥ë ¥ì—ì„œë„ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.  íŠ¹íˆ ì—­ìˆœ ì‹œ ì“°ê¸° ê³¼ì œì—ì„œ GPT-40ì„ ëŠ¥ê°€í•˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” LLaDAê°€ ê¸°ì¡´ì˜ ìê¸° íšŒê·€ ëª¨ë¸ì— ëŒ€í•œ ê°•ë ¥í•œ ëŒ€ì•ˆì´ ë  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 1: Zero/Few-Shot Benchmarks. We scale LLaDA to an unprecedented size of 8B parameters from scratch, achieving competitive performance with strong LLMsÂ (Dubey etÂ al., 2024).
> </details>





{{< table-caption >}}
|           | Forward | Reversal |
|-----------|---------|----------|
| GPT-4o (2024-08-06) | **82.7** | 34.3     |
| Qwen2.5 7B Instruct | 75.9    | 38.0     |
| LLaDA 8B Instruct   | 48.8    | **42.4**   |{{< /table-caption >}}

> ğŸ”¼ ë³¸ í‘œëŠ” ì‚¬ì „ í›ˆë ¨ëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë“¤ì˜ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. LLaDA 8B Base, LLaMA2 7B Base, LLaMA3 8B Base ì„¸ ëª¨ë¸ì€ ë™ì¼í•œ í”„ë¡œí† ì½œ(Appendix B.5 ì°¸ì¡°)ì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€ë˜ì—ˆìŒì„ ë‚˜íƒ€ë‚´ëŠ” * í‘œì‹œê°€ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. â€ ì™€ Â¶ë¡œ í‘œì‹œëœ ê²°ê³¼ëŠ” ê°ê° Chu et al. (2024), Yang et al. (2024), Bi et al. (2024) ë…¼ë¬¸ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒì…ë‹ˆë‹¤. ê´„í˜¸ ì•ˆì˜ ìˆ«ìëŠ” í‰ê°€ì— ì‚¬ìš©ëœ ìƒ·(shot) ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, '-'ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.  í‘œì—ëŠ” ì¼ë°˜ì ì¸ ì‘ì—…, ìˆ˜í•™ ë° ê³¼í•™, ì½”ë“œ, ì¤‘êµ­ì–´ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì— ëŒ€í•œ ì„±ëŠ¥ì´ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 1: Benchmark Results of Pre-trained LLMs. âˆ— indicates that LLaDA 8B Base, LLaMA2 7B Base, and LLaMA3 8B Base are evaluated under the same protocol, detailed in AppendixÂ B.5. Results indicated by â€  and Â¶ are sourced fromÂ Chu etÂ al. (2024); Yang etÂ al. (2024) andÂ Bi etÂ al. (2024) respectively. The numbers in parentheses represent the number of shots used for evaluation. â€œ-â€ indicates unknown data.
> </details>





### In-depth insights


#### LLaDA: Diffusion for LLMs
LLaDAëŠ” **í™•ì‚° ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í•œê³„ë¥¼ ê·¹ë³µ**í•˜ë ¤ëŠ” ì‹œë„ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ìê¸°íšŒê·€ ëª¨ë¸(ARM) ê¸°ë°˜ LLMê³¼ ë‹¬ë¦¬, LLaDAëŠ” ë°ì´í„°ì˜ í™•ë¥ ì  ë¶„í¬ë¥¼ í•™ìŠµí•˜ì—¬ **ë”ìš± ê°•ë ¥í•œ í™•ì¥ì„±ê³¼ ë›°ì–´ë‚œ ì„±ëŠ¥**ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ ì»¨í…ìŠ¤íŠ¸ í•™ìŠµ ë° ì§€ì‹œ ë”°ë¥´ê¸° ëŠ¥ë ¥ì—ì„œ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì—­ì¶”ë¡  ê³¼ì œì—ì„œë„ ê¸°ì¡´ ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ëŠ” LLaDAê°€ **ìê¸°íšŒê·€ì  íŠ¹ì„±ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ìƒì„± ëª¨ë¸ë§ì˜ ì›ë¦¬ë¥¼ í™œìš©**í•˜ì—¬ LLMì˜ í•µì‹¬ ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. **í™•ì‚° ëª¨ë¸ì˜ ì´ì **, ì¦‰ ì–‘ë°©í–¥ ì˜ì¡´ì„± ê³ ë ¤ ë° ì—­ì¶”ë¡  ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í†µí•´ LLaDAëŠ” LLMì˜ ë°œì „ì— ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ì•„ì§ **í™•ì¥ì„± ë° ì„±ëŠ¥ ë©´ì—ì„œ ê°œì„ ì˜ ì—¬ì§€**ê°€ ìˆìœ¼ë©°,  **ë‹¤ì–‘í•œ ê³¼ì œì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì—°êµ¬**ê°€ í•„ìš”í•©ë‹ˆë‹¤.  LLaDAì˜ ì„±ê³µì€ ìƒì„± ëª¨ë¸ë§ì˜ ì ì¬ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ë™ì‹œì—, í–¥í›„ LLM ì—°êµ¬ ë°©í–¥ì— ëŒ€í•œ ì¤‘ìš”í•œ ì‹œì‚¬ì ì„ ì œê³µí•©ë‹ˆë‹¤.

#### Masked Diffusion Modeling
ë§ˆìŠ¤í¬ í™•ì‚° ëª¨ë¸ë§ì€ **ìê¸° íšŒê·€ì  ì ‘ê·¼ ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì œì•ˆëœ ìƒì„± ëª¨ë¸ë§ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„**ì…ë‹ˆë‹¤.  ê¸°ì¡´ì˜ ìê¸° íšŒê·€ ëª¨ë¸ë“¤ì€ í† í°ì„ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¸í•´ í™•ì¥ì„±ê³¼ ê³„ì‚° ë¹„ìš© ë©´ì—ì„œ ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.  ë°˜ë©´ ë§ˆìŠ¤í¬ í™•ì‚° ëª¨ë¸ë§ì€ **ë°ì´í„°ë¥¼ ì ì§„ì ìœ¼ë¡œ ë§ˆìŠ¤í‚¹í•˜ëŠ” ìˆœë°©í–¥ ê³¼ì •ê³¼ ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ì˜ˆì¸¡í•˜ì—¬ ì›ë³¸ ë°ì´í„°ë¥¼ ë³µì›í•˜ëŠ” ì—­ë°©í–¥ ê³¼ì •**ì„ í†µí•´ í™•ë¥ ì  ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.  ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ **í† í° ê°„ì˜ ìƒí˜¸ ì˜ì¡´ì„±ì„ í¬ì°©**í•˜ê³  **ì–‘ë°©í–¥ ì •ë³´ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥**í•˜ê²Œ í•˜ì—¬ ìê¸° íšŒê·€ ëª¨ë¸ì˜ ë‹¨ì ì„ ê·¹ë³µí•©ë‹ˆë‹¤.  **ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì„±**ì´ ë†’ê³ , **ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ê°•ë ¥í•œ í™•ì¥ì„±ê³¼ ì„±ëŠ¥**ì„ ë³´ì—¬ì£¼ëŠ” ë“± ì—¬ëŸ¬ ì¥ì ì„ ê°€ì§€ê³  ìˆì§€ë§Œ,  **ëª¨ë¸ í›ˆë ¨ ë° ì¶”ë¡  ê³¼ì •ì˜ ë³µì¡ì„±**ì´ë¼ëŠ” ê³¼ì œë„ ìˆìŠµë‹ˆë‹¤.  í–¥í›„ ì—°êµ¬ëŠ” ì´ëŸ¬í•œ ê³¼ì œë¥¼ í•´ê²°í•˜ê³  ë§ˆìŠ¤í¬ í™•ì‚° ëª¨ë¸ë§ì˜ ì ì¬ë ¥ì„ ë”ìš± ë°œíœ˜í•˜ëŠ” ë° ì§‘ì¤‘ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

#### Scalability and Efficiency
ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ í™•ì‚° ëª¨ë¸ì˜ í™•ì¥ì„± ë° íš¨ìœ¨ì„±ì— ëŒ€í•œ ì‹¬ì¸µì ì¸ ë…¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. **ë§¤ê°œë³€ìˆ˜ì˜ ê·œëª¨ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì˜ íš¨ê³¼**ì™€ ê·¸ì— ë”°ë¥¸ ì„±ëŠ¥ í–¥ìƒ ë° ê³„ì‚° ë¹„ìš© ì¦ê°€ ì‚¬ì´ì˜ ê· í˜•ì ì„ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. **ë‹¨ìˆœíˆ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” íš¨ìœ¨ì„±ì´ ë³´ì¥ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ**, ëª¨ë¸ ì•„í‚¤í…ì²˜ ìµœì í™”, í›ˆë ¨ ì „ëµ ê°œì„ , ê·¸ë¦¬ê³  ì¶”ë¡  ê³¼ì •ì˜ íš¨ìœ¨í™” ë“± ë‹¤ì–‘í•œ ì¸¡ë©´ì—ì„œì˜ ê³ ë ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.  **ìì› ì œì•½ ì¡°ê±´ í•˜ì—ì„œ ìµœëŒ€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ì „ëµ**ì— ëŒ€í•œ ë…¼ì˜ë¥¼ í†µí•´ ì‹¤ì§ˆì ì¸ ì ìš© ê°€ëŠ¥ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  íŠ¹íˆ, **ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ í”Œë«í¼ì—ì„œì˜ ì„±ëŠ¥ ë¹„êµ** ë¶„ì„ì„ í†µí•´  í™•ì‚° ëª¨ë¸ì˜ ì‹¤ì œì ì¸ íš¨ìœ¨ì„±ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ë˜í•œ, ê¸°ì¡´ì˜ ìê¸° íšŒê·€ ëª¨ë¸ê³¼ì˜ ë¹„êµ ë¶„ì„ì„ í†µí•´ í™•ì‚° ëª¨ë¸ì˜ ê²½ìŸë ¥ì„ ëª…í™•íˆ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. **í™•ì¥ì„±ê³¼ íš¨ìœ¨ì„±ì€ ìƒí˜¸ ë³´ì™„ì ì¸ ê´€ê³„**ì´ë©°, ì´ ë‘˜ì˜ ê· í˜•ì„ ì˜ ë§ì¶°ì•¼ ì‹¤ì œ ì‘ìš© ë¶„ì•¼ì—ì„œ ì„±ê³µì ìœ¼ë¡œ í™•ì‚° ëª¨ë¸ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Instruction Following
ë…¼ë¬¸ì—ì„œ "Instruction Following"ì´ë¼ëŠ” ì œëª© ì•„ë˜ ì œì‹œëœ ë‚´ìš©ì€ **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)**ì´ ì§€ì‹œì‚¬í•­ì„ ì–¼ë§ˆë‚˜ ì˜ ë”°ë¥´ëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ” í•µì‹¬ ì§€í‘œì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì´ëŠ” ë‹¨ìˆœíˆ ëª…ë ¹ì–´ë¥¼ ì´í•´í•˜ëŠ” ìˆ˜ì¤€ì„ ë„˜ì–´, **ë³µì¡í•˜ê³  ë‹¤ì¸µì ì¸ ì§€ì‹œë¥¼ ìˆ˜í–‰í•˜ëŠ” ëŠ¥ë ¥**ê¹Œì§€ í¬í•¨í•©ë‹ˆë‹¤.  ë³¸ ë…¼ë¬¸ì€ LLaDA ëª¨ë¸ì´ ì§€ì‹œ ë”°ë¥´ê¸° ëŠ¥ë ¥ì—ì„œ ê¸°ì¡´ì˜ ìê°€ ìƒì„±ì  ëª¨ë¸ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„ì„ ê°•ì¡°í•˜ë©°, ì´ëŠ” **ìƒˆë¡œìš´ ëª¨ë¸ ì„¤ê³„** ë•ë¶„ì´ë¼ê³  ì£¼ì¥í•©ë‹ˆë‹¤.  **ë‹¤ì¤‘ íšŒì „ ëŒ€í™”**ì™€ ê°™ì€ ë³µì¡í•œ ê³¼ì œì—ì„œë„ LLaDAê°€ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì˜€ë‹¤ëŠ” ì ì€ ì£¼ëª©í•  ë§Œí•©ë‹ˆë‹¤.  ì´ëŠ” LLaDA ëª¨ë¸ì˜ **ê°•ë ¥í•œ í™•ì¥ì„±**ê³¼ **ìƒí˜¸ ì‘ìš© ëŠ¥ë ¥**ì„ ì‹œì‚¬í•˜ë©°, ê¸°ì¡´ LLMì˜ í•œê³„ë¥¼ ë›°ì–´ë„˜ëŠ” ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.  **ì—­ì¶”ë¡ (reversal reasoning)** ëŠ¥ë ¥ ë˜í•œ í‰ê°€ë˜ì—ˆëŠ”ë°, ì´ëŠ” LLaDAê°€ ìˆœì°¨ì  ì²˜ë¦¬ ë°©ì‹ì˜ í•œê³„ë¥¼ ë„˜ì–´ì„œ **ì–‘ë°©í–¥ì  ì‚¬ê³ ** ëŠ¥ë ¥ì„ ê°–ì¶”ì—ˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  ì´ëŠ” í–¥í›„ LLMì˜ ë°œì „ ë°©í–¥ì— ëŒ€í•œ ì¤‘ìš”í•œ ì‹œì‚¬ì ì„ ì œê³µí•©ë‹ˆë‹¤.

#### Future Research
ë³¸ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ í™•ì‚° ëª¨ë¸(LLaDA)ì˜ ì ì¬ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ì¤‘ìš”í•œ ì²«ê±¸ìŒì´ì§€ë§Œ, **ì—¬ì „íˆ ê°œì„ ì˜ ì—¬ì§€ê°€ ë§ë‹¤**ëŠ” ì ì„ ì¸ì§€í•´ì•¼ í•©ë‹ˆë‹¤. **í–¥í›„ ì—°êµ¬ëŠ” ëª¨ë¸ì˜ í™•ì¥ì„±ì„ ë†’ì´ê³ , ê³„ì‚° íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ë©°, ë‹¤ì–‘í•œ í•˜ë¥˜ ì‘ì—…ì—ì„œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì´ˆì ì„ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤.** íŠ¹íˆ, **ê°•í™” í•™ìŠµ(RL)ì„ í†µí•œ ì •ë ¬ ê³¼ì •**ì€ LLaDAì˜ ì§€ì‹œ ë”°ë¥´ê¸° ëŠ¥ë ¥ê³¼ ì•ˆì „ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.  ë˜í•œ, **ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹° ë°ì´í„°ë¥¼ í†µí•©**í•˜ì—¬ LLaDAì˜ ëŠ¥ë ¥ì„ ë”ìš± í™•ì¥í•˜ëŠ” ì—°êµ¬ë„ ì¤‘ìš”í•©ë‹ˆë‹¤.  **ì—­ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•œ ì—°êµ¬**ë„ í•„ìš”í•˜ë©°, ì´ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ìˆœì°¨ì  ì²˜ë¦¬ ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” ë° ë„ì›€ì´ ë  ê²ƒì…ë‹ˆë‹¤.  ë”ë¶ˆì–´, **ëª¨ë¸ì˜ í¸í–¥ì„±ì„ ì™„í™”í•˜ê³ , ì•ˆì „ì„±ì„ ê°•í™”í•˜ëŠ” ì—°êµ¬**ëŠ” ì‚¬íšŒì  ì±…ì„ì„ ë‹¤í•˜ëŠ” AI ê°œë°œì— í•„ìˆ˜ì ì…ë‹ˆë‹¤.  **ë³´ë‹¤ íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ í›ˆë ¨ ë° ì¶”ë¡  ì „ëµ** ë˜í•œ íƒêµ¬ë˜ì–´ì•¼ í•  ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.  ë§ˆì§€ë§‰ìœ¼ë¡œ, **LLaDAì™€ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì–¸ì–´ ëª¨ë¸ê³¼ì˜ ë¹„êµ ì—°êµ¬**ë¥¼ í†µí•´ ê°•ì ê³¼ ì•½ì ì„ ë¶„ì„í•˜ê³  ìƒí˜¸ ë³´ì™„ì ì¸ ë°©í–¥ì„ ëª¨ìƒ‰í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.  ì´ëŸ¬í•œ ë…¸ë ¥ë“¤ì„ í†µí•´ LLaDAëŠ” ë”ìš± ê°•ë ¥í•˜ê³  ìœ ìš©í•œ ì–¸ì–´ ëª¨ë¸ë¡œ ë°œì „í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2502.09992/x2.png)

> ğŸ”¼ ê·¸ë¦¼ 2ëŠ” LLaDAì˜ ê°œë…ì  ê°œìš”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. (a) ì‚¬ì „ í›ˆë ¨ ë‹¨ê³„ì—ì„œëŠ” 0ê³¼ 1 ì‚¬ì´ì˜ ê· ì¼í•œ í™•ë¥  ë¶„í¬(U[0,1])ë¥¼ ë”°ë¥´ëŠ” ì„ì˜ì˜ ë¹„ìœ¨ të¡œ ëª¨ë“  í† í°ì— ë…ë¦½ì ìœ¼ë¡œ ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤. (b) SFT(Supervised Fine-Tuning) ë‹¨ê³„ì—ì„œëŠ” ì‘ë‹µ í† í°ì—ë§Œ ë§ˆìŠ¤í¬ê°€ ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (c) ìƒ˜í”Œë§ ë‹¨ê³„ì—ì„œëŠ” LLaDAê°€ t=1(ì™„ì „íˆ ë§ˆìŠ¤í¬ ì²˜ë¦¬ë¨)ì—ì„œ t=0(ë§ˆìŠ¤í¬ ì²˜ë¦¬ë˜ì§€ ì•ŠìŒ)ìœ¼ë¡œ í™•ì‚° ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê³ , ìœ ì—°í•œ ë§ˆìŠ¤í¬ ì¬ì ìš© ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ê° ë‹¨ê³„ì—ì„œ ëª¨ë“  ë§ˆìŠ¤í¬ë¥¼ ë™ì‹œì— ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ LLaDAì˜ ì‚¬ì „ í›ˆë ¨, SFT ë° ìƒ˜í”Œë§ ê³¼ì •ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ì–´ LLaDAì˜ ì‘ë™ ì›ë¦¬ë¥¼ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 2: A Conceptual Overview of LLaDA. (a) Pre-training. LLaDA is trained on text with random masks applied independently to all tokens at the same ratio tâˆ¼Uâ¢[0,1]similar-toğ‘¡ğ‘ˆ01t\sim U[0,1]italic_t âˆ¼ italic_U [ 0 , 1 ]. (b) SFT. Only response tokens are possibly masked. (c) Sampling. LLaDA simulates a diffusion process from t=1ğ‘¡1t=1italic_t = 1 (fully masked) to t=0ğ‘¡0t=0italic_t = 0 (unmasked), predicting all masks simultaneously at each step with flexible remask strategies.
> </details>



![](https://arxiv.org/html/2502.09992/x3.png)

> ğŸ”¼ ê·¸ë¦¼ 8ì€ LLaDAì˜ ë°˜ìë™ íšŒê·€ì  ìƒ˜í”Œë§ ê³¼ì •ì„ ê°œë…ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ ì…ë ¥ í”„ë¡¬í”„íŠ¸ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, LLaDAê°€ ì–´ë–»ê²Œ ì—¬ëŸ¬ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìƒ˜í”Œë§ì„ ì§„í–‰í•˜ê³  ê° ë¸”ë¡ ë‚´ì—ì„œ ë§ˆìŠ¤í¬ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤. ê° ë¸”ë¡ì€ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±ë˜ë©°, ê° ë¸”ë¡ ë‚´ì—ì„œëŠ” ë§ˆìŠ¤í¬ í† í°ì˜ ì˜ˆì¸¡ í™•ë¥ ì— ë”°ë¼ ë§ˆìŠ¤í¬ë¥¼ ì¬ì¡°ì •í•˜ëŠ” ì „ëµì´ ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ë¦¼ì„ í†µí•´ LLaDAì˜ ìƒ˜í”Œë§ ë©”ì»¤ë‹ˆì¦˜ì„ ë³´ë‹¤ ëª…í™•í•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  LLaDAëŠ” ì´ëŸ¬í•œ ì „ëµì„ í†µí•´ ì–‘ë°©í–¥ ì˜ì¡´ì„±ì„ ê³ ë ¤í•˜ê³ , ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 8: A Conceptual Overview of the Semi-autoregressive Sampling.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Sampling Process |  |
|---|---| 
| User | Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour. How many kilometers can she run in 8 hours? |
| LLaDA | ![https://arxiv.org/html/2502.09992/extracted/6203716/imgs/visualization/diff_math.png](https://arxiv.org/html/2502.09992/extracted/6203716/imgs/visualization/diff_math.png) |
| Multi-round Dialogue Case |  |
| User | Could you please share with me what the initial two lines of the renowned poem â€˜The Road Not Takenâ€™? |
| LLaDA | Certainly! The initial two lines of the renowned poem â€œThe Road Not Takenâ€ by Robert Frost are: â€œTwo roads diverged in a yellow wood, And sorry I could not travel both.â€ These lines set the stage for the poem, introducing the idea of a speaker facing a choice between two paths. |
| User | Please help me translate into Chinese. |
| LLaDA | Sure! The first two lines of â€œThe Road Not Takenâ€ by Robert Frost can be translated into Chinese as: {CJK}UTF8gbsnâ€œä¸¤æ¡è·¯åˆ†å²”åœ¨é»„è‰²çš„æ ‘æ—ä¸­ï¼Œé—æ†¾æˆ‘ä¸èƒ½åŒæ—¶èµ°â€ |
| User | Please translate into German. |
| LLaDA | Sure! The first two lines of â€œThe Road Not Takenâ€ by Robert Frost can be translated into German as: â€œZwei Wege trennten sich im gelben Wald, und leider konnte ich nicht beide tun.â€ |
| User | Please also write a poem about life choices. I require 5 sentences, and the first word of each sentence starts with C as the first letter. |
| LLaDA | Certainly! Hereâ€™s a poem about life choices, with each sentence starting with the letter â€œCâ€: Choices cascade through lifeâ€™s journey, Creating connections with each decision, Careful consideration leads to better paths, Courageous choices shape our destiny, Choosing wisely guides us through the years. |{{< /table-caption >}}
> ğŸ”¼ í‘œ 2ëŠ” ì‚¬í›„ í•™ìŠµëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë“¤ì˜ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. LLaDAëŠ” ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì •(SFT)ë§Œ ì‚¬ìš©í•œ ë°˜ë©´, ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ ê°•í™” í•™ìŠµ(RL)ì„ ì¶”ê°€ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë³„í‘œ(*)ëŠ” LLaDA 8B Instruct, LLaMA2 7B Instruct, LLaMA3 8B Instructê°€ ë¶€ë¡ B.5ì— ìì„¸íˆ ì„¤ëª…ëœ ë™ì¼í•œ í”„ë¡œí† ì½œì— ë”°ë¼ í‰ê°€ë˜ì—ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. â€ ì™€ Â¶ëŠ” ê°ê° Yang et al.(2024)ê³¼ Bi et al.(2024)ì˜ ê²°ê³¼ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê´„í˜¸ ì•ˆì˜ ìˆ«ìëŠ” ë¬¸ë§¥ í•™ìŠµì— ì‚¬ìš©ëœ ìƒ·ì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, '-'ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 2: Benchmark Results of Post-trained LLMs. LLaDA only employs an SFT procedure while other models have extra reinforcement learning (RL) alignment. âˆ— indicates that LLaDA 8B Instruct, LLaMA2 7B Instruct, and LLaMA3 8B Instruct are evaluated under the same protocol, detailed in AppendixÂ B.5. Results indicated by â€  and Â¶ are sourced fromÂ Yang etÂ al. (2024) andÂ Bi etÂ al. (2024) respectively. The numbers in parentheses represent the number of shots used for in-context learning. â€œ-â€ indicates unknown data.
> </details>

{{< table-caption >}}
| | 4 steps | 5 steps | 6 steps |
|---|---|---|---|
| LLaMA3 8B Base | 38.0 | 35.0 | 34.0 |
| LLaDA 8B Base | **64.0** | **41.0** | **44.0** |{{< /table-caption >}}
> ğŸ”¼ í‘œ 3ì€ ì‹œ ì™„ì„± ì‘ì—…ì—ì„œ GPT-40, Qwen2.5 7B Instruct ë° LLaDA 8B Instruct ì„¸ ê°€ì§€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•œ í‘œì…ë‹ˆë‹¤. ê° ëª¨ë¸ì˜ ìˆœë°©í–¥(Forward) ë° ì—­ë°©í–¥(Reversal) ì‘ì—… ì •í™•ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì—­ë°©í–¥ ì‘ì—…ì€ íŠ¹íˆ ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë° ì¤‘ìš”í•˜ë©°, LLaDA 8B Instructê°€ GPT-40ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 3: Comparison in the Poem Completion Task.
> </details>

{{< table-caption >}}
| User | LLaDA |
|---|---| 
| What are the benefits of regular exercise for physical and mental health? | <img src="https://arxiv.org/html/2502.09992/extracted/6203716/imgs/visualization/sar.jpg" width="449" height="113"> |{{< /table-caption >}}
> ğŸ”¼ í‘œ 5ëŠ” LLaDAì™€ ê¸°ì¤€ ARM ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ë¥¼ ë¹„êµí•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.  1B ë° 7B íŒŒë¼ë¯¸í„° ê¸°ì¤€ ARM ëª¨ë¸ê³¼ 1B ë° 8B íŒŒë¼ë¯¸í„° LLaDA ëª¨ë¸, ê·¸ë¦¬ê³  8B íŒŒë¼ë¯¸í„° LLaMA3 ëª¨ë¸ì˜ ê³„ì¸µ ìˆ˜, ëª¨ë¸ ì°¨ì›, ì–´í…ì…˜ í—¤ë“œ ìˆ˜, ì–´íœ˜ í¬ê¸°, FFN ì°¨ì›, Key/Value í—¤ë“œ ìˆ˜, ê·¸ë¦¬ê³  ì´ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ ì œì‹œí•©ë‹ˆë‹¤. ì´ í‘œëŠ” ë³¸ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì–¸ì–´ ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ êµ¬ì„±ì„ ìì„¸í•˜ê²Œ ë³´ì—¬ì£¼ì–´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµì— ëŒ€í•œ ì´í•´ë¥¼ ë•ìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 5: Model Architecture. We provide the configurations of our 1B and 7B ARM baselines, as well as the 1B and 8B LLaDA and the 8B LLaMA3 models.
> </details>

{{< table-caption >}}
| User | LLaDA |
|---|---| 
| I want to have a pet, but I donâ€™t know whether to get a cat or a dog. Do you have any suggestions? | https://arxiv.org/html/2502.09992/extracted/6203716/imgs/visualization/multi0.png |
| Iâ€™m very busy at work, so it seems like a cat would be a more suitable option. Do you have any suggestions for getting a cat? | https://arxiv.org/html/2502.09992/extracted/6203716/imgs/visualization/multi1.png |{{< /table-caption >}}
> ğŸ”¼ ë³¸ ë…¼ë¬¸ì˜ í‘œ 6ì€ ë¦¬ë§ˆìŠ¤í‚¹(Remasking) ì „ëµì— ëŒ€í•œ ë¹„êµ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. GSM8K ì •í™•ë„ë¥¼ í‰ê°€ ì§€í‘œë¡œ ì‚¬ìš©í•˜ì—¬, ë¬´ì‘ìœ„ ë¦¬ë§ˆìŠ¤í‚¹, ìµœì € ì‹ ë¢°ë„ ë¦¬ë§ˆìŠ¤í‚¹, ê·¸ë¦¬ê³  ìµœì € ì‹ ë¢°ë„ì™€ ì¤€ìë™ íšŒê·€ ë¦¬ë§ˆìŠ¤í‚¹ì„ ë¹„êµí•©ë‹ˆë‹¤. ì¤€ìë™ íšŒê·€ ë¦¬ë§ˆìŠ¤í‚¹ ì „ëµì€ ë¸”ë¡ ë‹¨ìœ„ë¡œ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ìƒì„±í•˜ë©´ì„œ ê° ë¸”ë¡ ë‚´ì—ì„œ ìµœì € ì‹ ë¢°ë„ ë¦¬ë§ˆìŠ¤í‚¹ ì „ëµì„ ì ìš©í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì´ í‘œëŠ” ê° ë¦¬ë§ˆìŠ¤í‚¹ ì „ëµì´ LLaDA 8B ê¸°ë³¸ ëª¨ë¸ê³¼ LLaDA 8B ì§€ì‹œ ëª¨ë¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë³´ì—¬ì£¼ëŠ” ì‹¤í—˜ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 6: Ablation on Remasking. We use GSM8K accuracy as the evaluation metric. The lowest-confidence & semi-autoregressive remasking strategy refers to generating each block from left to right, while applying the lowest-confidence remasking strategy within each block.
> </details>

{{< table-caption >}}
| Daily life |  |
|---|---| 
| User | Whatâ€™s a piece of advice youâ€™ve received that youâ€™ve found helpful? |
| LLaDA | One helpful piece of advice Iâ€™ve received is to always be open to learning and growth. Iâ€™ve found that being willing to explore new ideas and experiences has helped me become more resilient and adaptable. By embracing a growth mindset, Iâ€™ve been able to identify opportunities for improvement and development, both personally and professionally. |
| Daily life |  |
| User | Please recommend me three famous movies |
| LLaDA | Certainly! Here are three famous movies that have achieved widespread popularity and critical acclaim: |
|  | 1. **The Shawshank Redemption** (1994) - Inspired by a novella and directed by Frank Darabont, this film tells the story of Andy Dufresne, a man convicted of murdering his wife, dealing with his harsh life in Shawshank Prison and forming an unlikely friendship with a fellow inmate. |
|  | 2. **The Godfather** (1972) - Directed by Francis Ford Coppola, this film is based on Mario Puzoâ€™s novel of the same name. It follows the story of the Corleone family, particularly Michael Corleone, who rises to power and takes over the familyâ€™s criminal empire. |
|  | 3. **The Empire Strikes Back** (1980) - Directed by George Lucas, this film continues the story of a young Skywalker in the Star Wars saga. It follows Luke Skywalker as he helps the Rebel Alliance fight against the Galactic Empire and destroy the powerful weapon, the Death Star. |
|  | All of these films are celebrated for their cinematic achievements, powerful storytelling, and memorable characters. |
| Translation |  |
| User | Please help me translate into Chinese: â€˜What is now proved was once only imaginedâ€™ |
| LLaDA | UTF8gbsnç°åœ¨è¢«è¯æ˜çš„æ›¾ç»åªæ˜¯æƒ³è±¡ |
| Translation |  |
| User | Please help me translate into German: â€˜What is now proved was once only imaginedâ€™ |
| LLaDA | Wer heute bestÃ¤tigt ist, war einst nur immaginiert. |
| Math |  |
| User | Emma decides to run 4 sprints 5 times a week. She runs 75 meters each sprint. How many total meters does she run a week? |
| LLaDA | Emma runs 4 sprints each time, so she runs a total of 4 x 5 = 20 sprints a week. Each sprint is 75 meters long, so Emma runs a total of 20 x 75 = 1500 meters a week. The final result is 1500 |
| Code |  |
| User | I am writing python code, I have an array, please help me sort it from largest to smallest |
| LLaDA | Certainly! You can sort an array from the largest to smallest in Python using the â€˜sort()â€™ method or the â€˜sorted()â€˜ function. Hereâ€™s an example: |
|  | ### Using â€˜sort()â€™ method: |
|  | my_array = [3, 1, 4, 1, 5, 2] |
|  | my_array.sort(reverse=True) |
|  | print(my_array) |{{< /table-caption >}}
> ğŸ”¼ í‘œ 7ì€ ìƒì„±ëœ ê¸¸ì´ì— ëŒ€í•œ ablation ì—°êµ¬ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  GSM8K ì •í™•ë„ë¥¼ í‰ê°€ ì§€í‘œë¡œ ì‚¬ìš©í–ˆìœ¼ë©°, ìƒ˜í”Œë§ ë‹¨ê³„ ìˆ˜ëŠ” ìƒì„±ëœ ê¸¸ì´ì˜ ì ˆë°˜ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.  ë‹¤ì–‘í•œ ê¸¸ì´(256, 512, 1024)ì— ëŒ€í•´ LLaDA 8B ê¸°ë³¸ ëª¨ë¸ê³¼ LLaDA 8B ì§€ì‹œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ ìƒì„± ê¸¸ì´ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 7: Ablation on Generated Length. We use the GSM8K accuracy as the evaluation metric and set the number of sampling steps to half of the generated length.
> </details>

{{< table-caption >}}
| Daily life |  |
|---|---| 
| User | How do I prepare for a job interview? |
| LLaDA | To prepare for a job interview, you should research the company and the job, practice common interview questions, and dress appropriately. |
| User | What are some common interview questions? |
| LLaDA | Some common interview questions include â€œTell me about yourselfâ€ and â€œWhy do you want to work for usâ€? |
| Daily life |  |
|---|---| 
| User | How do I start learning programming? |
| LLaDA | To learn programming, you can start by choosing a programming language that you are interested in. Some popular languages include Python, Java, and C++. |
| User | Which programming language should I learn first? |
| LLaDA | It depends on your interests and goals. Python is a great language for beginners for its simplicity and versatility. |
| User | How long does it take to learn Python? |
| LLaDA | It can take anywhere from a few weeks to several months to learn Python. |
| User | What are some good Python projects for beginners? |
| LLaDA | Some good Python projects for beginners include building a simple calculator, a game, and a web application. |
| Math |  |
|---|---| 
| User | Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? |
| LLaDA | Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold 48/2 = 24 clips in May. Altogether, Natalia sold 48 + 24 = 72 clips in April and May. The final result is 72. |
| User | Natalia sold three times as many items in June as she did in May. How many clips did Natalia sell altogether in these three months? |
| LLaDA | Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold 48/2 = 24 clips in May. Natalia sold three times as many items in June as she did in May, so she sold 24 * 3 = 72 clips in June. Altogether, Natalia sold 48 + 24 + 72 = 144 clips in these three months. The final result is 144. |{{< /table-caption >}}
> ğŸ”¼ í‘œ 8ì€ LLaDA 8B Instruct ëª¨ë¸ì˜ ì¶”ë¡  ì„¤ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ low-confidence remasking ì „ëµì„ ì ìš©í•©ë‹ˆë‹¤. ë¸”ë¡ ê¸¸ì´ê°€ ë‹µë³€ ê¸¸ì´ë³´ë‹¤ ì§§ìœ¼ë©´ semi-autoregressive remasking ì „ëµì„ ì‚¬ìš©í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. MMLUì™€ HellaSwagì˜ ê²½ìš° ë‹µë³€ ê¸¸ì´ê°€ ì§§ì€ ì´ìœ ëŠ” í•˜ë‚˜ì˜ í† í°(A, B, C, ë˜ëŠ” D)ê³¼ ë‘ ê°œì˜ íŠ¹ìˆ˜ ì¢…ê²° í† í°(ëª¨ë¸ ì‘ë‹µ ì¢…ë£Œ ë° ë¬¸ì¥ ì¢…ë£Œ)ë§Œ ìƒì„±í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 8: Inference Configuration for LLaDA 8B Instruct. We apply the low-confidence remasking strategy across all datasets. A block length smaller than the answer length indicates the use of the semi-autoregressive remasking strategy; otherwise, it is not applied. The shorter answer lengths for MMLU and HellaSwag result from their requirement to generate only a single token (i.e., A, B, C, or D) and two special end tokens (i.e., end of the model response and end of the sentence).
> </details>

{{< table-caption >}}
| Question | User | GPT-4o | LLaDA |
|---|---|---|---|
| _Question1_ |  |  |  |
|  | â€œä½†é—»äººè¯­å“â€çš„ä¸Šä¸€å¥æ˜¯ä»€ä¹ˆï¼Ÿ | â€œä½†é—»äººè¯­å“â€çš„ä¸Šä¸€å¥æ˜¯â€œæ— å¯å¥ˆä½•èŠ±è½å»â€,è¿™ä¸¤å¥å‡ºè‡ªå”ä»£è¯äººææ¸…ç…§çš„ï¼ˆæ­¦é™µæ˜¥ï¼Œæ˜¥æ™š: æ— å¯å¥ˆä½•èŠ±è½å»ï¼Œä½†é—»äººè¯­å“ã€‚è¿™é¦–è¯è¡¨è¾¾äº†ä½œè€…åœ¨æ˜¥å¤©æ—¶èŠ‚çš„å­¤ç‹¬ä¸æ„Ÿä¼¤ï¼Œå€ŸèŠ±è½å’Œäººè¯­å£°çš„æå†™ï¼Œåæ˜ å‡ºè¯äººå†…å¿ƒçš„æ— äº²ä¸å¤±è½ã€‚ | â€œä½†é—»äººè¯­å“â€çš„ä¸Šä¸€å¥æ˜¯â€œç©ºå±±ä¸è§äººâ€ |
| _Question2_ |  |  |  |
|  | â€œå¤œé™æ˜¥å±±ç©ºâ€çš„ä¸Šä¸€å¥æ˜¯ä»€ä¹ˆï¼Ÿ | â€œå¤œé™æ˜¥å±±ç©ºâ€çš„ä¸Šä¸€å¥æ˜¯â€œç©ºå±±æ–°é›¨åâ€.è¿™ä¸¤å¥å‡ºè‡ªå”ä»£è¯—äººç‹ç»´çš„ï¼ˆå±±å±…ç§‹è†œï¼‰:ç©ºå±±æ–°é›¨åï¼Œå¤œé™æ˜¥å±±ç©ºã€‚è¿™é¦–è¯—æç»˜äº†æ˜¥å¤©å±±ä¸­é›¨åçš„å¯‚é™ä¸ç©ºçµï¼Œè¡¨ç°å‡ºè‡ªç„¶çš„å®é™ä¸å’Œè°ç¾ã€‚ | â€œå¤œé™æ˜¥å±±ç©ºâ€çš„ä¸Šä¸€å¥æ˜¯â€œäººé—²æ¡‚èŠ±è½â€ |{{< /table-caption >}}
> ğŸ”¼ í‘œ 9ëŠ” iGSM ë°ì´í„°ì…‹ì—ì„œ LLaDA 8B ê¸°ë³¸ ëª¨ë¸ê³¼ LLaMA 3 ê¸°ë³¸ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•œ í‘œì…ë‹ˆë‹¤.  ìƒ˜í”Œë§ ë‹¨ê³„ë¥¼ 4, 5, 6ë‹¨ê³„ë¡œ ë³€í™”ì‹œì¼œê°€ë©° ê° ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤. iGSM ë°ì´í„°ì…‹ì€ GSM8Kì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë¬´í•œí•œ í¬ê¸°ì˜ í•©ì„± ë°ì´í„°ì…‹ì´ë©°, ë¬¸ì œì˜ ë‚œì´ë„ë¥¼ ì œì–´í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ í‘œëŠ” LLaDA ëª¨ë¸ì˜ í™•ì¥ì„±ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ëŠ” ì‹¤í—˜ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 9: Comparison on iGSM Dataset.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}