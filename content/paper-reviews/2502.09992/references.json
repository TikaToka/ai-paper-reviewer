{"references": [{"fullname_first_author": "Austin", "paper_title": "Structured denoising diffusion models in discrete state-spaces", "publication_date": "2021-12-12", "reason": "This paper introduces the foundational concept of masked diffusion models, a key component of the proposed LLaDA architecture."}, {"fullname_first_author": "Ou", "paper_title": "Your absorbing discrete diffusion secretly models the conditional distributions of clean data", "publication_date": "2024-06-06", "reason": "This paper provides theoretical justification for the use of masked diffusion models and offers insights into their underlying mathematical principles."}, {"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-12", "reason": "This foundational paper introduces the Transformer architecture, a crucial component of the LLaDA model and most modern LLMs."}, {"fullname_first_author": "Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-07", "reason": "This paper introduces the LLaMA model family, which serves as a primary baseline for comparison in the experiments."}, {"fullname_first_author": "Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-07", "reason": "This paper introduces the LLaMA 3 model, which is another important baseline against which the performance of LLaDA is measured."}]}