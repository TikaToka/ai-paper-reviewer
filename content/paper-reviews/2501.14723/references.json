{"references": [{"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-MM-DD", "reason": "This paper is highly relevant due to its focus on compute-optimal large language models, a key factor in scaling test-time compute for software engineering."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-MM-DD", "reason": "This paper is crucial because it establishes scaling laws for neural language models, which directly inform the cost analysis and resource allocation strategies in scaling test-time compute."}, {"fullname_first_author": "Neil Chowdhury", "paper_title": "Introducing SWE-bench Verified", "publication_date": "2024-MM-DD", "reason": "This paper introduces the SWE-bench Verified dataset, the benchmark used to evaluate the CodeMonkeys system's performance, therefore establishing the evaluation methodology."}, {"fullname_first_author": "Carlos E. Jimenez", "paper_title": "Swe-bench: Can language models resolve real-world github issues?", "publication_date": "2024-MM-DD", "reason": "This paper introduces SWE-bench, the primary dataset used for evaluating the CodeMonkeys system's performance, shaping the core research problem."}, {"fullname_first_author": "Paul Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-MM-DD", "reason": "This paper is relevant as it provides the foundation of reinforcement learning techniques for training LLMs, which are used to improve selection accuracy in the CodeMonkeys system."}]}