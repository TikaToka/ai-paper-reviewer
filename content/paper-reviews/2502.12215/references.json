{"references": [{"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-11-28", "reason": "This paper introduces the concept of Chain-of-Thought prompting, a crucial technique for improving reasoning capabilities in LLMs and directly related to the core topic of the current paper."}, {"fullname_first_author": "OpenAI", "paper_title": "Learning to reason with LLMs", "publication_date": "2024-12-05", "reason": "This paper introduces OpenAI's o1 series models, which are central to the study of test-time scaling in LLMs and are the primary focus of the current paper's investigation."}, {"fullname_first_author": "DeepSeek-AI", "paper_title": "Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning", "publication_date": "2025-01-01", "reason": "This paper introduces the Deepseek-R1 model, a key model used in the comparative analysis of test-time scaling capabilities, forming a significant part of the current paper's empirical evaluation."}, {"fullname_first_author": "Aman Madaan", "paper_title": "Self-refine: Iterative refinement with self-feedback", "publication_date": "2023-12-10", "reason": "This paper discusses self-revision in LLMs, a critical aspect of sequential test-time scaling that is investigated and analyzed in detail in the current paper."}, {"fullname_first_author": "Charlie Snell", "paper_title": "Scaling LLM test-time compute optimally can be more effective than scaling model parameters", "publication_date": "2024-08-01", "reason": "This paper investigates the different dimensions of test-time scaling (parallel vs. sequential), providing crucial context and background for the current paper's analysis of the two methods."}]}