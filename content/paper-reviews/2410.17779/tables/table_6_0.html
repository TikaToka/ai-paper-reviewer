<br><table id='4' style='font-size:16px'><tr><td colspan="2">Input: text Xt, image Xi, low-rank projection matrix W',</td></tr><tr><td>Output:</td><td>scales S E Zn, drop ratio 2 fused feature XI</td></tr><tr><td colspan="2"></td></tr><tr><td></td><td>1: Xl ← Tokenizer(xt)</td></tr><tr><td>2:</td><td>X v , Xv,cls ← CLIP(xi)</td></tr><tr><td></td><td>3: X ← concat( [X. v,cls, Xi]) 1</td></tr><tr><td>4:</td><td>X v ← Xv W'</td></tr><tr><td>5:</td><td>X' ← X v v</td></tr><tr><td>6:</td><td>for S in S do</td></tr><tr><td>7:</td><td>さ ← pooling(Xv, s) v,s</td></tr><tr><td>8:</td><td>← concat( [X'⌀,</td></tr><tr><td></td><td>X'o s]) v</td></tr><tr><td>9:</td><td>end for ▷ Multiscale visual prompt (Sec. III-C)</td></tr><tr><td>10:</td><td>for layer in LLM do</td></tr><tr><td>11:</td><td>Xl ← layer(Xi)</td></tr><tr><td>12: 13:</td><td>attention A ← silu(Xt)silu(X.)T ▷ Parameter-free cross- attention (Sec. III-B)</td></tr><tr><td>14:</td><td>Asorted ← torch.sort(A, dim=1)</td></tr><tr><td>15:</td><td>Index 2 ← int(y x A.size(dim=1))</td></tr><tr><td>16:</td><td>threshold T ← Asorted [:,2]</td></tr><tr><td>17:</td><td>mask M ← torch.ones. _like(A)</td></tr><tr><td>18: 19:</td><td>M [torch.where(A < T)] ← 0 Adaptine fusion (Sec. III-D)</td></tr><tr><td></td><td>A ← A · M▷</td></tr><tr><td>20:</td><td>X1 ← Xl + AX⌀T</td></tr><tr><td>21:</td><td>end for</td></tr></table>