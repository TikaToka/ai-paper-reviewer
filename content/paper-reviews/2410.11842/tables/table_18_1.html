<table id='7' style='font-size:14px'><tr><td></td><td>MoH-LLM-S 100B (LLM-S 100B)</td><td>MoH-LLM-B 100B (LLM-B 100B)</td><td>MoH-LLM-B 200B (LLM-B 200B)</td></tr><tr><td>Training budget</td><td>100B</td><td>100B</td><td>200B</td></tr><tr><td>Maximum learning rate</td><td>3e-4</td><td>5e-4</td><td>5e-4</td></tr><tr><td>Final learning rate</td><td>3e-5</td><td>5e-5</td><td>5e-5</td></tr><tr><td>LR warmup init</td><td>1e-7</td><td>1e-7</td><td>1e-7</td></tr><tr><td>LR warmup iters</td><td>2000</td><td>500</td><td>500</td></tr><tr><td>Sequence length</td><td>2048</td><td>2048</td><td>2048</td></tr><tr><td>Batch size (tokens)</td><td>4M</td><td>4M</td><td>4M</td></tr><tr><td>B for Lb</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td>Tensor parallel</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Pipeline parallel</td><td>1</td><td>1</td><td>1</td></tr></table>