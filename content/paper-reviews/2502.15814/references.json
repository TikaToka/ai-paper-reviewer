{"references": [{"fullname_first_author": "Jonas Geiping", "paper_title": "Cramming: Training a language model on a single GPU in one day", "publication_date": "2023-XX-XX", "reason": "This paper is foundational to the current work as it explores training a language model on limited resources, inspiring the current research on speech language models."}, {"fullname_first_author": "Michael Hassid", "paper_title": "Textually pretrained speech language models", "publication_date": "2024-XX-XX", "reason": "This paper introduces the TWIST model family which provides the starting point for evaluating the SLM and offers insights for pre-training."}, {"fullname_first_author": "Santiago Cuervo", "paper_title": "Scaling properties of speech language models", "publication_date": "2024-XX-XX", "reason": "This paper introduces scaling laws that are essential in determining the feasibility and computational cost of training high-quality SLMs."}, {"fullname_first_author": "Kushal Lakhotia", "paper_title": "On generative spoken language modeling from raw audio", "publication_date": "2021-XX-XX", "reason": "This paper introduces generative spoken language models, providing a starting point for developing and evaluating SLMs."}, {"fullname_first_author": "Tu Anh Nguyen", "paper_title": "SpiritLM: Interleaved spoken and written language model", "publication_date": "2025-XX-XX", "reason": "This paper introduces the novel concept of interleaving speech and text data, a technique which is explored to improve the performance of SLMs."}]}