[{"content": "| Model | Data | Data | Metric | Metric | Metric | Metric |\n|---|---|---|---|---|---|---|\n|  | Div. | Syn. | sBLIMP \u2191 | sSC \u2191 | tSC \u2191 | GenPPL \u2193 |\n| OPT125M | \u2717 | \u2713 | 55.28 | **55.46** | **75.18** | **96.8** |\n|  | \u2713 | \u2713 | 55.06 | 55.00 | 74.83 | 116.6 |\n|  | \u2717 | \u2717 | **55.88** | 54.52 | 70.82 | 160.3 |\n|  | \u2713 | \u2717 | 55.65 | 54.78 | 70.18 | 172.7 |\n| Qwen-0.5B | \u2717 | \u2713 | 56.45 | **55.59** | **78.01** | **88.3** |\n|  | \u2713 | \u2713 | 56.17 | 55.37 | 77.13 | 101.3 |\n|  | \u2717 | \u2717 | **56.60** | 53.50 | 71.14 | 145.4 |\n|  | \u2713 | \u2717 | 56.10 | 53.72 | 70.66 | 161.8 |", "caption": "Table 1: Analysing impact of training data diversity and synthetic data on SLM performance. The default Slam recipe does not use diverse data (only Libri-light and LibriSpeech), but uses the synthetic sTinyStories data.", "description": "\ud45c 1\uc740 \ub2e4\uc591\ud55c \uc74c\uc131 \ub370\uc774\ud130\uc640 \ud569\uc131 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ud55c SLM(\uc74c\uc131 \uc5b8\uc5b4 \ubaa8\ub378)\uc758 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\ubcf8 Slam \ub808\uc2dc\ud53c\ub294 \ub2e4\uc591\ud55c \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 Libri-light \ubc0f LibriSpeech \ub370\uc774\ud130\ub9cc \uc0ac\uc6a9\ud558\uc9c0\ub9cc, \ud569\uc131 sTinyStories \ub370\uc774\ud130\ub294 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \ub370\uc774\ud130 \uc870\ud569\uc5d0 \ub530\ub978 SLM \uc131\ub2a5 \uc9c0\ud45c(Div, Syn, SBLIMP\u2191, SSC, SC\u2191, GenPPL\u2193)\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc5ec\uae30\uc11c Div\ub294 \ub2e4\uc591\uc131, Syn\uc740 \ud569\uc131 \ub370\uc774\ud130 \uc0ac\uc6a9 \uc5ec\ubd80\ub97c \ub098\ud0c0\ub0b4\uace0, \ub098\uba38\uc9c0\ub294 \uc131\ub2a5 \uc9c0\ud45c\uc785\ub2c8\ub2e4.  SBLIMP, SSC, SC\ub294 \uac01\uac01 \ub192\uc744\uc218\ub85d \uc88b\uc740 \uc131\ub2a5\uc744 \uc758\ubbf8\ud558\uba70, GenPPL\uc740 \ub0ae\uc744\uc218\ub85d \uc88b\uc740 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4.2 Data"}, {"content": "| Model | Compute (GPU days) | Parameters | sBLIMP \u2191 | sStoryCloze \u2191 | tStoryCloze \u2191 | GenPPL \u2193 | Auto-BLEU \u2193 |\n|---|---|---|---|---|---|---|---| \n| TWIST-350M [Hassid et al. (2024)] | 40*V100 | 305M | 56.20 | - | - | 137.3 | 3.46 |\n| TWIST-1.3B [Hassid et al. (2024)] | 160*V100 | 1B | 57.00 | 52.4 | 70.6 | 131.8 | 3.20 |\n| TWIST-7B [Hassid et al. (2024)] | ? | 7B | 59.00 | 55.3 | 74.1 | 93.7 | 3.06 |\n| TWIST-13B [Hassid et al. (2024)] | ? | 13B | 59.20 | 55.4 | 76.4 | - | - |\n| Scaled Optimal [Cuervo and Marxer (2024)] | ? | 823M | **61.3** | **56.7** | **78.0** | - | - |\n| Predicted Optimal [Cuervo and Marxer (2024)] | 1*A5000 | 78M | 56.85 | 54.09 | 70.49 | - | - |\n| TWIST-350M (Original recipe) | 1*A5000 | 305M | 51.52 \u00b1 .19 | 53.65 \u00b1 .57 | 68.80 \u00b1 .47 | 259.2 \u00b1 6.7 | 3.26 \u00b1 .46 |\n| TWIST-350M + sTinyStories | 1*A5000 | 305M | 51.21 \u00b1 .26 | 54.17 \u00b1 .54 | 72.40 \u00b1 .18 | 159.0 \u00b1 6.0 | 4.18 \u00b1 .24 |\n| _Slam_ (-DPO) (ours) | 1*A5000 | 358M | <span class=\"ltx_text ltx_framed ltx_framed_underline\">56.45</span> \u00b1 .17 | <span class=\"ltx_text ltx_framed ltx_framed_underline\">55.59</span> \u00b1 .30 | <span class=\"ltx_text ltx_framed ltx_framed_underline\">78.01</span> \u00b1 .27 | <span class=\"ltx_text ltx_framed ltx_framed_underline\">88.3</span> \u00b1 1.0 | 3.47 \u00b1 .17 |\n| _Slam_ (ours) | 1*A5000 | 358M | **58.86** \u00b1 .20 | **58.04** \u00b1 .51 | **82.04** \u00b1 .21 | **62.8** \u00b1 4.1 | 3.88 \u00b1 .11 |", "caption": "Table 2: Comparing slamming to leading SLMs, and predicted optimal performance for the compute. We also consider TWIST-350350350350M using our code and compute budget, but with the original training recipe. \u00b1plus-or-minus\\pm\u00b1 indicates distance to min/max of 3333 seeds.", "description": "\ud45c 2\ub294 \uc81c\uc548\ub41c Slam \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \uae30\uc874 \ucd5c\uace0 \uc218\uc900\uc758 \uc74c\uc131 \uc5b8\uc5b4 \ubaa8\ub378(SLM)\ub4e4\uacfc \ube44\uad50\ud558\uace0, \uacc4\uc0b0 \uc790\uc6d0\uc5d0 \ub530\ub978 \uc608\uce21 \ucd5c\uc801 \uc131\ub2a5\uc744 \ud568\uaed8 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Slam \ubc29\ubc95\uc744 \uc800\ud76c \ucf54\ub4dc\uc640 \uacc4\uc0b0 \uc790\uc6d0\uc744 \uc0ac\uc6a9\ud558\uc5ec TWIST-350M \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0a8 \uacb0\uacfc\ub3c4 \uae30\uc874 \ud6c8\ub828 \ubc29\uc2dd\uacfc \ube44\uad50\ud558\uc5ec \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \u00b1 \uae30\ud638\ub294 3\ubc88\uc758 \uc2dc\ub4dc(seed) \uc2e4\ud589\uc5d0 \ub300\ud55c \ucd5c\uc18c\uac12\uacfc \ucd5c\ub300\uac12\uc758 \ucc28\uc774\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc989,  \ubcf8 \ud45c\ub294 \uc81c\ud55c\ub41c \uacc4\uc0b0 \uc790\uc6d0\uc73c\ub85c \uace0\ud488\uc9c8 SLM\uc744 \ud6c8\ub828\uc2dc\ud0ac \uc218 \uc788\ub294 Slam\uc758 \ud6a8\uc728\uc131\uc744 \ub2e4\uc591\ud55c \uce21\uba74\uc5d0\uc11c \ubcf4\uc5ec\uc8fc\ub294 \uacb0\uacfc\ub97c \uc885\ud569\uc801\uc73c\ub85c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "5 \ucd5c\uc885 \ub808\uc2dc\ud53c"}, {"content": "|       | GPUs        | Params | Num tokens | sBLIMP \u2191 | sStoryCloze \u2191 | tStoryCloze \u2191 | GenPPL \u2193 | Auto-BLEU \u2193 |\n|-------|--------------|--------|------------|----------|-------------|-------------|----------|------------|\n| **Speech only pre-training** |              |        |            |          |             |             |          |            |\n| GSLM [Lakhotia et al. (2021)](https://arxiv.org/html/2502.15814/bib.bib34) | 8*V100      | 100M   | 1B         | 54.2     | 53.3        | 66.6        | \u2205        | \u2205          |\n| SyllableLM [Baade et al. (2024)](https://arxiv.org/html/2502.15814/bib.bib3) | 4*A40       | 300M   | 16B        | 63.7     | \u2205           | 75.4        | \u2205        | \u2205          |\n| TWIST-350M [Hassid et al. (2024)](https://arxiv.org/html/2502.15814/bib.bib22) | 8*V100      | 305M   | 10.8B      | 56.20    | \u2205           | \u2205           | 137.3    | 3.46       |\n| TWIST-1.3B [Hassid et al. (2024)](https://arxiv.org/html/2502.15814/bib.bib22) | 32*V100     | 1B     | 10.8B      | 57.00    | 52.4        | 70.6        | 131.8    | 3.20       |\n| TWIST-7B [Hassid et al. (2024)](https://arxiv.org/html/2502.15814/bib.bib22) | 32*V100     | 7B     | 36B        | 59.00    | 55.3        | 74.1        | 93.74    | 3.06       |\n| TWIST-13B [Hassid et al. (2024)](https://arxiv.org/html/2502.15814/bib.bib22) | 32*V100     | 13B    | 36B        | 59.20    | 55.4        | 76.4        | \u2205        | \u2205          |\n| Scaled Optimal [Cuervo and Marxer (2024)](https://arxiv.org/html/2502.15814/bib.bib9) | \u2205           | 823M   | 82B        | **61.3** | 56.7        | 78.0        | \u2205        | \u2205          |\n| Moshi [D\u00e9fossez et al. (2024)](https://arxiv.org/html/2502.15814/bib.bib12) | ?*H100      | 7B     | ?          | 58.9     | **58.7**   | **81.8**   | \u2205        | \u2205          |\n| SpiritLM [Nguyen et al. (2025)](https://arxiv.org/html/2502.15814/bib.bib53) | 64*A100     | 7B     | 100B       | 58.0     | 54.8        | 72.9        | \u2205        | \u2205          |\n| **Joint speech-text pre-training / preference optimisation** |              |        |            |          |             |             |          |            |\n| [Zeng et al. (2024)](https://arxiv.org/html/2502.15814/bib.bib84) | \u2205           | 9B     | \u223c1T        | \u2205         | **62.4**   | 82.9        | \u2205        | \u2205          |\n| Moshi [D\u00e9fossez et al. (2024)](https://arxiv.org/html/2502.15814/bib.bib12) | ?*H100      | 7B     | \u223c720B      | 58.8     | 60.8        | 83.0        | \u2205        | \u2205          |\n| SpiritLM [Nguyen et al. (2025)](https://arxiv.org/html/2502.15814/bib.bib53) | 64*A100     | 7B     | 100B       | 58.3     | 61.0        | 82.9        | \u2205        | \u2205          |\n| AlignSLM-1.3B [Lin et al. (2024)](https://arxiv.org/html/2502.15814/bib.bib39) | 64*A100     | 1B     | 10.8B + \u223c158B | 59.8     | 55.0        | 80.0        | \u2205        | \u2205          |\n| AlignSLM-7B [Lin et al. (2024)](https://arxiv.org/html/2502.15814/bib.bib39) | 64*A100     | 7B     | 36B + \u223c158B | **62.3** | 61.1        | **86.8**   | \u2205        | \u2205          |\n| *Slam* (-DPO) | 2*A100      | 358M   | 16.7B      | 58.53    | 58.15       | 80.71       | 67.3     | 3.25       |\n| *Slam* | 1*A5000     | 358M   | 1.4B + 5M   | 58.86    | 58.04       | 82.04       | 62.8     | 3.88       |\n| *Slam* (scaled) | 2*A100      | 358M   | 16.7B + 9M | **61.11** | **61.30**  | **84.18**  | **46.6** | 3.75       |", "caption": "Table 3: Analysing the effect of scaling up compute for Slam. Number tokens refers to total, not necessarily unique, tokens used for training (estimated from the provided information). We separately mark DPO tokens with a +.", "description": "\ud45c 3\uc740 Slam \ubaa8\ub378 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub418\ub294 \ucef4\ud4e8\ud305 \uc790\uc6d0\uc744 \ud655\uc7a5\ud588\uc744 \ub54c\uc758 \ud6a8\uacfc\ub97c \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'Number tokens' \uc5f4\uc740 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ucd1d \ud1a0\ud070 \uc218\ub97c \ub098\ud0c0\ub0b4\uc9c0\ub9cc, \uc2e4\uc81c\ub85c\ub294 \uc911\ubcf5 \ud1a0\ud070\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc744 \uc218 \uc788\uc73c\ubbc0\ub85c, \uc81c\uacf5\ub41c \uc815\ubcf4\ub97c \ubc14\ud0d5\uc73c\ub85c \ucd94\uc815\ud55c \uac12\uc785\ub2c8\ub2e4.  '+' \ud45c\uc2dc\ub294 DPO(Direct Preference Optimization) \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ud1a0\ud070\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc989,  \ubcf8 \ud45c\ub294 \ub2e4\uc591\ud55c \uaddc\ubaa8\uc758 \ubaa8\ub378\uacfc \ucef4\ud4e8\ud305 \uc790\uc6d0\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ube44\uad50 \ubd84\uc11d\ud45c\uc785\ub2c8\ub2e4.", "section": "6 Increasing Compute"}, {"content": "| Parameter | Value |\n|---|---| \n| Text Base Model | Qwen2.5-0.5B |\n| TWIST initialisation | True |\n| Data | Librilight + Librispeech + sTinyStories |\n| Train Time | 23.5 hours \u2243 17625 steps |\n| RoPE theta | 10000 |\n| Context length | 1024 |\n| Per device Batch Size | 8 |\n| Gradient Accumulation | 16 |\n| Base Learning Rate | 1e-3 |\n| Warmup Ratio | 1% |\n| Optimizer | AdamW |\n| Learning Rate Scheduler | cosine with min 5e-5 |\n| Max Grad Norm | 0.5 |\n| Dtype | bfloat16 |", "caption": "Table 4: Slam (-DPO) Pre Training Recipe", "description": "\ud45c 4\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 Slam \ubc29\ubc95\ub860\uc758 \uc804\ucc98\ub9ac \ub2e8\uacc4(DPO \uc774\uc804)\uc5d0 \ub300\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378, \ub370\uc774\ud130, \ud559\uc2b5 \uc2dc\uac04, ROPE \uc138\ud0c0 \uac12, \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774, \ubc30\uce58 \ud06c\uae30, \uadf8\ub798\ub514\uc5b8\ud2b8 \ub204\uc801, \ud559\uc2b5\ub960, \uc6dc\uc5c5 \ube44\uc728, \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998, \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904\ub7ec, \ucd5c\ub300 \uadf8\ub798\ub514\uc5b8\ud2b8 \uc815\uaddc\ud654, \ub370\uc774\ud130 \ud0c0\uc785 \ub4f1\uc758 \uc138\ubd80\uc801\uc778 \uc124\uc815 \uac12\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 Slam \ubc29\ubc95\ub860\uc758 \uc7ac\ud604\uc131\uc744 \ub192\uc774\uace0, \ub2e4\ub978 \uc5f0\uad6c\uc790\ub4e4\uc774 \ub3d9\uc77c\ud55c \uc124\uc815\uc73c\ub85c \uc2e4\ud5d8\uc744 \uc218\ud589\ud560 \uc218 \uc788\ub3c4\ub85d \uc790\uc138\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4.1 Model & Optimisation"}, {"content": "| Parameter | Value |\n|---|---| \n| Initial Model | _Slam_ (-DPO) |\n| Data | SpokenSwag with auto-bleu smaller than 0.3 |\n| Train Time | 0.5 hour \u2243 813 steps |\n| RoPE theta | 10000 |\n| Context length | 1024 |\n| Per device Batch Size | 4 |\n| Gradient Accumulation | 16 |\n| Base Learning Rate | 5e-5 |\n| Optimizer | AdamW |\n| Learning Rate Scheduler | inverse sqrt |\n| Max Grad Norm | 0.5 |\n| Dtype | bfloat16 |\n| DPO \u03b2 | 0.1 |", "caption": "Table 5: Slam DPO Training Recipe", "description": "\ud45c 5\ub294 \ub17c\ubb38\uc758 \uc2e4\ud5d8 \uc124\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c Slam \ubaa8\ub378\uc758 DPO(Direct Preference Optimization) \ud559\uc2b5 \ub808\uc2dc\ud53c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  DPO\ub294 \ud569\uc131 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uae30\ubc95\uc785\ub2c8\ub2e4. \uc774 \ud45c\uc5d0\ub294 DPO \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ucd08\uae30 \ubaa8\ub378, \ub370\uc774\ud130, \ud559\uc2b5 \uc2dc\uac04, ROPE(Rotary Position Embedding) \u03b8 \uac12, \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774, \ubc30\uce58 \ud06c\uae30, \uadf8\ub798\ub514\uc5b8\ud2b8 \ub204\uc801, \uae30\ubcf8 \ud559\uc2b5\ub960, \ucd5c\uc801\ud654\uae30, \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904\ub7ec, \ucd5c\ub300 \uadf8\ub798\ub514\uc5b8\ud2b8 \ub188, \ub370\uc774\ud130 \ud0c0\uc785 \ub4f1\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.1 Model & Optimisation"}, {"content": "| Model Name | Number of Params |\n|---|---| \n| MobileLLM-125M [Liu et al. (2024b)](https://arxiv.org/html/2502.15814v1#bib.bib41) | 106,492,608 |\n| MobileLLM-350M [Liu et al. (2024b)](https://arxiv.org/html/2502.15814v1#bib.bib41) | 315,117,120 |\n| OPT-125M [Zhang et al. (2022)](https://arxiv.org/html/2502.15814v1#bib.bib85) | 87,015,936 |\n| OPT-350M [Zhang et al. (2022)](https://arxiv.org/html/2502.15814v1#bib.bib85) | 305,714,176 |\n| QWEN2.5-0.5B [Yang et al. (2024a)](https://arxiv.org/html/2502.15814v1#bib.bib80) | 358,347,904 |\n| SmolLM2-135M [Allal et al. (2025)](https://arxiv.org/html/2502.15814v1#bib.bib1) | 106,492,608 |\n| SmolLM2-360M [Allal et al. (2025)](https://arxiv.org/html/2502.15814v1#bib.bib1) | 315,117,120 |\n| Pythia-160M [Biderman et al. (2023)](https://arxiv.org/html/2502.15814v1#bib.bib5) | 85,827,072 |\n| Pythia-410M [Biderman et al. (2023)](https://arxiv.org/html/2502.15814v1#bib.bib5) | 303,339,520 |", "caption": "Table 6: Model names and parameter counts after changing vocabulary to speech only units (500).", "description": "\uc774 \ud45c\ub294 \uc74c\uc131 \uc804\uc6a9 \ub2e8\uc704(500\uac1c)\ub85c \uc5b4\ud718\uc9d1\uc744 \ubcc0\uacbd\ud55c \ud6c4\uc758 \ubaa8\ub378 \uc774\ub984\uacfc \ub9e4\uac1c\ubcc0\uc218 \uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378 \uc774\ub984\uacfc \ud06c\uae30 \uc815\ubcf4\ub97c \ud1b5\ud574,  \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uaddc\ubaa8\ub97c \ube44\uad50\ud558\uace0 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218\ub294 \uc74c\uc131 \uc778\uc2dd \ubc0f \uc0dd\uc131 \uc131\ub2a5\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc911\uc694\ud55c \uc694\uc18c\uc785\ub2c8\ub2e4.", "section": "B \ubaa8\ub378 \ud06c\uae30"}, {"content": "| Dataset | Number of Hours | Number of Tokens |\n|---|---|---|\n| Libri-Light [Kahn et al. (2020)](https://arxiv.org/html/2502.15814v1#bib.bib29) | 50K | 3.5B |\n| LibriSpeech [Panayotov et al. (2015)](https://arxiv.org/html/2502.15814v1#bib.bib57) | 960 | 67M |\n| SWC [Baumann et al. (2018)](https://arxiv.org/html/2502.15814v1#bib.bib4) | 750 | 19M |\n| Tedlium [Hernandez et al. (2018)](https://arxiv.org/html/2502.15814v1#bib.bib23) | 1.6K | 110M |\n| PeopleSpeech [Galvez et al. (2021)](https://arxiv.org/html/2502.15814v1#bib.bib18) | 7K | 480M |\n| VoxPopuli [Wang et al. (2021b)](https://arxiv.org/html/2502.15814v1#bib.bib74) | 24K | 1.64B |\n| sTinyStories | 30K | 2.2B |", "caption": "Table 7: Dataset train set sizes that we use.", "description": "\ud45c 7\uc740 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30 \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc774\ub984\uacfc \ud568\uaed8 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \uc2dc\uac04(\uc2dc\uac04), \ud1a0\ud070 \uc218(\uac1c\uc218)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud06c\uae30\uc640 \ud2b9\uc9d5\uc744 \uac00\uc9c4 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc74c\uc131 \uc5b8\uc5b4 \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0a4\ub294 \uc2e4\ud5d8 \uc124\uc815\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "4.2 Data"}]