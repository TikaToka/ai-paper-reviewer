{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational model for vision-language representation learning that the current research builds upon."}, {"fullname_first_author": "Christoph Schuhmann", "paper_title": "LAION-400M: Open dataset of CLIP-filtered 400 million image-text pairs", "publication_date": "2021-00-00", "reason": "This paper introduces LAION-400M, a large-scale dataset crucial for training and benchmarking vision-language models, which is used for comparison in the current work."}, {"fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "OBELICS: An open web-scale filtered dataset of interleaved image-text documents", "publication_date": "2024-00-00", "reason": "This paper provides the OBELICS dataset, the primary data source for the current research, which is a large-scale, interleaved image-text dataset."}, {"fullname_first_author": "Minwoo Byeon", "paper_title": "COYO-700M: Image-text pair dataset", "publication_date": "2022-00-00", "reason": "This paper introduces COYO-700M, another large-scale image-text dataset used for comparison, highlighting the importance of large-scale data in vision-language research."}, {"fullname_first_author": "Quan Sun", "paper_title": "EVA-CLIP: Improved training techniques for CLIP at scale", "publication_date": "2023-00-00", "reason": "This paper introduces EVA-CLIP, a model used in the current research for image embedding and semantic filtering, which is critical for improving the quality of the dataset."}]}