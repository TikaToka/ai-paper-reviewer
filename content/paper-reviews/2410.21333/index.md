---
title: "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse"
summary: "Chain-of-thought prompting can hurt LLMs' performance on tasks where human deliberation worsens accuracy; this research identifies those tasks and offers a new tool for prompt design."
categories: ["AI Generated", "ü§ó Daily Papers"]
tags: ["Natural Language Processing", "Large Language Models", "üè¢ Princeton University",]
showSummary: true
date: 2024-10-27
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2410.21333 {{< /keyword >}}
{{< keyword icon="writer" >}} Ryan Liu et el. {{< /keyword >}}
 
{{< keyword icon="hf-logo" >}} 2024-10-30 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2410.21333" target="_self" >}}
‚Üó arXiv
{{< /button >}}
&nbsp; 
{{< button href="https://huggingface.co/papers/2410.21333" target="_self" >}}
‚Üó Hugging Face
{{< /button >}}

### TL;DR


{{< lead >}}

This paper investigates the effectiveness of chain-of-thought (CoT) prompting, a widely used technique for enhancing large language models' (LLMs) performance.  While CoT has shown improvements in many areas, its impact remains an active research question, especially regarding scenarios where it negatively affects performance.  The researchers explore this by drawing parallels between human cognitive processes and LLMs, focusing on situations where verbal reasoning impairs human accuracy.

The study systematically examines six tasks across various LLM categories. They identify three task types where both human and model performance decreases with deliberation (implicit statistical learning, visual recognition, and classification with exceptions). In these scenarios, CoT significantly reduces model accuracy. In contrast, CoT benefits tasks where the constraints governing human and model performance differ. The findings highlight the need for careful consideration of task characteristics when using CoT, suggesting that it might not be universally beneficial, and providing insights into the limitations of current LLMs.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} **Chain-of-thought (CoT) prompting doesn't always improve LLM performance.**  In fact, it can significantly reduce accuracy on certain tasks. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} **Tasks where human deliberation hurts performance are good candidates for CoT failure.** This provides a useful heuristic for identifying problematic scenarios. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} **The study introduces a novel framework that links cognitive psychology and LLM evaluation.** This interdisciplinary approach offers new tools for understanding and improving LLMs. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
**This paper is crucial for researchers working with large language models (LLMs)**. It challenges the common assumption that chain-of-thought (CoT) prompting always improves performance, offering valuable insights into when CoT can be detrimental. This work **bridges cognitive psychology and machine learning**, providing a novel heuristic for understanding LLM limitations.  It also opens **new avenues for research** into prompt engineering and the development of more robust and reliable LLMs.

------
#### Visual Insights



![](https://ai-paper-reviewer.com/2410.21333/figures_16_0.png)

> üîº The figure shows six tasks evaluated to determine whether chain-of-thought prompting reduces performance, categorized by whether verbal thinking hurts human performance and whether the constraints generalizing human performance also apply to LLMs.
> <details>
> <summary>read the caption</summary>
> Figure 1: Tasks evaluated for reductions in performance from CoT prompting. Implicit Statistal Learning (ISL): Classification of strings generated by an artificial grammar. Face Recognition (FR): Recognition of a face from a set that shares similar descriptions. Classification of Data with Exceptions (CDE): Learning labels in the presence of exceptions. Natural Language Inference (NLI): Recognizing a logical inconsistency. Spatial intuitions (SI): Tilting water glasses. Working Memory (WM): Aggregating features for a decision. Humans show reductions in performance when engaging in verbal thinking in all tasks, we show that the first three have similar effects on LLMs and VLMs, while the last three differ between humans and models in meaningful ways.
> </details>





![](https://ai-paper-reviewer.com/2410.21333/charts_21_0.png)

> üîº The chart shows the learning curves of GPT-40 using direct prompting and chain-of-thought prompting on a classification task with exceptions, revealing that direct prompting leads to faster and more accurate learning than chain-of-thought prompting.
> <details>
> <summary>read the caption</summary>
> Figure 5: Aggregate learning curve (number of correct objects classified out of 10) for GPT-40 prompted via direct prompting and chain-of-thought over 15 iterations. Direct prompting attains perfection very quickly, whereas chain-of-thought prompting results in stagnation.
> </details>





{{< table-caption >}}
<br><table id='1' style='font-size:16px'><tr><td></td><td>Zero-shot</td><td>CoT</td><td>Performance decrease</td><td>p-value</td></tr><tr><td>GPT-4o (subset)</td><td>94.00%</td><td>-</td><td rowspan="2">36.30%</td><td rowspan="2">< 0.0001</td></tr><tr><td>OpenAI o1-preview (subset)</td><td>-</td><td>57.70%</td></tr><tr><td>GPT-4o</td><td>87.50%</td><td>64.40%</td><td>23.10%</td><td>< 0.0001</td></tr><tr><td>Claude 3 Opus</td><td>70.70%</td><td>62.70%</td><td>8.00%</td><td>< 0.0001</td></tr><tr><td>Claude 3.5 Sonnet</td><td>65.90%</td><td>67.70%</td><td>-1.80%</td><td>0.969</td></tr><tr><td>Gemini 1.5 Pro</td><td>68.00%</td><td>61.95%</td><td>6.05%</td><td>< 0.0001</td></tr><tr><td>Llama 3 8B Instruct</td><td>59.70%</td><td>57.90%</td><td>1.80%</td><td>< 0.05</td></tr><tr><td>Llama 3 70B Instruct</td><td>60.50%</td><td>58.30%</td><td>2.20%</td><td>< 0.05</td></tr><tr><td>Llama 3.1 8B Instruct</td><td>53.52%</td><td>51.54%</td><td>1.98%</td><td>< 0.0001</td></tr><tr><td>Llama 3.1 70B Instruct</td><td>65.90%</td><td>57.10%</td><td>8.80%</td><td>< 0.0001</td></tr></table>{{< /table-caption >}}

> üîº The table presents the performance of various LLMs on an artificial grammar learning task using zero-shot and chain-of-thought prompting, showing the significant drop in performance with CoT.
> <details>
> <summary>read the caption</summary>
> Table 1: Results contrasting zero-shot and CoT for artificial grammar learning.
> </details>





### More visual insights




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<br><table id='1' style='font-size:16px'><tr><td></td><td>Zero-shot</td><td>CoT</td><td>Performance decrease (absolute)</td><td>Performance decrease (relative)</td><td>p-value</td></tr><tr><td>GPT-4o</td><td>64.00%</td><td>51.20%</td><td>12.80%</td><td>20.00%</td><td>< 0.01</td></tr><tr><td>Claude 3 Opus</td><td>44.00%</td><td>29.60%</td><td>14.40%</td><td>32.73%</td><td>< 0.0001</td></tr><tr><td>Claude 3.5 Sonnet</td><td>97.80%</td><td>94.80%</td><td>3.00%</td><td>3.07%</td><td>< 0.05</td></tr><tr><td>Gemini 1.5 Pro</td><td>66.00%</td><td>54.60%</td><td>11.40%</td><td>17.27%</td><td>< 0.05</td></tr><tr><td>Intern VL2 26B</td><td>9.20%</td><td>6.00%</td><td>3.20%</td><td>34.78%</td><td>< 0.05</td></tr><tr><td>Intern VL2 Llama3 76B</td><td>15.77%</td><td>13.77%</td><td>2.00%</td><td>12.68%</td><td>0.44</td></tr></table>{{< /table-caption >}}
> üîº The table shows the performance of various large multimodal models on a facial recognition task using zero-shot and chain-of-thought prompting, revealing consistent drops in accuracy for all models when using CoT.
> <details>
> <summary>read the caption</summary>
> Table 2: Comparison of zero-shot and CoT prompts for facial recognition.
> </details>

{{< table-caption >}}
<br><table id='1' style='font-size:20px'><tr><td></td><td>Direct</td><td>CoT</td><td># Rounds increase (absolute)</td><td># Rounds increase (relative)</td><td>p-value</td></tr><tr><td>GPT-4o</td><td>2.9</td><td>12.5</td><td>9.6</td><td>331%</td><td>< 0.0001</td></tr><tr><td>Claude 3.5 Sonnet</td><td>2.3</td><td>6.4</td><td>4.1</td><td>178%</td><td>< 0.0001</td></tr><tr><td>Claude 3 Opus</td><td>2.4</td><td>5.5</td><td>3.1</td><td>129%</td><td>< 0.05</td></tr></table>{{< /table-caption >}}
> üîº The table presents the average number of rounds needed for three large language models to correctly classify all vehicles in a dataset using direct prompting versus chain-of-thought prompting, showing a significant increase in rounds needed with chain-of-thought.
> <details>
> <summary>read the caption</summary>
> Table 3: Average number of rounds for models to learn labels using either direct or CoT prompting.
> </details>

{{< table-caption >}}
<br><table id='1' style='font-size:14px'><tr><td></td><td colspan="2">MNLI</td><td colspan="2">SNLI</td><td colspan="2">Synthetic</td></tr><tr><td></td><td>Zero-shot</td><td>CoT</td><td>Zero-shot</td><td>CoT</td><td>Zero-shot</td><td>CoT</td></tr><tr><td>OpenAI o1-preview (subset)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>86.5%</td></tr><tr><td>GPT-4o</td><td>53.2%</td><td>93.9%</td><td>51.4%</td><td>94.3%</td><td>51.0%</td><td>74.0%</td></tr><tr><td>Claude 3.5 Sonnet</td><td>65.2%</td><td>67.5%</td><td>67.4%</td><td>69.8%</td><td>56.7%</td><td>57.8%</td></tr><tr><td>Claude 3 Opus</td><td>62.7%</td><td>58.8%</td><td>66.2%</td><td>58.7%</td><td>54.5%</td><td>51.8%</td></tr><tr><td>Gemini 1.5 Pro</td><td>73.2%</td><td>68.2%</td><td>68.8%</td><td>63.9%</td><td>60.5%</td><td>61.5%</td></tr><tr><td>Llama 3.1 70B Instruct</td><td>55.6%</td><td>81.6%</td><td>50.4%</td><td>82.3%</td><td>50.0%</td><td>65.8%</td></tr></table>{{< /table-caption >}}
> üîº The table presents the zero-shot and chain-of-thought performance of several LLMs on a logical inconsistency task, using stimuli from three different datasets.
> <details>
> <summary>read the caption</summary>
> Table 4: Results comparing zero-shot and CoT across the logical inconsistency task using stimuli from MNLI, SNLI, and synthetic LLM generation.
> </details>

{{< table-caption >}}
<br><table id='3' style='font-size:18px'><tr><td></td><td>Zero-shot</td><td>CoT</td><td>Performance change (absolute)</td><td>Performance change (relative)</td><td>p-value</td></tr><tr><td>GPT-4o</td><td>38%</td><td>40%</td><td>+2%</td><td>+5.00%</td><td>0.61</td></tr><tr><td>Claude 3.5 Sonnet</td><td>42%</td><td>38%</td><td>-4%</td><td>-10.53%</td><td>0.28</td></tr><tr><td>Claude 3 Opus</td><td>42%</td><td>38%</td><td>-4%</td><td>-10.53%</td><td>0.28</td></tr><tr><td>Gemini 1.5 Pro</td><td>35%</td><td>36%</td><td>+1%</td><td>+2.78%</td><td>0.99</td></tr><tr><td>InternVL2 Llama3 76B</td><td>39%</td><td>31%</td><td>-8%</td><td>-25.81%</td><td>0.67</td></tr></table>{{< /table-caption >}}
> üîº Table 5 shows the comparison of zero-shot and chain-of-thought prompting on the spatial intuition task, indicating performance change (absolute and relative) and p-values for various models.
> <details>
> <summary>read the caption</summary>
> Table 5: Results comparing zero-shot and CoT on the spatial intuition task.
> </details>

{{< table-caption >}}
<br><table id='1' style='font-size:16px'><tr><td></td><td colspan="2">[0.1, 0.3]</td><td colspan="2">[0.3, 0.5]</td><td colspan="2">[0.5, 1]</td></tr><tr><td></td><td>Zero-shot</td><td>CoT</td><td>Zero-shot</td><td>CoT</td><td>Zero-shot</td><td>CoT</td></tr><tr><td>GPT-4o</td><td>47%</td><td>45%</td><td>57%</td><td>56%</td><td>80%</td><td>87%</td></tr><tr><td>Claude 3.5 Sonnet</td><td>50%</td><td>62%</td><td>62%</td><td>72%</td><td>81%</td><td>95%</td></tr><tr><td>Claude 3 Opus</td><td>35%</td><td>50%</td><td>57%</td><td>58%</td><td>72%</td><td>84%</td></tr><tr><td>Llama 3.1 70B Instruct</td><td>42%</td><td>6%</td><td>44%</td><td>5%</td><td>43%</td><td>20%</td></tr></table>{{< /table-caption >}}
> üîº The table presents the zero-shot and chain-of-thought prompting performance of four LLMs on an apartment selection task, categorized by three ranges of apartment quality differences.
> <details>
> <summary>read the caption</summary>
> Table 6: Results for apartment selection task across four models and three ranges of Œî.
> </details>

{{< table-caption >}}
<table id='4' style='font-size:20px'><tr><td>Table 7: Example prompt for artificial grammar learning task, zero shot.</td></tr><tr><td>Prompt:</td></tr><tr><td>These strings were generated according to a certain set of rules. Does the following string also follow the same set of rules?</td></tr><tr><td>[test example]</td></tr><tr><td>Please ONLY answer "Yes" or "No".</td></tr></table>{{< /table-caption >}}
> üîº The table presents a comparison of the performance of various large language models on an artificial grammar learning task using zero-shot prompting versus chain-of-thought prompting, showing a significant performance decrease with CoT prompting.
> <details>
> <summary>read the caption</summary>
> Table 1: Results contrasting zero-shot and CoT for artificial grammar learning.
> </details>

{{< table-caption >}}
<br><table id='10' style='font-size:16px'><tr><td></td><td>Zero-shot</td><td>CoT</td><td>ToT</td><td>Performance decrease (CoT)</td><td>Performance decrease (ToT)</td><td>p-value (CoT)</td><td>p-value (ToT)</td></tr><tr><td>GPT-4o</td><td>94.00%</td><td>62.52%</td><td>64.55%</td><td>31.48%</td><td>29.45%</td><td>< 0.0001</td><td>< 0.0001</td></tr></table>{{< /table-caption >}}
> üîº The table presents the performance of GPT-40 model on artificial grammar learning task using zero-shot, chain-of-thought, and tree-of-thought prompting methods, showing the accuracy and p-values for each method.
> <details>
> <summary>read the caption</summary>
> Table 8: Results comparing zero-shot, CoT, and ToT on a subset of the artificial grammar learning task.
> </details>

{{< table-caption >}}
<table id='0' style='font-size:14px'><tr><td>Unique features</td><td>Pattern-related features</td><td colspan="3">Irrelevant features</td></tr><tr><td>License Plate</td><td>'Cold' (Class A)/'Warm' (Class B) climate</td><td>Transmission</td><td>Seat covers</td><td>Doors</td></tr><tr><td>A23BCD</td><td>Drives on glaciers</td><td>Manual</td><td>Cloth</td><td>Two</td></tr><tr><td>B34EFG</td><td>Made in Norway</td><td>Automatic</td><td>Vinyl</td><td>Two</td></tr><tr><td>C45HIJ</td><td>Used in mountain climbing</td><td>Automatic</td><td>Vinyl</td><td>Four</td></tr><tr><td>D56KLM</td><td>Drives in jungles</td><td>Manual</td><td>Vinyl</td><td>Four</td></tr><tr><td>E67NOP</td><td>Has treads</td><td>Manual</td><td>Cloth</td><td>Two</td></tr><tr><td>F78QRS</td><td>Heavily insulated</td><td>Manual</td><td>Vinyl</td><td>Four</td></tr><tr><td>G89TUV</td><td>Made in Africa</td><td>Manual</td><td>Cloth</td><td>Four</td></tr><tr><td>H90WXY</td><td>Has wheels</td><td>Automatic</td><td>Cloth</td><td>Two</td></tr><tr><td>J12ZAB</td><td>Lightly insulated</td><td>Manual</td><td>Vinyl</td><td>Two</td></tr><tr><td>K23CDE</td><td>Used on safaris</td><td>Automatic</td><td>Vinyl</td><td>Two</td></tr></table>{{< /table-caption >}}
> üîº The table presents the performance of nine different language models on an artificial grammar learning task, comparing their accuracy with zero-shot prompting versus chain-of-thought prompting, showing consistent performance decrease with CoT prompting.
> <details>
> <summary>read the caption</summary>
> Table 1: Results contrasting zero-shot and CoT for artificial grammar learning.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2410.21333/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/21.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/22.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/23.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/24.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.21333/25.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}