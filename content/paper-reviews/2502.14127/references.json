{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a comprehensive technical report on GPT-4, a highly influential large language model, providing valuable context for evaluating LLMs."}, {"fullname_first_author": "Rishi Bommasani", "paper_title": "Holistic evaluation of language models", "publication_date": "2023-MM-DD", "reason": "This paper offers a broad perspective on LLM evaluation, highlighting the need for a more comprehensive approach that goes beyond single metrics."}, {"fullname_first_author": "Frank B Baker", "paper_title": "The basics of item response theory", "publication_date": "2001-MM-DD", "reason": "This paper introduces Item Response Theory (IRT), a statistical model crucial for understanding and improving the quality of MCQs used in educational and LLM evaluation."}, {"fullname_first_author": "Thomas M Haladyna", "paper_title": "A taxonomy of multiple-choice item-writing rules", "publication_date": "1989-MM-DD", "reason": "This paper provides a detailed taxonomy of rules for writing effective multiple-choice questions, essential for building high-quality datasets for LLM evaluation."}, {"fullname_first_author": "Rowan Zellers", "paper_title": "SWAG: A large-scale adversarial dataset for grounded commonsense inference", "publication_date": "2018-MM-DD", "reason": "This paper introduces the SWAG dataset, which has significantly influenced the development of datasets and benchmarks for evaluating common sense reasoning in LLMs."}]}