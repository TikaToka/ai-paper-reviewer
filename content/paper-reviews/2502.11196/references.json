{"references": [{"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-12-06", "reason": "This paper introduces a novel method for training generalized multi-query transformer models, which is relevant to the paper's focus on improving continual pre-training strategies for LLMs."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "Physics of language models: Part 3.2, knowledge manipulation", "publication_date": "2023-09-14", "reason": "This paper provides a theoretical framework for understanding knowledge manipulation in LLMs, which is directly relevant to the paper's investigation of how LLMs acquire new knowledge."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "Physics of language models: Part 3.1, knowledge storage and extraction", "publication_date": "2024-07-21", "reason": "This paper delves into the mechanisms of knowledge storage and extraction in LLMs, providing a foundation for the current paper's analysis of knowledge circuit evolution."}, {"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This seminal work demonstrates the few-shot learning capabilities of LLMs, which is crucial context for the paper's focus on enhancing LLMs' ability to acquire new knowledge."}, {"fullname_first_author": "Damai Dai", "paper_title": "Knowledge neurons in pretrained transformers", "publication_date": "2022-05-22", "reason": "This paper introduces the concept of \"knowledge neurons\" in transformers, providing a foundation for the current work's concept of \"knowledge circuits\"."}]}