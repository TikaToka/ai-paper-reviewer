{"references": [{"fullname_first_author": "Vladimir Arkhipkin", "paper_title": "Kandinsky 3.0 technical report", "publication_date": "2023-12-03", "reason": "This paper describes Kandinsky 3.0, a key text-guided image-to-image diffusion model used in the experiments."}, {"fullname_first_author": "Tim Brooks", "paper_title": "Instructpix2pix: Learning to follow image editing instructions", "publication_date": "2023-00-00", "reason": "This paper introduces InstructPix2Pix, a relevant text-guided image-to-image diffusion model that is compared to the proposed method."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-00-00", "reason": "This paper introduces Stable Diffusion, a foundational model for the research and is extensively analyzed in the experiments."}, {"fullname_first_author": "Zo\u00eb Papakipos", "paper_title": "Results and findings of the 2021 image similarity challenge", "publication_date": "2022-00-00", "reason": "This paper introduces the DISC21 dataset, which is the source of the images for the OriPID dataset used in the experiments."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a vision-language model that is compared to the proposed method for origin identification."}]}