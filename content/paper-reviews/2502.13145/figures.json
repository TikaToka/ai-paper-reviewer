[{"figure_path": "https://arxiv.org/html/2502.13145/x2.png", "caption": "Figure 1: \nComprehensive comparison of mmMamba.\n(a) Our mmMamba can build linear-complexity and hybrid decoder-only VLM by distilling the knowledge in Transformer to Mamba-2.\n(b) By distilling from the quadratic-complexity decoder-only VLM HoVLE, our mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs with fewer parameters (e.g., 2\u00d7\\times\u00d7 fewer than EVE-7B), while mmMamba-hybrid surpasses them across all benchmarks and approaches the teacher model HoVLE\u2019s performance.\n(c)-(d) We compare the speed and memory of mmMamba-linear and mmMamba-hybrid with the teacher model HoVLE on the same single NVIDIA 4090 GPU. mmMamba-linear maintains consistently low latency and memory usage, while mmMamba-hybrid\u2019s resource consumption scales significantly better than HoVLE. At 103K tokens, mmMamba-linear demonstrates 20.6\u00d7\\times\u00d7 speedup compared to HoVLE and saves 75.8% GPU memory, while mmMamba-hybrid achieves 13.5\u00d7\\times\u00d7 speedup and saves 60.2% GPU memory.", "description": "\uadf8\ub9bc 1\uc740 mmMamba\uc758 \uc131\ub2a5\uc744 \uc885\ud569\uc801\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 Transformer\uc758 \uc9c0\uc2dd\uc744 Mamba-2\ub85c \uc99d\ub958\ud558\uc5ec \uc120\ud615 \ubcf5\uc7a1\ub3c4 \ubc0f \ud558\uc774\ube0c\ub9ac\ub4dc \ub514\ucf54\ub354 \uc804\uc6a9 VLM\uc744 \uad6c\ucd95\ud558\ub294 mmMamba\uc758 \uad6c\uc870\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (b)\ub294 \uc774\ucc28 \ubcf5\uc7a1\ub3c4\uc758 \ub514\ucf54\ub354 \uc804\uc6a9 VLM\uc778 HoVLE\ub85c\ubd80\ud130 \uc99d\ub958\ud558\uc5ec, mmMamba-linear\uac00 \uae30\uc874\uc758 \uc120\ud615 \ubc0f \uc774\ucc28 \ubcf5\uc7a1\ub3c4 VLM\uc5d0 \ube44\ud574 \uacbd\uc7c1\ub825 \uc788\ub294 \uc131\ub2a5\uc744 \ub354 \uc801\uc740 \ub9e4\uac1c\ubcc0\uc218\ub85c \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4 (\uc608: EVE-7B\ubcf4\ub2e4 2\ubc30 \uc801\uc74c). mmMamba-hybrid\ub294 \ubaa8\ub4e0 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uae30\uc874 \ubaa8\ub378\ub4e4\uc744 \ub2a5\uac00\ud558\uace0, teacher \ubaa8\ub378\uc778 HoVLE\uc758 \uc131\ub2a5\uc5d0 \uadfc\uc811\ud569\ub2c8\ub2e4. (c)\uc640 (d)\ub294 \ub3d9\uc77c\ud55c \ub2e8\uc77c NVIDIA 4090 GPU\uc5d0\uc11c mmMamba-linear\uc640 mmMamba-hybrid\uc758 \uc18d\ub3c4\uc640 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 teacher \ubaa8\ub378\uc778 HoVLE\uc640 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. mmMamba-linear\ub294 \uc77c\uad00\ub418\uac8c \ub0ae\uc740 \uc9c0\uc5f0 \uc2dc\uac04\uacfc \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \uc720\uc9c0\ud558\ub294 \ubc18\uba74, mmMamba-hybrid\uc758 \uc790\uc6d0 \uc18c\ube44\ub294 HoVLE\ubcf4\ub2e4 \ud6e8\uc52c \ud6a8\uc728\uc801\uc73c\ub85c \ud655\uc7a5\ub429\ub2c8\ub2e4. 103K \ud1a0\ud070\uc5d0\uc11c mmMamba-linear\ub294 HoVLE\uc5d0 \ube44\ud574 20.6\ubc30\uc758 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ubcf4\uc774\uace0 GPU \uba54\ubaa8\ub9ac\ub97c 75.8% \uc808\uc57d\ud558\ub294 \ubc18\uba74, mmMamba-hybrid\ub294 13.5\ubc30\uc758 \uc18d\ub3c4 \ud5a5\uc0c1\uacfc 60.2%\uc758 GPU \uba54\ubaa8\ub9ac \uc808\uc57d\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13145/x3.png", "caption": "Figure 2: \nInitialize Mamba-2 from Transformer. By comparing the mechanism similarity in Sec.\u00a03, we directly inherit \ud835\udc7eQsubscript\ud835\udc7e\ud835\udc44\\boldsymbol{W}_{Q}bold_italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT, \ud835\udc7eKsubscript\ud835\udc7e\ud835\udc3e\\boldsymbol{W}_{K}bold_italic_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT, \ud835\udc7eVsubscript\ud835\udc7e\ud835\udc49\\boldsymbol{W}_{V}bold_italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT, \ud835\udc7eOsubscript\ud835\udc7e\ud835\udc42\\boldsymbol{W}_{O}bold_italic_W start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT parameters (blue) from trained Transformer layer and carefully initialize the extra parameters (orange) including a\ud835\udc4eaitalic_a, \ud835\udc7e\u03b3subscript\ud835\udc7e\ud835\udefe\\boldsymbol{W}_{\\gamma}bold_italic_W start_POSTSUBSCRIPT italic_\u03b3 end_POSTSUBSCRIPT, \ud835\udc7econvsubscript\ud835\udc7econv\\boldsymbol{W}_{\\text{conv}}bold_italic_W start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT, and \ud835\udc7eGsubscript\ud835\udc7e\ud835\udc3a\\boldsymbol{W}_{G}bold_italic_W start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT in Mamba-2 to initially mimic the Transformer\u2019s behavior, providing a strong foundation for subsequent distillation.", "description": "\uadf8\ub9bc 2\ub294 Transformer\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c Mamba-2\ub85c \ucd08\uae30\ud654\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubcf8 \ub17c\ubb38\uc758 3\uc7a5\uc5d0\uc11c \uc124\uba85\ud558\ub294 Transformer\uc640 Mamba-2 \uba54\ucee4\ub2c8\uc998\uc758 \uc720\uc0ac\uc131\uc744 \ubc14\ud0d5\uc73c\ub85c, Transformer \uacc4\uce35\uc5d0\uc11c \ud835\udc7eQ, \ud835\udc7eK, \ud835\udc7eV, \ud835\udc7eO \ub9e4\uac1c\ubcc0\uc218\ub4e4\uc744 Mamba-2\ub85c \uc9c1\uc811 \uc774\uc5b4\ubc1b\uc2b5\ub2c8\ub2e4.  \ub098\uba38\uc9c0 \ucd94\uac00\uc801\uc778 \ub9e4\uac1c\ubcc0\uc218(a, \ud835\udc7e\u03b3, \ud835\udc7econv, \ud835\udc7eG)\ub294 Transformer\uc758 \ub3d9\uc791\uc744 \ucd08\uae30\uc801\uc73c\ub85c \ubaa8\ubc29\ud558\ub3c4\ub85d \uc2e0\uc911\ud558\uac8c \ucd08\uae30\ud654\ud558\uc5ec, \ud6c4\uc18d \uc99d\ub958 \uacfc\uc815\uc5d0 \ub300\ud55c \uacac\uace0\ud55c \ud1a0\ub300\ub97c \ub9c8\ub828\ud569\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ud30c\ub780\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub41c \uae30\uc874 \ub9e4\uac1c\ubcc0\uc218\uc640 \uc8fc\ud669\uc0c9\uc73c\ub85c \ud45c\uc2dc\ub41c \ucd94\uac00 \ub9e4\uac1c\ubcc0\uc218\ub97c \uba85\ud655\ud558\uac8c \uad6c\ubd84\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Method"}]