{"references": [{"fullname_first_author": "Dao, T.", "paper_title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality", "publication_date": "2024-05-21", "reason": "This paper introduces the Mamba-2 model, a key component of the proposed mmMamba architecture, providing the foundational linear-complexity sequence modeling mechanism."}, {"fullname_first_author": "Chen, Y.", "paper_title": "A single transformer for scalable vision-language modeling", "publication_date": "2024-07-06", "reason": "This work is highly relevant due to its exploration of linear-complexity vision-language models, which is directly addressed and improved by mmMamba."}, {"fullname_first_author": "Tao, C.", "paper_title": "HoVLE: Unleashing the power of monolithic vision-language models with holistic vision-language embedding", "publication_date": "2024-12-16", "reason": "The teacher model used in mmMamba's distillation process; HoVLE's strong performance and decoder-only architecture serve as a benchmark and starting point."}, {"fullname_first_author": "Bavishi, R.", "paper_title": "Introducing our multimodal models", "publication_date": "2023-00-00", "reason": "This paper showcases the potential of decoder-only VLMs, influencing mmMamba's design choice and demonstrating the feasibility of a purely decoder-based approach."}, {"fullname_first_author": "Gu, A.", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-00", "reason": "This paper introduces the original Mamba model, which is a predecessor to Mamba-2 and provides a foundation for understanding the evolution of linear-time sequence modeling."}]}