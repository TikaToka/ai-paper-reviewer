{"references": [{"fullname_first_author": "Bai", "paper_title": "LongWriter: Unleashing 10,000+ word generation from long context LLMs", "publication_date": "2024-08-00", "reason": "This paper is a direct competitor to the proposed LongDPO method, focusing on improving long-form text generation capabilities in LLMs and thus provides a strong baseline for comparison."}, {"fullname_first_author": "Zhang", "paper_title": "LongReward: Improving long-context large language models with AI feedback", "publication_date": "2024-10-00", "reason": "This paper addresses the limitation of outcome supervision in long-form generation by introducing a novel reward model, LongReward, which directly informs the model about the desired length and quality of the output, providing a related approach to the current work."}, {"fullname_first_author": "Pham", "paper_title": "Suri: Multi-constraint instruction following in long-form text generation", "publication_date": "2024-11-00", "reason": "This paper tackles long-form text generation through a method which is similar to LongDPO, suggesting the importance of outcome supervision and provides an alternative approach to the proposed LongDPO."}, {"fullname_first_author": "Browne", "paper_title": "A survey of Monte Carlo tree search methods", "publication_date": "2012-00-00", "reason": "This foundational paper provides the theoretical framework for Monte Carlo Tree Search (MCTS), which is a core component of the proposed LongDPO, showing its importance in exploring the search space and making optimal decisions in complex scenarios."}, {"fullname_first_author": "Xie", "paper_title": "Monte Carlo tree search boosts reasoning via iterative preference learning", "publication_date": "2024-05-00", "reason": "This paper demonstrates the effectiveness of MCTS in improving reasoning capabilities, which is relevant to the current work since LongDPO leverages MCTS to enhance its ability to select high-quality stepwise preference pairs for long-form text generation."}]}