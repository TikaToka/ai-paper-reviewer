{"references": [{"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to the field of aligning language models with human values and introduces Reinforcement Learning from Human Feedback (RLHF), a key concept discussed in the target paper."}, {"fullname_first_author": "Rafailov, R.", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-05-01", "reason": "This paper introduces Direct Preference Optimization (DPO), a core algorithm in Direct Alignment Algorithms (DAAs) and a major focus of the target paper's comparative analysis."}, {"fullname_first_author": "Hong, J.", "paper_title": "Orpo: Monolithic preference optimization without reference model", "publication_date": "2024-03-01", "reason": "This paper introduces Odds Ratio Preference Optimization (ORPO), a significant DAA discussed and analyzed extensively in the target paper."}, {"fullname_first_author": "Wang, R.", "paper_title": "ASFT: Aligned Supervised Fine-Tuning through absolute likelihood", "publication_date": "2024-09-01", "reason": "This paper details Aligned Supervised Fine-Tuning (ASFT), another key DAA compared and contrasted in the target paper's experimental analysis."}, {"fullname_first_author": "Stiennon, N.", "paper_title": "Learning to summarize from human feedback", "publication_date": "2020-12-01", "reason": "This paper is among the earliest works demonstrating the use of human feedback in fine-tuning language models, providing a historical context and foundational techniques for the methods explored in the target paper."}]}