---
title: "The Differences Between Direct Alignment Algorithms are a Blur"
summary: "μ§μ ‘ μ •λ ¬ μ•κ³ λ¦¬μ¦(DAA)μ μ„±λ¥ μ°¨μ΄ ν•΄μ†: SFT λ‹¨κ³„ μ¶”κ°€ λ° Ξ² νλΌλ―Έν„° λ„μ…μΌλ΅ μ„±λ¥ ν–¥μƒ λ‹¬μ„±!"
categories: ["AI Generated", "π¤— Daily Papers"]
tags: ["Natural Language Processing", "Large Language Models", "πΆ T-Tech",]
showSummary: true
date: 2025-02-03
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2502.01237 {{< /keyword >}}
{{< keyword icon="writer" >}} Alexey Gorbatovski et el. {{< /keyword >}}
 
{{< keyword >}} π¤— 2025-02-04 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2502.01237" target="_self" >}}
β†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2502.01237" target="_self" >}}
β†— Hugging Face
{{< /button >}}




### TL;DR


{{< lead >}}

λ³Έ λ…Όλ¬Έμ€ λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ(LLM)μ„ μΈκ°„μ κ°€μΉμ— λ§μ¶”λ” κ³Όμ •μΈ **λ¨λΈ μ •λ ¬**μ—μ„ μµκ·Ό μ£Όλ©λ°›κ³  μλ” **μ§μ ‘ μ •λ ¬ μ•κ³ λ¦¬μ¦(DAA)**μ— λ€ν• μ—°κµ¬μ…λ‹λ‹¤.  κΈ°μ΅΄ DAAλ“¤μ€ μ΄λ΅ μ  μ„¤κ³„, κµ¬ν„ μ„Έλ¶€ μ‚¬ν•­, SFT(Supervised Fine-Tuning) λ‹¨κ³„ μ λ¬΄ λ“±μ—μ„ μ°¨μ΄λ¥Ό λ³΄μ΄λ©°, μ΄λ΅ μΈν•΄ μ„±λ¥ λΉ„κµ λ° μµμ ν™”κ°€ μ–΄λ ¤μ› μµλ‹λ‹¤. νΉν, λ‹¨μΌ λ‹¨κ³„ λ°©μ‹κ³Ό 2λ‹¨κ³„ λ°©μ‹μ μ„±λ¥ μ°¨μ΄μ— λ€ν• λ…ν™•ν• λ¶„μ„μ΄ λ¶€μ΅±ν–μµλ‹λ‹¤.

λ³Έ μ—°κµ¬μ—μ„λ” **λ…μ‹μ μΈ SFT λ‹¨κ³„λ¥Ό λ‹¨μΌ λ‹¨κ³„ DAAμ— ν†µν•©**ν•κ³ , **Ξ² νλΌλ―Έν„°λ¥Ό λ„μ…**ν•μ—¬ μ•κ³ λ¦¬μ¦μ„ μΌλ°ν™”ν–μµλ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ λ‹¨μΌ λ‹¨κ³„ λ°©λ²•(ORPO, ASFT)μ μ„±λ¥μ„ 2λ‹¨κ³„ λ°©λ²• μμ¤€μΌλ΅ ν–¥μƒμ‹μΌ°μΌλ©°, μ„±λ¥ μ°¨μ΄μ μ£Όμ” μ›μΈμ΄ μλ°©ν–¥(pairwise) vs. λ‹¨μΌ(pointwise) λ©μ  ν•¨μ μ„ νƒμ— μμμ„ λ°ν”μµλ‹λ‹¤. λν•, SFT λ‹¨κ³„μ— μ‚¬μ©λλ” λ°μ΄ν„° μ–‘μ μν–¥μ„ λ¶„μ„ν•μ—¬, **μ†λ‰μ λ°μ΄ν„°λ§μΌλ΅λ„ μƒλ‹Ήν• μ„±λ¥ ν–¥μƒ**μ„ μ–»μ„ μ μμμ„ λ³΄μ€μµλ‹λ‹¤. μ΄ μ—°κµ¬λ” DAAμ μ„±λ¥μ„ ν–¥μƒμ‹ν‚¤κ³  μµμ ν™”ν•λ” λ° μ¤‘μ”ν• ν†µμ°°λ ¥μ„ μ κ³µν•©λ‹λ‹¤.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} λ‹¨μΌ λ‹¨κ³„ DAAλ” μ΄μ „μ— μƒκ°ν–λ κ²ƒλ³΄λ‹¤ 2λ‹¨κ³„ DAAλ³΄λ‹¤ μ„±λ¥μ΄ λ–¨μ–΄μ§ {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} SFT λ‹¨κ³„μ™€ μµμ ν™” νλΌλ―Έν„° Ξ²λ¥Ό λ„μ…ν•λ©΄ DAAμ μ„±λ¥μ„ ν¬κ² ν–¥μƒμ‹ν‚¬ μ μμ {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Pairwise μ ‘κ·Ό λ°©μ‹μ΄ Pointwise μ ‘κ·Ό λ°©μ‹λ³΄λ‹¤ μ„±λ¥μ΄ μ°μν•¨ {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
λ³Έ λ…Όλ¬Έμ€ **λ‹¤μ–‘ν• μ§μ ‘ μ •λ ¬ μ•κ³ λ¦¬μ¦(DAA)μ μ„±λ¥ μ°¨μ΄λ¥Ό μ²΄κ³„μ μΌλ΅ λ¶„μ„**ν•κ³ , **μ„±λ¥ ν–¥μƒμ„ μ„ν• μƒλ΅μ΄ λ°©λ²•λ΅ μ„ μ μ‹**ν•¨μΌλ΅μ¨, μμ—°μ–΄ λ¨λΈ μ •λ ¬ λ¶„μ•Όμ μ—°κµ¬ λ°μ „μ— ν¬κ² κΈ°μ—¬ν•©λ‹λ‹¤. νΉν, **λ‹¨μΌ λ‹¨κ³„ λ° 2λ‹¨κ³„ λ°©λ²•μ μ°¨μ΄μ μ„ λ…ν™•ν λ°νκ³ **, **μµμ ν™” νλΌλ―Έν„° Ξ²λ¥Ό λ„μ…**ν•μ—¬ μ•κ³ λ¦¬μ¦μ μΌλ°ν™” λ° μ„±λ¥ ν–¥μƒμ„ μ΄λμ–΄λƒλ‹¤λ” μ μ—μ„ λ†’μ€ κ°€μΉλ¥Ό μ§€λ‹™λ‹λ‹¤. λν•, **SFT λ‹¨κ³„μ μ¤‘μ”μ„±κ³Ό λ°μ΄ν„° ν¬κΈ°μ μν–¥**μ„ μ‹¤ν—μ μΌλ΅ κ²€μ¦ν•μ—¬, ν–¥ν›„ μ—°κµ¬ λ°©ν–¥μ„ μ μ‹ν•κ³  μμµλ‹λ‹¤. μ΄λ¬ν• κ²°κ³Όλ” μμ—°μ–΄ λ¨λΈ μ •λ ¬ λ¶„μ•Όμ μ—°κµ¬μλ“¤μ—κ² μ‹¤μ§μ μΈ λ„μ›€μ„ μ¤„ λΏλ§ μ•„λ‹λΌ, μƒλ΅μ΄ μ—°κµ¬ μμ—­μ„ κ°μ²™ν•λ” λ° μ¤‘μ”ν• μ—­ν• μ„ ν•  κ²ƒμΌλ΅ κΈ°λ€λ©λ‹λ‹¤.

------
#### Visual Insights



![](https://arxiv.org/html/2502.01237/x1.png)

> π”Ό κ·Έλ¦Ό 1μ€ ASFTμ™€ ORPO μ•κ³ λ¦¬μ¦μ μ„±λ¥μ— λ―ΈμΉλ” Ξ² λ§¤κ°λ³€μμ μν–¥μ„ λ³΄μ—¬μ¤λ‹λ‹¤. Ξ² λ§¤κ°λ³€μλ¥Ό μ΅°μ •(3.1.2μ  μ°Έμ΅°)ν•λ©΄ ASFTμ™€ ORPO μ•κ³ λ¦¬μ¦μ μ„±λ¥μ΄ μ–΄λ–»κ² λ³€ν•λ”μ§€λ¥Ό λ³΄μ—¬μ£Όλ” κ·Έλ¦Όμ…λ‹λ‹¤. Llama 3.2 3B TL;DR μ„¤μ •μ—μ„λ” GPT-4 μΉλ¥ μ„, Llama 3.1 8B UF μ‹λ‚λ¦¬μ¤μ—μ„λ” AlpacaEval 2 LC μΉλ¥ μ„ κΈ°μ¤€μΌλ΅ κ²°κ³Όλ¥Ό μ μ‹ν•©λ‹λ‹¤.  ν•™μµλ¥  λ“± λ‹¤λ¥Έ λ¨λ“  ν•μ΄νΌνλΌλ―Έν„°λ” Ξ²=1μΌ λ• κ° λ°©λ²•μ μµμ  κµ¬μ„±μ„ κΈ°μ¤€μΌλ΅ κ·Έλ¦¬λ“ κ²€μƒ‰μ„ ν†µν•΄ μ„ νƒλμ—μµλ‹λ‹¤. μμ„Έν• λ‚΄μ©μ€ 5.2μ μ„ μ°Έμ΅°ν•μ‹­μ‹μ¤.
> <details>
> <summary>read the caption</summary>
> Figure 1:  Impact of the Ξ²π›½\betaitalic_Ξ² Parameter on ASFT and ORPO Alignment Quality. The plot shows how tuning Ξ²π›½\betaitalic_Ξ² (SectionΒ 3.1.2) affects both ASFT and ORPO performance. Results are reported for GPT-4 Win Rate in the Llama 3.2 3B TL;DR setup and for AlpacaEval 2 LC Win Rate in the Llama 3.1 8B UF scenario. All other hyperparameters (e.g., learning rates) are selected via grid search, using each methodβ€™s best configuration at Ξ²=1π›½1\beta=1italic_Ξ² = 1 as the baseline. See SectionΒ 5.2 for more details.
> </details>





{{< table-caption >}}
| Win / Tie / Lose Rate % |
|---|---|---|
| 35.6 / 4.8 / **59.6** | 91.2 / 1.0 / 7.8  |  91.4 / 0.4 / 8.2 |
| 91.6 / 0.2 / 8.2 | 90.2 / 0.6 / 9.2 | 92.6 / 0.6 / 6.8 |
| 91.8 / 1.0 / 7.2 | 91.4 / 0.4 / 8.2 | 87.2 / 1.0 / 11.8 |{{< /table-caption >}}

> π”Ό ν‘ 1μ€ Llama 3.1 8B λ¨λΈκ³Ό UF λ°μ΄ν„°μ…‹μ„ μ‚¬μ©ν• κΈ°λ³Έ λ° SFT μ΄κΈ°ν™” μ •λ ¬ μ•κ³ λ¦¬μ¦μ„ λ³΄μ—¬μ¤λ‹λ‹¤. SFT μ΄κΈ°ν™” λ°©λ²•μ€ κΈ°μ΅΄μ β„’SFTκ°€ μ—†λ” λ°©λ²•μ— λΉ„ν•΄ λ” λ‚μ€ μ„±λ¥μ„ λ³΄μ—¬μ¤λ‹λ‹¤. β€  ν‘μ‹λ” μµμ μ ν•μ΄νΌνλΌλ―Έν„°(ORPOμ κ²½μ° lr=1Γ—10β»β¶, ASFTμ κ²½μ° lr=7Γ—10β»β·)λ¥Ό μ‚¬μ©ν•μ—¬ β„’SFTλ΅ ν•™μµν• κ²°κ³Όλ¥Ό λ‚νƒ€λƒ…λ‹λ‹¤. λ‹¤λ¥Έ μ„¤μ •μ κ²½μ° μµμ  ν•μ΄νΌνλΌλ―Έν„°λ” ν‘μ¤€ SFT ORPO/ASFTμ— λ€ν•΄ lr=5Γ—10β»β·, κΈ°λ³Έ ORPO/ASFTμ— λ€ν•΄ lr=1Γ—10β»βµ/6Γ—10β»β¶μ…λ‹λ‹¤.
> <details>
> <summary>read the caption</summary>
> Table 1: Base and SFT-initialized alignment methods on the Llama 3.1 8B model with the UF dataset. SFT-initialized methods demonstrate better performance compared to their traditional formulations without β„’SFTsubscriptβ„’SFT\mathcal{L}_{\mathrm{SFT}}caligraphic_L start_POSTSUBSCRIPT roman_SFT end_POSTSUBSCRIPT. Results marked with β€ β€ {\dagger}β€  correspond to training with β„’SFTsubscriptβ„’SFT\mathcal{L}_{\mathrm{SFT}}caligraphic_L start_POSTSUBSCRIPT roman_SFT end_POSTSUBSCRIPT, using the best hyperparameters: lr=1Γ—10β’6lr1superscript106\text{lr}=1\times 10^{-6}lr = 1 Γ— 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT for ORPO and lr=7Γ—10β’7lr7superscript107\text{lr}=7\times 10^{-7}lr = 7 Γ— 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT for ASFT. For other setups, the best hyperparameters are: lr=5Γ—10β’7lr5superscript107\text{lr}=5\times 10^{-7}lr = 5 Γ— 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT for standard SFT ORPO/ASFT, and lr=1Γ—10β’5lr1superscript105\text{lr}=1\times 10^{-5}lr = 1 Γ— 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT/6Γ—10β’66superscript1066\times 10^{-6}6 Γ— 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT for Base ORPO/ASFT.
> </details>





### In-depth insights


#### DAA Taxonomy
λ³Έ λ…Όλ¬Έμ—μ„λ” λ‹¤μ–‘ν• μ§μ ‘ μ •λ ¬ μ•κ³ λ¦¬μ¦(Direct Alignment Algorithm, DAA)λ“¤μ„ **μ„Έ κ°€μ§€ μ£Όμ” νΉμ§•**μ„ μ¤‘μ‹¬μΌλ΅ λ¶„λ¥ν•λ” μ²΄κ³„μ μΈ λ¶„λ¥λ²•, μ¦‰ DAA λ¶„λ¥ μ²΄κ³„λ¥Ό μ μ‹ν•©λ‹λ‹¤. μ²«μ§Έ, **μ†μ‹¤ ν•¨μμ μΆ…λ¥**μ— λ”°λ¥Έ λ¶„λ¥λ΅, μλ³„ μμ„ λ§¤κΈ°κΈ°(pairwise) μ†μ‹¤κ³Ό μ λ³„(pointwise) μ†μ‹¤ ν•¨μλ΅ λ‚λ•λ‹λ‹¤. λ‘μ§Έ, **λ³΄μƒμ μ ν•**μ— λ”°λ¥Έ λ¶„λ¥λ΅, λ…μ‹μ μΈ λ³΄μƒ ν•¨μλ¥Ό μ‚¬μ©ν•λ” λ°©λ²•κ³Ό μ•”μ‹μ μΈ λ³΄μƒ ν•¨μλ¥Ό μ‚¬μ©ν•λ” λ°©λ²•μΌλ΅ κµ¬λ¶„ν•©λ‹λ‹¤. λ§μ§€λ§‰μΌλ΅, **ν•™μµ λ‹¨κ³„**μ μ λ¬΄μ— λ”°λΌ λ‹¨μΌ λ‹¨κ³„(one-stage)μ™€ 2λ‹¨κ³„(two-stage) λ°©λ²•μΌλ΅ λ¶„λ¥ν•©λ‹λ‹¤. μ΄λ¬ν• λ‹¤μ°¨μ›μ  λ¶„λ¥λ¥Ό ν†µν•΄ κ° DAAμ κ°•μ κ³Ό μ•½μ μ„ λ…ν™•ν νμ•…ν•κ³ , **μ•κ³ λ¦¬μ¦ μ„ νƒμ— λ€ν• μ μ©ν• μ§€μΉ¨**μ„ μ κ³µν•©λ‹λ‹¤. νΉν, SFT(Supervised Fine-Tuning) λ‹¨κ³„μ μ λ¬΄κ°€ μ„±λ¥μ— λ―ΈμΉλ” μν–¥κ³Ό μλ³„ λ€ μ λ³„ μ†μ‹¤ ν•¨μ μ„ νƒμ μ¤‘μ”μ„±μ„ κ°•μ΅°ν•λ©°,  **μ•κ³ λ¦¬μ¦ μ„±λ¥ λΉ„κµμ μ¤‘μ”μ„±**μ„ λ‹¤μ‹ ν•λ² μƒκΈ°μ‹μΌ μ¤λ‹λ‹¤.  **λ³΄λ‹¤ ν¨μ¨μ μ΄κ³  μ •ν™•ν• μ–Έμ–΄ λ¨λΈ μ •λ ¬**μ„ μ„ν• μ—°κµ¬ λ°©ν–¥μ„ μ μ‹ν•λ” λ° μ¤‘μ”ν• μ—­ν• μ„ ν•©λ‹λ‹¤.

#### SFT's Impact
λ³Έ λ…Όλ¬Έμ€ SFT(Supervised Fine-Tuning)μ μν–¥μ„ λ‹¤κ°μ μΌλ΅ λ¶„μ„ν•©λ‹λ‹¤. **λ‹¨κ³„μ  μ ‘κ·Όλ²•μ—μ„ SFTλ” μ„±λ¥ ν–¥μƒμ— ν•„μμ μ„μ„ λ³΄μ—¬μ¤λ‹λ‹¤.**  SFT λ‹¨κ³„λ¥Ό κ±°μΉ ν›„μ— μ§μ ‘ μ •λ ¬ μ•κ³ λ¦¬μ¦(DAA)μ„ μ μ©ν•λ©΄ ORPOμ™€ ASFT λ¨λ‘ μ„±λ¥μ΄ ν¬κ² ν–¥μƒλλ” κ²ƒμ„ ν™•μΈν–μµλ‹λ‹¤.  ν¥λ―Έλ΅μ΄ μ μ€ **SFTμ— μ „μ²΄ λ°μ΄ν„°μ…‹μ„ μ‚¬μ©ν•  ν•„μ”κ°€ μ—†λ‹¤λ” μ **μ…λ‹λ‹¤.  μΌλ¶€ λ°μ΄ν„°λ§μΌλ΅λ„ μƒλ‹Ήν• μ„±λ¥ ν–¥μƒμ„ μ–»μ„ μ μμΌλ©°, κ³„μ‚° λΉ„μ©μ„ μ κ°ν•λ” ν¨κ³Όλ¥Ό κ°€μ Έμµλ‹λ‹¤.  μ΄λ¬ν• κ²°κ³Όλ” **SFT λ‹¨κ³„μ μ¤‘μ”μ„±**κ³Ό λ”λ¶μ–΄, ν¨μ¨μ μΈ SFT μ „λµμ κ°€λ¥μ„±μ„ μ‹μ‚¬ν•©λ‹λ‹¤.  **λ°μ΄ν„° ν¬κΈ°μ™€ SFTμ μƒκ΄€κ΄€κ³„**λ¥Ό λ…ν™•ν κ·λ…ν•μ—¬, ν–¥ν›„ μ—°κµ¬ λ°©ν–¥μ„ μ μ‹ν•λ”λ° ν° κΈ°μ—¬λ¥Ό ν•  κ²ƒμ…λ‹λ‹¤.

#### Beta Tuning
λ³Έ λ…Όλ¬Έμ—μ„ "λ² νƒ€ νλ‹"μ΄λΌλ” κ°λ…μ€ λ…μ‹μ μΌλ΅ μ μ‹λμ§€ μ•μ•μ§€λ§, λ² νƒ€(Ξ²) νλΌλ―Έν„°λ¥Ό ν™μ©ν• ASFTμ™€ ORPO μ•κ³ λ¦¬μ¦μ μ„±λ¥ ν–¥μƒμ— λ€ν• λ¶„μ„μ—μ„ μ μ¶”ν•  μ μμµλ‹λ‹¤.  **λ² νƒ€ νλΌλ―Έν„°λ” μ„ νΈλ„ μµμ ν™”μ κ°•λ„λ¥Ό μ μ–΄ν•λ” μ—­ν• **μ„ ν•λ©°, μ΄λ¥Ό ν†µν•΄ λ‹¨μΌ λ‹¨κ³„ λ°©λ²•λ΅ μΈ ASFTμ™€ ORPOμ μ„±λ¥μ„ κΈ°μ΅΄μ μ΄λ‹¨κ³„ λ°©λ²•λ΅ (DPO λ“±)κ³Ό μ μ‚¬ν• μμ¤€κΉμ§€ λμ–΄μ¬λ Έλ‹¤λ” μ μ΄ μ¤‘μ”ν•©λ‹λ‹¤. **λ² νƒ€ κ°’μ„ μ΅°μ ν•¨μΌλ΅μ¨ μ„ νΈλ„λ¥Ό λ”μ± κ³µκ²©μ μΌλ΅(μ‘μ€ λ² νƒ€ κ°’) λλ” μ™„ν™”μ μΌλ΅(ν° λ² νƒ€ κ°’) μµμ ν™”**ν•  μ μμΌλ©°, μ΄λ” λ¨λΈμ μ„±λ¥κ³Ό μΌλ°ν™” λ¥λ ¥ μ‚¬μ΄μ κ· ν•μ„ λ§μ¶”λ” λ° μ¤‘μ”ν• μ—­ν• μ„ ν•©λ‹λ‹¤.  **λ² νƒ€ νλ‹μ€ λ‹¨μν• ν•μ΄νΌνλΌλ―Έν„° νλ‹μ„ λ„μ–΄, λ¨λΈμ ν•™μµ λ™μ—­ν•™μ— λ€ν• μ‹¬μΈµμ μΈ μ΄ν•΄λ¥Ό λ°”νƒ•μΌλ΅ μµμ μ μ„±λ¥μ„ λ„μ¶ν•λ” μ¤‘μ”ν• κ³Όμ •**μ„μ„ μ‹μ‚¬ν•©λ‹λ‹¤.  μ‹¤ν— κ²°κ³Όλ¥Ό ν†µν•΄ ν™•μΈλ λ°”μ™€ κ°™μ΄, λ² νƒ€ κ°’μ μ μ ν• μ΅°μ •μ€ ASFTμ™€ ORPOμ μ„±λ¥μ„ ν¬κ² ν–¥μƒμ‹ν‚¤λ” ν¨κ³Όκ°€ μμΌλ©°, μ΄λ” ν–¥ν›„ λ‹¤μ–‘ν• μ§μ ‘ μ •λ ¬ μ•κ³ λ¦¬μ¦μ μ„¤κ³„ λ° μµμ ν™”μ— μ¤‘μ”ν• μ‹μ‚¬μ μ„ μ κ³µν•©λ‹λ‹¤.  **λ² νƒ€ νλ‹μ„ ν†µν•΄ μ–»μ€ κ²°κ³Όλ” λ‹¨μν μ„±λ¥ κ°μ„ μ— κ·ΈμΉμ§€ μ•κ³ ,  μ§μ ‘ μ •λ ¬ μ•κ³ λ¦¬μ¦μ μ„¤κ³„ μ›λ¦¬μ™€ μ„±λ¥ ν–¥μƒ μ „λµμ— λ€ν• κ·€μ¤‘ν• ν†µμ°°λ ¥μ„ μ κ³µ**ν•΄ μ¤λ‹λ‹¤.

#### Pairwise vs. Pointwise
λ³Έ λ…Όλ¬Έμ—μ„ μ €μλ“¤μ€ **Pairwise μ ‘κ·Ό λ°©μ‹**κ³Ό **Pointwise μ ‘κ·Ό λ°©μ‹** κ°„μ μ°¨μ΄μ μ„ λ©΄λ°€ν μ΅°μ‚¬ν•μ—¬ μ–Έμ–΄ λ¨λΈ μ •λ ¬μ λ§¥λ½μ—μ„ κ° μ ‘κ·Ό λ°©μ‹μ κ°•μ κ³Ό μ•½μ μ„ μ‹¬μΈµμ μΌλ΅ λ¶„μ„ν•©λ‹λ‹¤.  Pairwise λ°©λ²•μ€ λ‘ μƒν” κ°„μ μƒλ€μ  μ„ νΈλ„λ¥Ό μ§μ ‘μ μΌλ΅ μµμ ν™”ν•λ” λ°λ©΄, Pointwise λ°©λ²•μ€ κ°λ³„ μƒν”μ μ λ€μ  μ μλ¥Ό μµμ ν™”ν•©λ‹λ‹¤.  **Pairwise λ°©λ²•**μ€ μƒλ€μ  μμ„λ¥Ό λ” μ ν¬μ°©ν•μ§€λ§, κ³„μ‚° λΉ„μ©μ΄ λ” λ§μ΄ λ“¤ μ μμµλ‹λ‹¤. λ°λ©΄μ— **Pointwise λ°©λ²•**μ€ κ³„μ‚° ν¨μ¨μ„±μ΄ λ†’μ§€λ§, μƒλ€μ  μμ„ μ •λ³΄λ¥Ό λ μ •ν™•ν•κ² λ°μν•  μ μμµλ‹λ‹¤. μ΄λ¬ν• μ°¨μ΄μ μ€ λ‹¤μ–‘ν• λ°μ΄ν„° ν¬κΈ° λ° λ¨λΈ μ©λ‰μ— λ”°λΌ λ‹¤λ¥Έ μν–¥μ„ λ―ΈμΉλ©°, **λ°μ΄ν„° ν¬κΈ°κ°€ μ¦κ°€ν•¨μ— λ”°λΌ pairwise λ°©λ²•μ μ°μμ„±μ΄ λ”μ± λ‘λ“λ¬μ§€κ² λ‚νƒ€λ‚©λ‹λ‹¤.**  μ €μλ“¤μ€ μ‹¤ν— κ²°κ³Όλ¥Ό ν†µν•΄ μ΄λ¬ν• μ£Όμ¥μ„ λ’·λ°›μΉ¨ν•κ³ , **νΉμ • λ¬Έμ  λ° λ¨λΈ μ•„ν‚¤ν…μ²μ— κ°€μ¥ μ ν•©ν• μ ‘κ·Ό λ°©μ‹**μ„ μ„ νƒν•λ” λ° λ„μ›€μ΄ λλ” μ§€μΉ¨μ„ μ κ³µν•©λ‹λ‹¤.

#### Future Work
λ³Έ λ…Όλ¬Έμ€ λ‹¤μ–‘ν• μ§μ ‘ μ •λ ¬ μ•κ³ λ¦¬μ¦(DAA)μ„ ν¬κ΄„μ μΌλ΅ λ¶„μ„ν•κ³  λΉ„κµν•μ—¬, **μ§ λΉ„κµ λ°©μ‹μ΄ μ μ λΉ„κµ λ°©μ‹λ³΄λ‹¤ μ°μν•λ©°, νΉν λ€κ·λ¨ λ¨λΈμ—μ„ λ”μ± ν¨κ³Όμ **μ„μ„ λ³΄μ—¬μ¤λ‹λ‹¤.  ν•μ§€λ§, λ³Έ μ—°κµ¬λ” μ ν•λ λ°μ΄ν„°μ…‹κ³Ό λ¨λΈ ν¬κΈ°μ— μμ΅΄ν•λ©°, GPT κΈ°λ° ν‰κ°€μ νΈν–¥μ„±μ΄ μ΅΄μ¬ν•©λ‹λ‹¤. λ”°λΌμ„ **ν–¥ν›„ μ—°κµ¬λ” λ‹¤μ–‘ν• λ°μ΄ν„°μ…‹κ³Ό λ¨λΈ ν¬κΈ°λ¥Ό μ‚¬μ©ν•μ—¬ μΌλ°ν™” μ„±λ¥μ„ κ²€μ¦**ν•κ³ , **λ”μ± κ°κ΄€μ μΈ ν‰κ°€ μ§€ν‘λ¥Ό κ°λ°**ν•΄μ•Ό ν•©λ‹λ‹¤. λν•, **λ‹¨μΌ λ‹¨κ³„μ™€ 2λ‹¨κ³„ λ°©λ²•μ μ„±λ¥ μ°¨μ΄μ— λ€ν• μ‹¬μΈµμ  λ¶„μ„**μ„ ν†µν•΄ κ° λ°©λ²•μ μ¥λ‹¨μ μ„ λ…ν™•ν νμ•…ν•κ³ , **λ¨λΈμ μ©λ‰κ³Ό DAAμ μ ν• κ°„ μƒνΈ μ‘μ©**μ„ κ·λ…ν•λ” μ—°κµ¬κ°€ ν•„μ”ν•©λ‹λ‹¤.  **λ‹¤μ–‘ν• ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” μ „λµ**μ„ λΉ„κµλ¶„μ„ν•κ³ , **λΉ„μ© ν¨μ¨μ μΈ SFT λ°μ΄ν„° μ‚¬μ© μ „λµ**μ„ μ—°κµ¬ν•μ—¬ μ‹¤μ  μ μ© κ°€λ¥μ„±μ„ λ†’μ—¬μ•Ό ν•©λ‹λ‹¤.  λ§μ§€λ§‰μΌλ΅, **λ‹¤λ¥Έ μ •λ ¬ λ°©λ²•λ“¤κ³Όμ λΉ„κµ μ—°κµ¬**λ¥Ό ν†µν•΄ DAAμ μ¥μ κ³Ό ν•κ³„λ¥Ό λ…ν™•ν λ°νλ” κ²ƒμ΄ μ¤‘μ”ν•©λ‹λ‹¤.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2502.01237/x3.png)

> π”Ό κ·Έλ¦Ό 2λ” Llama 3.2 3B λ¨λΈμ„ μ‚¬μ©ν•μ—¬ Reddit TL;DR μ”μ•½ μ‘μ—…μ— λ€ν• GPT-4 ν‰κ°€ κ²°κ³Όλ¥Ό λ³΄μ—¬μ¤λ‹λ‹¤. μ—¬λ¬ μ •λ ¬ μ•κ³ λ¦¬μ¦(ν–‰)μ΄ μµμ μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ‚¬μ©ν•μ—¬ λΉ„κµλμ—μΌλ©°, κ° μ•κ³ λ¦¬μ¦μ€ κ°„κ²°ν•κ³  μ •ν™•ν• μ”μ•½μ„ μƒμ„±ν•λ” κ²ƒμ„ λ©ν‘λ΅ ν•©λ‹λ‹¤. λ€λ¶€λ¶„μ μ•κ³ λ¦¬μ¦μ΄ 90% μ΄μƒμ μΉλ¥ μ„ κΈ°λ΅ν• λ°λ©΄, ASFTλ” 87.2%μ μΉλ¥ μ„ κΈ°λ΅ν•μ—¬ κ²¬κ³ ν• μ”μ•½ μ„±λ¥μ„ μ μ§€ν–μµλ‹λ‹¤. μμ„Έν• λ‚΄μ©μ€ 5.2μ μ„ μ°Έμ΅°ν•μ‹­μ‹μ¤.
> <details>
> <summary>read the caption</summary>
> Figure 2: GPT-4 Evaluation of Llama 3.2 3B TL;DR setup. The comparison shows multiple alignment methods (rows) using their best hyperparameters, where each approach aims to generate concise and accurate summaries. Most methods exceed 90% Win Rate; ASFT achieves 87.2%, maintaining robust summarization performance. See SectionΒ 5.2 for more details.
> </details>



![](https://arxiv.org/html/2502.01237/x4.png)

> π”Ό κ·Έλ¦Ό 3μ€ Llama 3.1 8B UF λ¨λΈμ„ AlpacaEval 2 LC μ§€ν‘λ΅ ν‰κ°€ν• κ²°κ³Όλ¥Ό λ³΄μ—¬μ£Όλ” νλ ν†  ν”„λ°νΈμ…λ‹λ‹¤.  μν‰μ¶•μ€ κΈ°μ¤€ λ¨λΈκ³Όμ KL λ°μ‚°μ„ λ‚νƒ€λ‚΄κ³ , μμ§μ¶•μ€ μ •λ ¬ μ„±λ¥(LC κ°’)μ„ λ‚νƒ€λƒ…λ‹λ‹¤.  κ·Έλ¦Όμ—μ„ μ• μ μλ“―μ΄, μ§ λΉ„κµ λ°©μ‹(pairwise)μ„ μ‚¬μ©ν• λ°©λ²•λ“¤μ΄ μ  λΉ„κµ λ°©μ‹(pointwise)μ„ μ‚¬μ©ν• λ°©λ²•λ“¤λ³΄λ‹¤ λ” λ†’μ€ LC κ°’μ„ λ‹¬μ„±ν–μµλ‹λ‹¤.  ν•μ§€λ§ μ‹ λΆ° κµ¬κ°„μ΄ κ²ΉμΉλ” λ¶€λ¶„μ΄ μμ–΄ ν†µκ³„μ μΌλ΅ μ μλ―Έν• μ°¨μ΄κ°€ μλ‹¤κ³  λ‹¨μ •ν•  μλ” μ—†μµλ‹λ‹¤. μμ„Έν• λ‚΄μ©μ€ λ³Έλ¬Έ 5.3μ μ„ μ°Έμ΅°ν•μ„Έμ”.
> <details>
> <summary>read the caption</summary>
> Figure 3: Pareto front for alignment quality and KL divergence. Results for Llama 3.1 8B UF on AlpacaEval 2 LC. Methods are grouped into pairwise and pointwise categories, with pairwise achieving higher LC values while remaining within overlapping confidence intervals. See Section 5.3 for more details.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Init | Method | LC% (std) | WR% (std) | AH% (CI) |
|---|---|---|---|---|
| Base | SFT | 6.7 (0.43) | 4.5 (0.63) | 3.5 (-0.7, 0.8) |
| SFT | ORPO | **24.1** (0.84) | **17.8** (1.17) | **15.3** (-1.6, 1.8) |
| SFT | ASFT | 16.4 (0.72) | 11.9 (0.99) | 10.6 (-1.2, 1.3) |
| Base | ORPO | 14.8 (0.71) | 10.3 (0.95) | 8.4 (-1.3, 1.3) |
| Base | ASFT | 14.5 (0.73) | 10.2 (0.94) | 7.5 (-1.1, 1.2) |
| SFT | ORPO<sup>β€ </sup> | 13.4 (0.69) | 9.3 (0.91) | 7.7 (-0.9, 1.1) |
| SFT | ASFT<sup>β€ </sup> | 11.4 (0.63) | 7.5 (0.83) | 7.5 (-1.1, 1.1) |
| SFT | DPO | **23.4** (0.85) | **20.0** (1.18) | **17.5** (-1.8, 1.8) |{{< /table-caption >}}
> π”Ό ν‘ 2λ” Llama 3.2 3B λ° Llama 3.1 8B λ¨λΈμ— λ€ν• AlpacaEval 2 λ° ArenaHard ν‰κ°€ κ²°κ³Όλ¥Ό λ³΄μ—¬μ¤λ‹λ‹¤.  SFT(Supervised Fine-Tuning) λ¨λΈμ€ UltraChat λ°μ΄ν„°μ…‹μΌλ΅ ν•™μµλμ—μµλ‹λ‹¤. κ° λ°©λ²•μ— λ€ν• μµμ μ ν•μ΄νΌνλΌλ―Έν„°λ” 4.2μ μ— λ”°λΌ μ„ νƒλμ—μµλ‹λ‹¤.  κµµμ€ κ°’μ€ κ° λ²¤μΉλ§ν¬μ—μ„ μµκ³  μ„±λ¥μ„ λ‚νƒ€λ‚΄κ³ , λ°‘μ¤„ μΉ κ°’μ€ λ‘ λ²μ§Έλ΅ λ†’μ€ μ„±λ¥μ„ λ‚νƒ€λƒ…λ‹λ‹¤. μμ„Έν• λ‚΄μ©μ€ 5.3μ μ„ μ°Έμ΅°ν•μ‹­μ‹μ¤.
> <details>
> <summary>read the caption</summary>
> Table 2: AlpacaEval 2 and ArenaHard Results for Llama 3.2 3B and Llama 3.1 8B UF. The SFT model was trained on the UltraChat dataset. The best hyperparameters for each method were selected according to SectionΒ 4.2. Bold values indicate the best performance for each benchmark, while underlined values represent the second-best performance. See Section 5.3 for more details.
> </details>

{{< table-caption >}}
| Method | Llama 3.2 3B UF |  |  | Llama 3.1 8B UF |  |  |
|---|---|---|---|---|---|---|
|  | AlpacaEval 2 | ArenaHard |  | AlpacaEval 2 | ArenaHard |  |
|---|---|---|---|---|---|---|
|  | LC% (std) | WR% (std) | WR% (CI) | LC% (std) | WR% (std) | WR% (CI) |
|---|---|---|---|---|---|---|
| SFT | 5.02 (0.34) | 3.21 (0.55) | 1.4 (-0.4, 0.4) | 10.27 (0.54) | 5.44 (0.70) | 2.6 (-0.5, 0.6) |
| DPO | 11.43 (0.58) | 11.79 (0.99) | 6.8 (-1.0, 0.9) | 26.82 (0.77) | 23.69 (1.25) | 19.0 (-1.9, 1.8) |
| IPO | 11.24 (0.60) | 11.67 (1.01) | 6.8 (-1.0, 1.1) | 28.18 (0.83) | 24.43 (1.26) | 19.1 (-1.6, 1.5) |
| SimPO | 10.56 (0.44) | 11.94 (0.95) | 6.4 (-1.0, 1.1) | 27.65 (0.77) | 25.62 (1.29) | 21.5 (-1.9, 1.9) |
| ORPO | 10.67 (0.50) | 12.23 (0.97) | 6.6 (-1.0, 1.1) | 28.25 (0.71) | 28.59 (1.33) | 20.9 (-2.0, 2.0) |
| APO Zero | 10.36 (0.53) | 11.22 (0.98) | 6.0 (-1.0, 0.9) | 23.15 (0.76) | 19.03 (1.18) | 17.3 (-1.8, 1.8) |
| NCA | 10.33 (0.53) | 11.02 (0.97) | 5.1 (-0.7, 0.8) | 23.21 (0.80) | 18.67 (1.17) | 15.1 (-1.5, 1.6) |
| Cal-DPO | 10.62 (0.57) | 10.15 (0.94) | 4.8 (-0.9, 0.9) | 23.19 (0.82) | 18.85 (1.18) | 15.2 (-1.5, 1.6) |
| ASFT | 10.63 (0.55) | 9.21 (0.88) | 5.1 (-0.9, 0.9) | 20.82 (0.79) | 16.34 (1.13) | 13.5 (-1.6, 1.5) |{{< /table-caption >}}
> π”Ό μ΄ ν‘λ” λ‹¤μ–‘ν• μ§μ ‘ μ •λ ¬ μ•κ³ λ¦¬μ¦(Direct Alignment Algorithms, DAAs)λ“¤μ΄ μ›λ μ μ‹λμ—μ„ λ• κΈΈμ΄ κΈ°λ° ν™•λ¥  μ •κ·ν™”(length-based probability normalization)λ¥Ό μ‚¬μ©ν–λ”μ§€ μ—¬λ¶€λ¥Ό λ³΄μ—¬μ¤λ‹λ‹¤.  β“λ” κΈΈμ΄ κΈ°λ° μ •κ·ν™”λ¥Ό μ‚¬μ©ν–μμ„, β—λ” μ‚¬μ©ν•μ§€ μ•μ•μμ„ μλ―Έν•©λ‹λ‹¤.  ν‘λ¥Ό ν†µν•΄ κ° μ•κ³ λ¦¬μ¦μ μ›λ μ„¤κ³„μ—μ„μ μ°¨μ΄μ μ„ λ…ν™•ν μ΄ν•΄ν•  μ μμµλ‹λ‹¤.
> <details>
> <summary>read the caption</summary>
> Table 3: Methods that include (β“) or omit (β—) length-based probability normalization in their original formulation.
> </details>

{{< table-caption >}}
| Method | Use normalization |
|---|---| 
| DPO (Rafailov et al., 2023) | β— |
| IPO (Azar et al., 2023) | β— |
| SimPO (Meng et al., 2024) | β“ |
| NCA (Chen et al., 2024) | β— |
| Cal-DPO (Xiao et al., 2024) | β— |
| APO-Zero (Dβ€™Oosterlinck et al., 2024) | β— |
| ORPO (Hong et al., 2024) | β“ |
| ASFT (Wang et al., 2024) | β“ |{{< /table-caption >}}
> π”Ό μ΄ ν‘λ” Llama 3.2 3B λ° Llama 3.1 8B λ¨λΈμ— λ€ν• λ€ν‘μ μΈ ν›λ ¨ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό λ³΄μ—¬μ¤λ‹λ‹¤.  ν‘μ—λ” μµλ€ ν† ν° κΈΈμ΄, μ—ν¬ν¬ μ, ν•™μµλ¥ (SFT, κΈ°λ³Έ μ΄κΈ°ν™”, μ •λ ¬), μµμ ν™”κΈ°, μµν‹°λ§μ΄μ €μ ν•μ΄νΌνλΌλ―Έν„°(Adam Ξ²1, Ξ²2), λ°°μΉ ν¬κΈ°, ν•™μµ μΌμ •, μ›μ—… λΉ„μ¨, μµλ€ κ·Έλλ””μ–ΈνΈ λ†, λ©”λ¨λ¦¬ μµμ ν™”, μ–΄ν…μ… λ©”μ»¤λ‹μ¦ λ“± λ‹¤μ–‘ν• ν•μ΄νΌνλΌλ―Έν„°κ°€ ν¬ν•¨λμ–΄ μμµλ‹λ‹¤. μ΄λ¬ν• ν•μ΄νΌνλΌλ―Έν„°λ” Llama λ¨λΈμ„ ν›λ ¨ν•λ” λ° μ‚¬μ©λ μ„¤μ •μ„ μμ„Έν μ„¤λ…ν•μ—¬ μ¬ν„μ„±μ„ λ†’μ΄λ” λ° λ„μ›€μ΄ λ©λ‹λ‹¤.
> <details>
> <summary>read the caption</summary>
> Table 4: Representative training hyperparameters for Llama 3.2 3B and Llama 3.1 8B models.
> </details>

{{< table-caption >}}
| Hyperparameter | Value |
|---|---| 
| Max Tokens Length | 1024 (TL;DR setup), 4096 (UF setup) |
| Epochs | 1 (or 2 when specified) |
| Learning Rate (SFT) | 6.0 Γ— 10<sup>-6</sup> |
| Learning Rate (Base Init.) | {6.0 Γ— 10<sup>-6</sup>, 8.0 Γ— 10<sup>-6</sup>, 1.0 Γ— 10<sup>-5</sup>} |
| Learning Rate (Alignment) | {3.0 Γ— 10<sup>-7</sup>, 5.0 Γ— 10<sup>-7</sup>, 7.0 Γ— 10<sup>-7</sup>, 1.0 Γ— 10<sup>-6</sup>} |
| Optimizer | Adam (Kingma & Ba, 2014) |
| Adam Ξ²<sub>1</sub> | 0.9 |
| Adam Ξ²<sub>2</sub> | 0.95 |
| Batch Size | 128 |
| Learning Schedule | Linear Decay |
| Warm-up Ratio | 0.03 |
| Max Gradient Norm | 2 |
| Memory Optimization | DeepSpeed (Rasley et al., 2020) |
| Attention Mechanism | Flash Attention 2 (Dao, 2023) |{{< /table-caption >}}
> π”Ό μ΄ ν‘λ” λ³Έ λ…Όλ¬Έμ—μ„ μ‚¬μ©λ μ„Έ κ°€μ§€ λ°μ΄ν„°μ…‹(UltraChat, UltraFeedback, Reddit TL;DR)μ ν¬κΈ°λ¥Ό μ”μ•½ν•μ—¬ λ³΄μ—¬μ¤λ‹λ‹¤. κ° λ°μ΄ν„°μ…‹μ€ ν›λ ¨κ³Ό κ²€μ¦μ— μ‚¬μ©λ μμ μ μλ¥Ό λ‚νƒ€λƒ…λ‹λ‹¤. Reddit TL;DR λ°μ΄ν„°μ…‹μ€ SFT(Supervised Fine-Tuning)μ™€ μ„ νΈλ„(Preference) ν‰κ°€μ— κ°κ° λ‹¤λ¥Έ ν¬κΈ°μ λ°μ΄ν„°κ°€ μ‚¬μ©λμ—μμ„ λ³΄μ—¬μ¤λ‹λ‹¤.
> <details>
> <summary>read the caption</summary>
> Table 5: Summary of dataset sizes used for training and validation.
> </details>

{{< table-caption >}}
| Dataset | Training Examples | Validation Examples |
|---|---|---|
| UltraChat | 207,865 | 23,110 |
| UltraFeedback | 61,135 | 2,000 |
| Reddit TL;DR (SFT) | 41,947 | 11,941 |
| Reddit TL;DR (Preference) | 73,396 | 21,198 |{{< /table-caption >}}
> π”Ό μ΄ ν‘λ” λ…Όλ¬Έμ μ‹¤ν— μ„¤μ •μ—μ„ λ‹¤μ–‘ν• μ§μ ‘ μ •λ ¬ μ•κ³ λ¦¬μ¦(DAA)μ— λ€ν•΄ ν…μ¤νΈλ Ξ² κ°’μ λ²”μ„λ¥Ό λ³΄μ—¬μ¤λ‹λ‹¤. κ° DAA λ©”μ„λ“μ— λ€ν•΄ μ—¬λ¬ κ°μ Ξ² κ°’μ„ μ‚¬μ©ν•μ—¬ μ„±λ¥κ³Ό KL λ°μ‚° κ°„μ μ μ¶©μ„ μ΅°μ‚¬ν–μµλ‹λ‹¤.  Ξ² κ°’μ€ κ° μ•κ³ λ¦¬μ¦μ μµμ  μ„±λ¥μ„ μ°ΎκΈ° μ„ν•΄ μ‹ μ¤‘ν•κ² μ„ νƒλμ—μµλ‹λ‹¤.
> <details>
> <summary>read the caption</summary>
> Table 6: Range of Ξ²π›½\betaitalic_Ξ² values tested for each DAA method on all scenarios.
> </details>

{{< table-caption >}}
| Method | \beta Values Tested |
|---|---| 
| DPO | {0.001, 0.003, 0.005, 0.01, 0.05, 0.1} |
| IPO | {0.0007, 0.001, 0.005, 0.01, 0.05, 0.1} |
| SimPO | {0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0} |
| ORPO | {0.05, 0.1, 0.2, 0.5, 1.0, 2.0} |
| ASFT | {0.05, 0.1, 0.2, 0.5, 1.0, 2.0} |
| APO-Zero | {0.001, 0.003, 0.005, 0.01, 0.05, 0.1, 0.2} |
| Cal-DPO | {0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003} |
| NCA | {0.0001, 0.0003, 0.0005, 0.001, 0.005, 0.007, 0.01, 0.03, 0.05} |{{< /table-caption >}}
> π”Ό ν‘ 7μ€ μ„Έ κ°€μ§€ λ‹¤λ¥Έ μ„¤μ •(Llama 3.2 3B TL;DR, Llama 3.2 3B UF, Llama 3.1 8B UF)μ—μ„ λ‹¤μ–‘ν• μ§μ ‘ μ •λ ¬ μ•κ³ λ¦¬μ¦(DAA)μ— λ€ν•΄ μµμ μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό λ³΄μ—¬μ¤λ‹λ‹¤. κ° DAA λ°©λ²•κ³Ό μ„¤μ •μ— λ€ν•΄ μµμƒμ μ„±λ¥μ„ λ‹¬μ„±ν• ν•™μµλ¥ κ³Ό λ² νƒ€(Ξ²) κ°’μ„ ν™•μΈν•  μ μμµλ‹λ‹¤. μ΄ ν‘λ” κ° μ•κ³ λ¦¬μ¦μ μ„±λ¥μ„ μµμ ν™”ν•λ” λ° μ‚¬μ©λ κµ¬μ²΄μ μΈ ν•μ΄νΌνλΌλ―Έν„° μ„¤μ •μ„ μ΄ν•΄ν•λ” λ° λ„μ›€μ΄ λ©λ‹λ‹¤.
> <details>
> <summary>read the caption</summary>
> Table 7: Best hyperparameters for each DAA method across setups.
> </details>

{{< table-caption >}}
Method | Llama 3.2 3B TL;DR |  | Llama 3.2 3B UF |  | Llama 3.1 8B UF |  | 
---|---|---|---|---|---|---
DPO | 7.0x10^-7 | 0.05 | 1.0x10^-6 | 0.01 | 1.0x10^-6 | 0.003 
IPO | 1.0x10^-6 | 0.005 | 7.0x10^-7 | 0.001 | 1.0x10^-6 | 0.001 
SimPO | 3.0x10^-7 | 0.5 | 7.0x10^-7 | 1.0 | 6.0x10^-7 | 1.0 
ORPO | 3.0x10^-7 | 0.5 | 5.0x10^-7 | 0.2 | 5.0x10^-7 | 0.5 
ASFT | 3.0x10^-7 | 0.001 | 1.0x10^-6 | 0.2 | 7.0x10^-7 | 0.1 
APO Zero | 3.0x10^-7 | 0.001 | 3.0x10^-7 | 0.005 | 3.0x10^-7 | 0.003 
NCA | 3.0x10^-7 | 0.0001 | 3.0x10^-7 | 0.0005 | 3.0x10^-7 | 0.0003 
Cal-DPO | 3.0x10^-7 | 0.00003 | 5.0x10^-7 | 0.0003 | 3.0x10^-7 | 0.0003{{< /table-caption >}}
> π”Ό μ΄ ν‘λ” Llama 3.1 8B λ° Llama 3.2 3B λ¨λΈμ— μ‚¬μ©λ ν…μ¤νΈ μƒμ„± κ΄€λ ¨ ν•μ΄νΌνλΌλ―Έν„°λ“¤μ„ λ³΄μ—¬μ¤λ‹λ‹¤.  κµ¬μ²΄μ μΌλ΅λ” μ¨λ„(temperature), Top-k, Top-p λ° μµλ€ ν† ν° μ(Max New Tokens) λ“±μ κ°’λ“¤μ΄ ν¬ν•¨λμ–΄ μμΌλ©°,  Llama 3.1 8Bμ™€ Llama 3.2 3B λ¨λΈ κ°κ°μ— λ€ν•΄ μ„λ΅ λ‹¤λ¥Έ μ„¤μ •μ΄ μμμ„ ν™•μΈν•  μ μμµλ‹λ‹¤. μ΄ ν•μ΄νΌνλΌλ―Έν„°λ“¤μ€ λ¨λΈμ΄ ν…μ¤νΈλ¥Ό μƒμ„±ν•λ” λ°©μ‹μ— μν–¥μ„ λ―ΈμΉλ” μ¤‘μ”ν• μ”μ†λ“¤μ…λ‹λ‹¤.  μλ¥Ό λ“¤μ–΄, μ¨λ„κ°€ λ†’μ„μλ΅ μƒμ„±λλ” ν…μ¤νΈλ” λ”μ± λ‹¤μ–‘ν•κ³  μμΈ΅ λ¶κ°€λ¥ν•΄μ§€λ” λ°λ©΄, Top-kλ‚ Top-pλ” μƒμ„± κ³Όμ •μ λ¬΄μ‘μ„μ„±μ„ μ΅°μ ν•λ” μ—­ν• μ„ ν•©λ‹λ‹¤.  μµλ€ ν† ν° μλ” μƒμ„±λλ” ν…μ¤νΈμ μµλ€ κΈΈμ΄λ¥Ό μ ν•ν•©λ‹λ‹¤.
> <details>
> <summary>read the caption</summary>
> Table 8: Generation hyperparameters for Llama 3.1 8B and Llama 3.2 3B models.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}