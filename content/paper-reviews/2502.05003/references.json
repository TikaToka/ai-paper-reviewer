{"references": [{"fullname_first_author": "Hoffmann, J.", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-MM-DD", "reason": "This paper introduces scaling laws for LLMs, providing a framework to understand the relationship between model size, training data, and compute, which is crucial for optimizing training efficiency and resource allocation."}, {"fullname_first_author": "Touvron, H.", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-MM-DD", "reason": "This paper introduces Llama 2, an open-source large language model architecture used as the basis for the experiments in the QuEST paper, making it a fundamental reference for understanding the model's architecture and performance."}, {"fullname_first_author": "Frantar, E.", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "publication_date": "2022-MM-DD", "reason": "This paper presents GPTQ, a post-training quantization method that achieves high accuracy in compressing LLMs which provides a baseline method for comparison to the training-time quantization performed in QuEST."}, {"fullname_first_author": "Frantar, E.", "paper_title": "Compression scaling laws: Unifying sparsity and quantization", "publication_date": "2025-MM-DD", "reason": "This paper extends scaling laws to encompass both sparsity and quantization, which is directly relevant to QuEST\u2019s exploration of low-precision training and the tradeoffs between model size and accuracy."}, {"fullname_first_author": "Kumar, T.", "paper_title": "Scaling laws for precision", "publication_date": "2024-MM-DD", "reason": "This paper investigates scaling laws for the precision of LLMs, establishing Pareto-optimal bitwidths which sets a state-of-the-art that QuEST aims to improve upon."}]}