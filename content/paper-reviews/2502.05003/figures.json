[{"figure_path": "https://arxiv.org/html/2502.05003/x1.png", "caption": "Figure 1: The scaling law induced by QuEST when training Llama-family models from 30 to 800M parameters on C4, with quantized weights and activations from 1 to 4 bits, in the 100 tokens/parameter regime (higher compression uses proportionally more data at fixed memory). QuEST allows for stable training at 1-bit weights and activations (W1A1), and the QuEST W4A4 model is Pareto-dominant relative to BF16, with lower loss at lower size.", "description": "\uadf8\ub9bc 1\uc740 QuEST\ub97c \uc0ac\uc6a9\ud558\uc5ec Llama \uacc4\uc5f4 \ubaa8\ub378\uc744 30M\uc5d0\uc11c 800M \ud30c\ub77c\ubbf8\ud130\uae4c\uc9c0 C4 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\uc2dc\ud0a8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\uc911\uce58\uc640 \ud65c\uc131\ud654 \ud568\uc218\ub294 1~4\ube44\ud2b8\ub85c \uc591\uc790\ud654\ub418\uc5c8\uace0, \ud1a0\ud070/\ud30c\ub77c\ubbf8\ud130 \ube44\uc728\uc740 100\uc73c\ub85c \uace0\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc555\ucd95\ub960\uc774 \ub192\uc544\uc9c8\uc218\ub85d \uace0\uc815\ub41c \uba54\ubaa8\ub9ac \ud06c\uae30\uc5d0\uc11c \ub354 \ub9ce\uc740 \ub370\uc774\ud130\uac00 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. QuEST\ub294 1\ube44\ud2b8 \uac00\uc911\uce58\uc640 \ud65c\uc131\ud654 \ud568\uc218(W1A1)\uc5d0\uc11c\ub3c4 \uc548\uc815\uc801\uc778 \ud559\uc2b5\uc744 \uac00\ub2a5\ud558\uac8c \ud558\uba70, QuEST W4A4 \ubaa8\ub378\uc740 BF16 \ubaa8\ub378\ubcf4\ub2e4 \ub354 \uc791\uc740 \ud06c\uae30\ub85c \ub354 \ub0ae\uc740 \uc190\uc2e4\uc744 \ub2ec\uc131\ud558\uc5ec Pareto \uc6b0\uc704\ub97c \ubcf4\uc785\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.05003/x2.png", "caption": "Figure 2: Gradient alignment comparison for a 30M Llama model after training on 2.7B tokens in 8-bit precision.", "description": "\uadf8\ub9bc 2\ub294 8\ube44\ud2b8 \uc815\ubc00\ub3c4\ub85c 27\uc5b5 \uac1c\uc758 \ud1a0\ud070\uc744 \ud559\uc2b5\uc2dc\ud0a8 \ud6c4 3\ucc9c\ub9cc \ub9e4\uac1c\ubcc0\uc218 Llama \ubaa8\ub378\uc5d0 \ub300\ud55c \uae30\uc6b8\uae30 \uc815\ub82c \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc138 \uac00\uc9c0 \ub2e4\ub978 \uae30\uc6b8\uae30 \ucd94\uc815 \ubc29\ubc95(Hadamard \ubcc0\ud658\uc774 \uc788\ub294 QuEST\uc758 \uc2e0\ub8b0 \ucd94\uc815\uae30, Hadamard \ubcc0\ud658\uc774 \uc5c6\ub294 QuEST\uc758 \uc2e0\ub8b0 \ucd94\uc815\uae30, STE)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac01 \ubcc0\ud658\uae30 \ube14\ub85d \uc774\ud6c4\uc758 \ud65c\uc131\ud654\uc5d0 \ub300\ud55c \uc911\uac04 \uae30\uc6b8\uae30\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. \ud65c\uc131\ud654 \uc591\uc790\ud654\ub97c \ube44\ud65c\uc131\ud654\ud558\uace0 '\ucc38' \uae30\uc6b8\uae30\ub97c \uacc4\uc0b0\ud558\uc5ec \uadf8\ub798\ub514\uc5b8\ud2b8 \uc815\ub82c(\ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4)\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4. \ub192\uc740 \uc720\uc0ac\ub3c4\ub294 \ucd94\uc815\uae30\uac00 \uc804\uccb4 \uc815\ubc00\ub3c4\uc5d0 \uac00\uae4c\uc6b4 \uace0\ud488\uc9c8 \uae30\uc6b8\uae30\ub97c \uc0dd\uc131\ud568\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  QuEST\uc758 \uc2e0\ub8b0 \ucd94\uc815\uae30(Hadamard \ubcc0\ud658 \ud3ec\ud568 \ubc0f \ubbf8\ud3ec\ud568)\ub294 \uae4a\uc774\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ub9e4\uc6b0 \ub192\uace0 \uc77c\uad00\ub41c \uc815\ub82c(0.8 \uc774\uc0c1)\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ubc18\uba74, STE\ub294 \uc815\ub82c\uc774 \ub0ae\uace0 \ubd84\uc0b0\uc774 \ud07d\ub2c8\ub2e4.  \uc774\ub294 QuEST\uc758 \uc2e0\ub8b0 \ucd94\uc815\uae30\uac00 \ub354 \ub098\uc740 \uae30\uc6b8\uae30 \ucd94\uc815\uc744 \uc81c\uacf5\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3. QUEST"}, {"figure_path": "https://arxiv.org/html/2502.05003/x3.png", "caption": "Figure 3: Perplexity (PPL) across bit-widths with QuEST vs. a tuned variant of LSQ on a 30M model. QuEST leads to consistently lower PPL, with the advantage growing with compression.", "description": "\uadf8\ub9bc 3\uc740 30M \ud06c\uae30\uc758 \ubaa8\ub378\uc744 \ub300\uc0c1\uc73c\ub85c, QuEST\uc640 \uc131\ub2a5\uc774 \uc870\uc815\ub41c LSQ \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5ec\ub7ec \ube44\ud2b8 \ub108\ube44\uc5d0\uc11c\uc758 perplexity(PPL)\ub97c \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. QuEST\ub294 \uc77c\uad00\ub418\uac8c \ub354 \ub0ae\uc740 PPL \uac12\uc744 \ub098\ud0c0\ub0b4\uba70, \uc555\ucd95\ub960\uc774 \ub192\uc544\uc9c8\uc218\ub85d \uadf8 \ucc28\uc774\uac00 \ub354\uc6b1 \ucee4\uc9c0\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc989, QuEST\ub294 \ubaa8\ub378 \ud06c\uae30 \uac10\uc18c\uc5d0 \ub530\ub978 \uc815\ud655\ub3c4 \uc800\ud558\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \uc644\ud654\ud55c\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "4.2. Comparison to Prior QAT Methods"}, {"figure_path": "https://arxiv.org/html/2502.05003/x4.png", "caption": "Figure 4: Illustration of the efficiency factors eff\u2062(P)/Peff\ud835\udc43\ud835\udc43\\text{eff}(P)/Peff ( italic_P ) / italic_P, arising from our analysis, for different numerical precisions P\ud835\udc43Pitalic_P and formats (INT, FP, INT+sparse). Higher is better. INT4 appears to have the highest efficiency among hardware-supported formats.", "description": "\uadf8\ub9bc 4\ub294 \ub2e4\uc591\ud55c \uc218\uce58\uc801 \uc815\ubc00\ub3c4(P)\uc640 \ud615\uc2dd(INT, FP, INT+sparse)\uc5d0 \ub300\ud574 \ubd84\uc11d\uc5d0\uc11c \ub3c4\ucd9c\ub41c \ud6a8\uc728\uc131 \uacc4\uc218 eff(P)/P\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud6a8\uc728\uc131 \uacc4\uc218\ub294 \ubaa8\ub378 \ud06c\uae30 \ub300\ube44 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0b4\ub294 \uc9c0\ud45c\uc774\uba70, \uac12\uc774 \ud074\uc218\ub85d \ud6a8\uc728\uc801\uc784\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uadf8\ub9bc\uc740 INT4\uac00 \ud558\ub4dc\uc6e8\uc5b4\uc5d0\uc11c \uc9c0\uc6d0\ud558\ub294 \ud615\uc2dd \uc911 \uac00\uc7a5 \ub192\uc740 \ud6a8\uc728\uc131\uc744 \ubcf4\uc784\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.  \uc989, INT4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0a4\ub294 \uac83\uc774 \ub2e4\ub978 \ud615\uc2dd\ubcf4\ub2e4 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9 \ub300\ube44 \ub354 \ub098\uc740 \uc815\ud655\ub3c4\ub97c \uc81c\uacf5\ud55c\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uc774\ub7ec\ud55c \ud6a8\uc728\uc131 \ud5a5\uc0c1\uc740 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \uc904\uc774\uace0 \ud6c8\ub828 \uc18d\ub3c4\ub97c \ub192\uc774\ub294 \ub370 \uae30\uc5ec\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.4. Finding the \u201cOptimal\u201d Precision"}, {"figure_path": "https://arxiv.org/html/2502.05003/x5.png", "caption": "Figure 5: Additional scaling laws induced by QuEST: (a, left) compares INT, FP, and INT+sparse formats at 4-bit precision, (b, middle) shows the scaling laws for weight-only quantization, where 2-bit appears to be Pareto-dominant, while (c, right) shows that trust estimation benefits significantly from Hadamard normalization.", "description": "\uadf8\ub9bc 5\ub294 QuEST\uc758 \ucd94\uac00\uc801\uc778 \ud655\uc7a5\uc131 \ubc95\uce59\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 4\ube44\ud2b8 \uc815\ubc00\ub3c4\uc5d0\uc11c INT, FP, INT+Sparse \ud615\uc2dd\ub4e4\uc744 \ube44\uad50\ud558\uace0, (b)\ub294 \uac00\uc911\uce58\ub9cc \uc591\uc790\ud654\ud588\uc744 \ub54c\uc758 \ud655\uc7a5\uc131 \ubc95\uce59\ub4e4\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub370, \uc5ec\uae30\uc11c 2\ube44\ud2b8\uac00 \ud30c\ub808\ud1a0 \ucd5c\uc801\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (c)\ub294 \uc2e0\ub8b0\ub3c4 \ucd94\uc815\uc774 Hadamard \uc815\uaddc\ud654\ub85c\ubd80\ud130 \uc0c1\ub2f9\ud55c \uc774\uc810\uc744 \uc5bb\ub294\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, QuEST\uac00 \ub2e4\uc591\ud55c \uc591\uc790\ud654 \ubc29\uc2dd\uacfc \ubaa8\ub378 \ud06c\uae30\uc5d0 \ub300\ud574 \uc548\uc815\uc801\uc778 \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  INT\uc640 FP, \uadf8\ub9ac\uace0 INT+Sparse \ube44\uad50\ub294 \ubaa8\ub378 \ud06c\uae30 \ub300\ube44 \uc815\ud655\ub3c4 \uce21\uba74\uc5d0\uc11c QuEST\uc758 \ud6a8\uc728\uc131\uc744,  \uac00\uc911\uce58 \uc591\uc790\ud654\ub9cc\uc758 \uacb0\uacfc\ub294 \uc591\uc790\ud654 \ube44\ud2b8 \uc218\ub97c \uc904\uc77c \ub54c QuEST\uc758 \uc131\ub2a5\uc744, Hadamard \uc815\uaddc\ud654\uc758 \ud6a8\uacfc\ub294 QuEST\uc758 \ud575\uc2ec \uc54c\uace0\ub9ac\uc998 \uc131\ub2a5\uc744 \uac01\uac01 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experimental Validation"}, {"figure_path": "https://arxiv.org/html/2502.05003/x6.png", "caption": "Figure 6: Per-layer speedups for QuEST INT4 vs BF16, on a single RTX 4090 GPU. The results take into account quantization/dequantization costs for QuEST, and include the cost of the Hadamard transform (orange bar). We present results for the 800M 4-bit QuEST model we trained, as well as inference speedups for a proportional 7B-parameter model.", "description": "\uadf8\ub9bc 6\uc740 \ub2e8\uc77c RTX 4090 GPU\uc5d0\uc11c QuEST INT4\uc640 BF16\uc758 \uacc4\uce35\ubcc4 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uacb0\uacfc\uc5d0\ub294 QuEST\uc758 \uc591\uc790\ud654/\uc5ed\uc591\uc790\ud654 \ube44\uc6a9\uacfc Hadamard \ubcc0\ud658 \ube44\uc6a9(\uc8fc\ud669\uc0c9 \ub9c9\ub300)\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ud6c8\ub828\ub41c 8\uc5b5 \ub9e4\uac1c\ubcc0\uc218 QuEST 4\ube44\ud2b8 \ubaa8\ub378\uacfc \ube44\ub840\ud558\ub294 70\uc5b5 \ub9e4\uac1c\ubcc0\uc218 \ubaa8\ub378\uc758 \ucd94\ub860 \uc18d\ub3c4 \ud5a5\uc0c1 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \uac01 \uacc4\uce35\uc758 \uc5f0\uc0b0\uc5d0 \ub300\ud55c \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc8fc\uba70, Hadamard \ubcc0\ud658\uc758 \uc601\ud5a5\uacfc \uc591\uc790\ud654/\uc5ed\uc591\uc790\ud654\uc5d0 \ub4dc\ub294 \uc624\ubc84\ud5e4\ub4dc\uae4c\uc9c0 \uace0\ub824\ud55c \uacb0\uacfc\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c, 8\uc5b5 \ub9e4\uac1c\ubcc0\uc218 \ubaa8\ub378\ubfd0 \uc544\ub2c8\ub77c, 70\uc5b5 \ub9e4\uac1c\ubcc0\uc218 \ubaa8\ub378\uc5d0 \ub300\ud55c \ucd94\ub860 \uc18d\ub3c4 \ud5a5\uc0c1 \uacb0\uacfc\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ud558\uc5ec \ubaa8\ub378 \ud06c\uae30\uc758 \uc601\ud5a5\uae4c\uc9c0 \uace0\ub824\ud55c \ubd84\uc11d\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "5. GPU Execution Support for QuEST Models"}, {"figure_path": "https://arxiv.org/html/2502.05003/x7.png", "caption": "Figure 7: End-to-end prefill speedups for QuEST INT4 vs BF16, across different batch sizes, using the 800M parameter model on a single RTX 4090 GPU. As expected, QuEST is most effective for larger batch sizes, where the workload is more compute-bound.", "description": "\uadf8\ub9bc 7\uc740 \ub2e8\uc77c RTX 4090 GPU\uc5d0\uc11c 8\uc5b5 \uac1c \ub9e4\uac1c\ubcc0\uc218 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \ubc30\uce58 \ud06c\uae30\uc5d0 \ub530\ub978 QuEST INT4\uc640 BF16\uc758 \uc885\ub2e8 \uac04 \uc0ac\uc804 \ucc44\uc6b0\uae30 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. QuEST\ub294 \uacc4\uc0b0 \uc791\uc5c5\uc774 \ub354 \ub9ce\uc740 \ud070 \ubc30\uce58 \ud06c\uae30\uc5d0\uc11c \ub354 \ud6a8\uacfc\uc801\uc785\ub2c8\ub2e4.", "section": "4.6 GPU \ucee4\ub110 \uc9c0\uc6d0"}, {"figure_path": "https://arxiv.org/html/2502.05003/x8.png", "caption": "Figure 8: Fraction of weights for which M\u03b1\u2217=0subscript\ud835\udc40superscript\ud835\udefc0M_{\\alpha^{*}}=0italic_M start_POSTSUBSCRIPT italic_\u03b1 start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = 0 as a function of number of training iterations for a 30M model trained with QuEST.", "description": "\uadf8\ub9bc 8\uc740 QuEST\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ub41c 30M \ubaa8\ub378\uc5d0 \ub300\ud55c \ud559\uc2b5 \ubc18\ubcf5 \ud69f\uc218\uc758 \ud568\uc218\ub85c\uc11c, M\u03b1\u2217=0M_{\"{\u03b1}^{*}\"=0}\uc778 \uac00\uc911\uce58\uc758 \ube44\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 Hadamard \ubcc0\ud658(HT)\uc758 \uc720\ubb34\uc5d0 \ub530\ub978 \uac00\uc911\uce58 \ub9c8\uc2a4\ud06c\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc2dc\uac01\uc801 \uc790\ub8cc\uc785\ub2c8\ub2e4.  HT\ub97c \uc801\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\ub294 \uc608\uc0c1\ubcf4\ub2e4 \ud6e8\uc52c \ub9ce\uc740 \uac00\uc911\uce58\uac00 \ub9c8\uc2a4\ud06c \ucc98\ub9ac\ub418\uc5c8\uace0, \uadf8 \ube44\uc728\uc774 \ud559\uc2b5\uc774 \uc9c4\ud589\ub428\uc5d0 \ub530\ub77c \uc77c\uc815\ud558\uc9c0 \uc54a\uc558\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubc18\uba74 HT\ub97c \uc801\uc6a9\ud588\uc744 \ub54c\ub294 \ub9c8\uc2a4\ud06c\ub41c \uac00\uc911\uce58\uc758 \ube44\uc728\uc774 \uc608\uc0c1\uce58\uc5d0 \ub354 \uac00\uae4c\uc6e0\uace0, \ud6e8\uc52c \uc548\uc815\uc801\uc73c\ub85c \ubcc0\ud654\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 HT\uac00 QuEST\uc758 \uac00\uc911\uce58 \ub9c8\uc2a4\ud06c \ud6a8\uc728\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \uc99d\uba85\ud569\ub2c8\ub2e4.", "section": "A.1 Trust Mask Analysis"}, {"figure_path": "https://arxiv.org/html/2502.05003/x9.png", "caption": "Figure 9: Fraction of masked values retained from an old iteration to a new iteration for a 30M model trained with QuEST W8A8.", "description": "\uadf8\ub9bc 9\ub294 QuEST W8A8\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ub41c 30M \ubaa8\ub378\uc5d0\uc11c \uc774\uc804 \ubc18\ubcf5\uc5d0\uc11c \uc0c8\ub85c\uc6b4 \ubc18\ubcf5\uc73c\ub85c \uc720\uc9c0\ub418\ub294 \ub9c8\uc2a4\ud06c\ub41c \uac12\uc758 \ube44\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 Hadamard \ubcc0\ud658\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud574,  Hadamard \ubcc0\ud658\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0\uc640 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0 \ub450 \uac00\uc9c0 \uc0c1\ud669\uc744 \ube44\uad50\ud558\uc5ec \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uac01 \uc140\uc740 \ud2b9\uc815 \uc774\uc804 \ubc18\ubcf5\uc5d0\uc11c \ub9c8\uc2a4\ud06c\ub41c \uac12 \uc911 \uc5bc\ub9c8\ub098 \ub9ce\uc740 \ube44\uc728\uc774 \ub098\uc911 \ubc18\ubcf5\uc5d0\uc11c\ub3c4 \ub9c8\uc2a4\ud06c\ub41c \uc0c1\ud0dc\ub85c \ub0a8\uc544\uc788\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ubc31\ubd84\uc728\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 Hadamard \ubcc0\ud658\uc774 \ub9c8\uc2a4\ud06c\uc758 \uc9c0\uc18d\uc131\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5, \uc989 \ub9c8\uc2a4\ud06c\ub41c \uac12\uc758 \uc548\uc815\uc131\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ud3c9\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  Hadamard \ubcc0\ud658\uc774 \uc801\uc6a9\ub41c \uacbd\uc6b0 \ub9c8\uc2a4\ud06c\ub41c \uac12\uc758 \uc9c0\uc18d\uc131\uc774 \uac10\uc18c\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "A. \ucd94\uac00\uc801\uc778 \"\uc2e0\ub8b0\" \uc138\ubd80 \uc815\ubcf4"}, {"figure_path": "https://arxiv.org/html/2502.05003/x10.png", "caption": "Figure 10: Performance of QuEST as a function of the outer trust scaling factor s\ud835\udc60sitalic_s for a 30M model pretraining.", "description": "\uadf8\ub9bc 10\uc740 3000\ub9cc \ub9e4\uac1c\ubcc0\uc218\ub97c \uac00\uc9c4 \ubaa8\ub378\uc758 \uc0ac\uc804 \ud6c8\ub828\uc5d0\uc11c \uc678\ubd80 \uc2e0\ub8b0 \uc2a4\ucf00\uc77c\ub9c1 \uacc4\uc218(s)\uac00 QuEST \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. QuEST\ub294 \uc2e0\ub8b0 \uae30\ubc18 \uae30\uc6b8\uae30 \ucd94\uc815\uae30\ub97c \uc0ac\uc6a9\ud558\ub294 \uc591\uc790\ud654 \uc778\uc2dd \ud6c8\ub828(QAT) \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc774 \uadf8\ub798\ud504\ub294 \ub2e4\uc591\ud55c s \uac12\uc5d0 \ub530\ub978 \uac80\uc99d \uc190\uc2e4\uc744 \ub098\ud0c0\ub0b4\uc5b4 \ucd5c\uc801\uc758 s \uac12\uc744 \uacb0\uc815\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  \uc678\ubd80 \uc2e0\ub8b0 \uc2a4\ucf00\uc77c\ub9c1 \uacc4\uc218\ub294 \uc591\uc790\ud654 \uc624\ucc28\uac00 \ud070 \ud56d\ubaa9\uc5d0 \ub300\ud55c \uae30\uc6b8\uae30 \uc5c5\ub370\uc774\ud2b8\uc758 \uc911\uc694\ub3c4\ub97c \uc870\uc808\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "3. QUEST"}, {"figure_path": "https://arxiv.org/html/2502.05003/x11.png", "caption": "Figure 11: Training loss curves for a 30M model trained on 3B tokens with W4A4 bitwidth, comparing QuEST (ours), LSQ, PACT, and BF16. (a) Full training loss curves, showing that QuEST closely follows BF16 and consistently outperforms LSQ, while PACT struggles with high loss. (b) Zoomed-in view of training steps after 1000, excluding PACT for clarity, highlighting that QuEST maintains a lower loss than LSQ throughout training.", "description": "\uadf8\ub9bc 11\uc740 30M \ub9e4\uac1c\ubcc0\uc218 \ubaa8\ub378\uc744 3B \ud1a0\ud070\uc73c\ub85c W4A4 \ube44\ud2b8 \ub108\ube44\ub85c \ud559\uc2b5\uc2dc\ud0a8 \uacb0\uacfc\uc5d0 \ub300\ud55c \uc190\uc2e4 \uace1\uc120\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. QuEST, LSQ, PACT, BF16 \ub124 \uac00\uc9c0 \ubc29\ubc95\uc744 \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4. (a) \uc804\uccb4 \ud559\uc2b5 \uc190\uc2e4 \uace1\uc120\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\uc5d0\uc11c\ub294 QuEST\uac00 BF16\uacfc \uc720\uc0ac\ud558\uac8c \ud559\uc2b5\uc774 \uc798 \ub418\ub294 \ubc18\uba74 LSQ\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ud6e8\uc52c \uc6b0\uc218\ud558\uace0, PACT\ub294 \uc190\uc2e4\uc774 \ub9e4\uc6b0 \ucee4\uc11c \ud559\uc2b5\uc774 \uc81c\ub300\ub85c \ub418\uc9c0 \uc54a\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (b) 1000\ubc88\uc758 \ubc18\ubcf5 \uc774\ud6c4 \ud559\uc2b5 \ub2e8\uacc4\ub97c \ud655\ub300\ud558\uc5ec \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\uc5d0\uc11c\ub294 QuEST\uac00 LSQ\ubcf4\ub2e4 \ud6e8\uc52c \ub0ae\uc740 \uc190\uc2e4 \uac12\uc744 \uc720\uc9c0\ud558\uba70 \ud559\uc2b5 \uc804 \uacfc\uc815\uc5d0\uc11c \uc548\uc815\uc801\uc774\uace0 \uc815\ud655\ud55c \uacb0\uacfc\ub97c \uc5bb\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experimental Validation"}, {"figure_path": "https://arxiv.org/html/2502.05003/x12.png", "caption": "Figure 12: Hyperparameter search for PACT on a 30M parameter model with 4-bit weights and activations, trained on 10% of the dataset. The search explores different values for learning rate scaling (LR Scale) and alpha weight decay, with validation loss indicated by the color gradient. Lower validation loss (darker colors) corresponds to better configurations.", "description": "\uadf8\ub9bc 12\ub294 30M \ub9e4\uac1c\ubcc0\uc218 \ubaa8\ub378\uc5d0 \ub300\ud574 4\ube44\ud2b8 \uac00\uc911\uce58\uc640 \ud65c\uc131\ud654 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub370\uc774\ud130\uc14b\uc758 10%\ub85c \ud559\uc2b5\uc2dc\ud0a8 PACT\uc5d0 \ub300\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac80\uc0c9 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac80\uc0c9\uc740 \ud559\uc2b5\ub960 \uc2a4\ucf00\uc77c\ub9c1(LR Scale)\uacfc \uc54c\ud30c \uac00\uc911\uce58 \uac10\uc1e0\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \uac12\uc744 \ud0d0\uc0c9\ud558\uba70, \uac80\uc99d \uc190\uc2e4\uc740 \uc0c9\uc0c1 \uae30\uc6b8\uae30\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4. \uac80\uc99d \uc190\uc2e4\uc774 \ub354 \ub0ae\uc744\uc218\ub85d(\uc5b4\ub450\uc6b4 \uc0c9\uc0c1) \ub354 \ub098\uc740 \uad6c\uc131\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 \ud559\uc2b5\ub960 \uc2a4\ucf00\uc77c\ub9c1\uacfc \uac00\uc911\uce58 \uac10\uc1e0\uc758 \ucd5c\uc801 \uc870\ud569\uc744 \ucc3e\ub294 \uacfc\uc815\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 \uad6c\ud604 \uc138\ubd80 \uc815\ubcf4"}, {"figure_path": "https://arxiv.org/html/2502.05003/extracted/6183871/figures/PACT-hparam-search.png", "caption": "Figure 13: Scaling law\u00a0(5) fit for 3 and 4 bit QuEST with tokens/parameters ratios in {25,50,100}2550100\\{25,50,100\\}{ 25 , 50 , 100 }.", "description": "\uadf8\ub9bc 13\uc740 \ud1a0\ud070/\ud30c\ub77c\ubbf8\ud130 \ube44\uc728\uc774 {25, 50, 100}\uc778 3\ube44\ud2b8 \ubc0f 4\ube44\ud2b8 QuEST\uc5d0 \ub300\ud55c \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59 (5)\uc758 \uc801\ud569 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ubaa8\ub378 \ud06c\uae30\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc190\uc2e4\uc774 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\ub85c, QuEST\uc758 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \ubaa8\ub378 \ud06c\uae30\uc5d0\uc11c \ud3c9\uac00\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4. \ud2b9\ud788, \ud1a0\ud070/\ud30c\ub77c\ubbf8\ud130 \ube44\uc728\uc744 \ubcc0\ud654\uc2dc\ud0a4\uba74\uc11c 3\ube44\ud2b8\uc640 4\ube44\ud2b8 QuEST\uc758 \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uc5b4\ub5a4 \ube44\uc728\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294\uc9c0 \ud655\uc778\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "4.3. \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59"}, {"figure_path": "https://arxiv.org/html/2502.05003/x13.png", "caption": "Figure 14: Different QuEST precision performance as a function of tokens-to-parameters ratio at a fixed model memory footprint. The gray line indicates a 4-bit optimality threshold.", "description": "\uadf8\ub9bc 14\ub294 \uace0\uc815\ub41c \ubaa8\ub378 \uba54\ubaa8\ub9ac \uc6a9\ub7c9\uc5d0\uc11c \ud1a0\ud070-\ud30c\ub77c\ubbf8\ud130 \ube44\uc728\uc758 \ud568\uc218\ub85c\uc11c \ub2e4\uc591\ud55c QuEST \uc815\ubc00\ub3c4\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub798\ud504\ub294 \ubaa8\ub378 \ud06c\uae30\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c(1.6GB, 6.4GB, 14GB, 140GB) 4\ube44\ud2b8 \uc815\ubc00\ub3c4\uac00 \ucd5c\uc801\uc810\uc5d0 \ub3c4\ub2ec\ud558\ub294 \uc2dc\uc810\uc774 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud68c\uc0c9\uc120\uc740 4\ube44\ud2b8 \ucd5c\uc801\uc131 \uc784\uacc4\uac12\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc989, \ud2b9\uc815 \ubaa8\ub378 \ud06c\uae30\uc5d0\uc11c \ud1a0\ud070-\ud30c\ub77c\ubbf8\ud130 \ube44\uc728\uc774 \uc99d\uac00\ud558\uba74 \ud2b9\uc815 \uc2dc\uc810\ubd80\ud130 4\ube44\ud2b8\uac00 \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4.", "section": "4.4. Finding the \u201cOptimal\u201d Precision"}]