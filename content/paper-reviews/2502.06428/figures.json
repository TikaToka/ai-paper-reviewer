[{"figure_path": "https://arxiv.org/html/2502.06428/extracted/6195800/figures/motivation_new.png", "caption": "Figure 1: The effects of changing shot-sampling rates on video understanding\ntask performance on videos of different lengths in the\nVideoMME\u00a0(Fu et\u00a0al., 2024a) dataset. Two models are evaluated\nincluding LongVA\u00a0(Zhang et\u00a0al., 2024a) and\nVideo-XL\u00a0(Shu et\u00a0al., 2024). As the number of sampled shots increased, performance did not consistently improve across various video lengths. That is because while sparse sampling may miss crucial details, exhaustive sampling often overwhelms the model with excessive irrelevant content. This illustrates the key challenge of optimal shot selection especially in long video understanding. That is, how to sample variable details in order to maximise semantic task information extraction whilst minimising distractions from irrelevant details (noise) in video understanding.", "description": "\uadf8\ub9bc 1\uc740 VideoMME \ub370\uc774\ud130\uc14b(Fu et al., 2024a)\uc5d0\uc11c \ub2e4\uc591\ud55c \uae38\uc774\uc758 \ube44\ub514\uc624\uc5d0 \ub300\ud55c \ube44\ub514\uc624 \uc774\ud574 \uc791\uc5c5 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc0f7 \uc0d8\ud50c\ub9c1 \ube44\uc728 \ubcc0\ud654\uc758 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LongVA(Zhang et al., 2024a)\uc640 Video-XL(Shu et al., 2024) \ub450 \uac00\uc9c0 \ubaa8\ub378\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. \uc0d8\ud50c\ub9c1\ub41c \uc0f7\uc758 \uc218\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ub2e4\uc591\ud55c \ube44\ub514\uc624 \uae38\uc774\uc5d0 \uac78\uccd0 \uc131\ub2a5\uc774 \uc77c\uad00\ub418\uac8c \ud5a5\uc0c1\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \ub4dc\ubb38 \uc0d8\ud50c\ub9c1\uc740 \uc911\uc694\ud55c \uc138\ubd80 \uc815\ubcf4\ub97c \ub193\uce60 \uc218 \uc788\uace0, \ubc18\ub300\ub85c \uacfc\ub3c4\ud55c \uc0d8\ud50c\ub9c1\uc740 \ubaa8\ub378\uc744 \uacfc\ub3c4\ud55c \ubb34\uad00\ud55c \ucf58\ud150\uce20\ub85c \uc555\ub3c4\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \uc774\ub294 \ud2b9\ud788 \uae34 \ube44\ub514\uc624 \uc774\ud574\uc5d0\uc11c \ucd5c\uc801\uc758 \uc0f7 \uc120\ud0dd\uc774\ub77c\ub294 \uc8fc\uc694 \uacfc\uc81c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, \ube44\ub514\uc624 \uc774\ud574\uc5d0\uc11c \ubb34\uad00\ud55c \uc138\ubd80 \uc815\ubcf4(\uc7a1\uc74c)\ub85c \uc778\ud55c \ubc29\ud574\ub97c \ucd5c\uc18c\ud654\ud558\uba74\uc11c \uc758\ubbf8\uc801 \uc791\uc5c5 \uc815\ubcf4 \ucd94\ucd9c\uc744 \uadf9\ub300\ud654\ud558\uae30 \uc704\ud574 \uac00\ubcc0\uc801\uc778 \uc138\ubd80 \uc815\ubcf4\ub97c \uc5b4\ub5bb\uac8c \uc0d8\ud50c\ub9c1\ud560 \uac83\uc778\uac00 \ud558\ub294 \uac83\uc785\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.06428/extracted/6195800/figures/motivation_sce2_v3.png", "caption": "Figure 2: The critical problem of how to select shots in video understanding.\nIn a video that depicts how a boy gradually gains a dragon\u2019s trust,\ndifferent sampling methods create two distinct narratives: split\nvideo A shows the boy being attacked by the dragon, while split\nvideo B shows him happily sharing food with the dragon.\nThis shows that minor differences in video sampling leads to significant variations in semantic understanding (interpretation).", "description": "\uadf8\ub9bc 2\ub294 \ube44\ub514\uc624 \uc774\ud574\uc5d0\uc11c \uc5b4\ub5bb\uac8c \uc0f7\uc744 \uc120\ud0dd\ud558\ub294 \uac83\uc774 \uc911\uc694\ud55c\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uc18c\ub144\uc774 \uc810\ucc28\uc801\uc73c\ub85c \uc6a9\uc758 \uc2e0\ub8b0\ub97c \uc5bb\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ube44\ub514\uc624\uc5d0\uc11c, \uc11c\ub85c \ub2e4\ub978 \uc0d8\ud50c\ub9c1 \ubc29\ubc95\uc5d0 \ub530\ub77c \uc644\uc804\ud788 \ub2e4\ub978 \uc2a4\ud1a0\ub9ac\uac00 \ub9cc\ub4e4\uc5b4\uc9d1\ub2c8\ub2e4. A \ube44\ub514\uc624\uc5d0\uc11c\ub294 \uc18c\ub144\uc774 \uc6a9\uc5d0\uac8c \uacf5\uaca9\uc744 \ubc1b\ub294 \ubc18\uba74, B \ube44\ub514\uc624\uc5d0\uc11c\ub294 \uc18c\ub144\uc774 \uc6a9\uacfc \ud568\uaed8 \uc74c\uc2dd\uc744 \ub098\ub204\ub294 \ubaa8\uc2b5\uc774 \ubcf4\uc5ec\uc9d1\ub2c8\ub2e4. \uc774\ub294 \ube44\ub514\uc624 \uc0d8\ud50c\ub9c1\uc758 \uc791\uc740 \ucc28\uc774\uac00 \uc758\ubbf8 \ud574\uc11d\uc5d0 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce60 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.06428/extracted/6195800/figures/framework_v5.png", "caption": "Figure 3: The overall framework of CoS. It first utilises LLaVA to\nperform a mosaicing binary coding to bootstrap video\nsummarisation for temporal grounding on a long video. Specifically, every\nfour shots are aggregated into a mosaicing composition image. LLaVA\ndetermines whether task-related elements exist within each\ncomposition image by encoding a binary value of 1 or 0 (\u2018yes\u2019\nor \u2018no\u2019),\nthereby identifying sparsely distributed task-related shots to\nachieve pseudo temporal grounding. Given this binary video\nsummary, task-related positive shots\nSpsuperscript\ud835\udc46\ud835\udc5dS^{p}italic_S start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT and irrelevant negative shots\nSnsuperscript\ud835\udc46\ud835\udc5bS^{n}italic_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT are generated and represented by binary codes.\nSpsuperscript\ud835\udc46\ud835\udc5dS^{p}italic_S start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT, Snsuperscript\ud835\udc46\ud835\udc5bS^{n}italic_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and the original frame sequence X\ud835\udc4bXitalic_X sampled from\noriginal video V\ud835\udc49Vitalic_V are then fed into the MLLM for co-reasoning, minimising\ninterference of irrelevant video content.", "description": "\uadf8\ub9bc 3\uc740 CoS\uc758 \uc804\uccb4 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uba3c\uc800 LLaVA\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\uc790\uc774\ud06c \uc774\uc9c4 \ucf54\ub529\uc744 \uc218\ud589\ud558\uc5ec \uae34 \ube44\ub514\uc624\uc5d0 \ub300\ud55c \uc2dc\uac04\uc801 \uae30\ubc18\uc744 \ub9c8\ub828\ud558\uae30 \uc704\ud55c \ube44\ub514\uc624 \uc694\uc57d\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c, \ub9e4 4\uac1c\uc758 \uc0f7\uc744 \ubaa8\uc544 \ubaa8\uc790\uc774\ud06c \ud569\uc131 \uc774\ubbf8\uc9c0\ub97c \ub9cc\ub4ed\ub2c8\ub2e4. LLaVA\ub294 \uac01 \ud569\uc131 \uc774\ubbf8\uc9c0 \ub0b4\uc5d0 \uc791\uc5c5 \uad00\ub828 \uc694\uc18c\uac00 \uc788\ub294\uc9c0 \uc5ec\ubd80\ub97c \ud310\ub2e8\ud558\uc5ec 1 \ub610\ub294 0( '\uc608' \ub610\ub294 '\uc544\ub2c8\uc624')\uc758 \uc774\uc9c4 \uac12\uc73c\ub85c \uc778\ucf54\ub529\ud558\uc5ec, \ub4dc\ubb3c\uac8c \ubd84\ud3ec\ub41c \uc791\uc5c5 \uad00\ub828 \uc0f7\uc744 \uc2dd\ubcc4\ud558\uace0 \uc758\uc0ac \uc2dc\uac04\uc801 \uae30\ubc18\uc744 \ub9c8\ub828\ud569\ub2c8\ub2e4. \uc774 \uc774\uc9c4 \ube44\ub514\uc624 \uc694\uc57d\uc744 \ubc14\ud0d5\uc73c\ub85c \uc791\uc5c5 \uad00\ub828 \uc591\uc131 \uc0f7 S<sup>p</sup>\uc640 \ubb34\uad00\ud55c \uc74c\uc131 \uc0f7 S<sup>n</sup>\uc744 \uc0dd\uc131\ud558\uace0 \uc774\uc9c4 \ucf54\ub4dc\ub85c \ud45c\ud604\ud569\ub2c8\ub2e4. S<sup>p</sup>, S<sup>n</sup>, \uadf8\ub9ac\uace0 \uc6d0\ubcf8 \ube44\ub514\uc624 V\uc5d0\uc11c \uc0d8\ud50c\ub9c1\ub41c \uc6d0\ubcf8 \ud504\ub808\uc784 \uc2dc\ud000\uc2a4 X\ub97c MLLM\uc5d0 \uacf5\ub3d9 \ucd94\ub860\ud558\uc5ec \ubb34\uad00\ud55c \ube44\ub514\uc624 \ucf58\ud150\uce20\uc758 \uac04\uc12d\uc744 \ucd5c\uc18c\ud654\ud569\ub2c8\ub2e4.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2502.06428/extracted/6195800/figures/sample_v3.png", "caption": "Figure 4: An qualitative evaluation example from MLVU\u00a0(Zhou et\u00a0al., 2024) dataset.", "description": "\uadf8\ub9bc 4\ub294 MLVU \ub370\uc774\ud130\uc14b(Zhou et al., 2024)\uc5d0\uc11c \uc815\uc131\uc801 \ud3c9\uac00\uc758 \ud55c \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub2f5\ubcc0\uc744 \ub3c4\ucd9c\ud558\uae30 \uc704\ud574 \ubaa8\ub378\uc774 \ube44\ub514\uc624\uc758 \uad00\ub828 \ubd80\ubd84\uc744 \uc120\ud0dd\ud558\uace0 \ud574\uc11d\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2dc\uac01\uc801 \uc124\uba85\uc785\ub2c8\ub2e4.  \ube44\ub514\uc624\uc758 \ub2e4\uc591\ud55c \uc7a5\uba74\ub4e4\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uae0d\uc815\uc801(Positive Video SP)\uacfc \ubd80\uc815\uc801(Negative Video SN) \uc2dc\ud000\uc2a4\ub4e4\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \ubaa8\ub378\uc774 \uc5b4\ub5bb\uac8c \uc774\ub7ec\ud55c \uc2dc\ud000\uc2a4\ub4e4\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc9c8\ubb38\uc5d0 \ub300\ud55c \uc815\ud655\ud55c \ub2f5\uc744 \ucc3e\uc544\ub0b4\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc744 \ud1b5\ud574, \uc81c\uc548\ub41c \ubc29\ubc95\uc778 CoS\uac00 \uc2dc\uac04\uc801\uc73c\ub85c \uba40\ub9ac \ub5a8\uc5b4\uc838 \uc788\uac70\ub098 \uc0b0\ubc1c\uc801\uc73c\ub85c \ubd84\ud3ec\ub41c \uc911\uc694\ud55c \uc815\ubcf4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ucd94\ucd9c\ud558\uace0, \ube44\ub514\uc624\uc758 \ubb34\uad00\ud55c \ubd80\ubd84\uc73c\ub85c \uc778\ud55c \ubc29\ud574\ub97c \ucd5c\uc18c\ud654\ud568\uc73c\ub85c\uc368 \uc7a5\ubb38\uc758 \ube44\ub514\uc624\ub97c \uc774\ud574\ud558\ub294 \ub370 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experiments"}]