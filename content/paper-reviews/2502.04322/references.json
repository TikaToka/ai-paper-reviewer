{"references": [{"fullname_first_author": "Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-04-05", "reason": "This paper is foundational to the work on safety alignment, providing a benchmark for reinforcement learning from human feedback techniques that are later analyzed in this paper."}, {"fullname_first_author": "Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-15", "reason": "This paper introduces a strong baseline jailbreak method (GCG-T) that is later improved upon by this paper's proposed SPEAK EASY framework."}, {"fullname_first_author": "Mazeika", "paper_title": "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal", "publication_date": "2024-00-00", "reason": "This paper provides a crucial benchmark (HarmBench) for evaluating the harmfulness of LLM outputs, which is directly relevant to the evaluation of jailbreaks presented in this paper."}, {"fullname_first_author": "Mehrotra", "paper_title": "Tree of attacks: Jailbreaking black-box LLMs automatically", "publication_date": "2023-12-02", "reason": "This paper introduces another strong baseline jailbreak method (TAP-T) that is compared against in this paper's experimental setup."}, {"fullname_first_author": "Xie", "paper_title": "Systematically evaluating large language model safety refusal behaviors", "publication_date": "2024-00-00", "reason": "This paper provides another relevant benchmark (SORRY-Bench) that is used to evaluate the performance of different jailbreak techniques."}]}