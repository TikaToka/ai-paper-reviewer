{"references": [{"fullname_first_author": "Adnan, M.", "paper_title": "Keyformer: Kv cache reduction through key tokens selection for efficient generative inference", "publication_date": "2024-00-00", "reason": "This paper proposes an innovative approach to address the KV cache size challenge through selective token retention strategies, which is a key topic of the main paper."}, {"fullname_first_author": "Brown, T.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This foundational paper introduces the concept of large language models as few-shot learners, which is highly relevant to the main paper's exploration of LLMs' fundamental capabilities."}, {"fullname_first_author": "Cobbe, K.", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-00-00", "reason": "This paper introduces GSM8K, a benchmark for evaluating arithmetic reasoning capabilities, which is a core task used in the main paper's evaluation."}, {"fullname_first_author": "Hendrycks, D.", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2020-00-00", "reason": "This paper introduces MMLU, a benchmark for assessing world knowledge capabilities, which is used in the main paper's comprehensive evaluation across diverse tasks."}, {"fullname_first_author": "Liu, X.", "paper_title": "ChunkKV: Semantic-preserving KV cache compression for efficient long-context LLM inference", "publication_date": "2025-00-00", "reason": "This paper introduces ChunkKV, a novel compression approach that distinctly handles prefill and decoding phases, which is compared to the main paper's proposed ShotKV method."}]}