{"references": [{"fullname_first_author": "Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "This paper introduces LoRA, a foundational low-rank adaptation technique that is central to the proposed FSLORA method and much of the related work discussed."}, {"fullname_first_author": "McMahan", "paper_title": "Communication-efficient learning of deep networks from decentralized data", "publication_date": "2017-00-00", "reason": "This is a seminal paper in federated learning, providing the theoretical underpinnings for collaborative fine-tuning which is a core component of the FSLORA approach."}, {"fullname_first_author": "Bai", "paper_title": "Federated fine-tuning of large language models under heterogeneous tasks and client resources", "publication_date": "2024-02-00", "reason": "This paper directly addresses the challenge of heterogeneous device resources in federated LoRA, making it highly relevant to the FSLORA's goal of resource-adaptive fine-tuning."}, {"fullname_first_author": "Wang", "paper_title": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "publication_date": "2018-04-00", "reason": "The GLUE benchmark is used for experimental evaluation of the FSLORA model, and this paper describes the benchmark tasks and their importance in NLP."}, {"fullname_first_author": "Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-00", "reason": "The LLaMA model is used in the experiments, and this paper describes this family of models, which are important in the current landscape of LLMs."}]}