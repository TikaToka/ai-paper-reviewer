[{"content": "| Method | GPU hours | QNLI | MRPC | CoLA | MNLI | RTE | SST-2 | QQP | Avg. |\n|---|---|---|---|---|---|---|---|---|---|\n| HeteroLoRA | 10.7h | 87.5 \u00b10.5 | 84.4 \u00b10.9 | 75.3 \u00b11.2 | 66.3 \u00b10.8 | 69.0 \u00b11.7 | 89.5 \u00b10.0 | 85.3 \u00b10.1 | 79.6 |\n| FlexLoRA | 12.6h | 88.5 \u00b10.2 | 81.2 \u00b10.4 | 77.5 \u00b11.2 | 63.0 \u00b10.5 | 62.2 \u00b11.9 | 92.8 \u00b10.4 | 87.4 \u00b10.1 | 78.9 |\n| FedStackLoRA | 12.3h | 87.2 \u00b10.3 | 78.1 \u00b10.7 | 77.4 \u00b11.7 | 74.6 \u00b10.5 | 54.4 \u00b12.1 | 93.4 \u00b10.1 | 87.1 \u00b10.3 | 78.9 |\n| FSLoRA | 10.9h | 88.0 \u00b10.3 | 87.3 \u00b10.2 | 82.2 \u00b10.5 | 76.4 \u00b10.2 | 69.8 \u00b11.2 | 93.5 \u00b10.1 | 85.8 \u00b10.1 | 83.3 |", "caption": "Table 5.1: Testing accuracy over 3 independent runs for fine-tuning the RoBERTa model on the GLUE benchmark.\nFSLoRA achieves a notable improvement in average performance compared to the baselines.", "description": "\ud45c 5.1\uc740 RoBERTa \ubaa8\ub378\uc744 GLUE \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ubbf8\uc138 \uc870\uc815\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc138 \ubc88\uc758 \ub3c5\ub9bd\uc801\uc778 \uc2e4\ud589\uc5d0 \ub300\ud55c \ud14c\uc2a4\ud2b8 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0b4\uba70, FSLORA\uac00 \uae30\uc874 \ubc29\ubc95\ub4e4\uc5d0 \ube44\ud574 \ud3c9\uade0 \uc131\ub2a5\uc774 \ub208\uc5d0 \ub744\uac8c \ud5a5\uc0c1\ub418\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubc29\ubc95\uc5d0 \ub300\ud55c GPU \uc2dc\uac04, QNLI, MRPC, COLA, MNLI, RTE, SST-2, QQP \ubc0f \ud3c9\uade0 \uc815\ud655\ub3c4\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.2 \uc8fc\uc694 \uacb0\uacfc"}, {"content": "| Method | GPU hours | ARC-c | ARC-e | BoolQ | HellaSwag | OBQA | PIQA | SIQA | WinoGrande | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| HeteroLoRA | 44.2h | 69.2 \u00b10.2 | 84.6 \u00b10.2 | 68.4 \u00b10.5 | 80.0 \u00b10.7 | 69.9 \u00b10.0 | 77.3 \u00b10.0 | 68.7 \u00b10.3 | 72.0 \u00b10.3 | 73.8 |\n| FlexLoRA | 60.8h | 69.9 \u00b10.3 | 84.7 \u00b10.4 | 66.9 \u00b10.2 | 80.5 \u00b10.4 | 72.3 \u00b10.1 | 78.1 \u00b10.2 | 70.4 \u00b10.4 | 73.3 \u00b10.5 | 74.5 |\n| FedStackLoRA | 56.2h | 67.5 \u00b10.7 | 83.1 \u00b10.5 | 65.8 \u00b10.9 | 78.4 \u00b10.5 | 69.2 \u00b10.7 | 75.5 \u00b10.6 | 67.1 \u00b10.3 | 71.5 \u00b10.5 | 72.3 |\n| FSLoRA | 44.5h | 73.8 \u00b10.6 | 86.2 \u00b10.1 | 68.5 \u00b10.1 | 83.1 \u00b11.1 | 78.7 \u00b10.3 | 82.0 \u00b10.2 | 75.8 \u00b10.0 | 74.8 \u00b10.6 | 77.9 |", "caption": "Table 5.2: \nTesting accuracy over 3 independent runs for fine-tuning the LLaMA-3.2-3B model on the commonsense reasoning benchmark.\nFSLoRA demonstrates consistent performance improvement across these tasks compared to baselines.", "description": "\ud45c 5.2\ub294 LLaMA-3.2-3B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0c1\uc2dd \ucd94\ub860 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ubbf8\uc138 \uc870\uc815\uc744 \uc218\ud589\ud55c \uacb0\uacfc\uc5d0 \ub300\ud55c \ud14c\uc2a4\ud2b8 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc138 \ubc88\uc758 \ub3c5\ub9bd\uc801\uc778 \uc2e4\ud589\uc5d0 \uac78\uccd0 \uc5bb\uc740 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uba70, \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc815\ud655\ub3c4\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.  FSLORA\ub294 \uae30\uc900 \ubc29\ubc95\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \ubaa8\ub4e0 \uc791\uc5c5\uc5d0\uc11c \uc77c\uad00\ub418\uac8c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc784\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc0c1\uc2dd \ucd94\ub860 \uc791\uc5c5\uc5d0\uc11c FSLORA\uc758 \ud6a8\uacfc\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "5.2 \uc8fc\uc694 \uacb0\uacfc"}, {"content": "| Hyperparameter | RoBERTa & GLUE | LLaMA-3.2-3B & Commensense Reasoning |\n|---|---|---|\n| Batch size | 16 | 16 |\n| LoRA dropout rate | 0.1 | 0.1 |\n| Learning rate, \u03b3 | 5e-4 | 3e-4 |\n| Communication round, T | 200 | 750 |\n| Local iteration number, H | 50 | 20 |\n| Number of edge devices, N | 20 | 20 |\n| Target module | [\u201cquery\u201d, \u201cvalue\u201d, \u201cclassification head\u201d] | [\u201cq_proj\u201d, \u201ck_proj\u201d, \u201cv_proj\u201d, \u201cup_proj\u201d, \u201cdown_proj\u201d] |", "caption": "Table A.1: \nThe hyperparameters for RoBERTa & GLUE and LLaMA-3.2-3B & Commensense Reasoning benchmarks.", "description": "\ud45c A.1\uc740 RoBERTa \ubaa8\ub378\uacfc GLUE \ubca4\uce58\ub9c8\ud06c, \uadf8\ub9ac\uace0 LLaMA-3.2-3B \ubaa8\ub378\uacfc \uc0c1\uc2dd \ucd94\ub860 \ubca4\uce58\ub9c8\ud06c\uc5d0 \uc0ac\uc6a9\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ubc30\uce58 \ud06c\uae30, LoRA \ub4dc\ub86d\uc544\uc6c3 \ube44\uc728, \ud559\uc2b5\ub960, \ud1b5\uc2e0 \ub77c\uc6b4\ub4dc \uc218, \ub85c\uceec \ubc18\ubcf5 \ud69f\uc218, \uc5d0\uc9c0 \ub514\ubc14\uc774\uc2a4 \uc218, \uadf8\ub9ac\uace0 \ubaa9\ud45c \ubaa8\ub4c8 \ub4f1\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac12\ub4e4\uc774 \uac01 \ubca4\uce58\ub9c8\ud06c\ubcc4\ub85c \uc0c1\uc138\ud558\uac8c \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc2e4\ud5d8 \uc124\uc815\uc758 \uc7ac\ud604\uc131\uc744 \ub192\uc774\uace0, \ub2e4\ub978 \uc5f0\uad6c\uc790\ub4e4\uc774 \ub3d9\uc77c\ud55c \uc2e4\ud5d8 \ud658\uacbd\uc744 \uad6c\ucd95\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc8fc\uae30 \uc704\ud574 \ud3ec\ud568\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "A. Details of Hyperparameters"}, {"content": "| Dataset | Input Template |\n|---|---| \n| ARC-c/e | <img src=\"https://arxiv.org/html/2501.19389/A2.T1.1.1.1.1.1.p1.pic1.png\" > | \n| BoolQ | <img src=\"https://arxiv.org/html/2501.19389/A2.T1.2.2.1.1.1.p1.pic1.png\"> | \n| HellaSwag | <img src=\"https://arxiv.org/html/2501.19389/A2.T1.3.3.1.1.1.p1.pic1.png\"> | \n| OBQA | <img src=\"https://arxiv.org/html/2501.19389/A2.T1.4.4.1.1.1.p1.pic1.png\"> | \n| PIQA | <img src=\"https://arxiv.org/html/2501.19389/A2.T1.5.5.1.1.1.p1.pic1.png\"> | \n| SIQA | <img src=\"https://arxiv.org/html/2501.19389/A2.T1.6.6.1.1.1.p1.pic1.png\"> | \n| WinoGrande | <img src=\"https://arxiv.org/html/2501.19389/A2.T1.7.7.1.1.1.p1.pic1.png\"> |", "caption": "Table B.1: The prompt template of the Commonsense170K dataset (Hu et\u00a0al., 2023).", "description": "\ud45c B.1\uc740 Commonsense170K \ub370\uc774\ud130\uc14b(Hu et al., 2023)\uc758 \ud504\ub86c\ud504\ud2b8 \ud15c\ud50c\ub9bf\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Commonsense170K \ub370\uc774\ud130\uc14b\uc740 ARC-c/e, BoolQ, HellaSwag, OBQA, PIQA, SIQA, WinoGrande \ub4f1 \uc5ec\ub7ec \uac1c\uc758 \ub370\uc774\ud130\uc14b\uc744 \ud569\uccd0 \ub9cc\ub4e0 \uac83\uc73c\ub85c, \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc9c8\ubb38 \uc720\ud615\uacfc \ub2f5\ubcc0 \ud615\uc2dd\uc774 \ub2e4\ub985\ub2c8\ub2e4. \ud45c\ub294 \uac01 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ud504\ub86c\ud504\ud2b8(\uc9c8\ubb38)\uc758 \uc608\uc2dc\uc640 \ub2f5\ubcc0 \ud615\uc2dd\uc744 \ubcf4\uc5ec\uc8fc\uc5b4, \ubaa8\ub378\uc774 \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc9c8\ubb38 \uc720\ud615\uc5d0 \ub9de\ucdb0 \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud558\ub3c4\ub85d \ub3d5\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.  \uac01 \ud589\uc740 \ud558\ub098\uc758 \ub370\uc774\ud130\uc14b\uc744 \ub098\ud0c0\ub0b4\uace0, 'Dataset' \uc5f4\uc5d0\ub294 \ub370\uc774\ud130\uc14b \uc774\ub984, 'Input Template' \uc5f4\uc5d0\ub294 \ud574\ub2f9 \ub370\uc774\ud130\uc14b\uc758 \uc9c8\ubb38 \ud15c\ud50c\ub9bf\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.", "section": "B. Details of Datasets"}]