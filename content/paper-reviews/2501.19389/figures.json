[{"figure_path": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/Overview.png", "caption": "Figure 1: An illustration of our proposed methodology where the server maintains a pair of global LoRA modules while the devices adaptively update submatrices of the global LoRA modules through sketching during each round.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \uc81c\uc548\ub41c \ubc29\ubc95\ub860\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc11c\ubc84\ub294 \ub450 \uac1c\uc758 \uc804\uc5ed LoRA \ubaa8\ub4c8\uc744 \uc720\uc9c0 \uad00\ub9ac\ud558\uace0, \uac01 \uc7a5\uce58\ub294 \ub9e4 \ub77c\uc6b4\ub4dc\ub9c8\ub2e4 \uc2a4\ucf00\uce58\ub97c \ud1b5\ud574 \uc804\uc5ed LoRA \ubaa8\ub4c8\uc758 \ud558\uc704 \ud589\ub82c\uc744 \uc801\uc751\uc801\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4.  \ub354 \uc790\uc138\ud788 \uc124\uba85\ud558\uba74, \uc11c\ubc84\ub294 \uac01 \uc7a5\uce58\uc758 \uacc4\uc0b0 \ubc0f \ud1b5\uc2e0 \uc81c\uc57d \uc870\uac74\uc5d0 \ub9de\uac8c \uc870\uc815\ub418\ub294 \uc2a4\ucf00\uce58 \ud589\ub82c\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uac01 \uc7a5\uce58\ub294 \ud574\ub2f9 \uc2a4\ucf00\uce58 \ud589\ub82c\uc744 \uc0ac\uc6a9\ud558\uc5ec LoRA \ubaa8\ub4c8\uc758 \ud558\uc704 \ud589\ub82c\ub9cc \uc5c5\ub370\uc774\ud2b8\ud558\uace0, \uc5c5\ub370\uc774\ud2b8\ub41c \ud558\uc704 \ud589\ub82c\uc744 \uc11c\ubc84\ub85c \uc804\uc1a1\ud569\ub2c8\ub2e4. \uc11c\ubc84\ub294 \ubaa8\ub4e0 \uc7a5\uce58\ub85c\ubd80\ud130 \ubc1b\uc740 \uc5c5\ub370\uc774\ud2b8\ub97c \uc9d1\uacc4\ud558\uc5ec \uc804\uc5ed LoRA \ubaa8\ub4c8\uc744 \uc5c5\ub370\uc774\ud2b8\ud558\uace0, \uc5c5\ub370\uc774\ud2b8\ub41c \uc804\uc5ed LoRA \ubaa8\ub4c8\uc744 \ubaa8\ub4e0 \uc7a5\uce58\uc5d0 \ub2e4\uc2dc \ubc30\ud3ec\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacfc\uc815\uc744 \ubc18\ubcf5\ud568\uc73c\ub85c\uc368, \uc81c\uc548\ub41c \ubc29\ubc95\ub860\uc740 \uc7a5\uce58 \uac04\uc758 \uc774\uae30\uc885\uc131\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \ud574\uacb0\ud558\uace0, \uc7a5\uce58\uc758 \uc81c\ud55c\ub41c \ub9ac\uc18c\uc2a4\ub97c \ud65c\uc6a9\ud558\uc5ec \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/Results_RoBERTa.png", "caption": "Figure 2: Convergence behavior of FSLoRA and baselines on the GLUE benchmark with the RoBERTa model. Testing accuracy is averaged over seven tasks.", "description": "\uadf8\ub9bc 2\ub294 RoBERTa \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec GLUE \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc218\ud589\ub41c FSLORA\uc640 \uae30\uc900 \ubaa8\ub378\ub4e4\uc758 \uc218\ub834 \uac70\ub3d9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 7\uac00\uc9c0 \uacfc\uc81c\uc5d0 \ub300\ud55c \ud3c9\uade0 \ud14c\uc2a4\ud2b8 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. FSLORA\ub294 \ubc18\ubcf5 \ud69f\uc218\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ub2e4\ub978 \uae30\uc900 \ubaa8\ub378\ubcf4\ub2e4 \ube60\ub974\uac8c \uc218\ub834\ud558\uace0 \ub354 \ub192\uc740 \uc815\ud655\ub3c4\uc5d0 \ub3c4\ub2ec\ud569\ub2c8\ub2e4.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/Results_RoBERTa_detail.png", "caption": "Figure 3: Comparison between FSLoRA with and without sketching, where the upload budget for devices is set to 100\u00d7100\\times100 \u00d7 the full global LoRA modules at the corresponding rank. The experiment is performed on the GLUE benchmark and the RoBERTa model. FSLoRA with sketching obtains a better performance, validating the effectiveness of sketching.", "description": "\uadf8\ub9bc 3\uc740 \uc7a5\uce58\ub2f9 \uc5c5\ub85c\ub4dc \uc6a9\ub7c9\uc744 \ud574\ub2f9 \uacc4\uce35\uc758 \uc804\uccb4 \uae00\ub85c\ubc8c LoRA \ubaa8\ub4c8 \ud06c\uae30\uc758 100\ubc30\ub85c \uc124\uc815\ud558\uace0, \uc2a4\ucf00\uce58 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud55c FSLoRA\uc640 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 FSLoRA\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. GLUE \ubca4\uce58\ub9c8\ud06c\uc640 RoBERTa \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud5d8\uc744 \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c \uc2a4\ucf00\uce58 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud55c FSLoRA\uac00 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uc5b4 \uc2a4\ucf00\uce58 \uae30\ubc95\uc758 \ud6a8\uacfc\ub97c \uac80\uc99d\ud588\uc2b5\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 LoRA \ubaa8\ub4c8\uc5d0 \ub300\ud574 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec \uc2a4\ucf00\uce58 \uae30\ubc95\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/Results_LLaMA_detail.png", "caption": "Figure 6: Comparison of FSLoRA with and without sketching, with an upload budget 400\u00d7400\\times400 \u00d7 the global LoRA module size at each rank. This is based on the commonsense reasoning benchmark and the LLaMA-3.2-3B model. We observe that the sketching mechanism improves performance across all considered tasks. The average accuracy of the eight tasks is shown in Figure LABEL:fig:rank_varying.", "description": "\uadf8\ub9bc 6\uc740 \uacf5\ud1b5 \uc0c1\uc2dd \ucd94\ub860 \ubca4\uce58\ub9c8\ud06c\uc640 LLaMA-3.2-3B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2a4\ucf00\uce58 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud55c FSLORA\uc640 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 FSLORA\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uac01 \uc7a5\uce58\uc758 \uc5c5\ub85c\ub4dc \uc6a9\ub7c9\uc740 \uae00\ub85c\ubc8c LoRA \ubaa8\ub4c8 \ud06c\uae30\uc758 400\ubc30\ub85c \uc124\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc2e4\ud5d8 \uacb0\uacfc, \uc2a4\ucf00\uce58 \uae30\ubc95\uc774 \ubaa8\ub4e0 \uacfc\uc81c\uc5d0\uc11c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uac00\uc838\uc654\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 8\uac00\uc9c0 \uacfc\uc81c\uc758 \ud3c9\uade0 \uc815\ud655\ub3c4\ub294 \uadf8\ub9bc LABEL:fig:rank_varying\uc5d0 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/Impact_global_rank_detail.png", "caption": "Figure 7: Impact of the rank of global LoRA modules on FSLoRA, given a fixed rank for the updated submatrices at the devices. This is based on the commonsense reasoning benchmark and the LLaMA-3.2-3B model. Overall, FSLoRA demonstrates improved performance as the global rank increases. The average accuracy of the eight tasks is shown in Figure LABEL:fig:global_rank.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \uace0\uc815\ub41c \ud06c\uae30\uc758 \ub85c\uceec LoRA \ubaa8\ub4c8 \uc5c5\ub370\uc774\ud2b8\ub97c \uc0ac\uc6a9\ud558\uc5ec \uae00\ub85c\ubc8c LoRA \ubaa8\ub4c8\uc758 \uacc4\uce35\uc5d0 \ub530\ub978 FSLORA\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Commonsense Reasoning \ubca4\uce58\ub9c8\ud06c\uc640 LLaMA-3.2-3B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud5d8\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4. \uae00\ub85c\ubc8c LoRA \ubaa8\ub4c8\uc758 \uacc4\uce35\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c FSLORA\uc758 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. 8\uac00\uc9c0 \uc791\uc5c5\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc815\ud655\ub3c4\ub294 \uadf8\ub9bc LABEL:fig:global_rank\uc5d0 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/Impact_topk.png", "caption": "Figure 8: Comparison of top-k compression and its integration with sketching, evaluated on the commonsense reasoning benchmark using the LLaMA-3.2-3B model. The results show that combining these two orthogonal techniques significantly enhances performance, demonstrating the benefits of integrating sketching with top-k compression.", "description": "\uadf8\ub9bc 7\uc740 \uacf5\ud1b5 \uc0c1\uc2dd \ucd94\ub860 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c LLaMA-3.2-3B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ud55c top-k \uc555\ucd95 \ubc0f \uc2a4\ucf00\uce58 \uae30\ubc95 \ud1b5\ud569\uc758 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uacb0\uacfc\ub294 \ub450 \uc9c1\uad50 \uae30\ubc95\uc758 \uc870\ud569\uc774 \uc131\ub2a5\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\uba70, \uc2a4\ucf00\uce58\uc640 top-k \uc555\ucd95\uc744 \ud1b5\ud569\ud558\ub294 \uc774\uc810\uc744 \uac15\uc870\ud569\ub2c8\ub2e4. \ubcf4\ub2e4 \uad6c\uccb4\uc801\uc73c\ub85c, x\ucd95\uc740 \uc7a5\uce58\ub2f9 \uc5c5\ub85c\ub4dc \ud1b5\uc2e0 \ubd80\ud558(MB)\ub97c \ub098\ud0c0\ub0b4\uace0 y\ucd95\uc740 \ud14c\uc2a4\ud2b8 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uc2a4\ucf00\uce58 \ube44\uc728(ki/r)\uc5d0\uc11c top-k \uc555\ucd95\uacfc \uc2a4\ucf00\uce58\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uc2a4\ucf00\uce58 \ube44\uc728\uc774 \ub0ae\uc744\uc218\ub85d \ub3d9\uc77c\ud55c \ud1b5\uc2e0 \ube44\uc6a9\uc5d0 \ub300\ud574 \ub354 \ub192\uc740 \ud14c\uc2a4\ud2b8 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \ub354\uc6b1 \ub192\uc774\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "C.2. Sketching\uacfc Top-k Compression \ud1b5\ud569"}, {"figure_path": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/More_Device_50_detail.png", "caption": "Figure 9: Comparison of FSLoRA with and without sketching, with an upload budget 400\u00d7400\\times400 \u00d7 the global LoRA module size at each rank, evaluated on the commonsense reasoning benchmark and the LLaMA-3.2-3B model. The number of devices is set to 50505050.\nWe observe that the sketching mechanism improves performance across all considered tasks. The average accuracy of the eight tasks is shown in Figure 10.", "description": "\uadf8\ub9bc 9\ub294 Commonsense Reasoning benchmark \ubc0f LLaMA-3.2-3B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ud55c, \uc2a4\ucf00\uce58 \uae30\ubc95 \uc801\uc6a9 \uc720\ubb34\uc5d0 \ub530\ub978 FSLORA\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ub4f1\uae09\uc5d0\uc11c \uae00\ub85c\ubc8c LoRA \ubaa8\ub4c8 \ud06c\uae30\uc758 400\ubc30\uc5d0 \ud574\ub2f9\ud558\ub294 \uc5c5\ub85c\ub4dc \uc6a9\ub7c9\uc744 \uc0ac\uc6a9\ud588\uc73c\uba70, \ub514\ubc14\uc774\uc2a4 \uc218\ub294 50\uac1c\ub85c \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4. \uc2a4\ucf00\uce58 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c \ubaa8\ub4e0 \uacfc\uc81c\uc5d0\uc11c \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub428\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. 8\uac1c \uacfc\uc81c\uc758 \ud3c9\uade0 \uc815\ud655\ub3c4\ub294 \uadf8\ub9bc 10\uc5d0 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/More_Device_50.png", "caption": "Figure 10: Comparison of FSLoRA with and without sketching, with an upload budget 400\u00d7400\\times400 \u00d7 the global LoRA module size at each rank, evaluated on the LLaMA-3.2-3B model. The number of devices is set to 50505050. The results are averaged over eight tasks from the commonsense reasoning benchmark.", "description": "\uadf8\ub9bc 10\uc740 \uacf5\ud1b5 \uc0c1\uc2dd \ucd94\ub860 \ubca4\uce58\ub9c8\ud06c\uc758 8\uac00\uc9c0 \uc791\uc5c5\uc5d0 \ub300\ud574 \ud3c9\uade0\ud654\ub41c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc791\uc5c5\uc5d0 \ub300\ud574 LLaMA-3.2B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. \uc7a5\uce58 \uc218\ub294 50\uac1c\ub85c \uc124\uc815\ud558\uace0, \uc5c5\ub85c\ub4dc \uc6a9\ub7c9\uc740 \uac01 \uacc4\uce35\uc758 \uae00\ub85c\ubc8c LoRA \ubaa8\ub4c8 \ud06c\uae30\uc758 400\ubc30\ub85c \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4.  FSLORA(Federated Sketching LoRA)\uc640 \uc2a4\ucf00\uce6d \uc5c6\uc774 FSLORA\ub97c \ube44\uad50\ud558\uc5ec \uc2a4\ucf00\uce6d\uc774 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uadf8\ub798\ud504\uc758 x\ucd95\uc740 LoRA \ubaa8\ub4c8\uc758 \uacc4\uce35(rank)\uc744 \ub098\ud0c0\ub0b4\uace0, y\ucd95\uc740 \ud14c\uc2a4\ud2b8 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "5. Experiments"}]