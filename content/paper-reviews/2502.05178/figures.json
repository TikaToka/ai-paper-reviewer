[{"figure_path": "https://arxiv.org/html/2502.05178/extracted/6182739/figures/teaser.png", "caption": "Figure 1: State-of-the-art visual tokenizers excel at either understanding (high zero-shot accuracy,\u00a0e.g. SigLIP\u00a0[96]) or reconstruction (low reconstruction FID,\u00a0e.g. MAGVIT2\u00a0[93]), but not both.\nQLIP can perform well on both understanding and reconstruction with a marginal performance drop, opening up an opportunity for unified multi-modal understanding and generation.", "description": "\uadf8\ub9bc 1\uc740 \ucd5c\ucca8\ub2e8 \uc2dc\uac01 \ud1a0\ud070\ud654 \uae30\ubc95\ub4e4\uc774 \uc601\uc0c1 \uc774\ud574(\ub192\uc740 \uc81c\ub85c\uc0f7 \uc815\ud655\ub3c4, \uc608: SigLIP [96]) \ub610\ub294 \uc601\uc0c1 \uc7ac\uad6c\uc131(\ub0ae\uc740 FID, \uc608: MAGVIT2 [93]) \uc911 \ud558\ub098\uc5d0\ub9cc \ud0c1\uc6d4\ud558\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub450 \uac00\uc9c0 \ubaa8\ub450\uc5d0\uc11c \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uae30\ubc95\uc740 \ub4dc\ubb3c\uc5c8\uc2b5\ub2c8\ub2e4.  QLIP\ub294 \uc601\uc0c1 \uc774\ud574\uc640 \uc7ac\uad6c\uc131 \ubaa8\ub450\uc5d0\uc11c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\uba70, \uc131\ub2a5 \uc800\ud558\uac00 \ubbf8\ubbf8\ud558\uc5ec \ub2e4\uc591\ud55c \ubaa8\ub4dc\ub97c \ud1b5\ud569\ud558\ub294 \ubaa8\ub378 \uac1c\ubc1c\uc5d0 \uc0c8\ub85c\uc6b4 \uac00\ub2a5\uc131\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.05178/x1.png", "caption": "Figure 2: \nOverview.\n(a-b) Two-stage training pipeline of QLIP.\n(a) In Stage 1, we train QLIP with a combination of alignment loss and MSE loss.\n(b) In Stage 2, we drop the text encoder, freeze the visual encoder, and no longer optimize the contrastive loss.\nOnly the bottleneck quantizer and the decoder are fine-tuned.\n(c) With the text-aligned visual tokenizer, we transform the image into visual tokens, concatenate them with text tokens, and use an auto-regressive multi-modal model (Sec\u00a04.1) to model jointly.", "description": "\uadf8\ub9bc 2\ub294 QLIP\uc758 \ud6c8\ub828 \uacfc\uc815\uacfc \ub2e4\uc911 \ubaa8\ub4dc \ubaa8\ub378\ub9c1\uc744 \uac1c\ub7b5\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\uc640 (b)\ub294 QLIP\uc758 \ub450 \ub2e8\uacc4 \ud6c8\ub828 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 1\ub2e8\uacc4\uc5d0\uc11c\ub294 \uc815\ub82c \uc190\uc2e4\uacfc MSE \uc190\uc2e4\uc744 \uacb0\ud569\ud558\uc5ec QLIP\ub97c \ud6c8\ub828\ud569\ub2c8\ub2e4. 2\ub2e8\uacc4\uc5d0\uc11c\ub294 \ud14d\uc2a4\ud2b8 \uc778\ucf54\ub354\ub97c \uc81c\uac70\ud558\uace0, \ube44\uc8fc\uc5bc \uc778\ucf54\ub354\ub97c \uace0\uc815\ud558\uace0, \ub300\uc870 \uc190\uc2e4\uc744 \ub354 \uc774\uc0c1 \ucd5c\uc801\ud654\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc624\uc9c1 \ubcd1\ubaa9 \uc9c0\uc810\uc758 \uc591\uc790\ud654\uae30\uc640 \ub514\ucf54\ub354\ub9cc \ubbf8\uc138 \uc870\uc815\ud569\ub2c8\ub2e4. (c)\uc5d0\uc11c\ub294 \ud14d\uc2a4\ud2b8 \uc815\ub82c \ube44\uc8fc\uc5bc \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0\ub97c \ube44\uc8fc\uc5bc \ud1a0\ud070\uc73c\ub85c \ubcc0\ud658\ud558\uace0, \uc774\ub97c \ud14d\uc2a4\ud2b8 \ud1a0\ud070\uacfc \uc5f0\uacb0\ud558\uc5ec \uc790\ub3d9 \ud68c\uadc0 \ub2e4\uc911 \ubaa8\ub4dc \ubaa8\ub378(4.1\uc808)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uacf5\ub3d9\uc73c\ub85c \ubaa8\ub378\ub9c1\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 Quantized Language-Image Pre-training"}, {"figure_path": "https://arxiv.org/html/2502.05178/x2.png", "caption": "Figure 3: Memory usage of QLIP.", "description": "\uadf8\ub9bc 3\uc740 QLIP\uc758 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub450 \uac1c\uc758 \uace1\uc120\uc740 \uac01\uac01 LLPIPS\uc640 LGAN \uc190\uc2e4 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0\uc640 \uc0ac\uc6a9\ud55c \uacbd\uc6b0\uc758 GPU \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  x\ucd95\uc740 \uac01 \uc7a5\uce58\ub2f9 \ubc30\uce58 \ud06c\uae30\ub97c \ub098\ud0c0\ub0b4\uace0, y\ucd95\uc740 \ucd5c\ub300 GPU \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9(GB)\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \uadf8\ub798\ud504\ub294 LLPIPS\uc640 LGAN \uc190\uc2e4 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uba74 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc774 \ud06c\uac8c \uc99d\uac00\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 \uc774\ub7ec\ud55c \uc190\uc2e4 \ud568\uc218\uac00 \uacc4\uc0b0\uc801\uc73c\ub85c \ube44\uc6a9\uc774 \ub9ce\uc774 \ub4dc\ub294 \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd\uc5d0 \uc758\uc874\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c, QLIP\uc740 \uba54\ubaa8\ub9ac \ud6a8\uc728\uc801\uc778 \ud6c8\ub828\uc744 \uc704\ud574 \ub450 \ub2e8\uacc4 \ud6c8\ub828 \uc804\ub7b5\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.", "section": "2. \uad00\ub828 \uc5f0\uad6c"}, {"figure_path": "https://arxiv.org/html/2502.05178/x3.png", "caption": "Figure 4: \nComparison of reconstruction results to the input image after the first and second stage.\nThe second-stage model produces more high-frequency details.\nThe figure is best viewed on a PDF viewer with zoom-in.", "description": "\uc774 \uadf8\ub9bc\uc740 QLIP(Quantized Language-Image Pretraining) \ubaa8\ub378\uc758 \ub450 \ub2e8\uacc4 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uc774\ubbf8\uc9c0 \uc7ac\uad6c\uc131 \uacb0\uacfc\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. 1\ub2e8\uacc4\uc5d0\uc11c\ub294 \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8 \uc815\ub82c\uacfc \uc7ac\uad6c\uc131 \uc190\uc2e4\uc744 \ub3d9\uc2dc\uc5d0 \ucd5c\uc801\ud654\ud558\uace0, 2\ub2e8\uacc4\uc5d0\uc11c\ub294 \ud14d\uc2a4\ud2b8 \uc778\ucf54\ub354\ub97c \uc81c\uac70\ud558\uace0 \uc2dc\uac01\uc801 \uc778\ucf54\ub354\ub97c \uace0\uc815\ud558\uc5ec \uc7ac\uad6c\uc131 \uc190\uc2e4\ub9cc\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ud569\ub2c8\ub2e4.  2\ub2e8\uacc4 \ubaa8\ub378\uc740 \uace0\uc8fc\ud30c\uc218 \ub514\ud14c\uc77c\uc744 \ub354 \uc798 \uc7ac\ud604\ud569\ub2c8\ub2e4.  PDF \ubdf0\uc5b4\uc5d0\uc11c \ud655\ub300\ud558\uc5ec \ubcf4\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4.", "section": "4. Quantized Language-Image Pre-training"}, {"figure_path": "https://arxiv.org/html/2502.05178/x4.png", "caption": "Figure 5: \nComparison of gradient magnitude.\nHere, \ud835\udc98\ud835\udc98{\\bm{w}}bold_italic_w refers to the linear layer in the visual encoder\u2019s last MLP.", "description": "\uadf8\ub9bc 5\ub294 \uc2dc\uac01\uc801 \uc778\ucf54\ub354\uc758 \ub9c8\uc9c0\ub9c9 MLP(\ub2e4\uce35 \ud37c\uc149\ud2b8\ub860)\uc5d0 \uc788\ub294 \uc120\ud615 \ub808\uc774\uc5b4\ub97c \uac00\ub9ac\ud0a4\ub294 w\ub97c \uc0ac\uc6a9\ud558\uc5ec \uadf8\ub798\ub514\uc5b8\ud2b8 \ud06c\uae30\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub450 \uac00\uc9c0 \ubaa9\ud45c(\ub300\uc870\uc801 \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8 \uc815\ub82c \ubc0f \ud53d\uc140 \uc7ac\uad6c\uc131) \uac04\uc758 \uadf8\ub798\ub514\uc5b8\ud2b8 \ud06c\uae30 \ucc28\uc774\uac00 \uc0c1\ub2f9\ud568\uc744 \ubcf4\uc5ec\uc8fc\uace0, \uc774\ub294 \uc11c\ub85c \ub2e4\ub978 \uc218\ub834 \uc18d\ub3c4\ub85c \uc774\uc5b4\uc9c4\ub2e4\ub294 \uc810\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ucc28\uc774\ub85c \uc778\ud574 \ub450 \uac00\uc9c0 \uc190\uc2e4 \ud56d\ubaa9 \uac04\uc758 \uade0\ud615\uc744 \ub9de\ucd94\ub294 \ub370 \uc5b4\ub824\uc6c0\uc774 \ubc1c\uc0dd\ud569\ub2c8\ub2e4.  \uadf8\ub798\ub514\uc5b8\ud2b8 \ud06c\uae30\uc758 \ucc28\uc774\uac00 \ub450 \uc190\uc2e4 \ud568\uc218\uc758 \uc218\ub834 \uc18d\ub3c4\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2dc\uac01\uc801 \ud45c\ud604\uc785\ub2c8\ub2e4.", "section": "4. Quantized Language-Image Pre-training"}, {"figure_path": "https://arxiv.org/html/2502.05178/x5.png", "caption": "Figure 6: Comparison of generated images with conditioning captions in the bottom.\nFor each pair, the left is from LlamaGen+VQGAN and the right is from LlamaGen+QLIP-B/16 (ours).\nThe caption is also provided at the bottom.", "description": "\uadf8\ub9bc 6\uc740 LlamaGen\uc774\ub77c\ub294 \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubaa8\ub378\uc5d0 VQGAN\uacfc QLIP-B/16(\ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ubc29\ubc95)\uc744 \uac01\uac01 \uc801\uc6a9\ud588\uc744 \ub54c \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\ub4e4\uc744 \ube44\uad50\ud55c \uadf8\ub9bc\uc785\ub2c8\ub2e4. \uac01 \uc30d\uc758 \uc774\ubbf8\uc9c0\ub294 \uc67c\ucabd\uc774 LlamaGen+VQGAN\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ud55c \uc774\ubbf8\uc9c0, \uc624\ub978\ucabd\uc774 LlamaGen+QLIP-B/16\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ud55c \uc774\ubbf8\uc9c0\uc785\ub2c8\ub2e4. \uac01 \uc774\ubbf8\uc9c0 \uc544\ub798\uc5d0\ub294 \ud574\ub2f9 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub41c \ucea1\uc158\uc774 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  QLIP-B/16\uc744 \uc0ac\uc6a9\ud55c \uc774\ubbf8\uc9c0\uac00 VQGAN\uc744 \uc0ac\uc6a9\ud55c \uc774\ubbf8\uc9c0\ubcf4\ub2e4 \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\uc758 \ud488\uc9c8, \uc989 \ucea1\uc158\uacfc\uc758 \uc77c\uce58\ub3c4\uac00 \ub354 \ub192\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.3 QLIP\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2502.05178/x6.png", "caption": "Figure 7: Comparison of generated images with conditioning captions in the bottom.\nFor each pair, the left is from LlamaGen+VQGAN and the right is from LlamaGen+QLIP-B/16 (ours).\nThe caption is also provided at the bottom.", "description": "\uc774 \uadf8\ub9bc\uc740 LlamaGen\uc774\ub77c\ub294 \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubaa8\ub378\uc5d0 VQGAN\uacfc QLIP-B/16(\ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \ubc29\ubc95)\uc744 \uac01\uac01 \uc801\uc6a9\ud558\uc5ec \uc0dd\uc131\ud55c \uc774\ubbf8\uc9c0\ub4e4\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uac01 \uc774\ubbf8\uc9c0 \uc30d\uc5d0\uc11c \uc67c\ucabd\uc740 LlamaGen+VQGAN, \uc624\ub978\ucabd\uc740 LlamaGen+QLIP-B/16\uc73c\ub85c \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\uc774\uba70, \uc544\ub798\uc5d0\ub294 \uac01 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud55c \ucea1\uc158\uc774 \ud568\uaed8 \uc81c\uacf5\ub429\ub2c8\ub2e4.  QLIP-B/16\uc744 \uc0ac\uc6a9\ud55c \uc774\ubbf8\uc9c0\uac00 VQGAN\uc744 \uc0ac\uc6a9\ud55c \uc774\ubbf8\uc9c0\ubcf4\ub2e4 \ucea1\uc158\uacfc \ub354 \uc798 \uc77c\uce58\ud558\ub294 \uc138\ubd80\uc801\uc778 \ubb18\uc0ac\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.3 QLIP\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc"}]