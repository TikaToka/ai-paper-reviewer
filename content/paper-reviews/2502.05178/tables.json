[{"content": "| Dataset | Images | Text (# tok/src) | Usage/Metrics |\n|---|---|---|---| \n| DataComp-1B [27] | 1B | 20B/alt-text | QLIP |\n| LAION-COCO [67]<sup>2</sup> <a href=\"hf.co/datasets/guangyil/laion-coco-aesthetic\">hf.co/datasets/guangyil/laion-coco-aesthetic</a> | 4M/600M | 40M/BLIP2 | T2I (LlamaGen), UM<sup>3</sup> |\n| SA-1B [41] | 11M | 400M/Qwen2VL-7B | T2I (LlamaGen), UM<sup>3</sup> |\n| CC-12M [9] | 6M/12M | 200M/Qwen2VL-7B | UM<sup>3</sup> |\n| DCLM [45] | - | 300B/raw+filtered | UM<sup>3</sup> |\n| LAION-CC-SBU [51] | 558K | -/BLIP2 | VLM (LLaVA-1.5) |\n| LLaVA-Instruct [51] | 665K | -/convo. | VLM (LLaVA-1.5) |\n| ImageNet [20] | 1.3M | -/label | Classi. (ZS), Recon. (RC) |\n| MS-COCO [48, 11] | 160K | 10M/MTurk | Caption, generation |", "caption": "Table 1: Dataset summary.\nWe list the statistics of datasets used throughout the paper, including the number of images, the number of text tokens with source, and the usage of the respective dataset.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38 \uc804\uccb4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \ud1b5\uacc4\ub97c \uc694\uc57d\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub370\uc774\ud130\uc14b\uc758 \uc774\ub984, \uc774\ubbf8\uc9c0 \uc218, \uc18c\uc2a4\ubcc4 \ud14d\uc2a4\ud2b8 \ud1a0\ud070 \uc218, \uac01 \ub370\uc774\ud130\uc14b\uc758 \uc6a9\ub3c4(\uc774\ubbf8\uc9c0 \ubd84\ub958, \ucea1\uc158 \uc0dd\uc131, \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8 \uc30d \ud559\uc2b5 \ub4f1)\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.  \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub300\uaddc\ubaa8 \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b\uc758 \uaddc\ubaa8\uc640 \ub2e4\uc591\uc131\uc744 \ud30c\uc545\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.", "section": "5.1. Datasets"}, {"content": "| 0-shot | Seen Data | Acc. \u2191 | # bits | Comp. Ratio | rFID \u2193 | PSNR \u2191 | SSIM \u2191 |\n|---|---|---|---|---|---|---|---|---|\n| (Base backbone) |  |  |  |  |  |  |  |  |\n| CLIP [59] | WIT-400M | 68.3 | / | / | / | / | / |\n| EVA-CLIP [75] | Merged-2B | 74.7 | / | / | / | / | / |\n| SigLIP-B [96] | WL-10B | 76.7 | / | / | / | / | / |\n| VQGAN [24] | IN-1k | / | 14 | 438.8 | 4.98 | - | - |\n| MaskGIT [8] | IN-1k | / | 10 | 614.4 | 1.98 | 18.63 | 0.4619 |\n| MoVQGAN [100] | IN-1k | / | 40 | 153.6 | 1.12 | 22.42 | 0.6731 |\n| RQ-VAE/f32 [44] | IN-1k | / | 112 | 219.4 | 2.69 | - | - |\n| (Base backbone, Smaller patch) |  |  |  |  |  |  |  |  |\n| SigLIP-B [96] | WL-10B | 79.2 | / | / | / | / | / |\n| DALL-E dVAE [62] | CC3M+YF | / | 13 | 118.2 | 32.63 | 27.31 | 0.7943 |\n| ViT-VQGAN [91] | IN-1k | / | 13 | 118.2 | 1.55 | - | - |\n| SD-VAE 1.x [63] | OI-2M | / | 14 | 109.7 | 1.40 | 23.65 | 0.6354 |\n| SD-VAE 2.x [58] | OI-2M+LAae | / | 64 | 24 | 0.70 | 26.90 | 0.7592 |\n| SDXL-VAE [58] | OI-2M+LAae++ | / | 64 | 24 | 0.67 | 27.37 | 0.7814 |\n| SBER-MoVQGAN [66] | LAHR-166M | / | 14 | 109.7 | 0.96 | 26.45 | 0.7250 |\n| BSQViT [98] | IN-1k | / | 18 | 85.3 | 0.99 | 27.78 | 0.8171 |\n| OpenCLIP-B [13] | DC-1B | 73.5 | / | - | / | / | / |\n| BSQViT [98] | DC-1B | / | 28 | 219.4 | 3.81 | 24.12 | 0.6638 |\n| QLIP-B (ours) | DC-1B | 74.3 | 28 | 219.4 | 3.21 | 23.16 | 0.6286 |\n| (Large backbone) |  |  |  |  |  |  |  |  |\n| CLIP/f14 [59] | WIT-400M | 75.5 | / | / | / | / | / |\n| SigLIP-L [96] | WL-10B | 80.5 | / | / | / | / | / |\n| OpenCLIP-L [13] | DC-1B | 79.2 | / | / | / | / | / |\n| EVA-CLIP-L [75] | Merged-2B | 79.8 | / | / | / | / | / |\n| Open-MAGVIT2 [93, 54] | IN-1k | / | 18 | 85.3 | 1.17 | 21.90 | - |\n| VILA-U [89] | WL-10B+CY-1B | 73.3 | 56 | 27.4 | 1.80 | - | - |\n| (Large backbone, high resolution) |  |  |  |  |  |  |  |  |\n| CLIP/f14 [59] | WIT-400M | 76.6 | / | / | / | / | / |\n| SigLIP-L [96] | WL-10B | 82.1 | / | / | / | / | / |\n| EVA-CLIP-L [75] | Merged-2B | 80.4 | / | / | / | / | / |\n| VILA-U [89] | WL-10B+CY-1B | 78.0 | 224 | 21 | 1.25 | - | - |\n| QLIP-L (ours) | DC-1B | 79.1 | 28 | 168 | 1.46 | 25.36 | 0.6903 |", "caption": "Table 2: Comparison to state-of-the-art visual encoders or tokenizers.\nWe highlight rows that are most comparable in each group.\n\u2020: our reproduction.\n#: effective number of bits when latents are stored in bf16.\n&: quantizer uses residual quantization (RQ), where the total bits are multiplied by RQ depth.", "description": "\ud45c 2\ub294 \ucd5c\ucca8\ub2e8 \ube44\uc8fc\uc5bc \uc778\ucf54\ub354 \ub610\ub294 \ud1a0\ud06c\ub098\uc774\uc800\uc640 QLIP\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. \uac01 \uadf8\ub8f9\uc5d0\uc11c \uac00\uc7a5 \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \ud589\ub4e4\uc744 \uac15\uc870 \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4.  \u2020 \ud45c\uc2dc\ub294 \uc7ac\ud604 \uacb0\uacfc\uc784\uc744 \ub098\ud0c0\ub0b4\uba70, # \ud45c\uc2dc\ub294 latents\ub97c bf16\uc73c\ub85c \uc800\uc7a5\ud588\uc744 \ub54c\uc758 \uc720\ud6a8 \ube44\ud2b8 \uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. & \ud45c\uc2dc\ub294 \uc794\ucc28 \uc591\uc790\ud654(RQ)\ub97c \uc0ac\uc6a9\ud558\ub294 \uc591\uc790\ud654\uae30\uc774\uba70, \ucd1d \ube44\ud2b8 \uc218\uc5d0 RQ \uae4a\uc774\ub97c \uacf1\ud55c \uac12\uc785\ub2c8\ub2e4.  \uc989, \ube44\uc8fc\uc5bc \ud1a0\ud06c\ub098\uc774\uc800\uc758 \uc131\ub2a5(0-shot \uc815\ud655\ub3c4, \uc7ac\uad6c\uc131 FID, PSNR, SSIM)\uc744 \ub2e4\uc591\ud55c \uc555\ucd95 \ube44\uc728 \ubc0f \ubaa8\ub378 \ud06c\uae30\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec QLIP\uc758 \uacbd\uc7c1\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.3 QLIP\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| \u03b1a:\u03b1r | ZS(%) | RC(rFID)\u2193 | RC(PSNR) |\n|---|---|---|---|\n| 1:0 | 75.7 | 367.8 | 11.7 |\n| 1:1 | 75.1 | 162.6 | 17.8 |\n| 1:10\u00b2 | 74.7 | 41.7 | 22.5 |\n| 1:10\u00b3 | 74.3 | 35.3 | 24.5 |\n| 1:10\u2074 | 35.4 | 35.6 | 24.5 |\n| 0:1 | 0.1 | 35.7 | 24.5 |", "caption": "(a) Balancing Loss.", "description": "\uc774 \ud45c\ub294 QLIP(Quantized Language-Image Pre-training) \ubaa8\ub378 \ud559\uc2b5 \uc2dc \uc7ac\uad6c\uc131 \uc190\uc2e4(reconstruction loss)\uacfc \uc815\ub82c \uc190\uc2e4(alignment loss)\uc758 \ube44\uc728\uc744 \uc870\uc815\ud588\uc744 \ub54c\uc758 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \u03b1r \uacfc \u03b1a \ub294 \uac01\uac01 \uc7ac\uad6c\uc131 \uc190\uc2e4\uacfc \uc815\ub82c \uc190\uc2e4\uc758 \uac00\uc911\uce58\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud45c\ub294  \u03b1r \uacfc \u03b1a \uc758 \ub2e4\uc591\ud55c \ube44\uc728\uc5d0\uc11c \uc81c\ub85c\uc0f7 \ubd84\ub958 \uc815\ud655\ub3c4(ZS)\uc640 \uc7ac\uad6c\uc131 \uc131\ub2a5(RC, rFID, PSNR)\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90c\uc73c\ub85c\uc368, \ub450 \uc190\uc2e4 \ud56d\ubaa9 \uac04\uc758 \uade0\ud615\uc744 \ub9de\ucd94\ub294 \uac83\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \uc911\uc694\ud568\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "5.3 QLIP\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Pretrain | ZS (%) | RC (rFID) | RC (PSNR) |\n|---|---|---|---| \n| None | 26.4 | 35.0 | 24.8 |\n| MIM [25] | 74.3 | 35.3 | 24.5 |\n| CLIP [75] | 74.7 | 41.7 | 23.9 |", "caption": "(b) Initialization.", "description": "\ud45c 3b\ub294 \ub2e4\uc591\ud55c \ubc29\ubc95\uc73c\ub85c \uc2dc\uac01\uc801 \uc778\ucf54\ub354\ub97c \ucd08\uae30\ud654\ud588\uc744 \ub54c\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c\ub294 (1) \ub79c\ub364 \ucd08\uae30\ud654, (2) ImageNet-21k\uc5d0\uc11c MIM \ubaa9\uc801 \ud568\uc218\ub85c \ud559\uc2b5\ub41c EVA-02 [25], (3) Merged-2B\uc5d0\uc11c CLIP \ubaa9\uc801 \ud568\uc218\ub85c \ud559\uc2b5\ub41c EVA-CLIP [75] \uc138 \uac00\uc9c0 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \ub79c\ub364 \ucd08\uae30\ud654\ub97c \uc0ac\uc6a9\ud560 \uacbd\uc6b0, \uc81c\ub85c\uc0f7 \uc815\ud655\ub3c4\uac00 \ub0ae\uac8c \ub098\ud0c0\ub0ac\ub294\ub370, \uc774\ub294 20\uc5b5 \uac1c\uc758 \uc0d8\ud50c\uc774 \uc2dc\uac01\uc801 \uc778\ucf54\ub354\uac00 \ud14d\uc2a4\ud2b8 \uac10\ub3c5\uc73c\ub85c\ubd80\ud130 \ud559\uc2b5\ud558\uae30\uc5d0\ub294 \ubd88\ucda9\ubd84\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \ubc18\uba74, MIM\uacfc CLIP \ucd08\uae30\ud654 \ubc29\ubc95\uc740 \uc81c\ub85c\uc0f7 \uc815\ud655\ub3c4\uac00 \ub192\uc558\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 MIM\uc774 CLIP\ubcf4\ub2e4 \uc7ac\uad6c\uc131 \uce21\uba74\uc5d0\uc11c \ub354 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. CLIP\uc5d0\uc11c \uace0\ub178\ub984 \uc774\uc0c1\uce58 \ud1a0\ud070\uc774 \uc7ac\uad6c\uc131\uc5d0 \uc545\uc601\ud5a5\uc744 \ubbf8\uce60 \uc218 \uc788\ub2e4\ub294 \ucd94\uce21\uc785\ub2c8\ub2e4.", "section": "5.3 QLIP\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "|       |       | ZS (%) | RC (rFID) | RC (PSNR) |\n| :---- | :---- | ----: | ----: | ----: |\n|       | (1) \u2130t, \u2130v, \ud835\udcac, \ud835\udca2 | 35.3 | 24.49 |  |\n| Recipe 1 | (2) Finetune \ud835\udca2 | 74.3 | 3.21 | 23.16 |\n|       | (2)* (on IN-1k) |  | 2.90 | 23.33 |\n| Recipe 2 | (1) \u2130t, \u2130v, \ud835\udca2 | 75.0 | 17.2 | 26.72 |\n|       | (2) Train \ud835\udcac |  | 13.7 | 23.34 |\n|       | + Finetune \ud835\udca2 |  |  |  |", "caption": "(c) Training Recipe.", "description": "\ud45c 3\uc758 (c) \uc5f4\uc740 QLIP \ud6c8\ub828 \ub808\uc2dc\ud53c\uc5d0 \ub300\ud55c ablation study \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c\ub294, \ub450 \ub2e8\uacc4 \ud6c8\ub828 \uacfc\uc815\uc5d0\uc11c \uac01 \ub2e8\uacc4\uc758 \ud6c8\ub828 \ubc29\ubc95\uacfc \ucd08\uae30\ud654 \uc804\ub7b5\uc744 \ubcc0\uacbd\ud588\uc744 \ub54c, zero-shot \ubd84\ub958 \uc815\ud655\ub3c4(ZS)\uc640 \uc7ac\uad6c\uc131 \ud488\uc9c8(RC, rFID, PSNR)\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4. \uccab \ubc88\uc9f8 \ub2e8\uacc4\uc5d0\uc11c\ub294 text encoder, visual encoder, quantizer, decoder\ub97c \ud568\uaed8 \ud559\uc2b5\uc2dc\ud0a4\uace0, \ub450 \ubc88\uc9f8 \ub2e8\uacc4\uc5d0\uc11c\ub294 text encoder\ub97c \uc81c\uac70\ud558\uace0 visual encoder\ub97c \uace0\uc815\uc2dc\ucf1c quantizer\uc640 decoder\ub9cc \ubbf8\uc138 \uc870\uc815\ud569\ub2c8\ub2e4.  \uc5ec\ub7ec \ucd08\uae30\ud654 \ubc29\ubc95(\uc608: \ub79c\ub364 \ucd08\uae30\ud654, MIM, CLIP \uc0ac\uc804 \ud6c8\ub828\ub41c visual encoder)\uc744 \ube44\uad50\ud558\uace0, loss \uac00\uc911\uce58(alignment loss\uc640 reconstruction loss\uc758 \ube44\uc728)\uc5d0 \ub530\ub978 \uc131\ub2a5 \ubcc0\ud654\ub3c4 \ud655\uc778\ud569\ub2c8\ub2e4.", "section": "5.3 QLIP\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Method | Vision Encoder | Res | LLM | VQAv2 | GQA | TextVQA | POPE | MME | MM-Vet |\n|---|---|---|---|---|---|---|---|---|---| \n| SEED-X [28] | ViT-bigG-14 | 448 | LLaMA-2-13B | - | 47.9 | - | 84.2 | 1435.7 | - |\n| LaVIT [39] | ViT-G | 224 | LLaMA-2-7B | 68.2 | 48.0 | - | - | - | - |\n| EVE [21] | - | 1344 | Vicuna-1.5-7B | 78.6* | 62.6* | 56.8 | 85.0 | 1305.7 | 25.7 |\n| Fuyu | - | 1080 | Persimmon-8B | 74.2 | - | - | 74.1 | 728.6 | 21.4 |\n| VILA-U [89] | SigLIP-SO400M | 384 | LLaMA-2-7B | 79.4* | 60.8* | 60.8 | 85.8 | 1401.8 | 33.5 |\n| Chameleon [77] | VQ-VAE | 512 | LLaMA-2-34B+ | 69.6 | - | - | - | - | - |\n| Show-o [90] | MAGVIT-v2 | 256 | Phi-1.5-1.3B | 59.3* | 48.7* | - | 73.8 | 948.4 | - |\n| Emu3 [85] | MoVQGAN | 512 | LLaMA-2-8B+ | 75.1* | 60.3* | 64.7 | 85.2 | - | 37.2 |\n| LLaVA-1.5 [51] | CLIP-Large (orig.) | 336 | Vicuna-1.5-7B | 78.5* | 62.0* | 58.2 | 85.9 | 1510.7 | 30.5 |\n|  | CLIP-Large (repro.) | 392 |  | 79.1* (+0.0) | 62.3* (+0.0) | 55.4 (+0.0) | 87.5 (+0.0) | 1484.9 (+0.0) | 33.3 (+0.0) |\n|  | QLIP-Large (ours) | 392 |  | 78.3* (-0.8) | 61.8* (-0.5) | 55.2 (-0.2) | 86.1 (-1.4) | 1498.3 (+13.4) | 33.3 (+0.0) |", "caption": "Table 3: \nAblation studies of training QLIP.\nZS: zero-shot classification; RC: reconstruction.\nWe highlight the default setting.", "description": "\ud45c 3\uc740 QLIP \ubaa8\ub378 \ud559\uc2b5\uc5d0 \ub300\ud55c ablation study \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 ablation study\ub294 zero-shot classification (ZS) \uc131\ub2a5\uacfc reconstruction (RC) \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. ZS\ub294 zero-shot \uc774\ubbf8\uc9c0 \ubd84\ub958 \uc815\ud655\ub3c4\ub97c, RC\ub294 reconstruction FID (rFID), PSNR, SSIM \uac12\uc73c\ub85c \uce21\uc815\ud55c \uc7ac\uad6c\uc131 \ud488\uc9c8\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uc190\uc2e4 \ud568\uc218 \uac00\uc911\uce58 \uc870\uc815, \ucd08\uae30\ud654 \ubc29\ubc95, \ud559\uc2b5 \ub2e8\uacc4 \ub4f1 \ub2e4\uc591\ud55c \uc694\uc18c\ub97c \ubcc0\uacbd\ud588\uc744 \ub54c\uc758 \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uae30\ubcf8 \uc124\uc815(default setting)\uacfc \ube44\uad50\ud558\uc5ec \uac01 \ubcc0\ud654\uac00 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \uc5b4\ub5a4 \uc601\ud5a5\uc744 \uc8fc\ub294\uc9c0 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "5.3 QLIP\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Method | # Params | ARC-C | Hellaswag | PIQA | SIQA | Winogrande | BLEU@4 | METEOR | CIDEr | gFID\u2193 | CLIPScore\u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Llama-3.2 [22] | 1.4B | 34.90 | 48.70 | 75.95 | 43.50 | 61.48 | - | - | - | - | - |\n| ZeroCap [79] | 0.5B | - | - | - | - | - | 2.6 | 11.5 | 14.6 | - | - |\n| LlamaGen [74] | 0.8B | - | - | - | - | - | - | - | - | 33.4\u2217 | 0.257\u2217 |\n| UM\u00b3 (Ours) | 1.5B | 34.30 | 45.35 | 74.65 | 43.09 | 54.22 | 8.6 | 20.2 | 17.3 | 44.1 | 0.250 |", "caption": "Table 4: \nComparison to vision-language modeling on vision-language understanding benchmarks.\nQLIP\u2019s encoder works on par with LLaVA-1.5 with our reproduced CLIP-Large under a controlled experiment.", "description": "\ubcf8 \ud45c\ub294 \ube44\uc804-\uc5b8\uc5b4 \uc774\ud574 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ube44\uc804-\uc5b8\uc5b4 \ubaa8\ub378\ub9c1\uacfc\uc758 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc81c\uc5b4\ub41c \uc2e4\ud5d8 \ud658\uacbd \ud558\uc5d0\uc11c QLIP\uc758 \uc778\ucf54\ub354 \uc131\ub2a5\uc774 \uc7ac\ud604\ub41c CLIP-Large \ubc0f LLaVA-1.5\uc640 \ub3d9\ub4f1\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, \ub2e4\uc591\ud55c \ube44\uc804-\uc5b8\uc5b4 \uc774\ud574 \uacfc\uc81c(VQAv2, GQA, TextVQA, POPE, MME, MM-Vet)\uc5d0 \ub300\ud55c \uc5ec\ub7ec \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50\ud558\uc5ec QLIP \uc778\ucf54\ub354\uc758 \uacbd\uc7c1\ub825\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ube44\uc804 \uc778\ucf54\ub354, \uc0ac\uc6a9\ub41c \uc5b8\uc5b4 \ubaa8\ub378, \uadf8\ub9ac\uace0 \uac01 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc131\ub2a5 \uc9c0\ud45c(\uc815\ud655\ub3c4 \ub4f1)\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc7ac\ud604\ub41c CLIP-Large \ubaa8\ub378\uacfc\uc758 \ube44\uad50\ub97c \ud1b5\ud574 QLIP\uc758 \uc131\ub2a5\uc744 \uac1d\uad00\uc801\uc73c\ub85c \ud3c9\uac00\ud558\uace0\uc790 \ud558\uc600\uc2b5\ub2c8\ub2e4.", "section": "5.4. \uc2e4\ud5d8 \uacb0\uacfc: \ub2e4\uc911 \ubaa8\ub2ec \uc774\ud574 \ubc0f \uc0dd\uc131"}, {"content": "| Contra. | Recon. | layer# | use <math alttext=\"{\nmathcal{Q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.1.1.1.1.1.m1.1\"><semantics id=\"S5.T6.1.1.1.1.1.m1.1a\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S5.T6.1.1.1.1.1.m1.1.1\" xref=\"S5.T6.1.1.1.1.1.m1.1.1.cmml\">\ud835\udcac</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.1.1.1.1.1.m1.1b\"><ci id=\"S5.T6.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T6.1.1.1.1.1.m1.1.1\">\ud835\udcac</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.1.1.1.1.1.m1.1c\">{\nmathcal{Q}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T6.1.1.1.1.1.m1.1d\">caligraphic_Q</annotation></semantics></math> | ZS (%) | GQA | TextVQA | POPE | MME |\n|---|---|---|---|---|---|---|---|---|\n| \u2713(CLIP-B) | \u2717 | -2 | \u2717 | 68.3 | 59.9 | 51.2 | 84.3 | 1397.9 |\n| \u2713 | \u2717 | -2 | \u2717 | 75.7 | 62.1 | 51.7 | 85.9 | 1411.0 |\n| \u2713 | \u2713 | -2 | \u2717 | 74.3 | 61.2 | 51.1 | 86.1 | 1398.7 |\n|  |  | -1 | \u2717 |  | 50.7 | 45.0 | 77.2 | 1077.7 |\n|  |  | -1 | \u2713 |  | 40.4 | 43.0 | 50.6 | 677.17 |\n| \u2717 | \u2713 | -2 | \u2717 | 0.1 | 50.8 | 43.8 | 78.3 | 1093.8 |", "caption": "Table 5: \nResults of the Unified Multi-modal Language Model.\nThe number with \u2217 is obtained using the checkpoint trained with a similar number of seen image tokens (60M image samples, or 30B visual tokens) as ours.", "description": "\ud45c 5\ub294 \ud1b5\ud569 \ub2e4\uc911 \ubaa8\ub4dc \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \uc804\uc6a9 \ubca4\uce58\ub9c8\ud06c(ARC-Challenge, HellaSwag, PIQA, SocialIQA, Winogrande)\uc640 \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8 \uc0dd\uc131 \uc791\uc5c5(MS-COCO Karpathy \ubd84\ud560\uc5d0 \ub300\ud55c BLEU@4, METEOR, CIDEr) \ubc0f \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uc0dd\uc131 \uc791\uc5c5(MS-COCO 30k\uc5d0 \ub300\ud55c FID, CLIPScore)\uc5d0 \ub300\ud55c \uc131\ub2a5\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ubcc4\ud45c(*)\uac00 \ud45c\uc2dc\ub41c \uc22b\uc790\ub294 \uc800\ud76c \ubaa8\ub378\uacfc \ube44\uc2b7\ud55c \uc218\uc758 \uc774\ubbf8\uc9c0 \ud1a0\ud070(6\ucc9c\ub9cc \uac1c\uc758 \uc774\ubbf8\uc9c0 \uc0d8\ud50c \ub610\ub294 300\uc5b5 \uac1c\uc758 \ube44\uc8fc\uc5bc \ud1a0\ud070)\uc73c\ub85c \ud6c8\ub828\ub41c \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5bb\uc740 \uacb0\uacfc\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc989, \uc774\ubbf8\uc9c0 \ub370\uc774\ud130 \uc591\uc744 \ub9de\ucdb0 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "5.3 \uc2e4\ud5d8 \uacb0\uacfc QLIP"}, {"content": "| Tokenizer | # Images | MS-COCO 30K gFID \u2193 | MS-COCO 30K CLIPScore \u2191 | GenEval Overall \u2191 | DPG-Bench Overall \u2191 |\n|---|---|---|---|---|---| \n| VQGAN (used in [74]) | 50M | 15.68 | 0.309 | 0.32 | 43.22 |\n| BSQViT-B/16 | 15M | 19.03 | 0.303 | 0.31 | 34.03 |\n| QLIP-B/16 | 15M | 15.29 | 0.316 | 0.48 | 78.17 |", "caption": "Table 6: Ablations studies on vision-language understanding benchmarks.\nThe first row denotes the original CLIP-B model while all other rows are from our models.\n\u201cuse \ud835\udcac\ud835\udcac{\\mathcal{Q}}caligraphic_Q\u201d means that the feature is after the quantizer.", "description": "\ud45c 6\uc740 \ube44\uc804-\uc5b8\uc5b4 \uc774\ud574 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c QLIP \ubaa8\ub378\uc758 ablation \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uccab \ubc88\uc9f8 \ud589\uc740 \uc6d0\ubcf8 CLIP-B \ubaa8\ub378\uc744 \ub098\ud0c0\ub0b4\uace0, \ub098\uba38\uc9c0 \ud589\uc740 \uc81c\uc548\ub41c QLIP \ubaa8\ub378\uc758 \ubcc0\ud615\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \"use Q\" \uc5f4\uc740 \ud2b9\uc9d5 \ubca1\ud130\uac00 \uc591\uc790\ud654\uae30(Quantizer)\ub97c \uac70\uce5c \ud6c4\uc758 \ud2b9\uc9d5 \ubca1\ud130\ub97c \uc0ac\uc6a9\ud588\ub294\uc9c0 \uc5ec\ubd80\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc591\uc790\ud654\uae30\uc758 \uc0ac\uc6a9 \uc5ec\ubd80, contrastive \ud559\uc2b5\uacfc \uc7ac\uad6c\uc131 \uc190\uc2e4 \uac04\uc758 \uade0\ud615, \uadf8\ub9ac\uace0 \ubaa8\ub378\uc758 \ucd08\uae30\ud654 \ubc29\uc2dd \ub4f1\uc774 \ube44\uc804-\uc5b8\uc5b4 \uc774\ud574 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "5.3 QLIP\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| config | Stage 1 | Stage 2 |\n|---|---|---|\n| peak learning rate | 5e-4 | 5e-4 |\n| \\mathcal{E}_v learning rate | 2e-4 | 0 |\n| \\mathcal{E}_t learning rate | 2e-5 | 0 |\n| \\mathcal{G} learning rate | 2e-3 | 1e-4 |\n| learning rate schedule | cosine annealing | cosine annealing |\n| optimizer | LAMB | AdamW |\n| optimizer (\\beta_1,\\beta_2) | (0.9, 0.95) | (0.9, 0.95) |\n| weight decay | 0.05 | 0.05 |\n| gradient clip | 5 | 1 |\n| input resolution | 256 | 256 |\n| patch size | 8 | 8 |\n| warm-up iterations | 2,000 | 2,000 |\n| total iterations | 120,000 | 120,000 |\n| batch size per device | 512 | 128 |\n| total batch size | 65,536 | 16,384 |\n| \\mathcal{D} optimizer | - | AdamW |\n| \\mathcal{D} learning rate | - | 1e-4 |\n| reconstruction loss weight \\alpha_r | 1e3 | 1 |\n| contrastive loss weight \\alpha_a | 1 | 0 |\n| quantization loss weight \\alpha_q | 1 | 1 |\n| perceptual loss weight \\alpha_p | 0 | 0.1 |\n| GAN loss weight \\alpha_g | 0 | 0.1 |\n| commitment loss weight \\alpha_z | 1.0 | 0 |", "caption": "Table 7: \nZero-shot generation results on MS-COCO 30K, GenEval\u00a0[29], and DPG-Bench\u00a0[36].\nAll use LlamaGen-XL\u00a0[74].", "description": "\ud45c 7\uc740 LlamaGen-XL [74]\uc744 \uc0ac\uc6a9\ud558\uc5ec MS-COCO 30K, GenEval [29], DPG-Bench [36] \ub370\uc774\ud130\uc14b\uc5d0\uc11c Zero-shot \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc740 gFID(Generative FID), CLIPScore, GenEval \ubc0f DPG-Bench \uc810\uc218\ub97c \ud1b5\ud574 \ud3c9\uac00\ub429\ub2c8\ub2e4.  \uc774 \ud45c\ub294 QLIP \uae30\ubc18\uc758 LlamaGen\uacfc \uae30\uc874 VQGAN \uae30\ubc18\uc758 LlamaGen\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec QLIP\uac00 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \ud488\uc9c8 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.3 \uc2e4\ud5d8 \uacb0\uacfc: QLIP"}, {"content": "| config | Training UM<sup>3</sup> |\n|---|---| \n| peak learning rate | 1e-4 |\n| learning rate schedule | cosine annealing |\n| optimizer | AdamW |\n| optimizer (\u03b2<sub>1</sub>,\u03b2<sub>2</sub>) | (0.9, 0.95) |\n| weight decay | 0.1 |\n| gradient clip | 1 |\n| warm-up iterations | 2,000 |\n| total iterations | 600,000 |\n| batch size per device | 8 |\n| total batch size | 512 |\n| sequence length | 4,096 |\n| calm-down steps | 10,000 |\n| mix ratio (r<sub>text,0</sub>:r<sub>i2t</sub>:r<sub>t2i</sub>) | 60:1:3 |\n| mix ratio (r<sub>text,T</sub>:r<sub>i2t</sub>:r<sub>t2i</sub>) | 12:1:3 |\n| sampling temperature | 1.0 |\n| sampling top-p | 0.95 |", "caption": "Table 8: Hyperparamters for training QLIP. Please refer to Sec.4 for the notions of loss weights.", "description": "\ud45c 8\uc740 QLIP \ubaa8\ub378 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Stage 1\uacfc Stage 2\ub85c \ub098\ub258\uc5b4\uc838 \uc788\uc73c\uba70, \uac01 \ub2e8\uacc4\ubcc4 \ud559\uc2b5\ub960, \ucd5c\uc801\ud654 \uae30\ubc95(\uc635\ud2f0\ub9c8\uc774\uc800), \uac00\uc911\uce58 \uac10\uc18c, \uadf8\ub798\ub514\uc5b8\ud2b8 \ud074\ub9ac\ud551, \uc785\ub825 \ud574\uc0c1\ub3c4, \ud328\uce58 \ud06c\uae30, \uc6dc\uc5c5 \ubc18\ubcf5 \ud69f\uc218, \ucd1d \ubc18\ubcf5 \ud69f\uc218, \ubc30\uce58 \ud06c\uae30 \ub4f1\uc758 \uc815\ubcf4\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, \uc7ac\uad6c\uc131 \uc190\uc2e4 \uac00\uc911\uce58(ar), \ub300\uc870 \uc190\uc2e4 \uac00\uc911\uce58(aa), \uc591\uc790\ud654 \uc190\uc2e4 \uac00\uc911\uce58(aq), \uc9c0\uac01 \uc190\uc2e4 \uac00\uc911\uce58(ap), GAN \uc190\uc2e4 \uac00\uc911\uce58(ag), \ucee4\ubc0b\uba3c\ud2b8 \uc190\uc2e4 \uac00\uc911\uce58(az) \ub4f1 \uc190\uc2e4 \ud568\uc218\uc758 \uac00\uc911\uce58\ub97c \uc870\uc808\ud558\ub294 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uc774\ub294 \ubcf8 \ub17c\ubb38\uc758 4\uc7a5\uc5d0\uc11c \uc790\uc138\ud788 \uc124\uba85\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \ub530\ub77c\uc11c \ubcf8 \ud45c\ub97c \uc774\ud574\ud558\ub824\uba74 4\uc7a5\uc758 \ub0b4\uc6a9\uc744 \ud568\uaed8 \ucc38\uace0\ud574\uc57c \ud569\ub2c8\ub2e4.", "section": "4. Quantized Language-Image Pre-training"}, {"content": "| Method | Seen Data | Probing Pos. | IN-1k Acc. (%) | \n|---|---|---|---| \n| (Base backbone) |  |  |  | \n| VQVAE [83] | IN-1k | / | 18.4 | \n| LQAE [50] | IN-1k | / | 39.7 | \n| EVA-CLIP-B [75] | Merged-2B | *cls*-token | 82.7 | \n| BSQViT [98]\u2020 | DC-1B | *cls*-token | 29.3 | \n| BSQViT [98]\u2020 | DC-1B | *ft* (avg.) | 25.4 | \n| QLIP-B (ours) | DC-1B | *cls*-token | 81.8 | \n| QLIP-B (ours) | DC-1B | *ft* (avg.) | 77.7 | \n| QLIP-B (ours) | DC-1B | *cls* + *ft* | 82.1 | \n| (Large backbone, high resolution) |  |  |  | \n| EVA-CLIP-L [75] | Merged-2B | *cls*-token | 86.3 | \n| QLIP-L (ours) | DC-1B | *cls*-token | 85.2 | ", "caption": "Table 9: Hyperparamters for training UM3.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 4.1\uc808 \"\ud1b5\ud569\ub41c \ub2e4\uc911 \ubaa8\ub2ec \uc774\ud574 \ubc0f \uc0dd\uc131\"\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \ud1b5\ud569 \ub2e4\uc911 \ubaa8\ub2ec \ubaa8\ub378(UM\u00b3)\uc758 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ud559\uc2b5\ub960, \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998, \uac00\uc911\uce58 \uac10\uc18c, \uadf8\ub798\ub514\uc5b8\ud2b8 \ud074\ub9ac\ud551, \uc6cc\ubc0d\uc5c5 \ubc18\ubcf5 \ud69f\uc218, \ucd1d \ubc18\ubcf5 \ud69f\uc218, \ubc30\uce58 \ud06c\uae30, \uc2dc\ud000\uc2a4 \uae38\uc774, \ucf5c\ub2e4\uc6b4 \ub2e8\uacc4, \ud63c\ud569 \ube44\uc728(\ud14d\uc2a4\ud2b8 \uc804\uc6a9, \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8, \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0), \uc0d8\ud50c\ub9c1 \uc628\ub3c4, \uc0d8\ud50c\ub9c1 \ucd5c\uc0c1\uc704-p \ub4f1 UM\u00b3 \ud559\uc2b5\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \ub2e4\uc591\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\uc758 \uac12\uc740 UM\u00b3 \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \ucd5c\uc801\ud654\ud558\uae30 \uc704\ud574 \uc870\uc815\ub41c \uac12\ub4e4\uc785\ub2c8\ub2e4.", "section": "4. Quantized Language-Image Pre-training"}, {"content": "| config | ImageNet linear probing |\n|---|---| \n| peak learning rate | 0.2 / 1.0 (BSQViT) |\n| learning rate schedule | cosine annealing |\n| optimizer | AdamW |\n| optimizer <math alttext=\"(\\beta_{1},\\beta_{2})\" class=\"ltx_Math\" display=\"inline\">(\\beta_{1},\\beta_{2})</math> | (0.9, 0.999) |\n| weight decay | 0. |\n| input resolution | 256 (QLIP-B) / 392 (QLIP-L) |\n| patch size | 16 (QLIP-B) / 14 (QLIP-L) |\n| warm-up epochs | 1 |\n| total epochs | 10 / 20 (BSQViT) |\n| batch size per device | 128 |\n| total batch size | 1,024 |", "caption": "Table 10: Linear evaluation on image classification.", "description": "\ubcf8 \ud45c\ub294 \uc774\ubbf8\uc9c0 \ubd84\ub958 \uc791\uc5c5\uc5d0\uc11c \uc120\ud615 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ubc31\ubcf8(\uae30\ubcf8, \uc791\uc740 \ud328\uce58, \ud070 \ubc31\ubcf8, \uace0\ud574\uc0c1\ub3c4 \ud070 \ubc31\ubcf8)\uacfc \ud568\uaed8 \uc5ec\ub7ec \uac00\uc9c0 \uc2dc\uac01\uc801 \ud1a0\ud070\ud654 \ubc29\ubc95(CLIP, EVA-CLIP, SigLIP, VQGAN, MoVQGAN, MaskGIT, Open-MAGVIT, OpenCLIP, BSQViT, \uadf8\ub9ac\uace0 QLIP)\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4. \uac01 \ubc29\ubc95\uc5d0 \ub300\ud574 0-shot \uc815\ud655\ub3c4\uc640 \uc7ac\uad6c\uc131 \ud488\uc9c8(FID, PSNR, SSIM)\uc744 \ud3c9\uac00\ud558\uc5ec, \uc2dc\uac01\uc801 \uc774\ud574\uc640 \uc7ac\uad6c\uc131 \uc131\ub2a5 \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubd84\uc11d\ud569\ub2c8\ub2e4.  [CLS] \ud1a0\ud070 \ub610\ub294 \ud3c9\uade0 \ud2b9\uc9d5 \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud55c \uc120\ud615 \ud0d0\uce68\uc744 \ud1b5\ud574 \uc2dc\uac01\uc801 \uc778\ucf54\ub354\uac00 \uc5bc\ub9c8\ub098 \uc758\ubbf8\ub860\uc801 \uc815\ubcf4\ub97c \uc798 \ud559\uc2b5\ud558\ub294\uc9c0 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "5.3 QLIP\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Model | Tokenizer | Overall | Single Obj. | Two Obj. | Counting | Colors | Position | Attribute |\n|---|---|---|---|---|---|---|---|---|\n| LlamaGen | VQGAN | 0.32 | 0.69 | 0.36 | 0.20 | 0.57 | 0.06 | 0.02 |\n| (0.8B) | BSQ-ViT | 0.31 | 0.77 | 0.26 | 0.13 | 0.56 | 0.05 | 0.06 |\n| **QLIP (Ours)** |  | **0.48** | **0.91** | **0.59** | **0.22** | **0.80** | **0.17** | **0.24** |\n| SDv1.5 (0.9B) |  | 0.43 | 0.97 | 0.38 | 0.35 | 0.76 | 0.04 | 0.06 |", "caption": "Table 11: Hyperparamters for ImageNet linear probing.", "description": "\ud45c 11\uc740 ImageNet \uc120\ud615 \ud3c9\uac00\ub97c \uc704\ud55c \ucd08\ub9e4\uac1c\ubcc0\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 ImageNet \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc120\ud615 \ud3c9\uac00\ub97c \uc218\ud589\ud560 \ub54c \uc0ac\uc6a9\ub41c \ud559\uc2b5 \uc124\uc815(\ucd5c\ub300 \ud559\uc2b5\ub960, \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904, \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998, \ucd5c\uc801\ud654\uae30 \ub9e4\uac1c\ubcc0\uc218, \uac00\uc911\uce58 \uac10\uc1e0, \uc785\ub825 \ud574\uc0c1\ub3c4, \ud328\uce58 \ud06c\uae30, \uc6dc\uc5c5 \uc5d0\ud3ed, \ucd1d \uc5d0\ud3ed, \ubc30\uce58 \ud06c\uae30 \ub4f1)\uc744 \uc790\uc138\ud788 \uc124\uba85\ud569\ub2c8\ub2e4.  \uc774\ub7ec\ud55c \uc124\uc815\ub4e4\uc740 QLIP(Quantized Language-Image Pre-training) \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "5.3. QLIP\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Model | Tokenizer | Average | Global | Entity | Attribute | Relation | Other |\n|---|---|---|---|---|---|---|---| \n| LlamaGen | VQGAN | 43.22 | 76.60 | 57.88 | 66.96 | 75.78 | 42.80 |\n| (0.8B) | BSQ-ViT | 34.03 | 68.39 | 47.70 | 63.40 | 73.77 | 33.60 |\n| **QLIP (Ours)** |  | **78.17** | **82.37** | **84.68** | **86.97** | **92.50** | **79.20** |\n| SDv1.5 (0.9B) |  | 63.18 | 74.63 | 74.23 | 75.39 | 73.49 | 67.81 |", "caption": "Table 12: Evaluation on GenEval.", "description": "\ubcf8 \ud45c\ub294 GenEval \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. GenEval\uc740 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubaa8\ub378\uc758 \ud488\uc9c8\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \ubca4\uce58\ub9c8\ud06c\ub85c, \uac1c\uccb4, \uac1c\uccb4 \uac04\uc758 \uad00\uacc4, \uc0c9\uc0c1 \ub4f1 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc18d\uc131\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \ud45c\uc5d0\ub294 VQGAN, BSQ-ViT, QLIP \ub4f1 \uc5ec\ub7ec \ubaa8\ub378\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc740 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc18d\uc131\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub85c \uce21\uc815\ub429\ub2c8\ub2e4. QLIP \ubaa8\ub378\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.3 QLIP\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "## Table 1: Ablation Study of the Proposed QLIP Model\n\n| Model          | Dataset    | # Param. | Understanding | # bits | 0-shot Acc. \u2191 | rFID \u2193 | PSNR \u2191 | SSIM \u2191 |\n|-----------------|-------------|------------|-----------------|--------|---------------|--------|--------|--------|\n| (Base backbone) |             |            |                 |        |               |        |        |        |\n| CLIP [59]       | WIT-400M    | 87M+0+0    |                 | /      | 68.3           | /      | /      | /      |\n| EVA-CLIP [75]   | Merged-2B   | 87M+0+0    |                 | /      | 74.7           | /      | /      | /      |\n| SigLIP-B [96]   | WL-10B      | 87M+0+0    |                 | /      | 76.7           | /      | /      | /      |\n| VQGAN [24]      | IN-1k      | 29M+42M+4M |                 | 14     | /              | 4.98   | -      | -      |\n| MoVQGAN [100]   | IN-1k      | (82.7M)   |                 | 40     | /              | 1.12   | 22.42  | 0.6731 |\n| MaskGIT [8]     | IN-1k      | 24M+30M+6k |                 | 10     | /              | 1.98   | 18.63  | 0.4619 |\n| Open-MAGVIT2 [93, 54] | IN-1k      | 25M+40M+18k |                 | 18     | /              | 1.53   | 21.53  | -      |\n| OpenCLIP-B [13] | DC-1B      | 87M+0+0    |                 | /      | 73.5           | /      | /      | /      |\n| BSQViT [98]<sup>\u2020</sup> | DC-1B      | 87M+87M+1M |                 | 28     | /              | 3.81   | 24.12  | 0.6638 |\n| QLIP-B (ours)   | DC-1B      | 87M+87M+1M |                 | 28     | 74.3           | 3.21   | 23.16  | 0.6286 |\n| (Base backbone, Smaller patch) |             |            |                 |        |               |        |        |        |\n| SigLIP-B [96]   | WL-10B      | 87M+0+0    |                 | /      | 79.2           | /      | /      | /      |\n| DALL-E dVAE [62] | CC3M+YF     | 54M+44M+0  |                 | 13     | /              | 32.63  | 27.31  | 0.7943 |\n| ViT-VQGAN [91]  | IN-1k      | 91M+91M+0.5M|                 | 13     | /              | 1.55   | -      | -      |\n| SD-VAE 1.x [63] | OI-2M      | 34M+49M+0  |                 | 14     | /              | 1.40   | 23.65  | 0.6354 |\n| SD-VAE 2.x [58]<sup>#</sup> | OI-2M+LA-ae | 34M+49M+0  |                 | 64     | /              | 0.70   | 26.90  | 0.7592 |\n| SDXL-VAE [58]<sup>#</sup> | OI-2M+LA-ae++| 34M+49M+0  |                 | 64     | /              | 0.67   | 27.37  | 0.7814 |\n| SBER-MoVQGAN [66]| LAHR-166M   | 29M+42M+4M |                 | 14     | /              | 0.96   | 26.45  | 0.7250 |\n| BSQViT [98]     | IN-1k      | 87M+87M+28k|                 | 18     | /              | 0.99   | 27.78  | 0.8171 |\n| EVA-CLIP [75]<sup>\u2020</sup> | DC-1B      | 87M+0+0    |                 | /      | 77.2           | /      | /      | /      |\n| QLIP-B (ours)   | DC-1B      | 87M+87M+1M |                 | 28     | 75.6           | 0.70   | 26.79  | 0.7905 |\n| (Large backbone) |             |            |                 |        |               |        |        |        |\n| CLIP/f14 [59]   | WIT-400M    | 304M+0+0   |                 | /      | 75.5           | /      | /      | /      |\n| SigLIP-L [96]   | WL-10B      | 304M+0+0   |                 | /      | 80.5           | /      | /      | /      |\n| OpenCLIP-L [13] | DC-1B      | 304M+0+0   |                 | /      | 79.2           | /      | /      | /      |\n| EVA-CLIP-L [75] | Merged-2B   | 304M+0+0   |                 | /      | 79.8           | /      | /      | /      |\n| Open-MAGVIT2 [93, 54] | IN-1k      | 50M+65M+18k |                 | 18     | /              | 1.17   | 21.90  | -      |\n| VILA-U [89]<sup>&</sup> | WL-10B+CY-1B| 316M+42M+134M|                 | 56     | 73.3           | 1.80   | -      | -      |\n| (Large backbone, high resolution) |             |            |                 |        |               |        |        |        |\n| CLIP/f14 [59]   | WIT-400M    | 304M+0+0   |                 | /      | 76.6           | /      | /      | /      |\n| SigLIP-L [96]   | WL-10B      | 304M+0+0   |                 | /      | 82.1           | /      | /      | /      |\n| EVA-CLIP-L [75] | Merged-2B   | 304M+0+0   |                 | /      | 80.4           | /      | /      | /      |\n| VILA-U [89]<sup>&</sup> | WL-10B+CY-1B| 428M+42M+537M|                 | 224    | 78.0           | 1.25   | -      | -      |\n| QLIP-L (ours)   | DC-1B      | 304M+304M+2M|                 | 28     | 79.1           | 1.46   | 25.36  | 0.6903 |\n\n<sup>\u2020</sup> indicates that the model uses a different training scheme.\n<sup>#</sup> indicates that the model is from Stable Diffusion.\n<sup>&</sup> indicates that the model is from VILA-U. ", "caption": "Table 13: Evaluation on DPG-Bench.", "description": "DPG-Bench \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.  LlamaGen \uae30\ubc18\uc758 \uc138 \uac00\uc9c0 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubaa8\ub378(VQGAN, BSQViT, QLIP)\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00 \uc9c0\ud45c\uc778 Average, Global, Entity, Attribute, Relation, Other \uce21\uba74\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc9c0\ud45c\ub294 0~100 \uc0ac\uc774\uc758 \uc810\uc218\ub85c, \ub192\uc744\uc218\ub85d \uc131\ub2a5\uc774 \uc6b0\uc218\ud568\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 QLIP \uae30\ubc18 \ubaa8\ub378\uc774 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \uc804\ubc18\uc801\uc73c\ub85c \ub354 \ub192\uc740 \uc810\uc218\ub97c \ubc1b\uc558\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.3 QLIP\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc"}]