[{"content": "| [Image] | [Question] | \n|---|---| \n| (A) [Choice A] |  | \n| (B) [Choice B] |  | \n| (C) [Choice C] |  | \n| (D) [Choice D] |  | \n| (E) [Choice E] |  |", "caption": "Table 1: Prompt setting of MME-RealWorld.", "description": "\ubcf8 \ub17c\ubb38\uc758 \ud45c 1\uc740 MME-RealWorld \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ud504\ub86c\ud504\ud2b8 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub354 \uc790\uc138\ud788 \uc124\uba85\ud558\uc790\uba74,  \ub2e4\uc591\ud55c \uc2e4\uc81c \uc0c1\ud669(real-world scenarios)\uc744 \ubc18\uc601\ud55c \uba40\ud2f0\ubaa8\ub2ec \uc9c8\ubb38\uc751\ub2f5(multimodal question answering) \uc791\uc5c5\uc744 \uc704\ud55c \ud504\ub86c\ud504\ud2b8\uc758 \uc885\ub958, \uac01 \uc885\ub958\uc5d0 \ub300\ud55c \uc124\uba85, \uadf8\ub9ac\uace0 \uac01 \ud504\ub86c\ud504\ud2b8 \uc720\ud615\uc5d0 \ud574\ub2f9\ud558\ub294 \uc9c8\ubb38\uacfc \ub2f5\ubcc0\uc758 \uc608\uc2dc \ub4f1\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc744 \uac83\uc73c\ub85c \uc608\uc0c1\ub429\ub2c8\ub2e4.  \ud45c\ub97c \ud1b5\ud574 MME-RealWorld \ub370\uc774\ud130\uc14b\uc774 \uc5b4\ub5bb\uac8c \uad6c\uc131\ub418\uc5c8\ub294\uc9c0, \uadf8\ub9ac\uace0 \uc5b4\ub5a4 \uc885\ub958\uc758 \uc9c8\ubb38\uacfc \ub2f5\ubcc0\uc744 \ud3ec\ud568\ud558\uace0 \uc788\ub294\uc9c0\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uc815\ubcf4\ub97c \ud30c\uc545\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.1 Benchmarks and Experimental Details"}, {"content": "| [Image] [Question] The choices are listed below: |\n|---|---| \n|(A) [Choice A] |\n|(B) [Choice B] |\n|(C) [Choice C] |\n|(D) [Choice D] |\n|(E) [Choice E] |\n| Select the best answer to the above multiple-choice question based on the image. Respond with only the letter (A, B, C, D, or E) of the correct option. |\n| The best answer is: |", "caption": "Table 2: Comparison of benchmarks. MME-RealWorld is the largest fully human-annotated dataset, featuring the highest average resolution and the most challenging tasks.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MME-RealWorld\ub294 \uac00\uc7a5 \ud070 \uaddc\ubaa8\uc758 \uc644\uc804\ud55c \uc218\ub3d9 \uc8fc\uc11d \ub370\uc774\ud130\uc14b\uc774\uba70, \ud3c9\uade0 \ud574\uc0c1\ub3c4\uac00 \uac00\uc7a5 \ub192\uace0 \uac00\uc7a5 \uc5b4\ub824\uc6b4 \uc791\uc5c5\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc758 \ud2b9\uc9d5\uc744 \ube44\uad50\ud558\uc5ec, \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30, \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4, \uc791\uc5c5\uc758 \ubcf5\uc7a1\uc131 \ub4f1\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4. \ud2b9\ud788 MME-RealWorld \ub370\uc774\ud130\uc14b\uc740 \uc218\ub3d9 \uc8fc\uc11d \uc791\uc5c5\uc758 \uc5b4\ub824\uc6c0\uacfc \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0 \uc0ac\uc6a9\uc73c\ub85c \uc778\ud574 \ub2e4\ub978 \ub370\uc774\ud130\uc14b\ubcf4\ub2e4 \ub354\uc6b1 \uae4c\ub2e4\ub85c\uc6b4 \uc791\uc5c5\ub4e4\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc74c\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "5.1 \ubca4\uce58\ub9c8\ud06c \ubc0f \uc2e4\ud5d8 \uc138\ubd80 \uc815\ubcf4"}, {"content": "| Benchmark | # QA-Pair | Fully Human Annotation | CN | Average Resolution | LLaVA-1.5-7B Performance |\n|---|---|---|---|---|---| \n| VizWiz | 8000 | \u00d7 | \u00d7 | 1224x1224 | 50.0 |\n| RealWorldQA | 765 | \u00d7 | \u00d7 | 1536x863 | - |\n| TextVQA | 5734 | \u2713 | \u00d7 | 985x768 | 58.2 |\n| MME | 2374 | \u2713 | \u00d7 | 1161x840 | 76.0 |\n| MMBench | 3217 | \u2713 | \u2713 | 512x270 | 64.3 |\n| MMStar | 1500 | \u00d7 | \u00d7 | 512x375 | 30.3 |\n| ScienceQA | 21000 | \u00d7 | \u00d7 | 378x249 | 71.6 |\n| ChartQA | 32719 | \u00d7 | \u00d7 | 840x535 | - |\n| MM-Vet | 218 | \u00d7 | \u00d7 | 1200x675 | 31.1 |\n| Seed-Bench | 19242 | \u00d7 | \u00d7 | 1024x931 | 66.1 |\n| SEED-Bench-2-Plus | 2300 | \u00d7 | \u00d7 | 1128x846 | 36.8 |\n| MMT-Bench | 32325 | \u00d7 | \u00d7 | 2365x377 | 49.5 |\n| MathVista | 735 | \u00d7 | \u00d7 | 539x446 | 26.1 |\n| TouchStone | 908 | \u00d7 | \u00d7 | 897x803 | - |\n| VisIT-Bench | 1159 | \u00d7 | \u00d7 | 765x1024 | - |\n| BLINK | 3807 | \u00d7 | \u00d7 | 620x1024 | 37.1 |\n| CV-Bench | 2638 | \u00d7 | \u00d7 | 1024x768 | - |\n| MME-RealWorld | 29429 | \u2713 | \u2713 | 2000x1500 | 24.9 |", "caption": "Table 3: Experimental results on the perception tasks. Models are ranked according to their average performance. Rows corresponding to proprietary models are highlighted in gray for distinction. \u201cOCR\u201d, \u201cRS\u201d, \u201cDT\u201d, \u201cMO\u201d, and \u201cAD\u201d each indicate a specific task domain: Optical Character Recognition in the Wild, Remote Sensing, Diagram and Table, Monitoring, and Autonomous Driving, respectively. \u201cAvg\u201d and \u201cAvg-C\u201d indicate the weighted average accuracy\nand the unweighted average accuracy across\nsubtasks in each domain.", "description": "\ud45c 3\uc740 \ub2e4\uc591\ud55c \uc9c0\uac01 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\uc740 \ud3c9\uade0 \uc131\ub2a5\uc5d0 \ub530\ub77c \uc21c\uc704\uac00 \ub9e4\uaca8\uc9d1\ub2c8\ub2e4. \ub3c5\uc810 \ubaa8\ub378\uc5d0 \ud574\ub2f9\ud558\ub294 \ud589\uc740 \uad6c\ubd84\uc744 \uc704\ud574 \ud68c\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \"OCR\", \"RS\", \"DT\", \"MO\", \"AD\"\ub294 \uac01\uac01 \ud2b9\uc815 \uc791\uc5c5 \ub3c4\uba54\uc778(\uc57c\uc0dd\uc5d0\uc11c\uc758 \uad11\ud559 \ubb38\uc790 \uc778\uc2dd, \uc6d0\uaca9 \uac10\uc9c0, \ub2e4\uc774\uc5b4\uadf8\ub7a8 \ubc0f \ud45c, \ubaa8\ub2c8\ud130\ub9c1, \uc790\uc728 \uc8fc\ud589)\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \"Avg\"\uc640 \"Avg-C\"\ub294 \uac01 \ub3c4\uba54\uc778\uc758 \ud558\uc704 \uc791\uc5c5\uc5d0 \ub300\ud55c \uac00\uc911 \ud3c9\uade0 \uc815\ud655\ub3c4\uc640 \ube44\uac00\uc911 \ud3c9\uade0 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "5.1 Benchmarks and Experimental Details"}, {"content": "| Fully Human | Annotation |\n|---|---|", "caption": "Table 4: Experimental results on the reasoning tasks. Models are ranked according to their average performance. Rows corresponding to proprietary models are highlighted in gray for distinction. \u201cOCR\u201d, \u201cRS\u201d, \u201cDT\u201d, \u201cMO\u201d, and \u201cAD\u201d each indicate a specific task domain: Optical Character Recognition in the Wild, Remote Sensing, Diagram and Table, Monitoring, and Autonomous Driving, respectively. \u201cAvg\u201d and \u201cAvg-C\u201d indicate the weighted average accuracy\nand the unweighted average accuracy across\nsubtasks in each domain.", "description": "\ud45c 4\ub294 \ucd94\ub860 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\uc740 \ud3c9\uade0 \uc131\ub2a5\uc5d0 \ub530\ub77c \uc21c\uc704\uac00 \ub9e4\uaca8\uc9d1\ub2c8\ub2e4. \ub3c5\uc810 \ubaa8\ub378\uc5d0 \ud574\ub2f9\ud558\ub294 \ud589\uc740 \uad6c\ubd84\uc744 \uc704\ud574 \ud68c\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \"OCR\", \"RS\", \"DT\", \"MO\", \"AD\"\ub294 \uac01\uac01 \ud2b9\uc815 \uacfc\uc81c \uc601\uc5ed(Optical Character Recognition in the Wild, Remote Sensing, Diagram and Table, Monitoring, Autonomous Driving)\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \"Avg\"\uc640 \"Avg-C\"\ub294 \uac01 \uc601\uc5ed\uc758 \ud558\uc704 \uacfc\uc81c\uc5d0 \ub300\ud55c \uac00\uc911 \ud3c9\uade0 \uc815\ud655\ub3c4\uc640 \ube44\uac00\uc911 \ud3c9\uade0 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ucd94\ub860 \uacfc\uc81c\uc5d0\uc11c \uc5ec\ub7ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uac01 \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ud30c\uc545\ud558\uace0, \ud2b9\ud788 \ub3c5\uc810 \ubaa8\ub378\uacfc \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378 \uac04\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uba85\ud655\ud788 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\uc911 \ud3c9\uade0\uacfc \ube44\uac00\uc911 \ud3c9\uade0\uc744 \ubaa8\ub450 \uc81c\uc2dc\ud558\uc5ec \ub2e4\uc591\ud55c \uad00\uc810\uc5d0\uc11c\uc758 \uc131\ub2a5 \ud3c9\uac00\ub97c \uac00\ub2a5\ud558\uac8c \ud569\ub2c8\ub2e4.", "section": "5. Experiments"}, {"content": "| Average | Resolution |\n|---|---|", "caption": "Table 5: Experimental results on the perception tasks of MME-RealWorld-CN. Models are ranked according to their average performance. Rows corresponding to proprietary models are highlighted in gray for distinction. \u201cOCR\u201d, \u201cRS\u201d, \u201cDT\u201d, \u201cMO\u201d, and \u201cAD\u201d each indicate a specific task domain: Optical Character Recognition in the Wild, Remote Sensing, Diagram and Table, Monitoring, and Autonomous Driving, respectively. \u201cAvg\u201d and \u201cAvg-C\u201d indicate the weighted average accuracy\nand the unweighted average accuracy across\nsubtasks in each domain.", "description": "\ud45c 5\ub294 MME-RealWorld-CN\uc758 \uc9c0\uac01 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\uc740 \ud3c9\uade0 \uc131\ub2a5\uc5d0 \ub530\ub77c \uc21c\uc704\uac00 \ub9e4\uaca8\uc9d1\ub2c8\ub2e4. \ub3c5\uc810 \ubaa8\ub378\uc5d0 \ud574\ub2f9\ud558\ub294 \ud589\uc740 \uad6c\ubd84\uc744 \uc704\ud574 \ud68c\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \"OCR\", \"RS\", \"DT\", \"MO\", \"AD\"\ub294 \uac01\uac01 \uc57c\uc678 \uad11\ud559 \ubb38\uc790 \uc778\uc2dd, \uc6d0\uaca9 \uac10\uc9c0, \ub2e4\uc774\uc5b4\uadf8\ub7a8 \ubc0f \ud45c, \ubaa8\ub2c8\ud130\ub9c1, \uc790\uc728 \uc8fc\ud589\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \"Avg\"\uc640 \"Avg-C\"\ub294 \uac01 \ub3c4\uba54\uc778\uc758 \ud558\uc704 \uacfc\uc81c\uc5d0 \ub300\ud55c \uac00\uc911 \ud3c9\uade0 \uc815\ud655\ub3c4\uc640 \ube44\uac00\uc911 \ud3c9\uade0 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "5.2 MM-RLHF \ubc0f MM-DPO \ud3c9\uac00"}, {"content": "| Model Name | Performance |\n|---|---| \n| LLaVA-1.5-7B |  |", "caption": "Table 6: Experimental results on the reasoning tasks of MME-RealWorld-CN. Models are ranked according to their average performance. Rows corresponding to proprietary models are highlighted in gray for distinction. \u201cOCR\u201d, \u201cDT\u201d, \u201cMO\u201d, and \u201cAD\u201d each indicate a specific task domain: Optical Character Recognition in the Wild, Diagram and Table, Monitoring and Autonomous Driving, respectively. \u201cAvg\u201d and \u201cAvg-C\u201d indicate the weighted average accuracy\nand the unweighted average accuracy across\nsubtasks in each domain.", "description": "\ud45c 6\ub294 MME-RealWorld-CN\uc758 \ucd94\ub860 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\uc740 \ud3c9\uade0 \uc131\ub2a5\uc5d0 \ub530\ub77c \uc21c\uc704\uac00 \ub9e4\uaca8\uc9d1\ub2c8\ub2e4. \ub3c5\uc810 \ubaa8\ub378\uc5d0 \ud574\ub2f9\ud558\ub294 \ud589\uc740 \uad6c\ubd84\uc744 \uc704\ud574 \ud68c\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \"OCR\", \"DT\", \"MO\", \"AD\"\ub294 \uac01\uac01 \uc57c\uc0dd\uc758 \uad11\ud559 \ubb38\uc790 \uc778\uc2dd, \ub2e4\uc774\uc5b4\uadf8\ub7a8 \ubc0f \ud45c, \ubaa8\ub2c8\ud130\ub9c1 \ubc0f \uc790\uc728 \uc8fc\ud589\uc744 \ub098\ud0c0\ub0b4\ub294 \ud2b9\uc815 \uc791\uc5c5 \uc601\uc5ed\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \"Avg\"\uc640 \"Avg-C\"\ub294 \uac01 \uc601\uc5ed\uc758 \ud558\uc704 \uacfc\uc81c\uc5d0 \ub300\ud55c \uac00\uc911 \ud3c9\uade0 \uc815\ud655\ub3c4\uc640 \ube44\uac00\uc911 \ud3c9\uade0 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "5.2 MM-RLHF \ubc0f MM-DPO \ud3c9\uac00"}, {"content": "| Method | LLM | OCR | RS | DT | MO | AD | Avg | Avg-C |\n|---|---|---|---|---|---|---|---|---|\n| Qwen2-VL | Qwen2-7B | 81.38 | 44.81 | 70.18 | 37.3 | 34.62 | 58.96 | 53.66 |\n| InternVL-2 | InternLM2.5-7B-Chat | 73.92 | 39.35 | 62.80 | 53.19 | 35.46 | 55.82 | 52.94 |\n| Claude 3.5 Sonnet | - | 72.47 | 25.74 | 67.44 | 32.19 | 40.77 | 52.90 | 47.72 |\n| InternLM-XComposer2.5 | InternLM2-7B | 69.25 | 36.12 | 63.92 | 39.48 | 33.63 | 52.47 | 48.48 |\n| InternVL-Chat-V1.5 | InternLM2-Chat-20B | 71.51 | 33.55 | 55.83 | 51.16 | 31.42 | 51.36 | 48.69 |\n| Mini-Gemini-34B-HD | Nous-Hermes-2-Yi-34B | 69.55 | 40.40 | 44.36 | 39.61 | 32.70 | 48.05 | 45.32 |\n| MiniCPM-V 2.5 | Llama3-8B | 66.79 | 27.69 | 52.81 | 38.70 | 34.15 | 47.37 | 44.03 |\n| Cambrian-1-34B | Nous-Hermes-2-Yi-34B | 66.45 | 38.63 | 40.44 | 45.98 | 33.61 | 46.68 | 45.02 |\n| GPT-4o | - | 77.69 | 28.92 | 46.68 | 33.93 | 22.43 | 46.43 | 41.93 |\n| CogVLM2-llama3-Chat | Llama3-8B | 69.97 | 28.76 | 47.51 | 33.74 | 30.22 | 45.84 | 42.04 |\n| Cambrian-1-8B | Llama3-8B-Instruct | 58.68 | 40.05 | 32.73 | 47.68 | 38.52 | 43.82 | 43.53 |\n| SliME-8B | Llama3-8B | 53.45 | 42.27 | 29.34 | 40.62 | 33.66 | 40.29 | 39.87 |\n| Gemini-1.5-pro | - | 67.62 | 13.99 | 39.90 | 31.11 | 26.64 | 39.63 | 35.85 |\n| GPT-4o-mini | - | 62.51 | 6.69 | 44.23 | 26.50 | 24.18 | 37.12 | 32.82 |\n| Monkey | Qwen-7B | 54.63 | 24.99 | 32.51 | 28.01 | 29.67 | 36.30 | 33.96 |\n| mPLUG-DocOwl 1.5 | Llama-7B | 51.15 | 23.71 | 29.34 | 24.97 | 28.28 | 33.71 | 31.49 |\n| DeepSeek-VL | DeepSeek-LLM-7b-base | 49.55 | 25.49 | 23.38 | 26.97 | 33.39 | 33.14 | 31.76 |\n| SliME-13B | Vicuna-13B | 50.58 | 25.82 | 20.93 | 24.73 | 27.16 | 31.50 | 29.84 |\n| Mini-Gemini-7B-HD | Vicuna-7B-v1.5 | 42.02 | 31.30 | 22.31 | 34.15 | 24.81 | 31.07 | 30.92 |\n| YI-VL-34B | Yi-34B-Chat | 44.95 | 31.62 | 15.99 | 34.85 | 28.31 | 30.97 | 31.14 |\n| LLaVA-Next | Llama3-8B | 47.94 | 25.42 | 26.63 | 19.46 | 18.66 | 30.14 | 27.62 |\n| LLaVA-Next | Qwen-72B | 37.07 | 29.13 | 27.68 | 29.37 | 17.98 | 29.01 | 28.25 |\n| LLaVA1.5-13B | Vicuna-13B | 44.10 | 23.27 | 20.17 | 20.45 | 26.12 | 28.42 | 26.82 |\n| ShareGPT4V-13B | Vicuna-13B | 44.55 | 23.06 | 20.17 | 19.26 | 26.12 | 28.38 | 26.63 |\n| MiniGPT-v2 | Llama 2-7B-Chat | 39.02 | 23.33 | 20.41 | 19.26 | 25.96 | 26.94 | 25.60 |\n| ShareGPT4V-7B | Vicuna-7B | 39.39 | 22.10 | 20.08 | 19.13 | 26.04 | 26.73 | 25.35 |\n| LLaVA1.5-7B | Vicuna-7B | 38.69 | 22.12 | 20.08 | 19.13 | 26.04 | 26.54 | 25.21 |\n| Qwen-VL-Chat | Qwen-7B | 32.37 | 15.14 | 15.59 | 22.13 | 15.08 | 20.75 | 20.06 |\n| TextMonkey | Qwen-7B | 37.30 | 11.69 | 5.93 | 16.14 | 14.26 | 18.18 | 17.06 |\n| # QA pairs |  | 5740 | 3738 | 5433 | 2196 | 3660 | 20767 | 20767 |", "caption": "Table 7: MME-RealWorld-SafetyBench: summary of Task Data, Evaluation Metrics, and Comparison Methods for Safety and Adversarial Testing. This table provides an overview of various tasks used for evaluating multimodal models\u2019 safety and adversarial robustness. The tasks are categorized based on attack type (adversarial or safety), and the evaluation metrics include success rates of adversarial attacks or model rejection rates for harmful outputs. The arrows in the Comparison column indicate whether higher (\u2191\u2191\\uparrow\u2191) or lower (\u2193\u2193\\downarrow\u2193) values of the evaluation metric are preferred.", "description": "\ud45c 7\uc740 MM-RLHF-SafetyBench\uc758 \uc694\uc57d \uc815\ubcf4, \ud3c9\uac00 \uc9c0\ud45c \ubc0f \ube44\uad50 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc885\ub958\uc758 \uc791\uc5c5(\uc801\ub300\uc801 \uacf5\uaca9 \ub610\ub294 \uc548\uc804 \uad00\ub828)\uc5d0 \ub300\ud55c \ub2e4\uc911 \ubaa8\ub4dc \ubaa8\ub378\uc758 \uc548\uc804\uc131 \ubc0f \uc801\ub300\uc801 \uac15\uac74\uc131\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub41c \uc5ec\ub7ec \uc791\uc5c5\ub4e4\uc744 \uac1c\uad04\uc801\uc73c\ub85c \uc124\uba85\ud569\ub2c8\ub2e4. \uc791\uc5c5\uc740 \uacf5\uaca9 \uc720\ud615(\uc801\ub300\uc801 \ub610\ub294 \uc548\uc804)\uc5d0 \ub530\ub77c \ubd84\ub958\ub418\uace0, \ud3c9\uac00 \uc9c0\ud45c\uc5d0\ub294 \uc801\ub300\uc801 \uacf5\uaca9\uc758 \uc131\uacf5\ub960 \ub610\ub294 \uc720\ud574\ud55c \ucd9c\ub825\uc744 \uac70\ubd80\ud558\ub294 \ubaa8\ub378\uc758 \uac70\ubd80\uc728\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4. \ube44\uad50 \uc5f4\uc758 \ud654\uc0b4\ud45c\ub294 \ud3c9\uac00 \uc9c0\ud45c\uc758 \ub354 \ub192\uc740(\u2191\u2191) \ub610\ub294 \ub354 \ub0ae\uc740(\u2193\u2193) \uac12\uc774 \uc120\ud638\ub418\ub294\uc9c0 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "5.2 MM-RLHF \ubc0f MM-DPO \ud3c9\uac00"}, {"content": "| Method | LLM | OCR | DT | MO | AD | Avg | Avg-C |\n|---|---|---|---|---|---|---|---| \n| Task Split |  |  |  |  |  |  |  |\n| # QA pairs |  | 500 | 500 | 498 | 1334 | 2832 | 2832 |\n| Claude 3.5 Sonnet | - | 61.90 | 61.20 | 41.79 | 31.92 | 44.12 | 49.20 |\n| Qwen2-VL | Qwen2-7B | 63.40 | 48.60 | 33.13 | 31.47 | 40.39 | 44.15 |\n| InternVL-2 | InternLM2.5-7B-Chat | 57.40 | 39.00 | 43.57 | 29.84 | 38.74 | 42.45 |\n| GPT-4o | - | 61.40 | 44.80 | 36.51 | 26.41 | 37.61 | 42.28 |\n| CogVLM2-llama3-Chat | Llama3-8B | 54.00 | 32.80 | 41.16 | 31.18 | 37.25 | 39.79 |\n| InternVL-Chat-V1-5 | InternLM2-Chat-20B | 56.80 | 35.40 | 37.35 | 28.94 | 36.48 | 39.62 |\n| Cambrian-1-8B | Llama3-8B-Instruct | 53.20 | 27.40 | 42.37 | 30.73 | 36.16 | 38.43 |\n| SliME-8B | Llama3-8B | 53.20 | 29.40 | 36.14 | 31.55 | 35.80 | 37.57 |\n| MiniCPM-V 2.5 | Llama3-8B | 44.00 | 31.80 | 36.95 | 31.03 | 34.50 | 35.95 |\n| SliME-13B | Vicuna-13B | 41.00 | 39.00 | 33.13 | 30.80 | 34.46 | 35.98 |\n| InternLM-XComposer2.5 | InternLM2-7B | 53.40 | 41.00 | 17.67 | 29.99 | 33.90 | 35.52 |\n| GPT-4o-mini | - | 47.00 | 39.80 | 25.81 | 26.79 | 32.48 | 34.85 |\n| YI-VL-34B | Yi-34B-Chat | 42.40 | 26.00 | 31.33 | 31.55 | 32.45 | 32.82 |\n| LLaVA-Next | Llama3-8B | 55.20 | 23.40 | 21.08 | 30.73 | 32.06 | 32.60 |\n| Mini-Gemini-34B-HD | Nous-Hermes-2-Yi-34B | 59.20 | 39.20 | 20.48 | 22.84 | 31.73 | 35.43 |\n| Gemini-1.5-pro | - | 52.70 | 33.20 | 28.33 | 19.20 | 29.19 | 33.36 |\n| Monkey | Qwen-7B | 27.20 | 20.80 | 27.31 | 33.04 | 28.84 | 27.09 |\n| DeepSeek-VL | DeepSeek-LLM-7b-base | 45.20 | 23.80 | 16.67 | 27.31 | 27.98 | 28.25 |\n| LLaVA-Next | Qwen-72B | 17.20 | 34.20 | 27.31 | 29.69 | 27.86 | 27.10 |\n| Cambrian-1-34B | Nous-Hermes-2-Yi-34B | 55.00 | 36.00 | 19.48 | 16.07 | 27.06 | 31.64 |\n| mPLUG-DocOwl 1.5 | Llama-7B | 42.60 | 19.80 | 20.48 | 26.04 | 26.88 | 27.23 |\n| Mini-Gemini-7B-HD | Vicuna-7B-v1.5 | 35.40 | 24.60 | 25.90 | 23.29 | 26.12 | 27.30 |\n| LLaVA1.5-13B | Vicuna-13B | 30.20 | 20.80 | 27.51 | 24.78 | 25.51 | 25.82 |\n| ShareGPT4V-13B | Vicuna-13B | 26.00 | 20.80 | 27.31 | 24.55 | 24.63 | 24.67 |\n| LLaVA1.5-7B | Vicuna-7B | 26.00 | 20.60 | 25.90 | 24.18 | 24.17 | 24.17 |\n| ShareGPT4V-7B | Vicuna-7B | 24.15 | 20.60 | 26.10 | 24.18 | 23.88 | 23.76 |\n| MiniGPT-v2 | Llama 2-7B-Chat | 30.00 | 20.40 | 16.87 | 23.66 | 23.01 | 22.73 |\n| Qwen-VL-Chat | Qwen-7B | 28.60 | 13.60 | 16.47 | 24.63 | 21.95 | 20.83 |\n| TextMonkey | Qwen-7B | 30.40 | 2.20 | 4.42 | 20.01 | 15.96 | 14.26 |", "caption": "Table 8: Example of the Prompt Used for Augmenting Human Annotations.", "description": "\uc774 \ud45c\ub294 \uc778\uac04\uc758 \uc8fc\uc11d\uc744 \ubcf4\uac15\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub41c \ud504\ub86c\ud504\ud2b8\uc758 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc778\uac04 \ud3c9\uac00\uc790\ub294 \uc9c8\ubb38, \ubaa8\ub378 \uc751\ub2f5, \uadf8\ub9ac\uace0 \uc751\ub2f5\uc5d0 \ub300\ud55c \uc778\uac04 \uc804\ubb38\uac00\uc758 \uc758\uacac\uc744 \ubc1b\uc2b5\ub2c8\ub2e4. \uc774 \ud504\ub86c\ud504\ud2b8\ub294 \uc778\uac04\uc758 \uc758\uacac\uc744 \ub354\uc6b1 \uc790\uc138\ud558\uace0 \ub17c\ub9ac\uc801\uc73c\ub85c \ud655\uc7a5\ud558\uc5ec \ubaa8\ub378 \ud559\uc2b5\uc5d0 \ub354 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\uae30 \uc704\ud574 \uace0\uc548\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud504\ub86c\ud504\ud2b8\ub294 \uc9c8\ubb38, \ub2f5\ubcc0, \uadf8\ub9ac\uace0 \uc778\uac04 \ud3c9\uac00\uc790\uc758 \uc758\uacac\uc744 \ud3ec\ud568\ud558\uba70, \uc778\uac04 \ud3c9\uac00\uc790\ub294 \uc774\ub97c \ubc14\ud0d5\uc73c\ub85c \ub354\uc6b1 \uc790\uc138\ud558\uace0 \ub17c\ub9ac\uc801\uc778 \uc124\uba85\uc744 \ucd94\uac00\ud574\uc57c \ud569\ub2c8\ub2e4.  \ucd94\uac00\ub41c \uc124\uba85\uc740 \uae30\uc874 \uc758\uacac\uc5d0 \uae30\ubc18\ud574\uc57c \ud558\uba70, \ucd94\uce21\uc774\ub098 \ubd88\ud655\uc2e4\ud55c \ub0b4\uc6a9\uc744 \ud3ec\ud568\ud574\uc11c\ub294 \uc548 \ub429\ub2c8\ub2e4.", "section": "2.3. Fine-grained Human Annotation"}]