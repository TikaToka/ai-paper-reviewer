{"references": [{"fullname_first_author": "Guanzheng Chen", "paper_title": "CLEX: Continuous length extrapolation for large language models", "publication_date": "2024-00-00", "reason": "This paper proposes a method for continuously extrapolating the length of large language models, a problem directly addressed in the current paper."}, {"fullname_first_author": "Ta-Chung Chi", "paper_title": "Kerple: Kernelized relative positional embedding for length extrapolation", "publication_date": "2022-00-00", "reason": "This paper introduces a kernelized relative positional embedding method, which is related to the topic of improving length generalization in language models."}, {"fullname_first_author": "Krzysztof Choromanski", "paper_title": "Learning a Fourier transform for linear relative positional encodings in transformers", "publication_date": "2024-00-00", "reason": "This paper explores the use of Fourier transforms for relative positional encodings, providing a related theoretical background to the current work's frequency-domain analysis."}, {"fullname_first_author": "Ofir Press", "paper_title": "Train short, test long: Attention with linear biases enables input length extrapolation", "publication_date": "2021-00-00", "reason": "This is a seminal paper that introduced the concept of linear biases in attention for extrapolating input length, which is directly relevant to the current paper's focus on length generalization."}, {"fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "publication_date": "2024-00-00", "reason": "This paper enhanced the transformer model using rotary position embedding, a technique that the current paper builds upon and improves."}]}