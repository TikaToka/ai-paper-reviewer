{"references": [{"fullname_first_author": "Su, J.", "paper_title": "RoFormer: Enhanced Transformer with rotary position embedding", "publication_date": "2024-XX-XX", "reason": "This paper introduces Rotary Position Embedding (RoPE), a core concept that VideoRoPE builds upon and analyzes for video data."}, {"fullname_first_author": "Wang, P.", "paper_title": "Qwen2-VL: Enhancing vision-language model's perception of the world at any resolution", "publication_date": "2024-XX-XX", "reason": "This paper introduces M-ROPE, a previous state-of-the-art method for video position embedding, which serves as a baseline and comparison for VideoRoPE."}, {"fullname_first_author": "Zhang, Z.", "paper_title": "Vision Needle-in-a-Haystack", "publication_date": "2024-XX-XX", "reason": "This paper introduces the V-NIAH benchmark task, which is extended into V-NIAH-D for VideoRoPE's evaluation, a crucial part of the paper's analysis."}, {"fullname_first_author": "Wu, H.", "paper_title": "LongVideoBench: A benchmark for long-context interleaved video-language understanding", "publication_date": "2024-XX-XX", "reason": "This paper introduces the LongVideoBench benchmark dataset, one of the key datasets used to evaluate VideoRoPE's performance."}, {"fullname_first_author": "Zhou, J.", "paper_title": "MLVU: A comprehensive benchmark for multi-task long video understanding", "publication_date": "2024-XX-XX", "reason": "This paper introduces the MLVU benchmark dataset, another key dataset used to evaluate VideoRoPE's performance."}]}