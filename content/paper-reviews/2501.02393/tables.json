[{"content": "| Hyperparameter | Value |\n|---|---| \n| Learning Rate | 1e-4 |\n| Per Device Train Batch Size | 8 |\n| Per Device Eval Batch Size | 4 |\n| Gradient Accumulation Steps | 4 |\n| Number of Training Epochs | 9 |\n| Weight Decay | 0.01 |\n| Learning Rate Scheduler Type | Constant |\n| Warmup Steps | 250 |\n| Packing | False |\n| Max Gradient Norm | 1 |", "caption": "Table 1: Hyperparameters for model training (reference, GIN-Attention, PNA-Attention, and all variants), implemented in SFTTrainer that is part of the Hugging Face Transformer Reinforcement Learning (TRL) package (https://huggingface.co/docs/trl/en/index). Train and test loss is computed every 100 steps.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 \uc2e4\ud5d8 \uc124\uc815\uc5d0 \uc0ac\uc6a9\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'Reference' \ubaa8\ub378(\uae30\ubcf8 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378), GIN-Attention \ubaa8\ub378, PNA-Attention \ubaa8\ub378, \uadf8\ub9ac\uace0 \uac01 \ubaa8\ub378\uc758 \uc5ec\ub7ec \ubcc0\ud615\ub4e4\uc744 \ud6c8\ub828\uc2dc\ud0a4\ub294 \ub370 \uc0ac\uc6a9\ub41c \ud559\uc2b5\ub960, \ubc30\uce58 \ud06c\uae30, \uac00\uc911\uce58 \uac10\uc1e0, \ucd5c\ub300 \uadf8\ub798\ub514\uc5b8\ud2b8 \ub188 \ub4f1\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac12\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Hugging Face\uc758 TRL \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc774 \ud6c8\ub828\ub418\uc5c8\uc73c\uba70, \ud6c8\ub828 \ubc0f \ud14c\uc2a4\ud2b8 \uc190\uc2e4\uc740 100 \uc2a4\ud15d\ub9c8\ub2e4 \uacc4\uc0b0\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud45c\uc758 \ubaa9\uc801\uc740 \ub2e4\uc591\ud55c \ubaa8\ub378 \ubcc0\ud615\uc5d0 \uac78\uccd0 \uc77c\uad00\ub41c \uc2e4\ud5d8 \uc124\uc815\uc744 \uc81c\uacf5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4.", "section": "2.2 \uc2e4\ud5d8 \uacb0\uacfc: \uadf8\ub798\ud504 \uc778\uc2dd \uc5b4\ud150\uc158"}, {"content": "| Hyperparameter | Value |\n|---|---| \n| Learning Rate | 2e-4 |\n| Per Device Train Batch Size | 1 |\n| Per Device Eval Batch Size | 2 |\n| Gradient Accumulation Steps | 4 |\n| Number of Training Epochs | 3 |\n| Weight Decay | 0.01 |\n| Learning Rate Scheduler Type | Constant |\n| Warmup Steps | 50 |\n| Packing | False |\n| Max Gradient Norm | 0.5 |", "caption": "Table 2: Hyperparameters for LoRA and Sparse-GIN model training, implemented in SFTTrainer (https://huggingface.co/docs/trl/en/index).", "description": "\ubcf8 \ub17c\ubb38\uc758 \ud45c 2\ub294 Hugging Face\uc758 Transformer \uac15\ud654 \ud559\uc2b5(TRL) \ud328\ud0a4\uc9c0\uc758 SFTTrainer\ub97c \uc0ac\uc6a9\ud558\uc5ec LoRA\uc640 Sparse-GIN \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a4\ub294 \ub370 \uc0ac\uc6a9\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud559\uc2b5\ub960, \ubc30\uce58 \ud06c\uae30, \uac00\uc911\uce58 \uac10\uc1e0, \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904\ub7ec, \uc6cc\ubc0d\uc5c5 \ub2e8\uacc4 \ub4f1 \ub2e4\uc591\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac12\ub4e4\uc774 \uba85\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubaa8\ub378 \ud559\uc2b5 \uacfc\uc815\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\uc5ec \uc7ac\ud604\uc131\uc744 \ub192\uc774\uace0, \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ud574\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4. ", "section": "4.2 Sparse GIN fine-tuning model"}]