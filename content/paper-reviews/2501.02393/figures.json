[{"figure_path": "https://arxiv.org/html/2501.02393/x2.png", "caption": "Figure 1: Decoder-only transformer architecture (panel A), adapted here by using a GNN-based self-attention mechanism with a graph neural network (Figure\u00a03 shows how GNN-Attention is constructed for the specific case of GIN-Attention). Thereby Q\ud835\udc44Qitalic_Q and K\ud835\udc3eKitalic_K values are used to construct a per-head adjacency matrix, which is then used to define a causal graph. Whereas in standard transformer models the multiplication with V\ud835\udc49Vitalic_V corresponds to a summation aggregation via a single linear layer, in GNN-Attenion we conduct more complex graph operations, including the designation of a GIN and PNA variant. As another variant (panel B) suitable for fine-tuning a pre-trained model akin to a LoRA model, we introduce another option where we retain the adjacency matrix predicted by the pretrained model but instead use it to construct a sparse adjacency matrix. A Sparse GIN is defined based on this and the signal from the original attention mechanism and the GIN output is added, whereas the GIN signal is scaled by a trainable scale parameter. In this variant, the pre-trained Transformer architecture is kept intact except for the addition of the Sparse GIN block.", "description": "\uadf8\ub9bc 1\uc740 \uc81c\uc2dc\ub41c \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 GNN \uae30\ubc18 \uc790\uae30 \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uc744 \ud1b5\ud569\ud55c \ub514\ucf54\ub354 \uc804\uc6a9 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (A) \ud328\ub110\uc740 \uae30\uc874\uc758 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc8fc\uace0, GNN\uc774 \uc5b4\ub5bb\uac8c \uc790\uae30 \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uc5d0 \ud1b5\ud569\ub418\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Q\uc640 K \uac12\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac01 \ud5e4\ub4dc\uc5d0 \ub300\ud55c \uc778\uc811 \ud589\ub82c\uc744 \uad6c\uc131\ud558\uace0, \uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc778\uacfc \uadf8\ub798\ud504\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. \uae30\uc874\uc758 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\uc5d0\uc11c\ub294 V\uc640\uc758 \uacf1\uc148\uc774 \ub2e8\uc77c \uc120\ud615 \uacc4\uce35\uc744 \ud1b5\ud55c \ud569\uacc4 \uc9d1\uacc4\uc5d0 \ud574\ub2f9\ud558\uc9c0\ub9cc, GNN-Attention\uc5d0\uc11c\ub294 GIN\uacfc PNA \ubcc0\ud615\uc744 \ud3ec\ud568\ud55c \ub354 \ubcf5\uc7a1\ud55c \uadf8\ub798\ud504 \uc5f0\uc0b0\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. (B) \ud328\ub110\uc740 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \ubbf8\uc138 \uc870\uc815\uc5d0 \uc801\ud569\ud55c \ub610 \ub2e4\ub978 \ubcc0\ud615\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ubcc0\ud615\uc5d0\uc11c\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc5d0 \uc758\ud574 \uc608\uce21\ub41c \uc778\uc811 \ud589\ub82c\uc744 \uc720\uc9c0\ud558\uc9c0\ub9cc, \uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud76c\uc18c \uc778\uc811 \ud589\ub82c\uc744 \uad6c\uc131\ud569\ub2c8\ub2e4. \uc774\ub97c \uae30\ubc18\uc73c\ub85c \ud76c\uc18c GIN\uc774 \uc815\uc758\ub418\uace0, \uc6d0\ub798\uc758 \uc790\uae30 \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uc758 \uc2e0\ud638\uac00 \ucd94\uac00\ub418\uba70, GIN \uc2e0\ud638\ub294 \ud6c8\ub828 \uac00\ub2a5\ud55c \uc2a4\ucf00\uc77c \ub9e4\uac1c\ubcc0\uc218\uc5d0 \uc758\ud574 \uc870\uc815\ub429\ub2c8\ub2e4. \uc774 \ubcc0\ud615\uc5d0\uc11c\ub294 \ud76c\uc18c GIN \ube14\ub85d\uc744 \ucd94\uac00\ud558\ub294 \uac83\uc744 \uc81c\uc678\ud558\uace0\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc544\ud0a4\ud14d\ucc98\uac00 \uadf8\ub300\ub85c \uc720\uc9c0\ub429\ub2c8\ub2e4.", "section": "2.1 \uc774\ub860\uc801 \uae30\ubc18: \uadf8\ub798\ud504 \uc778\uc2dd \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998"}, {"figure_path": "https://arxiv.org/html/2501.02393/x3.png", "caption": "Figure 2: Visualization of adjacency matrices and interpretation of corresponding causal graphs. Panel A: Visual representation of an adjacency matrix for one specific layer and one head, extracted from a pretrained model. Panel B, left shows a large-scale adjacency matrix, where interaction strengths are color-coded, with annotations highlighting specific points of interest. Panel B, right displays the corresponding causal graph, illustrating directional relationships between nodes based on the adjacency matrix. These visualizations provide insights into the structural and causal relationships encoded in the adjacency matrices.", "description": "\uadf8\ub9bc 2\ub294 \uc778\uc811 \ud589\ub82c\uc758 \uc2dc\uac01\ud654\uc640 \ud574\ub2f9 \uc778\uacfc \uadf8\ub798\ud504\uc758 \ud574\uc11d\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. A \ud328\ub110\uc740 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc5d0\uc11c \ucd94\ucd9c\ud55c \ud2b9\uc815 \uacc4\uce35 \ubc0f \ud5e4\ub4dc\uc5d0 \ub300\ud55c \uc778\uc811 \ud589\ub82c\uc758 \uc2dc\uac01\uc801 \ud45c\ud604\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. B \ud328\ub110\uc758 \uc67c\ucabd\uc740 \uc0c1\ud638 \uc791\uc6a9 \uac15\ub3c4\uac00 \uc0c9\uc0c1\uc73c\ub85c \uad6c\ubd84\ub41c \ub300\uaddc\ubaa8 \uc778\uc811 \ud589\ub82c\uc744 \ubcf4\uc5ec\uc8fc\uba70, \ud2b9\uc815 \uad00\uc2ec \uc9c0\uc810\uc744 \uac15\uc870 \ud45c\uc2dc\ud558\ub294 \uc8fc\uc11d\uc774 \uc788\uc2b5\ub2c8\ub2e4. B \ud328\ub110\uc758 \uc624\ub978\ucabd\uc740 \uc778\uc811 \ud589\ub82c\uc744 \uae30\ubc18\uc73c\ub85c \ub178\ub4dc \uac04\uc758 \ubc29\ud5a5\uc131 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud574\ub2f9 \uc778\uacfc \uadf8\ub798\ud504\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc2dc\uac01\ud654\ub294 \uc778\uc811 \ud589\ub82c\uc5d0 \uc778\ucf54\ub529\ub41c \uad6c\uc870\uc801 \ubc0f \uc778\uacfc\uc801 \uad00\uacc4\uc5d0 \ub300\ud55c \ud1b5\ucc30\ub825\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "2.1 \uadf8\ub798\ud504 \uc778\uc2dd \uc5b4\ud150\uc158\uc758 \uc774\ub860\uc801 \ud1a0\ub300"}, {"figure_path": "https://arxiv.org/html/2501.02393/x4.png", "caption": "Figure 3: Construction of the GIN-Attention mechanism. The flowchart shows how input embeddings in the hidden states in each layer in the transformer via self-attention are used to construct the attention matrix. The output is processed further before aggregation and GIN-MLP application. The alternative PNA processing discussed in the paper is done in a conceptually similar way, except that we use query, key and value projections followed by developing up to four distinct aggregrations that are concatenated and then projected back into the hidden dimension via a MLP.", "description": "\uadf8\ub9bc 3\uc740 \uc81c\uc548\ub41c GIN-Attention \uba54\ucee4\ub2c8\uc998\uc758 \uad6c\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Transformer\uc758 \uac01 \uacc4\uce35\uc5d0\uc11c self-attention\uc744 \ud1b5\ud574 \uc0dd\uc131\ub41c attention matrix\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 Transformer\uc758 \uac01 \uce35\uc5d0\uc11c self-attention\uc744 \ud1b5\ud574 \uc5bb\uc5b4\uc9c4 \uc228\uaca8\uc9c4 \uc0c1\ud0dc(hidden states)\uc758 \uc785\ub825 \uc784\ubca0\ub529(input embeddings)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5b4\ub5bb\uac8c attention matrix\uac00 \uad6c\uc131\ub418\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \ud750\ub984\ub3c4\uc785\ub2c8\ub2e4.  attention matrix\uac00 \uacc4\uc0b0\ub41c \ud6c4, \ucd94\uac00\uc801\uc778 \ucc98\ub9ac(sharpening, thresholding \ub4f1)\ub97c \uac70\uccd0 GNN (Graph Neural Network) \uae30\ubc18\uc758 aggregation \uacfc\uc815\uc744 \uac70\uce58\uac8c \ub429\ub2c8\ub2e4.  GIN-MLP(Multi-Layer Perceptron)\ub97c \uc801\uc6a9\ud558\uae30 \uc804\uc5d0 \ucd94\uac00\uc801\uc778 \ucc98\ub9ac(aggregation) \uacfc\uc815\uc744 \uac70\uce69\ub2c8\ub2e4. \ub17c\ubb38\uc5d0\uc11c \uc124\uba85\ud558\ub294 \ub300\uccb4\uc801\uc778 PNA(Principal Neighborhood Aggregation) \ucc98\ub9ac \ubc29\uc2dd\ub3c4 \uac1c\ub150\uc801\uc73c\ub85c \uc720\uc0ac\ud558\uc9c0\ub9cc, query, key, value projection \uc774\ud6c4 \ucd5c\ub300 4\uac00\uc9c0\uc758 \uc11c\ub85c \ub2e4\ub978 aggregation \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uace0, \uc774\ub97c \uc5f0\uacb0\ud558\uc5ec MLP\ub97c \ud1b5\ud574 \ub2e4\uc2dc hidden dimension\uc73c\ub85c projection \ud55c\ub2e4\ub294 \ucc28\uc774\uac00 \uc788\uc2b5\ub2c8\ub2e4. ", "section": "2.1 \uc774\ub860\uc801 \ud1a0\ub300: \uadf8\ub798\ud504 \uc778\uc2dd \uc5b4\ud150\uc158"}, {"figure_path": "https://arxiv.org/html/2501.02393/x5.png", "caption": "Figure 4: Training and validation performance of the regular transformer model (identified as \u201cReference\u201d model)) and the GIN model. A, Training loss comparing the regular transformer and GIN model, over training epochs. B, Validation perplexity comparing the regular transformer and GIN model, over training epochs. C, Minimum validation loss measured across all epochs. The minimum validation loss is found in epoch 5 for the regular transformer model, and in epoch 8 for the GIN model.", "description": "\uadf8\ub9bc 4\ub294 \uae30\ubcf8 \ubcc0\uc555\uae30 \ubaa8\ub378( \"\uae30\uc900\" \ubaa8\ub378\ub85c \uc2dd\ubcc4\ub428)\uacfc GIN \ubaa8\ub378\uc758 \ud6c8\ub828 \ubc0f \uac80\uc99d \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (A)\ub294 \ud6c8\ub828 \uc5d0\ud3ec\ud06c\uc5d0 \ub530\ub978 \uae30\ubcf8 \ubcc0\uc555\uae30 \ubaa8\ub378\uacfc GIN \ubaa8\ub378\uc758 \ud6c8\ub828 \uc190\uc2e4\uc744 \ube44\uad50\ud55c \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. (B)\ub294 \ud6c8\ub828 \uc5d0\ud3ec\ud06c\uc5d0 \ub530\ub978 \uae30\ubcf8 \ubcc0\uc555\uae30 \ubaa8\ub378\uacfc GIN \ubaa8\ub378\uc758 \uac80\uc99d \ud37c\ud50c\ub809\uc11c\ud2f0\ub97c \ube44\uad50\ud55c \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. (C)\ub294 \ubaa8\ub4e0 \uc5d0\ud3ec\ud06c\uc5d0\uc11c \uce21\uc815\ub41c \ucd5c\uc18c \uac80\uc99d \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub9c9\ub300 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. \uae30\ubcf8 \ubcc0\uc555\uae30 \ubaa8\ub378\uc758 \ucd5c\uc18c \uac80\uc99d \uc190\uc2e4\uc740 5\ubc88\uc9f8 \uc5d0\ud3ec\ud06c\uc5d0\uc11c, GIN \ubaa8\ub378\uc758 \ucd5c\uc18c \uac80\uc99d \uc190\uc2e4\uc740 8\ubc88\uc9f8 \uc5d0\ud3ec\ud06c\uc5d0\uc11c \ubc1c\uacac\ub429\ub2c8\ub2e4.", "section": "2.2 \uc2e4\ud5d8 \uacb0\uacfc: \uadf8\ub798\ud504 \uc778\uc2dd \uc5b4\ud150\uc158"}, {"figure_path": "https://arxiv.org/html/2501.02393/x6.png", "caption": "Figure 5: \nThe distribution of the sharpening parameter \u03b1isubscript\ud835\udefc\ud835\udc56\\alpha_{i}italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT across all layers i\ud835\udc56iitalic_i in the GIN model at the end of training. The sharpening parameter \u03b1isubscript\ud835\udefc\ud835\udc56\\alpha_{i}italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT controls the focus of the attention mechanism by scaling the logits before applying the softmax function. A value of \u03b1i=1.0subscript\ud835\udefc\ud835\udc561.0\\alpha_{i}=1.0italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0 corresponds to the standard softmax behavior, where no additional sharpening or smoothing is applied. The variation of \u03b1isubscript\ud835\udefc\ud835\udc56\\alpha_{i}italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT indicates how different layers adjust their focus during training. Layers with \u03b1i>1.0subscript\ud835\udefc\ud835\udc561.0\\alpha_{i}>1.0italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > 1.0 exhibit sharper attention distributions, focusing more strongly on specific tokens, while layers with \u03b1i<1.0subscript\ud835\udefc\ud835\udc561.0\\alpha_{i}<1.0italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT < 1.0 produce smoother attention distributions, allowing a more even consideration of all tokens. This behavior reflects the adaptive nature of the GIN model in optimizing attention mechanisms for different layers to improve overall performance. All models are constructed to have approximately the same number of parameters, 25M.", "description": "\uadf8\ub9bc 5\ub294 GIN \ubaa8\ub378 \ud559\uc2b5 \uc885\ub8cc \uc2dc\uc810\uc5d0\uc11c \ubaa8\ub4e0 \ub808\uc774\uc5b4 i\uc5d0 \ub300\ud55c \uc120\uba85\ub3c4 \ub9e4\uac1c\ubcc0\uc218 \u03b1i\uc758 \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc120\uba85\ub3c4 \ub9e4\uac1c\ubcc0\uc218 \u03b1i\ub294 softmax \ud568\uc218\ub97c \uc801\uc6a9\ud558\uae30 \uc804\uc5d0 \ub85c\uc9d3\uc744 \uc870\uc815\ud558\uc5ec \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \ucd08\uc810\uc744 \uc81c\uc5b4\ud569\ub2c8\ub2e4. \u03b1i=1.0 \uac12\uc740 \ucd94\uac00\uc801\uc778 \uc120\uba85\ud654 \ub610\ub294 \uc2a4\ubb34\ub529 \uc5c6\uc774 \ud45c\uc900 softmax \ub3d9\uc791\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \u03b1i\uc758 \ubcc0\ud654\ub294 \ud559\uc2b5 \uc911\uc5d0 \uc11c\ub85c \ub2e4\ub978 \ub808\uc774\uc5b4\uac00 \ucd08\uc810\uc744 \uc5b4\ub5bb\uac8c \uc870\uc815\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \u03b1i>1.0\uc778 \ub808\uc774\uc5b4\ub294 \ubcf4\ub2e4 \ub0a0\uce74\ub85c\uc6b4 \uc5b4\ud150\uc158 \ubd84\ud3ec\ub97c \ub098\ud0c0\ub0b4\uc5b4 \ud2b9\uc815 \ud1a0\ud070\uc5d0 \ub354 \uc9d1\uc911\ud558\ub294 \ubc18\uba74, \u03b1i<1.0\uc778 \ub808\uc774\uc5b4\ub294 \ubcf4\ub2e4 \ubd80\ub4dc\ub7ec\uc6b4 \uc5b4\ud150\uc158 \ubd84\ud3ec\ub97c \uc0dd\uc131\ud558\uc5ec \ubaa8\ub4e0 \ud1a0\ud070\uc744 \ubcf4\ub2e4 \uace0\ub974\uac8c \uace0\ub824\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ub3d9\uc791\uc740 \ub2e4\uc591\ud55c \ub808\uc774\uc5b4\uc5d0 \ub300\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \ucd5c\uc801\ud654\ud558\uc5ec \uc804\ubc18\uc801\uc778 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 GIN \ubaa8\ub378\uc758 \uc801\uc751\uc801 \ud2b9\uc131\uc744 \ubc18\uc601\ud569\ub2c8\ub2e4. \ubaa8\ub4e0 \ubaa8\ub378\uc740 \uc57d 25M\uac1c\uc758 \ub3d9\uc77c\ud55c \ub9e4\uac1c\ubcc0\uc218 \uc218\ub97c \uac00\uc9c0\ub3c4\ub85d \uad6c\uc131\ub429\ub2c8\ub2e4.", "section": "2.1 \uadf8\ub798\ud504 \uc778\uc2dd \uc5b4\ud150\uc158"}, {"figure_path": "https://arxiv.org/html/2501.02393/x7.png", "caption": "Figure 6: Minimum training loss and minimum validation perplexity measured, across a variety of cases. The case identified as \u201cReference\u201d is a regular transformer architecture. Cases considered include PNA attention, GIN attention, and variations within each scenario. The best performing model with lowest validation perplexity is GIN attention with softmax and a MLP multiplier \u03b3\ud835\udefe\\gammaitalic_\u03b3 of 0.5, with trainable sharpening parameter. Except for one case, all GIN model architectures perform better than the reference standard attention. None of the PNA architectures improves upon the reference case, suggesting that this architectural concept is not viable.", "description": "\uadf8\ub9bc 6\uc740 \ub2e4\uc591\ud55c \uc2e4\ud5d8 \uacb0\uacfc\uc5d0\uc11c \uce21\uc815\ub41c \ucd5c\uc18c \ud6c8\ub828 \uc190\uc2e4\uacfc \ucd5c\uc18c \uac80\uc99d \ub2f9\ud669\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \"\uae30\uc900\"\uc73c\ub85c \uc2dd\ubcc4\ub41c \uacbd\uc6b0\ub294 \uc77c\ubc18\uc801\uc778 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc544\ud0a4\ud14d\ucc98\uc785\ub2c8\ub2e4. \uace0\ub824\ub41c \uacbd\uc6b0\uc5d0\ub294 PNA \uc5b4\ud150\uc158, GIN \uc5b4\ud150\uc158 \ubc0f \uac01 \uc2dc\ub098\ub9ac\uc624 \ub0b4\uc758 \ubcc0\ud615\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4. \uac00\uc7a5 \ub0ae\uc740 \uac80\uc99d \ub2f9\ud669\ub3c4\ub97c \uac00\uc9c4 \ucd5c\uace0 \uc131\ub2a5 \ubaa8\ub378\uc740 \uc18c\ud504\ud2b8\ub9e5\uc2a4\uc640 MLP \uc2b9\uc218 \u03b3\uac00 0.5\uc778 GIN \uc5b4\ud150\uc158\uc774\uba70, \ud6c8\ub828 \uac00\ub2a5\ud55c \uc120\uba85\ud654 \ub9e4\uac1c\ubcc0\uc218\uac00 \uc788\uc2b5\ub2c8\ub2e4. \ud55c \uacbd\uc6b0\ub97c \uc81c\uc678\ud558\uace0 \ubaa8\ub4e0 GIN \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\ub294 \uae30\uc900 \ud45c\uc900 \uc5b4\ud150\uc158\ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc6b0\uc218\ud569\ub2c8\ub2e4. \uc5b4\ub5a4 PNA \uc544\ud0a4\ud14d\ucc98\ub3c4 \uae30\uc900 \uc0ac\ub840\ub97c \uac1c\uc120\ud558\uc9c0 \ubabb\ud558\uc5ec \uc774 \uc544\ud0a4\ud14d\ucc98 \uac1c\ub150\uc740 \uc2e4\ud604 \uac00\ub2a5\ud558\uc9c0 \uc54a\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "2.2 \uc2e4\ud5d8 \uacb0\uacfc: \uadf8\ub798\ud504 \uc778\uc2dd \uc5b4\ud150\uc158"}, {"figure_path": "https://arxiv.org/html/2501.02393/x8.png", "caption": "Figure 7: Further training dynamics analysis. Panel A: Generalization gap for selected cases that perform well overall, measured after 9 training epochs. The reference model shows the highest generalization gap, indicating overfitting. Models using GIN (Graph Isomorphism Network) with Softmax and varying MLP multipliers demonstrate reduced generalization gaps, with the GIN configuration using a multiplier \u03b3\ud835\udefe\\gammaitalic_\u03b3 of 0.5 and sharpening achieving without o_proj one of the lowest gaps. The PNA configuration with SharpSoftplus activation and a fixed threshold also exhibits improved generalization compared to the reference. This comparison highlights the effect of architectural choices on model generalization. Panel B: Ratio of lowest training loss to lowest validation loss achieved. The lowest ratio is also found for the GIN model using a multiplier \u03b3\ud835\udefe\\gammaitalic_\u03b3 of 0.5 and sharpening achieving without o_proj.", "description": "\uadf8\ub9bc 7\uc740 9\ubc88\uc758 \ud559\uc2b5 \uc5d0\ud3ec\ud06c \ud6c4 \uce21\uc815\ub41c \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \uc131\ub2a5\uacfc \ud6c8\ub828 \ud6a8\uc728\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Panel A\ub294 \uc5ec\ub7ec \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\uc900 \ubaa8\ub378(Reference)\uc740 \uacfc\uc801\ud569\uc73c\ub85c \uc778\ud574 \uac00\uc7a5 \ud070 \uc77c\ubc18\ud654 \ucc28\uc774\ub97c \ubcf4\uc785\ub2c8\ub2e4. Softmax\uc640 \ub2e4\uc591\ud55c MLP \uc2b9\uc218\ub97c \uc0ac\uc6a9\ud558\ub294 GIN(\uadf8\ub798\ud504 \ub3d9\ud615 \ub124\ud2b8\uc6cc\ud06c) \ubaa8\ub378\uc740 \uc77c\ubc18\ud654 \ucc28\uc774\uac00 \uac10\uc18c\ud588\uc2b5\ub2c8\ub2e4. \ud2b9\ud788 \uc2b9\uc218 \u03b3\uac00 0.5\uc774\uace0 o_proj\uac00 \uc5c6\ub294 GIN \ubaa8\ub378\uc740 \uac00\uc7a5 \ub0ae\uc740 \uc77c\ubc18\ud654 \ucc28\uc774\ub97c \ubcf4\uc600\uc2b5\ub2c8\ub2e4. SharpSoftplus \ud65c\uc131\ud654 \ud568\uc218\uc640 \uace0\uc815 \uc784\uacc4\uac12\uc744 \uc0ac\uc6a9\ud558\ub294 PNA \ubaa8\ub378 \ub610\ud55c \uae30\uc900 \ubaa8\ub378\ubcf4\ub2e4 \uc77c\ubc18\ud654 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378 \uad6c\uc870 \uc120\ud0dd\uc774 \uc77c\ubc18\ud654 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Panel B\ub294 \uac00\uc7a5 \ub0ae\uc740 \ud6c8\ub828 \uc190\uc2e4\uacfc \uac80\uc99d \uc190\uc2e4\uc758 \ube44\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc2b9\uc218 \u03b3\uac00 0.5\uc774\uace0 o_proj\uac00 \uc5c6\ub294 GIN \ubaa8\ub378\uc5d0\uc11c \uac00\uc7a5 \ub0ae\uc740 \ube44\uc728\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4.", "section": "2.2 \uc2e4\ud5d8 \uacb0\uacfc: \uadf8\ub798\ud504 \uc778\uc2dd \uc5b4\ud150\uc158"}, {"figure_path": "https://arxiv.org/html/2501.02393/x9.png", "caption": "Figure 8: Minimum validation perplexity as a function of the GIN MLP multiplier ratio \u03b3\ud835\udefe\\gammaitalic_\u03b3. The plot demonstrates the relationship between the GIN MLP multiplied \u03b3\ud835\udefe\\gammaitalic_\u03b3 and validation perplexity for various configurations: MLP_mult=0.5, sharpening, MLP_mult=0.5, MLP_mult=1, and MLP_mult=4. The data points are fitted with a power law trend line, indicating an increase in validation perplexity as the GIN MLP multiplier ratio grows. Configurations with lower MLP ratios (e.g., MLP_mult=0.5) exhibit better validation perplexity, suggesting a trade-off between multiplier ratio and generalization.", "description": "\uadf8\ub9bc 8\uc740 GIN MLP \uc2b9\uc218 \ube44\uc728(\u03b3)\uc758 \ud568\uc218\ub85c\uc11c \ucd5c\uc18c \uac80\uc99d \ub2f9\ud669\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub798\ud504\ub294 \ub2e4\uc591\ud55c \uad6c\uc131(MLP_mult=0.5, sharpening, MLP_mult=0.5, MLP_mult=1, MLP_mult=4)\uc5d0 \ub300\ud574 GIN MLP \uc2b9\uc218 \u03b3\uc640 \uac80\uc99d \ub2f9\ud669\ub3c4 \uac04\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub370\uc774\ud130 \uc810\uc740 \uc5ed \uc81c\uacf1 \ubc95\uce59 \ucd94\uc138\uc120\uc73c\ub85c \uc801\ud569\ub418\uc5b4 GIN MLP \uc2b9\uc218 \ube44\uc728\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uac80\uc99d \ub2f9\ud669\ub3c4\uac00 \uc99d\uac00\ud568\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ub0ae\uc740 MLP \ube44\uc728(\uc608: MLP_mult=0.5)\uc744 \uac00\uc9c4 \uad6c\uc131\uc740 \ub354 \ub098\uc740 \uac80\uc99d \ub2f9\ud669\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\uc5b4 \uc2b9\uc218 \ube44\uc728\uacfc \uc77c\ubc18\ud654 \uac04\uc758 \uc0c1\ucda9 \uad00\uacc4\ub97c \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "2.2 \uc2e4\ud5d8 \uacb0\uacfc: \uadf8\ub798\ud504 \uc778\uc2dd \uc5b4\ud150\uc158"}, {"figure_path": "https://arxiv.org/html/2501.02393/x10.png", "caption": "Figure 9: Performance of LoRA fine-tuning (panel A) and sparse GIN fine-tuning. In sparse GIN fine-tuning, we interpret the attention matrix computed by the pre-trained model as an adjacency matrix. Here, we sum attention matrices across all heads and clamp at 1.0, and then use it as an input to a GIN model. Only adjancy matrix values above a threshold of 0.2 are considered, introducing a sparseness and computational efficiency. Both LoRA and sparse GIN feature the same number of trainable parameters. Panel A: Training loss over epochs for LoRA and sparse GIN. Sparse GIN demonstrates faster convergence and lower final training loss compared to LoRA, indicating improved optimization efficiency. Panel B: Validation perplexity over epochs for LoRA and sparse GIN. Sparse GIN achieves lower perplexity across all epochs, suggesting better generalization to unseen data.", "description": "\uadf8\ub9bc 9\ub294 LoRA \ubbf8\uc138 \uc870\uc815\uacfc \ud76c\uc18c GIN \ubbf8\uc138 \uc870\uc815\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \ud76c\uc18c GIN \ubbf8\uc138 \uc870\uc815\uc5d0\uc11c\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc5d0\uc11c \uacc4\uc0b0\ub41c \uc5b4\ud150\uc158 \ud589\ub82c\uc744 \uc778\uc811 \ud589\ub82c\ub85c \ud574\uc11d\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc11c \ubaa8\ub4e0 \ud5e4\ub4dc\uc5d0 \uac78\uccd0 \uc5b4\ud150\uc158 \ud589\ub82c\uc744 \ud569\uc0b0\ud558\uace0 1.0\uc73c\ub85c \ud074\ub7a8\ud551\ud55c \ud6c4, \uc774\ub97c GIN \ubaa8\ub378\uc758 \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. 0.2\ubcf4\ub2e4 \ud070 \uc778\uc811 \ud589\ub82c \uac12\ub9cc \uace0\ub824\ud558\uc5ec, \ud76c\uc18c\uc131\uacfc \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \ub192\uc600\uc2b5\ub2c8\ub2e4. LoRA\uc640 \ud76c\uc18c GIN\uc740 \ub3d9\uc77c\ud55c \uc218\uc758 \ud6c8\ub828 \uac00\ub2a5\ud55c \ub9e4\uac1c\ubcc0\uc218\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ud328\ub110 A\ub294 LoRA\uc640 \ud76c\uc18c GIN\uc5d0 \ub300\ud55c \uc5d0\ud3ec\ud06c\ub2f9 \ud6c8\ub828 \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud76c\uc18c GIN\uc740 LoRA\ubcf4\ub2e4 \ub354 \ube60\ub978 \uc218\ub834\uacfc \ub0ae\uc740 \ucd5c\uc885 \ud6c8\ub828 \uc190\uc2e4\uc744 \ubcf4\uc5ec\uc8fc\uc5b4, \ud5a5\uc0c1\ub41c \ucd5c\uc801\ud654 \ud6a8\uc728\uc131\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud328\ub110 B\ub294 LoRA\uc640 \ud76c\uc18c GIN\uc5d0 \ub300\ud55c \uc5d0\ud3ec\ud06c\ub2f9 \uac80\uc99d \ud37c\ud50c\ub809\uc11c\ud2f0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud76c\uc18c GIN\uc740 \ubaa8\ub4e0 \uc5d0\ud3ec\ud06c\uc5d0\uc11c \ub354 \ub0ae\uc740 \ud37c\ud50c\ub809\uc11c\ud2f0\ub97c \ub2ec\uc131\ud558\uc5ec, \ubcf4\uc774\uc9c0 \uc54a\ub294 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \ub354 \ub098\uc740 \uc77c\ubc18\ud654\ub97c \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "2.2 \uc2e4\ud5d8 \uacb0\uacfc: \uadf8\ub798\ud504 \uc778\uc2dd \uc5b4\ud150\uc158"}, {"figure_path": "https://arxiv.org/html/2501.02393/x11.png", "caption": "Figure 10: Trainable scale parameter \u03bb\ud835\udf06\\lambdaitalic_\u03bb over all k\ud835\udc58kitalic_k layers in the model, plotted over all epochs. The trainable scale parameter delineates the relative importance of the sparse GIN as it is added to the original signal. The plot illustrates how the scale parameter evolves over both the layer index and the epoch fraction. Early in training, higher layers exhibit stronger scaling values, indicating a higher reliance on sparse GIN adjustments. As training progresses, the scaling values stabilize, suggesting convergence in the relative importance of the sparse GIN contributions across layers. The color gradient reflects the magnitude of the scale parameter, with warmer colors (red) indicating higher values and cooler colors (blue) indicating lower values. This visualization provides insights into the adaptive behavior of the trainable scale parameter over the course of training.", "description": "\uadf8\ub9bc 10\uc740 \ubaa8\ub378\uc758 \ubaa8\ub4e0 k \uacc4\uce35\uc5d0 \uac78\uccd0 \ubaa8\ub4e0 \uc5d0\ud3ec\ud06c\uc5d0 \uac78\uccd0 \uadf8\ub824\uc9c4 \ud559\uc2b5 \uac00\ub2a5\ud55c \uc2a4\ucf00\uc77c \ub9e4\uac1c\ubcc0\uc218 \u03bb\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud559\uc2b5 \uac00\ub2a5\ud55c \uc2a4\ucf00\uc77c \ub9e4\uac1c\ubcc0\uc218\ub294 \uc6d0\ub798 \uc2e0\ud638\uc5d0 \ucd94\uac00\ub41c \ud76c\uc18c GIN\uc758 \uc0c1\ub300\uc801 \uc911\uc694\uc131\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud50c\ub86f\uc740 \uc2a4\ucf00\uc77c \ub9e4\uac1c\ubcc0\uc218\uac00 \uacc4\uce35 \uc0c9\uc778\uacfc \uc5d0\ud3ec\ud06c \ube44\uc728 \ubaa8\ub450\uc5d0 \ub530\ub77c \uc5b4\ub5bb\uac8c \ubcc0\ud654\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud559\uc2b5 \ucd08\uae30\uc5d0\ub294 \uc0c1\uc704 \uacc4\uce35\uc774 \ub354 \uac15\ud55c \uc2a4\ucf00\uc77c \uac12\uc744 \ub098\ud0c0\ub0b4\uc5b4 \ud76c\uc18c GIN \uc870\uc815\uc5d0 \ub354 \ub9ce\uc774 \uc758\uc874\ud568\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud559\uc2b5\uc774 \uc9c4\ud589\ub428\uc5d0 \ub530\ub77c \uc2a4\ucf00\uc77c \uac12\uc774 \uc548\uc815\ud654\ub418\uc5b4 \uacc4\uce35 \uc804\uccb4\uc5d0\uc11c \ud76c\uc18c GIN \uae30\uc5ec\uc758 \uc0c1\ub300\uc801 \uc911\uc694\uc131\uc774 \uc218\ub834\ub428\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4. \uc0c9\uc0c1 \uadf8\ub77c\ub514\uc5b8\ud2b8\ub294 \uc2a4\ucf00\uc77c \ub9e4\uac1c\ubcc0\uc218\uc758 \ud06c\uae30\ub97c \ubc18\uc601\ud558\uba70, \ub530\ub73b\ud55c \uc0c9\uc0c1(\ube68\uac04\uc0c9)\uc740 \ub354 \ub192\uc740 \uac12\uc744, \ucc28\uac00\uc6b4 \uc0c9\uc0c1(\ud30c\ub780\uc0c9)\uc740 \ub354 \ub0ae\uc740 \uac12\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \uc2dc\uac01\ud654\ub294 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \ud559\uc2b5 \uac00\ub2a5\ud55c \uc2a4\ucf00\uc77c \ub9e4\uac1c\ubcc0\uc218\uc758 \uc801\uc751\uc801 \ub3d9\uc791\uc5d0 \ub300\ud55c \ud1b5\ucc30\ub825\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "2.3 Sparse graph-aware attention as a fine-tuning strategy"}, {"figure_path": "https://arxiv.org/html/2501.02393/x12.png", "caption": "Figure 11: Global dynamics of the trainable scale parameter \u03bb\ud835\udf06\\lambdaitalic_\u03bb during training and across model layers k\ud835\udc58kitalic_k.\nPanel A visualizes the average trainable scale parameter over training steps. The plot illustrates a rapid decline in the average scale parameter during the initial stages of training, indicating early adaptation of the sparse GIN contributions. After the initial drop, the scale stabilizes and gradually increases slightly, suggesting the model fine-tunes the integration of sparse GIN as training progresses. Panel B displays the trainable scale parameter for each layer at the last epoch. The scale parameter exhibits an increasing trend from lower to higher layers, reflecting the progressively stronger reliance on sparse GIN in deeper layers of the model. This layer-wise scaling suggests that deeper layers benefit more from the structural adjustments provided by sparse GIN.", "description": "\uadf8\ub9bc 11\uc740 \ud6c8\ub828 \uc911 \ubc0f \ubaa8\ub378 \uacc4\uce35 k\uc5d0 \ub530\ub978 \ud6c8\ub828 \uac00\ub2a5\ud55c \uc2a4\ucf00\uc77c \ub9e4\uac1c\ubcc0\uc218 \u03bb\uc758 \uc804\uc5ed \ub3d9\uc5ed\ud559\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud328\ub110 A\ub294 \ud6c8\ub828 \ub2e8\uacc4\uc5d0 \ub530\ub978 \ud3c9\uade0 \ud6c8\ub828 \uac00\ub2a5\ud55c \uc2a4\ucf00\uc77c \ub9e4\uac1c\ubcc0\uc218\ub97c \uc2dc\uac01\ud654\ud569\ub2c8\ub2e4. \uc774 \ud50c\ub86f\uc740 \ud6c8\ub828 \ucd08\uae30 \ub2e8\uacc4\uc5d0\uc11c \ud3c9\uade0 \uc2a4\ucf00\uc77c \ub9e4\uac1c\ubcc0\uc218\uac00 \ube60\ub974\uac8c \uac10\uc18c\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\uba70, \uc774\ub294 \uc2a4\ud30c\uc2a4 GIN \uae30\uc5ec\ub3c4\uc758 \ucd08\uae30 \uc801\uc751\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ucd08\uae30 \uac10\uc18c \ud6c4 \uc2a4\ucf00\uc77c\uc740 \uc548\uc815\ud654\ub418\uace0 \uc810\uc9c4\uc801\uc73c\ub85c \uc57d\uac04 \uc99d\uac00\ud558\uc5ec \ubaa8\ub378\uc774 \ud6c8\ub828\uc774 \uc9c4\ud589\ub428\uc5d0 \ub530\ub77c \uc2a4\ud30c\uc2a4 GIN \ud1b5\ud569\uc744 \ubbf8\uc138 \uc870\uc815\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4. \ud328\ub110 B\ub294 \ub9c8\uc9c0\ub9c9 \uc5d0\ud3ec\ud06c\uc758 \uac01 \uacc4\uce35\uc5d0 \ub300\ud55c \ud6c8\ub828 \uac00\ub2a5\ud55c \uc2a4\ucf00\uc77c \ub9e4\uac1c\ubcc0\uc218\ub97c \ud45c\uc2dc\ud569\ub2c8\ub2e4. \uc2a4\ucf00\uc77c \ub9e4\uac1c\ubcc0\uc218\ub294 \ud558\uc704 \uacc4\uce35\uc5d0\uc11c \uc0c1\uc704 \uacc4\uce35\uc73c\ub85c \uc99d\uac00\ud558\ub294 \ucd94\uc138\ub97c \ubcf4\uc774\uba70, \ubaa8\ub378\uc758 \uc0c1\uc704 \uacc4\uce35\uc5d0\uc11c \uc2a4\ud30c\uc2a4 GIN\uc5d0 \ub300\ud55c \uc810\uc9c4\uc801\uc73c\ub85c \uac15\ud55c \uc758\uc874\uc131\uc744 \ubc18\uc601\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacc4\uce35\ubcc4 \uc2a4\ucf00\uc77c\ub9c1\uc740 \uc2a4\ud30c\uc2a4 GIN\uc758 \uad6c\uc870\uc801 \uc870\uc815\uc73c\ub85c\ubd80\ud130 \uc0c1\uc704 \uacc4\uce35\uc774 \ub354 \ub9ce\uc740 \uc774\uc810\uc744 \uc5bb\ub294\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "2.3 Sparse graph-aware attention as a fine-tuning strategy"}, {"figure_path": "https://arxiv.org/html/2501.02393/x13.png", "caption": "Figure 12: Validation perplexity comparison between different model configurations. Panel A: The bar plot illustrates the validation perplexity values for GIN, GIN with fixed \u03bb\ud835\udf06\\lambdaitalic_\u03bb, GIN with fixed \u03bb\ud835\udf06\\lambdaitalic_\u03bb and a smaller GNN, and LoRA. Measured values and error bars represent the standard deviation of the measured perplexity in the last training epoch. GIN achieves the lowest perplexity, while LoRA exhibits the highest perplexity. Panel B shows the trainable scale parameter \u03bb\ud835\udf06\\lambdaitalic_\u03bb. Similar to the earlier results, the parameter is found to ultimately be smallest in earlier layers, and largest in deep layers.", "description": "\uadf8\ub9bc 12\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378 \uad6c\uc131\uc5d0 \ub530\ub978 \uac80\uc99d \ud37c\ud50c\ub809\uc11c\ud2f0 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (A) \ub9c9\ub300 \uadf8\ub798\ud504\ub294 GIN, \uace0\uc815\ub41c \u03bb\ub97c \uc0ac\uc6a9\ud55c GIN, \uace0\uc815\ub41c \u03bb\uc640 \ub354 \uc791\uc740 GNN\uc744 \uc0ac\uc6a9\ud55c GIN, \uadf8\ub9ac\uace0 LoRA\uc5d0 \ub300\ud55c \uac80\uc99d \ud37c\ud50c\ub809\uc11c\ud2f0 \uac12\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uce21\uc815\ub41c \uac12\uacfc \uc624\ucc28 \ub9c9\ub300\ub294 \ub9c8\uc9c0\ub9c9 \ud559\uc2b5 \uc5d0\ud3ec\ud06c\uc5d0\uc11c \uce21\uc815\ub41c \ud37c\ud50c\ub809\uc11c\ud2f0\uc758 \ud45c\uc900 \ud3b8\ucc28\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. GIN\uc774 \uac00\uc7a5 \ub0ae\uc740 \ud37c\ud50c\ub809\uc11c\ud2f0\ub97c \ub2ec\uc131\ud55c \ubc18\uba74, LoRA\ub294 \uac00\uc7a5 \ub192\uc740 \ud37c\ud50c\ub809\uc11c\ud2f0\ub97c \ubcf4\uc600\uc2b5\ub2c8\ub2e4. (B) \ud559\uc2b5 \uac00\ub2a5\ud55c \ud06c\uae30 \ub9e4\uac1c\ubcc0\uc218 \u03bb\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\uc804 \uacb0\uacfc\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c, \uc774 \ub9e4\uac1c\ubcc0\uc218\ub294 \ucd5c\uc885\uc801\uc73c\ub85c \ucd08\uae30 \ub808\uc774\uc5b4\uc5d0\uc11c\ub294 \uac00\uc7a5 \uc791\uace0 \uae4a\uc740 \ub808\uc774\uc5b4\uc5d0\uc11c\ub294 \uac00\uc7a5 \ud070 \uac12\uc744 \uac16\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4.", "section": "2.2 \uc2e4\ud5d8 \uacb0\uacfc: \uadf8\ub798\ud504 \uc778\uc2dd \uc5b4\ud150\uc158"}, {"figure_path": "https://arxiv.org/html/2501.02393/x14.png", "caption": "Figure 13: Dynamic Graph Representation Learning with GIN-Attention in Transformers\nThis schematic illustrates the iterative process of GIN-Attention in a Transformer architecture, applied to material microstructures, here conceptually shown for a model with two layers and two heads. Starting with raw microstructural data (e.g., proteins or polymers), an initial graph representation is constructed. At each layer, multiple attention heads dynamically build and refine graph structures by updating adjacency matrices based on learned attention scores. The outputs of all heads are merged to produce updated graph representations, which are iteratively refined across layers. The final learned representation integrates structural and relational insights, enabling the model to predict material properties, uncover structure-property relationships, and design novel materials. This framework highlights the simultaneous graph construction and feature learning facilitated by GIN-Attention.", "description": " \uadf8\ub9bc 13\uc740 Transformer \uad6c\uc870\uc5d0\uc11c GIN-Attention\uc758 \ubc18\ubcf5\uc801\uc778 \uacfc\uc815\uc744 \uac1c\ub150\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub450 \uac1c\uc758 \ub808\uc774\uc5b4\uc640 \ub450 \uac1c\uc758 \ud5e4\ub4dc\ub97c \uac00\uc9c4 \ubaa8\ub378\uc744 \uc608\uc2dc\ub85c \uc0ac\uc6a9\ud558\uc5ec \uc7ac\ub8cc\uc758 \ubbf8\uc138\uad6c\uc870\uc5d0 \uc801\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \ubbf8\uc138\uad6c\uc870 \ub370\uc774\ud130(\uc608: \ub2e8\ubc31\uc9c8 \ub610\ub294 \ud3f4\ub9ac\uba38)\ub85c \uc2dc\uc791\ud558\uc5ec \ucd08\uae30 \uadf8\ub798\ud504 \ud45c\ud604\uc744 \uad6c\uc131\ud569\ub2c8\ub2e4. \uac01 \ub808\uc774\uc5b4\uc5d0\uc11c \uc5ec\ub7ec \uc5b4\ud150\uc158 \ud5e4\ub4dc\ub294 \ud559\uc2b5\ub41c \uc5b4\ud150\uc158 \uc810\uc218\ub97c \uae30\ubc18\uc73c\ub85c \uc778\uc811 \ud589\ub82c\uc744 \uc5c5\ub370\uc774\ud2b8\ud558\uc5ec \ub3d9\uc801\uc73c\ub85c \uadf8\ub798\ud504 \uad6c\uc870\ub97c \uad6c\ucd95\ud558\uace0 \uac1c\uc120\ud569\ub2c8\ub2e4. \ubaa8\ub4e0 \ud5e4\ub4dc\uc758 \ucd9c\ub825\uc740 \uacb0\ud569\ub418\uc5b4 \uc5c5\ub370\uc774\ud2b8\ub41c \uadf8\ub798\ud504 \ud45c\ud604\uc744 \uc0dd\uc131\ud558\uace0, \uc774\ub294 \ub808\uc774\uc5b4\ub97c \uac70\uce58\uba74\uc11c \ubc18\ubcf5\uc801\uc73c\ub85c \uac1c\uc120\ub429\ub2c8\ub2e4. \ucd5c\uc885 \ud559\uc2b5\ub41c \ud45c\ud604\uc740 \uad6c\uc870\uc801 \ubc0f \uad00\uacc4\uc801 \ud1b5\ucc30\ub825\uc744 \ud1b5\ud569\ud558\uc5ec \uc7ac\ub8cc \ud2b9\uc131\uc744 \uc608\uce21\ud558\uace0, \uad6c\uc870-\ud2b9\uc131 \uad00\uacc4\ub97c \ubc1d\ud788\uba70, \uc0c8\ub85c\uc6b4 \uc7ac\ub8cc\ub97c \uc124\uacc4\ud560 \uc218 \uc788\uac8c \ud569\ub2c8\ub2e4. \uc774 \ud504\ub808\uc784\uc6cc\ud06c\ub294 GIN-Attention\uc774 \ub3d9\uc2dc\uc5d0 \uadf8\ub798\ud504 \uad6c\uc131\uacfc \ud2b9\uc9d5 \ud559\uc2b5\uc744 \uac00\ub2a5\ud558\uac8c \ud568\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "2 Results and Discussion"}]