[{"figure_path": "https://arxiv.org/html/2501.11325/x2.png", "caption": "Figure 1: Examples of CatV2TON\u2019s unified virtual try-on capabilities, demonstrating high-quality garment consistency across both image-based and video-based try-on tasks, including dynamic long-video scenarios.", "description": "\uadf8\ub9bc 1\uc740 CatV2TON\uc758 \ud1b5\ud569\ub41c \uac00\uc0c1 \ud53c\ud305 \uae30\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ubbf8\uc9c0 \uae30\ubc18\uacfc \ube44\ub514\uc624 \uae30\ubc18 \ud53c\ud305 \uc791\uc5c5 \ubaa8\ub450\uc5d0\uc11c \uace0\ud488\uc9c8 \uc758\ub958 \uc77c\uad00\uc131\uc744 \ubcf4\uc5ec\uc8fc\uba70, \ub3d9\uc801\uc778 \uae34 \ube44\ub514\uc624 \uc2dc\ub098\ub9ac\uc624\ub3c4 \ud3ec\ud568\ud569\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc758\ub958\uc640 \uc790\uc138\uc5d0 \uac78\uccd0 \uc790\uc5f0\uc2a4\ub7fd\uace0 \ud604\uc2e4\uac10 \uc788\ub294 \uac00\uc0c1 \ud53c\ud305 \uacb0\uacfc\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ubbf8\uc9c0 \uae30\ubc18 \ud53c\ud305\uc5d0\uc11c\ub294 \uc815\uc9c0\ub41c \uc774\ubbf8\uc9c0\uc5d0 \uc758\ub958\ub97c \uc785\ud78c \ubaa8\uc2b5\uc744, \ube44\ub514\uc624 \uae30\ubc18 \ud53c\ud305\uc5d0\uc11c\ub294 \ub3d9\uc791\uc774 \uc788\ub294 \ube44\ub514\uc624\uc5d0 \uc758\ub958\ub97c \uc785\ud78c \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 \uae34 \ube44\ub514\uc624\uc5d0\uc11c\uc758 \uc758\ub958 \uc77c\uad00\uc131 \uc720\uc9c0\uac00 \uc911\uc694\ud55c \ubd80\ubd84\uc785\ub2c8\ub2e4.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2501.11325/x3.png", "caption": "Figure 2: Overview of the CatV2TON architecture. CatV2TON uses DiT [32] as the backbone, with the first DiT block duplicated as the Pose Encoder. The person and garment conditions are concatenated temporally as try-on conditions. The entire trainable portion consists only of the self-attention layers and Pose Encoder, accounting for less than 1/5 of the total parameters.", "description": "\uadf8\ub9bc 2\ub294 CatV2TON \uc544\ud0a4\ud14d\ucc98\uc758 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. CatV2TON\uc740 \ubc31\ubcf8\uc73c\ub85c DiT [32]\ub97c \uc0ac\uc6a9\ud558\uba70, \uccab \ubc88\uc9f8 DiT \ube14\ub85d\uc740 Pose Encoder\ub85c \ubcf5\uc81c\ub429\ub2c8\ub2e4. \uc0ac\ub78c\uacfc \uc758\ub958 \uc870\uac74\uc740 \uc2dc\uacc4\uc5f4\uc801\uc73c\ub85c \uc5f0\uacb0\ub418\uc5b4 \uc2dc\ub3c4 \uc870\uac74\uc774 \ub429\ub2c8\ub2e4. \uc804\uccb4 \ud6c8\ub828 \uac00\ub2a5 \ubd80\ubd84\uc740 \uc790\uae30 \uc8fc\uc758 \uacc4\uce35\uacfc Pose Encoder\ub85c \uad6c\uc131\ub418\uba70, \ucd1d \ub9e4\uac1c\ubcc0\uc218\uc758 1/5 \ubbf8\ub9cc\uc744 \ucc28\uc9c0\ud569\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 CatV2TON\uc758 \uad6c\uc870\ub97c \uc790\uc138\ud788 \uc124\uba85\ud558\uba70, DiT \ubc31\ubcf8, Pose Encoder\uc758 \uc5ed\ud560, \uadf8\ub9ac\uace0 \uc0ac\ub78c\uacfc \uc758\ub958 \uc815\ubcf4\uac00 \uc5b4\ub5bb\uac8c \uacb0\ud569\ub418\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \uc804\uccb4 \ubaa8\ub378\uc5d0\uc11c \ud6c8\ub828 \uac00\ub2a5\ud55c \ubd80\ubd84\uc774 \uc804\uccb4 \ub9e4\uac1c\ubcc0\uc218\uc758 \uadf9\ud788 \uc77c\ubd80\ubd84\ub9cc \ucc28\uc9c0\ud55c\ub2e4\ub294 \uc810\uc744 \uac15\uc870\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.11325/x4.png", "caption": "(a) Overlapping Clip-based Inference", "description": "\uadf8\ub9bc 3(a)\ub294 CatV2TON\uc758 \uae34 \ube44\ub514\uc624 \uc2dc\ud000\uc2a4\ub97c \uc704\ud55c \uc624\ubc84\ub7a9\ud551 \ud074\ub9bd \uae30\ubc18 \ucd94\ub860 \uc804\ub7b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae34 \ube44\ub514\uc624\ub294 \uacb9\uce58\ub294 \uc5ec\ub7ec \ud074\ub9bd\uc73c\ub85c \ub098\ub258\uace0, \uac01 \ud074\ub9bd\uc740 \ubc18\ubcf5\ub418\ub294 \ud504\ub808\uc784\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4. \uac01 \ud074\ub9bd\uc5d0 \ub300\ud55c \ucd94\ub860 \uacb0\uacfc\uc758 \ub9c8\uc9c0\ub9c9 k \ud504\ub808\uc784\uc740 \ub2e4\uc74c \ud074\ub9bd \uc0dd\uc131\uc744 \uc704\ud55c \ud504\ub86c\ud504\ud2b8 \ud504\ub808\uc784\uc73c\ub85c \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc774 \uacfc\uc815\uc740 \uc804\uccb4 \ube44\ub514\uc624\uc5d0 \uac78\uccd0 \ubc18\ubcf5\ub429\ub2c8\ub2e4.  \uc774\ub7ec\ud55c \ubc29\uc2dd\uc744 \ud1b5\ud574 CatV2TON\uc740 \ube44\ub514\uc624\uc758 \uc77c\ubd80\ubd84\uc744 \uc0dd\uc131\ud558\ub294 \ub300\uc2e0 \uae34 \ube44\ub514\uc624 \uc804\uccb4\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \uc0dd\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.2. Overlapping Clip-Based Inference"}, {"figure_path": "https://arxiv.org/html/2501.11325/x5.png", "caption": "(b) AdaCN", "description": "\uadf8\ub9bc 3(b)\ub294 \uc81c\uc548\ub41c \uc5b4\ub311\ud2f0\ube0c \ud074\ub9bd \uc815\uaddc\ud654(AdaCN) \uae30\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae34 \ube44\ub514\uc624\ub97c \uc0dd\uc131\ud560 \ub54c, \uc774\uc804\uc5d0 \uc0dd\uc131\ub41c \ud074\ub9bd\uc758 \ub9c8\uc9c0\ub9c9 \ud504\ub808\uc784\ub4e4\uc744 \ub2e4\uc74c \ud074\ub9bd \uc0dd\uc131\uc744 \uc704\ud55c \uac00\uc774\ub4dc \ud504\ub808\uc784\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294\ub370, \uc774 \uacfc\uc815\uc5d0\uc11c \uc0c9\uc0c1 \ubd88\uc77c\uce58 \ubc0f \uc6c0\uc9c1\uc784 \uc815\ub82c \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. AdaCN\uc740 \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 \uc774\uc804 \uac00\uc774\ub4dc \ud504\ub808\uc784\ub4e4\uc758 \ud3c9\uade0\uacfc \ud45c\uc900\ud3b8\ucc28\ub97c \uacc4\uc0b0\ud558\uc5ec, \ud604\uc7ac \ud074\ub9bd\uc758 \ud2b9\uc9d5\ub4e4\uc744 \uc815\uaddc\ud654\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ud074\ub9bd \uac04 \uc77c\uad00\uc131\uc744 \uc720\uc9c0\ud558\uace0 \ubd80\ub4dc\ub7ec\uc6b4 \uc804\ud658\uc744 \uac00\ub2a5\ud558\uac8c \ud569\ub2c8\ub2e4.", "section": "3.2 Overlapping Clip-Based Inference"}, {"figure_path": "https://arxiv.org/html/2501.11325/x6.png", "caption": "Figure 3: Illustration of the Overlapping Clip-Based Inference strategy. (a) A long video is divided into n\ud835\udc5bnitalic_n overlapping clips, with each clip consisting of repeated frames. The last k\ud835\udc58kitalic_k frames of each clip are used as prompt frames for generating the next clip. (b) Adaptive Clip Normalization (AdaCN) is applied to normalize the entire clip based on the mean and standard deviation of the prompt frame features and the denoised prompt frames, ensuring smooth continuity across clips in the generated video.", "description": "\uadf8\ub9bc 3\uc740 \uae34 \ube44\ub514\uc624\ub97c \ucc98\ub9ac\ud558\uae30 \uc704\ud55c \uc911\ucca9 \ud074\ub9bd \uae30\ubc18 \ucd94\ub860 \uc804\ub7b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \uae34 \ube44\ub514\uc624\uac00 \uc911\ubcf5\ub41c \ud504\ub808\uc784\uc744 \uac00\uc9c4 n\uac1c\uc758 \uc911\ucca9 \ud074\ub9bd\uc73c\ub85c \ub098\ub258\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ud074\ub9bd\uc758 \ub9c8\uc9c0\ub9c9 k\uac1c\uc758 \ud504\ub808\uc784\uc740 \ub2e4\uc74c \ud074\ub9bd \uc0dd\uc131\uc744 \uc704\ud55c \ud504\ub86c\ud504\ud2b8 \ud504\ub808\uc784\uc73c\ub85c \uc0ac\uc6a9\ub429\ub2c8\ub2e4. (b)\ub294 \uc5b4\ub311\ud2f0\ube0c \ud074\ub9bd \uc815\uaddc\ud654(AdaCN)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud504\ub86c\ud504\ud2b8 \ud504\ub808\uc784 \ud2b9\uc9d5\uacfc \uc7a1\uc74c \uc81c\uac70\ub41c \ud504\ub86c\ud504\ud2b8 \ud504\ub808\uc784\uc758 \ud3c9\uade0\uacfc \ud45c\uc900 \ud3b8\ucc28\ub97c \uae30\ubc18\uc73c\ub85c \uc804\uccb4 \ud074\ub9bd\uc744 \uc815\uaddc\ud654\ud558\uc5ec \uc0dd\uc131\ub41c \ube44\ub514\uc624\uc5d0\uc11c \ud074\ub9bd \uac04\uc758 \uc6d0\ud65c\ud55c \uc5f0\uc18d\uc131\uc744 \ubcf4\uc7a5\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. \ubc29\ubc95"}, {"figure_path": "https://arxiv.org/html/2501.11325/x7.png", "caption": "Figure 4: Qualitative comparison on the ViViD [13] dataset for dresses. We use Stable and OOTD as the short for StableVITON [24] and OOTDiffusion [51]. Additional comparison results are provided in the supplementary materials. Please zoom in for more details.", "description": "\uadf8\ub9bc 4\ub294 ViViD \ub370\uc774\ud130\uc14b\uc758 \ub4dc\ub808\uc2a4\uc5d0 \ub300\ud55c \uc815\uc131\uc801 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. StableVITON(Stable\ub85c \uc57d\uce6d)\uacfc OOTDiffusion(OOTD\ub85c \uc57d\uce6d)\uc758 \uacb0\uacfc\uc640 \ube44\uad50\ud558\uc5ec CatV2TON\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubc29\ubc95\uc740 \ub4dc\ub808\uc2a4 \uc774\ubbf8\uc9c0\uc640 \ub9c8\uc2a4\ud06c, \uadf8\ub9ac\uace0 \uc0dd\uc131\ub41c \ub4dc\ub808\uc2a4\ub97c \uc785\uc740 \uc0ac\ub78c \uc774\ubbf8\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\uba70, CatV2TON\uc774 \ub354 \uc790\uc5f0\uc2a4\ub7fd\uace0 \ud604\uc2e4\uc801\uc778 \uacb0\uacfc\ub97c \uc0dd\uc131\ud588\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubcf4\ub2e4 \uc790\uc138\ud55c \ube44\uad50 \uacb0\uacfc\ub294 \ubcf4\ucda9 \uc790\ub8cc\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc744 \ud655\ub300\ud558\uc5ec \uc790\uc138\ud788 \ubcf4\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4.", "section": "4.4 \uc815\uc131\uc801 \ube44\uad50"}, {"figure_path": "https://arxiv.org/html/2501.11325/x8.png", "caption": "Figure 5: Qualitative comparison on the ViViD [13] dataset for lower. We use Stable and OOTD as the short for StableVITON [24] and OOTDiffusion [51]. Additional comparison results are provided in the supplementary materials. Please zoom in for more details.", "description": "\uadf8\ub9bc 5\ub294 ViViD \ub370\uc774\ud130\uc14b\uc758 \ud558\uc758 \uac00\uc0c1 \ud53c\ud305 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. StableVITON\uacfc OOTDiffusion \ubc29\ubc95\uacfc \ube44\uad50\ud558\uc5ec CatV2TON\uc758 \uc131\ub2a5\uc744 \uc815\uc131\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\ub294 \uac01 \ubc29\ubc95\uc758 \uac00\uc0c1 \ud53c\ud305 \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \ubcf4\ub2e4 \uc790\uc138\ud55c \ube44\uad50 \uacb0\uacfc\ub294 \ubcf4\ucda9 \uc790\ub8cc\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc744 \ud655\ub300\ud558\uc5ec \uc790\uc138\ud788 \uc0b4\ud3b4\ubcf4\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4.", "section": "4.4. \uc815\uc131\uc801 \ube44\uad50"}, {"figure_path": "https://arxiv.org/html/2501.11325/x9.png", "caption": "Figure 6: Qualitative comparison on the ViViD [13] dataset for upper. We use Stable and OOTD as the short for StableVITON [24] and OOTDiffusion [51]. Additional comparison results are provided in the supplementary materials. Please zoom in for more details.", "description": "\uadf8\ub9bc 6\uc740 ViViD \ub370\uc774\ud130\uc14b\uc758 \uc0c1\uc758(upper)\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \uac00\uc0c1 \ud53c\ud305 \uacb0\uacfc\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. StableVITON (Stable\ub85c \uc57d\uce6d)\uacfc OOTDiffusion (OOTD\ub85c \uc57d\uce6d)\uc744 \ud3ec\ud568\ud55c \ub2e4\ub978 \ubc29\ubc95\ub4e4\uacfc CatV2TON\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub9bc\uc744 \ud655\ub300\ud558\uba74 \ub354 \uc790\uc138\ud55c \ub0b4\uc6a9\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc73c\uba70, \ubcf4\ucda9 \uc790\ub8cc\uc5d0\ub294 \ucd94\uac00 \ube44\uad50 \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ubc29\ubc95\uc758 \uc0c1\uc758 \uac00\uc0c1 \ud53c\ud305 \ud488\uc9c8\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ube44\uad50\ud558\uc5ec CatV2TON\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.4. \uc815\uc131\uc801 \ube44\uad50"}]