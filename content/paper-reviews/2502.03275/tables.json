[{"content": "| Model | Keys-Finding Maze |  | ProntoQA |  | ProsQA |  |\n|---|---|---|---|---|---|---|\n|  | 1-Feasible-10 (%) | Num. Tokens | Accuracy | Num. Tokens | Accuracy | Num. Tokens |\n|---|---|---|---|---|---|---|\n| Sol-Only | 3 | 645 | 93.8 | 3.0 | 76.7 | 8.2 |\n| CoT | 43 | 1312.0 | 98.8 | 92.5 | 77.5 | 49.4 |\n| Latent (ours) | 62.8 (<span style=\"color:#008000;\">\u2191+19.8</span>) | 374.6 | 100 (<span style=\"color:#008000;\">\u2191+1.2</span>) | 7.7 | 96.2 (<span style=\"color:#008000;\">\u2191+18.7</span>) | 10.9 |", "caption": "Table 4.1: Our latent approach surpasses the other baselines on Keys-Finding Maze, ProntoQA and ProsQA with a large margin\n. We use top-k\ud835\udc58kitalic_k (k=10\ud835\udc5810k=10italic_k = 10) decoding for Keys-Finding Maze and greedy decoding for ProntoQA and ProsQA. In terms of token efficiency,\nour latent approach also generates much shorter reasoning traces than the CoT baseline, closely tracking or even outperforming the Sol-Only approach.\nBold: best results. Underline: second best results. (\u2191\u2191\\uparrow\u2191 +Performance gain compared with the second best result.)", "description": "\ud45c 4.1\uc740 \uc81c\uc548\ub41c \uc7a0\uc7ac \ud1a0\ud070 \uc811\uadfc \ubc29\uc2dd\uc774 Keys-Finding Maze, ProntoQA, ProsQA \uc138 \uac00\uc9c0 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub2e4\ub978 \uae30\uc900 \ubaa8\ub378\ub4e4\ubcf4\ub2e4 \ud6e8\uc52c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Keys-Finding Maze\uc5d0\ub294 top-k (k=10) \ub514\ucf54\ub529\uc744, ProntoQA\uc640 ProsQA\uc5d0\ub294 \ud0d0\uc695\uc801 \ub514\ucf54\ub529\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \ud1a0\ud070 \ud6a8\uc728\uc131 \uce21\uba74\uc5d0\uc11c \uc81c\uc548\ub41c \ubc29\ubc95\uc740 CoT \uae30\uc900 \ubaa8\ub378\ubcf4\ub2e4 \ud6e8\uc52c \uc9e7\uc740 \ucd94\ub860 \ud2b8\ub808\uc774\uc2a4\ub97c \uc0dd\uc131\ud558\uba70, Sol-Only \uc811\uadfc \ubc29\uc2dd\uacfc \uac70\uc758 \ube44\uc2b7\ud558\uac70\ub098 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4.  \uad75\uc740 \uae00\uc528\ub294 \ucd5c\uace0 \uc131\ub2a5, \ubc11\uc904\uc740 \ub450 \ubc88\uc9f8\ub85c \uc88b\uc740 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud654\uc0b4\ud45c\ub294 \ub450 \ubc88\uc9f8\ub85c \uc88b\uc740 \uacb0\uacfc\uc640 \ube44\uad50\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8"}, {"content": "| Parameter | Value |\n|---|---| \n| Number of Layers (Transformer Blocks) | 12 |\n| Hidden Size (Embedding Size) | 768 |\n| Number of Attention Heads | 12 |\n| Vocabulary Size | 50,257 |\n| Total Number of Parameters | 117 million |", "caption": "Table 4.2: \nOur latent approach outperforms the baselines on various types of mathematical reasoning benchmarks. The models are fine-tuned on the MetaMathQA\u00a0(Yu et\u00a0al., 2023) dataset. The Math and GSM8K are in-domain datasets since they are used to generate MetaMathQA, while the others are out-of-domain. Bold: best results. Underscore: second best results. \u2191\u2191\\uparrow\u2191 +: \u2005Performance gain compared with the second best result.", "description": "\ud45c 4.2\ub294 \ub2e4\uc591\ud55c \uc218\ud559\uc801 \ucd94\ub860 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc81c\uc548\ub41c \uc7a0\uc7ac\uc801 \uc811\uadfc \ubc29\uc2dd\uc774 \uae30\uc900 \ubaa8\ub378\ub4e4\uc744 \ub2a5\uac00\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\ub4e4\uc740 MetaMathQA(Yu et al., 2023) \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubbf8\uc138 \uc870\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. Math\uc640 GSM8K\ub294 MetaMathQA\ub97c \uc0dd\uc131\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\uc5c8\uc73c\ubbc0\ub85c \ub3c4\uba54\uc778 \ub0b4 \ub370\uc774\ud130\uc14b\uc774\uba70, \ub2e4\ub978 \ub370\uc774\ud130\uc14b\ub4e4\uc740 \ub3c4\uba54\uc778 \uc678\ubd80 \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. \uad75\uc740 \uae00\uc528\ub294 \ucd5c\uace0\uc758 \uacb0\uacfc\ub97c, \ubc11\uc904\uc740 \ub450 \ubc88\uc9f8\ub85c \uc88b\uc740 \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \u2191\u2191+\ub294 \ub450 \ubc88\uc9f8\ub85c \uc88b\uc740 \uacb0\uacfc\uc640 \ube44\uad50\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8"}, {"content": "X=P\u2295C\u2295S | input text sample where \u2295 means concatenation\n---|---|---\nP | prompt of length t<sub>p</sub>\np<sub>i</sub> | the i-th token of prompt (in text)\nC | reasoning trace of length t<sub>c</sub>\nc<sub>i</sub> | the i-th token of trace (in text)\nS | solution of length t<sub>s</sub>\ns<sub>i</sub> | the i-th token of solution (in text)\nZ | the complete latent reasoning traces of length t<sub>z</sub>\nz<sub>i</sub> | the i-th token of latent trace\nr=t<sub>c</sub>/t<sub>z</sub> | compression rate\nm | number of trace tokens to be replaced by latent tokens during training\n\u0303X | modified input with mixed text and latent tokens\n\u2130 | codebook of VQ-VAE\ne<sub>i</sub> | the i-th vector in the codebook, which corresponds to the i-th latent token\nd | dimension of e<sub>i</sub>s\n\ud835\udcb1 | vocabulary of text tokens\nL | chunk size\nf<sub>enc</sub>(\u22c5) | encodes a chunk of L text tokens to L/r embedding vectors\nX\u0304=x\u0304<sub>1</sub>,\u2026,x\u0304<sub>L/r</sub> | embedding vectors of X outputted by f<sub>enc</sub>(\u22c5)\nq(\u22c5) | quantization operator that replaces, e.g., x\u0304<sub>1</sub> by its nearest neighbor in \u2130:\ng(\u22c5) | maps prompt to a d-dimensional embedding vector\nf<sub>dec</sub>(\u22c5,\u22c5) | decodes L/r quantized embedding vectors in \u2130 back to text tokens, conditioning on prompt embedding generated by g(\u22c5)", "caption": "Table 4.3: The average number of tokens in the generated responses. Compared with the CoT baseline, our latent approach achieves an 17%percent1717\\%17 % reduction in response length on average, while surpassing it in final performance according to\u00a0Table\u00a04.2. The iCoT method generates shorter responses than our approach, yet performs significantly worse, see\u00a0Table\u00a04.2. \u2193\u2193\\downarrow\u2193 -:\u2005Trace length reduction rate compared with CoT.", "description": "\ud45c 4.3\uc740 \uac01 \ubaa8\ub378\uc774 \uc0dd\uc131\ud55c \uc751\ub2f5\uc758 \ud1a0\ud070 \uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubcf8 \ub17c\ubb38\uc758 \uc81c\uc548\ub41c \ubc29\ubc95\uc740 CoT \uae30\uc900 \ub300\ube44 \ud3c9\uade0\uc801\uc73c\ub85c \uc751\ub2f5 \uae38\uc774\uac00 17% \uac10\uc18c\ud588\uc73c\uba70, \ud45c 4.2\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\ub4ef\uc774 \ucd5c\uc885 \uc131\ub2a5 \uba74\uc5d0\uc11c\ub3c4 CoT\ub97c \ub2a5\uac00\ud588\uc2b5\ub2c8\ub2e4. iCoT\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\ubcf4\ub2e4 \uc9e7\uc740 \uc751\ub2f5\uc744 \uc0dd\uc131\ud558\uc9c0\ub9cc, \ud45c 4.2\uc5d0\uc11c \uc54c \uc218 \uc788\ub4ef\uc774 \uc131\ub2a5\uc740 \ud604\uc800\ud788 \ub5a8\uc5b4\uc9d1\ub2c8\ub2e4. \u2193 \uae30\ud638\ub294 CoT \ub300\ube44 \ud1a0\ud070 \uae38\uc774 \uac10\uc18c\uc728\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4.2 \uc8fc\uc694 \uacb0\uacfc"}]