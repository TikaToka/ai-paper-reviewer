{"references": [{"fullname_first_author": "Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduces the chain-of-thought prompting method, which is a foundational concept for the current work and heavily cited throughout the paper."}, {"fullname_first_author": "Nye", "paper_title": "Show your work: Scratchpads for intermediate computation with language models", "publication_date": "2021-12-01", "reason": "This paper demonstrates the effectiveness of training LLMs on chain-of-thought data, which is a key aspect of the current paper's approach."}, {"fullname_first_author": "Deng", "paper_title": "Implicit chain of thought reasoning via knowledge distillation", "publication_date": "2023-11-01", "reason": "This paper explores methods for compressing reasoning traces, a central challenge addressed in the current paper."}, {"fullname_first_author": "Hao", "paper_title": "Training large language models to reason in a continuous latent space", "publication_date": "2024-12-01", "reason": "This paper investigates reasoning in continuous latent space, offering an alternative approach that is compared to the current work."}, {"fullname_first_author": "Yu", "paper_title": "Metamath: Bootstrap your own mathematical questions for large language models", "publication_date": "2023-09-01", "reason": "This paper provides a dataset for mathematical reasoning which is used as a benchmark in the experiments for the current paper."}]}