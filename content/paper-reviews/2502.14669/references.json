{"references": [{"fullname_first_author": "Duyu Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning", "publication_date": "2025-01-12", "reason": "This paper introduces GRPO, a key technique used in the current research and is directly compared to in the results section."}, {"fullname_first_author": "Michael Igorevich Ivanitskiy", "paper_title": "A configurable library for generating and manipulating maze datasets", "publication_date": "2023-09-10", "reason": "This paper describes the framework used to generate the maze datasets used in the experiments, a crucial element of the methodology."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduces chain-of-thought prompting, a core concept underpinning the approach used in the current research."}, {"fullname_first_author": "Zhihong Shao", "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models", "publication_date": "2024-02-01", "reason": "This paper is referenced for its use of GRPO which is also a technique used in the current research."}, {"fullname_first_author": "Xiaohu Jiang", "paper_title": "Supervised fine-tuning in turn improves visual foundation models", "publication_date": "2024-01-01", "reason": "This paper supports the use of supervised fine-tuning (SFT), a crucial first step in the proposed two-stage training framework."}]}