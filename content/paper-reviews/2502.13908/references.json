{"references": [{"fullname_first_author": "Emily M. Bender", "paper_title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?", "publication_date": "2021-03-10", "reason": "This paper highlights the potential risks and biases associated with large language models, which are central concerns when using LLMs for relevance assessment."}, {"fullname_first_author": "Guglielmo Faggioli", "paper_title": "Perspectives on large language models for relevance judgment", "publication_date": "2023-07-23", "reason": "This paper directly addresses the use of LLMs for relevance judgment, exploring different collaboration strategies between humans and LLMs, which is highly relevant to the LLMJudge challenge."}, {"fullname_first_author": "Paul Thomas", "paper_title": "Large language models can accurately predict searcher preferences", "publication_date": "2023-09-10", "reason": "This paper demonstrates the potential of LLMs in accurately predicting user preferences, a key aspect for evaluating the effectiveness of relevance judgments generated by LLMs."}, {"fullname_first_author": "Hossein A Rahmani", "paper_title": "Synthetic Test Collections for Retrieval Evaluation", "publication_date": "2024-05-07", "reason": "This paper explores the creation of synthetic test collections using LLMs, providing a method to reduce the cost and effort associated with building traditional relevance judgment datasets."}, {"fullname_first_author": "Shivani Upadhyay", "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor", "publication_date": "2024-06-06", "reason": "This paper presents an open-source reproduction of a relevance assessment approach used by Bing, offering a practical and accessible method for LLM-based relevance judgments."}]}