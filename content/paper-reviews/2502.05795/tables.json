[{"content": "| Model | LLaMA-130M | LLaMA-250M | LLaMA-350M | LLaMA-1B |\n|---|---|---|---|---|\n| Training Tokens | 2.2B | 3.9B | 6.0B | 8.9B |\n| Post-LN [Ba, 2016] | 26.95 | 1409.79 | 1368.33 | 1390.75 |\n| DeepNorm [Wang et al., 2024] | 27.17 | 22.77 | 1362.59 | 1409.08 |\n| Mix-LN [Li et al., 2024b] | 26.07 | 21.39 | 1363.21 | 1414.78 |\n| Pre-LN [Baevski and Auli, 2019] | 26.73 | 21.92 | 19.58 | 17.02 |\n| Pre-LN + LayerNorm Scaling | **25.76** | **20.35** | **18.20** | **15.71** |", "caption": "Table 1: Perplexity (\u2193) comparison of various layer normalization methods across various LLaMA sizes.", "description": "\ubcf8 \ud45c\ub294 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 LLaMA \ubaa8\ub378\uc5d0\uc11c \ub2e4\uc591\ud55c Layer Normalization \uae30\ubc95\uc758 \uc131\ub2a5\uc744 perplexity \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Pre-LN, Post-LN, DeepNorm, Mix-LN\uacfc \uc81c\uc548\ub41c LayerNorm Scaling \uae30\ubc95\uc758 perplexity \uac12\uc744 130M, 250M, 350M, 1B \ud30c\ub77c\ubbf8\ud130 \ubaa8\ub378\uc5d0 \ub300\ud574 \ube44\uad50\ud568\uc73c\ub85c\uc368, \uac01 \uae30\ubc95\uc758 \ud6a8\uc728\uc131\uacfc \uc548\uc815\uc131\uc744 \ud3c9\uac00\ud558\uace0 LayerNorm Scaling\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. Experiments"}, {"content": "| Pre-LN | Admin | Group-LN | Sandwich-LN | Mix-LN | LayerNorm Scaling |\n|---|---|---|---|---|---| \n| 26.73 | 27.91 | 28.01 | 26.51 | 26.07 | 25.76 |", "caption": "Table 2: Comparison against other normalization methods on LLaMA-130M. Perplexity (\u2193) is reported.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \uce35 \uc815\uaddc\ud654 \uae30\ubc95\uc744 LLaMA-130M \ubaa8\ub378\uc5d0 \uc801\uc6a9\ud588\uc744 \ub54c\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  \uc131\ub2a5 \uc9c0\ud45c\ub85c\ub294 perplexity(\ub0ae\uc744\uc218\ub85d \uc88b\uc74c)\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.  Pre-LN(Pre-Layer Normalization)\uc744 \uae30\uc900\uc73c\ub85c Admin, Group-LN, Sandwich-LN, Mix-LN, \uadf8\ub9ac\uace0 LayerNorm Scaling \ub4f1\uc758 \ub2e4\ub978 \uc815\uaddc\ud654 \ubc29\ubc95\uacfc\uc758 perplexity \uac12\uc744 \ube44\uad50\ud558\uc5ec LayerNorm Scaling\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8"}, {"content": "| Method | MMLU | BoolQ | ARC-e | PIQA | Hellaswag | OBQA | Winogrande | Average |\n|---|---|---|---|---|---|---|---|---|\n| **LLaMA-250M** |  |  |  |  |  |  |  |  |\n| Post-LN [Ba, 2016] | 22.95 | 37.83 | 26.94 | 52.72 | 26.17 | 11.60 | 49.56 | 32.54 |\n| DeepNorm [Wang et al., 2024] | 23.60 | 37.86 | 36.62 | 61.10 | 25.69 | 15.00 | 49.57 | 35.63 |\n| Mix-LN [Li et al., 2024b] | 26.53 | 56.12 | 41.68 | 66.34 | 30.16 | 18.00 | 50.56 | 41.34 |\n| Pre-LN [Baevski and Auli, 2019] | 24.93 | 38.35 | 40.15 | 63.55 | 26.34 | 16.20 | 49.01 | 36.93 |\n| Pre-LN + LayerNorm Scaling | **27.08** | **58.17** | **45.24** | **67.38** | **32.81** | **18.80** | **52.49** | **43.14** |\n| **LLaMA-1B** |  |  |  |  |  |  |  |  |\n| Post-LN [Ba, 2016] | 22.95 | 37.82 | 25.08 | 49.51 | 25.04 | 13.80 | 49.57 | 31.96 |\n| DeepNorm [Wang et al., 2024] | 23.35 | 37.83 | 27.06 | 52.94 | 26.19 | 11.80 | 49.49 | 32.67 |\n| Mix-LN [Li et al., 2024b] | 23.19 | 37.83 | 25.08 | 49.51 | 25.04 | 11.80 | 49.57 | 31.72 |\n| Pre-LN [Baevski and Auli, 2019] | 26.54 | **62.20** | 45.70 | 67.79 | 30.96 | 17.40 | 50.51 | 43.01 |\n| Pre-LN + LayerNorm Scaling | **28.69** | 61.80 | **48.85** | **67.92** | **33.94** | **18.60** | **54.30** | **44.87** |", "caption": "Table 3: Fine-tuning performance (\u2191\u2191\\uparrow\u2191) of LLaMA with various normalizations.", "description": "\ud45c 3\uc740 \ub2e4\uc591\ud55c \uc815\uaddc\ud654 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec LLaMA \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud588\uc744 \ub54c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud558\uc704 \uc791\uc5c5(MMLU, BoolQ, ARC-e, PIQA, HellaSwag, OBQA, Winogrande)\uc5d0 \ub300\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50\ud558\uc5ec LayerNorm Scaling\uc774 \ub2e4\ub978 \uc815\uaddc\ud654 \uae30\ubc95\ub4e4\ubcf4\ub2e4 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788 LLaMA-1B \ubaa8\ub378\uc5d0\uc11c\ub294 8\uac1c\uc758 \ud558\uc704 \uc791\uc5c5 \uc911 7\uac1c\uc5d0\uc11c LayerNorm Scaling\uc774 \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4.", "section": "5.2. Supervised Fine-tuning"}, {"content": "| Perplexity (\u2193) | **LLaMA-130M** | **LLaMA-250M** |\n|---|---|---|\n| Training Tokens | 2.2B | 3.9B |\n| Pre-LN | 26.73 | 21.92 |\n| + LayerScale | 27.93 | 23.45 |\n| + Scaled Initialization | 26.04 | 20.98 |\n| + LayerNorm Scaling | **25.76** | **20.35** |", "caption": "Table 4: Comparison against other scaling methods.", "description": "\ud45c 4\ub294 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 LLaMA \ubaa8\ub378\uc5d0\uc11c \ub2e4\uc591\ud55c Layer Normalization \uae30\ubc95\ub4e4\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Pre-LN\uc744 \uae30\uc900\uc73c\ub85c LayerScale, Scaled Initialization, \uadf8\ub9ac\uace0 LayerNorm Scaling \uc138 \uac00\uc9c0 \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 perplexity(\ub0ae\uc744\uc218\ub85d \uc88b\uc74c) \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\uad50\ud569\ub2c8\ub2e4.  \uac01 \ubc29\ubc95\uc758 perplexity \uac12\uc744 LLaMA-130M\uacfc LLaMA-250M \ubaa8\ub378\uc5d0 \ub300\ud574 \uc81c\uc2dc\ud558\uc5ec, \ubaa8\ub378 \ud06c\uae30\uc5d0 \ub530\ub978 \uc131\ub2a5 \ubcc0\ud654\ub97c \uc0b4\ud3b4\ubd05\ub2c8\ub2e4.  \uc774\ub294 LayerNorm Scaling\uc758 \ud6a8\uacfc\ub97c \ub2e4\ub978 scaling \uae30\ubc95\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8"}]