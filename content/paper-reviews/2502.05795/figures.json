[{"figure_path": "https://arxiv.org/html/2502.05795/x1.png", "caption": "Figure 1: \nLayerwise output variance. This figure compares the output variance across various layers for different setups: (1) Pre-LN; (2) Pre-LN with Scaled Initialization; and (3) LayerNorm Scaling. The experiments are conducted on the LLaM-130M model trained for 10,000 steps. The proposed LayerNorm Scaling effectively controls the variance across layers.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc124\uc815\uc5d0\uc11c \uc5ec\ub7ec \uacc4\uce35\uc5d0 \uac78\uce5c \ucd9c\ub825 \ubd84\uc0b0\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. \uc124\uc815\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. (1) \uc0ac\uc804 \ub808\uc774\uc5b4 \uc815\uaddc\ud654(Pre-LN), (2) \uc0ac\uc804 \ub808\uc774\uc5b4 \uc815\uaddc\ud654\uc640 \ud06c\uae30 \uc870\uc815\ub41c \ucd08\uae30\ud654\ub97c \uacb0\ud569\ud55c \ubc29\ubc95, (3) \uc81c\uc548\ub41c \ub808\uc774\uc5b4 \uc815\uaddc\ud654 \ud06c\uae30 \uc870\uc815(LayerNorm Scaling). 1\uc5b5 3\ucc9c\ub9cc \ub9e4\uac1c\ubcc0\uc218\uc758 LLaMA \ubaa8\ub378\uc744 1\ub9cc \ub2e8\uacc4 \ub3d9\uc548 \ud559\uc2b5\uc2dc\ud0a8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc81c\uc548\ub41c LayerNorm Scaling\uc740 \ubaa8\ub4e0 \uacc4\uce35\uc5d0\uc11c \ubd84\uc0b0\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \uc81c\uc5b4\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, Pre-LN\uc5d0\uc11c \ub098\ud0c0\ub098\ub294 \uae4a\uc740 \ub808\uc774\uc5b4\uc758 \ucd9c\ub825\uac12 \ubd84\uc0b0 \ud3ed\ubc1c \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uc5ec \uae4a\uc740 \ub808\uc774\uc5b4\uac00 \ud559\uc2b5\uc5d0 \ud6a8\uacfc\uc801\uc73c\ub85c \uae30\uc5ec\ud558\ub3c4\ub85d \ub9cc\ub4e4 \uc218 \uc788\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. LayerNorm Scaling"}, {"figure_path": "https://arxiv.org/html/2502.05795/x2.png", "caption": "Figure 2: \nPerformance drop of layer pruning across different LLMs.\n(a) BERT-Large (Post-LN), (b) Mistral-7B (Pre-LN), (c) Qwen-7B (Pre-LN), (d) DeepSeek-7B (Pre-LN), (e) LLaMA2-7B (Pre-LN), and (f) LLaMA2-13B (Pre-LN). The results show that Pre-LN models exhibit significant inefficiency in deeper layers, while Post-LN models maintain strong deep-layer contributions.", "description": "\uadf8\ub9bc 2\ub294 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \uc5b8\uc5b4 \ubaa8\ub378\uc5d0\uc11c \ub808\uc774\uc5b4 \uc81c\uac70(pruning)\uc5d0 \ub530\ub978 \uc131\ub2a5 \uc800\ud558\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. BERT-Large(Post-LN)\ub97c \uc81c\uc678\ud558\uace0 Mistral-7B, Qwen-7B, DeepSeek-7B, LLaMA2-7B, LLaMA2-13B\ub294 \ubaa8\ub450 Pre-LN\uc744 \uc0ac\uc6a9\ud558\ub294 \ubaa8\ub378\ub4e4\uc785\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c \ubcf4\ub4ef\uc774, Pre-LN \ubaa8\ub378\uc740 \uae4a\uc740 \ub808\uc774\uc5b4\uc77c\uc218\ub85d \uc131\ub2a5 \uc800\ud558\uac00 \uc801\uc740 \ubc18\uba74, Post-LN \ubaa8\ub378\uc740 \uae4a\uc740 \ub808\uc774\uc5b4\uc5d0\uc11c\ub3c4 \uc131\ub2a5 \uc800\ud558\uac00 \uc0c1\ub2f9\ud788 \ud06c\uac8c \ub098\ud0c0\ub0a9\ub2c8\ub2e4. \uc774\ub294 Pre-LN \ubaa8\ub378\uc758 \uae4a\uc740 \ub808\uc774\uc5b4\uac00 \ud6a8\uc728\uc801\uc73c\ub85c \ud559\uc2b5\ub418\uc9c0 \uc54a\uc74c\uc744 \uc2dc\uc0ac\ud558\uba70, Post-LN \ubaa8\ub378\uacfc\ub294 \ub300\uc870\uc801\uc778 \uacb0\uacfc\uc785\ub2c8\ub2e4.  \uc989, Pre-LN \uae30\ubc18 \ubaa8\ub378\uc5d0\uc11c\ub294 \uae4a\uc740 \ub808\uc774\uc5b4\uc758 \ud6a8\uc728\uc131\uc774 \ub5a8\uc5b4\uc9c0\uace0, Post-LN \uae30\ubc18 \ubaa8\ub378\uc5d0\uc11c\ub294 \uc595\uc740 \ub808\uc774\uc5b4\uc758 \ud6a8\uc728\uc131\uc774 \ub5a8\uc5b4\uc9d0\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2. Empirical Evidence of the Curse of Depth"}, {"figure_path": "https://arxiv.org/html/2502.05795/x3.png", "caption": "Figure 3: Comparison between Pre-LN (a) and LayerNorm Scaling (b). LayerNorm Scaling applies a scaling factor inversely proportional to the square root of the layer index l\ud835\udc59litalic_l, preventing excessive variance growth and stabilizing training dynamics across layers.", "description": "\uadf8\ub9bc 3\uc740 Pre-LN(a)\uacfc LayerNorm Scaling(b)\uc758 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. LayerNorm Scaling\uc740 \ub808\uc774\uc5b4 \uc778\ub371\uc2a4 l\uc758 \uc81c\uacf1\uadfc\uc5d0 \ubc18\ube44\ub840\ud558\ub294 \uc2a4\ucf00\uc77c\ub9c1 \uacc4\uc218\ub97c \uc801\uc6a9\ud558\uc5ec \uacfc\ub3c4\ud55c \ubd84\uc0b0 \uc99d\uac00\ub97c \ubc29\uc9c0\ud558\uace0 \ub808\uc774\uc5b4 \uac04\uc758 \uc548\uc815\uc801\uc778 \ud559\uc2b5 \uc5ed\ud559\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4.  \uc880 \ub354 \uc790\uc138\ud788 \uc124\uba85\ud558\uba74, Pre-LN(Pre-Layer Normalization)\uc740 \ub808\uc774\uc5b4\uc758 \uc785\ub825\uc744 \uc815\uaddc\ud654\ud55c \ud6c4\uc5d0 \uc8fc\uc694 \uc5f0\uc0b0(\uc608: \uc5b4\ud150\uc158, \ud53c\ub4dc\ud3ec\uc6cc\ub4dc \ub124\ud2b8\uc6cc\ud06c)\uc744 \uc218\ud589\ud558\ub294 \ubc18\uba74, LayerNorm Scaling\uc740 \ub808\uc774\uc5b4 \uc815\uaddc\ud654\uc758 \ucd9c\ub825\uc744 \ub808\uc774\uc5b4 \uae4a\uc774(l)\uc758 \uc81c\uacf1\uadfc\uc758 \uc5ed\uc218\ub9cc\ud07c \uc2a4\ucf00\uc77c\ub9c1\ud569\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uae4a\uc740 \ub808\uc774\uc5b4\uc5d0\uc11c \ucd9c\ub825 \ubd84\uc0b0\uc774 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \uc99d\uac00\ud558\ub294 \uac83\uc744 \uc5b5\uc81c\ud558\uc5ec, \ubaa8\ub4e0 \ub808\uc774\uc5b4\uac00 \ud6a8\uacfc\uc801\uc73c\ub85c \ud559\uc2b5\uc5d0 \uae30\uc5ec\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc2a4\ucf00\uc77c\ub9c1 \uae30\ubc95\uc740 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \uadf8\ub798\ub514\uc5b8\ud2b8\uc758 \uc548\uc815\uc131\uc744 \uc720\uc9c0\ud558\uace0, \uc804\uccb4 \ubaa8\ub378 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud569\ub2c8\ub2e4.", "section": "4. LayerNorm Scaling"}, {"figure_path": "https://arxiv.org/html/2502.05795/x4.png", "caption": "Figure 4: \nPerformance drop of layer pruning on LLaMA-130M. LayerNorm Scaling enables deep layers to make a meaningful contribution to the model.", "description": "\uadf8\ub9bc 4\ub294 LLaMA-130M \ubaa8\ub378\uc5d0\uc11c \ub808\uc774\uc5b4 \uac00\uc9c0\uce58\uae30\ub97c \ud588\uc744 \ub54c \uc131\ub2a5 \uc800\ud558\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Pre-LN\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0 \uae4a\uc740 \ub808\uc774\uc5b4\ub97c \uc81c\uac70\ud574\ub3c4 \uc131\ub2a5 \uc800\ud558\uac00 \uac70\uc758 \uc5c6\uc5c8\uc9c0\ub9cc, LayerNorm Scaling\uc744 \uc801\uc6a9\ud55c \uacbd\uc6b0\uc5d0\ub294 \uae4a\uc740 \ub808\uc774\uc5b4\ub97c \uc81c\uac70\ud588\uc744 \ub54c \uc131\ub2a5 \uc800\ud558\uac00 \uc0c1\ub2f9\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub294 LayerNorm Scaling\uc774 \uae4a\uc740 \ub808\uc774\uc5b4\uac00 \ubaa8\ub378\uc5d0 \uc758\ubbf8\uc788\ub294 \uae30\uc5ec\ub97c \ud558\ub3c4\ub85d \ub9cc\ub4e4\uc5c8\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.  \uae4a\uc740 \ub808\uc774\uc5b4\uc758 \uc911\uc694\uc131\uc744 \uac15\uc870\ud558\uba70, LayerNorm Scaling\uc758 \ud6a8\uacfc\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "2. \uae4a\uc774 \uc800\uc8fc\uc758 \uacbd\ud5d8\uc801 \uc99d\uac70"}, {"figure_path": "https://arxiv.org/html/2502.05795/x5.png", "caption": "Figure 5: Training loss of LLaMA-1B with Pre-LN and LayerNorm Scaling.", "description": "\uadf8\ub9bc 5\ub294 LLaMA-1B \ubaa8\ub378\uc5d0 \ub300\ud574 Pre-LN(Pre-Layer Normalization)\uacfc LayerNorm Scaling\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \ud559\uc2b5 \uc190\uc2e4 \uace1\uc120\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Pre-LN\uc740 \uae4a\uc774\uac00 \uae4a\uc5b4\uc9d0\uc5d0 \ub530\ub77c \uc131\ub2a5\uc774 \uc800\ud558\ub418\ub294 \ud604\uc0c1\uc744 \ubcf4\uc774\ub294 \ubc18\uba74, LayerNorm Scaling\uc740 \uae4a\uc740 \ub808\uc774\uc5b4\uc758 \uae30\uc5ec\ub3c4\ub97c \ub192\uc5ec \ud559\uc2b5 \uacfc\uc815\uc744 \uc548\uc815\ud654\uc2dc\ud0a4\uace0 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub450 \ubc29\ubc95 \ubaa8\ub450 \ud559\uc2b5 \ucd08\uae30\uc5d0\ub294 \uc190\uc2e4\uc774 \ube60\ub974\uac8c \uac10\uc18c\ud558\uc9c0\ub9cc, LayerNorm Scaling\uc774 Pre-LN\ubcf4\ub2e4 \ub354 \ube60\ub974\uac8c \uc218\ub834\ud558\uace0 \ub0ae\uc740 \uc190\uc2e4 \uac12\uc744 \uc720\uc9c0\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 LayerNorm Scaling\uc774 LLM\uc758 \ud559\uc2b5 \ud6a8\uc728\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \ud6a8\uacfc\uc801\uc784\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8"}, {"figure_path": "https://arxiv.org/html/2502.05795/x6.png", "caption": "Figure 6: \nVariance growth across layers in LLaMA-130M with Pre-LN.\nEach subplot shows the variance at different training stages (1000, 3000, and 6000 epochs).\nIn all cases, the variance follows an exponential growth pattern as depth increases, indicating that deeper layers experience uncontrolled variance amplification regardless of training progress.", "description": "\uadf8\ub9bc 6\uc740 Pre-LN(Pre-Layer Normalization)\uc744 \uc0ac\uc6a9\ud558\ub294 LLaMA-130M \ubaa8\ub378\uc5d0\uc11c \uac01 \ub808\uc774\uc5b4\uc758 \ubd84\uc0b0(variance)\uc774 \ud6c8\ub828 \uacfc\uc815\uc5d0\uc11c \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc138 \uac1c\uc758 \uc11c\ube0c\ud50c\ub86f\uc740 \uac01\uac01 1000, 3000, 6000 \uc5d0\ud3ed(epoch)\uc5d0\uc11c\uc758 \ubd84\uc0b0\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ubaa8\ub4e0 \uacbd\uc6b0\uc5d0 \uc788\uc5b4\uc11c, \ub808\uc774\uc5b4\uc758 \uae4a\uc774\uac00 \uae4a\uc5b4\uc9d0\uc5d0 \ub530\ub77c \ubd84\uc0b0\uc740 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \uc99d\uac00\ud558\ub294 \ud328\ud134\uc744 \ubcf4\uc774\uba70, \uc774\ub294 \ud6c8\ub828 \uc9c4\ud589 \uc815\ub3c4\uc5d0 \uad00\uacc4\uc5c6\uc774 \uae4a\uc740 \ub808\uc774\uc5b4\uc5d0\uc11c \uc81c\uc5b4\ub418\uc9c0 \uc54a\uc740 \ubd84\uc0b0 \uc99d\ud3ed\uc774 \ubc1c\uc0dd\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "2. \uae4a\uc774 \uc800\uc8fc \ud604\uc0c1\uc758 \uc2e4\uc99d\uc801 \uc99d\uac70"}]