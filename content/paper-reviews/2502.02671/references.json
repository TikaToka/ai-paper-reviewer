{"references": [{"fullname_first_author": "Gao, L.", "paper_title": "Scaling laws for reward model overoptimization", "publication_date": "2023", "reason": "This paper introduces the concept of reward hacking in RLHF, which is directly analogous to the teacher hacking explored in the main paper, providing a strong theoretical foundation for the study."}, {"fullname_first_author": "Hinton, G.", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015", "reason": "This foundational paper introduced knowledge distillation (KD), the core technique upon which the main paper builds, and it is vital for understanding the context and limitations of the method."}, {"fullname_first_author": "Menon, A. K.", "paper_title": "A statistical perspective on distillation", "publication_date": "2021", "reason": "This paper provides a critical perspective on knowledge distillation by highlighting the imperfect nature of the teacher model and its potential implications, directly addressing a key assumption challenged in the main paper."}, {"fullname_first_author": "Zhang, R.", "paper_title": "Do not blindly imitate the teacher: Using perturbed loss for knowledge distillation", "publication_date": "2023", "reason": "This paper explores the issues of overfitting and imperfect teacher models in KD, offering an alternative approach that is relevant to mitigating the issues described in the main study."}, {"fullname_first_author": "Amodei, D.", "paper_title": "Concrete problems in ai safety", "publication_date": "2016", "reason": "This paper discusses the broader concerns about AI safety and potential risks associated with advanced models, establishing a relevant safety-critical context for the careful study of teacher hacking."}]}