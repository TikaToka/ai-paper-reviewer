{"references": [{"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic algorithms and applications", "publication_date": "2018-12-06", "reason": "This paper introduces the Soft Actor-Critic (SAC) algorithm, a key off-policy model-free deep RL method that serves as the foundation for MAXINFORL and several baselines."}, {"fullname_first_author": "Yuri Burda", "paper_title": "Exploration by random network distillation", "publication_date": "2018-10-31", "reason": "This work introduces Random Network Distillation (RND), a popular intrinsic reward for exploration that MAXINFORL builds upon and compares against."}, {"fullname_first_author": "Deepak Pathak", "paper_title": "Curiosity-driven exploration by self-supervised prediction", "publication_date": "2017-01-01", "reason": "This paper presents a curiosity-driven exploration approach based on self-supervised prediction, serving as a cornerstone for intrinsic motivation in RL and as a baseline comparison for MAXINFORL."}, {"fullname_first_author": "Ramanan Sekar", "paper_title": "Planning to explore via self-supervised world models", "publication_date": "2020-01-01", "reason": "This paper utilizes self-supervised world models for planning exploration in reinforcement learning, demonstrating the effectiveness of model-based information gain for exploration and providing context for MAXINFORL's approach."}, {"fullname_first_author": "Denis Yarats", "paper_title": "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels", "publication_date": "2021-01-01", "reason": "This work introduces DrQ, a state-of-the-art visual control algorithm using image augmentation that MAXINFORL extends for visual control tasks, demonstrating its application in complex scenarios."}]}