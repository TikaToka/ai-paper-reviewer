<br><table id='9' style='font-size:16px'><tr><td>Type</td><td>Source Dataset</td></tr><tr><td>Visual Grounding</td><td>RefCOCO (Kazemzadeh et al., 2014), TAO (Dave et al., 2020) ILSVRC2015-VID (Russakovsky et al., 2015), Object365 (Shao et al., 2019)</td></tr><tr><td>Recognition</td><td>CustomConcept101 (Kumari et al., 2023), CelebA (Liu et al., 2015)</td></tr><tr><td>Caption & Description</td><td>RefCOCO (Kazemzadeh et al., 2014), TAO (Dave et al., 2020) Object365 (Shao et al., 2019), CustomConcept101 (Kumari et al., 2023)</td></tr><tr><td>Question Answering</td><td>RefCOCO (Kazemzadeh et al., 2014), TAO (Dave et al., 2020) Object365 (Shao et al., 2019), CustomConcept101 (Kumari et al., 2023) CelebA (Liu et al., 2015)</td></tr><tr><td>LLaVA-Instruction</td><td>LLaVA-Instruct-665K (Liu et al., 2023a)</td></tr></table>