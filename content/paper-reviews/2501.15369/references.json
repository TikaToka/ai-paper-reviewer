{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-11", "reason": "This paper introduced the vision transformer (ViT), a groundbreaking approach that significantly impacted the field and inspired many subsequent works, including this one."}, {"fullname_first_author": "Zhuang Liu", "paper_title": "A convnet for the 2020s", "publication_date": "2022-00-00", "reason": "This paper presented ConvNeXt, a highly efficient convolutional neural network that serves as the foundation for the proposed iFormer architecture."}, {"fullname_first_author": "Kaiming He", "paper_title": "Mask R-CNN", "publication_date": "2017-00-00", "reason": "This paper introduced Mask R-CNN, a widely used object detection and instance segmentation model used for evaluating iFormer on downstream tasks."}, {"fullname_first_author": "Sachin Mehta", "paper_title": "MobileViT: light-weight, general-purpose, and mobile-friendly vision transformer", "publication_date": "2021-10-02", "reason": "This paper introduced MobileViT, a hybrid model that effectively combines CNNs and ViTs for mobile applications, setting a benchmark for similar approaches."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-00-00", "reason": "This paper introduced efficient training techniques for Vision Transformers and knowledge distillation, which are relevant to improving the accuracy and efficiency of iFormer."}]}