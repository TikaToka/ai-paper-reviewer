[{"figure_path": "https://arxiv.org/html/2501.15369/x1.png", "caption": "Figure 1: Comparison of latency and accuracy between our iFormer and other existing methods on ImageNet-1k. The latency is measured on an iPhone 13. Our iFormer is Pareto-optimal.", "description": "\uadf8\ub9bc 1\uc740 ImageNet-1k \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec iFormer\uc640 \ub2e4\ub978 \uae30\uc874 \uacbd\ub7c9\ud654 \ubaa8\ub378\ub4e4\uc758 \uc9c0\uc5f0 \uc2dc\uac04\uacfc \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. iPhone 13\uc5d0\uc11c \uce21\uc815\ud55c \uc9c0\uc5f0 \uc2dc\uac04\uc744 \uae30\uc900\uc73c\ub85c, iFormer\ub294 \ub2e4\ub978 \ubaa8\ub378\ub4e4\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\uba70 Pareto \ucd5c\uc801\uc810\uc5d0 \uc704\uce58\ud558\uace0 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \uc9c0\uc5f0 \uc2dc\uac04\uc744 \ub354 \uc904\uc774\uc9c0 \uc54a\uace0\ub3c4 \uc815\ud655\ub3c4\ub97c \ub354 \ub192\uc77c \uc218 \uc788\ub294 \uc131\ub2a5\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc758 \uc9c0\uc5f0 \uc2dc\uac04\uacfc \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec iFormer\uc758 \uc6b0\uc218\uc131\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 \uc774\ubbf8\uc9c0 \ubd84\ub958"}, {"figure_path": "https://arxiv.org/html/2501.15369/x2.png", "caption": "Figure 2: Illustration of the evolution from the ConvNeXt baseline towards the lightweight iFormer. The orange bars are model accuracies and the light blue bars are model latencies. We also include a red latency outline for better visualization.", "description": "\uadf8\ub9bc 2\ub294 \uc77c\ubc18\uc801\uc778 ConvNeXt \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c \uacbd\ub7c9\ud654\ub41c iFormer \ub124\ud2b8\uc6cc\ud06c\ub85c\uc758 \ubc1c\uc804 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc8fc\ud669\uc0c9 \ub9c9\ub300\ub294 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4(Top-1 Accuracy)\ub97c, \uc5f0\ud55c \ud30c\ub780\uc0c9 \ub9c9\ub300\ub294 \ubaa8\ub378\uc758 \uc9c0\uc5f0 \uc2dc\uac04(Latency)\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ube68\uac04\uc0c9 \uc810\uc120\uc740 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \ub354\uc6b1 \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud55c \uac00\uc774\ub4dc\ub77c\uc778\uc785\ub2c8\ub2e4. ConvNeXt\ub97c \uae30\ubc18\uc73c\ub85c, \uc5ec\ub7ec \ub2e8\uacc4\uc758 \ucd5c\uc801\ud654\ub97c \uac70\uce58\uba74\uc11c \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud558\uac70\ub098 \ud5a5\uc0c1\uc2dc\ud0a4\uba74\uc11c \uc9c0\uc5f0 \uc2dc\uac04\uc744 \uc904\uc5ec\ub098\uac00\ub294 \uacfc\uc815\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ub2e8\uacc4\ubcc4 \ucd5c\uc801\ud654 \uae30\ubc95(\uc608: \ucd08\uae30 \ucee8\ubcfc\ub8e8\uc158 \uac1c\uc120, \uc815\uaddc\ud654 \ubc29\uc2dd \ubcc0\uacbd, \ub124\ud2b8\uc6cc\ud06c \uc2ec\ud654, \ub2e8\uacc4 \ube44\uc728 \uc870\uc815, \ucee4\ub110 \ud06c\uae30 \uc870\uc815, \ub2e8\uc77c \ud5e4\ub4dc \ubcc0\uc870 \uc5b4\ud150\uc158 \uc0ac\uc6a9, \uc704\uce58 \uc784\ubca0\ub529 \ucd94\uac00 \ub4f1)\uc758 \ud6a8\uacfc\ub97c \uc815\ud655\ub3c4\uc640 \uc9c0\uc5f0 \uc2dc\uac04 \ubcc0\ud654\ub97c \ud1b5\ud574 \uc9c1\uad00\uc801\uc73c\ub85c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.15369/x4.png", "caption": "Figure 4: Overview of iFormer architecture, detailed convolutional stem, block design, and SHMA. The hatched area in SHMA indicates extra memory-intensive reshaping operations that are eliminated by SHMA. S\u2062(\u22c5)\ud835\udc46\u22c5S(\\cdot)italic_S ( \u22c5 ) denotes the softmax function. R\ud835\udc45Ritalic_R is the ratio for reducing channels of query and key. It is set to 2 in iFormer. We omit BN following project or convolution for simplicity.", "description": "\uadf8\ub9bc 4\ub294 iFormer \uc544\ud0a4\ud14d\ucc98\uc758 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. iFormer\ub294 \ud6a8\uc728\uc801\uc778 \ubaa8\ubc14\uc77c \ube44\uc804 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud574 \uc124\uacc4\ub41c \ud558\uc774\ube0c\ub9ac\ub4dc \ub124\ud2b8\uc6cc\ud06c\ub85c, \ube60\ub978 \uc9c0\uc5ed\uc801 \ud45c\ud604 \ub2a5\ub825\uc744 \uac16\ucd98 \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd(CNN)\uacfc \ud6a8\uc728\uc801\uc778 \uc804\uc5ed \ubaa8\ub378\ub9c1 \uae30\ub2a5\uc744 \uac16\ucd98 \uc790\uae30 \uc8fc\uc758(self-attention) \uba54\ucee4\ub2c8\uc998\uc744 \ud1b5\ud569\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc740 iFormer\uc758 \uad6c\uc131 \uc694\uc18c\uc778 \uc138\ubd80\uc801\uc778 \ud569\uc131\uacf1 \uc2a4\ud15c, \ube14\ub85d \ub514\uc790\uc778 \ubc0f SHMA(Single-Head Modulation Attention)\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. SHMA\ub294 \uba54\ubaa8\ub9ac \uc9d1\uc57d\uc801\uc778 \uc7ac\uad6c\uc131 \uc5f0\uc0b0\uc744 \uc81c\uac70\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \ub192\uc600\uc73c\uba70, \ucffc\ub9ac\uc640 \ud0a4 \ucc44\ub110 \uc218\ub97c \uc904\uc774\ub294 \ube44\uc728 R(iFormer\uc5d0\uc11c\ub294 2\ub85c \uc124\uc815)\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ub2e8\uc21c\ud654\ub97c \uc704\ud574 \ud504\ub85c\uc81d\ud2b8 \ub610\ub294 \ud569\uc131\uacf1 \uc774\ud6c4\uc758 BN(Batch Normalization)\uc740 \uc0dd\ub7b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc744 \ud1b5\ud574 iFormer\uc758 \uc544\ud0a4\ud14d\ucc98\uc640 \uac01 \uad6c\uc131 \uc694\uc18c\uc758 \uc0c1\ud638 \uc791\uc6a9\uc744 \uc774\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.15369/x5.png", "caption": "Figure 5: Comparison of SHMA and SHA in SHViT. In SHViT, r\u2062C\ud835\udc5f\ud835\udc36rCitalic_r italic_C channels are utilized for spatial attention, where r\ud835\udc5fritalic_r is set to 14.6714.67\\frac{1}{4.67}divide start_ARG 1 end_ARG start_ARG 4.67 end_ARG. SHMA projects the input into a higher dimension of 1212\\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARGC (i.e., R=2) and avoids split and concatenation operations.", "description": "\uadf8\ub9bc 5\ub294 iFormer\uc5d0\uc11c \uc81c\uc548\ud558\ub294 SHMA(Single-Head Modulation Attention)\uc640 SHViT(Single-Head self-Attention)\uc758 \ucc28\uc774\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. SHViT\ub294 \uacf5\uac04\uc801 \uc5b4\ud150\uc158\uc744 \uc704\ud574 \uc785\ub825 \ucc44\ub110\uc758 1/4.67\ub9cc\uc744 \uc0ac\uc6a9\ud558\ub294 \ubc18\uba74, SHMA\ub294 \uc785\ub825\uc744 2\ubc30\uc758 \ucc28\uc6d0\uc73c\ub85c \ud22c\uc601\ud558\uc5ec \ubd84\ud560 \ubc0f \uc5f0\uacb0 \uc5f0\uc0b0 \uc5c6\uc774 \ud6a8\uc728\uc801\uc778 \uc5b4\ud150\uc158\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0\uc11c \uba54\ubaa8\ub9ac\uc640 \uc5f0\uc0b0\ub7c9\uc744 \uc904\uc774\ub294 \ub370 \ud6a8\uacfc\uc801\uc785\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ub450 \ubc29\ubc95\uc758 \uad6c\uc870\uc801 \ucc28\uc774\ub97c \uba85\ud655\ud558\uac8c \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.3 Single Head Modulation Attention"}]