[{"content": "| Models | Latency (ms) | Top-1 Acc. (%) |\n|---|---|---|\n| MHA Baseline | 1.40 | 79.9 |\n| SHA Baseline | 1.12 (1.25\u00d7) | 79.8 |", "caption": "Table 1: Latency comparison between multi-head and single-head baseline.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 3.3\uc808 Single Head Modulation Attention\uc5d0\uc11c \ub2e4\ub8e8\ub294 \ub0b4\uc6a9\uc73c\ub85c, \uba40\ud2f0-\ud5e4\ub4dc \uc5b4\ud150\uc158(MHA) \uae30\ubc18\uc758 \uae30\uc900 \ubaa8\ub378\uacfc \uc2f1\uae00-\ud5e4\ub4dc \uc5b4\ud150\uc158(SHA) \uae30\ubc18\uc758 \uae30\uc900 \ubaa8\ub378 \uac04\uc758 \uc9c0\uc5f0 \uc2dc\uac04(Latency) \ubc0f Top-1 \uc815\ud655\ub3c4(Accuracy)\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  MHA\ub294 \uba40\ud2f0-\ud5e4\ub4dc \uba54\ucee4\ub2c8\uc998\uc73c\ub85c \uc778\ud574 \ucd94\uac00\uc801\uc778 \uc5f0\uc0b0 \ubc0f \uba54\ubaa8\ub9ac \uc811\uadfc\uc774 \ud544\uc694\ud558\uc5ec \uc9c0\uc5f0 \uc2dc\uac04\uc774 \uae38\uc5b4\uc9c0\ub294 \ubc18\uba74, SHA\ub294 \uc5f0\uc0b0\ub7c9\uc774 \uc801\uc5b4 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \ub2e8\ucd95\uc2dc\ud0ac \uc218 \uc788\ub2e4\ub294 \uc810\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "3.3 Single Head Modulation Attention"}, {"content": "| Kernel Size | Latency (ms) |\n|---|---| \n| 3\u00d73 | 1.00 |\n| 7\u00d77 | 1.01 |", "caption": "Table 2: Latency under different convolutional kernel sizes.", "description": "\uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \ud569\uc131\uacf1 \uc5f0\uc0b0 \ucee4\ub110\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \ucee4\ub110 \ud06c\uae30\uc758 \uc601\ud5a5\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  \ud2b9\ud788, \ubaa8\ubc14\uc77c \ud658\uacbd\uc5d0\uc11c\uc758 \uc2e4\uc2dc\uac04 \ucc98\ub9ac \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc704\ud55c \ucd5c\uc801\uc758 \ucee4\ub110 \ud06c\uae30\ub97c \uc120\ud0dd\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3.2 LIGHTENING BASELINE"}, {"content": "| Model | Params (M) | GMACs | Latency (ms) | Reso. | Epochs | Top-1 (%) |\n|---|---|---|---|---|---|---|\n| MobileNetV2 1.0x (2018) | 3.4 | 0.30 | 0.73 | 224 | 500 | 72.0 |\n| MobileNetV3-Large 0.75x (2019) | 4.0 | 0.16 | 0.67 | 224 | 600 | 73.3 |\n| MNV4-Conv-S (2024) | 3.8 | 0.20 | 0.60 | 224 | 500 | 73.8 |\n| **iFormer-T** | **2.9** | **0.53** | **0.60** | **224** | **300** | **74.1** |\n| MobileNetV2 1.4x (2018) | 6.9 | 0.59 | 1.02 | 224 | 500 | 74.7 |\n| MobileNetV3-Large 1.0x (2019) | 5.4 | 0.22 | 0.76 | 224 | 600 | 75.2 |\n| SwiftFormer-XS (2023) | 3.5 | 0.60 | 0.95 | 224 | 300 | 75.7 |\n| SBCFormer-XS (2024) | 5.6 | 0.70 | 0.79 | 224 | 300 | 75.8 |\n| GhostNetV3 1.0x\u2020 (2024) | 6.1 | 0.17 | 0.99 | 224 | 600 | 77.1 |\n| MobileOne-S2 (2023b) | 7.8 | 1.30 | 0.92 | 224 | 300 | 77.4 |\n| RepViT-M1.0 (2024) | 6.8 | 1.10 | 0.85 | 224 | 300 | 78.6 |\n| **iFormer-S** | **6.5** | **1.09** | **0.85** | **224** | **300** | **78.8** |\n| EfficientMod-xxs (2024) | 4.7 | 0.60 | 1.29 | 224 | 300 | 76.0 |\n| SBCFormer-S (2024) | 8.5 | 0.90 | 1.02 | 224 | 300 | 77.7 |\n| MobileOne-S3 (2023b) | 10.1 | 1.90 | 1.16 | 224 | 300 | 78.1 |\n| SwiftFormer-S (2023) | 6.1 | 1.00 | 1.12 | 224 | 300 | 78.5 |\n| GhostNetV3 1.3x\u2020 (2024) | 8.9 | 0.27 | 1.24 | 224 | 600 | 79.1 |\n| FastViT-T12 (2023a) | 6.8 | 1.40 | 1.12 | 256 | 300 | 79.1 |\n| RepViT-M1.1 (2024) | 8.2 | 1.30 | 1.04 | 224 | 300 | 79.4 |\n| MNV4-Conv-M (2024) | 9.2 | 1.00 | 1.08 | 256 | 500 | 79.9 |\n| **iFormer-M** | **8.9** | **1.64** | **1.10** | **224** | **300** | **80.4** |\n| Mobile-Former-294M (2022b) | 11.4 | 0.29 | 2.66 | 224 | 450 | 77.9 |\n| MobileViT-S (2021) | 5.6 | 2.00 | 3.55 | 256 | 300 | 78.4 |\n| MobileOne-S4 (2023b) | 14.8 | 2.98 | 1.74 | 224 | 300 | 79.4 |\n| SBCFormer-B (2024) | 13.8 | 1.60 | 1.44 | 224 | 300 | 80.0 |\n| GhostNetV3 1.6x\u2020 (2024) | 12.3 | 0.40 | 1.49 | 224 | 600 | 80.4 |\n| EfficientViT-B1-r288 (2023) | 9.1 | 0.86 | 3.87 | 288 | 450 | 80.4 |\n| FastViT-SA12 (2023a) | 10.9 | 1.90 | 1.50 | 256 | 300 | 80.6 |\n| MNV4-Hybrid-M (2024) | 10.5 | 1.20 | 1.75 | 256 | 500 | 80.7 |\n| SwiftFormer-L1 (2023) | 12.1 | 1.60 | 1.60 | 224 | 300 | 80.9 |\n| EfficientMod-s (2024) | 12.9 | 1.40 | 2.57 | 224 | 300 | 81.0 |\n| RepViT-M1.5 (2024) | 14.0 | 2.30 | 1.54 | 224 | 300 | 81.2 |\n| **iFormer-L** | **14.7** | **2.63** | **1.60** | **224** | **300** | **81.9** |", "caption": "Table 3: Classification results on ImageNet-1K. \u2020 indicates models that are trained with a variety of advanced training strategies including complex reparameterization, distillation, optimizer, and so on. We provide a more comprehensive comparison in Sec.\u00a0G in the supplementary material.", "description": "\ud45c 3\uc740 ImageNet-1K \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud55c \uc774\ubbf8\uc9c0 \ubd84\ub958 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud30c\ub77c\ubbf8\ud130 \uc218, \uc5f0\uc0b0\ub7c9(GMACS), \uc9c0\uc5f0 \uc2dc\uac04(ms), \uc785\ub825 \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4, \ud559\uc2b5 \uc5d0\ud3ed \uc218, \uadf8\ub9ac\uace0 Top-1 \uc815\ud655\ub3c4(%)\ub97c \uae30\uc900\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.  \u2020 \ud45c\uc2dc\ub294 \uc7ac\ub9e4\uac1c\ubcc0\uc218\ud654, \uc9c0\uc2dd \uc99d\ub958, \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998 \ub4f1 \ub2e4\uc591\ud55c \uace0\uae09 \ud559\uc2b5 \uc804\ub7b5\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ub41c \ubaa8\ub378\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ubcf8 \ub17c\ubb38\uc758 \ubcf4\ucda9 \uc790\ub8cc G\uc808\uc5d0 \ub354 \uc790\uc138\ud55c \ube44\uad50 \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.1 \uc774\ubbf8\uc9c0 \ubd84\ub958"}, {"content": "| Model | Latency (ms) | Reso. | Epochs | Top-1 (%) |\n|---|---|---|---|---|\n| EfficientFormerV2-S1 (2023) | 1.02 | 224 | 300 | 79.0 |\n| EfficientFormerV2-S1 (2023) | 1.02 | 224 | 450 | 79.7 |\n| MobileViGv2-S* (2024) | 1.24 | 224 | 300 | 79.8 |\n| FastViT-T12* (2023a) | 1.12 | 256 | 300 | 80.3 |\n| RepViT-M1.1* (2024) | 1.04 | 224 | 300 | 80.7 |\n| **iFormer-M** | **1.10** | 224 | 300 | **81.1** |\n| SHViT-S4 (2024) | 1.48 | 224 | 300 | 80.2 |\n| EfficientFormerV2-S2 (2023) | 1.60 | 224 | 300 | 81.6 |\n| MobileViGv2-M (2024) | 1.70 | 224 | 300 | 81.7 |\n| FastViT-SA12* (2023a) | 1.50 | 256 | 300 | 81.9 |\n| EfficientFormerV2-S2 (2023) | 1.60 | 224 | 450 | 82.0 |\n| RepViT-M1.5* (2024) | 1.54 | 224 | 300 | 82.3 |\n| **iFormer-L** | **1.60** | 224 | 300 | **82.7** |", "caption": "Table 4: Results with distillation on ImageNet-1K. * indicates the model is trained with a strong training strategy (i.e., reparameterization).", "description": "\ud45c 4\ub294 ImageNet-1K \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc9c0\uc2dd \uc99d\ub958 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub41c \ubaa8\ub378\ub4e4\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \ubaa8\ub378\uc758 \uc9c0\uc5f0\uc2dc\uac04(ms), \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4, \uc5d0\ud3ed \uc218, \uadf8\ub9ac\uace0 Top-1 \uc815\ud655\ub3c4(%)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  '*' \ud45c\uc2dc\ub294 \uc7ac\ub9e4\uac1c\ubcc0\uc218\ud654\uc640 \uac19\uc740 \uac15\ub825\ud55c \ud6c8\ub828 \uc804\ub7b5\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub41c \ubaa8\ub378\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uacbd\ub7c9\ud654 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uace0 \uc9c0\uc2dd \uc99d\ub958\uac00 \ubaa8\ub378 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "4.1 \uc774\ubbf8\uc9c0 \ubd84\ub958"}, {"content": "| Backbone | Param (M) | Latency (ms) | Object Detection AP<sup>box</sup> | AP<sup>box</sup><sub>50</sub> | AP<sup>box</sup><sub>75</sub> | Instance Segmentation AP<sup>mask</sup> | AP<sup>mask</sup><sub>50</sub> | AP<sup>mask</sup><sub>75</sub> | Semantic mIoU |\n|---|---|---|---|---|---|---|---|---|---| \n| EfficientNet-B0 [2019] | 5.3 | 4.55 | 31.9 | 51.0 | 34.5 | 29.4 | 47.9 | 31.2 | - |\n| ResNet18 [2016] | 11.7 | 2.85 | 34.0 | 54.0 | 36.7 | 31.2 | 51.0 | 32.7 | 32.9 |\n| PoolFormer-S12 [2022] | 11.9 | 5.70 | 37.3 | 59.0 | 40.1 | 34.6 | 55.8 | 36.9 | 37.2 |\n| EfficientFormer-L1 [2022b] | 12.3 | 3.50 | 37.9 | 60.3 | 41.0 | 35.4 | 57.3 | 37.3 | 38.9 |\n| FastViT-SA12 [2023a] | 10.9 | 5.27 | 38.9 | 60.5 | 42.2 | 35.9 | 57.6 | 38.1 | 38.0 |\n| RepViT-M1.1 [2024] | 8.2 | 3.18 | 39.8 | 61.9 | 43.5 | 37.2 | 58.8 | 40.1 | 40.6 |\n| iFormer-M | 8.9 | 4.00 | 40.8 | 62.5 | 44.8 | 37.9 | 59.7 | 40.7 | 42.4 |\n| ResNet50 [2016] | 25.5 | 7.20 | 38.0 | 58.6 | 41.4 | 34.4 | 55.1 | 36.7 | 36.7 |\n| PoolFormer-S24 [2022] | 21.4 | 10.0 | 40.1 | 62.2 | 43.4 | 37.0 | 59.1 | 39.6 | 40.3 |\n| ConvNeXt-T [2022] | 29.0 | 13.6 | 41.0 | 62.1 | 45.3 | 37.7 | 59.3 | 40.4 | 41.4 |\n| EfficientFormer-L3 [2022b] | 31.3 | 8.40 | 41.4 | 63.9 | 44.7 | 38.1 | 61.0 | 40.4 | 43.5 |\n| RepViT-M1.5 [2024] | 14.0 | 5.00 | 41.6 | 63.2 | 45.3 | 38.6 | 60.5 | 41.5 | 43.6 |\n| PVTv2-B1 [2022] | 14.0 | 27.00 | 41.8 | 64.3 | 45.9 | 38.8 | 61.2 | 41.6 | 42.5 |\n| FastViT-SA24 [2023a] | 20.6 | 8.97 | 42.0 | 63.5 | 45.8 | 38.0 | 60.5 | 40.5 | 41.0 |\n| EfficientMod-S [2024] | 32.6 | 24.30 | 42.1 | 63.6 | 45.9 | 38.5 | 60.8 | 41.2 | 43.5 |\n| Swin-T [2021a] | 28.3 | Failed | 42.2 | 64.4 | 46.2 | 39.1 | 61.6 | 42.0 | 41.5 |\n| iFormer-L | 14.7 | 6.60 | 42.2 | 64.2 | 46.0 | 39.1 | 61.4 | 41.9 | 44.5 |", "caption": "Table 5: \nObject detection & instance segmentation results on MS COCO 2017 using Mask R-CNN.\nSemantic segmentation results on ADE20K using the Semantic FPN framework. We measure all backbone latencies with image crops of 512\u00d7\\times\u00d7512 on iPhone 13 by Core ML Tools. Failed indicated that the model runs too long to report latency by the Core ML.", "description": "\ud45c 5\ub294 Mask R-CNN\uc744 \uc0ac\uc6a9\ud558\uc5ec MS COCO 2017 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc218\ud589\ub41c \uac1d\uccb4 \ud0d0\uc9c0 \ubc0f \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560 \uacb0\uacfc\uc640 Semantic FPN \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec ADE20K \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc218\ud589\ub41c \uc758\ubbf8 \ubd84\ud560 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubc31\ubcf8\uc758 \uc9c0\uc5f0 \uc2dc\uac04\uc740 iPhone 13\uc5d0\uc11c Core ML \ud234\uc744 \uc0ac\uc6a9\ud558\uc5ec 512x512 \ud06c\uae30\uc758 \uc774\ubbf8\uc9c0\ub97c \ucc98\ub9ac\ud558\ub294 \uc2dc\uac04\uc73c\ub85c \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4. 'Failed'\ub294 Core ML\uc5d0\uc11c \ubaa8\ub378 \uc2e4\ud589 \uc2dc\uac04\uc774 \ub108\ubb34 \uae38\uc5b4 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \ubcf4\uace0\ud560 \uc218 \uc5c6\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4.2 \uac1d\uccb4 \ud0d0\uc9c0 \ubc0f \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560"}, {"content": "| SHMA Setting | Params (M) | GMACs | Latency (ms) | Top-1 Acc. (%) |\n|---|---|---|---|---|\n| SiLU + Post-BN | 8.9 | 1.60 | 1.10ms | Diverged |\n| SiLU + Pre-LN | 8.9 | 1.64 | 1.17ms | 80.3 |\n| Sigmoid + Post-BN | 8.9 | 1.60 | 1.10ms | 80.4 |", "caption": "Table 6: Activation function comparison in SHMA. Post-BN indicates that BN is applied after projection. Pre-LN means that LN is implemented before the projection, as in standard MHA\u00a0(Vaswani, 2017).", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 5\uc7a5(Ablation Studies)\uc5d0\uc11c SHMA(Single-Head Modulation Attention)\uc758 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \ud65c\uc131\ud654 \ud568\uc218\uc758 \uc601\ud5a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Post-BN\uc740 \ud22c\uc601 \ud6c4 \ubc30\uce58 \uc815\uaddc\ud654(BN)\ub97c \uc801\uc6a9\ud55c \uacbd\uc6b0\uc774\uace0, Pre-LN\uc740 \ud45c\uc900 MHA(Vaswani, 2017)\ucc98\ub7fc \ud22c\uc601 \uc804 \ub808\uc774\uc5b4 \uc815\uaddc\ud654(LN)\ub97c \uc801\uc6a9\ud55c \uacbd\uc6b0\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud65c\uc131\ud654 \ud568\uc218(SiLU, Sigmoid)\uc640 BN/LN \uc801\uc6a9 \uc704\uce58\uc758 \uc870\ud569\uc5d0 \ub530\ub978 \ubaa8\ub378\uc758 \uc131\ub2a5(\ub9e4\uac1c\ubcc0\uc218 \uc218, GMACs, \uc9c0\uc5f0 \uc2dc\uac04, Top-1 \uc815\ud655\ub3c4)\uc744 \ube44\uad50\ud558\uc5ec \ucd5c\uc801\uc758 \uc124\uc815\uc744 \ucc3e\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "5 ABLATION STUDIES"}, {"content": "| Ratio Setting | Params (M) | GMACs | Latency (ms) | Top-1 Acc. (%) |\n|---|---|---|---|---|\n| Baseline | 9.4M | 1760M | 1.0ms | 79.4 |\n| Replacing 22% Conv Blocks in Stage 3 as SHA | 9.1M | 1724M | 1.02ms | 79.5 |\n| Replacing 22% Conv Blocks in Stage 3 as SHMA | 9.2M | 1739M | 1.04ms | 79.6 |\n| Replacing 50% Conv Blocks in Stage 3 as SHA | 8.8M | 1689M | 1.04ms | 79.5 |\n| Replacing 50% Conv Blocks in Stage 3 as SHMA | 8.9M | 1712M | 1.07ms | 79.8 |\n| Replacing 78% Conv Blocks in Stage 3 as SHA | 8.3M | 1635M | 1.12ms | 79.3 |\n| Replacing 78% Conv Blocks in Stage 3 as SHMA | 8.5M | 1685M | 1.17ms | 79.6 |\n| Replacing 100% Conv Blocks in Stage 3 as SHA | 7.9M | 1599M | 1.17ms | 78.1 |\n| Replacing 100% Conv Blocks in Stage 3 as SHMA | 8.3M | 1665M | 1.25ms | 79.0 |\n| Replacing 100% Conv Blocks in Stage 3 as SHMA and 100% in Stage 4 | 10.0M | 1792M | 1.15ms | 80.4 |", "caption": "Table 7: Different ratio of ViT Block.", "description": "\uc774 \ud45c\ub294 iFormer \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \ub300\ud55c \ube44\uad50 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, ConvNeXt \uae30\ubc18 \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c ViT \ube14\ub85d\uc758 \ube44\uc728\uc744 \ub2e4\ub974\uac8c \uc801\uc6a9\ud588\uc744 \ub54c\uc758 \uc815\ud655\ub3c4\uc640 \uc9c0\uc5f0 \uc2dc\uac04\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ConvNeXt \ub124\ud2b8\uc6cc\ud06c\uc758 \uc77c\ubd80 \ub610\ub294 \uc804\ubd80\ub97c \uc0c8\ub85c \uc81c\uc548\ub41c SHMA \ube14\ub85d\uc73c\ub85c \ub300\uccb4\ud558\uc5ec \ub124\ud2b8\uc6cc\ud06c\uc758 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uc2e4\ud5d8 \uacb0\uacfc\uac00 \ub2f4\uaca8 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uc5f4\uc740 ViT \ube14\ub85d \ub300\uccb4 \ube44\uc728, \ubaa8\ub378 \ud06c\uae30, \uc5f0\uc0b0\ub7c9, \uc9c0\uc5f0 \uc2dc\uac04, \uadf8\ub9ac\uace0 ImageNet-1K \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 Top-1 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ube44\uc728\uc758 ViT \ube14\ub85d\uc744 \uc0ac\uc6a9\ud558\ub294 iFormer \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uacfc \uc815\ud655\ub3c4 \uac04\uc758 \uade0\ud615\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3.3 Single Head Modulation Attention"}, {"content": "| Model | Params (M) | GMACs (G) | Top-1 Acc. (%) |\n|---|---|---|---|\n| ConvNeXt-Base [2022] | 89 | 15.4 | 83.8 |\n| TransNeXt-Base [2024] | 90 | 18.4 | 84.8 |\n| iFormer-H (ours) | 99 | 15.5 | 84.8 |\n| MaxViT-Base [2022] | 120 | 24.0 | 84.9 |", "caption": "Table 8: Scaling to the larger model with 99M parameters.", "description": "\ud45c 8\uc740 \ub9e4\uac1c\ubcc0\uc218 99M\uac1c\ub97c \uac00\uc9c4 \ub354 \ud070 \ubaa8\ub378\ub85c \ud655\uc7a5\ud558\ub294 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\uc874\uc758 \uacbd\ub7c9 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud558\uc5ec iFormer-H \ubaa8\ub378\uc758 \uc131\ub2a5\uacfc \ud6a8\uc728\uc131\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.  \ud2b9\ud788, \uacc4\uc0b0\ub7c9(GMACS) \ub300\ube44 \uc815\ud655\ub3c4(Top-1 Accuracy)\ub97c \ube44\uad50\ud558\uc5ec iFormer\uc758 \ud655\uc7a5\uc131 \ubc0f \uc131\ub2a5\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.1 \uc774\ubbf8\uc9c0 \ubd84\ub958"}, {"content": "| Training Config             | iFormer-T/S/M/L/H |\n|------------------------------|------------------|\n| Resolution                  | 224<sup>2</sup>     |\n| Weight Init                 | trunc. normal (0.2) |\n| Optimizer                   | AdamW              |\n| Base Learning Rate          | 4e-3 (T/S/M/L) 8e-3 |\n| Weight Decay                | 0.05               |\n| Optimizer Momentum          | \u03b2<sub>1</sub>,\u03b2<sub>2</sub>=0.9,0.999 |\n| Batch Size                  | 4096 [T/S/M/L] 8192 [H] |\n| Training Epochs             | 300                |\n| Learning Rate Schedule      | cosine decay       |\n| Warmup Epochs               | 20                 |\n| Warmup Schedule             | linear             |\n| Layer-wise LR Decay         | None                |\n| Randaugment                 | (9, 0.5)           |\n| Mixup                       | 0.8                |\n| Cutmix                      | 1.0                |\n| Random Erasing              | 0.25               |\n| Label Smoothing             | 0.1                |\n| Stochastic Depth            | 0.0 [T/S/M] 0.1 [L] 0.6 [H] |\n| Layer Scale                 | None [T/S/M/L] 1e-6 [H] |\n| Head Init Scale             | None                |\n| Gradient Clip               | None                |\n| Exp. Mov. Avg. (EMA)       | None                |", "caption": "Table 9: ImageNet-1K training settings.", "description": "\ud45c 9\ub294 \ub17c\ubb38\uc758 \uc2e4\ud5d8 \uc124\uc815 \ubd80\ubd84\uc5d0\uc11c ImageNet-1K \ub370\uc774\ud130\uc14b\uc744 \uc774\uc6a9\ud55c \uc774\ubbf8\uc9c0 \ubd84\ub958 \uc791\uc5c5\uc5d0 \ub300\ud55c \ud559\uc2b5 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \ud559\uc2b5 \ud574\uc0c1\ub3c4, \uac00\uc911\uce58 \ucd08\uae30\ud654 \ubc29\ubc95, \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998, \ud559\uc2b5\ub960, \uac00\uc911\uce58 \uac10\uc1e0, \ubaa8\uba58\ud140, \ubc30\uce58 \ud06c\uae30, \uc5d0\ud3ed \uc218, \ud559\uc2b5\ub960 \uc870\uc808 \ubc29\uc2dd, \uc6dc\uc5c5 \uc5d0\ud3ed, \ub370\uc774\ud130 \uc99d\uac15 \uae30\ubc95(RandAugment, Mixup, CutMix, Random Erasing), \ub808\uc774\ube14 \uc2a4\ubb34\ub529, \ud655\ub960\uc801 \uc2ec\uce35\ud654, \uacc4\uce35\ubcc4 \uc2a4\ucf00\uc77c\ub9c1, \ud5e4\ub4dc \ucd08\uae30\ud654 \uc2a4\ucf00\uc77c, \uadf8\ub798\ub514\uc5b8\ud2b8 \ud074\ub9ac\ud551, \uc9c0\uc218 \uc774\ub3d9 \ud3c9\uade0(EMA) \ub4f1\uc758 \uc138\ubd80\uc801\uc778 \ud559\uc2b5 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c ImageNet-1K \uc774\ubbf8\uc9c0 \ubd84\ub958 \ubaa8\ub378 \ud559\uc2b5 \uacfc\uc815\uc744 \uba85\ud655\ud788 \uc774\ud574\ud558\ub294\ub370 \ud544\uc218\uc801\uc778 \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4.1 \uc774\ubbf8\uc9c0 \ubd84\ub958"}, {"content": "| Reducing Setting | Params (M) | GMACs | Latency (ms) | Top-1 Acc. (%) |\n|---|---|---|---|---|\n| Baseline | 10.0 | 1.79 | 1.15 | 80.4 |\n| Number of Blocks | 8.4 | 1.70 | 1.07 | 79.7 |\n| FFN Width | 8.6 | 1.62 | 1.07 | 79.8 |\n| Attn. Head and FFN Width | 8.9 | 1.64 | 1.10 | 80.2 |", "caption": "Table 10: Different ways for reducing latency.", "description": "\ubcf8 \ud45c\ub294 \ubaa8\ub378\uc758 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \uc904\uc774\uae30 \uc704\ud55c \uc11c\ub85c \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\uc900 \ubaa8\ub378(Baseline)\uc744 \ubc14\ud0d5\uc73c\ub85c \ube14\ub85d \uac1c\uc218, FFN(Feed-Forward Network) \ub108\ube44, \uc5b4\ud150\uc158 \ud5e4\ub4dc \uc218, FFN \ud655\uc7a5 \ucc28\uc6d0 \ub4f1\uc744 \ubcc0\ud654\uc2dc\ucf1c\uac00\uba70 \uc131\ub2a5\uacfc \uc9c0\uc5f0 \uc2dc\uac04\uc758 \ubcc0\ud654\ub97c \uce21\uc815\ud558\uc600\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378 \uacbd\ub7c9\ud654 \uacfc\uc815\uc5d0\uc11c \uc5b4\ub5a4 \uc694\uc18c\ub4e4\uc774 \uc9c0\uc5f0 \uc2dc\uac04\uc5d0 \uac00\uc7a5 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0, \uadf8\ub9ac\uace0 \uc131\ub2a5 \uc800\ud558\ub97c \ucd5c\uc18c\ud654\ud558\uba74\uc11c \ud6a8\uc728\uc801\uc73c\ub85c \uc9c0\uc5f0 \uc2dc\uac04\uc744 \uc904\uc774\ub294 \ubc29\ubc95\uc740 \ubb34\uc5c7\uc778\uc9c0\ub97c \uc54c\uc544\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.2 LIGHTENING BASELINE"}, {"content": "| DW Conv in FFN | Params (M) | GMACs | Latency (ms) | Top-1 Acc. (%) |\n|---|---|---|---|---|\n| with | 9.6 | 1.83 | 1.43 | 80.5 |\n| w/o. | 8.9 | 1.60 | 1.10 | 80.4 |", "caption": "Table 11: Comparison of FFN with and without depthwise convolution.", "description": "\ud45c 11\uc740 FFN(Feed-Forward Network)\uc5d0 depthwise convolution\uc744 \uc801\uc6a9\ud588\uc744 \ub54c\uc640 \uc801\uc6a9\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  depthwise convolution\uc744 \uc801\uc6a9\ud558\uba74 \uc5f0\uc0b0\ub7c9(FLOPs)\uc774 14% \uc99d\uac00\ud558\uace0 \uc9c0\uc5f0 \uc2dc\uac04\uc774 0.33ms \uc99d\uac00\ud558\uc9c0\ub9cc, \uc815\ud655\ub3c4 \ud5a5\uc0c1\uc740 \ubbf8\ubbf8\ud569\ub2c8\ub2e4. \uc774\ub294 depthwise convolution\uc774 FFN\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ud06c\uac8c \uae30\uc5ec\ud558\uc9c0 \uc54a\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3.2 LIGHTENING BASELINE"}, {"content": "| Model | Params (M) | Latency (ms) | Reso. | Epochs | Top-1 (%) |\n|---|---|---|---|---|---| \n| ConvNeXt-B (2022) | 89.0 | 7.54 | 224 | 300 | 83.8 |\n| EfficientFormerV2-L (2023) | 26.1 | 2.40 | 224 | 450 | 83.5 |\n| **iFormer-L2** | **24.5** | **2.30** | 224 | 450 | **83.9** |", "caption": "Table 12: Training with distillation for 450 epochs on ImageNet-1K.", "description": "\uc774 \ud45c\ub294 ImageNet-1K \ub370\uc774\ud130\uc14b\uc5d0\uc11c 450 \uc5d0\ud3ed \ub3d9\uc548 \uc99d\ub958(distillation) \uae30\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ud55c \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378\uc758 \ud30c\ub77c\ubbf8\ud130 \uc218, \uc9c0\uc5f0 \uc2dc\uac04(latency), \ud574\uc0c1\ub3c4, \uc5d0\ud3ed \uc218, \uadf8\ub9ac\uace0 Top-1 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec \ubaa8\ub378\uc758 \ud6a8\uc728\uc131\uacfc \uc815\ud655\ub3c4\ub97c \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.  ConvNeXt-B, EfficientFormerV2-L\uacfc iFormer-L2 \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uc5b4 \uc11c\ub85c \ub2e4\ub978 \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\uc758 450 \uc5d0\ud3ed \ud6c8\ub828 \uacb0\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4.", "section": "4.1 \uc774\ubbf8\uc9c0 \ubd84\ub958"}, {"content": "| Backbone | Param (M) | Latency (ms) | Pretrain Epochs | AP<sup>box</sup> | AP<sup>box</sup><sub>50</sub> | AP<sup>box</sup><sub>75</sub> | AP<sup>mask</sup> | AP<sup>mask</sup><sub>50</sub> | AP<sup>mask</sup><sub>75</sub> | Semantic mIoU |\n|---|---|---|---|---|---|---|---|---|---|---|\n| ResNet50 (2016) | 25.5 | 7.20 | 300 | 38.0 | 58.6 | 41.4 | 34.4 | 55.1 | 36.7 | 36.7 |\n| PoolFormer-S24 (2022) | 21.4 | 12.30 | 300 | 40.1 | 62.2 | 43.4 | 37.0 | 59.1 | 39.6 | 40.3 |\n| ConvNeXt-T (Liu et al., 2022) | 29.0 | 12.6 | 300 | 41.0 | 62.1 | 45.3 | 37.7 | 59.3 | 40.4 | 41.4 |\n| EfficientFormer-L3 (2022b) | 31.3 | 8.40 | 300 | 41.4 | 63.9 | 44.7 | 38.1 | 61.0 | 40.4 | 43.5 |\n| RepViT-M1.5 (2024) | 14.0 | 5.00 | 300 | 41.6 | 63.2 | 45.3 | 38.6 | 60.5 | 41.5 | 43.6 |\n| PVTv2-B1 (2022) | 14.0 | 27.00 | 300 | 41.8 | 64.3 | 45.9 | 38.8 | 61.2 | 41.6 | 42.5 |\n| FastViT-SA24 (2023a) | 20.6 | 8.97 | 300 | 42.0 | 63.5 | 45.8 | 38.0 | 60.5 | 40.5 | 41.0 |\n| EfficientMod-S (2024) | 32.6 | 24.30 | 300 | 42.1 | 63.6 | 45.9 | 38.5 | 60.8 | 41.2 | 43.5 |\n| Swin-T (2021a) | 28.3 | Failed | 300 | 42.2 | 64.4 | 46.2 | 39.1 | 61.6 | 42.0 | 41.5 |\n| iFormer-L | 14.7 | 6.60 | 300 | 42.2 | 64.2 | 46.0 | 39.1 | 61.4 | 41.9 | 44.5 |\n| EfficientFormerV2-L (2023) | 26.1 | 12.5 | 450 | 44.7 | 66.3 | 48.8 | 40.4 | 63.5 | 43.2 | 45.2 |\n| iFormer-L2 | 24.5 | 9.06 | 450 | 44.6 | 66.7 | 49.1 | 41.1 | 64.0 | 44.1 | 46.2 |", "caption": "Table 13: \nObject detection & Semantic segmentation results using backbone pretrained for 450 epochs.", "description": "\ubcf8 \ud45c\ub294 ImageNet-1K \ub370\uc774\ud130\uc14b\uc5d0\uc11c 450 \uc5d0\ud3ed \ub3d9\uc548 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubc31\ubcf8 \ub124\ud2b8\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec Mask R-CNN \ubc0f Semantic FPN \ud504\ub808\uc784\uc6cc\ud06c\ub85c \uc218\ud589\ub41c \uac1d\uccb4 \ud0d0\uc9c0 \ubc0f \uc758\ubbf8 \ubd84\ud560 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubc31\ubcf8 \ubaa8\ub378\uc5d0 \ub300\ud55c \uc815\ubc00\ub3c4(APbox, APmask) \ubc0f mIoU \uc9c0\ud45c\uc640 \ud568\uaed8 iPhone 13 \uae30\uae30\uc5d0\uc11c \uce21\uc815\ub41c \ubc31\ubcf8\uc758 \ucc98\ub9ac \uc18d\ub3c4(Latency)\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uc774\ub294 \ub2e4\uc591\ud55c \uacbd\ub7c9\ud654 \ubc31\ubcf8 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4.2 \uac1d\uccb4 \ud0d0\uc9c0 \ubc0f \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560"}, {"content": "| Modification | Params(M) | GMACs | Latency (ms) | Top-1(%) |\n|---|---|---|---|---|\n| SHA Baseline without Modulation | 9.9M | 1758M | 1.12ms | 79.4 |\n| + split | 9.9M | 1758M | 1.18ms | - |\n| + attention on 1/4 channels | 8.3M | 1547M | 1.02ms | - |\n| + concat | 8.7M | 1579M | 1.11ms | 79.5 |", "caption": "Table 14: Process of converting SHA in iFormer towards SHViT. Intermediate models are only measured by latency.", "description": "\uc774 \ud45c\ub294 iFormer\uc5d0\uc11c \uc0ac\uc6a9\ub41c SHA(Single-Head self-Attention)\ub97c SHViT(Single-Head Vision Transformer)\uc758 SHA\uc640 \ube44\uad50\ud558\uc5ec \ubcc0\ud658 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub2e8\uacc4\ubcc4\ub85c \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218, GMAC(giga multiply-accumulate operations), \uc9c0\uc5f0 \uc2dc\uac04(ms), \uadf8\ub9ac\uace0 ImageNet-1k \uc0c1\uc704 1% \uc815\ud655\ub3c4\ub97c \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4.  \uc911\uac04 \ubaa8\ub378\uc758 \uacbd\uc6b0 \uc9c0\uc5f0 \uc2dc\uac04\ub9cc \uce21\uc815\ud558\uc600\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 SHA\uc758 \ud6a8\uc728\uc131\uacfc SHViT\uc640\uc758 \ucc28\uc774\uc810\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ubd84\uc11d\ud558\uc5ec iFormer\uc758 \uc124\uacc4 \ubc29\uc2dd\uc744 \ubcf4\ub2e4 \uba85\ud655\ud558\uac8c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "3.3 Single Head Modulation Attention"}, {"content": "Stem | Output Size (Downs. Rate) | iFormer-T | iFormer-S | iFormer-M | iFormer-L\n---|---|---|---|---|---\nStem | 56\u00d756 (4\u00d7) | [Conv-BN-GELU 5\u00d75 s2 d16] \u00d7 1 | [Conv-BN-GELU 5\u00d75 s2 d16] \u00d7 1 | [Conv-BN-GELU 5\u00d75 s2 d24] \u00d7 1 | [Conv-BN-GELU 5\u00d75 s2 d24] \u00d7 1\n |  | [Conv-BN 5\u00d75 s2 d64]\n[Conv-BN 1\u00d71 s1 d32] \u00d7 1 | [Conv-BN 5\u00d75 s2 d64]\n[Conv-BN 1\u00d71 s1 d32] \u00d7 1 | [Conv-BN 5\u00d75 s2 d96]\n[Conv-BN 1\u00d71 s1 d48] \u00d7 1 | [Conv-BN 5\u00d75 s2 d96]\n[Conv-BN 1\u00d71 s1 d48] \u00d7 1\nStage 1 | 56\u00d756 (4\u00d7) | [Conv-BN 7\u00d77 s1 d32]\n[Conv-BN-GELU 1\u00d71 s1 d96]\nConv-BN 1x1 s1 d32 \u00d7 2 | [Conv-BN 7\u00d77 s1 d32]\n[Conv-BN-GELU 1\u00d71 s1 d128]\nConv-BN 1x1 s1 d32 \u00d7 2 | [Conv-BN 7\u00d77 s1d48]\n[Conv-BN-GELU 1\u00d71 s1 d192]\nConv-BN 1x1 s1 d48 \u00d7 2 | [Conv-BN 7\u00d77 s1 d48]\n[Conv-BN-GELU 1\u00d71 s1 d192]\nConv-BN 1x1 s1 d48 \u00d7 2\nStage 2 | 28\u00d728 (8\u00d7) | [Conv-BN 3\u00d73 s2 d64] \u00d7 1 | [Conv-BN 3\u00d73 s2 d64] \u00d7 1 | [Conv-BN 7\u00d77 s1 d64]\n[Conv-BN-GELU 1\u00d71 s1 d192]\nConv-BN 1x1 s1 d64 \u00d7 2 | [Conv-BN 7\u00d77 s1 d64]\n[Conv-BN-GELU 1\u00d71 s1 d256]\nConv-BN 1x1 s1 d64 \u00d7 2\nStage 3 | 14\u00d714 (16\u00d7) | [Conv-BN 3\u00d73 s2 d128] \u00d7 1 | [Conv-BN 3\u00d73 s2 d176] \u00d7 1 | [Conv-BN 7\u00d77 s1 d128]\n[Conv-BN-GELU 1\u00d71 s1 d384]\nConv-BN 1\u00d71 s1 d128 \u00d7 6 | [Conv-BN 7\u00d77 s1 d176]\n[Conv-BN-GELU 1\u00d71 s1 d704]\nConv-BN 1x1 s1 d176 \u00d7 9\nStage 4 | 7\u00d77 (32\u00d7) | [Conv-BN 3\u00d73 s2 d256] \u00d7 1 | [Conv-BN 3\u00d73 s2 d320] \u00d7 1 | [Conv-BN 3\u00d73 s2 d384] \u00d7 1 | [Conv-BN 3\u00d73 s2 d384] \u00d7 1\n |  | [CPE 3\u00d73]\nSHMA hd64\nFFN r2 \u00d7 2 | [CPE 3\u00d73]\nSHMA hd80\nFFN r3 \u00d7 2 | [CPE 3\u00d73]\nSHMA hd96\nFFN r3 \u00d7 2 | [CPE 3\u00d73]\nSHMA hd128\nFFN r3 \u00d7 8\nParams (M) |  | 2.9 | 6.5 | 8.9 | 14.7\nGMacs |  | 0.53 | 1.09 | 1.64 | 2.63", "caption": "Table 15: iFormer architecture configurations. BN stands for Batch Normalization. SHMA stands for Singe-Head Modulation Attention. DW stands for Depthwise convolution. s and d means the stride and output dimension in convolution. hd denotes the head dimension in SHMA and the number of attention heads in all variants is 1. r means the expansion ratio in FFN.", "description": "\ud45c 15\ub294 \ub17c\ubb38\uc758 iFormer \uc544\ud0a4\ud14d\ucc98 \uad6c\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uad6c\uc131\uc694\uc18c\uc758 \uc138\ubd80 \uc815\ubcf4\uc640 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc0c1\uc138\ud55c \ud45c\uc785\ub2c8\ub2e4.  BN(\ubc30\uce58 \uc815\uaddc\ud654), SHMA(\ub2e8\uc77c \ud5e4\ub4dc \ubcc0\uc870 \uc5b4\ud150\uc158), DW(\uae4a\uc774 \ubc29\ud5a5 \ud569\uc131\uacf1)\uacfc \uac19\uc740 \uc57d\uc5b4\uac00 \uc0ac\uc6a9\ub418\uba70, \ud569\uc131\uacf1 \uc5f0\uc0b0\uc758 stride\uc640 output \ucc28\uc6d0(s\uc640 d), SHMA\uc5d0\uc11c\uc758 head \ucc28\uc6d0(hd), \uadf8\ub9ac\uace0 FFN(\ud53c\ub4dc \ud3ec\uc6cc\ub4dc \ub124\ud2b8\uc6cc\ud06c)\uc5d0\uc11c\uc758 \ud655\uc7a5 \ube44\uc728(r) \ub4f1\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ubaa8\ub4e0 \ubcc0\ud615\uc5d0\uc11c \uc5b4\ud150\uc158 \ud5e4\ub4dc\uc758 \uc218\ub294 1\uac1c\uc774\uba70, \uac01 \ubcc0\ud615\uc758 \uc8fc\uc694 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\uac00 \uba85\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3 METHOD"}, {"content": "| Output Size | (Downs. Rate) |\n|---|---|", "caption": "Table 16: Comparison of different attention designs in iFormer-M. For the sake of simplicity, we exclude other blocks that are not related to attention. ws is the window size for window attention.", "description": "\ud45c 16\uc740 iFormer-M\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uc11c\ub85c \ub2e4\ub978 \uc5b4\ud150\uc158 \uad6c\uc870\ub97c \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. \ub2e8\uc21c\ud654\ub97c \uc704\ud574 \uc5b4\ud150\uc158\uacfc \uad00\ub828 \uc5c6\ub294 \ub2e4\ub978 \ube14\ub85d\ub4e4\uc740 \uc81c\uc678\ud588\uc2b5\ub2c8\ub2e4. ws\ub294 \uc708\ub3c4\uc6b0 \uc5b4\ud150\uc158\uc758 \uc708\ub3c4\uc6b0 \ud06c\uae30\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\uc5d0\uc11c\ub294 iFormer-M\uc758 Stage 3\uacfc Stage 4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uae30\ubcf8 SHMA(Single-Head Modulation Attention),  SHMA\uc5d0 \uc704\uce58 \uc815\ubcf4\ub97c \ucd94\uac00\ud55c Hybrid SHMA, \uadf8\ub9ac\uace0 SHMA\uc5d0 \uc708\ub3c4\uc6b0 \ubc29\uc2dd\uc744 \ucd94\uac00\ud55c Chunk Hybrid SHMA\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uac01 \uad6c\uc870\uc758 \ud2b9\uc9d5\uacfc \uc7a5\ub2e8\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uad6c\uc870\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218, \uc5f0\uc0b0\ub7c9, \uc9c0\uc5f0 \uc2dc\uac04, \uadf8\ub9ac\uace0 ImageNet-1K \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec \uc5b4\ub5a4 \uc5b4\ud150\uc158 \uad6c\uc870\uac00 iFormer-M\uc5d0 \uac00\uc7a5 \uc801\ud569\ud55c\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.3 Single Head Modulation Attention"}, {"content": "Stage 3|Attention|SHMA|Hybrid SHMA|Chunk Hybrid SHMA\n---|---|---|---|---\n|14x14 (16x)|[CPE 3x3, Window Partitioning, ws16, Window SHMA hd96, ws16, FFN r3] x 1|[CPE 3x3, Chunk Window Partitioning, ws16, Window SHMA hd96, ws16, FFN r3] x 1\n|[CPE 3x3, SHMA hd96, FFN r3] x 4|[CPE 3x3, Window SHMA hd96, ws16, FFN r3] x 2|[CPE 3x3, Window SHMA hd96, ws16, FFN r3] x 2\n|[CPE 3x3, Window Reversing, ws16, SHMA hd96, FFN r3] x 1|[CPE 3x3, Chunk Window Reversing, ws16, SHMA hd96, FFN r3] x 1\nStage 4|Attention|SHMA|Hybrid SHMA|Chunk Hybrid SHMA\n---|---|---|---|---\n|7x7 (32x)|[CPE 3x3, Window Partitioning, ws16, Window SHMA hd96, FFN r3] x 1|[CPE 3x3, Chunk Window Partitioning, ws16, Window SHMA hd96, FFN r3] x 1\n|[CPE 3x3, SHMA hd64, FFN r2] x 2|[CPE 3x3, Window Reversing, ws16, SHMA hd64, FFN r3] x 1|[CPE 3x3, Chunk Window Reversing, ws16, SHMA hd64, FFN r3] x 1", "caption": "Table 17: Latency comparison of different attention mechanisms.", "description": "\ud45c 17\uc740 \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\ub4e4\uc758 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  \ub2e8\uc21c\ud788 \uc9c0\uc5f0 \uc2dc\uac04\ub9cc \ube44\uad50\ud558\ub294 \uac83\uc774 \uc544\ub2c8\ub77c,  \ub2e4\uc591\ud55c \uc5b4\ud150\uc158 \ubc29\uc2dd(SHMA, Hybrid SHMA, Chunk Hybrid SHMA)\uc744 512x512 \ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0\uc5d0 \uc801\uc6a9\ud588\uc744 \ub54c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uac01 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \ud6a8\uc728\uc131\uacfc \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0 \ucc98\ub9ac\uc5d0 \ub300\ud55c \uc801\ud569\uc131\uc744 \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5 ABLATION STUDIES"}, {"content": "| Attention | Resolution | Latency (ms) |\n|---|---|---|\n| SHMA | 224 | 1.10 |\n| SHMA | 512 | Failed |\n| Hybrid SHMA | 512 | 11.46 |\n| CC Hybrid SHMA | 512 | 4.0 |", "caption": "Table 18: Comprehensive comparison between iFormer and the previously proposed models on ImageNet-1K.  Failed indicated that the model runs too long to report latency by the Core ML, often caused by excessive memory access.", "description": "\ud45c 18\uc740 ImageNet-1K \ub370\uc774\ud130\uc14b\uc5d0\uc11c iFormer\uc640 \uae30\uc874 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218, \uc5f0\uc0b0\ub7c9(GMACS), \uc9c0\uc5f0 \uc2dc\uac04(ms), \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4, \ud559\uc2b5 \uc5d0\ud3ec\ud06c \uc218, \uadf8\ub9ac\uace0 Top-1 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec iFormer\uc758 \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 'Failed' \ud45c\uc2dc\ub294 Core ML\uc744 \uc0ac\uc6a9\ud558\uc5ec \uce21\uc815\ud588\uc744 \ub54c \ubaa8\ub378 \uc2e4\ud589 \uc2dc\uac04\uc774 \ub108\ubb34 \uae38\uc5b4 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \uce21\uc815\ud560 \uc218 \uc5c6\uc5c8\uc74c\uc744 \uc758\ubbf8\ud558\uba70, \uc774\ub294 \uacfc\ub3c4\ud55c \uba54\ubaa8\ub9ac \uc811\uadfc\uc73c\ub85c \uc778\ud55c \uac83\uc73c\ub85c \ucd94\uc815\ub429\ub2c8\ub2e4.", "section": "4.1 \uc774\ubbf8\uc9c0 \ubd84\ub958"}]