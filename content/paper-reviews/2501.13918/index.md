---
title: "Improving Video Generation with Human Feedback"
summary: "ì¸ê°„ í”¼ë“œë°±ìœ¼ë¡œ ë¹„ë””ì˜¤ ìƒì„± ê°œì„ : ë‹¤ì°¨ì›ì  ë³´ìƒ ë° ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ ì œì‹œ"
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Computer Vision", "Video Understanding", "ğŸ¢ Tsinghua University",]
showSummary: true
date: 2025-01-23
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2501.13918 {{< /keyword >}}
{{< keyword icon="writer" >}} Jie Liu et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2025-01-24 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2501.13918" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2501.13918" target="_self" >}}
â†— Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/improving-video-generation-with-human" target="_self" >}}
â†— Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

ìµœê·¼ ë¹„ë””ì˜¤ ìƒì„± ê¸°ìˆ ì€ ë°œì „í–ˆì§€ë§Œ, **ì›€ì§ì„ ë¶ˆì•ˆì •, í”„ë¡¬í”„íŠ¸ ë¶ˆì¼ì¹˜, í™”ì§ˆ ì €í•˜** ë“±ì˜ ë¬¸ì œì ì´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ” ì£¼ë¡œ ì €í’ˆì§ˆ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ í–ˆìœ¼ë‚˜, ìµœì‹  ëª¨ë¸ì˜ ë°œì „ ì†ë„ë¥¼ ë”°ë¼ê°€ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 

ë³¸ ì—°êµ¬ëŠ” **18ë§Œ 2ì²œ ê°œì˜ ì¸ê°„ ì„ í˜¸ë„ ë°ì´í„°**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, **ë‹¤ì°¨ì›ì  ë¹„ë””ì˜¤ ë³´ìƒ ëª¨ë¸**ì„ ì œì‹œí•˜ê³ , ì´ë¥¼ í†µí•´ **ë¹„ë””ì˜¤ í’ˆì§ˆ ë° í”„ë¡¬í”„íŠ¸ ì¼ì¹˜ë„**ë¥¼ ê°œì„ í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, **ìµœì‹  íë¦„ ê¸°ë°˜ ëª¨ë¸**ì— ì í•©í•œ ì„¸ ê°€ì§€ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ì—¬, **í›ˆë ¨ ì‹œê°„ ë° ì¶”ë¡  ì‹œê°„** ëª¨ë‘ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì‚¬ìš©ì ë§ì¶¤í˜• ë¹„ë””ì˜¤ ìƒì„± ê²½í—˜ì„ ì œê³µí•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} ëŒ€ê·œëª¨ ë‹¤ì°¨ì›ì  ë¹„ë””ì˜¤ ì„ í˜¸ë„ ë°ì´í„°ì…‹ êµ¬ì¶• {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} ë‹¤ì°¨ì›ì  ë¹„ë””ì˜¤ ë³´ìƒ ëª¨ë¸ ë° í‰ê°€ ë²¤ì¹˜ë§ˆí¬ ì œì‹œ {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} ìµœì‹  íë¦„ ê¸°ë°˜ ëª¨ë¸ì„ ìœ„í•œ íš¨ìœ¨ì ì¸ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜(Flow-DPO, Flow-RWR, Flow-NRG) ê°œë°œ {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
ë³¸ ë…¼ë¬¸ì€ **ì¸ê°„ì˜ í”¼ë“œë°±ì„ í™œìš©í•˜ì—¬ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•**ì„ ì œì‹œí•˜ì—¬, **ë¹„ë””ì˜¤ ìƒì„± ë¶„ì•¼ì˜ í•µì‹¬ì ì¸ ë¬¸ì œì  í•´ê²°**ì— í¬ê²Œ ê¸°ì—¬í•©ë‹ˆë‹¤.  **ëŒ€ê·œëª¨ ì„ í˜¸ë„ ë°ì´í„°ì…‹ êµ¬ì¶•**, **ë‹¤ì°¨ì›ì  ë³´ìƒ ëª¨ë¸ ê°œë°œ**, **ìƒˆë¡œìš´ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ ì œì‹œ** ë“±ì˜ ì—°êµ¬ ê²°ê³¼ëŠ” í–¥í›„ ë¹„ë””ì˜¤ ìƒì„± ê¸°ìˆ  ë°œì „ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì´ë©°, **ë‹¤ì–‘í•œ ì—°êµ¬ ë¶„ì•¼ì—ì„œì˜ í™œìš© ê°€ëŠ¥ì„±**ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  íŠ¹íˆ, **ìµœì‹  ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì˜ íŠ¹ì§•ì„ ê³ ë ¤í•œ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ**ì€ ì´ ë¶„ì•¼ ì—°êµ¬ì˜ ìƒˆë¡œìš´ ë°©í–¥ì„ ì œì‹œí•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

------
#### Visual Insights



![](https://arxiv.org/html/2501.13918/x1.png)

> ğŸ”¼ ë³¸ ê·¸ë¦¼ì€ ë…¼ë¬¸ì˜ ë¹„ë””ì˜¤ ì •ë ¬ íŒ¨ëŸ¬ë‹¤ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. (a)ì—ì„œëŠ” 182,000ê°œì˜ (í”„ë¡¬í”„íŠ¸, ë¹„ë””ì˜¤ A, ë¹„ë””ì˜¤ B) ì„¸ ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì—¬ ì‹œê°ì  í’ˆì§ˆ(VQ), ë™ì‘ í’ˆì§ˆ(MQ), í…ìŠ¤íŠ¸ ì •ë ¬(TA)ì— ëŒ€í•œ ì‚¬ëŒì˜ ì„ í˜¸ë„ë¥¼ ì£¼ì„ìœ¼ë¡œ ë‹¬ì•˜ìŠµë‹ˆë‹¤. (b)ì—ì„œëŠ” ë¸Œë˜ë“¤ë¦¬-í…Œë¦¬-ëª¨ë¸-ìœ„ë“œ-íƒ€ì´ì¦ˆ(Bradley-Terry-Model-with-Ties) ê³µì‹ì„ ì‚¬ìš©í•˜ì—¬ VLM ê¸°ë°˜ ë³´ìƒ ëª¨ë¸ì„ í•™ìŠµì‹œì¼°ìŠµë‹ˆë‹¤. (c)ì—ì„œëŠ” íë¦„ ê¸°ë°˜ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì— DPO, RWR, ë³´ìƒ ì•ˆë‚´ì™€ ê°™ì€ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ ê·¸ íš¨ê³¼ë¥¼ ë¹„êµ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 1: Overview of Our Video Alignment Paradigm. (a) Human Preference AnnotationÂ (Sec.Â 3.1). We construct a dataset of 182k (prompt, video A, video B) triplets, collecting preference annotations on Visual QualityÂ (VQ), Motion QualityÂ (MQ), and Text AlignmentÂ (TA) from human evaluators. (b) Reward Mode TrainingÂ (Sec.Â 3.2). We train a VLM-based reward model using the Bradley-Terry-Model-with-Ties formulation. (c) Video AlignmentÂ (Sec.Â 4). We adapt alignment techniques â€” DPO, RWR, and reward guidance â€” to flow-based video generation models and provide a comprehensive comparison of their effectiveness.
> </details>





{{< table-caption >}}
| T2V Model | Date | #Videos | #Anno Triplets | Resolution | Duration |
|---|---|---|---|---|---|---|
| **Pre-Sora-Era Models** |  |  |  |  |  |  |
| Gen2 [2023] | 23.06 | 6k | 13k | 768 Ã— 1408 | 4s |
| SVD [2023] | 23.11 | 6k | 13k | 576 Ã— 1024 | 4s |
| Pika 1.0 [2023] | 23.12 | 6k | 13k | 720 Ã— 1280 | 3s |
| Vega [2023] | 23.12 | 6k | 13k | 576 Ã— 1024 | 4s |
| Pixverse v1 [2024] | 24.01 | 6k | 13k | 768 Ã— 1408 | 4s |
| HiDream [2024] | 24.01 | 0.3k | 0.3k | 768 Ã— 1344 | 5s |
| **Modern Models** |  |  |  |  |  |  |
| Dreamina [2024] | 24.03 | 16k | 68k | 720 Ã— 1280 | 6s |
| Luma [2024] | 24.06 | 16k | 57k | 752 Ã— 1360 | 5s |
| Gen3 [2024] | 24.06 | 16k | 55k | 768 Ã— 1280 | 5s |
| Kling 1.0 [2024] | 24.06 | 6k | 33k | 384 Ã— 672 | 5s |
| Pixverse v2 [2024] | 24.07 | 16k | 58k | 576 Ã— 1024 | 5s |
| Kling 1.5 [2024] | 24.09 | 7k | 28k | 704 Ã— 1280 | 5s |{{< /table-caption >}}

> ğŸ”¼ ì´ í‘œëŠ” ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ í›ˆë ¨ ë°ì´í„°ì…‹ì˜ í†µê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ 16,000ê°œì˜ ê³ ìœ  í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ 12ê°œì˜ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ëª¨ë¸ë¡œ 108,000ê°œì˜ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì„ í†µí•´ í”„ë¡¬í”„íŠ¸ì™€ ë‘ ê°œì˜ ë¹„ë””ì˜¤, ê·¸ë¦¬ê³  í•´ë‹¹ ì„ í˜¸ë„ ì£¼ì„ìœ¼ë¡œ êµ¬ì„±ëœ 182,000ê°œì˜ ì–´ë…¸í…Œì´ì…˜ëœ íŠ¸ë¦¬í”Œë¦¿ì„ ì–»ì—ˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 1: Statistics of the collected training dataset. We utilized 12 text-to-video models to generate a total of 108k videos from 16k unique prompts. This process ultimately resulted in 182k annotated triplets, each consisting of a prompt paired with two videos and corresponding preference annotations.
> </details>





### In-depth insights


#### Human Feedback Loop
ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ â€˜ì¸ê°„ í”¼ë“œë°± ë£¨í”„â€™ëŠ” **ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ í•µì‹¬ ì „ëµ**ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.  ì´ëŠ” ë‹¨ìˆœí•œ í‰ê°€ ë„êµ¬ë¥¼ ë„˜ì–´, ì¸ê°„ì˜ ì„ í˜¸ë„ ë°ì´í„°ë¥¼ í†µí•´ ëª¨ë¸ í•™ìŠµ ê³¼ì •ì— ì§ì ‘ì ìœ¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” **ê°•í™” í•™ìŠµ ê¸°ë°˜ ì ‘ê·¼ë²•**ì„ ì œì‹œí•©ë‹ˆë‹¤.  **ë‹¤ì°¨ì›ì  ì„ í˜¸ë„ ë°ì´í„°ì…‹ êµ¬ì¶•**ì„ í†µí•´ ì‹œê°ì  í’ˆì§ˆ, ë™ì‘ì˜ ìì—°ìŠ¤ëŸ¬ì›€, í”„ë¡¬í”„íŠ¸ì™€ì˜ ì •í•©ì„± ë“± ë‹¤ì–‘í•œ ì¸¡ë©´ì„ í¬ê´„ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” ì¤‘ìš”í•œ ë°œì „ì…ë‹ˆë‹¤.  **ë‹¤ì°¨ì› ë³´ìƒ ëª¨ë¸(VideoReward)**ì„ í™œìš©í•˜ì—¬ ì¸ê°„ì˜ ë¯¸ë¬˜í•œ ì„ í˜¸ë„ê¹Œì§€ ë°˜ì˜, ë”ìš± ì •êµí•œ í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” ì ë„ ì£¼ëª©í•  ë§Œí•©ë‹ˆë‹¤.  **ì—¬ëŸ¬ ê°€ì§€ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜**ì„ í†µí•´ ì´ëŸ¬í•œ í”¼ë“œë°±ì„ ëª¨ë¸ í•™ìŠµì— íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ì‹œë„ëŠ”, í–¥í›„ **ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ë¹„ë””ì˜¤ ìƒì„± ê¸°ìˆ  ë°œì „**ì— í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.  **íŠ¹íˆ, íë¦„ ê¸°ë°˜ ëª¨ë¸ì— ëŒ€í•œ ì ìš© ë° ê°œì„ **ì€ ì°¨ì„¸ëŒ€ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ì‹ ë¢°ë„ í–¥ìƒì— ì¤‘ìš”í•œ ê¸°ì—¬ë¥¼ í•  ê²ƒìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.

#### Reward Model Design
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **ë³´ìƒ ëª¨ë¸ ì„¤ê³„**ì— ëŒ€í•´ ì‹¬ë„ìˆëŠ” ë…¼ì˜ê°€ ë¶€ì¡±í•˜ì§€ë§Œ,  ì¸ê°„ì˜ ì„ í˜¸ë„ë¥¼ ë°˜ì˜í•˜ëŠ” ë³´ìƒ ëª¨ë¸ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ê³  ìˆìŠµë‹ˆë‹¤.  **ë‹¤ì°¨ì› ë³´ìƒ ëª¨ë¸**ì„ ì œì‹œí•˜ì—¬ ì‹œê°ì  í’ˆì§ˆ, ë™ì‘ í’ˆì§ˆ, ê·¸ë¦¬ê³  í”„ë¡¬í”„íŠ¸ì™€ì˜ ì •í•©ì„±ì„ í‰ê°€í•˜ëŠ” ë‹¤ì°¨ì›ì  ì ‘ê·¼ì„ ì‹œë„í•œ ì ì€ ì£¼ëª©í•  ë§Œí•©ë‹ˆë‹¤.  **ë¸Œë˜ë“¤ë¦¬-í…Œë¦¬ ëª¨ë¸(Bradley-Terry model)**ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ìƒëŒ€ì  ì„ í˜¸ë„ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ì±„íƒí•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ìŒì„ ì´ë£¬ ë¹„ë””ì˜¤ì˜ ìƒëŒ€ì  í’ˆì§ˆì„ íš¨ê³¼ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì´ ë  ê²ƒì…ë‹ˆë‹¤.  ë˜í•œ, **ì†ì‹¤ í•¨ìˆ˜**ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ê³¼ì •ì´ ë¬˜ì‚¬ë˜ì–´ ìˆìœ¼ë‚˜, êµ¬ì²´ì ì¸ ì„¤ê³„ ë° ìµœì í™” ì „ëµì— ëŒ€í•œ ì„¸ë¶€ì ì¸ ì •ë³´ëŠ” ë¶€ì¡±í•˜ì—¬ ì¶”ê°€ì ì¸ ì—°êµ¬ê°€ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.  **ë³´ìƒ ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€**ëŠ” GenAI-Bench ë° VideoGen-RewardBench ì™€ ê°™ì€ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰ë˜ì—ˆìœ¼ë‚˜, ë³´ìƒ ëª¨ë¸ ì„¤ê³„ ìì²´ì— ëŒ€í•œ ë¶„ì„ì€ ë¶€ì¡±í•˜ë¯€ë¡œ,  ë‹¤ì–‘í•œ ì„¤ê³„ ì„ íƒì§€ë“¤ì˜ ì˜í–¥ì— ëŒ€í•œ ë”ìš± ìì„¸í•œ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.  **ë¹„êµ ë¶„ì„**ì„ í†µí•´ ë‹¤ë¥¸ ë³´ìƒ ëª¨ë¸ë“¤ê³¼ì˜ ì„±ëŠ¥ ë¹„êµë¥¼ ìˆ˜í–‰í•˜ê³  ìˆìœ¼ë‚˜,  ê·¸ ì°¨ì´ì— ëŒ€í•œ ëª…í™•í•œ ì„¤ëª…ì€ ì œí•œì ì…ë‹ˆë‹¤.  ê²°ë¡ ì ìœ¼ë¡œ, ë³¸ ë…¼ë¬¸ì˜ ë³´ìƒ ëª¨ë¸ ì„¤ê³„ëŠ” ì¸ê°„ì˜ ì„ í˜¸ë„ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë°˜ì˜í•˜ê¸° ìœ„í•œ ë…¸ë ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ,  **ì„¤ê³„ ê³¼ì •ì˜ íˆ¬ëª…ì„± ë° ë¶„ì„ì˜ ì‹¬ë„**ë¥¼ ë†’ì´ëŠ” ê²ƒì´ í–¥í›„ ì—°êµ¬ ë°©í–¥ìœ¼ë¡œ ì œì‹œë  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

#### Flow-Based Alignment
ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ íë¦„ ê¸°ë°˜ ì •ë ¬(Flow-Based Alignment) ë°©ë²•ì€ ê¸°ì¡´ í™•ì‚° ëª¨ë¸ ê¸°ë°˜ì˜ ì ‘ê·¼ ë°©ì‹ì„ ë›°ì–´ë„˜ì–´, **ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì¸ ì •ë¥˜ íë¦„(Rectified Flow)ì— ì§ì ‘ì ìœ¼ë¡œ ì ìš©**í•¨ìœ¼ë¡œì¨ íš¨ìœ¨ì„±ì„ ë†’ì´ê³ ì í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, **ê°•í™” í•™ìŠµ(Reinforcement Learning) ê´€ì ì—ì„œ 3ê°€ì§€ ì£¼ìš” ì•Œê³ ë¦¬ì¦˜**ì„ ì œì‹œí•©ë‹ˆë‹¤. í›ˆë ¨ ì‹œê°„ ì „ëµì¸ Flow-DPO ì™€ Flow-RWRì€ ê°ê° ì§ì ‘ì  ì„ í˜¸ë„ ìµœì í™” ë° ë³´ìƒ ê°€ì¤‘ íšŒê·€ ë°©ì‹ì„ ì±„íƒí•˜ì—¬ ëª¨ë¸ì„ ì •ë ¬í•©ë‹ˆë‹¤. ì¶”ë¡  ì‹œê°„ ì „ëµì¸ Flow-NRGëŠ” ì¡ìŒì´ ìˆëŠ” ë¹„ë””ì˜¤ì— ì§ì ‘ ë³´ìƒ ì§€ì¹¨ì„ ì ìš©í•˜ì—¬ ì‚¬ìš©ì ë§ì¶¤í˜• ë¹„ë””ì˜¤ í’ˆì§ˆì„ ì œê³µí•©ë‹ˆë‹¤. íŠ¹íˆ, **Flow-DPOëŠ” KL ì •ê·œí™”ë¥¼ í†µí•´ ë³´ìƒì„ ê·¹ëŒ€í™”**í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, **ìƒìˆ˜ Î²të¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì•ˆì •ì„±ì„ í™•ë³´**í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤ëŠ” ì ì´ í•µì‹¬ì…ë‹ˆë‹¤.  **ë‹¤ì°¨ì› ë³´ìƒ ëª¨ë¸ì„ ì´ìš©í•œ ì‹¤í—˜ ê²°ê³¼**, ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ë“¤ì´ ê¸°ì¡´ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì´ë©°, íŠ¹íˆ Flow-DPOê°€ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤ëŠ” ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤.  ê²°ë¡ ì ìœ¼ë¡œ, ë³¸ ë…¼ë¬¸ì˜ íë¦„ ê¸°ë°˜ ì •ë ¬ ë°©ë²•ë¡ ì€ ë›°ì–´ë‚œ ì„±ëŠ¥ê³¼ ì‚¬ìš©ì ë§ì¶¤í˜• ê¸°ëŠ¥ì„ ì œê³µí•˜ë©°, í–¥í›„ ë¹„ë””ì˜¤ ìƒì„± ê¸°ìˆ  ë°œì „ì— ê¸°ì—¬í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

#### Multi-objective Metrics
ë‹¤ì–‘í•œ ìš”ì†Œë¥¼ ê³ ë ¤í•˜ëŠ” **ë‹¤ì°¨ì›ì  í‰ê°€**ëŠ” ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì˜ ì§ˆì  í–¥ìƒì— í•„ìˆ˜ì ì…ë‹ˆë‹¤. ë‹¨ìˆœí•œ ì²™ë„(ì˜ˆ: FID) ëŒ€ì‹ , **ì‹œê°ì  ì§ˆ(VQ), ë™ì‘ ì§ˆ(MQ), í…ìŠ¤íŠ¸ ì •í•©ë„(TA)** ë“± ë‹¤ì–‘í•œ ì¸¡ë©´ì„ ì•„ìš°ë¥´ëŠ” ë‹¤ì¤‘ ëª©í‘œ ì§€í‘œê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ì‚¬ìš©ìì˜ ë¯¸ì  ê¸°ì¤€ê³¼ ì˜ë„ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë°˜ì˜í•˜ëŠ”ì§€, ë¹„ë””ì˜¤ì˜ ì‹œê°ì  ì™„ì„±ë„ì™€ ì¼ê´€ì„±, ê·¸ë¦¬ê³  í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì™€ì˜ ì •í™•í•œ ì¼ì¹˜ ì—¬ë¶€ ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  **ìƒí˜¸ ë³´ì™„ì ì¸ ë‹¤ì¤‘ ì§€í‘œ**ë¥¼ í†µí•´ ë‹¨ì¼ ì§€í‘œì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , ëª¨ë¸ ê°œì„  ë°©í–¥ì„ ë³´ë‹¤ ëª…í™•íˆ ì œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  **ê°€ì¤‘ì¹˜ ì¡°ì •**ì„ í†µí•´ ì‚¬ìš©ìì˜ ì„ í˜¸ë„ì— ë”°ë¥¸ ë§ì¶¤í˜• í‰ê°€ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ë‹¤ì¤‘ ì§€í‘œì˜ **ìƒê´€ ê´€ê³„ ë° ê°€ì¤‘ì¹˜ ê²°ì •**ì€ ì‹ ì¤‘í•œ ê²€í† ê°€ í•„ìš”í•˜ë©°, ì§€í‘œ ê°„ì˜ ìƒí˜¸ì‘ìš© ë° í¸í–¥ì„±ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.  ê²°ë¡ ì ìœ¼ë¡œ, ë‹¤ì¤‘ ëª©í‘œ ì§€í‘œëŠ” ëª¨ë¸ì˜ ì „ë°˜ì ì¸ ì„±ëŠ¥ì„ ë”ìš± í¬ê´„ì ì´ê³  ì •í™•í•˜ê²Œ í‰ê°€í•˜ëŠ” ë° ì¤‘ìš”í•˜ë©°, í–¥í›„ ì—°êµ¬ì—ì„œëŠ” ì´ëŸ¬í•œ ì§€í‘œë“¤ì˜ ê°œë°œ ë° ìµœì í™”ì— ë”ìš± ì§‘ì¤‘í•´ì•¼ í•©ë‹ˆë‹¤.

#### Future Research
ë³¸ ë…¼ë¬¸ì€ ì¸ê°„ í”¼ë“œë°±ì„ í™œìš©í•œ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ ê°œì„ ì— ëŒ€í•œ ì‹¬ë„ìˆëŠ” ì—°êµ¬ë¥¼ ì œì‹œí•˜ë©°, íŠ¹íˆ íë¦„ ê¸°ë°˜ ëª¨ë¸ì˜ ì •ë ¬ì— ì´ˆì ì„ ë§ì¶”ì—ˆìŠµë‹ˆë‹¤.  í•˜ì§€ë§Œ, **ê³¼ë„í•œ í›ˆë ¨ìœ¼ë¡œ ì¸í•œ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ë¬¸ì œ**ì™€ **ë³´ìƒ í•´í‚¹(reward hacking) ë¬¸ì œ**ëŠ” í–¥í›„ ì—°êµ¬ì˜ ì¤‘ìš”í•œ ê³¼ì œë¡œ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.  **ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ í™œìš©í•œ ì§€ë„ í•™ìŠµê³¼ ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜(ì˜ˆ: PPO)ì˜ ì¶”ê°€ì ì¸ ì ìš©**ì„ í†µí•´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ë‹¤ì–‘í•œ ì¡°ê±´ë¶€ ìƒì„± ì‘ì—…(ì˜ˆ: ì´ë¯¸ì§€-ë¹„ë””ì˜¤ ìƒì„±)ì—ë„ ì ìš© ê°€ëŠ¥ì„±ì„ í™•ëŒ€í•˜ëŠ” ì—°êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë˜í•œ, **ë”ìš± ê²¬ê³ í•œ ë³´ìƒ ëª¨ë¸ ê°œë°œ**ê³¼ **ë¶ˆí™•ì‹¤ì„± ì¶”ì •**ì„ í†µí•œ ëª¨ë¸ ê°œì„ ë„ ì¤‘ìš”í•©ë‹ˆë‹¤.  **ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œ ë„ì…**ê³¼ **ë‹¤ë¥¸ ì¡°ê±´ë¶€ ìƒì„± ì‘ì—…ìœ¼ë¡œì˜ í™•ì¥**ì„ í†µí•´ ë³¸ ì—°êµ¬ì˜ ë²”ìš©ì„±ì„ ë†’ì´ëŠ” ê²ƒë„ ì¤‘ìš”í•œ ë¯¸ë˜ ì—°êµ¬ ë°©í–¥ì…ë‹ˆë‹¤. íŠ¹íˆ, í˜„ì¬ ì—°êµ¬ì—ì„œ ë‹¤ë£¨ì§€ ì•Šì€ ë‹¤ë¥¸ ìœ í˜•ì˜ ì¸ê°„ í”¼ë“œë°±(ì˜ˆ: ìì—°ì–´ ì„¤ëª…)ì„ í†µí•©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¤ëŠ” ì—°êµ¬ë„ ê³ ë ¤í•´ ë³¼ ë§Œí•©ë‹ˆë‹¤.  **ë³´ìƒ ëª¨ë¸ì˜ í•´ì„ë ¥ í–¥ìƒ** ë˜í•œ ì¤‘ìš”í•œë°, ì´ëŠ” ëª¨ë¸ì˜ ì‹ ë¢°ì„±ê³¼ íˆ¬ëª…ì„±ì„ ë†’ì´ê³  ì‚¬ìš©ìì˜ ì´í•´ë„ë¥¼ ì¦ì§„ì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•  ê²ƒì…ë‹ˆë‹¤.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2501.13918/x2.png)

> ğŸ”¼ ê·¸ë¦¼ 2ëŠ” ë…¼ë¬¸ì˜ í›ˆë ¨ ë°ì´í„° í†µê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  (a)ëŠ” í”„ë¡¬í”„íŠ¸ì˜ ì¹´í…Œê³ ë¦¬ ë¶„í¬ë¥¼, (b)ëŠ” í”„ë¡¬í”„íŠ¸ ë‚´ ë‹¨ì–´ë“¤ì˜ ì›Œë“œ í´ë¼ìš°ë“œë¥¼, (c)ëŠ” í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ë¶„í¬ë¥¼, (d)ëŠ” ë¹„ë””ì˜¤ì˜ í•´ìƒë„ì™€ ì§€ì† ì‹œê°„ ë¶„í¬ë¥¼, (e)ëŠ” ì‚¬ëŒì˜ ì„ í˜¸ë„ ë¶„í¬ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  ì´ 12ê°œì˜ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì—ì„œ ìƒì„±ëœ 108,000ê°œ ì´ìƒì˜ ë¹„ë””ì˜¤ì™€ 182,000ê°œ ì´ìƒì˜ ì–´ë…¸í…Œì´ì…˜ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ ë°ì´í„°ì…‹ì˜ ë‹¤ì–‘ì„±ê³¼ ê·œëª¨ë¥¼ ë³´ì—¬ì£¼ì–´, ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ì— ì‚¬ìš©ëœ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 2: Statistics of our training data.
> </details>



![](https://arxiv.org/html/2501.13918/x3.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ Bradley-Terry(BT) ëª¨ë¸ê³¼ íšŒê·€ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ì— ë”°ë¼ ë¹„êµí•œ ê·¸ë˜í”„ì…ë‹ˆë‹¤. ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ í‘œí˜„ëœ í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ì— ë”°ë¥¸ ë‘ ëª¨ë¸ì˜ ì •í™•ë„ ë³€í™”ë¥¼ ë³´ì—¬ì£¼ë©°, BT ëª¨ë¸ì´ íšŒê·€ ëª¨ë¸ë³´ë‹¤ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì´ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ í¬ê¸°ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ë‘ ëª¨ë¸ì˜ ì •í™•ë„ëŠ” ëª¨ë‘ í–¥ìƒë˜ì§€ë§Œ, BT ëª¨ë¸ì´ ì§€ì†ì ìœ¼ë¡œ ë” ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ì´ëŠ” ìŒë°© ë¹„êµ(pairwise comparison) ë°©ì‹ì˜ BT ëª¨ë¸ì´ ë°ì´í„°ì…‹ í¬ê¸°ê°€ ì‘ì„ ë•Œ ìƒëŒ€ì ì¸ ì°¨ì´ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ê¸° ë•Œë¬¸ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ì´ ì»¤ì§ˆìˆ˜ë¡ ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ ì°¨ì´ëŠ” ì¤„ì–´ë“­ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 3: Accuracy comparison between the BT and regression reward models across varying training data fractions (log scale).
> </details>



![](https://arxiv.org/html/2501.13918/x4.png)

> ğŸ”¼ ì´ ê·¸ë¦¼ì€ Bradley-Terry(BT) ëª¨ë¸ê³¼ Bradley-Terry-With-Tied(BTT) ëª¨ë¸ì˜ ì°¨ì´ì ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. BT ëª¨ë¸ì€ ì„ íƒ/ê±°ë¶€ ìŒì„ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ë§Œ, ë™ì  ìŒì„ ì •í™•í•˜ê²Œ êµ¬ë³„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ë°˜ë©´ì— BTT ëª¨ë¸ì€ ë™ì  ìŒì„ ë” ì •í™•í•˜ê²Œ êµ¬ë³„í•˜ì—¬ ì„ íƒ/ê±°ë¶€ ìŒê³¼ ë™ì  ìŒì„ ë” ì˜ êµ¬ë¶„í•˜ëŠ” ì¼ë°˜í™”ëœ ê²°ì • ê²½ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ì´ëŠ” BTT ëª¨ë¸ì´ ë™ì ì„ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•¨ìœ¼ë¡œì¨ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 4: Visualization of the Î”â¢rÎ”ğ‘Ÿ\Delta rroman_Î” italic_r distribution for the BT reward modelÂ (Left) and the BTT reward modelÂ (Right). The BTT model effectively distinguishes tie pairs from chosen/rejected pairs.
> </details>



![](https://arxiv.org/html/2501.13918/x5.png)

> ğŸ”¼ ê·¸ë¦¼ 5ëŠ” ì›ë˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ Flow-DPO ì •ë ¬ ëª¨ë¸ì´ ìƒì„±í•œ ë¹„ë””ì˜¤ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë¹„êµí•œ ê²ƒì…ë‹ˆë‹¤.  ê°ê°ì˜ ë¹„ë””ì˜¤ëŠ” ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ë˜ì—ˆìœ¼ë©°,  ë‘ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë‚˜ë€íˆ ë°°ì¹˜í•˜ì—¬ ì‹œê°ì  ì°¨ì´ë¥¼ ëª…í™•í•˜ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.  Flow-DPO ì •ë ¬ ëª¨ë¸ì„ í†µí•´ ë¹„ë””ì˜¤ì˜ ì‹œê°ì  í’ˆì§ˆê³¼ í…ìŠ¤íŠ¸ì™€ì˜ ì •ë ¬ë„ê°€ ì–¼ë§ˆë‚˜ í–¥ìƒë˜ì—ˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ì´ ê·¸ë¦¼ì€ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” Flow-DPO ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ì¤‘ìš”í•œ ê·¼ê±° ìë£Œì…ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 5: Visual comparison of videos generated by the original pretrained model and the Flow-DPO aligned model.
> </details>



![](https://arxiv.org/html/2501.13918/x6.png)

> ğŸ”¼ ê·¸ë¦¼ 6ì€ VideoGen-Eval ë°ì´í„°ì…‹ì˜ 400ê°œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ Flow-DPOë¥¼ ì ìš©í•œ ëª¨ë¸ì˜ ë¹„ë””ì˜¤ ìƒì„± ì„±ëŠ¥ì„ ì‚¬ëŒì´ í‰ê°€í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì„¸ ê°€ì§€ í‰ê°€ ê¸°ì¤€(ì‹œê°ì  í’ˆì§ˆ, ë™ì‘ í’ˆì§ˆ, í…ìŠ¤íŠ¸ ì¼ì¹˜)ì— ëŒ€í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ Flow-DPO ëª¨ë¸ì˜ ìŠ¹ë¥ , ë¬´ìŠ¹ë¶€ ë¹„ìœ¨ì„ ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ì–´ ê° ëª¨ë¸ì˜ ê°•ì ê³¼ ì•½ì ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Flow-DPOê°€ íŠ¹íˆ í…ìŠ¤íŠ¸ ì¼ì¹˜ ì¸¡ë©´ì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì–´ëƒˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 6: Human evaluation of Flow-DPO aligned model vs. pretrained model on VideoGen-Eval, which contains 400 prompts.
> </details>



![](https://arxiv.org/html/2501.13918/x7.png)

> ğŸ”¼ ê·¸ë¦¼ 7ì€ TA(Text Alignment)ì— ëŒ€í•œ Flow-DPO(Flow-based Direct Preference Optimization) ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì‹œê°„ì— ë”°ë¼ ë³€í™”í•˜ëŠ” Î²ê°’(Î²t)ê³¼ ì¼ì •í•œ Î²ê°’ì„ ì‚¬ìš©í–ˆì„ ë•Œì˜ ì •í™•ë„ë¥¼ ë¹„êµí•œ ê·¸ë˜í”„ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì„¤ì •ì—ì„œ ì¼ì •í•œ Î²ê°’ì„ ì‚¬ìš©í•œ Flow-DPOê°€ ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” Î²tê°’ì„ ì‚¬ìš©í•œ Flow-DPOë³´ë‹¤ ì„±ëŠ¥ì´ ê¾¸ì¤€íˆ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” Î²ê°’ì´ ê°ê¸° ë‹¤ë¥¸ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ì—ì„œ ë¶ˆê· ì¼í•œ í•™ìŠµì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 7: Accuracy of time-dependent Î²tsubscriptğ›½ğ‘¡\beta_{t}italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT vs. constant Î²ğ›½\betaitalic_Î² for TA: Flow-DPO with a constant Î²ğ›½\betaitalic_Î² consistently outperforms the timestep-dependent Î²ğ›½\betaitalic_Î² across various settings.
> </details>



![](https://arxiv.org/html/2501.13918/x8.png)

> ğŸ”¼ ê·¸ë¦¼ 8ì€ GenAI-Benchì™€ VideoGen-RewardBench ë°ì´í„°ì…‹ì— í¬í•¨ëœ ë¹„ë””ì˜¤ì˜ ì§€ì† ì‹œê°„ê³¼ í•´ìƒë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. GenAI-BenchëŠ” ì£¼ë¡œ ê¸°ì¡´ì˜ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ë¡œ ìƒì„±ëœ ë¹„ë””ì˜¤ë¥¼ í¬í•¨í•˜ë©°, ë¹„ë””ì˜¤ ê¸¸ì´ê°€ ì§§ê³ (2~2.5ì´ˆ) í•´ìƒë„ê°€ ë‚®ì€(ìµœëŒ€ 320x512) íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´ VideoGen-RewardBenchëŠ” ìµœì‹  ëª¨ë¸ë¡œ ìƒì„±ëœ ë¹„ë””ì˜¤ë¥¼ í¬í•¨í•˜ë©°, ë¹„ë””ì˜¤ ê¸¸ì´ê°€ ë” ê¸¸ê³ (4~6ì´ˆ) í•´ìƒë„ê°€ ë” ë†’ìŠµë‹ˆë‹¤(ìµœëŒ€ 720x1280). ì´ëŠ” ìµœì‹  ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë°˜ì˜í•©ë‹ˆë‹¤. ë‘ ë°ì´í„°ì…‹ ëª¨ë‘ ë‹¤ì–‘í•œ í•´ìƒë„ì™€ ì§€ì† ì‹œê°„ì˜ ë¹„ë””ì˜¤ë¥¼ í¬í•¨í•˜ê³  ìˆì–´, ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ìœ ìš©í•˜ê²Œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 8: Video Duration and Resolution in GenAI-Bench and VideoGen-Reward Bench
> </details>



![](https://arxiv.org/html/2501.13918/x9.png)

> ğŸ”¼ ê·¸ë¦¼ 9ëŠ” ë‹¤ì–‘í•œ ê¸°ì¤€ ëª¨ë¸ë“¤ê³¼ ë‘ ê°€ì§€ í‰ê°€ ë²¤ì¹˜ë§ˆí¬ì— ê±¸ì³ ëª¨ë¸ ì ìš© ë²”ìœ„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. VideoScore, VisionReward, GenAI-BenchëŠ” ì£¼ë¡œ SoRA ì´ì „ ì‹œëŒ€ì˜ ëª¨ë¸ì— ì´ˆì ì„ ë§ì¶”ëŠ” ë°˜ë©´, ë³¸ ë…¼ë¬¸ì˜ í›ˆë ¨ ì„¸íŠ¸ì™€ VideoGen-RewardBenchëŠ” ìµœì²¨ë‹¨ T2V ëª¨ë¸ì— ì§‘ì¤‘í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ ê° ëª¨ë¸ì˜ í›ˆë ¨ ë°ì´í„°ì…‹ê³¼ í‰ê°€ ë²¤ì¹˜ë§ˆí¬ì— ì‚¬ìš©ëœ T2V ëª¨ë¸ì˜ ì‹œëŒ€ì  íë¦„ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ì–´, ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ë°©ë²•ë¡ ì˜ ë§¥ë½ê³¼ ê¸°ì—¬ë„ë¥¼ ëª…í™•íˆ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 9: The model coverage across the training sets of different baselines and the two evaluation benchmarks. VideoScore, VisionReward, and GenAI-Bench primarily focus on pre-SoRA-era models, while our training set and VideoGen-RewardBench concentrate on state-of-the-art T2V models.
> </details>



![](https://arxiv.org/html/2501.13918/x10.png)

> ğŸ”¼ ê·¸ë¦¼ 10ì€ TA-Hard í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ Flow-DPOì˜ ì„±ëŠ¥ì„ ì‚¬ëŒì´ í‰ê°€í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  TA-Hard í”„ë¡¬í”„íŠ¸ëŠ” í…ìŠ¤íŠ¸ ì •ë ¬ì´ ì–´ë ¤ìš´ ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ê·¸ë˜í”„ëŠ” Flow-DPOë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë¹„ë””ì˜¤ì™€ ê¸°ì¡´ ëª¨ë¸ì˜ ë¹„ë””ì˜¤ë¥¼ ë¹„êµí•˜ì—¬ ê°ê°ì˜ ë¹„ë””ì˜¤ í’ˆì§ˆ, ì›€ì§ì„ í’ˆì§ˆ, í…ìŠ¤íŠ¸ ì •ë ¬ì— ëŒ€í•œ ì‚¬ìš©ì ì„ í˜¸ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  ì„¸ ê°œì˜ ë²”ì£¼ì—ì„œ Flow-DPOê°€ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ì–¼ë§ˆë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Figure 10: Human evaluation of Flow-DPO on TA-Hard prompt.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Method | GenAI-Bench |  | VideoGen-RewardBench |  |  |  |  |  |  |  | 
|---|---|---|---|---|---|---|---|---|---|---|---| 
|  | Overall Accuracy |  | Overall Accuracy | VQ Accuracy |  | MQ Accuracy |  | TA Accuracy |  |  | 
|  | w/ Ties | w/o Ties | w/ Ties | w/o Ties | w/ Ties | w/o Ties | w/ Ties | w/o Ties | w/ Ties | w/o Ties | 
| Random | 33.67 | 49.84 | 41.86 | 50.30 | 47.42 | 49.86 | 59.07 | 49.64 | 37.25 | 50.40 | 
| VideoScore (He et al., 2024) | 49.03 | 71.69 | 41.80 | 50.22 | 47.41 | 47.72 | 59.05 | 51.09 | 37.24 | 50.34 | 
| LiFT* (Wang et al., 2024b) | 37.06 | 58.39 | 39.08 | 57.26 | 47.53 | 55.97 | 59.04 | 54.91 | 33.79 | 55.43 | 
| VisionRewrd (Xu et al., 2024a) | 51.56 | 72.41 | 56.77 | 67.59 | 47.43 | 59.03 | 59.03 | 60.98 | 46.56 | 61.15 | 
| Ours | 49.41 | 72.89 | 61.26 | 73.59 | 59.68 | 75.66 | 66.03 | 74.70 | 53.80 | 72.20 | {{< /table-caption >}}
> ğŸ”¼ í‘œ 2ëŠ” GenAI-Benchì™€ VideoGen-Eval ë‘ ë°ì´í„°ì…‹ì—ì„œ ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì˜ ì„ í˜¸ë„ ì •í™•ë„ë¥¼ ë¹„êµ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  'w/ Ties'ëŠ” ë™ì ì„ í¬í•¨í•˜ì—¬ ì •í™•ë„ë¥¼ ê³„ì‚°í•œ ê²°ê³¼ì´ê³ , 'w/o Ties'ëŠ” ë™ì ì„ ì œì™¸í•˜ê³  ê³„ì‚°í•œ ê²°ê³¼ì…ë‹ˆë‹¤. LiFT ë°©ë²•ì˜ ê²½ìš°, ëª¨ë¸ì´ ìƒì„±í•œ ë™ì  ì˜ˆì¸¡ê°’ì´ ë§ì•„ ë¬´ì‘ìœ„ë¡œ ì„ íƒ/ê±°ë¶€ë¡œ ë³€í™˜(0.5 í™•ë¥ )í•˜ì—¬ ì •í™•ë„ë¥¼ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤.  í‘œì—ì„œ ê°€ì¥ ë†’ì€ ì •í™•ë„ë¥¼ ê¸°ë¡í•œ ë°©ë²•ì€ êµµê²Œ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.  GenAI-BenchëŠ” ì´ì „(pre-Sora) ì‹œëŒ€ì˜ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì— ì´ˆì ì„ ë§ì¶”ê³  VideoGen-Evalì€ ìµœì‹  ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ì— ì´ˆì ì„ ë§ì¶˜ ì ì„ ì°¸ê³ í•´ì•¼ í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 2: Preference accuracy on GenAI-Bench and VideoGen-Eval. w/ Ties indicates that accuracy is calculated with ties includedÂ (Deutsch etÂ al., 2023), and w/o Ties excludes tied pairs when calculating accuracy. * denotes that for LiFT, ties prediction are randomly converted to chosen/rejected with a 0.5 probability due to a large number of ties produced by the model. Bold: best performance.
> </details>

{{< table-caption >}}
| Variants | RM Type | Separate Tokens | GenAI-Bench Overall Accuracy w/ Ties | GenAI-Bench Overall Accuracy w/o Ties | VideoGen-RewardBench Overall Accuracy w/ Ties | VideoGen-RewardBench Overall Accuracy w/o Ties | VideoGen-RewardBench VQ Accuracy w/ Ties | VideoGen-RewardBench VQ Accuracy w/o Ties | VideoGen-RewardBench MQ Accuracy w/ Ties | VideoGen-RewardBench MQ Accuracy w/o Ties | VideoGen-RewardBench TA Accuracy w/ Ties | VideoGen-RewardBench TA Accuracy w/o Ties |
|---|---|---|---|---|---|---|---|---|---|---|---|---|
| I | Regression |  | 48.28 | 71.13 | 58.39 | 70.16 | 54.23 | 73.61 | 61.16 | 65.56 | 52.60 | 70.95 |
| II | BT |  | 47.74 | 71.21 | 61.22 | 72.58 | 52.33 | 77.10 | 59.43 | 73.50 | 53.06 | 71.62 |
| III | BTT |  | 48.27 | 70.89 | 61.50 | 73.39 | 60.52 | 76.31 | 64.64 | 72.40 | 53.55 | 72.12 |
| IV | BTT | âœ“ | 49.41 | 72.89 | 61.26 | 73.59 | 59.68 | 75.66 | 66.03 | 74.70 | 53.80 | 72.20 |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ë¹„ë””ì˜¤ ë³´ìƒ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ëŒ€í•œ ablation study ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  êµ¬ì²´ì ìœ¼ë¡œ, ë³´ìƒ ëª¨ë¸ ìœ í˜•(íšŒê·€, Bradley-Terry, Bradley-Terry-With-Ties), í† í° ë¶„ë¦¬ ì‚¬ìš© ì—¬ë¶€, ê·¸ë¦¬ê³  ë°ì´í„° ì¦ê°•ì˜ ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.  ê° ì‹¤í—˜ ì¡°ê±´ì—ì„œ GenAI-Bench ë° VideoGen-RewardBench ë‘ ê°€ì§€ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì „ì²´ ì •í™•ë„ì™€ VQ, MQ, TA ì„¸ ê°€ì§€ ì°¨ì›ë³„ ì •í™•ë„ë¥¼ ì œì‹œí•˜ë©°, ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²°ê³¼ë¥¼ êµµê²Œ í‘œì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‹¤ì°¨ì› ë¹„ë””ì˜¤ ë³´ìƒ ëª¨ë¸ì˜ ì„¤ê³„ ì„ íƒì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ êµ¬ì„±ì„ ë„ì¶œí•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 3: Ablation study on reward model type, seprate tokens and data augmentation. Bold: Best Performance.
> </details>

{{< table-caption >}}
| Method | VBench |  |  |  |  |  | VideoGen-Eval |  |  | TA-Hard |  |  |
|---|---|---|---|---|---|---|---|---|---|---|---|---|
|  | Total | Quality | Sementic | VQ | MQ | TA | VQ | MQ | TA | VQ | MQ | TA |
|---|---|---|---|---|---|---|---|---|---|---|---|---|
| Pretrained | 83.19 | **84.37** | 78.46 | 50.0 | 50.0 | 50.0 | 50.0 | 50.0 | 50.0 | 50.0 | 50.0 | 50.0 |
| SFT | 82.31 | 83.13 | 79.04 | 51.28 | 65.21 | 52.84 | 61.27 | 76.13 | 46.35 | 57.75 | 76.06 | 57.75 |
| Flow-RWR | 82.27 | 83.19 | 78.59 | 51.55 | 63.9 | 53.43 | 59.05 | 69.7 | 48.35 | 61.97 | 78.87 | 55.71 |
| Flow-DPO (<math alttext="\beta_{t}=\beta(1-t)^{2}" class="ltx_Math" display="inline" id="S5.T4.5.1.1.1.m1.1"><semantics id="S5.T4.5.1.1.1.m1.1a"><mrow id="S5.T4.5.1.1.1.m1.1.1" xref="S5.T4.5.1.1.1.m1.1.1.cmml"><msub id="S5.T4.5.1.1.1.m1.1.1.3" xref="S5.T4.5.1.1.1.m1.1.1.3.cmml"><mi id="S5.T4.5.1.1.1.m1.1.1.3.2" xref="S5.T4.5.1.1.1.m1.1.1.3.2.cmml">Î²</mi><mi id="S5.T4.5.1.1.1.m1.1.1.3.3" xref="S5.T4.5.1.1.1.m1.1.1.3.3.cmml">t</mi></msub><mo id="S5.T4.5.1.1.1.m1.1.1.2" xref="S5.T4.5.1.1.1.m1.1.1.2.cmml">=</mo><mrow id="S5.T4.5.1.1.1.m1.1.1.1" xref="S5.T4.5.1.1.1.m1.1.1.1.cmml"><mi id="S5.T4.5.1.1.1.m1.1.1.1.3" xref="S5.T4.5.1.1.1.m1.1.1.1.3.cmml">Î²</mi><mo id="S5.T4.5.1.1.1.m1.1.1.1.2" xref="S5.T4.5.1.1.1.m1.1.1.1.2.cmml">â¢</mo><msup id="S5.T4.5.1.1.1.m1.1.1.1.1" xref="S5.T4.5.1.1.1.m1.1.1.1.1.cmml"><mrow id="S5.T4.5.1.1.1.m1.1.1.1.1.1.1" xref="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.cmml"><mo id="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1" xref="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.cmml"><mn id="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.2" xref="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.1" xref="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.1.cmml">-</mo><mi id="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.3" xref="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.3.cmml">t</mi></mrow><mo id="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S5.T4.5.1.1.1.m1.1.1.1.1.3" xref="S5.T4.5.1.1.1.m1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.5.1.1.1.m1.1b"><apply id="S5.T4.5.1.1.1.m1.1.1.cmml" xref="S5.T4.5.1.1.1.m1.1.1"><eq id="S5.T4.5.1.1.1.m1.1.1.2.cmml" xref="S5.T4.5.1.1.1.m1.1.1.2"></eq><apply id="S5.T4.5.1.1.1.m1.1.1.3.cmml" xref="S5.T4.5.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T4.5.1.1.1.m1.1.1.3.1.cmml" xref="S5.T4.5.1.1.1.m1.1.1.3"><msub /></csymbol><ci id="S5.T4.5.1.1.1.m1.1.1.3.2.cmml" xref="S5.T4.5.1.1.1.m1.1.1.3.2">ğ›½</ci><ci id="S5.T4.5.1.1.1.m1.1.1.3.3.cmml" xref="S5.T4.5.1.1.1.m1.1.1.3.3">ğ‘¡</ci></apply><apply id="S5.T4.5.1.1.1.m1.1.1.1.cmml" xref="S5.T4.5.1.1.1.m1.1.1.1"><times id="S5.T4.5.1.1.1.m1.1.1.1.2.cmml" xref="S5.T4.5.1.1.1.m1.1.1.1.2"></times><ci id="S5.T4.5.1.1.1.m1.1.1.1.3.cmml" xref="S5.T4.5.1.1.1.m1.1.1.1.3">ğ›½</ci><apply id="S5.T4.5.1.1.1.m1.1.1.1.1.cmml" xref="S5.T4.5.1.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.T4.5.1.1.1.m1.1.1.1.1.2.cmml" xref="S5.T4.5.1.1.1.m1.1.1.1.1"><msup /></csymbol><apply id="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.T4.5.1.1.1.m1.1.1.1.1.1.1"><minus id="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.1"></minus><cn id="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S5.T4.5.1.1.1.m1.1.1.1.1.1.1.1.3">ğ‘¡</ci></apply><cn id="S5.T4.5.1.1.1.m1.1.1.1.1.3.cmml" type="integer" xref="S5.T4.5.1.1.1.m1.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.1.1.1.m1.1c">\beta_{t}=\beta(1-t)^{2}</annotation><annotation encoding="application/x-llamapun" id="S5.T4.5.1.1.1.m1.1d">italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_Î² ( 1 - italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>) | 80.90 | 81.52 | 78.42 | 87.78 | **82.36** | 51.02 | 88.44 | **91.23** | 28.14 | **84.29** | **83.10** | 38.03 |
| Flow-DPO | **83.41** | 84.19 | **80.26** | **93.42** | 69.08 | **75.43** | **90.95** | 81.01 | **68.26** | 77.46 | 71.43 | **73.24** |{{< /table-caption >}}
> ğŸ”¼ í‘œ 4ëŠ” VQ, MQ, TA ì„¸ ê°€ì§€ ì¸¡ë©´ì— ëŒ€í•´ 1:1:1 ë¹„ìœ¨ë¡œ ë‹¤ì°¨ì› ì •ë ¬ì„ ìˆ˜í–‰í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì‹œê°„ì— ë”°ë¼ ë³€í™”í•˜ëŠ” ë² íƒ€(Î²) ê°’ì„ ì‚¬ìš©í•œ Flow-DPOëŠ” VQì™€ MQì—ì„œ ë†’ì€ ë³´ìƒ ìŠ¹ë¥ ì„ ë‹¬ì„±í•˜ì§€ë§Œ, ìƒë‹¹í•œ ë³´ìƒ í•´í‚¹ ë¬¸ì œë¥¼ ë³´ì…ë‹ˆë‹¤. ë°˜ë©´, ì¼ì •í•œ ë² íƒ€(Î²) ê°’ì„ ì‚¬ìš©í•œ Flow-DPOëŠ” ë³´ìƒ í•´í‚¹ ì—†ì´ VQ, MQ, TA ì„¸ ê°€ì§€ ì¸¡ë©´ ëª¨ë‘ì—ì„œ ë†’ì€ ì ìˆ˜ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.  ì¦‰, ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” Î²ëŠ” ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ë˜ì§€ë§Œ, ë³´ìƒ í•´í‚¹ì´ë¼ëŠ” ë¶€ì‘ìš©ì„ ë°œìƒì‹œí‚¤ëŠ” ë°˜ë©´, ì¼ì •í•œ Î²ëŠ” ë³´ìƒ í•´í‚¹ ì—†ì´ë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 4: Multi-dimensional alignment with VQ:MQ:TA = 1:1:1. Bold: Best performance. Although Flow-DPO with a timestep-dependent Î²ğ›½\betaitalic_Î² achieves high VQ and MQ reward win rates, it exhibits significant reward hacking. In contrast, Flow-DPO with a constant Î²ğ›½\betaitalic_Î² achieves high VQ, MQ, and TA scores while avoiding reward hacking.
> </details>

{{< table-caption >}}
| Method | VBench Total | VBench Quality | VBench Semantic | VBench TA | VideoGen-Eval TA | TA-Hard TA |
|---|---|---|---|---|---|---|
| Pretrained | 83.19 | **84.37** | 78.46 | 50.00 | 50.00 | 50.00 |
| SFT | 82.71 | 83.48 | 79.62 | 52.88 | 53.81 | 64.79 |
| Flow-RWR | 82.40 | 83.36 | 78.58 | 59.66 | 49.50 | 66.20 |
| Flow-DPO (<math alttext="\beta_{t}=\beta(1-t)^{2}" class="ltx_Math" display="inline">\beta_{t}=\beta(1-t)^{2}</math>) | 82.35 | 83.00 | 79.75 | 63.67 | 55.95 | 71.83 |
| Flow-DPO | **83.38** | 84.28 | **79.80** | **69.09** | **65.49** | **84.51** |{{< /table-caption >}}
> ğŸ”¼ í‘œ 5ëŠ” í…ìŠ¤íŠ¸ ì •ë ¬(TA)ì— ëŒ€í•œ ë‹¨ì¼ ì°¨ì› ì •ë ¬ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  êµµì€ ê¸€ì”¨ëŠ” ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  ì¼ì •í•œ Î²ë¥¼ ì‚¬ìš©í•œ Flow-DPOê°€ ë³´ìƒ í•´í‚¹ ì—†ì´ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•œ ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ í‘œëŠ” ë‹¤ì–‘í•œ ë°©ë²•(SFT, Flow-RWR, ì‹œê°„ì— ë”°ë¥¸ Î²ë¥¼ ì‚¬ìš©í•œ Flow-DPO, ì¼ì •í•œ Î²ë¥¼ ì‚¬ìš©í•œ Flow-DPO)ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì •ë ¬(TA)ì— ëŒ€í•œ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì˜ ì •ë ¬ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ê° ë°©ë²•ì˜ ì „ë°˜ì ì¸ í’ˆì§ˆ, ì˜ë¯¸ë¡ ì  ì¼ê´€ì„± ë° TA ì ìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ ë³´ìƒ í•´í‚¹(reward hacking) ë¬¸ì œ ë°œìƒ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 5: Single-dimensional alignment with TA. Bold: Best performance. Flow-DPO with a constant Î²ğ›½\betaitalic_Î² is the most effective method, achieving best performance without reward hacking.
> </details>

{{< table-caption >}}
| VQ:MQ:TA | VQ | MQ | TA |
|---|---|---|---|
| 0:0:1 | 60.56 | 46.48 | 70.42 |
| 0.1:0.1:0.8 | 66.50 | 63.73 | 60.86 |
| 0.1:0.1:0.6 | 68.94 | 67.59 | 53.28 |
| 0.5:0.5:0 | 86.43 | 93.23 | 26.65 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 6ì€ VideoGen-Eval í”„ë¡¬í”„íŠ¸ì—ì„œ ë‹¤ì°¨ì› ë³´ìƒì„ ì‚¬ìš©í•œ ë³´ìƒ ì•ˆë‚´ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê°€ì¤‘ ë³´ìƒ ì•ˆë‚´ëŠ” ì‚¬ìš©ìê°€ ì¶”ë¡  ì¤‘ì— ì—¬ëŸ¬ ì •ë ¬ ëª©í‘œì— ì„ì˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ ê°œì¸ ë§ì¶¤í˜• ì‚¬ìš©ì ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.  ë‹¤ì‹œ ë§í•´, ì‚¬ìš©ìëŠ” ë¹„ì£¼ì–¼ í’ˆì§ˆ(VQ), ëª¨ì…˜ í’ˆì§ˆ(MQ), í…ìŠ¤íŠ¸ ì •ë ¬(TA) ì„¸ ê°€ì§€ ì¸¡ë©´ ì¤‘ ì–´ë–¤ ì¸¡ë©´ì— ë” ì¤‘ì ì„ ë‘˜ì§€ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ì—¬ ìƒì„±ëœ ë¹„ë””ì˜¤ì˜ í’ˆì§ˆì„ ìì‹ ì—ê²Œ ë§ì¶° ì¡°ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 6: Reward guidance using multi-dimensional rewards on VideoGen-Eval prompts. The weighted reward guidence allows users to apply arbitrary weightings to multiple alignment objectives during inference to meet personalized user requirements.
> </details>

{{< table-caption >}}
|       | VQ   | MQ   | TA   |
| :---- | :---- | :---- | :---- |
| VDM w/o noise | 37.1 | 38.6 | 52.9 |
| VDM w/ noise  | 66.2 | 74.6 | 32.4 |{{< /table-caption >}}
> ğŸ”¼ í‘œ 7ì€ TA-Hard í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ MQ ë³´ìƒë§Œì„ ì‚¬ìš©í•œ ë³´ìƒ ì•ˆë‚´ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¡ìŒì´ í¬í•¨ëœ ì ì¬ ë³€ìˆ˜ë¡œ í›ˆë ¨ëœ ë³´ìƒ ëª¨ë¸ì€ ìƒì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ ì•ˆë‚´í•˜ì§€ë§Œ, ì •ì œëœ ì ì¬ ë³€ìˆ˜ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì€ ì¡ìŒì´ í¬í•¨ëœ ì ì¬ ë³€ìˆ˜ì— ëŒ€í•´ ì˜ë¯¸ ìˆëŠ” ê¸°ìš¸ê¸° ì•ˆë‚´ë¥¼ ì œê³µí•˜ì§€ ëª»í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 7: Reward guidance using only MQ rewards on TA-Hard. The reward model trained with noised latents guides the generation effectively, while the model trained on cleaned latents fails to provide meaningful gradient guidance for noised latents.
> </details>

{{< table-caption >}}
| Evaluation Dimension | Key Points Summary |
|---|---| 
| Visual Quality | - **Image Reasonableness**: The image should be objectively reasonable. <br> - **Clarity**: The image should be clear and visually sharp. <br> - **Detail Richness**: The level of intricacy in the generation of details. <br> - **Aesthetic Creativity**: The generated videos should be aesthetically pleasing. <br> - **Safety**: The generated video should not contain any disturbing or uncomfortable content. |
| Motion Quality | - **Dynamic Stability**: The continuity and stability between frames. <br> - **Dynamic Reasonableness**: The dynamic movement should align with natural physical laws. <br> - **Motion Aesthetic Quality**: The dynamic elements should be harmonious and not stiff. <br> - **Naturalness of Dynamic Fusion**: The edges should be clear during the dynamic process. <br> - **Motion Clarity**: The motion should be easy to identify. <br> - **Dynamic Degree**: The movement should be clear, avoiding still scenes. |
| Text Alignment | - **Subject Relevance**: Relevance to the described subject characteristics and subject details. <br> - **Dynamic Information Relevance**: Relevance to actions and postures as described in the text. <br> - **Environmental Relevance**: Relevance of the environment to the input text. <br> - **Style Relevance**: Relevance to the style descriptions, if exists. <br> - **Camera Movement Relevance**: Relevance to the camera descriptions, if exists. |{{< /table-caption >}}
> ğŸ”¼ ì´ í‘œëŠ” ë…¼ë¬¸ì˜ ì£¼ì„ì— ì„¤ëª…ëœ ê²ƒì²˜ëŸ¼ ê° í‰ê°€ ì°¨ì›ì— ëŒ€í•´ ì–´ë…¸í…Œì´ì…˜ ì§€ì¹¨ì— ìš”ì•½ëœ ì£¼ìš” ìš”ì ë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  ë” ìì„¸íˆ ì„¤ëª…í•˜ìë©´,  ì‹œê°ì  í’ˆì§ˆ, ë™ì‘ í’ˆì§ˆ, ê·¸ë¦¬ê³  í…ìŠ¤íŠ¸ ì •í•© ì„¸ ê°€ì§€ ì£¼ìš” ì°¨ì›ì— ëŒ€í•œ í‰ê°€ ê¸°ì¤€ì„ ì„¸ë¶„í™”í•˜ì—¬ ê°ê°ì— ëŒ€í•œ í‰ê°€ í•­ëª©ë“¤ì„ ì œì‹œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì‹œê°ì  í’ˆì§ˆì—ëŠ” í•©ë¦¬ì„±, ì„ ëª…ë„, ë””í…Œì¼ í’ë¶€í•¨, ë¯¸ì  ì°½ì˜ì„±, ì•ˆì „ì„± ë“±ì˜ í•˜ìœ„ í•­ëª©ì´ í¬í•¨ë©ë‹ˆë‹¤. ê° í•˜ìœ„ í•­ëª©ì— ëŒ€í•´ í‰ê°€ìê°€ ê³ ë ¤í•´ì•¼ í•  êµ¬ì²´ì ì¸ ë‚´ìš©ë“¤ì´ ìì„¸íˆ ê¸°ìˆ ë˜ì–´ ìˆì–´ì„œ, ë³´ë‹¤ ì •í™•í•˜ê³  ì¼ê´€ì„± ìˆëŠ” ì–´ë…¸í…Œì´ì…˜ ì‘ì—…ì„ ìœ ë„í•©ë‹ˆë‹¤.  ì´ë¥¼ í†µí•´ ì£¼ê´€ì ì¸ íŒë‹¨ì„ ìµœì†Œí™”í•˜ê³ ,  ê°ê´€ì ì¸ í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 8: Key points summary outlined in annotation guidelines for each evaluation dimension.
> </details>

{{< table-caption >}}
| Dimensions | Description |
|---|---| 
| Image Reasonableness | The image should be objectively reasonable. |
| Clarity | The image should be clear and visually sharp. |
| Detail Richness | The level of intricacy in the generation of details. |
| Aesthetic Creativity | The generated videos should be aesthetically pleasing. |
| Safety | The generated video should not contain any disturbing or uncomfortable content. |{{< /table-caption >}}
> ğŸ”¼ í‘œ 9ëŠ” GenAI-Benchì™€ VideoGen-RewardBenchì˜ ë¹„êµë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ë‘ ë²¤ì¹˜ë§ˆí¬ ëª¨ë‘ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ì´ì§€ë§Œ, GenAI-BenchëŠ” Sora ì´ì „ì˜ êµ¬í˜• ëª¨ë¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì„±ëœ ë°˜ë©´, VideoGen-RewardBenchëŠ” Sora ì´í›„ ìµœì‹  ëª¨ë¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, GenAI-BenchëŠ” ë” ì§§ì€ ë¹„ë””ì˜¤ ê¸¸ì´(2~2.5ì´ˆ)ì™€ ë‚®ì€ í•´ìƒë„(ì£¼ë¡œ 320x512)ë¥¼ ê°–ëŠ” ë°˜ë©´, VideoGen-RewardBenchëŠ” ë” ê¸´ ë¹„ë””ì˜¤ ê¸¸ì´(4~6ì´ˆ)ì™€ ë†’ì€ í•´ìƒë„(480x720~576x1024)ë¥¼ ê°–ìŠµë‹ˆë‹¤. ë˜í•œ,  VideoGen-RewardBenchëŠ” ìµœì‹  ëª¨ë¸ì˜ ë‹¤ì–‘ì„±ì„ ë” ì˜ ë°˜ì˜í•˜ë©°, ë” í¬ê´„ì ì¸ ì¸¡ë©´ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ ì°¨ì›ì˜ í‰ê°€(ì „ì²´ì ì¸ í’ˆì§ˆ)ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ í‘œëŠ” ë‘ ë²¤ì¹˜ë§ˆí¬ì˜ ì£¼ìš” ì°¨ì´ì ê³¼ ê° ë²¤ì¹˜ë§ˆí¬ì— í¬í•¨ëœ ëª¨ë¸ ìˆ˜, í”„ë¡¬í”„íŠ¸ ìˆ˜, ë¹„ë””ì˜¤ ìˆ˜, ì£¼ì„ ìˆ˜, ë° í‰ê°€ ì°¨ì› ë“±ì„ ë¹„êµí•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 9: Comparison between GenAI-Bench and VideoGen-RewardBench. Eariler Models indicates that pre-Sora-era T2V models, and Modern Models indicates that T2V models after Sora.
> </details>

{{< table-caption >}}
| Dimensions | Description |
|---|---| 
| **Dynamic Stability** | The continuity and stability between frames. |
| **Dynamic Reasonableness** | The dynamic movement should align with natural physical laws. |
| **Motion Aesthetic Quality** | The dynamic elements should be harmonious and not stiff. |
| **Naturalness of Dynamic Fusion** | The edges should be clear during the dynamic process. |
| **Motion Clarity** | The motion should be easy to identify. |
| **Dynamic Degree** | The movement should be clear, avoiding still scenes. |{{< /table-caption >}}
> ğŸ”¼ í‘œ 10ì€ ë…¼ë¬¸ì˜ ë¹„ë””ì˜¤ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ì— ì‚¬ìš©ëœ ì´ˆë§¤ê°œë³€ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  ì—¬ê¸°ì—ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ì¸ SFT(Supervised Fine-Tuning), Flow-RWR(Reward Weighted Regression for Flow), Flow-DPO(Direct Preference Optimization for Flow)ì— ëŒ€í•œ ì´ˆë§¤ê°œë³€ìˆ˜ê°€ í¬í•¨ë©ë‹ˆë‹¤. ê° ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ í•™ìŠµ ì „ëµ, LoRA(Low-Rank Adaptation)ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°(ì•ŒíŒŒ, ë“œë¡­ì•„ì›ƒ, R), ìµœì í™”ê¸°, í•™ìŠµë¥ , ì—í¬í¬ ìˆ˜, ë°°ì¹˜ í¬ê¸° ë“±ì´ ìƒì„¸íˆ ë‚˜ì—´ë˜ì–´ ìˆìŠµë‹ˆë‹¤. Flow-DPOì˜ ê²½ìš° ê·œì œí™” íŒŒë¼ë¯¸í„°ì¸ ë² íƒ€(Î²)ê°’ë„ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 10: Hyperparameters for Alignment Algorithms
> </details>

{{< table-caption >}}
| Considering the **relevance** to the input text prompt description. |
|---|---| 
| - **Subject Relevance** Relevance to the described subject characteristics and subject details. | 
| - **Dynamic Information Relevance**: Relevance to actions and postures as described in the text. | 
| - **Environmental Relevance**: Relevance of the environment to the input text. | 
| - **Style Relevance**: Relevance to the style descriptions, if exists. | 
| - **Camera Movement Relevance**: Relevance to the camera descriptions, if exists. | {{< /table-caption >}}
> ğŸ”¼ í‘œ 11ì€ ë…¼ë¬¸ì˜ ë³´ìƒ ëª¨ë¸ë§ì— ì‚¬ìš©ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  VLM(Vision-Language Model)ê³¼ VDM(Video Diffusion Model) ë‘ ê°€ì§€ ëª¨ë¸ì— ëŒ€í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì„ ê°ê° ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.  ê° ëª¨ë¸ì˜ í•™ìŠµ ì „ëµ, LoRA(Low-Rank Adaptation) ê´€ë ¨ ì„¤ì •(ì•ŒíŒŒ, ë“œë¡­ì•„ì›ƒ, R, ëŒ€ìƒ ëª¨ë“ˆ), ìµœì í™” ê¸°ë²•, í•™ìŠµë¥ , ì—í­ ìˆ˜, ë°°ì¹˜ í¬ê¸°, ë³´ìƒ ì°¨ì› ë“±ì˜ ì„¸ë¶€ì ì¸ ì„¤ì • ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.  ì´ í‘œëŠ” ë³´ìƒ ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì„ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.
> <details>
> <summary>read the caption</summary>
> Table 11: Hyperparameters for reward modeling.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}