{"references": [{"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to the work presented because it demonstrates the effectiveness of using human feedback to align language models, which is adapted here for video generation models."}, {"fullname_first_author": "Rafailov, R.", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-12-01", "reason": "The Direct Preference Optimization (DPO) algorithm introduced in this paper is directly applied and extended in the current work for video alignment."}, {"fullname_first_author": "He, X.", "paper_title": "Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation", "publication_date": "2024-06-01", "reason": "This study provides a key dataset of human preferences on videos, which is the foundation for training the reward model in the current work."}, {"fullname_first_author": "Liu, X.", "paper_title": "Flow straight and fast: Learning to generate and transfer data with rectified flow", "publication_date": "2022-09-01", "reason": "The rectified flow model discussed in this paper is central to the models used in this work, providing the base architecture for the alignment techniques."}, {"fullname_first_author": "Ho, J.", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This paper introduces denoising diffusion probabilistic models (DDPMs), which are conceptually related to the flow-based models used here and provide a theoretical foundation for many of the alignment methods."}]}