[{"figure_path": "https://arxiv.org/html/2501.05452/x2.png", "caption": "Figure 1: Overview of ReFocus. ReFocus performs visual chain of thought via input-image editing on an example data from TableVQA\u00a0[16]. Given an image and question pair, ReFocus equips GPT-4 with editing tools (details in \u00a73), and GPT-4 generates pseudo code if an edit action is needed. ReFocus then executes the editing actions, and feeds GPT-4 with the new image until an answer is reached. In the above example, mask_column and draw_row are performed.", "description": "\uc774 \uadf8\ub9bc\uc740 ReFocus\uc758 \uc791\ub3d9 \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4. TableVQA \ub370\uc774\ud130\uc14b\uc758 \ud45c \uc774\ubbf8\uc9c0\uc640 \uc9c8\ubb38\uc774 \uc8fc\uc5b4\uc9c0\uba74, ReFocus\ub294 GPT-4\uc5d0 \uc774\ubbf8\uc9c0 \ud3b8\uc9d1 \ub3c4\uad6c\ub4e4\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. GPT-4\ub294 \ud544\uc694\uc5d0 \ub530\ub77c  pseudo code\ub97c \uc0dd\uc131\ud558\uace0, ReFocus\ub294 \uc774 \ucf54\ub4dc\ub97c \uc2e4\ud589\ud558\uc5ec \uc774\ubbf8\uc9c0\ub97c \ud3b8\uc9d1\ud569\ub2c8\ub2e4.  \ud3b8\uc9d1\ub41c \uc774\ubbf8\uc9c0\ub294 \ub2e4\uc2dc GPT-4\uc5d0 \uc785\ub825\ub418\uace0, \uc774 \uacfc\uc815\uc744 \ubc18\ubcf5\ud558\uc5ec \ucd5c\uc885 \ub2f5\ubcc0\uc5d0 \ub3c4\ub2ec\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc5d0\uc11c\ub294 mask_column\uacfc draw_row\ub77c\ub294 \ub450 \uac00\uc9c0 \ud3b8\uc9d1 \uc791\uc5c5\uc774 \uc218\ud589\ub41c \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \ubd88\ud544\uc694\ud55c \uc5f4\uc744 \uac00\ub9ac\uace0 \uad00\ub828 \ud589\uc744 \uac15\uc870\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \uc2dc\uac01\uc801 \ucd94\ub860 \uacfc\uc815\uc744 \uac1c\uc120\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. REFOCUS"}, {"figure_path": "https://arxiv.org/html/2501.05452/x3.png", "caption": "Figure 2: Example of how ReFocus + GPT-4o solves previously unsolvable problem in ChartQA dataset\u00a0[24] through improved visual grounding. Given the original horizontal bar image (left), GPT-4o grounds to the wrong bars and thus gets the wrong answer. ReFocus eliminates such possibility through editing, guiding the model to the correct answer (right).", "description": "\uc774 \uadf8\ub9bc\uc740 ReFocus\uac00 ChartQA \ub370\uc774\ud130\uc14b\uc758 \uae30\uc874 \ubc29\uc2dd\uc73c\ub85c\ub294 \ud480 \uc218 \uc5c6\uc5c8\ub358 \ubb38\uc81c\ub97c \uac1c\uc120\ub41c \uc2dc\uac01\uc801 \uae30\ubc18\uc73c\ub85c \ud574\uacb0\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc758 \uc6d0\ubcf8 \uac00\ub85c \ub9c9\ub300 \uadf8\ub798\ud504\uc5d0\uc11c GPT-4\ub294 \uc798\ubabb\ub41c \ub9c9\ub300\ub97c \uc9c0\ubaa9\ud558\uc5ec \uc798\ubabb\ub41c \ub2f5\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4. \ubc18\uba74 ReFocus\ub294 \uc774\ubbf8\uc9c0 \ud3b8\uc9d1\uc744 \ud1b5\ud574 \ubaa8\ub378\uc774 \uc62c\ubc14\ub978 \ub9c9\ub300\uc5d0 \uc9d1\uc911\ud558\ub3c4\ub85d \uc720\ub3c4\ud558\uc5ec \uc815\ub2f5\uc744 \ub3c4\ucd9c\ud574\ub0c5\ub2c8\ub2e4. ReFocus\ub294 \uc911\uac04 \uacfc\uc815\uc5d0\uc11c \uc2dc\uac01\uc801 \uc815\ubcf4\ub97c \ud65c\uc6a9\ud558\uc5ec \ub2e4\ub2e8\uacc4 \ucd94\ub860\uc744 \uac00\ub2a5\ud558\uac8c \ud569\ub2c8\ub2e4.", "section": "3. REFOCUS"}, {"figure_path": "https://arxiv.org/html/2501.05452/x4.png", "caption": "Figure 3: ReFocus equips GPT-4 with selective attention. Above is an example of how ReFocus + GPT-4o solves previously unsolvable problem in ChartXiv dataset\u00a0[34]. Specifically, ReFocus edits upon the original image by masking out all irrelevant information \u2013 the other three subplots that could be distracting. As a result, GPT-4o is able to conduct better reasoning with the edited image, and reach the correct answer.", "description": "\uadf8\ub9bc 3\uc740 REFOCUS\uac00 GPT-4\uc5d0 \uc120\ud0dd\uc801 \uc8fc\uc758 \uc9d1\uc911 \uae30\ub2a5\uc744 \ubd80\uc5ec\ud558\ub294 \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 REFOCUS\uc640 GPT-4\ub97c \uc0ac\uc6a9\ud558\uc5ec ChartXiv \ub370\uc774\ud130\uc14b [34]\uc5d0\uc11c \uc774\uc804\uc5d0\ub294 \ud480 \uc218 \uc5c6\uc5c8\ub358 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c REFOCUS\ub294 \ub2e4\ub978 \uc138 \uac1c\uc758 \uc11c\ube0c\ud50c\ub86f(\uc0b0\ub9cc\ud558\uac8c \uc791\uc6a9\ud560 \uc218 \uc788\ub294 \uc694\uc18c)\uc744 \ub9c8\uc2a4\ud0b9\ud558\uc5ec \uc6d0\ubcf8 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uad00\ub828 \uc5c6\ub294 \uc815\ubcf4\ub97c \uc81c\uac70\ud568\uc73c\ub85c\uc368 \uc774\ubbf8\uc9c0 \ud3b8\uc9d1\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \uadf8 \uacb0\uacfc, GPT-4\ub294 \ud3b8\uc9d1\ub41c \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub354 \ub098\uc740 \ucd94\ub860\uc744 \uc218\ud589\ud558\uace0 \uc815\ub2f5\uc5d0 \ub3c4\ub2ec\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. REFOCUS"}, {"figure_path": "https://arxiv.org/html/2501.05452/x5.png", "caption": "Figure 4: ReFocus unleashes better visual grounding and counting abilities for GPT-4 as in ChartQA\u00a0[24] Vertical Bar problems.", "description": "\uadf8\ub9bc 4\ub294 ChartQA \ub370\uc774\ud130\uc14b\uc758 \uc218\uc9c1 \ub9c9\ub300 \uadf8\ub798\ud504 \ubb38\uc81c\uc5d0\uc11c ReFocus\uac00 GPT-4\uc758 \uc2dc\uac01\uc801 \uae30\ubc18\uc744 \uac15\ud654\ud558\uace0 \uac1c\uc218\ub97c \uc138\ub294 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ReFocus\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 GPT-4\ub294 \uadf8\ub798\ud504\uc758 \ub9c9\ub300\ub97c \uc798\ubabb \ud574\uc11d\ud558\uc5ec \uc815\ud655\ud55c \ub2f5\uc744 \uc5bb\uc9c0 \ubabb\ud558\ub294 \ubc18\uba74, ReFocus\ub294 \uc2dc\uac01\uc801 \ud3b8\uc9d1\uc744 \ud1b5\ud574 \ubaa8\ub378\uc774 \uad00\ub828 \uc815\ubcf4\uc5d0 \uc9d1\uc911\ud558\ub3c4\ub85d \uc720\ub3c4\ud558\uc5ec \uc815\ud655\ud55c \ub2f5\uc744 \ub3c4\ucd9c\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 ReFocus\uac00 \uc2dc\uac01\uc801 \ucd94\ub860 \uacfc\uc815\uc744 \uac1c\uc120\ud558\uace0 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ub192\uc774\ub294 \ub370 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.2 \uc2dc\uac01\uc801 \ud3b8\uc9d1 \ub3c4\uad6c"}, {"figure_path": "https://arxiv.org/html/2501.05452/x6.png", "caption": "Figure 5: ReFocus unleashes better OCR for GPT-4. In this example from TableVQA\u00a0[16], ReFocus + GPT-4 conducts the edit action highlight_column. With this simple action, GPT-4 can focus more on the important subarea, and recognize the characters better.", "description": "TableVQA \ub370\uc774\ud130\uc14b [16]\uc758 \ud55c \uc608\uc2dc\uc5d0\uc11c, ReFocus\ub294 GPT-4\uac00 'highlight_column' \ud3b8\uc9d1 \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub3c4\ub85d \uc720\ub3c4\ud569\ub2c8\ub2e4.  \uc774 \uac04\ub2e8\ud55c \uc791\uc5c5\uc744 \ud1b5\ud574 GPT-4\ub294 \uc911\uc694\ud55c \uc601\uc5ed\uc5d0 \ub354 \uc9d1\uc911\ud558\uc5ec \ubb38\uc790\ub97c \ub354 \uc815\ud655\ud558\uac8c \uc778\uc2dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uae30\uc874 GPT-4\ub294 \uc774\ubbf8\uc9c0 \uc804\uccb4\ub97c \uace0\ub824\ud558\uc5ec \ud14d\uc2a4\ud2b8\ub97c \uc778\uc2dd\ud558\ub294\ub370 \uc5b4\ub824\uc6c0\uc744 \uacaa\uc9c0\ub9cc, ReFocus\ub97c \ud1b5\ud574 \ud2b9\uc815 \uc5f4\uc744 \uac15\uc870\ud568\uc73c\ub85c\uc368 \ubaa8\ub378\uc758 \uc2dc\uac01\uc801 \uc9d1\uc911\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uace0 OCR \uc131\ub2a5\uc744 \uac1c\uc120\ud569\ub2c8\ub2e4.  \uacb0\uacfc\uc801\uc73c\ub85c,  \ub354 \uc815\ud655\ud55c \ub370\uc774\ud130 \ucd94\ucd9c\uacfc \ubd84\uc11d\uc744 \uac00\ub2a5\ud558\uac8c \ud569\ub2c8\ub2e4.", "section": "3.2 Visual Editing Tools"}, {"figure_path": "https://arxiv.org/html/2501.05452/x7.png", "caption": "Figure 6: Training set collection using ReFocus on ChartQA dataset.", "description": "\uc774 \uadf8\ub9bc\uc740 ChartQA \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec ReFocus\ub97c \ud1b5\ud574 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  ChartQA \ub370\uc774\ud130\uc14b\uc758 \uc774\ubbf8\uc9c0\uc640 \uc9c8\ubb38\uc5d0 \ub300\ud574 ReFocus \ud504\ub808\uc784\uc6cc\ud06c\uac00  GPT-4\uc640 \uac19\uc740 \ub2e4\uc911 \ubaa8\ub2ec LLM\uc744 \ud65c\uc6a9\ud558\uc5ec \uc2dc\uac01\uc801 \ud3b8\uc9d1(visual editing)\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.  \ud3b8\uc9d1\ub41c \uc774\ubbf8\uc9c0,  \ud3b8\uc9d1\uc5d0 \uc0ac\uc6a9\ub41c \ucf54\ub4dc,  \uadf8\ub9ac\uace0 \ucd08\uc810 \uc601\uc5ed\uc758 \uacbd\uacc4 \uc0c1\uc790(bounding box) \uc815\ubcf4\ub97c \ud3ec\ud568\ud558\ub294 \uc911\uac04 \uacfc\uc815\uc758 \uc2dc\uac01\uc801 \ucd94\ub860(visual chain of thought) \uacfc\uc815\uc744 \uae30\ub85d\ud558\uc5ec \uc0c8\ub85c\uc6b4 \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uadf8\ub9bc\uc740 \uc774\ub7ec\ud55c \ub370\uc774\ud130 \uc218\uc9d1 \ud30c\uc774\ud504\ub77c\uc778\uc758 \uac1c\ub7b5\uc801\uc778 \ud750\ub984\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "5. ReFocus\uc640 \ud568\uaed8 \ubbf8\uc138 \uc870\uc815\ud558\uae30"}, {"figure_path": "https://arxiv.org/html/2501.05452/x8.png", "caption": "Figure 7: Statistics of how often visual editing are performed.", "description": "\uc774 \uadf8\ub9bc\uc740 REFOCUS\uac00 \uad6c\uc870\ud654\ub41c \uc774\ubbf8\uc9c0 \uc774\ud574 \uc791\uc5c5\uc5d0\uc11c \uc5bc\ub9c8\ub098 \uc790\uc8fc \uc2dc\uac01\uc801 \ud3b8\uc9d1\uc744 \uc218\ud589\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  GPT-4\ub294 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \uad6c\uc870\ud654\ub41c \uc774\ubbf8\uc9c0 \ub370\uc774\ud130\uc14b(VWTQ, VWTQ_syn, VTabFact, CharXiv, \uc218\ud3c9 \ub9c9\ub300 \ucc28\ud2b8, \uc218\uc9c1 \ub9c9\ub300 \ucc28\ud2b8)\uc5d0 \ub300\ud574 \uc2dc\uac01\uc801 \ud3b8\uc9d1\uc744 \uacb0\uc815\ud569\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc2dc\uac01\uc801 \ud3b8\uc9d1 \ube48\ub3c4\ub97c \ub9c9\ub300 \uadf8\ub798\ud504\ub85c \ub098\ud0c0\ub0b4\uc5b4, \ubaa8\ub378\uc774 \ud2b9\uc815 \uc720\ud615\uc758 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uc2dc\uac01\uc801 \ud3b8\uc9d1\uc744 \ub354 \uc790\uc8fc \uc0ac\uc6a9\ud558\ub294\uc9c0 \ub610\ub294 \ub35c \uc0ac\uc6a9\ud558\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 REFOCUS\uc758 \ud6a8\uc728\uc131\uacfc \uc801\uc6a9 \uac00\ub2a5\uc131\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \ubc0f \ubd84\uc11d"}, {"figure_path": "https://arxiv.org/html/2501.05452/x13.png", "caption": "Figure 8: Phi-3.5-vision finetuned with ReFocus visual chain of thought data outputs the areas to focus on. For illustration purposes, we draw these areas in red boxes, and compare with the ReFocus + GPT-4o prompting output.", "description": "\uadf8\ub9bc 8\uc740 ReFocus\uc758 \uc2dc\uac01\uc801 \uc0ac\uace0 \uacfc\uc815 \ub370\uc774\ud130\ub85c \ubbf8\uc138 \uc870\uc815\ub41c Phi-3.5-vision \ubaa8\ub378\uc774 \ucd08\uc810\uc744 \ub9de\ucdb0\uc57c \ud560 \uc601\uc5ed\uc744 \ucd9c\ub825\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc124\uba85\uc744 \uc704\ud574 \ud574\ub2f9 \uc601\uc5ed\uc744 \ube68\uac04\uc0c9 \uc0c1\uc790\ub85c \ud45c\uc2dc\ud558\uace0 ReFocus + GPT-4o \ud504\ub86c\ud504\ud305 \ucd9c\ub825 \uacb0\uacfc\uc640 \ube44\uad50\ud588\uc2b5\ub2c8\ub2e4.  Phi-3.5-vision \ubaa8\ub378\uc740 \uc2dc\uac01\uc801 \uc0ac\uace0 \uacfc\uc815 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \ucd08\uc810 \uc601\uc5ed\uc744 \ud14d\uc2a4\ud2b8 \ud615\uc2dd(\uc88c\ud45c \ud3ec\ud568)\uc73c\ub85c \ucd9c\ub825\ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \uc218\ud3c9 \ub9c9\ub300\ud615 \ucc28\ud2b8\uc640 \uc218\uc9c1 \ub9c9\ub300\ud615 \ucc28\ud2b8\uc5d0\uc11c \uac01\uac01 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Results"}]