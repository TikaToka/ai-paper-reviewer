{"references": [{"fullname_first_author": "S. Hu", "paper_title": "Minicpm: Unveiling the potential of small language models with scalable training strategies", "publication_date": "2024-04-06", "reason": "This paper introduces the Warmup-Stable-Decay (WSD) learning rate scheduler used in Xmodel-2, significantly improving training efficiency and stability."}, {"fullname_first_author": "G. Yang", "paper_title": "Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer", "publication_date": "2022-03-03", "reason": "This paper describes the Tensor Programs architecture that allows for efficient scaling of the Xmodel-2 model to different sizes."}, {"fullname_first_author": "H. Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-18", "reason": "The architecture of Xmodel-2 is similar to Llama 2, which is a state-of-the-art large language model."}, {"fullname_first_author": "J. Ye", "paper_title": "Data mixing laws: Optimizing data mixtures by predicting language modeling performance", "publication_date": "2024-03-16", "reason": "This work is referenced for its data ratio optimization experiments which informed Xmodel-2's training data distribution strategies."}, {"fullname_first_author": "Z. Yang", "paper_title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering", "publication_date": "2018-09-09", "reason": "This paper presents HotpotQA, one of the agent-based reasoning benchmarks used to evaluate Xmodel-2's capabilities."}]}