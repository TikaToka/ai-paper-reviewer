{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a vision encoder that is crucial for AnyStory's subject representation encoding, enabling high-fidelity subject detail preservation."}, {"fullname_first_author": "Tero Karras", "paper_title": "Elucidating the design space of diffusion-based generative models", "publication_date": "2022-12-01", "reason": "This paper provides foundational knowledge on diffusion models, the underlying architecture of AnyStory, which is essential for understanding and improving the generation process."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-01", "reason": "This paper introduces Stable Diffusion, the base model for AnyStory, which is critical to the overall performance and capabilities of the proposed method."}, {"fullname_first_author": "Junjie He", "paper_title": "UniPortrait: A unified framework for identity-preserving single- and multi-human image personalization", "publication_date": "2024-08-01", "reason": "This paper introduces the \"encode-then-route\" design and subject router concept that AnyStory is heavily based upon, forming the core framework for personalized subject generation."}, {"fullname_first_author": "Dongxu Li", "paper_title": "Blip-Diffusion: Pre-trained subject representation for controllable text-to-image generation and editing", "publication_date": "2024-01-01", "reason": "This paper proposes a pre-trained multimodal encoder for subject representation, which AnyStory improves upon by using a more powerful and generalizable model for better subject fidelity"}]}