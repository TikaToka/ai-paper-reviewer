{"references": [{"fullname_first_author": "Tim Brooks", "paper_title": "Video generation models as world simulators", "publication_date": "2024-00-00", "reason": "This paper introduces Sora, a state-of-the-art text-to-video generation model, which serves as a key point of comparison for LinGen, and highlights the computational challenges associated with current DiT models."}, {"fullname_first_author": "Hongjie Wang", "paper_title": "LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity", "publication_date": "2024-12-13", "reason": "This is the paper on LinGen itself, detailing the architecture and methods that enable efficient minute-long, high-resolution video generation."}, {"fullname_first_author": "Adam Polyak", "paper_title": "Movie Gen: A cast of media foundation models", "publication_date": "2024-10-26", "reason": "MovieGen is another high-quality video generation model that, like Sora, relies on large-scale DiTs, making its architectural details relevant for comparison and demonstrating the need for more efficient methods like LinGen."}, {"fullname_first_author": "Tri Dao", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-04", "reason": "This paper introduces Mamba, a linear-complexity sequence model that forms a core component of LinGen's MA-branch, addressing the quadratic complexity of traditional self-attention."}, {"fullname_first_author": "Tri Dao", "paper_title": "Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality", "publication_date": "2024-05-02", "reason": "This work unifies SSMs and masked efficient attention, introducing Mamba2, a more hardware-friendly linear-complexity block used in LinGen."}]}