{"references": [{"fullname_first_author": "Lianghua Huang", "paper_title": "Group diffusion transformers are unsupervised multitask learners", "publication_date": "2024-10-27", "reason": "This paper introduces the foundational Group Diffusion Transformers, a key component of ChatDiT's architecture, highlighting the model's ability to handle diverse visual tasks without modifications."}, {"fullname_first_author": "Lianghua Huang", "paper_title": "In-context LoRA for diffusion transformers", "publication_date": "2024-10-27", "reason": "This paper builds upon Group Diffusion Transformers, introducing in-context learning techniques crucial to ChatDiT's zero-shot task adaptation capabilities."}, {"fullname_first_author": "Chen Liang", "paper_title": "IDEA-Bench: How far are generative models from professional designing?", "publication_date": "2024-12-17", "reason": "This paper introduces IDEA-Bench, the benchmark dataset used to evaluate ChatDiT, providing a standardized metric for assessing the model's performance against existing state-of-the-art methods."}, {"fullname_first_author": "Aditya Ramesh", "paper_title": "Zero-shot text-to-image generation", "publication_date": "2021-07-01", "reason": "This paper is a seminal work in the field of text-to-image generation that directly inspires the underlying mechanisms of diffusion transformers used in ChatDiT."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-07-01", "reason": "This paper details the foundational improvements of transformers' capabilities in generating high-resolution images, which are leveraged in ChatDiT's architecture for higher-quality image outputs."}]}