{"references": [{"fullname_first_author": "Bang, F.", "paper_title": "GPTCache: An open-source semantic cache for LLM applications enabling faster answers and cost savings.", "publication_date": "2023", "reason": "This paper introduces GPTCache, a foundational semantic prompt caching system that the current research builds upon and improves."}, {"fullname_first_author": "Dasgupta, S.", "paper_title": "waLLMartCache: A distributed, multi-tenant and enhanced semantic caching system for LLMs.", "publication_date": "2025", "reason": "This paper presents waLLMartCache, another significant semantic prompt caching system which is directly compared against in the current work's evaluation."}, {"fullname_first_author": "Li, J.", "paper_title": "SCALM: Towards semantic caching for automated chat services with large language models.", "publication_date": "2024", "reason": "This paper proposes SCALM, a competing semantic prompt caching framework that the current work's method is shown to outperform."}, {"fullname_first_author": "Kwon, W.", "paper_title": "Efficient memory management for large language model serving with pagedattention.", "publication_date": "2023", "reason": "This paper discusses memory management in large language models, providing relevant background on the computational challenges that semantic prompt caching aims to address."}, {"fullname_first_author": "Touvron, H.", "paper_title": "Llama: Open and efficient foundation language models.", "publication_date": "2023", "reason": "This paper introduces the LLaMA model, which is used in the current research for LLM inference, highlighting its importance in the experimental setup."}]}