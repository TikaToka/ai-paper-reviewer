{"references": [{"fullname_first_author": "Ouyang, Long", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to the field of RLHF and directly inspired the training methodology used in this work."}, {"fullname_first_author": "Kaplan, Jared", "paper_title": "Scaling Laws for Neural Language Models", "publication_date": "2020-01-20", "reason": "This paper established scaling laws that guided the computational choices made in scaling up RL for LLMs."}, {"fullname_first_author": "Silver, David", "paper_title": "Mastering the game of go without human knowledge", "publication_date": "2017-01-01", "reason": "This paper showcases the power of reinforcement learning in achieving superhuman performance in complex domains, providing a strong rationale for applying RL to LLMs."}, {"fullname_first_author": "Vaswani, Ashish", "paper_title": "Attention is All you Need", "publication_date": "2017-12-01", "reason": "The Transformer architecture, introduced in this paper, is the basis for most modern LLMs, including the Kimi k1.5 model discussed in this report."}, {"fullname_first_author": "Wei, Jason", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduced the chain-of-thought prompting technique, which is central to the RL training approach used in this work, significantly improving reasoning capabilities."}]}