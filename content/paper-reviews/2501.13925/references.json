{"references": [{"fullname_first_author": "Bai, J.", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "This paper introduces the Qwen large language model, which serves as a foundational model for GeoPixel's architecture."}, {"fullname_first_author": "Liu, Z.", "paper_title": "InternLM2 technical report", "publication_date": "2024-XX-XX", "reason": "This paper introduces the InternLM2 language model, which GeoPixel utilizes for its high resolution image analysis and grounded conversations."}, {"fullname_first_author": "Radford, A.", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-XX-XX", "reason": "This paper introduces CLIP, a vision encoder that forms a core component for GeoPixel's visual processing."}, {"fullname_first_author": "He, K.", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-XX-XX", "reason": "This paper introduces the MAE (Masked Autoencoder) architecture, which is used by GeoPixel's SAM-2 grounding vision encoder for hierarchical feature extraction."}, {"fullname_first_author": "Kirillov, A.", "paper_title": "Segment Anything", "publication_date": "2023-XX-XX", "reason": "This paper introduces SAM, a powerful image segmentation model that provides pixel-level grounding capabilities for GeoPixel."}]}