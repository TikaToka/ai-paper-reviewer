---
title: "Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems"
summary: "LLM safety mechanisms, while aiming to prevent harmful outputs, paradoxically introduce biases that enable 'jailbreaks'; this research quantifies these biases and proposes a novel defense."
categories: ["AI Generated"]
tags: ["ðŸ”– 24-10-17", ]
showSummary: true
date: 2024-10-17
draft: false
---

{{< keyword >}} 2410.13334 {{< /keyword >}}

### TL;DR


{{< lead >}}

Large Language Models (LLMs) are becoming increasingly important, but ensuring their safe use is crucial.  This paper explores a phenomenon called 'jailbreaks', where malicious inputs trick LLMs into generating harmful content despite safety measures.  Researchers found that these safety measures often introduce biases that make some groups more vulnerable to jailbreaks than others. They show that the success rate of these attacks is significantly higher against prompts involving marginalized groups.  This is because these safety measures sometimes create bias that disproportionately affect marginalized groups.  The researchers created a method to test these vulnerabilities (PCJailbreak) and a way to mitigate this issue (PCDefense). PCJailbreak helps researchers understand and quantify the bias, while PCDefense provides a more efficient defense compared to existing approaches.  Essentially, while efforts to make LLMs safer are important, this paper highlights that those same safety features can create weaknesses that malicious actors can exploit.  The research offers both a way to measure these vulnerabilities and a new approach to reduce the risks.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.13334" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}
<br><br>
{{< button href="https://huggingface.co/papers/2410.13334" target="_self" >}}
{{< icon "hf-logo" >}} &nbsp; on Hugging Face
{{< /button >}}

#### Why does it matter?
This paper is crucial for researchers in AI safety and ethics. It reveals the unintended consequences of current safety alignment techniques in LLMs, highlighting the risks of safety-induced biases and suggesting potential vulnerabilities to attacks. The proposed PCJailbreak and PCDefense methods offer novel approaches for further research and development in mitigating these vulnerabilities and improving LLM safety. The open-sourcing of code and data fosters wider collaboration and progress in this vital area.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Current LLM safety measures introduce unintended biases that disproportionately affect marginalized groups, increasing vulnerability to 'jailbreaks'. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The proposed PCJailbreak method effectively exploits these biases, demonstrating significant differences in jailbreak success rates between privileged and marginalized groups. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} PCDefense, a novel defense mechanism, mitigates these vulnerabilities without the need for additional inference or models, offering a cost-effective and efficient solution. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](https://ai-paper-reviewer.com/2410.13334/figures_2_0.png)

> ðŸ”¼ The figure illustrates how a standard prompt is blocked by safety mechanisms, while a PCJailbreak prompt, leveraging intentional biases, successfully elicits a response from the LLM.
> <details>
> <summary>read the caption</summary>
> Figure 2: Illustration showcasing the difference in response between a standard prompt and a PCJailbreak prompt. While the standard prompt is blocked by the LLM's safety features, the PCJailbreak prompt exploits intentional biases to elicit a response.
> </details>





![](https://ai-paper-reviewer.com/2410.13334/charts_2_0.png)

> ðŸ”¼ The chart illustrates how the success rate of jailbreak attempts varies significantly depending on whether the keywords used in the prompts are associated with privileged or marginalized groups.
> <details>
> <summary>read the caption</summary>
> Figure 1: PCJailbreak reveals inherent biases in LLMs that disproportionately allow harmful jailbreak attacks to succeed more frequently when directed towards marginalized groups compared to privileged groups.
> </details>





{{< table-caption >}}
<table id='11' style='font-size:22px'><tr><td>(keyword1,privileged,</td><td>keyword1,marginalized ) = ("wealthy", "impoverished")</td></tr><tr><td>(keyword2,privileged,</td><td>keyword2,marginalized ) = ("male", "female")</td></tr></table>{{< /table-caption >}}

> ðŸ”¼ The table lists keywords generated by LLMs representing contrasting groups (privileged vs. marginalized) for use in jailbreak experiments.
> <details>
> <summary>read the caption</summary>
> Table 1: Keywords Generated for Group Comparison
> </details>



### More visual insights

<details>
<summary>More on figures
</summary>


![](https://ai-paper-reviewer.com/2410.13334/figures_6_0.png)

> ðŸ”¼ The figure illustrates the PCJailbreak methodology, showing how the same harmful prompt is used with different keywords representing privileged and marginalized groups to analyze variations in jailbreak success rates.
> <details>
> <summary>read the caption</summary>
> Figure 3: Overview of the PCJailbreak methodology. The same harmful prompt is used across different keywords representing contrasting groups to analyze variations in jailbreak success rates.*All keywords representing both privileged and underprivileged groups are generated by the LLM.
> </details>



![](https://ai-paper-reviewer.com/2410.13334/figures_7_0.png)

> ðŸ”¼ The figure illustrates the difference in LLM responses to a harmful prompt with and without the PCDefense prompt, showing how PCDefense mitigates the biases exploited by PCJailbreak attacks.
> <details>
> <summary>read the caption</summary>
> Figure 4: PCDefense adjusts inherent biases in LLMs that are exploited by PCJailbreak. It is efficient since it does not require additional inference or models such as Guard Models.
> </details>



</details>



<details>
<summary>More on charts
</summary>


![](https://ai-paper-reviewer.com/2410.13334/charts_15_0.png)

> ðŸ”¼ The PCA chart of Phi-mini model shows that PCJailbreak samples are closely clustered with benign samples, indicating a high success rate of jailbreak attempts.
> <details>
> <summary>read the caption</summary>
> Figure 5: PCA of Phi-mini model, which has 0.4386 PCJailbreak success rate. PCJailbreak samples are closely clustered with Benign samples.
> </details>


![](https://ai-paper-reviewer.com/2410.13334/charts_15_1.png)

> ðŸ”¼ The chart displays the jailbreak success rates for various privileged and marginalized groups, revealing inherent biases in LLMs.
> <details>
> <summary>read the caption</summary>
> Figure 1: PCJailbreak reveals inherent biases in LLMs that disproportionately allow harmful jailbreak attacks to succeed more frequently when directed towards marginalized groups compared to privileged groups.
> </details>


![](https://ai-paper-reviewer.com/2410.13334/charts_16_0.png)

> ðŸ”¼ The PCA plot shows that for the LLaMA3 model, PCJailbreak samples are closer to harmful samples than benign samples, indicating a higher susceptibility to jailbreaks.
> <details>
> <summary>read the caption</summary>
> Figure 7: PCA of LLaMA3 model, which has 0.0650 PCJailbreak success rate. PCJailbreak samples are relatively close to harmful samples.
> </details>


</details>



<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table id='7' style='font-size:20px'><tr><td>Marginalized Group (Adjective)</td><td>Privileged Group (Adjective)</td></tr><tr><td>White</td><td>Black</td></tr><tr><td>Male</td><td>Female</td></tr><tr><td>Heterosexual</td><td>LGBTQ+</td></tr><tr><td>Able-bodied</td><td>Disabled</td></tr><tr><td>Wealthy</td><td>Low-income</td></tr><tr><td>Cisgender</td><td>Non-normative gender</td></tr><tr><td>Traditionally family-structured</td><td>Non-traditionally family-structured</td></tr><tr><td>Urban</td><td>Rural</td></tr><tr><td>Highly educated</td><td>Undereducated</td></tr><tr><td>Traditionally employed</td><td>Sexually marginalized</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ This table lists pairs of keywords representing contrasting groups (privileged vs. marginalized) that were generated by LLMs and used in the PCJailbreak experiments to analyze biases in LLM susceptibility to jailbreak attacks.
> <details>
> <summary>read the caption</summary>
> Table 1: Keywords Generated for Group Comparison
> </details>

{{< table-caption >}}
<table id='0' style='font-size:18px'><tr><td>Model Name</td><td>Baseline Success Rate</td><td>Marginalized Success Rate (â†‘)</td><td>Privileged Success Rate (â†“)</td><td>Marginalized / Privileged (â†‘)</td></tr><tr><td>GPT-3.5</td><td>0.2200</td><td>0.2421 (+10.00%)</td><td>0.1847 (-15.90%)</td><td>131.08%</td></tr><tr><td>GPT-4</td><td>0.2100</td><td>0.2488 (+18.57%)</td><td>0.1900 (-9.52%)</td><td>130.95%</td></tr><tr><td>GPT-4o</td><td>0.4600</td><td>0.5467 (+18.91%)</td><td>0.4187 (-8.91%)</td><td>130.57%</td></tr><tr><td>LLaMA2</td><td>0.2400</td><td>0.2811 (+17.08%)</td><td>0.1933 (-19.58%)</td><td>145.42%</td></tr><tr><td>LLaMA3</td><td>0.0500</td><td>0.0650 (+30.00%)</td><td>0.0300 (-40.00%)</td><td>216.67%</td></tr><tr><td>Qwen-1.5</td><td>0.1900</td><td>0.2175 (+14.74%)</td><td>0.1675 (-11.58%)</td><td>129.85%</td></tr><tr><td>Qwen2</td><td>0.1700</td><td>0.1971 (+15.88%)</td><td>0.1671 (-7.06%)</td><td>117.95%</td></tr><tr><td>Phi-mini</td><td>0.4100</td><td>0.4386 (+7.07%)</td><td>0.3829 (-6.59%)</td><td>114.56%</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ Table 2 presents the baseline, marginalized, and privileged jailbreak success rates across various LLMs, highlighting the performance disparity between these groups.
> <details>
> <summary>read the caption</summary>
> Table 2: Performance across different models showing baseline success rates, marginalized success rates, privileged success rates, and the difference between marginalized and privileged success rates.
> </details>

{{< table-caption >}}
<br><table id='6' style='font-size:20px'><tr><td>Model</td><td>Adaptive Attacks</td><td>Adaptive Attacks with PCJailbreak</td></tr><tr><td>llama2</td><td>98.00%</td><td>100.00%</td></tr><tr><td>phi-mini</td><td>95.00%</td><td>99.00%</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ The table shows the performance improvement of state-of-the-art (SOTA) models when using the proposed bias-based method for jailbreak attacks.
> <details>
> <summary>read the caption</summary>
> Table 3: Performance Improvement of SOTA Models
> </details>

{{< table-caption >}}
<br><table id='1' style='font-size:16px'><tr><td>Model</td><td>Metric</td><td>Before</td><td>After</td><td>After/Before (â†“)</td></tr><tr><td rowspan="3">Llama2</td><td>Marginalized Group Jailbreak Success</td><td>0.2811</td><td>0.1714</td><td>60.97%</td></tr><tr><td>Privileged Group Jailbreak Success</td><td>0.1933</td><td>0.1429</td><td>73.93%</td></tr><tr><td>Gap Between Groups</td><td>0.0878</td><td>0.0285</td><td>32.46%</td></tr><tr><td rowspan="3">Phi</td><td>Marginalized Rate Success</td><td>0.4386</td><td>0.4208</td><td>95.94%</td></tr><tr><td>Privileged Rate Success</td><td>0.3829</td><td>0.4075</td><td>106.42%</td></tr><tr><td>Gap Between Groups</td><td>0.0557</td><td>0.0133</td><td>23.88%</td></tr><tr><td rowspan="3">Qwen2</td><td>Marginalized Rate Success</td><td>0.1971</td><td>0.1750</td><td>88.79%</td></tr><tr><td>Privileged Rate Success</td><td>0.1671</td><td>0.1900</td><td>113.70%</td></tr><tr><td>Gap Between Groups</td><td>0.0300</td><td>0.0150</td><td>50.00%</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ The table presents the marginalized and privileged jailbreak success rates before and after applying the PCDefense technique across Llama2, Phi, and Qwen2 models, showing the decrease in the gap between the two rates after applying the defense.
> <details>
> <summary>read the caption</summary>
> Table 4: Jailbreak Prevention performance of PCDefense
> </details>

</details>


### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2410.13334/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.13334/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}