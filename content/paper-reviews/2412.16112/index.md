---
title: "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up"
summary: "CLEAR: 선형화된 어텐션으로 고해상도 이미지 생성 속도를 획기적으로 높이다!"
categories: ["AI Generated", "🤗 Daily Papers"]
tags: ["Computer Vision", "Image Generation", "🏢 National University of Singapore",]
showSummary: true
date: 2024-12-20
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2412.16112 {{< /keyword >}}
{{< keyword icon="writer" >}} Songhua Liu et el. {{< /keyword >}}
 
{{< keyword >}} 🤗 2024-12-23 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2412.16112" target="_self" >}}
↗ arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2412.16112" target="_self" >}}
↗ Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/clear-conv-like-linearization-revs-pre" target="_self" >}}
↗ Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

고해상도 이미지 생성은 딥러닝 분야의 중요한 과제 중 하나이며, 최근 **확산 트랜스포머(Diffusion Transformer)** 모델이 주목받고 있습니다. 하지만, 기존 확산 트랜스포머의 어텐션 메커니즘은 계산 복잡도가 높아 고해상도 이미지 생성에 시간이 오래 걸리는 문제점이 있습니다. 이로 인해, 고해상도 이미지 생성의 속도와 효율성을 향상시키는 것이 중요한 연구 과제로 떠오르고 있습니다.



본 논문에서는 이러한 문제를 해결하기 위해 **새로운 선형 어텐션 메커니즘인 CLEAR**을 제안합니다. CLEAR는 **국소적인(local) 어텐션**을 사용하여 계산 복잡도를 선형으로 줄이는 동시에, 기존 모델과의 성능 차이를 최소화합니다. 실험 결과, CLEAR는 **고해상도 이미지 생성 속도를 최대 6.3배 향상**시키고 **어텐션 계산량을 99.5% 절감**하는 것으로 나타났습니다. 또한, 다양한 모델과 플러그인과의 호환성 및 GPU 병렬 처리 지원을 통해 실제적인 적용 가능성을 높였습니다.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} CLEAR는 기존 확산 트랜스포머의 어텐션 메커니즘을 선형화하여 계산 복잡도를 획기적으로 줄임. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} 고해상도 이미지 생성 속도를 최대 6.3배 향상시키며, 8K 해상도 이미지 생성에서 어텐션 계산량을 99.5% 절감. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} 다양한 모델 및 플러그인과 호환되며, GPU 병렬 처리를 지원하여 실제 적용 가능성을 높임. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
본 논문은 **고해상도 이미지 생성에서의 확장성 문제**를 해결하기 위해 제시된 **선형 복잡도의 새로운 어텐션 메커니즘 CLEAR**을 제시하여, **기존의 어텐션 메커니즘의 계산 복잡도를 크게 줄이고 속도를 향상시키는 방법**을 제시합니다.  이는 고해상도 이미지 생성 분야의 **최신 연구 동향과 밀접하게 연관**되어 있으며, **다양한 모델 및 플러그인과의 호환성, GPU 병렬 처리 지원** 등 실용적인 측면까지 고려하여 **실제적인 적용 가능성**을 높였습니다.  따라서, 고해상도 이미지 생성 분야 연구자들에게 **상당한 영향**을 미칠 것으로 예상됩니다.

------
#### Visual Insights



![](https://arxiv.org/html/2412.16112/x2.png)

> 🔼 그림 1은 연구팀이 제안한 CLEAR 기법을 적용하여 선형화된 FLUX-1-dev 모델로 생성한 초고해상도 이미지 결과물들을 보여줍니다. 각 이미지의 오른쪽 상단에는 너비×높이 형식으로 해상도가 표시되어 있습니다. 이미지 생성에 사용된 프롬프트는 부록에서 확인할 수 있습니다.  이 그림은 다양한 스타일과 해상도의 이미지들을 보여줌으로써 CLEAR 기법의 성능과 일반화 능력을 시각적으로 보여주는 역할을 합니다.  다양한 크기의 이미지들이 생성되었다는 점을 통해,  CLEAR 기법이 다양한 해상도에 효과적으로 적용될 수 있음을 강조합니다.
> <details>
> <summary>read the caption</summary>
> Figure 1: Ultra-resolution results generated by the linearized FLUX.1-dev model with our approach CLEAR. Resolution is marked on the top-right corner of each result in the format of width×\times×height. Corresponding prompts can be found in the appendix.
> </details>





{{< table-caption >}}
| Method | Locality | Formulation | High-Rank | Feature |  
|---|---|---|---|---| 
| Linear Attention [12, 38, 65, 30] | Yes | No | No | Yes | 
| Sigmoid Attention [48] | Yes | No | Yes | Yes | 
| PixArt-Sigma [6] | Yes | Yes | Yes | No | 
| Agent Attention [20] | Maybe | Yes | Yes | No | 
| Strided Attention [7] | No | Yes | Yes | Yes | 
| Swin Transformer [39] | Yes | Yes | No | Yes | 
| Neighborhood Attention [21] | Yes | Yes | Yes | Yes | {{< /table-caption >}}

> 🔼 본 논문의 표 1은 DiT(Diffusion Transformers)를 선형화하는 데 중요한 네 가지 요소(locality, formulation consistency, high-rank attention maps, feature integrity)를 기반으로 기존의 효율적인 어텐션 메커니즘들을 요약 정리한 표입니다. 각 메커니즘이 이 네 가지 요소를 얼마나 충족하는지를 보여줌으로써, DiT 선형화에 적합한 메커니즘을 선택하는 데 도움을 줍니다.
> <details>
> <summary>read the caption</summary>
> Table 1: Summary of existing efficient attention mechanisms based on the four factors crucial for linearizing DiTs.
> </details>





### In-depth insights


#### Linear DiT
선형 DiT는 기존의 Diffusion Transformer (DiT) 모델의 계산 복잡도를 낮추기 위한 핵심 개념입니다. **기존 DiT 모델의 어텐션 메커니즘은 이미지 해상도에 따라 이차적으로 증가하는 계산량**을 가지지만, 선형 DiT는 이를 선형으로 줄여 고해상도 이미지 생성의 속도와 효율성을 크게 향상시킵니다. 이는 **새로운 어텐션 메커니즘 또는 기존 어텐션 메커니즘의 수정**을 통해 가능하며, 이를 통해 **매개변수의 수를 줄이거나 연산량을 줄이는 효과**를 기대할 수 있습니다.  **로컬 어텐션(local attention) 전략**을 채택하여 각 쿼리 토큰이 인접한 토큰들과만 상호작용하도록 함으로써 계산 복잡도를 줄이는 것이 중요한 기술적 측면입니다. 그러나 선형성을 달성하기 위한 **정확도 저하의 문제**가 존재하며, 이를 해결하기 위한 다양한 방법론과 **지식 증류 (knowledge distillation)** 기법들이 연구되고 있습니다.  **모델의 일반화 성능과 다양한 플러그인 및 GPU 병렬 처리와의 호환성** 역시 중요한 고려사항입니다.

#### CLEAR Method
CLEAR 기법은 기존의 어텐션 메커니즘의 계산 복잡도를 해결하기 위해 제안된 **선형 복잡도의 새로운 어텐션 전략**입니다.  **국소적 어텐션**을 통해 각 쿼리 토큰이 주변의 토큰들과만 상호작용하도록 제한함으로써 이미지 해상도에 대한 선형적인 계산 복잡도를 달성합니다. 이는 사전 훈련된 DiT의 효율성을 크게 향상시키는 핵심 요소입니다.  또한, **사전 훈련된 DiT를 효과적으로 선형화**하기 위한 4가지 요소(국소성, 공식 일관성, 고차원 어텐션 맵, 특징 무결성)를 제시하고 CLEAR 기법이 이러한 요소들을 모두 만족시킴을 보여줍니다.  **단 1만개의 자체 생성 샘플**로 미세 조정하여 효과적인 지식 전이를 달성하며, **계산량을 99.5% 감소**시키고 **생성 속도를 6.3배 향상**시키는 놀라운 결과를 보여줍니다.  **다양한 모델과 플러그인에 대한 제로샷 일반화 성능**과 **멀티 GPU 병렬 추론 지원** 또한 뛰어납니다.  그러나, **낮은 해상도에서는 원래 모델보다 속도가 느릴 수 있다는 점**은 한계로 지적할 수 있습니다.

#### Ablation Study
본 논문의 "Ablation Study"는 **모델 성능에 영향을 미치는 각 요소의 중요성을 객관적으로 평가하기 위한 실험적 분석**을 의미합니다.  구체적으로, 제안된 방법(예: CLEAR)에서 특정 구성 요소를 제거하거나 변경했을 때 모델 성능이 어떻게 변화하는지 정량적으로 측정하여 각 요소의 기여도를 파악합니다. 이를 통해, **모델의 설계 원칙을 검증**하고 **향후 연구 방향을 제시**하는 데 중요한 역할을 합니다. 예를 들어,  Locality, Formulation Consistency, High-rank Attention Maps, Feature Integrity 등의 요소 각각이 제안된 방법에 얼마나 중요하게 작용하는지, 그리고 각 요소를 제거했을 때 성능 저하가 얼마나 발생하는지 분석하여 **CLEAR의 효율성과 성능을 뒷받침**하는 증거로 활용될 수 있습니다.  **다양한 변수 조합에 대한 실험 결과**를 통해 **모델 개선 방향**을 제시하고, **추가적인 연구 필요성**을 밝힐 수 있습니다.  결론적으로, ablation study는 제안된 방법의 강점과 약점을 명확히 보여주는 핵심적인 부분으로서, 연구의 신뢰성과 객관성을 높이는 데 기여합니다.

#### Multi-GPU Infer.
본 논문의 "Multi-GPU Infer." 부분은 **대규모 이미지 생성 모델의 처리 속도 향상을 위한 다중 GPU 병렬 처리 전략**에 대해 논의합니다.  기존의 어텐션 메커니즘은 이미지 해상도에 따라 계산 복잡도가 기하급수적으로 증가하지만, 제안된 CLEAR 방법은 **국소적 어텐션 메커니즘**을 사용하여 이 문제를 해결합니다.  이를 통해 각 GPU는 이미지의 일부 영역만 처리하여 계산 부하를 분산하고, **통신 오버헤드를 최소화**합니다. 특히, **텍스트 토큰 처리**에 있어서는 모든 이미지 토큰과의 상호작용이 필요하지 않다는 점을 이용하여 효율적인 병렬 처리를 구현하며,  **성능 저하 없이 처리 속도를 크게 향상**시킬 수 있음을 보여줍니다.  **패치 병렬 처리 패러다임**을 통해 GPU 간의 통신량을 최소화함으로써, 고해상도 이미지 생성에서의 성능 저하를 완화하고 효율적인 병렬 처리를 가능하게 합니다.

#### Future Works
본 논문의 "향후 연구 방향"에 대한 심층적인 고찰은 다음과 같습니다. **CLEAR의 효율성을 더욱 높이기 위해** 다양한 최적화 기법들을 적용하는 연구가 필요합니다.  특히, **메모리 접근 방식 개선** 및 **병렬 처리 성능 향상** 연구를 통해 고해상도 이미지 생성 속도를 더욱 가속화할 수 있을 것입니다.  또한, **다양한 DiT 모델 및 플러그인과의 호환성 테스트**를 확장하여 CLEAR의 범용성을 입증하고, **다양한 응용 분야**에 적용 가능성을 확인하는 연구가 필요합니다.  마지막으로, **현재 제한점으로 지적된 저해상도 이미지 처리 성능 개선**을 위한 추가 연구가 중요합니다. 이를 위해,  **소규모 토큰 간의 상호작용 최적화** 및 **저해상도 이미지 특징을 효과적으로 포착하는 새로운 기법** 개발이 필요합니다. 이러한 연구들을 통해 CLEAR의 성능과 활용성을 더욱 향상시킬 수 있을 것입니다.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2412.16112/x3.png)

> 🔼 그림 2는 제안된 선형화된 DiT와 원래 FLUX-1-dev 모델의 속도와 GFLOPS를 비교한 그래프입니다.  단일 H100 GPU에서 20회의 잡음 제거 단계를 수행하여 속도를 평가했으며, FLOPS는 4×∑M×c라는 근사치를 사용하여 계산했습니다. 여기서 c는 특징 차원이고, M은 어텐션 마스크를 나타냅니다.  가시성을 높이기 위해, 세로축에는 log2 스케일을 적용했습니다. 자세한 데이터는 부록에 수록되어 있습니다.
> <details>
> <summary>read the caption</summary>
> Figure 2: Comparison of speed and GFLOPS between the proposed linearized DiT and the original FLUX.1-dev. Speed is evaluated by performing 20 denoising steps on a single H100 GPU. FLOPS is calculated with the approximation: 4×∑M×c4𝑀𝑐4\times\sum M\times c4 × ∑ italic_M × italic_c, where c𝑐citalic_c is the feature dimension and M𝑀Mitalic_M denotes the attention masks. log2subscript2\log_{2}roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is applied on both vertical axes for better visualization. The raw data are supplemented in the appendix.
> </details>



![](https://arxiv.org/html/2412.16112/x4.png)

> 🔼 그림 3은 FLUX-1.dev 모델에서 다양한 효율적인 어텐션 메커니즘을 적용했을 때의 초기 결과를 보여줍니다.  '들판 위에 앉아 있는 작은 파란색 비행기' 라는 프롬프트를 사용하여 생성한 이미지들을 비교 분석하여 어떤 메커니즘이 사전 훈련된 DiT를 선형화하는 데 가장 효과적인지 확인하기 위한 것입니다. 각 메커니즘은 이미지 생성의 속도, 정확도 및 메모리 효율성 측면에서 상이한 결과를 나타냅니다. 이 그림은 본 논문의 3장, 효율적인 어텐션 메커니즘의 분석과 비교를 통해 얻은 결과를 시각적으로 제시합니다.
> <details>
> <summary>read the caption</summary>
> Figure 3: Preliminary results of various efficient attention methods on FLUX-1.dev. The prompt is “A small blue plane sitting on top of a field”.
> </details>



![](https://arxiv.org/html/2412.16112/x5.png)

> 🔼 이 그림은 사전 훈련된 확산 트랜스포머(DiT)에서 중간 잡음 제거 단계에 대한 다양한 헤드의 어텐션 맵을 시각화한 것입니다.  각 맵은 토큰 간의 관계를 보여주며, 어두운 색은 낮은 상관관계를, 밝은 색은 높은 상관관계를 나타냅니다. 이 그림을 통해 사전 훈련된 DiT의 어텐션 메커니즘이 주로 지역적(local) 상호 작용에 집중함을 보여줍니다. 즉, 각 토큰은 주변의 작은 영역 내의 다른 토큰들과 주로 상호작용한다는 것을 시각적으로 보여줍니다. 이는 효율적인 선형화 전략을 설계하는 데 중요한 통찰력을 제공합니다.
> <details>
> <summary>read the caption</summary>
> Figure 4: Visualization of attention maps by various heads for an intermediate denoising step. Attention in pre-trained DiTs is largely conducted in a local fashion.
> </details>



![](https://arxiv.org/html/2412.16112/x6.png)

> 🔼 그림 5는 회전 위치 임베딩에 필요한 상대 거리를 잘라서 원격 특징과 지역 특징을 각각 변경해본 실험 결과를 보여줍니다. 원격 특징을 변경해도 이미지 품질에는 큰 영향이 없지만, 지역 특징을 변경하면 이미지 왜곡이 심하게 발생합니다. 그림 3과 동일한 텍스트 프롬프트와 원본 생성 결과를 사용했습니다. 이 그림은 회전 위치 임베딩에서 지역적 특징 정보가 얼마나 중요한지를 보여주는 실험 결과를 시각적으로 보여주는 역할을 합니다.
> <details>
> <summary>read the caption</summary>
> Figure 5: We try perturbing remote and local features respectively through clipping the relative distances required for rotary position embedding. Perturbing remote features has no obvious impact on image quality, whereas altering local features results in significant distortion. The text prompt and the original generation result are consistent with Fig. 3.
> </details>



![](https://arxiv.org/html/2412.16112/x7.png)

> 🔼 본 그림은 제안된 합성곱과 유사한 선형화 전략을 사전 훈련된 DiT에 적용한 것을 보여줍니다. 각 텍스트-이미지 조인트 어텐션 모듈에서 텍스트 쿼리는 모든 텍스트 및 이미지 토큰으로부터 정보를 집계하지만, 각 이미지 토큰은 국부 원형 윈도우 내의 토큰으로부터만 정보를 수집합니다.  이 그림을 통해 텍스트 정보는 전체 이미지에 대한 맥락을 제공하지만, 이미지 토큰 간의 관계는 국부적으로 처리되어 계산 복잡도를 줄이는 방법을 시각적으로 보여줍니다.
> <details>
> <summary>read the caption</summary>
> Figure 6: Illustration of the proposed convolution-like linearization strategy for pre-trained DiTs. In each text-image joint attention module, text queries aggregate information from all text and image tokens, while each image token gathers information only from tokens within a local circular window.
> </details>



![](https://arxiv.org/html/2412.16112/x8.png)

> 🔼 이 그림은 논문의 3.4절 '다중 GPU 병렬 추론' 부분에 해당하며, 다중 GPU 환경에서 효율적인 병렬 처리를 위한 CLEAR 방법의 구조를 보여줍니다.  각 GPU는 이미지의 일부 영역(패치)만 처리하며, 텍스트 질의는 할당된 GPU의 패치에서 키-값 토큰만 집계합니다. 모든 GPU의 어텐션 결과를 평균하여 최종 결과를 도출하는 방식입니다. 이러한 접근 방식을 통해 고품질 이미지를 생성하면서 계산 효율성을 높일 수 있습니다.
> <details>
> <summary>read the caption</summary>
> Figure 7: To enhance multi-GPU parallel inference, each text query aggregates only the key-value tokens from the patch managed by its assigned GPU, then averages the attention results across all GPUs, which also generates high-quality images.
> </details>



![](https://arxiv.org/html/2412.16112/x9.png)

> 🔼 그림 8은 CLEAR를 적용한 선형화된 FLUX-1.dev 모델과 원본 모델이 생성한 이미지들을 보여줍니다.  각 이미지는 동일한 프롬프트를 사용하여 생성되었으며, CLEAR를 통해 선형화된 모델이 원본 모델과 비슷한 수준의 이미지 품질을 유지하면서도 계산 효율성을 크게 향상시켰음을 보여줍니다.  이미지의 크기와 세부 묘사를 비교해 보면, 선형화 과정에서 이미지의 중요한 특징들이 잘 보존되었음을 확인할 수 있습니다.
> <details>
> <summary>read the caption</summary>
> Figure 8: Qualitative examples by the linearized FLUX-1.dev models with CLEAR and the original model.
> </details>



![](https://arxiv.org/html/2412.16112/x10.png)

> 🔼 그림 9는 제시된 CLEAR 방법을 사용하여 고해상도 이미지 생성(왼쪽), FLUX-1.schnell 모델에 제로샷 방식으로 적용(가운데), 그리고 ControlNet을 이용한 이미지 생성(오른쪽)의 정성적 결과를 보여줍니다. 왼쪽 열은 SDEdit을 사용하여 저해상도 이미지에서 고해상도 이미지를 생성한 결과를 보여주고, 가운데 열은 FLUX-1.dev 모델에서 학습된 CLEAR 모듈을 FLUX-1.schnell 모델에 적용했을 때의 결과를, 오른쪽 열은 ControlNet 플러그인과 함께 CLEAR 방법을 사용한 결과를 보여줍니다. G.T.는 Ground Truth 이미지를, Cond.는 Condition 이미지를 각각 나타냅니다.
> <details>
> <summary>read the caption</summary>
> Figure 9: Qualitative examples of using CLEAR with SDEdit [40] for high-resolution generation (left), FLUX-1.schnell in a zero-shot manner (middle), and ControlNet [69] (right). G.T. and Cond. denote ground-truth and condition images, separately.
> </details>



![](https://arxiv.org/html/2412.16112/x11.png)

> 🔼 이 그림은 논문의 4.3절 실험적 연구 부분에 속하며, 실제 데이터로 미세 조정하는 것보다 자체 생성 합성 데이터로 미세 조정하는 것이 성능이 더 우수함을 보여줍니다.  그래프는 훈련 단계에 따른 손실 값의 변화를 보여주는데, 합성 데이터를 사용한 경우 손실이 훨씬 빨리 감소하고 더 낮은 수준에 도달하는 것을 알 수 있습니다. 이는 합성 데이터가 모델의 훈련 과정에 더 적합한 분포를 가지고 있음을 시사합니다.  실제 데이터는 모델이 학습 과정에서 어려움을 겪는 분포를 가지고 있어 훈련이 더 어려워짐을 보여줍니다.
> <details>
> <summary>read the caption</summary>
> Figure 10: Fine-tuning on real data results in inferior performance compared to fine-tuning on self-generated synthetic data.
> </details>



![](https://arxiv.org/html/2412.16112/x12.png)

> 🔼 그림 11은 본 논문에서 FLUX-1.dev 모델에 다양한 효율적인 어텐션 메커니즘을 적용했을 때의 학습 동향을 보여줍니다.  각각의 효율적인 어텐션 기법 (Sigmoid Attention, Linear Attention, PixArt-Sigma, Agent Attention, Strided Attention, Swin Transformer)을 FLUX-1.dev 모델에 적용하여 학습시켰을 때의 손실 함수 값 변화를 반복 횟수에 따라 나타낸 그래프입니다. 이 그래프를 통해 각 기법의 수렴 속도와 안정성을 비교 분석하여, 어떤 어텐션 메커니즘이 FLUX-1.dev 모델에 가장 적합한지, 그리고 효율성과 성능 간의 균형을 어떻게 맞출 수 있는지를 보여줍니다. 특히, 본 논문에서 제안하는 CLEAR 방법의 학습 결과도 포함되어 있어, 기존 방법들과 비교하여 CLEAR의 효율성과 성능을 확인할 수 있습니다.
> <details>
> <summary>read the caption</summary>
> Figure 11: Training dynamics of various efficient attention alternatives on FLUX-1.dev.
> </details>



![](https://arxiv.org/html/2412.16112/x13.png)

> 🔼 그림 12는 CLEAR 기법을 사용하여 선형화된 DiT(Diffusion Transformer)가 고해상도 추론을 위한 다양한 파이프라인과 호환됨을 보여줍니다.  즉, 기존의 고해상도 이미지 생성을 위한 여러 기법들(예: SDEdit, I-Max)과도 문제없이 연동되어 고품질의 고해상도 이미지를 생성할 수 있음을 시각적으로 보여주는 그림입니다.  그림 15에는 사용된 프롬프트가 나와있습니다.
> <details>
> <summary>read the caption</summary>
> Figure 12: The linearized DiTs by CLEAR are compatible with various pipelines dedicated for high-resolution inference. The prompt is shown in Fig. 15.
> </details>



![](https://arxiv.org/html/2412.16112/x14.png)

> 🔼 그림 13은 FLUX-1.dev(상단)과 SD3.5-Large(하단) 모델을 사용하여 생성한 이미지의 질적 비교 결과를 보여줍니다. 왼쪽은 원본 모델의 결과이고, 오른쪽은 CLEAR 선형화 모델의 결과입니다. 각 이미지에 사용된 프롬프트는 그림 16에 나열되어 있습니다. 이 그림은 원본 모델과 CLEAR 선형화 모델의 이미지 생성 품질을 직접적으로 비교하여 CLEAR 선형화 기법의 효과를 시각적으로 보여주는 역할을 합니다.  두 모델의 성능 차이를 좀 더 명확하게 비교하기 위해, 같은 프롬프트를 사용하여 생성된 이미지들을 나란히 배치하여 비교 분석하도록 구성되어 있습니다.
> <details>
> <summary>read the caption</summary>
> Figure 13: Qualitative comparisons on FLUX-1.dev (top) and SD3.5-Large (bottom). The left subplots are results by the original models while the right ones are by the CLEAR linearized models. Prompts are listed in Fig. 16.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Formulation | Consistency |
|---|---|{{< /table-caption >}}
> 🔼 표 2는 본 논문에서 제안된 CLEAR 모델을 포함하여 기존의 효율적인 어텐션 메커니즘과 원본 FLUX-1.dev 모델의 정량적 결과를 비교 분석한 표입니다. COCO2014 검증 데이터셋의 5,000개 이미지를 사용하여 1024x1024 해상도로 평가되었으며, FID, LPIPS, CLIP-I, DINO, IS, GFLOPS 등 다양한 지표를 통해 성능을 측정했습니다.  다양한 r (수신 범위) 값에 따른 CLEAR 모델의 성능 변화를 확인할 수 있습니다.  이를 통해 CLEAR 모델의 효율성과 성능을 정량적으로 보여줍니다.
> <details>
> <summary>read the caption</summary>
> Table 2: Quantitative results of the original FLUX-1.dev, previous efficient attention methods, and CLEAR proposed in this paper with various r𝑟ritalic_r on 5,000 images from the COCO2014 validation dataset at a resolution of 1024×1024102410241024\times 10241024 × 1024.
> </details>

{{< table-caption >}}
| High-Rank |
|---|---| 
| Attention Maps |{{< /table-caption >}}
> 🔼 표 3은 원본 FLUX-1.dev 모델과 다양한 크기의 지역적 수용 범위(r)를 갖는 제안된 CLEAR 방법의 정량적 결과를 보여줍니다. COCO2014 검증 데이터셋의 1,000개 이미지를 사용하여 2048x2048 및 4096x4096 해상도에서 평가했습니다.  FID, LPIPS, CLIP-I, DINO, PSNR, SSIM 지표를 통해 이미지 품질 및 유사성을 평가하였습니다. 이 표는 제안된 방법이 다양한 해상도에서도 원본 모델과 비슷하거나 더 나은 성능을 보이는지 확인하는 데 도움이 됩니다.
> <details>
> <summary>read the caption</summary>
> Table 3: Quantitative results of the original FLUX-1.dev and our CLEAR with various r𝑟ritalic_r on 1,000 images from the COCO2014 validation dataset at resolutions of 2048×2048204820482048\times 20482048 × 2048 and 4096×4096409640964096\times 40964096 × 4096.
> </details>

{{< table-caption >}}
| Feature | Integrity |
|---|---|{{< /table-caption >}}
> 🔼 이 표는 FLUX-1.dev에서 학습된 CLEAR 계층을 FLUX-1.schnell에 적용했을 때의 정량적 제로샷 일반화 결과를 보여줍니다.  FLUX-1.dev 모델에서 학습된 CLEAR 모듈을 FLUX-1.schnell 모델에 적용하여 성능 저하 없이 제로샷으로 일반화가 잘 되는지 평가한 결과입니다. FID, LPIPS, CLIP-I, DINO 지표를 사용하여 원본 모델과의 성능 차이를 비교 분석합니다.
> <details>
> <summary>read the caption</summary>
> Table 4: Quantitative zero-shot generalization results to FLUX-1.schnell using CLEAR layers trained on FLUX-1.dev.
> </details>

{{< table-caption >}}
| Method/Setting | Against Original |  |  |  | Against Real |  | CLIP-T (↑) | IS (↑) | GFLOPS (↓) |
|---|---|---|---|---|---|---|---|---|---|---|
| Original FLUX-1.dev | - | - | - | - | 34.93 | 0.81 | 31.06 | 38.25 | 260.9 |
| Sigmoid Attention [48] | 447.80 | 0.91 | 41.34 | 0.25 | 457.69 | 0.84 | 17.53 | 1.15 | 260.9 |
| Linear Attention [12, 38, 65, 30] | 324.54 | 0.85 | 51.37 | 2.17 | 325.58 | 0.87 | 19.16 | 2.91 | 174.0 |
| PixArt-Simga [6] | 30.64 | 0.56 | 86.43 | 71.45 | 33.38 | 0.88 | 31.12 | 32.14 | 67.7 |
| Agent Attention [20] | 69.85 | 0.65 | 78.18 | 56.09 | 54.31 | 0.87 | 30.38 | 21.03 | 80.5 |
| Strided Attention [7] | 24.88 | 0.61 | 85.50 | 70.72 | 35.27 | 0.89 | 30.62 | 32.05 | 67.7 |
| Swin Transformer [39] | 18.90 | 0.65 | 85.72 | 73.43 | 32.20 | 0.87 | 30.64 | 34.68 | 67.7 |
| CLEAR (r=8) | 15.53 | 0.64 | 86.47 | 74.36 | 32.06 | 0.83 | 30.69 | 34.47 | 63.5 |
| w. distill | 13.07 | 0.62 | 88.56 | 77.66 | 33.06 | 0.82 | 30.82 | 35.92 | 63.5 |
| CLEAR (r=16) | 14.27 | 0.60 | 88.51 | 78.35 | 32.36 | 0.89 | 30.90 | 37.13 | 80.6 |
| w. distill | 13.72 | 0.58 | 88.53 | 77.30 | 33.63 | 0.88 | 30.65 | 37.84 | 80.6 |
| CLEAR (r=32) | 11.07 | 0.52 | 89.92 | 81.20 | 33.47 | 0.82 | 30.96 | 37.80 | 154.1 |
| w. distill | 8.85 | 0.46 | 92.18 | 85.44 | 34.88 | 0.81 | 31.00 | 39.12 | 154.1 |{{< /table-caption >}}
> 🔼 이 표는 제안된 CLEAR 기법을 사전 훈련된 ControlNet에 적용했을 때의 제로샷 일반화 성능을 보여줍니다. 회색조 이미지 조건을 사용하여 COCO2014 검증 데이터셋의 1,000개 이미지에 대해 평가했습니다. RMSE는 조건 이미지와 비교하여 계산된 제곱근 평균 제곱 오차를 나타냅니다.  표에는 FID, LPIPS, CLIP-I, DINO와 같은 다양한 지표와 함께 PSNR, SSIM 및 RMSE 점수가 포함되어 있어, CLEAR의 이미지 생성 품질 및 조건 이미지와의 일관성을 다각적으로 평가할 수 있습니다.
> <details>
> <summary>read the caption</summary>
> Table 5: Quantitative zero-shot generalization results of the proposed CLEAR to a pre-trained ControlNet with grayscale image conditions on 1,000 images from the COCO2014 validation dataset. RMSE here denotes Root Mean Squared Error computed against condition images.
> </details>

{{< table-caption >}}
| Setting | PSNR (↑) | SSIM (↑) | FID (↓) | LPIPS (↓) | CLIP-I (↑) | DINO (↑) | CLIP-T (↑) | IS (↑) | GFLOPS (↓) |
|---|---|---|---|---|---|---|---|---|---| 
| **–1024×1024→2048×2048–** |  |  |  |  |  |  |  |  |  |
| FLUX-1.dev | - | - | - | - | - | - | 31.11 | 24.53 | 3507.9 |
| CLEAR (r=8) | 27.57 | 0.91 | 13.55 | 0.12 | 98.97 | 98.37 | 31.09 | 25.05 | 246.2 |
| CLEAR (r=16) | 27.60 | 0.92 | 13.43 | 0.12 | 98.97 | 98.34 | 31.08 | 25.46 | 352.6 |
| CLEAR (r=32) | 28.95 | 0.94 | 10.87 | 0.10 | 99.23 | 98.82 | 31.09 | 25.48 | 724.3 |
| **–2048×2048→4096×4096–** |  |  |  |  |  |  |  |  |  |
| FLUX-1.dev | - | - | - | - | - | - | 31.29 | 24.36 | 53604.4 |
| CLEAR (r=8) | 26.19 | 0.87 | 20.87 | 0.22 | 98.02 | 96.56 | 31.16 | 25.87 | 979.3 |
| CLEAR (r=16) | 26.98 | 0.88 | 16.20 | 0.19 | 98.48 | 97.64 | 31.25 | 25.13 | 1433.2 |
| CLEAR (r=32) | 27.70 | 0.90 | 13.56 | 0.17 | 98.72 | 98.21 | 31.20 | 24.81 | 3141.7 |{{< /table-caption >}}
> 🔼 표 6은 식 7에서 근사치를 사용하여 다양한 패치 수를 사용한 패치별 다중 GPU 병렬 추론 결과를 보여줍니다.  이 표는 다양한 GPU 수에 따른 처리 시간과 처리량을 비교하여 CLEAR 모델의 확장성과 효율성을 보여줍니다.  특히,  텍스트 토큰에 대한 정보를 모든 이미지 토큰에서 가져와야 하는 제약에도 불구하고,  CLEAR 모델이 다중 GPU 환경에서 효율적으로 작동함을 보여줍니다.  단일 GPU와 비교하여 처리 시간이 얼마나 단축되는지,  그리고 다양한 GPU 수에 따른 성능 향상을 정량적으로 제시합니다.
> <details>
> <summary>read the caption</summary>
> Table 6: Results of patch-wise multi-GPU parallel inference with various numbers of patches using the approximation in Eq. 7.
> </details>

{{< table-caption >}}
| Setting | Against Original |  |  |  |  | Against Real |  | CLIP-T (↑) | IS (↑) |
|---|---|---|---|---|---|---|---|---|---| 
| FLUX-1.dev | - | - | - | - | 29.19 | 0.83 | 31.53 | 36.41 |
| CLEAR (r=8) | 13.62 | 0.62 | 88.91 | 78.36 | 33.51 | 0.81 | 31.35 | 38.42 |
| CLEAR (r=16) | 12.51 | 0.58 | 90.43 | 81.32 | 34.43 | 0.82 | 31.38 | 39.66 |
| CLEAR (r=32) | 12.43 | 0.57 | 90.70 | 82.61 | 33.57 | 0.83 | 31.48 | 39.68 |{{< /table-caption >}}
> 🔼 표 7은 논문의 그림 2에서 보여지는 효율성 비교 결과에 대한 원시 데이터를 보여줍니다. 그림 2는 제안된 선형화된 DiT(확산 변환기)와 기존 FLUX-1-dev 모델의 속도와 GFLOPS(초당 부동 소수점 연산 수)를 비교한 그래프입니다. 표 7은 이 그래프를 생성하는 데 사용된 실제 데이터 값들을 보여주어, 그림 2의 결과에 대한 상세한 수치적 근거를 제공합니다.  픽셀 수에 따른 속도(이미지 생성 시간)와 GFLOPS(계산 복잡도)를 보여주는 값들이 포함되어 있습니다.
> <details>
> <summary>read the caption</summary>
> Table 7: Raw data for Fig. 2 on efficiency comparisons.
> </details>

{{< table-caption >}}
| Setting | PSNR (↑) | SSIM (↑) | FID (↓) | LPIPS (↓) | CLIP-I (↑) | DINO (↑) | Against Real FID (↓) | Against Real LPIPS (↓) | CLIP-T (↑) | IS (↑) | RMSE (↓) |
|---|---|---|---|---|---|---|---|---|---|---|---|
| FLUX-1.dev | - | - | - | - | - | - | 40.25 | 0.32 | 30.16 | 22.22 | 0.0385 |
| CLEAR (r=8) | 25.95 | 0.93 | 26.14 | 0.19 | 93.39 | 94.24 | 43.82 | 0.31 | 29.90 | 21.29 | 0.0357 |
| CLEAR (r=16) | 28.24 | 0.95 | 16.86 | 0.13 | 96.00 | 96.73 | 40.45 | 0.31 | 30.19 | 22.34 | 0.0395 |
| CLEAR (r=32) | 30.59 | 0.97 | 11.57 | 0.09 | 97.33 | 98.12 | 40.21 | 0.31 | 30.21 | 21.94 | 0.0419 |{{< /table-caption >}}
> 🔼 표 8은 본 논문에서 제안된 CLEAR 방법을 사용하여 선형화된 Stable Diffusion 3-Large 모델과 원본 모델의 정량적 결과를 비교 분석한 표입니다. COCO2014 검증 데이터셋의 5,000개 이미지를 사용하여 1024x1024 해상도에서 FID, LPIPS, CLIP-I, DINO 점수를 측정하였습니다.  CLEAR 모델의 성능이 원본 모델과 얼마나 유사한지, 그리고 계산 효율성 측면에서 얼마나 개선되었는지를 보여줍니다.
> <details>
> <summary>read the caption</summary>
> Table 8: Quantitative results of the original SD3-Large and its linearized version by CLEAR proposed in this paper on 5,000 images from the COCO2014 validation dataset at a resolution of 1024×1024102410241024\times 10241024 × 1024.
> </details>

{{< table-caption >}}
| Setting | Against Original | Against Real | CLIP-T (↑) | IS (↑) | 
|---|---|---|---|---| 
| CLEAR (r=16) | - | - | - | - | 33.63 | 0.88 | 30.65 | 37.84 |
| N=2 | 11.55 | 0.51 | 90.46 | 80.89 | 33.74 | 0.81 | 31.21 | 39.26 |
| N=4 | 12.78 | 0.54 | 89.74 | 79.99 | 33.07 | 0.81 | 31.27 | 40.01 |
| N=8 | 14.21 | 0.57 | 88.92 | 78.65 | 32.26 | 0.80 | 31.22 | 39.34 |{{< /table-caption >}}
> 🔼 이 표는 제안된 CLEAR 기법을 사전 훈련된 ControlNet에 적용했을 때의 제로샷 일반화 성능을 보여줍니다. 타일 이미지 조건과 흐릿한 이미지 조건 하에서 COCO2014 검증 데이터셋의 1,000개 이미지를 사용하여 평가했습니다. RMSE는 조건 이미지에 대한 제곱근 평균 제곱 오차를 나타냅니다. 즉,  CLEAR가 ControlNet과 얼마나 잘 호환되는지, 그리고 다양한 이미지 스타일(타일, 흐릿함)에 대한 일반화 능력을 정량적으로 보여줍니다. FID, LPIPS, CLIP-I, DINO와 같은 다양한 지표를 사용하여 이미지 품질과 유사성을 평가합니다.
> <details>
> <summary>read the caption</summary>
> Table 9: Quantitative zero-shot generalization results of the proposed CLEAR to a pre-trained ControlNet with tiled image conditions and blur image conditions on 1,000 images from the COCO2014 validation dataset. RMSE here denotes Root Mean Squared Error computed against condition images.
> </details>

{{< table-caption >}}
| Setting | 1024x1024 | 2048x2048 | 4096x4096 | 8192x8192 | 1024x1024 | 2048x2048 | 4096x4096 | 8192x8192 |
|---|---|---|---|---|---|---|---|---|
| **Running Time (Sec. / 50 Steps)** |  |  |  |  |  |  |  |  |
| FLUX-1.dev | 4.45 | 20.90 | 148.97 | 1842.48 | 0.26 | 3.51 | 53.60 | 847.73 |
| CLEAR (r=8) | 4.40 | 15.67 | 69.41 | 293.50 | 0.06 | 0.25 | 0.98 | 3.92 |
| CLEAR (r=16) | 4.56 | 17.19 | 83.13 | 360.83 | 0.09 | 0.35 | 1.43 | 5.79 |
| CLEAR (r=32) | 5.45 | 19.95 | 109.57 | 496.22 | 0.15 | 0.72 | 3.14 | 13.09 |{{< /table-caption >}}
> 🔼 표 10은 HGX H100 8-GPU 서버에서 50개의 디노이징 단계에 대한 다중 GPU 병렬 추론의 효율성을 초당 시간(sec./50 steps)과 계층당 TFLOPS(TFLOPS/Layer)로 측정한 결과를 보여줍니다. 비동기식 통신을 위해 Distrifusion [34]을 FLUX-1.dev에 적용했습니다. 가속 비율은 빨간색으로 강조 표시되어 있습니다. GPU당 처리되는 패치 크기가 경계 크기보다 작기 때문에 1024x1024 해상도에서 r=16인 CLEAR의 결과는 사용할 수 없습니다(NA). OOM은 메모리 부족 오류를 나타냅니다.  표는 다양한 해상도(1024x1024, 2048x2048, 4096x4096, 8192x8192)에서 GPU 개수(1, 2, 4, 8)에 따른 FLUX-1.dev와 CLEAR (r=8, r=16, r=32)의 실행 시간 및 TFLOPS를 비교 분석하여, 제안된 CLEAR 방법의 병렬 처리 성능 향상 효과를 정량적으로 보여줍니다.
> <details>
> <summary>read the caption</summary>
> Table 10: Efficiency of multi-GPU parallel inference measured by sec./50 denoising steps on a HGX H100 8-GPU server. We adapt Distrifusion [34] to FLUX-1.dev here for asynchronous communication. The ratios of acceleration are highlighted with red. Results of CLEAR with r=16𝑟16r=16italic_r = 16 at the 1024×1024102410241024\times 10241024 × 1024 resolution are not available (NA) because the patch size processed by each GPU is smaller than the boundary size. OOM denotes encountering out-of-memory error.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}