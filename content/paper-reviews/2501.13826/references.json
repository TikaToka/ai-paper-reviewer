{"references": [{"fullname_first_author": "Anthropic", "paper_title": "Claude Team. Introducing Claude 3.5 Sonnet.", "publication_date": "2024-00-00", "reason": "This paper introduces a large language model that is used as a baseline for comparison in the experiments of the main paper."}, {"fullname_first_author": "Michael Boratko", "paper_title": "A systematic classification of knowledge, reasoning, and context within the ARC dataset.", "publication_date": "2018-00-00", "reason": "This paper is referenced as an early benchmark for knowledge-driven tasks, providing context for the Video-MMMU benchmark."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding.", "publication_date": "2021-00-00", "reason": "This paper introduces a multi-task language understanding benchmark that is compared against Video-MMMU"}, {"fullname_first_author": "Pan Lu", "paper_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering.", "publication_date": "2022-00-00", "reason": "This paper is cited as a multimodal benchmark focusing on science questions, highlighting the increasing complexity and multi-modality of knowledge-driven benchmarks."}, {"fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI.", "publication_date": "2024-00-00", "reason": "This paper introduces a multi-discipline multimodal understanding benchmark that is directly compared to and contrasted with Video-MMMU, making it a key reference for understanding the landscape of similar benchmarks."}]}