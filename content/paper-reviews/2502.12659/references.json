{"references": [{"fullname_first_author": "Maksym Andriushchenko", "paper_title": "Agentharm: A benchmark for measuring harmfulness of llm agents", "publication_date": "2024-10-24", "reason": "This paper is a benchmark for evaluating the harmfulness of LLM agents, which is directly relevant to the paper's safety assessment of large language models."}, {"fullname_first_author": "Manish Bhatt", "paper_title": "Cyberseceval 2: A wide-ranging cybersecurity evaluation suite for large language models", "publication_date": "2024-04-13", "reason": "This paper provides a comprehensive cybersecurity evaluation suite for LLMs, which is crucial for assessing the safety of reasoning models in real-world applications."}, {"fullname_first_author": "Nicolai Dorka", "paper_title": "Quantile regression for distributional reward models in rlhf", "publication_date": "2024-09-10", "reason": "This paper introduces a novel method for evaluating reward models, which is relevant to the paper's harmfulness evaluation of model responses."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-21", "reason": "This paper introduces a family of Llama 3 models, which are used as baseline models for comparison in the safety assessment."}, {"fullname_first_author": "Melody Y Guan", "paper_title": "Deliberative alignment: Reasoning enables safer language models", "publication_date": "2024-12-16", "reason": "This paper explores the concept of deliberative alignment for improving the safety of language models, which is directly relevant to the paper's discussion of safety alignment techniques for reasoning models."}]}