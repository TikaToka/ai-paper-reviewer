{"references": [{"fullname_first_author": "Yash Goyal", "paper_title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering", "publication_date": "2017-00-00", "reason": "This paper is foundational to the field of Visual Question Answering (VQA), introducing a critical evaluation methodology that the current paper builds upon."}, {"fullname_first_author": "Drew A Hudson", "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "publication_date": "2019-00-00", "reason": "This paper introduced a significant VQA dataset (GQA) that has been widely used for benchmarking visual reasoning capabilities, forming a key dataset used in the current work."}, {"fullname_first_author": "Danna Gurari", "paper_title": "VizWiz grand challenge: Answering visual questions from blind people", "publication_date": "2018-00-00", "reason": "This paper introduced the VizWiz dataset, which focused on questions posed by blind users, addressing an important aspect of accessibility in VQA, and is used as a dataset in the current work."}, {"fullname_first_author": "Dustin Schwenk", "paper_title": "A-OKVQA: A benchmark for visual question answering using world knowledge", "publication_date": "2022-00-00", "reason": "This paper introduced the A-OKVQA dataset that explicitly tests the use of world knowledge, providing a relevant and challenging dataset for the assessment of VQA systems, which is used in the benchmark of the current paper."}, {"fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI", "publication_date": "2024-00-00", "reason": "This paper introduced a very recent, large-scale benchmark (MMMU) designed for evaluating multimodal reasoning in VLMs, and is used in the current paper to illustrate the effectiveness of the proposed method."}]}