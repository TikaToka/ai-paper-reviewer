<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer &#183; AI Paper Reviews by AI</title>
<meta name=title content="LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer &#183; AI Paper Reviews by AI"><meta name=description content="LLaVA-UHD v2는 계층적 윈도우 변환기를 이용, 고해상도 특징 피라미드를 통합하여 다양한 시각적 세부 정보를 포착하는 혁신적인 다중 모달 언어 모델입니다."><meta name=keywords content="Multimodal Learning,Vision-Language Models,🏢 Tsinghua University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13871/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13871/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer"><meta property="og:description" content="LLaVA-UHD v2는 계층적 윈도우 변환기를 이용, 고해상도 특징 피라미드를 통합하여 다양한 시각적 세부 정보를 포착하는 혁신적인 다중 모달 언어 모델입니다."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-18T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-18T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Vision-Language Models"><meta property="article:tag" content="🏢 Tsinghua University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13871/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13871/cover.png"><meta name=twitter:title content="LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer"><meta name=twitter:description content="LLaVA-UHD v2는 계층적 윈도우 변환기를 이용, 고해상도 특징 피라미드를 통합하여 다양한 시각적 세부 정보를 포착하는 혁신적인 다중 모달 언어 모델입니다."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer","headline":"LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer","abstract":"LLaVA-UHD v2는 계층적 윈도우 변환기를 이용, 고해상도 특징 피라미드를 통합하여 다양한 시각적 세부 정보를 포착하는 혁신적인 다중 모달 언어 모델입니다.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.13871\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-12-18T00:00:00\u002b00:00","datePublished":"2024-12-18T00:00:00\u002b00:00","dateModified":"2024-12-18T00:00:00\u002b00:00","keywords":["Multimodal Learning","Vision-Language Models","🏢 Tsinghua University"],"mainEntityOfPage":"true","wordCount":"3363"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.13871/cover_hu15592072681340090990.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.13871/>LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-18T00:00:00+00:00>18 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>3363 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">16 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.13871/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.13871/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🤗 Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/vision-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Vision-Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-tsinghua-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🏢 Tsinghua University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#visual-granularity>Visual Granularity</a></li><li><a href=#hiwin-transformer>Hiwin Transformer</a></li><li><a href=#feature-pyramid>Feature Pyramid</a></li><li><a href=#mllm-enhancement>MLLM Enhancement</a></li><li><a href=#future-mllm>Future MLLM</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#visual-granularity>Visual Granularity</a></li><li><a href=#hiwin-transformer>Hiwin Transformer</a></li><li><a href=#feature-pyramid>Feature Pyramid</a></li><li><a href=#mllm-enhancement>MLLM Enhancement</a></li><li><a href=#future-mllm>Future MLLM</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.13871</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Yipeng Zhang et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>🤗 2024-12-19</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.13871 target=_self role=button>↗ arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.13871 target=_self role=button>↗ Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/llava-uhd-v2-an-mllm-integrating-high target=_self role=button>↗ Papers with Code</a></p><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>대규모 언어 모델(LLM)에 시각 정보를 통합하는 것은 시각적 질문 응답과 같은 다양한 작업에 큰 진전을 가져왔습니다. 그러나 기존의 비전 트랜스포머(ViT) 기반 MLLM은 다양한 시각적 수준의 정보가 부족하여 다양한 의미적 세분성과의 정렬을 방해하여 보편적인 MLLM 작업 해결에 만족스럽지 못한 성능을 보였습니다. 이러한 문제는 다양한 시각적 세부 정보를 포착하고 통합하는 고해상도 특징 피라미드를 구축하지 못하기 때문입니다.</p><p>본 논문에서는 계층적 윈도우 변환기를 중심으로 한 고급 MLLM인 LLaVA-UHD v2를 제시합니다. LLaVA-UHD v2는 고해상도 특징 피라미드를 구성하고 통합함으로써 다양한 시각적 세분성을 포착합니다. 역 피라미드와 계층적 윈도우 어텐션이라는 두 가지 주요 모듈로 구성된 Hiwin 변환기는 다양한 시각적 세분성을 포착하고, 여러 수준의 특징 맵을 압축하여 언어 생성에 필요한 다양한 의미적 세분성을 제공합니다. 실험 결과, LLaVA-UHD v2는 여러 벤치마크에서 기존 MLLM보다 우수한 성능을 보였습니다.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-2888541790b3688e7d8917966f8529a9></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-2888541790b3688e7d8917966f8529a9",{strings:[" 계층적 윈도우 변환기를 사용하여 고해상도 특징 피라미드를 효과적으로 통합하는 새로운 방법 제시 "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-4409da973fbcf3d7a628e6346ef0cf5a></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-4409da973fbcf3d7a628e6346ef0cf5a",{strings:[" 기존 MLLM 대비 14개의 벤치마크에서 평균 3.7% 향상된 성능 달성 "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-6107aae25f2bb280b24050e8e15e5936></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-6107aae25f2bb280b24050e8e15e5936",{strings:[" 모든 데이터, 모델 및 코드 공개를 통해 후속 연구 지원 "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>본 논문은 <strong>다양한 시각적 세부 정보를 통합하여 언어 모델의 성능을 향상시키는 새로운 방법</strong>을 제시합니다. <strong>고해상도 특징 피라미드를 계층적 윈도우 변환기를 통해 통합</strong>하는 것은 시각 언어 모델링 분야에서 중요한 발전이며, 특히 고해상도 이미지 이해에 필요한 다양한 시각적 정보를 효과적으로 처리하는 데 기여합니다. <strong>공개된 데이터, 모델 및 코드</strong>는 후속 연구를 위한 중요한 자원이 될 것입니다.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.13871/x1.png alt></figure></p><blockquote><p>🔼 본 그림은 논문에서 제안하는 LLaVA-UHD v2 모델과 기존 다중 모달 대규모 언어 모델(MLLM)의 비교를 보여줍니다. (a)는 기존 MLLM들이 일반적으로 ViT 특징을 MLP 또는 perceiver re-sampler를 사용하여 언어 공간에 정렬하는 방식을 나타내며, 이는 시각적 세부 정보가 부족함을 의미합니다. (b)는 여러 개의 시각적 인코더를 결합하는 것이 보편적이지 않고 계산적으로 집약적임을 보여줍니다. (c)는 LLaVA-UHD v2가 계층적 윈도우 변환기를 사용하여 역 특징 피라미드를 구축하고 시각 토큰으로 압축하여 다양한 의미적 세분성을 제공함으로써 언어 생성에 도움을 주는 방식을 보여줍니다. 즉, LLaVA-UHD v2는 다양한 시각적 해상도를 효과적으로 활용하여 언어 생성 성능을 향상시킨다는 점을 강조합니다.</p><details><summary>read the caption</summary>Figure 1: Comparison of LLaVA-UHD v2 with other MLLMs. (a) MLLMs typically align ViT features to language space using MLPs [63] or perceiver re-samplers [6, 52], lacking visual granularity. (b) Combining multiple visual encoders is non-universal and computationally intensive. (c) LLaVA-UHD v2 employs the Hiwin transformer to build an inverse feature pyramid and compress it into visual tokens, providing various semantic granularity for language generation.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>#Data</th><th>MaxRes.</th><th>#FLOPs.</th><th>Avg.</th><th>VQA<sup>D</sup></th><th>Bench<sup><i>OCR</i></sup></th><th>VQA<sup>C</sup></th><th>VQA<sup>T</sup></th><th>AI2D</th><th>SQA</th><th>MMMU<sup>v</sup></th><th>GQA</th><th>SEED<sup>I</sup></th><th>MMB</th><th>MME<sup>P</sup></th><th>RWQA</th><th>Bench<sup><i>HR</i></sup></th></tr></thead><tbody><tr><td>Qwen-VL [10]</td><td>1.45B</td><td>448×448</td><td>4.0T</td><td>56.9</td><td>62.6</td><td>48.8</td><td><strong>66.3</strong></td><td>61.5</td><td>57.7</td><td>68.2</td><td>35.9</td><td>57.5</td><td>65.4</td><td>61.8</td><td>74.4</td><td>49.3</td><td>30.5</td></tr><tr><td>MiniGPT-v2 [16]</td><td>326M</td><td>448×448</td><td>4.0T</td><td>-</td><td>-</td><td>15.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>60.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>mPLUG-Owl2 [105]</td><td>401M</td><td>448×448</td><td>1.7T</td><td>-</td><td>-</td><td>-</td><td>-</td><td>58.2</td><td>-</td><td>68.7</td><td>-</td><td>56.1</td><td>57.8</td><td>64.5</td><td>72.5</td><td>-</td><td>-</td></tr><tr><td>UReader [104]</td><td>86M</td><td>896×1120</td><td>20.3T</td><td>-</td><td>65.4</td><td>-</td><td>59.3</td><td>57.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LLaVA-1.5 [63]</td><td>1.22M</td><td>336×336</td><td>8.0T</td><td>49.0</td><td>21.8</td><td>31.8</td><td>17.8</td><td>45.5</td><td>55.5</td><td>66.8</td><td>37.0</td><td>62.0</td><td>65.8</td><td>66.5</td><td>75.3</td><td>54.8</td><td>36.1</td></tr><tr><td>SPHINX-2k [60]</td><td>1.01B</td><td>762×762</td><td>42.2T</td><td>-</td><td>-</td><td>-</td><td>-</td><td>61.2</td><td>-</td><td><u><strong>70.6</strong></u></td><td>-</td><td>63.1</td><td><u><strong>71.6</strong></u></td><td>65.9</td><td>73.6</td><td>-</td><td>-</td></tr><tr><td>SPHINX-X [28]</td><td>15.3M</td><td>448×448</td><td>21.3T</td><td>-</td><td>56.3</td><td>-</td><td>39.7</td><td>58.1</td><td>63.0</td><td>70.4</td><td>-</td><td>56.2</td><td>68.8</td><td>57.9</td><td>63.0</td><td>-</td><td>-</td></tr><tr><td>LLaVA-HR [73]</td><td>1.22M</td><td>1024×1024</td><td>24.3T</td><td>-</td><td>-</td><td>-</td><td>-</td><td>67.1</td><td>-</td><td>65.1</td><td>-</td><td>64.2</td><td>64.2</td><td>-</td><td><u><strong>77.7</strong></u></td><td>-</td><td>-</td></tr><tr><td>VILA [56]</td><td>51M</td><td>336×336</td><td>8.2T</td><td>-</td><td>-</td><td>-</td><td>-</td><td>64.4</td><td>-</td><td>68.2</td><td>-</td><td>62.3</td><td>61.1</td><td><u><strong>68.9</strong></u></td><td>76.7</td><td>-</td><td>-</td></tr><tr><td>Honey-bee [15]</td><td>52.5M</td><td>336×336</td><td>2.6T</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>35.3</td><td>-</td><td>64.5</td><td><u><strong>70.1</strong></u></td><td><u><strong>79.2</strong></u></td><td>-</td><td>-</td></tr><tr><td>Mini-Gemini [54]</td><td>3.0M</td><td>672×672</td><td>54.6T</td><td>59.4</td><td>61.9</td><td>47.7</td><td>47.4</td><td>65.2</td><td><u><strong>68.2</strong></u></td><td>69.6</td><td>36.8</td><td><u><strong>64.5</strong></u></td><td>66.9</td><td>65.8</td><td>77.3</td><td>51.1</td><td><u><strong>50.1</strong></u></td></tr><tr><td>Monkey [55]</td><td>1.40B</td><td>896×1344</td><td>28.0T</td><td>59.2</td><td><u><strong>66.5</strong></u></td><td>51.4</td><td><u><strong>65.1</strong></u></td><td>67.6</td><td>62.6</td><td>69.4</td><td><u><strong>38.9</strong></u></td><td>60.7</td><td>64.3</td><td>59.8</td><td>73.6</td><td>51.6</td><td>38.0</td></tr><tr><td>LLaVA-Next [62]</td><td>1.34M</td><td>672×672</td><td>44.4T</td><td>61.0</td><td>63.6</td><td><u><strong>53.2</strong></u></td><td>54.3</td><td>64.9</td><td>67.0</td><td>70.1</td><td>35.8</td><td>64.2</td><td><u><strong>70.2</strong></u></td><td>67.4</td><td>76.0</td><td><u><strong>57.8</strong></u></td><td>47.9</td></tr><tr><td>LLaVA-UHD v2 (ours)</td><td>1.42M</td><td>1008×672</td><td>17.5T</td><td><strong>63.2</strong></td><td><strong>68.1</strong></td><td><strong>53.9</strong></td><td>64.5</td><td><strong>67.6</strong></td><td><strong>70.5</strong></td><td><strong>71.3</strong></td><td><u><strong>38.2</strong></u></td><td><strong>65.4</strong></td><td>70.0</td><td>68.2</td><td>74.7</td><td><strong>58.2</strong></td><td><strong>51.5</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 표 1은 여러 벤치마크에서 주요 모델들의 성능을 비교한 표입니다. 공정한 비교를 위해 Vicuna-7B와 같이 7B 수준의 LLM을 사용한 방법만을 보고했습니다. #Data는 MLLM의 사전 훈련 및 지도 학습에 사용된 데이터의 양을 나타냅니다. MaxRes는 MLLM이 접근할 수 있는 최대 해상도이고, Avg는 13개의 벤치마크에 대한 평균 결과입니다. 약어는 다음과 같습니다. VQAD: DocVQA, BenchOCR: OCR-Bench, VQAC: ChartQA, VQAT: TextVQA, SQA: Science-QA, MMMUv: MMMU-val, SEEDI: SEED-Image, MMEP: MME의 인식 하위 집합, RWQA: RealWorldQA, BenchHR: HR-Bench.</p><details><summary>read the caption</summary>Table 1: Main performance on popular benchmarks. For a fair comparison, we only report the method using 7B level LLM (e.g.formulae-sequence𝑒𝑔e.g.italic_e . italic_g ., Vicuna-7B). #Data denotes the volume of overall data during MLLM pre-training and supervised fine-tuning. “MaxRes.” is the maximum accessible resolution of MLLM. “Avg.”: average results of 13 benchmarks. “VQAD: DocVQA. “BenchOCR”: OCR-Bench. “VQAC”: ChartQA. “VQAT”: TextVQA. “SQA”: Science-QA. “MMMUv”: MMMU-val. “SEEDI”: SEED-Image. “MMEP”: perception sub-set of MME. “RWQA”: RealWorldQA. “BenchHR”: HR-Bench.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Visual Granularity<div id=visual-granularity class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-granularity aria-label=Anchor>#</a></span></h4><p>본 논문은 다양한 시각적 세부 수준을 포착하는 계층적 윈도우 변환기를 중심으로 하는 고급 다중 모달 대규모 언어 모델(MLLM)인 LLaVA-UHD v2를 제시합니다. **시각적 세분성(Visual Granularity)**은 다양한 의미적 과립성에 맞춰 언어 생성에 필요한 정보를 제공하는 핵심 개념입니다. 저해상도 특징과 고해상도 특징을 통합하여 다양한 시각적 세부 수준을 포착하고, 계층적 윈도우 어텐션 메커니즘을 통해 다중 수준의 특징 맵을 효율적으로 압축합니다. **역 특징 피라미드(Inverse Feature Pyramid)**를 통해 이미지 피라미드의 고주파 세부 정보를 활용하여 다양한 시각적 세분성을 캡처하는 것이 핵심입니다. 이를 통해 <strong>시각적 지각 능력 향상</strong>과 <strong>다양한 시각적 다중 모달 작업에 대한 성능 향상</strong>을 가져옵니다. 특히 문서 중심 시각적 질문 응답, 시각적 그라운딩, 고해상도 이미지 인식 등 다양한 벤치마크에서 기존 MLLM을 능가하는 우수한 성능을 보여줍니다. 따라서 <strong>시각적 세분성의 효과적 통합</strong>은 MLLM의 성능 향상에 중요한 역할을 한다는 것을 알 수 있습니다.</p><h4 class="relative group">Hiwin Transformer<div id=hiwin-transformer class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#hiwin-transformer aria-label=Anchor>#</a></span></h4><p>본 논문에서 제시된 Hiwin Transformer는 <strong>고해상도 특징 피라미드를 계층적으로 통합</strong>하는 혁신적인 비전-언어 프로젝터입니다. 기존의 단일 스케일 특징에 의존하는 ViT 기반 MLLM의 한계를 극복하기 위해 역피라미드 구조를 통해 다양한 시각적 세밀도를 포착하고, 계층적 윈도우 어텐션을 통해 다중 레벨 특징 맵을 효율적으로 압축합니다. <strong>역피라미드 구조</strong>는 ViT에서 파생된 특징 업샘플링 과정을 통해 이미지 피라미드의 고주파 정보를 활용하여 구축되며, <strong>계층적 윈도우 어텐션</strong>은 크로스 스케일 윈도우 내 주요 샘플링 특징에 집중하여 다중 레벨 특징을 효과적으로 압축합니다. 결과적으로, Hiwin Transformer는 다양한 시각적 세밀도를 제공하여 언어 생성 작업의 다양한 의미적 과립도와의 정렬을 개선하며, 여러 벤치마크에서 기존 MLLM을 능가하는 성능을 보여줍니다. <strong>JBU 모듈</strong>과의 결합은 고해상도 특징을 효과적으로 활용하고 고주파 정보를 통합하는 데 중요한 역할을 수행합니다. <strong>전체적으로, Hiwin Transformer는 고해상도 영상 이해를 위한 MLLM의 성능 향상에 기여하는 중요한 요소</strong>입니다.</p><h4 class="relative group">Feature Pyramid<div id=feature-pyramid class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#feature-pyramid aria-label=Anchor>#</a></span></h4><p>본 논문에서 제시된 Feature Pyramid는 <strong>다양한 시각적 세부 정보와 고차원 의미를 효과적으로 통합</strong>하는 핵심 요소입니다. 기존의 단일 스케일 특징 벡터 표현 방식의 한계를 극복하기 위해, <strong>고해상도 특징 피라미드</strong>를 구축하여 다양한 수준의 시각적 정보를 포착합니다. 이는 역 피라미드 구조를 통해 고주파수 세부 정보를 활용하고, 계층적 윈도우 어텐션을 통해 다중 스케일 특징 맵을 효율적으로 압축함으로써 구현됩니다. <strong>역 피라미드는 ViT-파생 특징 업샘플링 과정을 통해 고해상도 특징을 생성</strong>하고, <strong>계층적 윈도우 어텐션은 다양한 스케일의 윈도우 내 주요 샘플링 특징에 집중</strong>하여 다중 수준 특징을 효율적으로 압축합니다. 이러한 방식은 다양한 시각적 세부 정보와 고차원 의미를 포착하여 언어 생성에 필요한 다양한 의미적 세분성을 제공함으로써, 기존의 MLLM 모델 성능을 향상시킵니다.</p><h4 class="relative group">MLLM Enhancement<div id=mllm-enhancement class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#mllm-enhancement aria-label=Anchor>#</a></span></h4><p>본 논문은 다양한 시각적 수준의 정보 부족이 보편적인 다중 모달 대규모 언어 모델(MLLM) 작업 해결에 대한 성능 저하로 이어진다는 점을 <strong>강조</strong>합니다. 이러한 문제를 해결하기 위해, 고해상도 특징 피라미드를 계층적 윈도우 변환기를 통해 통합하는 LLaVA-UHD v2를 제시합니다. <strong>계층적 윈도우 변환기</strong>는 고해상도 특징 피라미드를 구성하고 통합하여 다양한 시각적 세분성을 포착할 수 있도록 합니다. 이는 역 특징 피라미드와 계층적 윈도우 어텐션이라는 두 가지 주요 모듈을 통해 구현됩니다. 역 특징 피라미드는 이미지 피라미드의 고주파수 정보를 활용하여 ViT 기반 특징 업샘플링 프로세스를 통해 구성되며, 계층적 윈도우 어텐션은 다중 수준 특징 맵을 압축하기 위해 다양한 스케일의 윈도우 내 주요 샘플링 특징에 집중합니다. <strong>실험 결과</strong>, LLaVA-UHD v2는 기존 MLLM보다 우수한 성능을 보이며, 여러 벤치마크에서 평균 3.7%의 성능 향상을 가져왔습니다. 특히 DocVQA에서는 9.3%의 향상을 보였습니다. <strong>본 연구의 핵심은 다양한 시각적 세분성을 포착하여 언어 생성과의 정렬을 개선하는 고해상도 특징 피라미드의 효과적인 통합</strong>에 있습니다. 이를 통해, MLLM의 시각적 이해 능력을 크게 향상시킬 수 있음을 보여줍니다.</p><h4 class="relative group">Future MLLM<div id=future-mllm class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-mllm aria-label=Anchor>#</a></span></h4><p>미래의 다중 모드 대규모 언어 모델(MLLM)은 <strong>더욱 정교한 시각적 이해 능력</strong>을 갖추게 될 것입니다. 이는 고해상도 이미지 처리 및 다양한 시각적 세부 정보 포착을 위한 향상된 아키텍처를 통해 가능해질 것입니다. 또한, <strong>더욱 효율적인 계산 과정</strong>을 위해, 고해상도 특징 피라미드를 압축하는 새로운 방법이 개발될 것입니다. <strong>다양한 시각적 과제 수행</strong>을 위한 시각적 다양성과, 고차원적 의미 이해를 위한 상호 작용적 학습 전략 또한 중요한 발전 방향입니다. <strong>지식 기반의 시각적 질의응답 시스템</strong>이 보다 강력해지면서, 폭넓은 영역을 포괄하는 정보 처리 역량을 갖춘 MLLM이 등장할 것입니다. <strong>학문적 데이터 뿐 아니라 대규모 실세계 데이터를 활용한 훈련</strong>을 통해 실제 세계 문제 해결 능력이 향상될 것입니다. <strong>모델의 투명성과 설명 가능성</strong> 또한 미래 MLLM 개발의 중요한 과제가 될 것입니다.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.13871/x2.png alt></figure></p><blockquote><p>🔼 제안된 LLaVA-UHD v2의 전체 아키텍처는 ViT(Vision Transformer), 계층적 윈도우 트랜스포머(Hiwin 트랜스포머), 그리고 LLM(Large Language Model)의 세 가지 주요 모듈로 구성됩니다. Hiwin 트랜스포머는 이미지를 여러 조각으로 나누고 개별 조각과 전체 이미지를 처리하여 다양한 수준의 표현을 포착하고 이를 공간적으로 일관된 토큰으로 압축하여 보다 효과적인 비전-언어 정렬을 가능하게 합니다. 이 과정을 통해 다양한 시각적 세부 정보를 포착하고 언어 생성에 필요한 다양한 의미적 세분성을 제공합니다.</p><details><summary>read the caption</summary>Figure 2: The overall architecture of proposed LLaVA-UHD v2, consisting of a ViT, our hierarchical window transformer (Hiwin transformer), and an LLM. Hiwin transformers process sliced patches and the overview image by capturing inner multi-level representations and compressing them into spatially consistent tokens for a better vision-language alignment.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.13871/x3.png alt></figure></p><blockquote><p>🔼 이 그림은 이미지 피라미드를 활용하여 고해상도 특징 맵을 생성하는 Joint Bilateral Upsampling (JBU) 모듈의 흐름도를 보여줍니다. JBU 모듈은 이미지 피라미드의 고주파수 정보를 활용하여 저해상도 특징 맵을 고해상도로 업샘플링하고, 이를 통해 고주파수 정보가 풍부한 고해상도 특징 맵을 생성합니다. 이 과정은 이미지 피라미드의 여러 레벨에서 고주파수 정보를 통합하여 수행되며, 최종적으로는 다양한 시각적 세부 정보를 포착하는 고해상도 특징 맵이 생성됩니다.</p><details><summary>read the caption</summary>Figure 3: Flowchart of the Joint Bilateral Upsampling (JBU) module, which leverages the image pyramid to guide feature up-sampling, integrating high-frequency information into the up-sampled feature maps.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.13871/x4.png alt></figure></p><blockquote><p>🔼 그림 4는 계층적 윈도우 어텐션의 흐름도를 보여줍니다. 피처 피라미드의 여러 레벨에서 나온 피처 맵들은 적응적으로 RoI-정렬되어 샘플링 피처가 되고, 그다음 길이 축을 따라 연결되어 학습 가능한 쿼리의 키 역할을 합니다. 즉, 다양한 해상도의 시각적 정보를 효율적으로 통합하고 압축하여 언어 모델이 이미지를 이해하는 데 도움을 주는 메커니즘을 보여줍니다. 각 레벨의 피처 맵은 지역적 맥락을 포착하는 여러 개의 윈도우로 나뉘며, 각 윈도우 내의 주요 피처들이 쿼리에 의해 선택되어 어텐션 연산에 사용됩니다. 이를 통해 다양한 시각적 세부 정보와 고차원적 의미를 효과적으로 결합하여 언어 생성에 활용할 수 있습니다.</p><details><summary>read the caption</summary>Figure 4: The flowchart of hierarchical window attention. Feature maps from different levels of the feature pyramid are adaptively RoI-aligned into sampling features and then concatenated along the length axis to serve as the key for the learnable queries.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.13871/x6.png alt></figure></p><blockquote><p>🔼 그림 5는 JBU 모듈과 일반적인 이중 선형 보간법을 사용하여 수행한 다양한 시각적 작업에 대한 성능을 보여줍니다. 세 가지 시각적 작업은 광학 문자 인식(OCR), 선형 프로빙 의미론적 분할(Seg), 그리고 SUB-200 데이터셋을 사용한 세분화된 분류(Cls)입니다. JBU 모듈을 사용했을 때 세 가지 작업 모두에서 더 나은 성능을 보여줍니다. 이는 JBU 모듈이 이미지 피라미드에서 고주파수 패턴을 캡처하여 특징 업샘플링을 안내함으로써 고해상도 특징을 효과적으로 통합하기 때문입니다.</p><details><summary>read the caption</summary>Figure 5: Performance on different visual tasks with JBU module and vanilla bilinear interpolation. “OCR” denotes the optical character recognition, “Seg” the Linear probing semantic segmentation, and “Cls” the fine-grained classification on SUB-200.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.13871/x7.png alt></figure></p><blockquote><p>🔼 그림 6은 고해상도 복합 지각 과제에서 제안된 LLaVA-UHD v2와 LLaVA-Next, Mini-Gemini, GPT-4V를 포함한 고급 MLLM을 정성적으로 비교한 것입니다. 이러한 과제는 세밀한 시각 정보와 고차원 의미 맥락을 통합해야 합니다. 그림은 다양한 시각적 세부 정보와 의미 맥락을 필요로 하는 복잡한 시각적 질문에 대한 각 모델의 응답을 보여줍니다. LLaVA-UHD v2는 세밀한 시각 정보와 고차원 의미 맥락을 효과적으로 통합하여 더욱 정확하고 포괄적인 응답을 생성하는 것을 보여줍니다.</p><details><summary>read the caption</summary>Figure 6: Qualitative comparison of proposed LLaVA-UHD v2 and advanced MLLMs, including LLaVA-Next, Mini-Gemini, and GPT-4V on high-resolution complex perception tasks, which require the integration of both fine-grained visual information and high-level semantic contexts.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.13871/x8.png alt></figure></p><blockquote><p>🔼 그림 7은 다양한 시각적 특징 수준에 대한 특정 텍스트 토큰의 활성화 응답을 보여줍니다. 빨간색 원은 서로 다른 수준 간의 명확한 차이점을 강조 표시합니다. 이 그림은 다양한 시각적 세부 수준을 포착하는 역 피라미드의 효과를 보여주는 것으로, 고해상도 특징이 세부적인 시각적 정보를 더 잘 캡처하는 반면, 저해상도 특징은 더 추상적인 시각적 개념을 캡처합니다. 이는 다양한 세부 수준에서 시각적 정보를 통합하는 모델의 능력을 보여주는 중요한 시각적 증거입니다. 최상의 이해를 위해서는 컬러로 확대하여 보는 것이 좋습니다.</p><details><summary>read the caption</summary>Figure 7: Activation response of specific textual tokens to different visual feature levels. Red circles highlight the obvious difference between levels. (Best viewed in color and zoomed-in)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.13871/x9.png alt></figure></p><blockquote><p>🔼 그림 8은 고해상도의 이미지에서 미세한 부분까지 정확하게 인식해야 하는 과제를 보여줍니다. LLaVA-UHD v2는 TV 프로그램 시작 시간, 공연 날짜, 운동 시간, 상품 가격 등을 정확하게 식별하여, 고해상도 이미지에서 복잡하게 밀집된 유사한 객체들 사이에서도 목표 객체의 경계를 정확하게 찾고 목표 객체를 정확하게 식별하는 능력을 보여줍니다. 반면 다른 모델들은 목표를 정확하게 찾지 못하거나(LLaVA-Next), 유사한 객체들과 구분하지 못하는(Mini-Gemini) 등의 한계를 보입니다.</p><details><summary>read the caption</summary>Figure 8: Qualitative comparison on high-resolution dense perception task which requires the capabilities of fine-grained details perception.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.13871/x10.png alt></figure></p><blockquote><p>🔼 그림 9는 고해상도의 미세한 질감 인식 능력이 필요한 고해상도 미세립자 인식 과제에 대한 정성적 비교를 보여줍니다. 이 그림에서는 다양한 최첨단 다중 모달 대규모 언어 모델(MLLM)이 고해상도 이미지에서 미세한 시각적 세부 사항을 정확하게 식별하고 해석하는 능력을 보여줍니다. 각 모델은 세부 정보가 많고 복잡한 시각적 장면을 다루는 데 있어 강점과 약점을 보여주는 몇 가지 예시를 통해 비교됩니다. 특히, LLaVA-UHD v2 모델은 작은 물체나 흐릿한 텍스트와 같이 미세한 시각적 세부 사항을 식별하는 능력을 강조하여, 다른 모델보다 우수한 성능을 보여줍니다.</p><details><summary>read the caption</summary>Figure 9: Qualitative comparison on high-resolution fine-grained perception task which requires robust fine-grained visual texture perception capabilities.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.13871/x11.png alt></figure></p><blockquote><p>🔼 그림 10은 고해상도 공간적 지각에 대한 정성적 비교를 보여줍니다. 이는 고차원 공간적 맥락을 파악하는 능력이 필요한 작업입니다. 그림은 다양한 고해상도 시각적 인식 작업에서 LLaVA-UHD v2, LLaVA-Next, Mini-Gemini, GPT-4V의 성능을 보여주는 여러 사례를 제시합니다. 각 사례는 고해상도 이미지에서 세부적인 시각적 정보와 고차원 의미적 맥락을 통합하여 정확하게 객체를 식별하고 상호 관계를 파악해야 하는 복잡한 작업입니다. LLaVA-UHD v2는 고해상도 공간적 지각 능력을 갖추고 있어 세부적인 시각적 정보와 고차원 의미적 맥락을 정확하게 통합하여 작업을 수행하는 것을 보여줍니다. 반면에 다른 모델들은 고해상도 공간적 지각 능력이 부족하여 일부 작업에서 어려움을 겪는 것으로 나타났습니다. 이는 LLaVA-UHD v2가 고해상도 공간적 인식에 대한 뛰어난 성능을 가짐을 시사합니다.</p><details><summary>read the caption</summary>Figure 10: Qualitative comparison on high-resolution spatial perception which necessitates the capabilities of high-level spatial contexts.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.13871/x12.png alt></figure></p><blockquote><p>🔼 그림 11은 자연 장면에 대해 JBU 모듈에 의해 업샘플링된 특징들의 PCA 시각화를 보여줍니다. 계층적 감독을 사용하면 고해상도 특징(8배)이 객체 경계와 텍스트 모양을 명확하게 묘사할 수 있습니다. 색상으로 보면 더욱 효과적입니다.</p><details><summary>read the caption</summary>Figure 11: PCA visualization of the up-sampled features by JBU module on nature scene. With hierarchical supervision, the high-resolution features (8×8\times8 ×) could clearly depict object boundary and text appearance. (Best viewed in color and zoomed in)</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>Average</th><th>VQA<sup><i>D</i></sup></th><th>Bench<sup><i>OCR</i></sup></th><th>VQA<sup><i>C</i></sup></th><th>VQA<sup><i>T</i></sup></th><th>AI2D</th><th>SQA</th><th>MMMU<sup><i>v</i></sup></th><th>GQA</th><th>SEED<sup><i>I</i></sup></th><th>MMB</th><th>MME<sup><i>P</i></sup></th><th>RWQA</th><th>REC</th><th>Bench<sup><i>HR</i></sup></th><th></th></tr></thead><tbody><tr><td>LLaVA-UHD [31]</td><td>58.0</td><td>56.7</td><td>40.9</td><td>56.3</td><td>62.2</td><td>55.4</td><td>70.7</td><td>37.0</td><td>63.8</td><td>65.6</td><td>64.8</td><td>70.0</td><td>54.4</td><td>68.3</td><td>45.6</td><td></td></tr><tr><td>+ JBU module</td><td>60.0</td><td>60.2</td><td>50.4</td><td>60.4</td><td>67.1</td><td>57.8</td><td>70.5</td><td>38.2</td><td>64.0</td><td>66.7</td><td>65.6</td><td>71.2</td><td>51.9</td><td>72.3</td><td>43.9</td><td></td></tr><tr><td>+ HFP integration</td><td>61.5</td><td>65.0</td><td>51.3</td><td>62.5</td><td>68.5</td><td>58.1</td><td>69.2</td><td>38.9</td><td>64.6</td><td>67.4</td><td>65.5</td><td>73.0</td><td>55.5</td><td>73.3</td><td>48.9</td><td></td></tr><tr><td>+ Token organization</td><td>61.7</td><td>66.0</td><td>50.1</td><td>62.8</td><td>66.8</td><td>59.4</td><td>69.8</td><td>37.6</td><td>64.0</td><td>67.4</td><td>66.1</td><td>73.6</td><td>56.9</td><td>74.0</td><td>49.0</td><td></td></tr><tr><td>Δ</td><td>+3.7</td><td>+9.3</td><td>+9.2</td><td>+6.5</td><td>+4.6</td><td>+4.0</td><td>-0.9</td><td>+0.6</td><td>+0.2</td><td>+1.8</td><td>+1.3</td><td>+3.6</td><td>+2.5</td><td>+5.7</td><td>+3.4</td><td></td></tr></tbody></table></table></figure><blockquote><p>🔼 표 2는 제안된 방법에서 각 모듈의 효과를 분석한 결과를 보여줍니다. &lsquo;HFP&rsquo;는 고해상도 특징 피라미드의 약자이며, &lsquo;Δ&rsquo;는 기준 방법 대비 전반적인 성능 향상을 나타냅니다. REC는 RefCOCO/g/+ 데이터셋에 대한 평균 정확도를 의미합니다. 이 표는 고해상도 특징 피라미드(HFP)를 구성하고 통합하는 과정에서 각 모듈(JBU, HFP 통합, 토큰 구성)이 성능 향상에 미치는 영향을 정량적으로 분석하여, 제안된 방법의 효과를 보여줍니다. 각 모듈이 추가됨에 따라 성능이 향상되는 것을 확인할 수 있으며, 특히 DocVQA 데이터셋에서 9.3%의 향상을 보여줍니다.</p><details><summary>read the caption</summary>Table 2: Ablation studies of modules in our proposed method. “HFP” is the abbreviation of high-resolution feature pyramid. “ΔΔ\Deltaroman_Δ” denotes the overall improvement compared to the baseline. REC reports the average accuracy of RefCOCO/g/+.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>Average</th><th>MME<sup>P</sup></th><th>GQA</th><th>AI2D</th><th>VQA<sup>C</sup></th><th>VQA<sup>T</sup></th><th>VQA<sup>D</sup></th><th>Bench<sup>HR</sup></th></tr></thead><tbody><tr><td>LLaVA-UHD</td><td>58.6</td><td>70.0</td><td>63.8</td><td>55.4</td><td>56.3</td><td>62.2</td><td>56.7</td><td>45.6</td></tr><tr><td><em>w. ConvNext</em></td><td>59.7</td><td>68.2</td><td>62.7</td><td>55.6</td><td>61.8</td><td>63.5</td><td>61.8</td><td>44.0</td></tr><tr><td><em>w. DeConv.</em></td><td>61.7</td><td>71.2</td><td>64.2</td><td>57.4</td><td>61.8</td><td>67.8</td><td>63.4</td><td>46.3</td></tr><tr><td><em>w. Bilinear</em></td><td>62.0</td><td>72.0</td><td>64.5</td><td>57.8</td><td>62.2</td><td>67.6</td><td>63.7</td><td>46.5</td></tr><tr><td><em>w. JBU module</em></td><td>63.0</td><td>73.0</td><td>64.6</td><td>58.3</td><td>62.5</td><td>68.5</td><td>65.0</td><td>48.9</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 3은 다양한 특징 피라미드 구성 방법의 비교 결과를 보여줍니다. CLIP-ViT 대신 CLIP-ConvNext [68]를 비주얼 인코더로 사용하고 여러 단계의 특징 맵을 최종 계층적 특징 피라미드로 직접 사용한 경우를 &lsquo;ConvNext&rsquo; 라고 표시했습니다. 이 표는 여러 가지 방법으로 생성된 특징 피라미드의 성능과 효율성을 비교 분석하여, 제안된 방법의 우수성을 보여주는 데 사용됩니다. 특히, 다양한 벤치마크에서의 성능과 계산 비용 측면에서의 비교가 이루어집니다.</p><details><summary>read the caption</summary>Table 3: Comparison of different methods for feature pyramid construction. “ConvNext” means we replace the CILP-ViT with CLIP-ConvNext [68] as visual encoder and directly use the feature maps from multiple stages as the final hierarchical feature pyramid.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>Period(h)</th><th>Latency(s)</th><th>Memory(G)</th><th>Efficiency Average</th><th>General MME<sup>P</sup></th><th>GQA</th><th>AI2D</th><th>VQA<sup>C</sup></th><th>VQA<sup>T</sup></th><th>VQA<sup>D</sup></th></tr></thead><tbody><tr><td><em>Pyramid</em></td><td>62.4</td><td>1.26</td><td>60.3</td><td>62.4</td><td>69.0</td><td>60.8</td><td>57.3</td><td>60.7</td><td>67.5</td><td>58.9</td></tr><tr><td><em>Fix [3×3]</em></td><td>26.9</td><td>0.62</td><td>41.7</td><td>64.6</td><td>73.8</td><td>63.9</td><td>58.8</td><td>60.9</td><td>66.2</td><td>63.8</td></tr><tr><td><em>Selective</em></td><td>27.7</td><td>0.54</td><td>39.4</td><td>65.3</td><td>73.0</td><td>64.6</td><td>58.3</td><td>62.5</td><td>68.5</td><td>65.0</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 4는 다양한 그리드 크기 선택에 따른 성능과 효율성을 비교 분석한 표입니다. &lsquo;Pyramid&rsquo; 방식은 여러 레벨의 특징 맵을 지역 수준의 특징 피라미드로 구성하는 방식을 의미하며, 예를 들어 레벨 0은 2x3, 레벨 1은 4x6, 레벨 2는 8x12 크기의 그리드를 사용합니다. &lsquo;Fix&rsquo; 방식은 모든 특징 맵을 3x3 그리드로 통합하는 방식입니다. 본 표에서는 8개의 A100 GPU를 사용하여 학습 시간을 측정하고, A100 GPU 1개를 사용하여 1008x672 이미지에 대한 지연 시간을 측정하며, 8개의 A100 GPU를 사용하고 GPU당 1개의 이미지로 GPU 메모리를 측정했습니다. 모두 지도 학습 미세 조정 단계에서 측정된 결과입니다.</p><details><summary>read the caption</summary>Table 4: Comparison of different choice of grid sizes on performance and efficiency. “Pyramid” means the feature grids from different levels form a region-level feature pyramid, e.g.formulae-sequence𝑒𝑔e.g.italic_e . italic_g ., [2×\times×3] for level-0, [4×\times×6] for level-1, [8×\times×12] for leval-2. “Fix” represents all feature maps are pooled into a 3×\times×3 feature grid. We measure the training period on 8×\times×A100s, the latency on an A100 with a 1008×\times×672 image, and the GPU memory on 8×\times×A100s with 1 image per GPU in supervised fine-tune phase.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Data</th><th>Size</th><th>Response formatting prompts</th></tr></thead><tbody><tr><td>LLaVA [63]</td><td>158K</td><td>–</td></tr><tr><td>ShareGPT [90]</td><td>40K</td><td>–</td></tr><tr><td>VQAv2 [29]</td><td>83K</td><td>Answer the question using a single word or phrase.</td></tr><tr><td>GQA [38]</td><td>72K</td><td></td></tr><tr><td>OKVQA [75]</td><td>9K</td><td></td></tr><tr><td>OCRVQA [82]</td><td>80K</td><td></td></tr><tr><td>DocVQA [95]</td><td>15K</td><td></td></tr><tr><td>ChartQA [76]</td><td>20K</td><td></td></tr><tr><td>A-OKVQA [88]</td><td>66K</td><td>Answer directly with the option’s letter from the given choices.</td></tr><tr><td>DVQA [41]</td><td>20K</td><td>–</td></tr><tr><td>TextCaps [92]</td><td>22K</td><td>Provide a one-sentence caption for the provided image.</td></tr><tr><td>ShareGPT4V [18]</td><td>55K</td><td>–</td></tr><tr><td>AI2D [43]</td><td>3K</td><td>–</td></tr><tr><td>LAION-GPT4V [3]</td><td>11K</td><td>–</td></tr><tr><td>SythDog-EN [46]</td><td>40K</td><td>–</td></tr><tr><td>LRV-Instruct [61]</td><td>30K</td><td>–</td></tr><tr><td>RefCOCO [42, 74]</td><td>48K</td><td>Provide a short description for this region. _ (for Region Caption)_</td></tr><tr><td>VG [48]</td><td>86K</td><td>Provide the bounding box coordinate of the region this sentence describes. _ (for Referring Expression Comprehension)_</td></tr><tr><td>Total</td><td>858K</td><td></td></tr></tbody></table></table></figure><blockquote><p>🔼 표 5는 논문에서 사용된 858,000개의 이미지-텍스트 데이터셋의 상세 구성을 보여줍니다. 데이터셋은 다양한 비전-언어 작업(VQA, OCR, 이미지 캡션 생성 등)을 위한 여러 개의 기존 데이터셋들을 하나로 합쳐 만든 혼합 데이터셋입니다. 각 데이터셋의 크기와 함께 해당 데이터셋에서 사용되는 응답 형식(예: 한 단어 또는 구절로 답하기, 객관식 답변, 영역 설명 제공 등)이 명시되어 있습니다. 이 표는 논문에서 제시된 모델의 성능을 평가하는 데 사용된 데이터의 종류와 특징을 자세히 보여줌으로써, 실험 결과의 신뢰성과 일반화 가능성을 높이는 데 기여합니다.</p><details><summary>read the caption</summary>Table 5: Detailed composition of our 858k-mixed dataset.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Level</th><th>Period(h)</th><th>Memory(G)</th><th>Average</th><th>GQA</th><th>SQA</th><th>REC</th><th>VQA<sup>C</sup></th><th>VQA<sup>T</sup></th><th>ESTVQA</th><th>MME<sup>P</sup></th></tr></thead><tbody><tr><td><em>0,2</em></td><td>27.7</td><td>41.9</td><td>63.4</td><td>63.9</td><td>69.5</td><td>71.5</td><td>60.5</td><td>66.5</td><td>40.6</td><td>71.0</td></tr><tr><td><em>0,1,2</em></td><td>28.0</td><td>41.9</td><td>63.7</td><td>63.8</td><td>70.2</td><td>71.8</td><td>60.5</td><td>66.9</td><td>40.8</td><td>72.1</td></tr><tr><td><em>0,1,2,3</em></td><td>45.6</td><td>53.0</td><td>63.8</td><td>64.4</td><td>69.3</td><td>72.6</td><td>60.7</td><td>66.4</td><td>41.6</td><td>71.4</td></tr><tr><td><em>0,1,2,3</em> (w/o HS.)</td><td>45.6</td><td>52.6</td><td>62.4</td><td>63.6</td><td>69.8</td><td>67.1</td><td>57.8</td><td>66.5</td><td>39.9</td><td>72.0</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 6은 다양한 수준의 특징(feature level)을 사용했을 때 성능과 효율성을 비교 분석한 결과를 보여줍니다. &lsquo;HS&rsquo;는 계층적 감독(hierarchical supervision)을 의미하며, ESTVQA [101]는 장면 텍스트 인식(scene text recognition)에 초점을 맞춘 VQA 벤치마크입니다. 표에는 각 특징 수준별로 학습 시간, 메모리 사용량, 그리고 GQA, SQA, REC, VQAC, VQAT, ESTVQA, MMEP 등 다양한 벤치마크에서의 성능이 나타나 있습니다.</p><details><summary>read the caption</summary>Table 6: Comparison of different choices of feature level on performance and efficiency. “HS.”: hierarchical supervision. ESTVQA [101] is a VQA benchmark focusing on scene text recognition.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-50d8d3f8ce29cd9fb78b9b79e2a9099c class=gallery><img src=paper_images/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13871/&amp;title=LLaVA-UHD%20v2:%20an%20MLLM%20Integrating%20High-Resolution%20Feature%20Pyramid%20via%20Hierarchical%20Window%20Transformer" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13871/&amp;text=LLaVA-UHD%20v2:%20an%20MLLM%20Integrating%20High-Resolution%20Feature%20Pyramid%20via%20Hierarchical%20Window%20Transformer" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13871/&amp;subject=LLaVA-UHD%20v2:%20an%20MLLM%20Integrating%20High-Resolution%20Feature%20Pyramid%20via%20Hierarchical%20Window%20Transformer" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.13871/index.md",oid_likes="likes_paper-reviews/2412.13871/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.14283/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-18T00:00:00+00:00>18 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.13501/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">GUI Agents: A Survey</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-18T00:00:00+00:00>18 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>