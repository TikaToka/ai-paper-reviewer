[{"figure_path": "https://arxiv.org/html/2412.13871/x1.png", "caption": "Figure 1: Comparison of LLaVA-UHD v2 with other MLLMs. (a) MLLMs typically align ViT features to language space using MLPs\u00a0[63] or perceiver re-samplers\u00a0[6, 52], lacking visual granularity. (b) Combining multiple visual encoders is non-universal and computationally intensive. (c) LLaVA-UHD v2 employs the Hiwin transformer to build an inverse feature pyramid and compress it into visual tokens, providing various semantic granularity for language generation.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 LLaVA-UHD v2 \ubaa8\ub378\uacfc \uae30\uc874 \ub2e4\uc911 \ubaa8\ub2ec \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(MLLM)\uc758 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \uae30\uc874 MLLM\ub4e4\uc774 \uc77c\ubc18\uc801\uc73c\ub85c ViT \ud2b9\uc9d5\uc744 MLP \ub610\ub294 perceiver re-sampler\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5b8\uc5b4 \uacf5\uac04\uc5d0 \uc815\ub82c\ud558\ub294 \ubc29\uc2dd\uc744 \ub098\ud0c0\ub0b4\uba70, \uc774\ub294 \uc2dc\uac01\uc801 \uc138\ubd80 \uc815\ubcf4\uac00 \ubd80\uc871\ud568\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. (b)\ub294 \uc5ec\ub7ec \uac1c\uc758 \uc2dc\uac01\uc801 \uc778\ucf54\ub354\ub97c \uacb0\ud569\ud558\ub294 \uac83\uc774 \ubcf4\ud3b8\uc801\uc774\uc9c0 \uc54a\uace0 \uacc4\uc0b0\uc801\uc73c\ub85c \uc9d1\uc57d\uc801\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (c)\ub294 LLaVA-UHD v2\uac00 \uacc4\uce35\uc801 \uc708\ub3c4\uc6b0 \ubcc0\ud658\uae30\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5ed \ud2b9\uc9d5 \ud53c\ub77c\ubbf8\ub4dc\ub97c \uad6c\ucd95\ud558\uace0 \uc2dc\uac01 \ud1a0\ud070\uc73c\ub85c \uc555\ucd95\ud558\uc5ec \ub2e4\uc591\ud55c \uc758\ubbf8\uc801 \uc138\ubd84\uc131\uc744 \uc81c\uacf5\ud568\uc73c\ub85c\uc368 \uc5b8\uc5b4 \uc0dd\uc131\uc5d0 \ub3c4\uc6c0\uc744 \uc8fc\ub294 \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, LLaVA-UHD v2\ub294 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \ud574\uc0c1\ub3c4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ud65c\uc6a9\ud558\uc5ec \uc5b8\uc5b4 \uc0dd\uc131 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4\ub294 \uc810\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.13871/x2.png", "caption": "Figure 2: The overall architecture of proposed LLaVA-UHD v2, consisting of a ViT, our hierarchical window transformer (Hiwin transformer), and an LLM. Hiwin transformers process sliced patches and the overview image by capturing inner multi-level representations and compressing them into spatially consistent tokens for a better vision-language alignment.", "description": "\uc81c\uc548\ub41c LLaVA-UHD v2\uc758 \uc804\uccb4 \uc544\ud0a4\ud14d\ucc98\ub294 ViT(Vision Transformer), \uacc4\uce35\uc801 \uc708\ub3c4\uc6b0 \ud2b8\ub79c\uc2a4\ud3ec\uba38(Hiwin \ud2b8\ub79c\uc2a4\ud3ec\uba38), \uadf8\ub9ac\uace0 LLM(Large Language Model)\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 \ubaa8\ub4c8\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. Hiwin \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub294 \uc774\ubbf8\uc9c0\ub97c \uc5ec\ub7ec \uc870\uac01\uc73c\ub85c \ub098\ub204\uace0 \uac1c\ubcc4 \uc870\uac01\uacfc \uc804\uccb4 \uc774\ubbf8\uc9c0\ub97c \ucc98\ub9ac\ud558\uc5ec \ub2e4\uc591\ud55c \uc218\uc900\uc758 \ud45c\ud604\uc744 \ud3ec\ucc29\ud558\uace0 \uc774\ub97c \uacf5\uac04\uc801\uc73c\ub85c \uc77c\uad00\ub41c \ud1a0\ud070\uc73c\ub85c \uc555\ucd95\ud558\uc5ec \ubcf4\ub2e4 \ud6a8\uacfc\uc801\uc778 \ube44\uc804-\uc5b8\uc5b4 \uc815\ub82c\uc744 \uac00\ub2a5\ud558\uac8c \ud569\ub2c8\ub2e4.  \uc774 \uacfc\uc815\uc744 \ud1b5\ud574 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc138\ubd80 \uc815\ubcf4\ub97c \ud3ec\ucc29\ud558\uace0 \uc5b8\uc5b4 \uc0dd\uc131\uc5d0 \ud544\uc694\ud55c \ub2e4\uc591\ud55c \uc758\ubbf8\uc801 \uc138\ubd84\uc131\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2412.13871/x3.png", "caption": "Figure 3: Flowchart of the Joint Bilateral Upsampling (JBU) module, which leverages the image pyramid to guide feature up-sampling, integrating high-frequency information into the up-sampled feature maps.", "description": "\uc774 \uadf8\ub9bc\uc740 \uc774\ubbf8\uc9c0 \ud53c\ub77c\ubbf8\ub4dc\ub97c \ud65c\uc6a9\ud558\uc5ec \uace0\ud574\uc0c1\ub3c4 \ud2b9\uc9d5 \ub9f5\uc744 \uc0dd\uc131\ud558\ub294 Joint Bilateral Upsampling (JBU) \ubaa8\ub4c8\uc758 \ud750\ub984\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. JBU \ubaa8\ub4c8\uc740 \uc774\ubbf8\uc9c0 \ud53c\ub77c\ubbf8\ub4dc\uc758 \uace0\uc8fc\ud30c\uc218 \uc815\ubcf4\ub97c \ud65c\uc6a9\ud558\uc5ec \uc800\ud574\uc0c1\ub3c4 \ud2b9\uc9d5 \ub9f5\uc744 \uace0\ud574\uc0c1\ub3c4\ub85c \uc5c5\uc0d8\ud50c\ub9c1\ud558\uace0, \uc774\ub97c \ud1b5\ud574 \uace0\uc8fc\ud30c\uc218 \uc815\ubcf4\uac00 \ud48d\ubd80\ud55c \uace0\ud574\uc0c1\ub3c4 \ud2b9\uc9d5 \ub9f5\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774 \uacfc\uc815\uc740 \uc774\ubbf8\uc9c0 \ud53c\ub77c\ubbf8\ub4dc\uc758 \uc5ec\ub7ec \ub808\ubca8\uc5d0\uc11c \uace0\uc8fc\ud30c\uc218 \uc815\ubcf4\ub97c \ud1b5\ud569\ud558\uc5ec \uc218\ud589\ub418\uba70, \ucd5c\uc885\uc801\uc73c\ub85c\ub294 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc138\ubd80 \uc815\ubcf4\ub97c \ud3ec\ucc29\ud558\ub294 \uace0\ud574\uc0c1\ub3c4 \ud2b9\uc9d5 \ub9f5\uc774 \uc0dd\uc131\ub429\ub2c8\ub2e4.", "section": "3.2 Inverse Feature Pyramid"}, {"figure_path": "https://arxiv.org/html/2412.13871/x4.png", "caption": "Figure 4: The flowchart of hierarchical window attention. Feature maps from different levels of the feature pyramid are adaptively RoI-aligned into sampling features and then concatenated along the length axis to serve as the key for the learnable queries.", "description": "\uadf8\ub9bc 4\ub294 \uacc4\uce35\uc801 \uc708\ub3c4\uc6b0 \uc5b4\ud150\uc158\uc758 \ud750\ub984\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud53c\ucc98 \ud53c\ub77c\ubbf8\ub4dc\uc758 \uc5ec\ub7ec \ub808\ubca8\uc5d0\uc11c \ub098\uc628 \ud53c\ucc98 \ub9f5\ub4e4\uc740 \uc801\uc751\uc801\uc73c\ub85c RoI-\uc815\ub82c\ub418\uc5b4 \uc0d8\ud50c\ub9c1 \ud53c\ucc98\uac00 \ub418\uace0, \uadf8\ub2e4\uc74c \uae38\uc774 \ucd95\uc744 \ub530\ub77c \uc5f0\uacb0\ub418\uc5b4 \ud559\uc2b5 \uac00\ub2a5\ud55c \ucffc\ub9ac\uc758 \ud0a4 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.  \uc989, \ub2e4\uc591\ud55c \ud574\uc0c1\ub3c4\uc758 \uc2dc\uac01\uc801 \uc815\ubcf4\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \ud1b5\ud569\ud558\uace0 \uc555\ucd95\ud558\uc5ec \uc5b8\uc5b4 \ubaa8\ub378\uc774 \uc774\ubbf8\uc9c0\ub97c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc8fc\ub294 \uba54\ucee4\ub2c8\uc998\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ub808\ubca8\uc758 \ud53c\ucc98 \ub9f5\uc740 \uc9c0\uc5ed\uc801 \ub9e5\ub77d\uc744 \ud3ec\ucc29\ud558\ub294 \uc5ec\ub7ec \uac1c\uc758 \uc708\ub3c4\uc6b0\ub85c \ub098\ub258\uba70, \uac01 \uc708\ub3c4\uc6b0 \ub0b4\uc758 \uc8fc\uc694 \ud53c\ucc98\ub4e4\uc774 \ucffc\ub9ac\uc5d0 \uc758\ud574 \uc120\ud0dd\ub418\uc5b4 \uc5b4\ud150\uc158 \uc5f0\uc0b0\uc5d0 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc138\ubd80 \uc815\ubcf4\uc640 \uace0\ucc28\uc6d0\uc801 \uc758\ubbf8\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \uacb0\ud569\ud558\uc5ec \uc5b8\uc5b4 \uc0dd\uc131\uc5d0 \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.3 \uacc4\uce35\uc801 \uc708\ub3c4\uc6b0 \uc5b4\ud150\uc158"}, {"figure_path": "https://arxiv.org/html/2412.13871/x6.png", "caption": "Figure 5: Performance on different visual tasks with JBU module and vanilla bilinear interpolation. \u201cOCR\u201d denotes the optical character recognition, \u201cSeg\u201d the Linear probing semantic segmentation, and \u201cCls\u201d the fine-grained classification on SUB-200.", "description": "\uadf8\ub9bc 5\ub294 JBU \ubaa8\ub4c8\uacfc \uc77c\ubc18\uc801\uc778 \uc774\uc911 \uc120\ud615 \ubcf4\uac04\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc218\ud589\ud55c \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc138 \uac00\uc9c0 \uc2dc\uac01\uc801 \uc791\uc5c5\uc740 \uad11\ud559 \ubb38\uc790 \uc778\uc2dd(OCR), \uc120\ud615 \ud504\ub85c\ube59 \uc758\ubbf8\ub860\uc801 \ubd84\ud560(Seg), \uadf8\ub9ac\uace0 SUB-200 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud55c \uc138\ubd84\ud654\ub41c \ubd84\ub958(Cls)\uc785\ub2c8\ub2e4. JBU \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c \uc138 \uac00\uc9c0 \uc791\uc5c5 \ubaa8\ub450\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 JBU \ubaa8\ub4c8\uc774 \uc774\ubbf8\uc9c0 \ud53c\ub77c\ubbf8\ub4dc\uc5d0\uc11c \uace0\uc8fc\ud30c\uc218 \ud328\ud134\uc744 \ucea1\ucc98\ud558\uc5ec \ud2b9\uc9d5 \uc5c5\uc0d8\ud50c\ub9c1\uc744 \uc548\ub0b4\ud568\uc73c\ub85c\uc368 \uace0\ud574\uc0c1\ub3c4 \ud2b9\uc9d5\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud1b5\ud569\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2412.13871/x7.png", "caption": "Figure 6: \nQualitative comparison of proposed LLaVA-UHD v2 and advanced MLLMs, including LLaVA-Next, Mini-Gemini, and GPT-4V on high-resolution complex perception tasks, which require the integration of both fine-grained visual information and high-level semantic contexts.", "description": "\uadf8\ub9bc 6\uc740 \uace0\ud574\uc0c1\ub3c4 \ubcf5\ud569 \uc9c0\uac01 \uacfc\uc81c\uc5d0\uc11c \uc81c\uc548\ub41c LLaVA-UHD v2\uc640 LLaVA-Next, Mini-Gemini, GPT-4V\ub97c \ud3ec\ud568\ud55c \uace0\uae09 MLLM\uc744 \uc815\uc131\uc801\uc73c\ub85c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacfc\uc81c\ub294 \uc138\ubc00\ud55c \uc2dc\uac01 \uc815\ubcf4\uc640 \uace0\ucc28\uc6d0 \uc758\ubbf8 \ub9e5\ub77d\uc744 \ud1b5\ud569\ud574\uc57c \ud569\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc138\ubd80 \uc815\ubcf4\uc640 \uc758\ubbf8 \ub9e5\ub77d\uc744 \ud544\uc694\ub85c \ud558\ub294 \ubcf5\uc7a1\ud55c \uc2dc\uac01\uc801 \uc9c8\ubb38\uc5d0 \ub300\ud55c \uac01 \ubaa8\ub378\uc758 \uc751\ub2f5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LLaVA-UHD v2\ub294 \uc138\ubc00\ud55c \uc2dc\uac01 \uc815\ubcf4\uc640 \uace0\ucc28\uc6d0 \uc758\ubbf8 \ub9e5\ub77d\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud1b5\ud569\ud558\uc5ec \ub354\uc6b1 \uc815\ud655\ud558\uace0 \ud3ec\uad04\uc801\uc778 \uc751\ub2f5\uc744 \uc0dd\uc131\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2412.13871/x8.png", "caption": "Figure 7: Activation response of specific textual tokens to different visual feature levels. Red circles highlight the obvious difference between levels. (Best viewed in color and zoomed-in)", "description": "\uadf8\ub9bc 7\uc740 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \ud2b9\uc9d5 \uc218\uc900\uc5d0 \ub300\ud55c \ud2b9\uc815 \ud14d\uc2a4\ud2b8 \ud1a0\ud070\uc758 \ud65c\uc131\ud654 \uc751\ub2f5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube68\uac04\uc0c9 \uc6d0\uc740 \uc11c\ub85c \ub2e4\ub978 \uc218\uc900 \uac04\uc758 \uba85\ud655\ud55c \ucc28\uc774\uc810\uc744 \uac15\uc870 \ud45c\uc2dc\ud569\ub2c8\ub2e4. \uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc2dc\uac01\uc801 \uc138\ubd80 \uc218\uc900\uc744 \ud3ec\ucc29\ud558\ub294 \uc5ed \ud53c\ub77c\ubbf8\ub4dc\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc73c\ub85c, \uace0\ud574\uc0c1\ub3c4 \ud2b9\uc9d5\uc774 \uc138\ubd80\uc801\uc778 \uc2dc\uac01\uc801 \uc815\ubcf4\ub97c \ub354 \uc798 \ucea1\ucc98\ud558\ub294 \ubc18\uba74, \uc800\ud574\uc0c1\ub3c4 \ud2b9\uc9d5\uc740 \ub354 \ucd94\uc0c1\uc801\uc778 \uc2dc\uac01\uc801 \uac1c\ub150\uc744 \ucea1\ucc98\ud569\ub2c8\ub2e4. \uc774\ub294 \ub2e4\uc591\ud55c \uc138\ubd80 \uc218\uc900\uc5d0\uc11c \uc2dc\uac01\uc801 \uc815\ubcf4\ub97c \ud1b5\ud569\ud558\ub294 \ubaa8\ub378\uc758 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc911\uc694\ud55c \uc2dc\uac01\uc801 \uc99d\uac70\uc785\ub2c8\ub2e4.  \ucd5c\uc0c1\uc758 \uc774\ud574\ub97c \uc704\ud574\uc11c\ub294 \uceec\ub7ec\ub85c \ud655\ub300\ud558\uc5ec \ubcf4\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4.", "section": "3.3 \uacc4\uce35\uc801 \uc708\ub3c4\uc6b0 \uc5b4\ud150\uc158"}, {"figure_path": "https://arxiv.org/html/2412.13871/x9.png", "caption": "Figure 8: Qualitative comparison on high-resolution dense perception task which requires the capabilities of fine-grained details perception.", "description": "\uadf8\ub9bc 8\uc740 \uace0\ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0\uc5d0\uc11c \ubbf8\uc138\ud55c \ubd80\ubd84\uae4c\uc9c0 \uc815\ud655\ud558\uac8c \uc778\uc2dd\ud574\uc57c \ud558\ub294 \uacfc\uc81c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LLaVA-UHD v2\ub294 TV \ud504\ub85c\uadf8\ub7a8 \uc2dc\uc791 \uc2dc\uac04, \uacf5\uc5f0 \ub0a0\uc9dc, \uc6b4\ub3d9 \uc2dc\uac04, \uc0c1\ud488 \uac00\uaca9 \ub4f1\uc744 \uc815\ud655\ud558\uac8c \uc2dd\ubcc4\ud558\uc5ec, \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\uc5d0\uc11c \ubcf5\uc7a1\ud558\uac8c \ubc00\uc9d1\ub41c \uc720\uc0ac\ud55c \uac1d\uccb4\ub4e4 \uc0ac\uc774\uc5d0\uc11c\ub3c4 \ubaa9\ud45c \uac1d\uccb4\uc758 \uacbd\uacc4\ub97c \uc815\ud655\ud558\uac8c \ucc3e\uace0 \ubaa9\ud45c \uac1d\uccb4\ub97c \uc815\ud655\ud558\uac8c \uc2dd\ubcc4\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubc18\uba74 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc740 \ubaa9\ud45c\ub97c \uc815\ud655\ud558\uac8c \ucc3e\uc9c0 \ubabb\ud558\uac70\ub098(LLaVA-Next),  \uc720\uc0ac\ud55c \uac1d\uccb4\ub4e4\uacfc \uad6c\ubd84\ud558\uc9c0 \ubabb\ud558\ub294(Mini-Gemini) \ub4f1\uc758 \ud55c\uacc4\ub97c \ubcf4\uc785\ub2c8\ub2e4.", "section": "4.3 \uc8fc\uc694 \uc131\ub2a5"}, {"figure_path": "https://arxiv.org/html/2412.13871/x10.png", "caption": "Figure 9: Qualitative comparison on high-resolution fine-grained perception task which requires robust fine-grained visual texture perception capabilities.", "description": "\uadf8\ub9bc 9\ub294 \uace0\ud574\uc0c1\ub3c4\uc758 \ubbf8\uc138\ud55c \uc9c8\uac10 \uc778\uc2dd \ub2a5\ub825\uc774 \ud544\uc694\ud55c \uace0\ud574\uc0c1\ub3c4 \ubbf8\uc138\ub9bd\uc790 \uc778\uc2dd \uacfc\uc81c\uc5d0 \ub300\ud55c \uc815\uc131\uc801 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc5d0\uc11c\ub294 \ub2e4\uc591\ud55c \ucd5c\ucca8\ub2e8 \ub2e4\uc911 \ubaa8\ub2ec \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(MLLM)\uc774 \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\uc5d0\uc11c \ubbf8\uc138\ud55c \uc2dc\uac01\uc801 \uc138\ubd80 \uc0ac\ud56d\uc744 \uc815\ud655\ud558\uac8c \uc2dd\ubcc4\ud558\uace0 \ud574\uc11d\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc740 \uc138\ubd80 \uc815\ubcf4\uac00 \ub9ce\uace0 \ubcf5\uc7a1\ud55c \uc2dc\uac01\uc801 \uc7a5\uba74\uc744 \ub2e4\ub8e8\ub294 \ub370 \uc788\uc5b4 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uba87 \uac00\uc9c0 \uc608\uc2dc\ub97c \ud1b5\ud574 \ube44\uad50\ub429\ub2c8\ub2e4. \ud2b9\ud788, LLaVA-UHD v2 \ubaa8\ub378\uc740 \uc791\uc740 \ubb3c\uccb4\ub098 \ud750\ub9bf\ud55c \ud14d\uc2a4\ud2b8\uc640 \uac19\uc774 \ubbf8\uc138\ud55c \uc2dc\uac01\uc801 \uc138\ubd80 \uc0ac\ud56d\uc744 \uc2dd\ubcc4\ud558\ub294 \ub2a5\ub825\uc744 \uac15\uc870\ud558\uc5ec, \ub2e4\ub978 \ubaa8\ub378\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.5 \uc2dc\uac01\ud654 \ubd84\uc11d"}, {"figure_path": "https://arxiv.org/html/2412.13871/x11.png", "caption": "Figure 10: Qualitative comparison on high-resolution spatial perception which necessitates the capabilities of high-level spatial contexts.", "description": "\uadf8\ub9bc 10\uc740 \uace0\ud574\uc0c1\ub3c4 \uacf5\uac04\uc801 \uc9c0\uac01\uc5d0 \ub300\ud55c \uc815\uc131\uc801 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \uace0\ucc28\uc6d0 \uacf5\uac04\uc801 \ub9e5\ub77d\uc744 \ud30c\uc545\ud558\ub294 \ub2a5\ub825\uc774 \ud544\uc694\ud55c \uc791\uc5c5\uc785\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uace0\ud574\uc0c1\ub3c4 \uc2dc\uac01\uc801 \uc778\uc2dd \uc791\uc5c5\uc5d0\uc11c LLaVA-UHD v2, LLaVA-Next, Mini-Gemini, GPT-4V\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc5ec\ub7ec \uc0ac\ub840\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uac01 \uc0ac\ub840\ub294 \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uc138\ubd80\uc801\uc778 \uc2dc\uac01\uc801 \uc815\ubcf4\uc640 \uace0\ucc28\uc6d0 \uc758\ubbf8\uc801 \ub9e5\ub77d\uc744 \ud1b5\ud569\ud558\uc5ec \uc815\ud655\ud558\uac8c \uac1d\uccb4\ub97c \uc2dd\ubcc4\ud558\uace0 \uc0c1\ud638 \uad00\uacc4\ub97c \ud30c\uc545\ud574\uc57c \ud558\ub294 \ubcf5\uc7a1\ud55c \uc791\uc5c5\uc785\ub2c8\ub2e4. LLaVA-UHD v2\ub294 \uace0\ud574\uc0c1\ub3c4 \uacf5\uac04\uc801 \uc9c0\uac01 \ub2a5\ub825\uc744 \uac16\ucd94\uace0 \uc788\uc5b4 \uc138\ubd80\uc801\uc778 \uc2dc\uac01\uc801 \uc815\ubcf4\uc640 \uace0\ucc28\uc6d0 \uc758\ubbf8\uc801 \ub9e5\ub77d\uc744 \uc815\ud655\ud558\uac8c \ud1b5\ud569\ud558\uc5ec \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubc18\uba74\uc5d0 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc740 \uace0\ud574\uc0c1\ub3c4 \uacf5\uac04\uc801 \uc9c0\uac01 \ub2a5\ub825\uc774 \ubd80\uc871\ud558\uc5ec \uc77c\ubd80 \uc791\uc5c5\uc5d0\uc11c \uc5b4\ub824\uc6c0\uc744 \uacaa\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4. \uc774\ub294 LLaVA-UHD v2\uac00 \uace0\ud574\uc0c1\ub3c4 \uacf5\uac04\uc801 \uc778\uc2dd\uc5d0 \ub300\ud55c \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \uac00\uc9d0\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4.5 \uc2dc\uac01\ud654 \ubd84\uc11d"}, {"figure_path": "https://arxiv.org/html/2412.13871/x12.png", "caption": "Figure 11: PCA visualization of the up-sampled features by JBU module on nature scene. With hierarchical supervision, the high-resolution features (8\u00d78\\times8 \u00d7) could clearly depict object boundary and text appearance. (Best viewed in color and zoomed in)", "description": "\uadf8\ub9bc 11\uc740 \uc790\uc5f0 \uc7a5\uba74\uc5d0 \ub300\ud574 JBU \ubaa8\ub4c8\uc5d0 \uc758\ud574 \uc5c5\uc0d8\ud50c\ub9c1\ub41c \ud2b9\uc9d5\ub4e4\uc758 PCA \uc2dc\uac01\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uacc4\uce35\uc801 \uac10\ub3c5\uc744 \uc0ac\uc6a9\ud558\uba74 \uace0\ud574\uc0c1\ub3c4 \ud2b9\uc9d5(8\ubc30)\uc774 \uac1d\uccb4 \uacbd\uacc4\uc640 \ud14d\uc2a4\ud2b8 \ubaa8\uc591\uc744 \uba85\ud655\ud558\uac8c \ubb18\uc0ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc0c9\uc0c1\uc73c\ub85c \ubcf4\uba74 \ub354\uc6b1 \ud6a8\uacfc\uc801\uc785\ub2c8\ub2e4.", "section": "3.2 \uc5ed \ud2b9\uc9d5 \ud53c\ub77c\ubbf8\ub4dc"}]