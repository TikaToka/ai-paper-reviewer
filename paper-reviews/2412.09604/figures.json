[{"figure_path": "https://arxiv.org/html/2412.09604/x1.png", "caption": "Figure 1: Comparison among exemplary unified MLLMs for synergizing image understanding and generation tasks. Compared with methods (a)\u223csimilar-to\\sim\u223c(d) that incorporate complicated designs of model architectures, training methods, and the use of external pretrained diffusion models, (e) encoder-free unified MLLMs adopt a simple design that uses the simple next token prediction framework for both images understanding and generation tasks, allowing for broader data distribution and better scalability.", "description": "\uc774 \uadf8\ub9bc\uc740 \uc774\ubbf8\uc9c0 \uc774\ud574 \ubc0f \uc0dd\uc131 \uc791\uc5c5\uc744 \uc704\ud55c \ub2e4\uc591\ud55c \ud1b5\ud569 MLLM(\ub2e4\uc911 \ubaa8\ub2ec \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378)\uc758 \uad6c\uc870\ub97c \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)~(d)\uc640 \uac19\uc774 \ubcf5\uc7a1\ud55c \ubaa8\ub378 \uad6c\uc870, \ud6c8\ub828 \ubc29\ubc95, \uc678\ubd80 \uc0ac\uc804 \ud6c8\ub828\ub41c \ud655\uc0b0 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uacfc \ub2ec\ub9ac, (e)\uc758 \uc778\ucf54\ub354\uac00 \uc5c6\ub294 \ud1b5\ud569 MLLM\uc740 \uc774\ubbf8\uc9c0 \uc774\ud574 \ubc0f \uc0dd\uc131 \uc791\uc5c5 \ubaa8\ub450\uc5d0 \uac04\ub2e8\ud55c \ub2e4\uc74c \ud1a0\ud070 \uc608\uce21 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ud558\ub294 \ub2e8\uc21c\ud55c \uc124\uacc4\ub97c \ucc44\ud0dd\ud558\uc5ec \ub354 \ub113\uc740 \ub370\uc774\ud130 \ubd84\ud3ec\uc640 \ud655\uc7a5\uc131\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "\uc18c\uac1c"}, {"figure_path": "https://arxiv.org/html/2412.09604/x2.png", "caption": "Figure 2: Comparision between SynerGen-VL and previous encoder-free unified MLLMs. SynerGen-VL adopts a token folding and unfolding mechanism and vision experts to build a strong and simple unified MLLM. With the same image context length, SynerGen-VL can support images of much higher resolutions, ensuring the performance of both high-resolution image understanding and generation.", "description": "SynerGen-VL\uc740 \ud1a0\ud070 \uc811\uae30 \ubc0f \ud3bc\uce58\uae30 \uba54\ucee4\ub2c8\uc998\uacfc \ube44\uc804 \uc804\ubb38\uac00\ub97c \ud65c\uc6a9\ud558\uc5ec \uac15\ub825\ud558\uace0 \ub2e8\uc21c\ud55c \ud1b5\ud569 MLLM\uc744 \uad6c\ucd95\ud569\ub2c8\ub2e4. \ub3d9\uc77c\ud55c \uc774\ubbf8\uc9c0 \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774\ub85c SynerGen-VL\uc740 \ud6e8\uc52c \ub354 \ub192\uc740 \ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0\ub97c \uc9c0\uc6d0\ud558\uc5ec \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0 \uc774\ud574 \ubc0f \uc0dd\uc131 \uc131\ub2a5\uc744 \ubcf4\uc7a5\ud569\ub2c8\ub2e4. \uae30\uc874\uc758 \uc778\ucf54\ub354 \uc5c6\ub294 \ud1b5\ud569 MLLM\uc740 \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\ub97c \ucc98\ub9ac\ud558\ub294 \ub370 \uc5b4\ub824\uc6c0\uc744 \uacaa\uc5c8\uc9c0\ub9cc SynerGen-VL\uc740 \ud1a0\ud070 \uc811\uae30\ub97c \ud1b5\ud574 \uc785\ub825 \uc774\ubbf8\uc9c0 \ud1a0\ud070 \uc2dc\ud000\uc2a4\ub97c \uc555\ucd95\ud558\uace0, \ube44\uc804 \uc804\ubb38\uac00\ub97c \ud1b5\ud574 \uc774\ubbf8\uc9c0 \ud45c\ud604 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ucf1c \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \ud574\uacb0\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c SynerGen-VL\uc740 \ub354 \uc791\uc740 \ubaa8\ub378 \ud06c\uae30\ub85c\ub3c4 \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0 \uc774\ud574 \ubc0f \uc0dd\uc131 \uc791\uc5c5\uc5d0\uc11c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9bc\uc740 SynerGen-VL\uacfc \uae30\uc874 MLLM\uc758 \ucc28\uc774\uc810\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. SynerGen-VL"}, {"figure_path": "https://arxiv.org/html/2412.09604/x3.png", "caption": "Figure 3: Overview of the proposed SynerGen-VL. The image and text are represented as discrete tokens, and modeled with a single LLM and unified next-token prediction paradigm. Text and vision expert FFNs are introduced to incorporate visual capabilities into the pretrained LLM. To support processing high-resolution images, the input image token sequence is folded to reduce its length, and unfolded by a shallow autoregressive Transformer head to generate images.", "description": "SynerGen-VL\uc740 \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8\ub97c \uc774\uc0b0 \ud1a0\ud070\uc73c\ub85c \ud45c\ud604\ud558\uace0 \ub2e8\uc77c LLM\uacfc \ud1b5\ud569\ub41c \ub2e4\uc74c \ud1a0\ud070 \uc608\uce21 \ud328\ub7ec\ub2e4\uc784\uc744 \uc0ac\uc6a9\ud558\ub294 \ud1b5\ud569 \uba40\ud2f0\ubaa8\ub2ec \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(MLLM)\uc785\ub2c8\ub2e4. \uc0ac\uc804 \ud6c8\ub828\ub41c LLM\uc5d0 \uc2dc\uac01\uc801 \uae30\ub2a5\uc744 \ud1b5\ud569\ud558\uae30 \uc704\ud574 \ud14d\uc2a4\ud2b8 \ubc0f \ube44\uc804 \uc804\ubb38\uac00 FFN\uc774 \ub3c4\uc785\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0 \ucc98\ub9ac\ub97c \uc9c0\uc6d0\ud558\uae30 \uc704\ud574 \uc785\ub825 \uc774\ubbf8\uc9c0 \ud1a0\ud070 \uc2dc\ud000\uc2a4\ub294 \uae38\uc774\ub97c \uc904\uc774\uae30 \uc704\ud574 \uc811\ud788\uace0(token folding), \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\uae30 \uc704\ud574 \uc595\uc740 \uc790\uae30 \ud68c\uadc0 \ubcc0\ud658\uae30 \ud5e4\ub4dc\uc5d0 \uc758\ud574 \ud3bc\uccd0\uc9d1\ub2c8\ub2e4(token unfolding).", "section": "3.1. Architecture"}, {"figure_path": "https://arxiv.org/html/2412.09604/x4.png", "caption": "Figure 4: Cosine similarity of visual features between generation and understanding tasks across different layers. The representations of the image understanding and generation tasks are similar in shallow layers but disentagle in deeper layers.", "description": "\uc774 \uadf8\ub9bc\uc740 \uc774\ubbf8\uc9c0 \uc0dd\uc131\uacfc \uc774\ud574 \uc791\uc5c5\uc5d0\uc11c \ucd94\ucd9c\ub41c \uc2dc\uac01\uc801 \ud2b9\uc9d5 \uac04\uc758 \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4\ub97c \uac01 \ub808\uc774\uc5b4\ubcc4\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0dd\uc131 \ubc0f \uc774\ud574 \uc791\uc5c5\uc5d0 \ub300\ud55c \ud45c\ud604\uc740 \uc595\uc740 \ub808\uc774\uc5b4\uc5d0\uc11c\ub294 \uc720\uc0ac\ud558\uc9c0\ub9cc \uae4a\uc740 \ub808\uc774\uc5b4\ub85c \uac08\uc218\ub85d \ucc28\uc774\uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4. \uc989, \ub450 \uc791\uc5c5 \ubaa8\ub450 \ucd08\uae30 \ub2e8\uacc4\uc5d0\uc11c\ub294 \uae30\ubcf8\uc801\uc778 \uc2dc\uac01\uc801 \ud45c\ud604\uc744 \uacf5\uc720\ud558\uc9c0\ub9cc, \ub808\uc774\uc5b4\uac00 \uae4a\uc5b4\uc9d0\uc5d0 \ub530\ub77c \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc740 \uad6d\ubd80\uc801 \uc138\ubd80 \uc0ac\ud56d\uc5d0, \uc774\ubbf8\uc9c0 \uc774\ud574\ub294 \uc804\uccb4\uc801\uc778 \ub9e5\ub77d\uc5d0 \uc9d1\uc911\ud558\uba74\uc11c \uc791\uc5c5\ubcc4\ub85c \ud2b9\ud654\ub41c \ud45c\ud604\uc744 \uac1c\ubc1c\ud569\ub2c8\ub2e4.", "section": "5.3. Analysis of Relationship Between Image Generation and Understanding"}, {"figure_path": "https://arxiv.org/html/2412.09604/x5.png", "caption": "Figure 5: Attention map visualization of understanding and generation tasks. In the second and fourth rows, we visualize a query token (red) and its attended tokens (blue) in the input image. Each token corresponds to a horizontal rectangular area in the original image due to the 2\u00d74242\\times 42 \u00d7 4 token folding. Darker blue indicates larger attention weights.", "description": "\uc774 \uadf8\ub9bc\uc740 \uc774\ud574 \ubc0f \uc0dd\uc131 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc5b4\ud150\uc158 \ub9f5 \uc2dc\uac01\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub450 \ubc88\uc9f8 \ubc0f \ub124 \ubc88\uc9f8 \ud589\uc5d0\uc11c \ucffc\ub9ac \ud1a0\ud070(\ube68\uac04\uc0c9)\uacfc \uc785\ub825 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uc8fc\ubaa9\ud558\ub294 \ud1a0\ud070(\ud30c\ub780\uc0c9)\uc744 \uc2dc\uac01\ud654\ud569\ub2c8\ub2e4. \uac01 \ud1a0\ud070\uc740 2x4 \ud1a0\ud070 \ud3f4\ub529\uc73c\ub85c \uc778\ud574 \uc6d0\ubcf8 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uac00\ub85c \uc9c1\uc0ac\uac01\ud615 \uc601\uc5ed\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4. \uc9c4\ud55c \ud30c\ub780\uc0c9\uc740 \ub354 \ud070 \uc5b4\ud150\uc158 \uac00\uc911\uce58\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774\ud574 \uc791\uc5c5\uc758 \uacbd\uc6b0, \ubaa8\ub378\uc740 \uc774\ubbf8\uc9c0\uc758 \uc804\uc5ed \ucee8\ud14d\uc2a4\ud2b8\uc5d0 \uc8fc\ubaa9\ud558\ub294 \ubc18\uba74 \uc0dd\uc131 \uc791\uc5c5\uc758 \uacbd\uc6b0 \ub85c\uceec \uc138\ubd80 \uc815\ubcf4\uc5d0 \ub354 \uc9d1\uc911\ud569\ub2c8\ub2e4. \uc774\ub294 \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\uc758 \uacf5\uac04\uc801 \uc77c\uad00\uc131\uacfc \uc758\ubbf8\uc801 \uc77c\uad00\uc131\uc744 \ubcf4\uc7a5\ud558\uae30 \uc704\ud574 \ub85c\uceec \uc138\ubd80 \uc815\ubcf4\uac00 \ud544\uc694\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \ub610\ud55c \ud14d\uc2a4\ud2b8 \ud1a0\ud070\uacfc \uc774\ubbf8\uc9c0 \uac04\uc758 \uc0c1\ud638 \uc791\uc6a9\uc740 \uc595\uc740 \ub808\uc774\uc5b4\ubcf4\ub2e4 \uae4a\uc740 \ub808\uc774\uc5b4\uc5d0\uc11c \ub354 \uc790\uc8fc \ubc1c\uc0dd\ud558\ub294\ub370, \uc774\ub294 \ud14d\uc2a4\ud2b8\uac00 \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.3. Analysis of Relationship Between Image Generation and Understanding"}, {"figure_path": "https://arxiv.org/html/2412.09604/x6.png", "caption": "Figure 6: Qualitative results of image generation. The images are of size 512\u00d7512512512512\\times 512512 \u00d7 512.", "description": "\uc774 \uadf8\ub9bc\uc740 SynerGen-VL \ubaa8\ub378\uc758 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \ub2a5\ub825\uc744 \uc815\uc131\uc801\uc73c\ub85c \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub2e4\uc591\ud55c \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ub41c 512x512 \ud06c\uae30\uc758 \uc774\ubbf8\uc9c0\ub4e4\uc744 \ubcf4\uc5ec\uc8fc\uace0 \uc788\uc73c\uba70, \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\ub4e4\uc774 \ud504\ub86c\ud504\ud2b8\uc758 \ub0b4\uc6a9\uc744 \uc5bc\ub9c8\ub098 \uc798 \ubc18\uc601\ud558\ub294\uc9c0, \uadf8\ub9ac\uace0 \uc774\ubbf8\uc9c0\uc758 \ud488\uc9c8\uc774 \uc5b4\ub5a4\uc9c0\ub97c \uc2dc\uac01\uc801\uc73c\ub85c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608\uc2dc\ub85c \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\ub4e4\uc740 \ub3c4\uc2dc \ud48d\uacbd, \uc790\uc5f0 \uacbd\uad00, \uc778\ubb3c, \ucd94\uc0c1\uc801\uc778 \uadf8\ub9bc \ub4f1 \ub2e4\uc591\ud55c \uc8fc\uc81c\ub97c \ub2e4\ub8e8\uace0 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}]