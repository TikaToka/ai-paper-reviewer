<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Sequence Matters: Harnessing Video Models in 3D Super-Resolution &#183; AI Paper Reviews by AI</title>
<meta name=title content="Sequence Matters: Harnessing Video Models in 3D Super-Resolution &#183; AI Paper Reviews by AI"><meta name=description content="비디오 초해상도 모델을 이용한 혁신적인 3D 초해상도 기법으로, 정렬 과정 없이도 최첨단 성능 달성!"><meta name=keywords content="Computer Vision,3D Vision,🏢 Department of Electrical and Computer Engineering,Sungkyunkwan University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="Sequence Matters: Harnessing Video Models in 3D Super-Resolution"><meta property="og:description" content="비디오 초해상도 모델을 이용한 혁신적인 3D 초해상도 기법으로, 정렬 과정 없이도 최첨단 성능 달성!"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-16T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-16T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="3D Vision"><meta property="article:tag" content="🏢 Department of Electrical and Computer Engineering, Sungkyunkwan University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/cover.png"><meta name=twitter:title content="Sequence Matters: Harnessing Video Models in 3D Super-Resolution"><meta name=twitter:description content="비디오 초해상도 모델을 이용한 혁신적인 3D 초해상도 기법으로, 정렬 과정 없이도 최첨단 성능 달성!"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Sequence Matters: Harnessing Video Models in 3D Super-Resolution","headline":"Sequence Matters: Harnessing Video Models in 3D Super-Resolution","abstract":"비디오 초해상도 모델을 이용한 혁신적인 3D 초해상도 기법으로, 정렬 과정 없이도 최첨단 성능 달성!","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.11525\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-12-16T00:00:00\u002b00:00","datePublished":"2024-12-16T00:00:00\u002b00:00","dateModified":"2024-12-16T00:00:00\u002b00:00","keywords":["Computer Vision","3D Vision","🏢 Department of Electrical and Computer Engineering, Sungkyunkwan University"],"mainEntityOfPage":"true","wordCount":"3903"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.11525/cover_hu13365937017901179153.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.11525/>Sequence Matters: Harnessing Video Models in 3D Super-Resolution</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Sequence Matters: Harnessing Video Models in 3D Super-Resolution</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-16T00:00:00+00:00>16 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>3903 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">19 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.11525/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.11525/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🤗 Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/3d-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">3D Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-department-of-electrical-and-computer-engineering-sungkyunkwan-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🏢 Department of Electrical and Computer Engineering, Sungkyunkwan University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#vsr-in-3d-super-res>VSR in 3D Super-Res</a></li><li><a href=#greedy-ordering>Greedy Ordering</a></li><li><a href=#artifact-mitigation>Artifact Mitigation</a></li><li><a href=#adaptive-subseq>Adaptive Subseq.</a></li><li><a href=#future-of-vsr>Future of VSR</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#vsr-in-3d-super-res>VSR in 3D Super-Res</a></li><li><a href=#greedy-ordering>Greedy Ordering</a></li><li><a href=#artifact-mitigation>Artifact Mitigation</a></li><li><a href=#adaptive-subseq>Adaptive Subseq.</a></li><li><a href=#future-of-vsr>Future of VSR</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.11525</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Hyun-kyu Ko et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>🤗 2024-12-23</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.11525 target=_self role=button>↗ arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.11525 target=_self role=button>↗ Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/sequence-matters-harnessing-video-models-in target=_self role=button>↗ Papers with Code</a></p><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>3D 초해상도는 저해상도의 다중 뷰 이미지로부터 고해상도 3D 모델을 생성하는 어려운 문제입니다. 기존의 단일 이미지 초해상도 기법은 각 이미지를 독립적으로 처리하기 때문에 뷰 사이의 일관성이 부족하고, 이를 해결하기 위한 후처리 기법은 계산 비용이 많이 들고 완벽한 해결책이 아닙니다. 본 논문에서는 이러한 문제를 해결하기 위해 비디오 초해상도(VSR) 모델을 활용하는 새로운 접근법을 제시합니다.</p><p>본 논문의 핵심 아이디어는 <strong>VSR 모델의 우수한 공간적 일관성을 활용</strong>하여 저해상도 이미지 시퀀스를 고해상도로 업스케일링하는 것입니다. 연구진은 간단하면서도 효과적인 이미지 정렬 알고리즘을 제안하여, VSR 모델의 미세 조정 없이도 표준 벤치마크 데이터셋에서 최첨단 성능을 달성했습니다. 특히, <strong>VSR 모델이 정확한 공간적 정렬이 부족한 시퀀스에서도 우수한 성능을 보이는 점을 확인</strong>하여, 실제 환경에서의 적용 가능성을 높였습니다.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-d20f1ecac8020a4b7332f1cb0c78808a></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-d20f1ecac8020a4b7332f1cb0c78808a",{strings:[" 비디오 초해상도(VSR) 모델을 활용하여 저해상도 다중 뷰 이미지로부터 고해상도 3D 모델을 효율적으로 재구성하는 새로운 기법 제시 "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-5c3d4ca0c9f38ec731fff32d63a95cc3></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-5c3d4ca0c9f38ec731fff32d63a95cc3",{strings:[" 정밀한 공간 정렬 없이도 VSR 모델이 높은 정확도를 유지하며, 기존 방식의 한계점인 뷰 일관성 부족 문제 해결 "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-5a1bb46dba06ba1c5fdf676ef8dc56c8></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-5a1bb46dba06ba1c5fdf676ef8dc56c8",{strings:[" NeRF-synthetic 및 Mip-NeRF 360 데이터셋에서 최첨단 성능 달성 "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>본 논문은 <strong>저해상도의 다중 뷰 이미지로부터 고해상도의 3D 모델을 재구성하는 3D 초해상도 문제</strong>에 대한 새로운 접근법을 제시합니다. 기존의 단일 이미지 초해상도 기법의 한계를 극복하고, 비디오 초해상도 모델을 활용하여 <strong>공간적 일관성을 높이고 계산 비용을 줄이는 효율적인 방법</strong>을 제시함으로써, 3D 초해상도 분야의 발전에 크게 기여할 수 있습니다. 특히, <strong>사전 훈련된 비디오 초해상도 모델을 활용</strong>하여 추가적인 미세 조정 없이도 우수한 성능을 달성한 점은 매우 중요한 의미를 지닙니다. 이는 향후 <strong>다양한 3D 모델링 및 시각화 연구</strong>에 널리 활용될 가능성이 높습니다.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11525/extracted/6087365/figures/stripy_and_blob_artifacts.jpg alt></figure></p><blockquote><p>🔼 이 그림은 3D Gaussian Splatting(3DGS)으로 렌더링된 저해상도(LR) 비디오의 VSR(Video Super-Resolution) 출력에서 발생하는 줄무늬 또는 얼룩과 같은 인공물을 보여줍니다. &lsquo;VSR-Render&rsquo;는 LR 렌더링 비디오의 VSR 출력을 보여주는 반면, &lsquo;VSR-GT&rsquo;는 실제(GT) LR 비디오의 VSR 출력을 보여줍니다. 즉, 3D 모델에서 생성된 LR 비디오와 실제 LR 비디오를 VSR 모델에 입력했을 때 출력 결과의 차이를 보여주는 것으로, 3DGS를 통해 생성된 비디오가 실제 비디오와 다르다는 것을 시각적으로 보여주는 중요한 그림입니다. 이는 VSR 모델의 성능에 부정적인 영향을 미치는 3DGS 렌더링 과정의 한계를 강조합니다.</p><details><summary>read the caption</summary>Figure 1: Illustration of stripy or blob-like artifacts generated in VSR outputs of LR videos rendered from 3DGS. ‘VSR-Render’ shows the VSR outputs of the LR rendered videos, while ‘VSR-GT’ displays the VSR outputs of the ground truth (GT) LR videos.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Index</th><th>L</th><th></th><th>R</th><th></th></tr></thead><tbody><tr><td></td><td>S</td><td>ALS</td><td>S</td><td>ALS</td></tr><tr><td>1</td><td>34.06</td><td>37.18</td><td>34.53</td><td>35.52</td></tr><tr><td>2</td><td>33.12</td><td>34.33</td><td>34.67</td><td>36.63</td></tr><tr><td>3</td><td>32.90</td><td>33.37</td><td>31.62</td><td>34.26</td></tr><tr><td>4</td><td>33.47</td><td>34.41</td><td>33.68</td><td>34.29</td></tr><tr><td>5</td><td>34.07</td><td>35.68</td><td>32.77</td><td>35.31</td></tr><tr><td>6</td><td>32.65</td><td>34.41</td><td>32.05</td><td>32.77</td></tr><tr><td>7</td><td>32.71</td><td>33.43</td><td>32.68</td><td>34.66</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 1은 제안된 정렬 알고리즘의 정량적 결과를 보여줍니다. &lsquo;S&rsquo;는 단순 탐욕적 알고리즘을, &lsquo;ALS&rsquo;는 적응적 길이 하위 시퀀스 알고리즘을 나타냅니다. 그림 4의 두 이미지 쌍에서 왼쪽 및 오른쪽 이미지의 PSNR 값을 각각 &lsquo;L&rsquo;과 &lsquo;R&rsquo;로 표시했습니다. 이 표는 제안된 두 가지 알고리즘(단순 탐욕적 알고리즘과 적응적 길이 하위 시퀀스 알고리즘)의 성능을 이미지 쌍별 PSNR 값을 통해 비교 분석한 결과를 보여줍니다. 각 알고리즘의 성능을 보다 명확하게 이해할 수 있도록 그림 4에 제시된 두 개의 이미지 쌍에 대한 왼쪽 및 오른쪽 이미지의 PSNR 값을 상세히 제시하고 있습니다. 이를 통해, 각 알고리즘이 이미지의 정렬에 따라 어떻게 다른 성능을 보이는지, 그리고 어떤 알고리즘이 더 나은 정렬 결과를 제공하는지를 정량적으로 비교 분석할 수 있습니다.</p><details><summary>read the caption</summary>Table 1: The quantitative results of the proposed ordering algorithms. S: the simple greedy algorithm, ALS: the adaptive-length subsequence. L and R denote the PSNR of the left and right image in two image pairs from Fig. 4.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">VSR in 3D Super-Res<div id=vsr-in-3d-super-res class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#vsr-in-3d-super-res aria-label=Anchor>#</a></span></h4><p>본 논문에서 제안하는 VSR(Video Super-Resolution) 기반 3D 초고해상도 기법은 기존의 SISR(Single Image Super-Resolution) 방식의 한계를 극복하기 위한 시도입니다. <strong>VSR 모델을 활용하여 다중 뷰 이미지 시퀀스의 공간적 일관성을 높임으로써 3D 모델 재구성의 정확도를 향상</strong>시키는 것이 핵심입니다. 단순히 해상도만 높이는 것이 아니라, 시간적 정보를 활용하여 보다 정확하고 디테일한 3D 모델을 생성하는 데 초점을 맞추고 있습니다. <strong>특히, LR(Low-Resolution) 이미지 시퀀스에서 발생할 수 있는 정렬 오류나 왜곡을 효과적으로 보정하기 위한 간단하지만 실용적인 알고리즘</strong>을 제안하여 주목할 만합니다. 이는 고품질의 3D 모델 생성에 필수적인 정확한 공간적 정합을 확보하는 데 크게 기여할 것으로 예상됩니다. <strong>기존의 VSR 기법이 갖는 한계점을 극복하기 위해, 훈련 데이터의 구조화된 시퀀스 생성 방식을 제안</strong>한 점도 돋보입니다. 이를 통해 VSR 모델의 성능을 최대화하고, 3D 초고해상도 작업에서 최첨단 결과를 달성할 수 있었습니다.</p><h4 class="relative group">Greedy Ordering<div id=greedy-ordering class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#greedy-ordering aria-label=Anchor>#</a></span></h4><p>본 논문에서 제안하는 &lsquo;탐욕적 순서 정렬(Greedy Ordering)&lsquo;은 비정렬된 다중 뷰 이미지들을 효율적으로 정렬하여 비디오와 유사한 시퀀스를 생성하는 알고리즘입니다. <strong>단순하면서도 효과적</strong>으로, 각 이미지의 이웃 이미지를 선택하여 시퀀스를 확장하는 방식을 취합니다. <strong>단순 탐욕적 접근 방식</strong>은 연산량이 적다는 장점이 있지만, 최적의 순서를 보장하지는 못합니다. 이러한 한계를 극복하고자, <strong>적응적 길이 하위 시퀀스(Adaptive-Length Subsequencing)</strong> 기법을 추가적으로 제안합니다. 이 기법은 여러 개의 하위 시퀀스를 생성하여, 각 이미지가 최소한 하나의 시퀀스에 포함되도록 함으로써, 단일 탐욕적 접근 방식의 제한점을 보완합니다. <strong>카메라 자세 및 시각적 특징</strong>을 활용한 유사도 측정 방식을 통해 이미지 간의 유사성을 평가합니다. 결론적으로, 탐욕적 순서 정렬은 비디오 초고해상도 모델의 성능 향상을 위해 비정렬 이미지들의 효과적인 시퀀스 생성이라는 중요한 역할을 수행하며, 단순성과 효율성을 갖춘 실용적인 방법임을 보여줍니다.</p><h4 class="relative group">Artifact Mitigation<div id=artifact-mitigation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#artifact-mitigation aria-label=Anchor>#</a></span></h4><p>본 논문에서 다루는 핵심 개념 중 하나인 &lsquo;아티팩트 완화&rsquo;는 저해상도 이미지로부터 고해상도 3D 모델을 재구성하는 과정에서 발생하는 인공적인 왜곡이나 잡음을 줄이는 기술을 의미합니다. <strong>특히, 저해상도 영상을 고해상도로 변환하는 과정에서 비디오 슈퍼해상도(VSR) 모델이 사용되는데, 이때 3D 모델 렌더링 과정에서 발생하는 줄무늬나 블롭과 같은 아티팩트가 VSR 모델 성능을 저해할 수 있습니다.</strong> 이러한 문제를 해결하기 위해, 논문에서는 <strong>3D 모델 렌더링 과정 자체의 개선이나 VSR 모델의 미세 조정 없이도, 훈련 데이터셋을 효과적으로 정렬하여 아티팩트를 줄이는 방법을 제안합니다.</strong> 이는 <strong>VSR 모델의 입력으로 사용되는 저해상도 이미지 시퀀스의 순서를 최적화함으로써, VSR 모델이 주변 정보를 더욱 효과적으로 활용하여 정확하고 상세한 3D 모델을 재구성하도록 유도하는 전략입니다.</strong> 이를 위해 제안된 알고리즘은 간결하지만 효과적이며, 표준 벤치마크 데이터셋에서 최첨단 성능을 달성함으로써 그 효용성을 입증합니다. <strong>즉, 아티팩트 완화에 대한 핵심은 저해상도 영상 자체의 품질 향상이 아닌, VSR 모델의 입력 데이터를 최적화하여 3D 재구성 과정에서의 효율성을 높이는 데 있습니다.</strong> 이러한 접근법은 계산 비용을 절감하고, 데이터셋에 대한 의존성을 줄이는 데 기여합니다.</p><h4 class="relative group">Adaptive Subseq.<div id=adaptive-subseq class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#adaptive-subseq aria-label=Anchor>#</a></span></h4><p>본 논문에서 제안하는 적응적 서브시퀀스(Adaptive Subseq.) 기법은 비정렬된 다중 뷰 이미지들을 효과적으로 정렬하여 비디오와 유사한 시퀀스를 생성하는 방법입니다. 기존의 단순 탐욕적 알고리즘(Simple Greedy Algorithm)의 한계를 극복하기 위해 **여러 임계값(Threshold)**을 사용하여 다양한 길이와 매끄러움을 가진 여러 개의 서브시퀀스를 생성합니다. <strong>높은 임계값</strong>은 매끄러운 시퀀스를 생성하지만 짧은 시퀀스가 될 수 있으며, <strong>낮은 임계값</strong>은 긴 시퀀스를 생성하지만 매끄럽지 않을 수 있습니다. 이러한 문제를 해결하기 위해 적응적 서브시퀀스 기법은 여러 단계의 임계값을 사용하여, 가장 매끄러운 시퀀스를 우선 생성하고 이미지가 남아있으면 임계값을 낮춰 더 많은 이미지를 포함한 시퀀스를 만듭니다. 결과적으로, <strong>다양한 특성의 이미지들</strong>을 모두 효율적으로 활용하여 VSR 모델의 성능을 최적화하는 데 기여합니다. 이 기법은 <strong>데이터셋의 다양한 특징</strong>을 고려하여, 객체 중심 데이터셋과 장면 중심 데이터셋 모두에서 우수한 성능을 보여주는 유연성을 제공합니다. <strong>단순하면서 효과적인 접근 방식</strong>으로 VSR 모델의 성능을 향상시키는 핵심 요소입니다.</p><h4 class="relative group">Future of VSR<div id=future-of-vsr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-of-vsr aria-label=Anchor>#</a></span></h4><p>VSR의 미래는 <strong>고해상도 비디오 생성의 정확성과 효율성을 향상시키는 데 중점을 둘 것</strong>으로 예상됩니다. 이는 더욱 발전된 신경망 아키텍처, 특히 <strong>더욱 효과적이고 효율적인 특징 추출 및 정렬 메커니즘</strong>을 통해 가능해질 것입니다. <strong>대용량 고품질 데이터셋</strong>의 등장 또한 VSR 모델의 성능 향상에 크게 기여할 것입니다. <strong>시간적 일관성 유지 및 다양한 비디오 유형에 대한 일반화 능력 향상</strong> 또한 중요한 연구 분야가 될 것입니다. 결국, 미래의 VSR 기술은 <strong>실시간 고품질 비디오 생성 및 처리</strong>를 가능하게 하여, 방송, 영화 제작, 의료 영상 처리, 자율 주행 등 다양한 분야에서 혁신을 주도할 것으로 전망됩니다. 또한, <strong>에너지 효율적인 모델 설계</strong>에 대한 연구도 활발해져, 환경 문제에 대한 사회적 책임을 다하는 기술 발전이 이뤄질 것입니다.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11525/extracted/6087365/figures/main.jpg alt></figure></p><blockquote><p>🔼 그림 2는 제안된 3D 슈퍼 해상도 기법의 개요를 보여줍니다. 먼저 저해상도(LR) 다중 뷰 이미지가 입력으로 주어집니다. 이 이미지들을 기반으로 단순 탐욕 알고리즘(Section 3.2)을 사용하여 각 이미지에서 시작하는 여러 개의 부분 수열(subsequence)을 생성합니다. 이 부분 수열들은 여러 임계값(threshold)으로 경계가 지정됩니다(Section 3.3). 이렇게 생성된 부분 수열들은 비디오 슈퍼 해상도(VSR) 모델을 통해 고해상도(HR) 이미지로 업스케일링됩니다. 마지막으로, 업스케일링된 HR 이미지들을 사용하여 3D 가우시안 스플래팅(3DGS) 모델을 학습시켜 3D 모델을 재구성합니다.</p><details><summary>read the caption</summary>Figure 2: Overview of the proposed method. Given LR multi-view images, we generate subsequences (Sec. 3.3) starting from each image using a simple greedy algorithm (Sec. 3.2) and these subsequences are bounded by multiple thresholds (Sec. 3.3). Finally, we train a 3DGS model for 3D reconstruction using the upsampled HR images.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11525/extracted/6087365/figures/subsequence.jpg alt></figure></p><blockquote><p>🔼 그림 3은 제안된 방법의 핵심인 서브 시퀀스 생성 과정을 보여줍니다. (a)는 순서가 없는 다중 뷰 이미지 데이터셋을 나타냅니다. (b)는 간단한 탐욕 알고리즘(알고리즘 1)을 적용하여 이미지들을 순차적으로 배열한 결과입니다. (c)는 이 알고리즘으로 인해 발생하는 이미지 정렬 오류를 보여주며, 연속적인 프레임 간의 자세 차이 임계값(빨간 점선)을 기준으로 이미지 시퀀스를 여러 개의 서브 시퀀스로 나누는 것을 제안합니다. 이를 통해 VSR 모델의 성능 저하를 야기하는 정렬 오류를 줄이고, 보다 효과적인 3D 초고해상도 재구성을 가능하게 합니다.</p><details><summary>read the caption</summary>Figure 3: Illustration of subsequence generation. (a) is an unordered multi-view image dataset. (b) is the result of using a simple greedy algorithm, Alg. 1. (c) highlights misalignments incurred by the algorithm, and we propose to split it into subsequences based on a pose difference threshold (red dotted line) between consecutive frames.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11525/extracted/6087365/figures/lego_simple_greedy_algorithm.jpg alt></figure></p><blockquote><p>🔼 그림 4는 간단한 탐욕 알고리즘을 NeRF 합성 데이터셋(Lego)에 적용한 결과의 예시입니다. 빨간색으로 강조된 두 개의 인접 이미지는 정렬 오류로 인해 발생하는 갑작스러운 전환을 보여줍니다. 이 그림은 간단한 탐욕 알고리즘이 이미지들을 순차적으로 나열할 때, 이미지 간의 시각적 유사성만을 고려하여 서로 관련 없는 이미지들을 연결하는 경우 발생할 수 있는 문제점을 보여줍니다. 즉, 정확한 공간적 정렬 없이 순차적으로 이미지들을 나열하는 경우, 결과물의 품질에 심각한 영향을 미칠 수 있음을 시각적으로 보여주는 예시입니다.</p><details><summary>read the caption</summary>Figure 4: An example result from the simple greedy algorithm applied to the NeRF-synthetic dataset (Lego). Two neighboring images highlighted in red demonstrate abrupt transitions caused by misalignments.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11525/extracted/6087365/figures/baseline_blender.jpg alt></figure></p><blockquote><p>🔼 그림 5는 NeRF-synthetic 데이터셋에 대한 정성적 결과를 보여줍니다. 각 이미지 패치에는 GT(Ground Truth)에 대한 PSNR(Peak Signal-to-Noise Ratio) 값이 표시되어 있습니다. 제안된 방법은 기존의 기준 모델들보다 특히 고주파수 디테일에 대한 성능이 우수함을 보여줍니다. 이 그림은 제안된 방법이 더욱 선명하고 디테일한 이미지 재구성을 달성했음을 시각적으로 보여줍니다. 특히, 기존 방법들로는 재구성하기 어려운 미세한 디테일까지 잘 표현하고 있다는 점을 강조합니다.</p><details><summary>read the caption</summary>Figure 5: Qualitative results on the NeRF-synthetic dataset. The PSNR values against GT are embedded in each image patch. Ours have shown superior results than the existing baselines, especially for high-frequency details.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11525/extracted/6087365/figures/baseline_mip360.jpg alt></figure></p><blockquote><p>🔼 그림 6은 Mip-NeRF 360 데이터셋에 대한 정성적 결과를 보여줍니다. 각 이미지 패치에는 GT(Ground Truth)에 대한 PSNR 값이 표시되어 있습니다. 제안된 방법은 기존의 기준 모델들보다 특히 고주파수 디테일에서 우수한 결과를 보여줍니다. 이는 제안된 방법이 고해상도 이미지의 세부적인 부분까지도 잘 복원함을 의미합니다. 즉, 기존 방법들로는 복원이 어려웠던 미세한 부분까지도 선명하게 복원하여 더욱 사실적인 3D 모델을 생성할 수 있음을 보여줍니다.</p><details><summary>read the caption</summary>Figure 6: Qualitative results on Mip-NeRF 360 dataset. The PSNR values against GT are embedded in each image patch. Ours have shown superior results than the existing baselines, especially for high-frequency details.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11525/extracted/6087365/figures/PSNR_LPIPS.jpg alt></figure></p><blockquote><p>🔼 그림 7은 제안된 방법의 성능을 기존 최첨단 방법들과 비교 분석한 결과를 보여줍니다. PSNR(Peak Signal-to-Noise Ratio)과 LPIPS(Learned Perceptual Image Patch Similarity) 지표를 사용하여 정량적으로 비교하였으며, 다양한 기존 방법들(NeRF-SR, ZS-SRT, CROP, FastSR-NeRF, DiSR-NeRF, SRGS, GaussianSR, SuperGaussian) 과 제안된 방법(Ours-S, Ours-ALS)의 성능 차이를 명확하게 보여줍니다. 이를 통해 제안된 방법이 기존 방법들에 비해 우수한 성능을 가짐을 시각적으로 확인할 수 있습니다. 특히, 제안된 방법의 두 가지 변형(Ours-S와 Ours-ALS) 모두 기존 최첨단 방법들보다 높은 PSNR 값과 낮은 LPIPS 값을 보이는 것을 알 수 있습니다.</p><details><summary>read the caption</summary>Figure 7: Comparison with baselines.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11525/extracted/6087365/figures/misalignment_trends_all_objects.jpg alt></figure></p><blockquote><p>🔼 그림 8은 시퀀스 내에서 이미지 정렬 오류의 추세를 보여줍니다. x축은 시퀀스 내 위치(0~100%), y축은 정렬 오류의 횟수를 나타냅니다. 각 선은 다른 물체에 대한 결과를 나타냅니다. 이 그래프는 그리디 알고리즘을 사용하여 시퀀스를 생성할 때 시퀀스의 후반부에서 정렬 오류가 증가하는 경향을 보여줍니다. 이는 그리디 알고리즘의 고유한 한계로 인해 발생하며, 긴 시퀀스에서는 연관성이 없는 특징들을 잘못 연결할 가능성이 높아지기 때문입니다. 이러한 정렬 오류는 VSR 모델의 성능을 저하시킵니다.</p><details><summary>read the caption</summary>Figure 8: Misalignment trends within a sequence.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11525/extracted/6087365/figures/misalignment.jpg alt></figure></p><blockquote><p>🔼 그림 9는 순차적으로 정렬되지 않은 멀티뷰 이미지들을 비디오처럼 연결하는 과정에서 발생하는 정렬 오류를 보여줍니다. 특히 ORB 특징점을 이용하여 이미지들을 연결할 때, 연속적인 프레임들을 정확하게 연결하지 못하는 경우가 발생하며, 이러한 오류는 시퀀스의 길이가 길어질수록 증가하는 경향이 있습니다. 이 그림은 시퀀스의 마지막 25% 구간에 집중하여, 잘못 정렬된 프레임들을 시각적으로 보여주고, 이러한 정렬 오류가 3D 초해상도 결과에 미치는 영향을 설명합니다. 잘못 정렬된 프레임들이 많을수록 3D 재구성의 정확도가 떨어집니다.</p><details><summary>read the caption</summary>Figure 9: Misalignment Error.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11525/extracted/6087365/figures/appendix_qualitative_blender.jpg alt></figure></p><blockquote><p>🔼 그림 10은 NeRF-synthetic 데이터셋에 대한 정량적 결과를 보여줍니다. 각 이미지 패치에는 GT(Ground Truth)에 대한 PSNR 값이 표시되어 있습니다. 제안된 방법은 특히 고주파수 디테일 측면에서 기존 기준 모델들보다 우수한 결과를 보여줍니다. 이 그림은 제안된 방법이 NeRF-synthetic 데이터셋에서 고해상도 이미지를 생성하는 데 있어 기존 방법들보다 성능이 뛰어나다는 것을 시각적으로 보여줍니다. 특히, 고주파수 성분(세부적인 디테일)을 더 잘 복원하여 더욱 사실적인 이미지를 생성하는 것을 확인할 수 있습니다.</p><details><summary>read the caption</summary>Figure 10: Qualitative results on the NeRF-synthetic dataset. The PSNR values against GT are embedded in each image patch. Ours have shown superior results than the existing baselines, especially for high-frequency details.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:left>S</th><th style=text-align:left>ALS</th></tr></thead><tbody><tr><td style=text-align:left>Chair</td><td style=text-align:left>32.11</td><td style=text-align:left>32.74</td></tr><tr><td style=text-align:left>Drums</td><td style=text-align:left>29.74</td><td style=text-align:left>30.26</td></tr><tr><td style=text-align:left>Ficus</td><td style=text-align:left>35.31</td><td style=text-align:left>35.96</td></tr><tr><td style=text-align:left>Hotdog</td><td style=text-align:left>37.85</td><td style=text-align:left>38.32</td></tr><tr><td style=text-align:left>Lego</td><td style=text-align:left>33.30</td><td style=text-align:left>34.73</td></tr><tr><td style=text-align:left>Materials</td><td style=text-align:left>35.24</td><td style=text-align:left>35.85</td></tr><tr><td style=text-align:left>Mic</td><td style=text-align:left>31.38</td><td style=text-align:left>31.62</td></tr><tr><td style=text-align:left>Ship</td><td style=text-align:left>30.03</td><td style=text-align:left>30.48</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 2는 논문에서 제안된 정렬 알고리즘의 성능을 NeRF 합성 데이터셋에서 비교 분석한 결과를 보여줍니다. 간단한 탐욕적 알고리즘(S)과 적응적 길이 하위 시퀀싱 알고리즘(ALS) 두 가지 방법의 PSNR 값을 다양한 물체(의자, 드럼, 무화과나무, 핫도그, 레고, 재료, 마이크, 배)에 대해 비교하여 각 알고리즘의 장단점과 개선 효과를 보여줍니다. ALS 알고리즘이 전반적으로 더 높은 PSNR 값을 달성하여 향상된 성능을 보임을 알 수 있습니다.</p><details><summary>read the caption</summary>Table 2: The comparison of the proposed ordering algorithms in the NeRF-synthetic dataset.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>PSNR ↑</th><th>SSIM ↑</th><th>LPIPS ↓</th></tr></thead><tbody><tr><td>Bicubic</td><td>27.56</td><td>0.9150</td><td>0.1040</td></tr><tr><td>SwinIR</td><td>30.77</td><td>0.9501</td><td>0.0550</td></tr><tr><td>Render-SR</td><td>28.90</td><td>0.9346</td><td>0.0683</td></tr><tr><td>NeRF-SR</td><td>28.46</td><td>0.9210</td><td>0.0760</td></tr><tr><td>ZS-SRT †</td><td>29.69</td><td>0.9290</td><td>0.0690</td></tr><tr><td>CROP †</td><td>30.71</td><td>0.9459</td><td>0.0671</td></tr><tr><td>FastSR-NeRF †</td><td>30.47</td><td>0.9440</td><td>0.0750</td></tr><tr><td>DiSR-NeRF</td><td>26.00</td><td>0.8898</td><td>0.1226</td></tr><tr><td>SRGS †</td><td>30.83</td><td>0.9480</td><td>0.0560</td></tr><tr><td>GaussianSR †</td><td>28.37</td><td>0.9240</td><td>0.0870</td></tr><tr><td>SuperGaussian †</td><td>28.44</td><td>0.9459</td><td>0.0670</td></tr><tr><td>Ours-ALS</td><td><strong>31.41</strong></td><td><strong>0.9520</strong></td><td><strong>0.0540</strong></td></tr><tr><td>3DGS-HR</td><td>33.31</td><td>0.9695</td><td>0.0303</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 3은 Blender 데이터셋에서 다양한 3D 초고해상도화 기법들을 비교 분석한 결과를 보여줍니다. 가로 해상도를 4배에서 1배로 축소한(×4→×1) 저해상도 이미지를 사용하여 초고해상도 3D 모델을 생성하는 다양한 기법들의 성능을 비교합니다. 표에는 PSNR, SSIM, LPIPS 지표를 사용하여 정량적으로 평가한 결과가 제시되어 있습니다. 일부 기법의 경우, 코드가 공개되지 않아 논문에 제시된 수치를 가져왔음을 † 표시로 나타냅니다. 즉, 본 연구에서 직접 재현한 결과가 아닌 기존 논문의 결과를 인용한 것임을 의미합니다.</p><details><summary>read the caption</summary>Table 3: Comparison of different methods for 3D super-resolution (×4→×1\times 4\rightarrow\times 1× 4 → × 1) in Blender Dataset. The numbers marked with † are sourced from their respective paper, as the code is not available at this time.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:left>VRT</th><th style=text-align:left>IART</th><th style=text-align:left>PSRT</th></tr></thead><tbody><tr><td style=text-align:left>SISR</td><td style=text-align:left>PSNR ↑ 31.20, SSIM ↑ 0.9497, LPIPS ↓ 0.0567</td><td style=text-align:left>PSNR ↑ 31.10, SSIM ↑ 0.9484, LPIPS ↓ 0.0590</td><td style=text-align:left>PSNR ↑ 31.10, SSIM ↑ 0.9516, LPIPS ↓ 0.0543</td></tr><tr><td style=text-align:left>S</td><td style=text-align:left>PSNR ↑ 31.25, SSIM ↑ 0.9505, LPIPS ↓ 0.0557</td><td style=text-align:left>PSNR ↑ 31.32, SSIM ↑ 0.9513, LPIPS ↓ 0.0550</td><td style=text-align:left>PSNR ↑ 31.35, SSIM ↑ 0.9513, LPIPS ↓ 0.0548</td></tr><tr><td style=text-align:left>ALS</td><td style=text-align:left>PSNR ↑ 31.37, SSIM ↑ 0.9516, LPIPS ↓ 0.0544</td><td style=text-align:left>PSNR ↑ 31.35, SSIM ↑ 0.9514, LPIPS ↓ 0.0548</td><td style=text-align:left>PSNR ↑ 31.41, SSIM ↑ 0.9520, LPIPS ↓ 0.0540</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 4는 다양한 VSR(Video Super-Resolution) 모델을 사용하여 Blender 데이터셋(해상도 4배 축소)에 대해 수행한 비교 실험 결과를 보여줍니다. SISR(Single-Image Super-Resolution)은 단일 이미지에 VSR을 적용하는 방식이고, S는 단순 탐욕 알고리즘(순서: 특징)을 사용하여 이미지 순서를 정렬하는 방식이며, ALS는 다중 임계값을 사용하여 특징을 기반으로 적응적 길이 부분 시퀀스를 생성하는 방식입니다. 각 방식에 대한 PSNR, SSIM, LPIPS 값을 비교하여 성능을 분석합니다.</p><details><summary>read the caption</summary>Table 4: Ablation comparison of Blender dataset (×4→×1\times 4\rightarrow\times 1× 4 → × 1) on various VSR models. SISR refers to Single-Image Super-Resolution (single image VSR), S refers to ordering by simple greedy algorithm (order: feature), and ALS refers to using adaptive-length subsequence (order: feature) with multi-threshold (threshold: pose).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Metric</th><th>PSNR ↑</th><th>SSIM ↑</th><th>LPIPS ↓</th></tr></thead><tbody><tr><td>S (last 25%)</td><td>31.32</td><td>0.9511</td><td>0.0552</td></tr><tr><td>ALS</td><td>31.41</td><td>0.9520</td><td>0.0540</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 5는 정렬 오류가 3D 초고해상도 결과에 미치는 영향을 보여줍니다. 간단히 말해, 제안된 적응형 길이 하위 시퀀싱 알고리즘(ALS)을 사용하면 단순 탐욕적 알고리즘(S)보다 3D 초고해상도 결과가 더 향상됩니다. 특히, 시퀀스의 마지막 25%에서 정렬 오류가 발생할 가능성이 높은데, ALS는 이러한 오류에 덜 민감합니다. 이는 ALS가 이미지 시퀀스 내에서 일관성 있는 시각적 흐름을 더 잘 유지하기 때문입니다. PSNR, SSIM, LPIPS 지표를 사용하여 정량적으로 비교 분석하였습니다.</p><details><summary>read the caption</summary>Table 5: Impact of misalignment on 3D super-resolution.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>chair</th><th>drums</th><th>ficus</th><th>hotdog</th><th>lego</th><th>materials</th><th>mic</th><th>ship</th><th>average</th></tr></thead><tbody><tr><td>Bicubic</td><td>29.02</td><td>23.75</td><td>28.24</td><td>31.86</td><td>27.46</td><td>26.47</td><td>27.97</td><td>25.71</td><td>27.56</td></tr><tr><td>PSRT (SISR)</td><td>30.94</td><td>25.56</td><td>33.49</td><td>35.82</td><td>32.20</td><td>30.06</td><td>31.75</td><td>28.96</td><td>31.10</td></tr><tr><td>SwinIR+3DGS</td><td>31.02</td><td>25.48</td><td>32.49</td><td>35.60</td><td>32.05</td><td>29.58</td><td>31.75</td><td>28.20</td><td>30.77</td></tr><tr><td>Render-SR</td><td>30.23</td><td>24.04</td><td>28.63</td><td>33.78</td><td>29.23</td><td>27.34</td><td>30.53</td><td>27.35</td><td>28.90</td></tr><tr><td>NeRF-SR</td><td>30.16</td><td>23.46</td><td>26.64</td><td>34.40</td><td>29.13</td><td>28.02</td><td>27.25</td><td>26.61</td><td>28.21</td></tr><tr><td>DiSR-NeRF</td><td>27.55</td><td>22.63</td><td>25.64</td><td>30.07</td><td>26.43</td><td>24.71</td><td>26.49</td><td>24.47</td><td>26.00</td></tr><tr><td>CROP<sup>†</sup></td><td>31.53</td><td>24.99</td><td>31.50</td><td>35.62</td><td>32.88</td><td>29.16</td><td>31.76</td><td>28.23</td><td>30.71</td></tr><tr><td>Ours-S</td><td>31.33</td><td>25.58</td><td>33.71</td><td>35.95</td><td>32.98</td><td>30.09</td><td>31.91</td><td>29.26</td><td>31.35</td></tr><tr><td>Ours-ALS</td><td>31.36</td><td>25.65</td><td>33.69</td><td>36.18</td><td>33.03</td><td>30.17</td><td>31.93</td><td>29.26</td><td>31.41</td></tr><tr><td>HR-3DGS</td><td>35.79</td><td>26.14</td><td>34.84</td><td>37.72</td><td>35.77</td><td>29.97</td><td>35.36</td><td>30.89</td><td>33.31</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 6은 합성 Blender 데이터셋에서 개체별 PSNR 비교 결과를 보여줍니다. 입력 이미지의 해상도를 4배 축소한 후 (×4 → ×1), 다양한 방법으로 초해상도를 적용하여 생성된 3D 모델의 화질을 평가했습니다. &lsquo;Ours-ALS&rsquo;는 제안된 적응적 길이 부분 시퀀스 기법을 사용한 결과를 나타냅니다. 각 개체(의자, 드럼, 무화과나무 등)에 대한 PSNR 값을 비교하여 제안된 방법의 성능을 다른 기존 방법들과 비교 분석했습니다.</p><details><summary>read the caption</summary>Table 6: Per-object PSNR comparison on the synthetic Blender dataset (×4absent4\times 4× 4 →→\rightarrow→ ×1absent1\times 1× 1). Ours-ALS refers to our method using adaptive-length subsequencing (ALS).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>chair</th><th>drums</th><th>ficus</th><th>hotdog</th><th>lego</th><th>materials</th><th>mic</th><th>ship</th><th>average</th></tr></thead><tbody><tr><td>Bicubic</td><td>0.9194</td><td>0.9003</td><td>0.9430</td><td>0.9526</td><td>0.9059</td><td>0.9220</td><td>0.9481</td><td>0.8291</td><td>0.9150</td></tr><tr><td>PSRT (SISR)</td><td>0.9475</td><td>0.9386</td><td>0.9762</td><td>0.9721</td><td>0.9572</td><td>0.9544</td><td>0.9732</td><td>0.8688</td><td>0.9516</td></tr><tr><td>SwinIR+3DGS</td><td>0.9469</td><td>0.9412</td><td>0.9760</td><td>0.9728</td><td>0.9601</td><td>0.9558</td><td>0.9747</td><td>0.8731</td><td>0.9501</td></tr><tr><td>Render-SR</td><td>0.9432</td><td>0.9163</td><td>0.9539</td><td>0.9677</td><td>0.9379</td><td>0.9322</td><td>0.9671</td><td>0.8582</td><td>0.9346</td></tr><tr><td>NeRF-SR</td><td>0.9366</td><td>0.9019</td><td>0.9026</td><td>0.9629</td><td>0.9292</td><td>0.9319</td><td>0.9432</td><td>0.8357</td><td>0.9180</td></tr><tr><td>DiSR-NeRF</td><td>0.9035</td><td>0.8618</td><td>0.9117</td><td>0.9332</td><td>0.8875</td><td>0.8816</td><td>0.9335</td><td>0.8053</td><td>0.8898</td></tr><tr><td>CROP<sup>†</sup></td><td>0.9513</td><td>0.9236</td><td>0.9709</td><td>0.9725</td><td>0.9641</td><td>0.9468</td><td>0.9740</td><td>0.8637</td><td>0.9459</td></tr><tr><td>Ours-S</td><td>0.9538</td><td>0.9391</td><td>0.9779</td><td>0.9738</td><td>0.9646</td><td>0.9541</td><td>0.9747</td><td>0.8724</td><td>0.9513</td></tr><tr><td>Ours-ALS</td><td>0.9539</td><td>0.9405</td><td>0.9777</td><td>0.9744</td><td>0.9649</td><td>0.9555</td><td>0.9750</td><td>0.8741</td><td>0.9520</td></tr><tr><td>HR-3DGS</td><td>0.9874</td><td>0.9544</td><td>0.9872</td><td>0.9853</td><td>0.9828</td><td>0.9603</td><td>0.9914</td><td>0.9067</td><td>0.9694</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 합성 Blender 데이터셋에서 개체별 SSIM 비교 결과를 보여줍니다. 입력 저해상도 이미지를 4배 업샘플링(4x → 1x)한 결과를 보여주며, 제안된 방법인 적응적 길이 하위 시퀀싱(ALS)을 사용한 결과와 기준 모델들(Bicubic, PSRT (SISR), SwinIR+3DGS, Render-SR, NeRF-SR, DiSR-NeRF, CROP)의 결과를 비교합니다. 각 개체(의자, 드럼, 무화과나무, 핫도그, 레고, 재료, 마이크, 배)에 대한 SSIM 값을 제시하여 제안된 방법의 성능을 다각적으로 평가합니다. HR-3DGS는 정답 고해상도 이미지를 사용한 3DGS 모델의 결과를 나타냅니다.</p><details><summary>read the caption</summary>Table 7: Per-object SSIM comparison on the synthetic Blender dataset (×4absent4\times 4× 4 →→\rightarrow→ ×1absent1\times 1× 1). Ours-ALS refers to our method using adaptive-length subsequencing (ALS).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:left>chair</th><th style=text-align:left>drums</th><th style=text-align:left>ficus</th><th style=text-align:left>hotdog</th><th style=text-align:left>lego</th><th style=text-align:left>materials</th><th style=text-align:left>mic</th><th style=text-align:left>ship</th><th style=text-align:left>average</th></tr></thead><tbody><tr><td style=text-align:left>Bicubic</td><td style=text-align:left>0.0899</td><td style=text-align:left>0.1106</td><td style=text-align:left>0.0619</td><td style=text-align:left>0.0768</td><td style=text-align:left>0.1272</td><td style=text-align:left>0.0892</td><td style=text-align:left>0.0626</td><td style=text-align:left>0.2136</td><td style=text-align:left>0.1040</td></tr><tr><td style=text-align:left>PSRT (SISR)</td><td style=text-align:left>0.0553</td><td style=text-align:left>0.0609</td><td style=text-align:left>0.0237</td><td style=text-align:left>0.0421</td><td style=text-align:left>0.0595</td><td style=text-align:left>0.0480</td><td style=text-align:left>0.0254</td><td style=text-align:left>0.1567</td><td style=text-align:left>0.0544</td></tr><tr><td style=text-align:left>SwinIR+3DGS</td><td style=text-align:left>0.0577</td><td style=text-align:left>0.0565</td><td style=text-align:left>0.0221</td><td style=text-align:left>0.0401</td><td style=text-align:left>0.0498</td><td style=text-align:left>0.0420</td><td style=text-align:left>0.0203</td><td style=text-align:left>0.1511</td><td style=text-align:left>0.0550</td></tr><tr><td style=text-align:left>Render-SR</td><td style=text-align:left>0.0563</td><td style=text-align:left>0.0743</td><td style=text-align:left>0.0396</td><td style=text-align:left>0.0462</td><td style=text-align:left>0.0691</td><td style=text-align:left>0.0597</td><td style=text-align:left>0.0312</td><td style=text-align:left>0.1698</td><td style=text-align:left>0.0683</td></tr><tr><td style=text-align:left>NeRF-SR</td><td style=text-align:left>0.0687</td><td style=text-align:left>0.1091</td><td style=text-align:left>0.1014</td><td style=text-align:left>0.0591</td><td style=text-align:left>0.0976</td><td style=text-align:left>0.0770</td><td style=text-align:left>0.0805</td><td style=text-align:left>0.1984</td><td style=text-align:left>0.0990</td></tr><tr><td style=text-align:left>DiSR-NeRF</td><td style=text-align:left>0.0943</td><td style=text-align:left>0.1429</td><td style=text-align:left>0.0905</td><td style=text-align:left>0.1001</td><td style=text-align:left>0.1378</td><td style=text-align:left>0.1293</td><td style=text-align:left>0.0751</td><td style=text-align:left>0.2106</td><td style=text-align:left>0.1226</td></tr><tr><td style=text-align:left>CROP<sup>†</sup></td><td style=text-align:left>0.0567</td><td style=text-align:left>0.0856</td><td style=text-align:left>0.0317</td><td style=text-align:left>0.0481</td><td style=text-align:left>0.0496</td><td style=text-align:left>0.0622</td><td style=text-align:left>0.0251</td><td style=text-align:left>0.1776</td><td style=text-align:left>0.0671</td></tr><tr><td style=text-align:left>Ours-S</td><td style=text-align:left>0.0478</td><td style=text-align:left>0.0585</td><td style=text-align:left>0.0216</td><td style=text-align:left>0.0395</td><td style=text-align:left>0.0470</td><td style=text-align:left>0.0488</td><td style=text-align:left>0.0240</td><td style=text-align:left>0.1509</td><td style=text-align:left>0.0547</td></tr><tr><td style=text-align:left>Ours-ALS</td><td style=text-align:left>0.0478</td><td style=text-align:left>0.0576</td><td style=text-align:left>0.0216</td><td style=text-align:left>0.0388</td><td style=text-align:left>0.0465</td><td style=text-align:left>0.0464</td><td style=text-align:left>0.0233</td><td style=text-align:left>0.1501</td><td style=text-align:left>0.0540</td></tr><tr><td style=text-align:left>HR-3DGS</td><td style=text-align:left>0.0117</td><td style=text-align:left>0.0371</td><td style=text-align:left>0.0116</td><td style=text-align:left>0.0199</td><td style=text-align:left>0.0154</td><td style=text-align:left>0.0341</td><td style=text-align:left>0.0060</td><td style=text-align:left>0.1063</td><td style=text-align:left>0.0303</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 8은 합성 Blender 데이터셋(4배 축소 → 1배)에서 개체별 LPIPS 비교 결과를 보여줍니다. 각 개체(의자, 드럼, 무화과나무, 핫도그, 레고, 재료, 마이크, 배)에 대해 여러 기준 모델(Bicubic, PSRT(SISR), SwinIR+3DGS, Render-SR, NeRF-SR, DiSR-NeRF, CROP, Ours-S, Ours-ALS, HR-3DGS)의 LPIPS 값을 비교하여 제시합니다. Ours-ALS는 본 논문에서 제안하는 적응형 길이 하위 시퀀싱(ALS) 기법을 사용한 방법을 나타냅니다. LPIPS 값이 낮을수록 이미지의 품질이 더 좋음을 의미합니다. 이 표를 통해 제안된 방법이 다른 기준 모델에 비해 LPIPS 값이 낮아, 이미지 품질이 우수함을 확인할 수 있습니다.</p><details><summary>read the caption</summary>Table 8: Per-object LPIPS comparison on the synthetic Blender dataset (×4absent4\times 4× 4 →→\rightarrow→ ×1absent1\times 1× 1). Ours-ALS refers to our method using adaptive-length subsequencing (ALS).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>bicycle</th><th>flowers</th><th>garden</th><th>stump</th><th>treehill</th><th>room</th><th>counter</th><th>kitchen</th><th>bonsai</th><th>average</th></tr></thead><tbody><tr><td>Bicubic</td><td>24.02</td><td>21.24</td><td>25.14</td><td>26.30</td><td>22.25</td><td>30.47</td><td>28.15</td><td>28.23</td><td>30.21</td><td>26.22</td></tr><tr><td>SwinIR + 3DGS</td><td>24.54</td><td>21.18</td><td>25.81</td><td>26.38</td><td>22.16</td><td>31.30</td><td>28.71</td><td>29.82</td><td>31.26</td><td>26.80</td></tr><tr><td>Ours-S</td><td>24.42</td><td>21.13</td><td>26.04</td><td>26.40</td><td>22.26</td><td>31.47</td><td>28.96</td><td>30.79</td><td>31.69</td><td>27.02</td></tr><tr><td>Ours-ALS</td><td>24.50</td><td>21.17</td><td>25.99</td><td>26.46</td><td>22.26</td><td>31.52</td><td>28.90</td><td>30.73</td><td>31.68</td><td>27.02</td></tr><tr><td>HR-3DGS</td><td>24.41</td><td>20.59</td><td>26.58</td><td>26.28</td><td>22.27</td><td>31.52</td><td>29.12</td><td>31.57</td><td>32.36</td><td>27.19</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 9는 Mip-NeRF 360 데이터셋에서의 8배 저해상도 이미지를 2배 고해상도 이미지로 초해상도 처리한 결과에 대한 각 장면별 PSNR(Peak Signal-to-Noise Ratio) 비교 결과를 보여줍니다. 저해상도 이미지에서 고해상도 이미지로의 변환 비율은 8배에서 2배입니다. 본 논문에서 제안하는 적응적 길이 하위 시퀀싱 (ALS) 방법을 사용한 결과(&lsquo;Ours-ALS&rsquo;)와 기타 기준 모델(Bicubic, SwinIR + 3DGS, Ours-S, HR-3DGS)의 성능을 비교하여 제안된 방법의 효과를 보여줍니다. 각 장면(bicycle, flowers, garden, stump, treehill, room, counter, kitchen, bonsai)에 대한 PSNR 값을 제시하며 평균 PSNR 값도 함께 제공합니다.</p><details><summary>read the caption</summary>Table 9: Per-scene PSNR comparison on the Mip-NeRF 360 dataset (×8absent8\times 8× 8 →→\rightarrow→×2absent2\times 2× 2). Ours-ALS refers to our method using adaptive-length subsequencing (ALS).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:left>bicycle</th><th style=text-align:left>flowers</th><th style=text-align:left>garden</th><th style=text-align:left>stump</th><th style=text-align:left>treehill</th><th style=text-align:left>room</th><th style=text-align:left>counter</th><th style=text-align:left>kitchen</th><th style=text-align:left>bonsai</th><th style=text-align:left>average</th></tr></thead><tbody><tr><td style=text-align:left>Bicubic</td><td style=text-align:left>0.6401</td><td style=text-align:left>0.5321</td><td style=text-align:left>0.6648</td><td style=text-align:left>0.7324</td><td style=text-align:left>0.5880</td><td style=text-align:left>0.8877</td><td style=text-align:left>0.8573</td><td style=text-align:left>0.8128</td><td style=text-align:left>0.8980</td><td style=text-align:left>0.7348</td></tr><tr><td style=text-align:left>SwinIR + 3DGS</td><td style=text-align:left>0.6810</td><td style=text-align:left>0.5498</td><td style=text-align:left>0.7259</td><td style=text-align:left>0.7468</td><td style=text-align:left>0.6020</td><td style=text-align:left>0.9063</td><td style=text-align:left>0.8837</td><td style=text-align:left>0.8724</td><td style=text-align:left>0.9235</td><td style=text-align:left>0.7657</td></tr><tr><td style=text-align:left>Ours-S</td><td style=text-align:left>0.6752</td><td style=text-align:left>0.5512</td><td style=text-align:left>0.7476</td><td style=text-align:left>0.7481</td><td style=text-align:left>0.6048</td><td style=text-align:left>0.9123</td><td style=text-align:left>0.8936</td><td style=text-align:left>0.9071</td><td style=text-align:left>0.9328</td><td style=text-align:left>0.7747</td></tr><tr><td style=text-align:left>Ours-ALS</td><td style=text-align:left>0.6783</td><td style=text-align:left>0.5503</td><td style=text-align:left>0.7462</td><td style=text-align:left>0.7467</td><td style=text-align:left>0.6028</td><td style=text-align:left>0.9123</td><td style=text-align:left>0.8918</td><td style=text-align:left>0.9062</td><td style=text-align:left>0.9323</td><td style=text-align:left>0.7741</td></tr><tr><td style=text-align:left>HR-3DGS</td><td style=text-align:left>0.7007</td><td style=text-align:left>0.5445</td><td style=text-align:left>0.8173</td><td style=text-align:left>0.7571</td><td style=text-align:left>0.6269</td><td style=text-align:left>0.9263</td><td style=text-align:left>0.9144</td><td style=text-align:left>0.9325</td><td style=text-align:left>0.9465</td><td style=text-align:left>0.7962</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 10은 Mip-NeRF 360 데이터셋에서의 8배 축소된 이미지를 2배로 초해상도 처리한 결과에 대한 장면별 구조 유사성 지표(SSIM) 비교를 보여줍니다. 각 장면(자전거, 꽃, 정원, 그루터기, 언덕, 방, 카운터, 주방, 분재)에 대한 SSIM 값을 보여주며, 제안된 방법(Ours-ALS)을 포함한 다양한 기준 모델들(Bicubic, SwinIR + 3DGS, Ours-S, HR-3DGS)과 비교 분석합니다. Ours-ALS는 제안된 적응형 길이 하위 시퀀스 기법을 사용한 결과를 나타냅니다.</p><details><summary>read the caption</summary>Table 10: Per-scene SSIM comparison on the Mip-NeRF 360 dataset (×8absent8\times 8× 8 →→\rightarrow→×2absent2\times 2× 2). Ours-ALS refers to our method using adaptive-length subsequencing (ALS).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:center>bicycle</th><th style=text-align:center>flowers</th><th style=text-align:center>garden</th><th style=text-align:center>stump</th><th style=text-align:center>treehill</th><th style=text-align:center>room</th><th style=text-align:center>counter</th><th style=text-align:center>kitchen</th><th style=text-align:center>bonsai</th><th style=text-align:center>average</th></tr></thead><tbody><tr><td style=text-align:left>Bicubic</td><td style=text-align:center>0.3688</td><td style=text-align:center>0.4315</td><td style=text-align:center>0.3469</td><td style=text-align:center>0.3334</td><td style=text-align:center>0.4391</td><td style=text-align:center>0.2750</td><td style=text-align:center>0.2671</td><td style=text-align:center>0.2598</td><td style=text-align:center>0.2392</td><td style=text-align:center>0.3290</td></tr><tr><td style=text-align:left>SwinIR + 3DGS</td><td style=text-align:center>0.3220</td><td style=text-align:center>0.4065</td><td style=text-align:center>0.2784</td><td style=text-align:center>0.3098</td><td style=text-align:center>0.4116</td><td style=text-align:center>0.2354</td><td style=text-align:center>0.2216</td><td style=text-align:center>0.1973</td><td style=text-align:center>0.2035</td><td style=text-align:center>0.2873</td></tr><tr><td style=text-align:left>Ours-S</td><td style=text-align:center>0.3344</td><td style=text-align:center>0.4091</td><td style=text-align:center>0.2613</td><td style=text-align:center>0.3142</td><td style=text-align:center>0.4162</td><td style=text-align:center>0.2218</td><td style=text-align:center>0.2074</td><td style=text-align:center>0.1536</td><td style=text-align:center>0.1927</td><td style=text-align:center>0.2790</td></tr><tr><td style=text-align:left>Ours-ALS</td><td style=text-align:center>0.3261</td><td style=text-align:center>0.4062</td><td style=text-align:center>0.2607</td><td style=text-align:center>0.3117</td><td style=text-align:center>0.4134</td><td style=text-align:center>0.2218</td><td style=text-align:center>0.2104</td><td style=text-align:center>0.1542</td><td style=text-align:center>0.1925</td><td style=text-align:center>0.2774</td></tr><tr><td style=text-align:left>HR-3DGS</td><td style=text-align:center>0.3230</td><td style=text-align:center>0.4188</td><td style=text-align:center>0.1777</td><td style=text-align:center>0.3130</td><td style=text-align:center>0.3997</td><td style=text-align:center>0.1931</td><td style=text-align:center>0.1800</td><td style=text-align:center>0.1136</td><td style=text-align:center>0.1758</td><td style=text-align:center>0.2550</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 11은 Mip-NeRF 360 데이터셋에서의 8배 저해상도에서 2배 고해상도로의 슈퍼 해상도 결과에 대한 LPIPS(Learned Perceptual Image Patch Similarity) 비교 결과를 보여줍니다. LPIPS는 이미지의 지각적 유사성을 측정하는 지표로, 낮을수록 더 높은 유사성을 의미합니다. 이 표는 제안된 방법(Ours-ALS)을 포함한 여러 비교 대상 방법들에 대한 각 장면별 LPIPS 값을 제시합니다. Ours-ALS는 본 논문에서 제안된 적응적 길이 하위 시퀀스 기법을 사용한 방법입니다. 표를 통해 각 방법의 주관적인 이미지 품질 차이를 수치적으로 비교 분석할 수 있습니다.</p><details><summary>read the caption</summary>Table 11: Per-scene LPIPS comparison on the Mip-NeRF 360 dataset (×8absent8\times 8× 8 →→\rightarrow→×2absent2\times 2× 2). Ours-ALS refers to our method using adaptive-length subsequencing (ALS).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>PSNR ↑</th><th>SSIM ↑</th><th>LPIPS ↓</th></tr></thead><tbody><tr><td>Bicubic</td><td>26.22</td><td>0.7349</td><td>0.3290</td></tr><tr><td>SwinIR</td><td>26.80</td><td>0.7657</td><td>0.2873</td></tr><tr><td>SRGS<sup>†</sup></td><td>26.88</td><td>0.7670</td><td>0.2860</td></tr><tr><td>Ours</td><td><strong>27.02</strong></td><td><strong>0.7747</strong></td><td><strong>0.2790</strong></td></tr><tr><td>3DGS-HR</td><td>27.19</td><td>0.7710</td><td>0.2802</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 12는 Mip-NeRF 360 데이터셋에서 8배 다운샘플링된 이미지를 2배로 업샘플링하는 다양한 기준 모델들과 제안된 방법의 성능 비교 결과를 보여줍니다. PSNR, SSIM, LPIPS 세 가지 지표를 사용하여 정량적으로 비교 분석하였습니다. 이 표는 제안된 방법의 성능 우수성을 보여주는 실험 결과의 일부분입니다.</p><details><summary>read the caption</summary>Table 12: Comparison with baseline models in Mip-NeRF 360 dataset (×8absent8\times 8× 8 →→\rightarrow→ ×2absent2\times 2× 2).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>FVD↓</th><th>PSNR↑</th></tr></thead><tbody><tr><td>Bicubic</td><td>195</td><td>27.56</td></tr><tr><td>SwinIR</td><td>113</td><td>30.77</td></tr><tr><td>Render-SR</td><td>134</td><td>28.90</td></tr><tr><td>NeRF-SR</td><td>169</td><td>28.21</td></tr><tr><td>DiSR-NeRF</td><td>304</td><td>26.00</td></tr><tr><td>Ours-S</td><td>110</td><td>31.35</td></tr><tr><td>Ours-ALS</td><td><strong>109</strong></td><td><strong>31.41</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 표 13은 Blender 데이터셋에서 제안된 방법의 시간적 일관성 및 공간적 화질 지표를 보여줍니다. 시간적 일관성은 비디오 프레임 간의 부드러운 전환을 나타내는 반면, 공간적 화질은 재구성된 3D 모델의 시각적 선명도를 나타냅니다. 다양한 기준 모델(Bicubic, SwinIR, Render-SR, NeRF-SR, DiSR-NeRF, Ours-S, Ours-ALS)과의 비교를 통해 제안된 방법의 우수성을 보여줍니다. 특히, Ours-ALS는 FVD(Fréchet Video Distance) 지표에서 가장 낮은 값을 기록하여, 시간적 일관성이 가장 뛰어남을 보여줍니다. PSNR(Peak Signal-to-Noise Ratio) 지표 또한 높은 값을 나타내어, 공간적 화질 또한 우수함을 보여줍니다.</p><details><summary>read the caption</summary>Table 13: Temporal Consistency and Spatial Quality Metrics on Blender Dataset.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-4f4ba3a847301e4bd25f1261291faad3 class=gallery><img src=paper_images/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/14.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/&amp;title=Sequence%20Matters:%20Harnessing%20Video%20Models%20in%203D%20Super-Resolution" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/&amp;text=Sequence%20Matters:%20Harnessing%20Video%20Models%20in%203D%20Super-Resolution" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/&amp;subject=Sequence%20Matters:%20Harnessing%20Video%20Models%20in%203D%20Super-Resolution" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.11525/index.md",oid_likes="likes_paper-reviews/2412.11525/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.11605/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-16T00:00:00+00:00>16 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.12094/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-16T00:00:00+00:00>16 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>