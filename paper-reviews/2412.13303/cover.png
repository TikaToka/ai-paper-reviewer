<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>FastVLM: Efficient Vision Encoding for Vision Language Models</title>
<!--Generated on Tue Dec 17 19:44:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2412.13303v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S1" title="In FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S2" title="In FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3" title="In FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.SS1" title="In 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>FastViT as VLM Image Encoder</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.SS1.SSS1" title="In 3.1 FastViT as VLM Image Encoder ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Multi-Scale Features</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.SS2" title="In 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>FastViTHD: High Resolution Encoder for VLM</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.SS2.SSS1" title="In 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Vision Encoder - Language Decoder Interplay</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.SS2.SSS2" title="In 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Static vs. Dynamic Input Resolution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.SS2.SSS3" title="In 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Comparison with Token Pruning &amp; Downsampling</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4" title="In FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4.SS1" title="In 4 Experiments ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Comparison with state-of-the-art</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S5" title="In FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S1a" title="In FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Training Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S2a" title="In FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Architecture Details</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S2.SS1" title="In B Architecture Details ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Naive Scaling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3a" title="In FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4a" title="In FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4.SS1a" title="In D Datasets ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>Pretraining Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4.SS2" title="In D Datasets ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2 </span>Visual Instruction Tuning Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4.SS3" title="In D Datasets ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.3 </span>Evaluations</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FastVLM: Efficient Vision Encoding for Vision Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pavan Kumar Anasosalu Vasu<sup class="ltx_sup" id="id15.3.id1">⋆</sup><sup class="ltx_sup" id="id16.4.id2">†</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fartash Faghri<sup class="ltx_sup" id="id17.2.id1">⋆</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chun-Liang Li<sup class="ltx_sup" id="id18.2.id1">⋆</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cem Koc<sup class="ltx_sup" id="id19.2.id1">⋆</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nate True
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Albert Antony
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gokul Santhanam
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">James Gabriel
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peter Grasch
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Oncel Tuzel<sup class="ltx_sup" id="id20.2.id1">⋆</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hadi Pouransari<sup class="ltx_sup" id="id21.3.id1">⋆</sup><sup class="ltx_sup" id="id22.4.id2">†</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Apple 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id23.3.id1" style="font-size:90%;">{panasosaluvasu,fartash,chunliang_li,cem_koc,otuzel,mpouransari}@apple.com</span>
<br class="ltx_break"/><sup class="ltx_sup" id="id24.4.id2"><span class="ltx_text" id="id24.4.id2.1" style="font-size:90%;">⋆</span></sup><span class="ltx_text" id="id10.2.1" style="font-size:90%;">Core authors; <sup class="ltx_sup" id="id10.2.1.1">†</sup>Project lead
</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id14.4">Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM—a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2<math alttext="\times" class="ltx_Math" display="inline" id="id11.1.m1.1"><semantics id="id11.1.m1.1a"><mo id="id11.1.m1.1.1" xref="id11.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id11.1.m1.1b"><times id="id11.1.m1.1.1.cmml" xref="id11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id11.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id11.1.m1.1d">×</annotation></semantics></math> improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152<math alttext="\times" class="ltx_Math" display="inline" id="id12.2.m2.1"><semantics id="id12.2.m2.1a"><mo id="id12.2.m2.1.1" xref="id12.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id12.2.m2.1b"><times id="id12.2.m2.1.1.cmml" xref="id12.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id12.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id12.2.m2.1d">×</annotation></semantics></math>1152), FastVLM achieves comparable performance on key benchmarks like SeedBench and MMMU, using the same 0.5B LLM, but with 85<math alttext="\times" class="ltx_Math" display="inline" id="id13.3.m3.1"><semantics id="id13.3.m3.1a"><mo id="id13.3.m3.1.1" xref="id13.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id13.3.m3.1b"><times id="id13.3.m3.1.1.cmml" xref="id13.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id13.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id13.3.m3.1d">×</annotation></semantics></math> faster TTFT and a vision encoder that is 3.4<math alttext="\times" class="ltx_Math" display="inline" id="id14.4.m4.1"><semantics id="id14.4.m4.1a"><mo id="id14.4.m4.1.1" xref="id14.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id14.4.m4.1b"><times id="id14.4.m4.1.1.cmml" xref="id14.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id14.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id14.4.m4.1d">×</annotation></semantics></math> smaller.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_missing ltx_missing_image" id="S1.F1.sf1.g1" src=""/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S1.F1.sf1.3.2" style="font-size:90%;">Qwen2-0.5B</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_missing ltx_missing_image" id="S1.F1.sf2.g1" src=""/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S1.F1.sf2.3.2" style="font-size:90%;">Vicuna-7B</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.6.3.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F1.4.2" style="font-size:90%;">FastVLM is more than 3<math alttext="\times" class="ltx_Math" display="inline" id="S1.F1.3.1.m1.1"><semantics id="S1.F1.3.1.m1.1b"><mo id="S1.F1.3.1.m1.1.1" xref="S1.F1.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.F1.3.1.m1.1c"><times id="S1.F1.3.1.m1.1.1.cmml" xref="S1.F1.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.1.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S1.F1.3.1.m1.1e">×</annotation></semantics></math> faster than prior work.<span class="ltx_text ltx_font_medium" id="S1.F1.4.2.1"> Comparison of commonly used vision encoders for VLMs with (a) Qwen2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite> 0.5B LLM and (b) Vicuna 7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite> LLM. All the vision encoders are CLIP <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite> pretrained. For a fair comparison all models are trained using LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> setup with the vision encoders made trainable for resolution adaptation, see <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4" title="4 Experiments ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a> for more details. Marker size for each model corresponds to number of parameters of the vision encoder. The <math alttext="x" class="ltx_Math" display="inline" id="S1.F1.4.2.1.m1.1"><semantics id="S1.F1.4.2.1.m1.1b"><mi id="S1.F1.4.2.1.m1.1.1" xref="S1.F1.4.2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S1.F1.4.2.1.m1.1c"><ci id="S1.F1.4.2.1.m1.1.1.cmml" xref="S1.F1.4.2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.2.1.m1.1d">x</annotation><annotation encoding="application/x-llamapun" id="S1.F1.4.2.1.m1.1e">italic_x</annotation></semantics></math>-axis is the sum of vision encoder latency and LLM prefilling time. All models are benchmarked on an M1 Macbook Pro. </span></span></figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Vision Language Models (VLMs) enable visual understanding alongside textual inputs. VLMs are often built by passing visual tokens from a pretrained vision backbone to a pretrained Large Language Model (LLM) through a projection layer (also known as the connector module). Previous works <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> have explored various training and fine-tuning strategies for these three components: the vision backbone, the adapter, and the LLM, which is typically a decoder-only model.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Several studies <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">56</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> highlight image resolution as a key factor in VLM performance, especially for text- and chart-rich data. However, increasing image resolution presents multiple challenges. First, pretrained vision encoders may not support high-resolution images, as this would make pretraining inefficient. To address this, one approach is to continuously pretrain the vision backbone to adapt it for high resolutions <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. Alternatively, tiling strategies, such as Sphinx <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>, S2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a>]</cite>, and AnyRes <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite>, divide images into subregions, with each subregion processed independently by the backbone. This approach is particularly suitable for ViT-based backbones, which cannot accept varying input resolutions.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">A further challenge is the runtime computational cost associated with high-resolution inference. Both single high-resolution inference and multiple inferences at lower resolution (the tiling strategy) result in significant latency when generating visual tokens. Additionally, high-resolution images naturally produce more tokens, which increases the LLM prefilling time (the LLM forward pass time on all tokens in the context, including visual tokens), thereby further increasing the time-to-first-token (TTFT), which is the sum of the vision encoder latency and the LLM prefilling time.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we study VLM design and training from a runtime efficiency perspective motivated by their on-device deployment. We explore the optimization landscape as image resolution increases, aiming to improve accuracy-latency trade-off, where latency includes both the vision encoder inference time and the LLM prefilling time. Using extensive experiments with different LLM sizes and resolutions, we establish the Pareto optimal curve for a specific vision backbone, showing the best accuracy achievable within a given runtime budget (TTFT) based on different choices of resolution and LLM size.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.4">We start by exploring the use of a hybrid convolutional-transformer architecture FastViT <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite>, pretrained with MobileCLIP <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite>, as a vision backbone for the VLM setup (<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.SS1" title="3.1 FastViT as VLM Image Encoder ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>). We demonstrate the potential of this hybrid backbone, which generates visual tokens over 4<math alttext="\times" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><mo id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><times id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">×</annotation></semantics></math> faster than a ViT model while achieving higher overall VLM accuracy with multi-scale features (<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.SS1.SSS1" title="3.1.1 Multi-Scale Features ‣ 3.1 FastViT as VLM Image Encoder ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1.1</span></a>). However, further architectural optimization is possible when the primary goal is a high-resolution VLM (rather than embedding generation as in MobileCLIP-pretrained FastViT). We introduce a new hybrid vision encoder, FastViTHD, specifically designed for efficient VLM performance on high-resolution images (<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.SS2" title="3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>), and use it as the vision backbone to obtain FastVLM through visual instruction tuning.
FastVLM demonstrates a significantly improved accuracy-latency trade-off over VLMs based on ViTs, convolutional encoders, and our previously discussed hybrid FastViT for different input image resolutions and LLM sizes (<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1(a)</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S1.F1.sf2" title="In Figure 1 ‣ 1 Introduction ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">1(b)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F4" title="Figure 4 ‣ 3.2.1 Vision Encoder - Language Decoder Interplay ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>).
In particular, FastVLM outperforms several prior works while being smaller, faster, and trained with less data (<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T6" title="In 3.2.3 Comparison with Token Pruning &amp; Downsampling ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">6</span></a>). Compared to LLaVa-OneVision <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> operating at the highest possible resolution (1152<math alttext="\times" class="ltx_Math" display="inline" id="S1.p5.2.m2.1"><semantics id="S1.p5.2.m2.1a"><mo id="S1.p5.2.m2.1.1" xref="S1.p5.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p5.2.m2.1b"><times id="S1.p5.2.m2.1.1.cmml" xref="S1.p5.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p5.2.m2.1d">×</annotation></semantics></math>1152), FastVLM obtains comparable performance with the same 0.5B LLM, but with 85<math alttext="\times" class="ltx_Math" display="inline" id="S1.p5.3.m3.1"><semantics id="S1.p5.3.m3.1a"><mo id="S1.p5.3.m3.1.1" xref="S1.p5.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p5.3.m3.1b"><times id="S1.p5.3.m3.1.1.cmml" xref="S1.p5.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p5.3.m3.1d">×</annotation></semantics></math> faster TTFT and a 3.4<math alttext="\times" class="ltx_Math" display="inline" id="S1.p5.4.m4.1"><semantics id="S1.p5.4.m4.1a"><mo id="S1.p5.4.m4.1.1" xref="S1.p5.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p5.4.m4.1b"><times id="S1.p5.4.m4.1.1.cmml" xref="S1.p5.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p5.4.m4.1d">×</annotation></semantics></math> smaller vision encoder.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The following is a summary of our contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We demonstrate the efficacy of hybrid vision backbones in VLMs compared to ViTs. We also introduce additional architectural interventions, such as multi-scale vision features, to further improve VLM performance while maintaining efficiency.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.4">We design and pretrain a new hybrid architecture, FastViTHD, optimized for efficient VLM performance with high resolution input for FastVLM. In a controlled experimental setup, where only the vision backbone is changed, we show that FastViTHD outperforms its ViT-based and convolution-based counterparts when used in VLMs: achieving 3.2<math alttext="\times" class="ltx_Math" display="inline" id="S1.I1.i2.p1.1.m1.1"><semantics id="S1.I1.i2.p1.1.m1.1a"><mo id="S1.I1.i2.p1.1.m1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.1.m1.1b"><times id="S1.I1.i2.p1.1.m1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.p1.1.m1.1d">×</annotation></semantics></math> faster TTFT and 3.6<math alttext="\times" class="ltx_Math" display="inline" id="S1.I1.i2.p1.2.m2.1"><semantics id="S1.I1.i2.p1.2.m2.1a"><mo id="S1.I1.i2.p1.2.m2.1.1" xref="S1.I1.i2.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.2.m2.1b"><times id="S1.I1.i2.p1.2.m2.1.1.cmml" xref="S1.I1.i2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.p1.2.m2.1d">×</annotation></semantics></math> smaller size than SigLIP-SO400M <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib87" title=""><span class="ltx_text" style="font-size:90%;">87</span></a>]</cite>, and 2.3<math alttext="\times" class="ltx_Math" display="inline" id="S1.I1.i2.p1.3.m3.1"><semantics id="S1.I1.i2.p1.3.m3.1a"><mo id="S1.I1.i2.p1.3.m3.1.1" xref="S1.I1.i2.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.3.m3.1b"><times id="S1.I1.i2.p1.3.m3.1.1.cmml" xref="S1.I1.i2.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.p1.3.m3.1d">×</annotation></semantics></math> faster TTFT and 1.7<math alttext="\times" class="ltx_Math" display="inline" id="S1.I1.i2.p1.4.m4.1"><semantics id="S1.I1.i2.p1.4.m4.1a"><mo id="S1.I1.i2.p1.4.m4.1.1" xref="S1.I1.i2.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.4.m4.1b"><times id="S1.I1.i2.p1.4.m4.1.1.cmml" xref="S1.I1.i2.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.p1.4.m4.1d">×</annotation></semantics></math> smaller size than ConvNeXT <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>. We further demonstrate that FastVLM scales effectively as more visual instruction tuning data becomes available.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We systematically study the VLM accuracy-latency trade-off by considering both the vision backbone latency and the LLM prefilling time on actual hardware benchmarks. Our results demonstrate an improved resolution-latency-accuracy trade-off achieved by FastVLM, measured on-device rather than estimates.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Large Multimodal Models.</span>
With the emergence of large language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">73</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">71</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">90</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">62</span></a>]</cite> and large pretrained vision models, such as CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>, trained on web-scale image-text datasets, several multimodal architectures have been proposed to encode images aligned with a large language model (LLM) to enable the interpretation of visual signals. Earlier works like Frozen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a>]</cite> and Florence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> used a cross-attention mechanism where the image embeddings are fused with text embeddings in intermediate layers of the LLM. More recently, auto-regressive architectures have gained popularity where the image embedding is fed alongside text as input to an LLM. Some prominent works that use this architecture are LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, mPLUG-Owl <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib82" title=""><span class="ltx_text" style="font-size:90%;">82</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib81" title=""><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>, InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>, BLIP-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">78</span></a>]</cite>, SPHINX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>, MiniGPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib91" title=""><span class="ltx_text" style="font-size:90%;">91</span></a>]</cite>, VILA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>, MM1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite>, Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, InternVL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> and Cambrian-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite>. Recently, Fuyu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> and EVE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> introduced a simplified architecture that passes raw images directly to the LLM decoder. Chameleon <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite> introduced early fusion mixed-modal models where images are tokenized using a pretrained codebook. While skipping the image encoder is an intriguing approach, the performance of this new class of models lags behind architectures that use a pretrained image encoder.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Efficient Image Encoding.</span>
CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite> pretrained vision transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> are widely used for encoding images to a vision-language model. Some of the common choices include SigLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib87" title=""><span class="ltx_text" style="font-size:90%;">87</span></a>]</cite>, EVA-CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite>, InternViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> and DFN-CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>. To further boost performance, recent works like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite> use an ensemble of vision encoders pretrained using different objectives. These works are orthogonal to our work as they can benefit from using an efficient vision encoder among the ensemble of vision encoders. Since ViT architecture has been widely adopted in VLMs and the number of visual tokens remains one of the main causes of inefficiency, works like LLaVA-PruMerge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> and Matryoshka-based token sampling methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> have emerged to improve efficiency of image encoding by pruning visual tokens dynamically. Other works like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> reduce the number of visual tokens by incorporating perceiver-style resamplers or simply pooling using depth-wise convolutions or average pooling. Rather than using an isotropic architecture like ViT and then designing custom resamplers and projectors, hierarchical architectures can be a simpler design choice. Hierarchical backbones like ConvNeXT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite> and FastViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite> produce fewer tokens as they downsample the input tensor at every stage of compute. Recently, ConvLLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> was introduced that uses a pure-convolutional vision encoder to encode images for a VLM. In our work, we introduce an improved convolution-transformer hybrid architecture for VLMs and discuss the pareto-optimal operating points when this architecture is scaled to higher input resolutions.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Architecture</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F2.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.7.3.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F2.4.2" style="font-size:90%;">Overview of the FastVLM architecture.<span class="ltx_text ltx_font_medium" id="S3.F2.4.2.2"> FastVLM consists of our novel vision encoder, FastViTHD, trained using the same setup as LLaVa. The FastViTHD architecture is designed and trained for low latency at high resolution,
utilizing novel multi-scale pooling, additional self-attention layers, and downsampling to generate 4<math alttext="\times" class="ltx_Math" display="inline" id="S3.F2.3.1.1.m1.1"><semantics id="S3.F2.3.1.1.m1.1b"><mo id="S3.F2.3.1.1.m1.1.1" xref="S3.F2.3.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.F2.3.1.1.m1.1c"><times id="S3.F2.3.1.1.m1.1.1.cmml" xref="S3.F2.3.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.1.1.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S3.F2.3.1.1.m1.1e">×</annotation></semantics></math> fewer tokens than FastViT, and 16<math alttext="\times" class="ltx_Math" display="inline" id="S3.F2.4.2.2.m2.1"><semantics id="S3.F2.4.2.2.m2.1b"><mo id="S3.F2.4.2.2.m2.1.1" xref="S3.F2.4.2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.F2.4.2.2.m2.1c"><times id="S3.F2.4.2.2.m2.1.1.cmml" xref="S3.F2.4.2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.2.2.m2.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S3.F2.4.2.2.m2.1e">×</annotation></semantics></math> fewer tokens than ViT-L/14 at resolution 336. </span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we first explore the adoption of the FastViT hybrid vision encoder for vision-language modeling. We then introduce architectural interventions to improve performance on VLM tasks. We present FastViTHD, a new hybrid vision encoder designed for efficient high-resolution VLM. We provide comprehensive ablations to demonstrate the optimality of FastViTHD over FastViT and prior works for different LLMs and input resolutions. <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F2" title="In 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> illustrates the overall architecture of FastVLM and FastViTHD. The training setup for all results in this section follows the same configuration as LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> with Vicuna-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite> as the LLM decoder, unless mentioned otherwise. See <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4" title="4 Experiments ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a> for more details.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>FastViT as VLM Image Encoder</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">VLMs such as LLaVA have three main components: an image encoder, a vision-language projector, and a large language model (LLM). Both the performance and runtime efficiency of a VLM highly depend on its vision backbone. Encoding images at high resolution is essential for achieving strong performance across various VLM benchmarks, especially for text-rich tasks. Therefore, a vision encoder with scalable resolution is particularly beneficial for VLMs. We identify hybrid vision encoders (convolutional layers followed by transformer blocks) as an ideal candidate for VLMs, as their convolutional component enables native resolution scaling, and their transformer blocks further refine high-quality visual tokens for consumption by the LLM.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.4">We use a CLIP-pretrained hybrid vision encoder in our experiments. Specifically, we use the MCi2 image encoder from MobileCLIP <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite>, which has 35.7M parameters, is pretrained on DataCompDR <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite>, and is based on the FastViT <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite> architecture. For simplicity, we refer to this encoder as “FastViT” throughout the rest of the paper. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T1" title="In 3.1 FastViT as VLM Image Encoder ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, using FastViT at its CLIP-pretrained resolution (256<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mo id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><times id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">×</annotation></semantics></math>256) alone does not yield a strong VLM. The main advantage of a hybrid encoder like FastViT lies in its favorable image resolution scaling characteristics, meaning it generates 5.2<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mo id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><times id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">×</annotation></semantics></math> fewer tokens than the ViT architecture with a patch size of 14. The considerable token reduction gives significant advantage to VLM, as it greatly reduces the prefilling time and time-to-first-token of the transformer decoders with quadratic-complexity.
When the input resolution of FastViT is scaled to 768<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mo id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><times id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">×</annotation></semantics></math>768, it produces the same number of visual tokens as ViT-L/14 with an input resolution of 336<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mo id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><times id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">×</annotation></semantics></math>336 but achieves better performance on VLM benchmarks. This performance gap is even more pronounced on text-rich benchmarks like TextVQA and DocVQA, despite both architectures producing the same number of visual tokens. Moreover, even if it results in same number of tokens with higher resolution, it takes much less image encoding time, thanks to the efficient convolution layers.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T1.2" style="width:214.6pt;height:60.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-84.8pt,23.8pt) scale(0.558482947586089,0.558482947586089) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.3.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Image</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.3.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">Input</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.3.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">#Visual</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.3.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">Latency</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.3.1.5" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.2.2.3.1.5.1">GQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.3.1.6" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.2.2.3.1.6.1">TextVQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.3.1.7" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.2.2.3.1.7.1">POPE</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.3.1.8" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.2.2.3.1.8.1">DocVQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.3.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">Seed</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.3.1.10" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.2.2.3.1.10.1">Avg-5</span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2.2">
<td class="ltx_td ltx_align_left" id="S3.T1.2.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">Encoder</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">Res.</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">Tokens</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Enc.(ms)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">Bench<sup class="ltx_sup" id="S3.T1.2.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S3.T1.2.2.2.2.1.1">I</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.4.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.4.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.4.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">576</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.4.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">127.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.4.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">62.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.4.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">58.2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.4.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">85.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.4.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">28.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.4.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">66.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.4.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">60.1</th>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2.5.3">
<td class="ltx_td ltx_align_left" id="S3.T1.2.2.5.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14<sup class="ltx_sup" id="S3.T1.2.2.5.3.1.1">†</sup>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.5.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.5.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">576</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.5.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">127.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.5.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">63.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.5.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">59.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.5.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">86.3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.5.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">28.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.5.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">68.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.5.3.10" style="padding-left:2.0pt;padding-right:2.0pt;">61.2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.6.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">FastViT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.6.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">256</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.6.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">64</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.6.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">3.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.6.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">60.2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.6.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">51.6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.6.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">82.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.6.4.8" style="padding-left:2.0pt;padding-right:2.0pt;">15.8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.6.4.9" style="padding-left:2.0pt;padding-right:2.0pt;">61.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.2.2.6.4.10" style="padding-left:2.0pt;padding-right:2.0pt;">54.4</th>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2.7.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.2.7.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">FastViT</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.2.7.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">768</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.2.7.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">576</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.2.7.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">34.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.2.7.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">62.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.2.7.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">62.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.2.7.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">86.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.2.7.5.8" style="padding-left:2.0pt;padding-right:2.0pt;">34.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.2.7.5.9" style="padding-left:2.0pt;padding-right:2.0pt;">67.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.2.7.5.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.7.5.10.1">62.6</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.6.2.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.4.1" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1">FastViT has higher accuracy than ViT-L/14 at near 4<math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.4.1.1.m1.1"><semantics id="S3.T1.4.1.1.m1.1b"><mo id="S3.T1.4.1.1.m1.1.1" xref="S3.T1.4.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.1.1.m1.1c"><times id="S3.T1.4.1.1.m1.1.1.cmml" xref="S3.T1.4.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.1.1.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.1.1.m1.1e">×</annotation></semantics></math> lower latency.</span>
To scale resolution up to 768, FastViT is made trainable during Stage-2 training of LLaVA-1.5 setup. †To have a fair comparison, we also report the performance of ViT-L/14 finetuned during Stage-2 training of LLaVA-1.5. All latencies are reported in milliseconds. See <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4" title="4 Experiments ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a> for details.
</span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Multi-Scale Features</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">Typical convolutional and hybrid architectures split up the computations into 4 distinct stages with a downsampling operation between them. While the VLM relies on features from the penultimate layer, features in earlier stages of the network extract information at different granularity. Aggregating information from multiple scales can complement high-level features from the penultimate layer. This design is commonly used in object detection models like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>. The architecture for multiple scale feature extraction is shown in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F2" title="In 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. We ablate between 2 designs to pool features from different stages, i.e. AvgPooling and 2D Depthwise convolutions. From <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T2" title="In 3.1.1 Multi-Scale Features ‣ 3.1 FastViT as VLM Image Encoder ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we find that using depthwise convolutions results in better performance. Along with multi-scale features, we also experimented with different connector designs for FastViT (more details provided in supplementary materials). Collectively, these model interventions benefit hierarchical backbones like ConvNeXt and FastViT.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T2.1" style="width:214.6pt;height:58.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-57.4pt,15.7pt) scale(0.651532579294501,0.651532579294501) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T2.1.1.2.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Image</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">Multi</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.2.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">Pool</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.2.1.4" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T2.1.1.2.1.4.1">GQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.2.1.5" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T2.1.1.2.1.5.1">TextVQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.2.1.6" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T2.1.1.2.1.6.1">POPE</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.2.1.7" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T2.1.1.2.1.7.1">DocVQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.2.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">Seed</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.2.1.9" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T2.1.1.2.1.9.1">Avg-5</span></th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S3.T2.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">Encoder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">Scale</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">Type</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Bench<sup class="ltx_sup" id="S3.T2.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S3.T2.1.1.1.1.1.1">I</span></sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.1.3.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">FastViT</th>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.3.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.3.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.3.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">62.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.3.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">62.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.3.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">86.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.3.1.7" style="padding-left:2.0pt;padding-right:2.0pt;">34.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.3.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">67.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.3.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">62.6</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.4.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">FastViT</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">AvgPool</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">63.0</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">62.2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">86.2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">35.1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">66.9</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">62.7</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.1.1.5.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">FastViT</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.5.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.5.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">DWConv</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.5.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">63.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.5.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">62.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.5.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">86.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.5.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">34.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.5.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">67.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.1.5.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">62.9</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.4.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S3.T2.5.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.T2.5.2.1">Pushing FastViT VLM performance using multi-scale features and pooling strategies.</span>
These modifications slightly improve FastViT. Training setup is LLaVA-1.5 with Vicuna 7B.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F3.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.5.2.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.2.1" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.F3.2.1.1">Novel scaling strategy of FastViTHD lowers latency
at various image resolutions.</span>
FastViT-Naive, a naive scaling of the FastViT architecture, and our proposed FastViTHD have the same number of parameters. ConvNeXt-L is provided for reference. All models are benchmarked on M1 Macbook Pro and trained with LLaVA-1.5 setup and Vicuna 7B. Note that the <math alttext="y" class="ltx_Math" display="inline" id="S3.F3.2.1.m1.1"><semantics id="S3.F3.2.1.m1.1b"><mi id="S3.F3.2.1.m1.1.1" xref="S3.F3.2.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.F3.2.1.m1.1c"><ci id="S3.F3.2.1.m1.1.1.cmml" xref="S3.F3.2.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.1.m1.1d">y</annotation><annotation encoding="application/x-llamapun" id="S3.F3.2.1.m1.1e">italic_y</annotation></semantics></math>-axis is in log scale.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>FastViTHD: High Resolution Encoder for VLM</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">While FastViT with the introduced model interventions performs well as an image encoder that is 8.7<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mo id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><times id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">×</annotation></semantics></math> smaller than ViT-L/14,
previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> have demonstrated that increasing the scale of the image encoder improves its generalization capabilities.
Common practice in hybrid architectures is to scale the number of self-attention layers along with the width of each layer in a 4-stage architecture like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">85</span></a>]</cite>, but this approach has its drawbacks.
From <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F3" title="In 3.1.1 Multi-Scale Features ‣ 3.1 FastViT as VLM Image Encoder ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, simply scaling-up the number of self-attention layers in stage 3 and 4 by following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">85</span></a>]</cite>, in the existing FastViT architecture is not optimal. In fact, it is even slower than ConvNeXT-L. More details on the naively scaled version of FastViT are provided in supplementary materials. To reduce the impact of the added self-attention layers, we introduce an extra stage preceded by a downsampling layer, see <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F2" title="In 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.
In this approach, the self-attention layers only handle tensors that have been downsampled by a factor of 32 on each side, compared to a factor of 16 in typical and more recent hybrid models like ViTamin <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>.
The self-attention layers with the widest MLP layers process input tensors downsampled by a factor of 64 on each side. Our design reduces image encoding latency and generates 4<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mo id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><times id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">×</annotation></semantics></math> fewer tokens for the compute-intensive LLM decoder, thereby decreasing the time-to-first-token (TTFT). The architecture schematic is shown in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F2" title="In 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, and we call this model <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.2.1">FastViTHD</span>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The model architecture consists of 5 stages, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F2" title="In 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, with the first three stages utilizing RepMixer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite> blocks and the last two stages employing multi-headed self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> blocks. The model depth at each stage is <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1">[2, 12, 24, 4, 2]</span>, and the embedding dimensions for each stage are <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.2">[96, 192, 384, 768, 1536]</span>. The MLP expansion ratio for the ConvFFN layers is set to 4.0. The model has 125.1M parameters, which is 3.5<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mo id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><times id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">×</annotation></semantics></math> larger than the largest FastViT variant from MobileCLIP, but is still smaller than popular ViT alternatives.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T3.2" style="width:214.6pt;height:45.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-146.1pt,31.1pt) scale(0.423454017267729,0.423454017267729) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.2.2.3.1.1">Image</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.3.1.2">Encoder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.3.1.3">Input</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T3.2.2.3.1.4">Latency</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.3.1.5">Zero-Shot</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.3.1.6">Avg Perf.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.3.1.7">Avg Perf.</th>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T3.2.2.2.3">Encoder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.1.1.1.1">Size(M)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.m1.1a"><mo id="S3.T3.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.2.2.2.4">Res.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T3.2.2.2.2">Enc.(ms)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.2.2.2.2.m1.1"><semantics id="S3.T3.2.2.2.2.m1.1a"><mo id="S3.T3.2.2.2.2.m1.1.1" stretchy="false" xref="S3.T3.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.2.m1.1b"><ci id="S3.T3.2.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.2.2.2.5">ImageNet</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.2.2.2.6">Retrieval</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T3.2.2.2.7">on 38 tasks</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.2.2.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.2.2.4.1.1">ViT-L/14 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.4.1.2">304</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.4.1.3">224</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.2.2.4.1.4">47.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.4.1.5">79.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.4.1.6">60.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.4.1.7">66.3</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T3.2.2.5.2.1">ViTamin-L <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.5.2.2">333</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.5.2.3">224</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.2.2.5.2.4">38.1</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.5.2.5">80.8</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.5.2.6">60.3</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.5.2.7">66.7</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T3.2.2.6.3.1">ConvNeXt-L</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.6.3.2">200</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.6.3.3">320</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.2.2.6.3.4">34.4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.6.3.5">76.8</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.6.3.6">64.8</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.6.3.7">63.9</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T3.2.2.7.4.1"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.7.4.1.1">FastViTHD</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.7.4.2">125</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.7.4.3">224</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T3.2.2.7.4.4">6.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.7.4.5">78.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.7.4.6">67.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.7.4.7">66.3</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.5.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S3.T3.6.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.T3.6.2.1">FastViTHD achieves competitive results on CLIP benchmarks at significantly lower latency.</span> We follow the same setup described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> to report average retrieval performance and setup described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> to report average performance on 38 tasks. All models are benchmarked on M1 Macbook Pro.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.6">We follow the CLIP pretraining setup of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite> using the DataCompDR-1B dataset to pretrain FastViTHD before employing it for FastVLM training. <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T3" title="In 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows that FastViTHD, despite being 2.4<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mo id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><times id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">×</annotation></semantics></math> smaller and 6.9<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mo id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><times id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">×</annotation></semantics></math> faster than ViT-L/14, achieves comparable average performance across 38 multi-modal zero-shot tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>. In comparison to ViTamin <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, a hybrid transformer architecture built for VLMs, FastViTHD delivers superior average retrieval performance while being 2.7<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mo id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><times id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">×</annotation></semantics></math> smaller and 5.6<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><mo id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><times id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">×</annotation></semantics></math> faster. In <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T4" title="In 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we compare FastViTHD with other CLIP-pretrained hierarchical backbones, i.e. ConvNeXT-L and ConvNeXT-XXL, for VLM tasks after LLaVa-1.5 training. FastViTHD performs as well as ConvNeXT-XXL while being 6.8<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p3.5.m5.1"><semantics id="S3.SS2.p3.5.m5.1a"><mo id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><times id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.5.m5.1d">×</annotation></semantics></math> smaller and 3.3<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p3.6.m6.1"><semantics id="S3.SS2.p3.6.m6.1a"><mo id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><times id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.6.m6.1d">×</annotation></semantics></math> faster.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T4.2" style="width:214.6pt;height:99.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-87.2pt,40.3pt) scale(0.551822437975897,0.551822437975897) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T4.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T4.2.2.3.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Image</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T4.2.2.3.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">Input</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T4.2.2.3.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">Latency</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T4.2.2.3.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">#Visual</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.2.3.1.5" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T4.2.2.3.1.5.1">GQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.2.3.1.6" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T4.2.2.3.1.6.1">TextVQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.2.3.1.7" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T4.2.2.3.1.7.1">POPE</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.2.3.1.8" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T4.2.2.3.1.8.1">DocVQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.2.3.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">Seed</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.2.3.1.10" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T4.2.2.3.1.10.1">Avg-5</span></th>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S3.T4.2.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">Encoder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S3.T4.2.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">Res.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S3.T4.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Enc.(ms)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T4.1.1.1.1.m1.1"><semantics id="S3.T4.1.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T4.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.1.m1.1b"><ci id="S3.T4.1.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S3.T4.2.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">Tokens</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T4.2.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">Bench<sup class="ltx_sup" id="S3.T4.2.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S3.T4.2.2.2.2.1.1">I</span></sup>
</th>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T4.2.2.4.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.2.2.4.2.1.1">FastViTHD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T4.2.2.4.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">256</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T4.2.2.4.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">10.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T4.2.2.4.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">16</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T4.2.2.4.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">60.6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T4.2.2.4.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">53.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T4.2.2.4.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">82.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T4.2.2.4.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">17.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T4.2.2.4.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">63.7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T4.2.2.4.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">55.5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.2.2.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.5.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">C.N-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.5.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">320</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.5.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">34.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.5.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">100</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.5.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">61.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.5.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">55.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.5.1.7" style="padding-left:2.0pt;padding-right:2.0pt;">85.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.5.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">21.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.5.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">64.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.5.1.10" style="padding-left:2.0pt;padding-right:2.0pt;">57.7</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T4.2.2.6.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">C.N-XXL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T4.2.2.6.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">256</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T4.2.2.6.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">89.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T4.2.2.6.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">64</th>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.6.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">62.7</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.6.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">56.3</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.6.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">85.3</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.6.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">21.6</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.6.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">65.6</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.6.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">58.3</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T4.2.2.7.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.2.2.7.3.1.1">FastViTHD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T4.2.2.7.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">512</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T4.2.2.7.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">33.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T4.2.2.7.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">64</th>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.7.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">63.0</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.7.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">59.3</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.7.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">86.4</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.7.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">25.7</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.7.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">67.1</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.7.3.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.2.2.7.3.10.1">60.4</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.8.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.2.2.8.4.1.1">FastViTHD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.8.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">768</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.8.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">122.6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.8.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">144</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.8.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">62.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.8.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">62.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.8.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">87.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.8.4.8" style="padding-left:2.0pt;padding-right:2.0pt;">32.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.8.4.9" style="padding-left:2.0pt;padding-right:2.0pt;">68.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.8.4.10" style="padding-left:2.0pt;padding-right:2.0pt;">62.8</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2.9.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.9.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">C.N-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.9.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">512</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.9.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">71.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.9.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">256</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.9.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">61.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.9.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">61.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.9.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">86.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.9.5.8" style="padding-left:2.0pt;padding-right:2.0pt;">30.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.9.5.9" style="padding-left:2.0pt;padding-right:2.0pt;">66.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.9.5.10" style="padding-left:2.0pt;padding-right:2.0pt;">61.3</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2.10.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T4.2.2.10.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">C.N-XXL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T4.2.2.10.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">512</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T4.2.2.10.6.3" style="padding-left:2.0pt;padding-right:2.0pt;">397.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T4.2.2.10.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">256</th>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.10.6.5" style="padding-left:2.0pt;padding-right:2.0pt;">62.3</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.10.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">65.1</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.10.6.7" style="padding-left:2.0pt;padding-right:2.0pt;">87.7</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.10.6.8" style="padding-left:2.0pt;padding-right:2.0pt;">36.2</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.10.6.9" style="padding-left:2.0pt;padding-right:2.0pt;">68.4</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.10.6.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.2.2.10.6.10.1">63.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2.11.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T4.2.2.11.7.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.2.2.11.7.1.1">FastViTHD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T4.2.2.11.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">1024</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T4.2.2.11.7.3" style="padding-left:2.0pt;padding-right:2.0pt;">235.6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T4.2.2.11.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">256</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.2.11.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">63.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.2.11.7.6" style="padding-left:2.0pt;padding-right:2.0pt;">64.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.2.11.7.7" style="padding-left:2.0pt;padding-right:2.0pt;">88.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.2.11.7.8" style="padding-left:2.0pt;padding-right:2.0pt;">35.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.2.11.7.9" style="padding-left:2.0pt;padding-right:2.0pt;">68.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.2.11.7.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.2.2.11.7.10.1">63.9</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T4.5.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S3.T4.6.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.T4.6.2.1">FastViTHD achieves higher accuracy than ConvNeXT while having lower latency at a higher resolution.</span>
The models are grouped based on the total number of visual tokens produced for the LLM to process. “C.N” stands for ConvNeXT. Training setup is LLaVA-1.5 with Vicuna 7B.
</span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Vision Encoder - Language Decoder Interplay</h4>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F4.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.5.2.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.2.1" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.F4.2.1.1">FastViTHD improves the Pareto-Optimal curve for accuracy versus time to first token compared with FastViT.</span>
Comparison of FastViT and FastViTHD backbones paired with Qwen2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite> family (chat variant) LLMs of varying sizes and different image resolutions (annotated for each point). The Pareto-optimal curve is highlighted for the two vision backbones. Training setup is LLaVA-1.5. Note that the <math alttext="x" class="ltx_Math" display="inline" id="S3.F4.2.1.m1.1"><semantics id="S3.F4.2.1.m1.1b"><mi id="S3.F4.2.1.m1.1.1" xref="S3.F4.2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.F4.2.1.m1.1c"><ci id="S3.F4.2.1.m1.1.1.cmml" xref="S3.F4.2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.2.1.m1.1d">x</annotation><annotation encoding="application/x-llamapun" id="S3.F4.2.1.m1.1e">italic_x</annotation></semantics></math>-axis is in log scale.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F5.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.4.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.F5.4.2.1">Vision latency dominates at high resolution.</span>
Breakdown of FastVLM’s time to first token for varying image resolutions. Vision encoder is FastViTHD and LLM is Qwen2-1.5B.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F6.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.6.3.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F6.4.2" style="font-size:90%;">Dynamic input resolution (AnyRes) is only optimal at the highest resolution when using fewer tiles (2<math alttext="\times" class="ltx_Math" display="inline" id="S3.F6.3.1.m1.1"><semantics id="S3.F6.3.1.m1.1b"><mo id="S3.F6.3.1.m1.1.1" xref="S3.F6.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.F6.3.1.m1.1c"><times id="S3.F6.3.1.m1.1.1.cmml" xref="S3.F6.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.3.1.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S3.F6.3.1.m1.1e">×</annotation></semantics></math>2).<span class="ltx_text ltx_font_medium" id="S3.F6.4.2.1"> The vision encoder is FastViTHD. The tile grid size is specified in parenthesis. Training setup is LLaVA-1.5 with Vicuna 7B. Note that the <math alttext="x" class="ltx_Math" display="inline" id="S3.F6.4.2.1.m1.1"><semantics id="S3.F6.4.2.1.m1.1b"><mi id="S3.F6.4.2.1.m1.1.1" xref="S3.F6.4.2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.F6.4.2.1.m1.1c"><ci id="S3.F6.4.2.1.m1.1.1.cmml" xref="S3.F6.4.2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.4.2.1.m1.1d">x</annotation><annotation encoding="application/x-llamapun" id="S3.F6.4.2.1.m1.1e">italic_x</annotation></semantics></math>-axis is in log scale.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">The accuracy-latency trade-off in a VLM is influenced by several factors. On one hand, the overall performance of the VLM depends on (1) the input image resolution, (2) the quantity and quality of visual tokens, and (3) the capability of the LLM. On the other hand, the total latency (time to first token generation) of a VLM is determined by (1) the latency of the vision encoder and (2) the prefilling time of the LLM. The latter is affected by both the number of tokens produced by the vision encoder and the size of the LLM.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">Due to the complex optimization landscape of VLMs, claims regarding the optimality of a vision encoder must be verified across various pairs of (Resolution, LLM). Here, we empirically demonstrate the optimality of FastViTHD over FastViT. For each vision encoder, we consider three LLMs, Qwen2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>-0.5B/1.5B/7B, along with a range of input image resolutions. For each (Resolution, LLM) pair, we conduct LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> pretraining and visual instruction tuning, and evaluate the resulting model over a range of tasks. The results are presented in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F4" title="In 3.2.1 Vision Encoder - Language Decoder Interplay ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">First, we observe that for a vision encoder, the Pareto-optimal curve (highlighted in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F4" title="In 3.2.1 Vision Encoder - Language Decoder Interplay ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>), which represents the maximum achievable performance for a given runtime budget (TTFT), consists of varying sizes of LLMs.
Specifically, pairing high resolution with a small LLM is suboptimal as
a small LLM cannot effectively utilize that many tokens, and TTFT will be dominated by the latency of the vision encoder (see <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F5" title="In 3.2.1 Vision Encoder - Language Decoder Interplay ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p4">
<p class="ltx_p" id="S3.SS2.SSS1.p4.1">Second, the Pareto-optimal curve for FastViTHD in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F4" title="In 3.2.1 Vision Encoder - Language Decoder Interplay ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a> is significantly better than that of FastViT. For a given runtime budget, considering all possible (Resolution, LLM) pairs, we achieve significantly better performance (an improvement of over 2.5 points on the Average-5 metric) with FastViTHD. Similarly, FastViTHD can reach a target VLM performance up to 3<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.1.m1.1"><semantics id="S3.SS2.SSS1.p4.1.m1.1a"><mo id="S3.SS2.SSS1.p4.1.m1.1.1" xref="S3.SS2.SSS1.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.1.m1.1b"><times id="S3.SS2.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p4.1.m1.1d">×</annotation></semantics></math> faster. It is important to note that in previous sections, we demonstrated that a FastViT-based VLM already represents a significant improvement over ViT-based VLMs, and yet FastViTHD provides substantial gains over FastViT.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Static vs. Dynamic Input Resolution</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">There are two approaches to input resolution scaling, the first approach is to change the input resolution of the model to the desired resolution. The second approach is to tile the input image and set the input resolution of the image encoder to the tile size. The tiled inference (AnyRes) was introduced in prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> to enable ViT models to process high resolution images. Since FastViTHD is designed to run inference efficiently on high input resolutions, we analyze the optimal operating point for various resolutions using the two strategies. From <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F6" title="In 3.2.1 Vision Encoder - Language Decoder Interplay ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, we see that simply setting the input resolution of the model to the desired resolution results in VLMs with the best accuracy-latency tradeoff. Only at extremely high image resolutions like 1536<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.1.m1.1"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mo id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><times id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.1.m1.1d">×</annotation></semantics></math>1536 do we see the benefits of dynamic resolution, as model inference at this resolution is mostly affected by memory bandwidth available on-device. If dynamic resolution is desired, using a setting with fewer tiles exhibits better accuracy-latency tradeoff. With advancements in hardware and improvements in memory size and bandwidth, we expect that FastVLM can be efficiently scaled to even higher resolutions without the need for tiling strategies.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Comparison with Token Pruning &amp; Downsampling</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">We further compare the performance of FastViTHD operating at different resolutions to popular token pruning methods in literature. From <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T5" title="In 3.2.3 Comparison with Token Pruning &amp; Downsampling ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, we find that VLMs achieve better accuracy to latency trade-off using a hierarchical backbone as opposed to using token pruning methods on isotropic architectures like ViT. By simply training the VLMs at lower input resolution, FastViTHD achieves visual token counts as low as 16, while improving over recent token pruning methods. Interestingly, even the most effective token pruning methods, such as those proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>, perform worse than FastViTHD trained at a lower input resolution of 256<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.1.m1.1"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mo id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><times id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.1.m1.1d">×</annotation></semantics></math>256.</p>
</div>
<figure class="ltx_table" id="S3.T5">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T5.5" style="width:214.6pt;height:177.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-154.4pt,127.4pt) scale(0.410093480568978,0.410093480568978) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.5.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T5.5.5.6.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T5.5.5.6.1.1" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T5.5.5.6.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T5.5.5.6.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">Input</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T5.5.5.6.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">#Visual</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T5.5.5.6.1.4" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T5.5.5.6.1.4.1">GQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T5.5.5.6.1.5" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T5.5.5.6.1.5.1">SQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T5.5.5.6.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">Text-</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T5.5.5.6.1.7" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T5.5.5.6.1.7.1">POPE</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T5.5.5.6.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">VQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T5.5.5.6.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">Seed</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.7.2">
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.7.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">Res.</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.7.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">Tokens</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.7.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">VQA</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.7.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">v2</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.7.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">Bench</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T5.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14 M<sup class="ltx_sup" id="S3.T5.1.1.1.1.1">3</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">58.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.1.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.1.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.1.1.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.7.1">83.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.1.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.1.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">55.4</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.8.3">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.8.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14 MQT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.8.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.8.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">16</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.8.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">57.6</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.8.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">67.5</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.8.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.8.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">80.8</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.8.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">71.1</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.8.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.9.4">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.9.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.9.4.1.1">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.9.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">256</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.9.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">16</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.9.4.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.9.4.4.1">60.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.9.4.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.9.4.5.1">69.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.9.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">53.1</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.9.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">82.3</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.9.4.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.9.4.8.1">74.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.9.4.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.9.4.9.1">58.8</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.10.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T5.5.5.10.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14 PruMerge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.10.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.10.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.10.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.10.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">68.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.10.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">56.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.10.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">76.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.10.5.8" style="padding-left:2.0pt;padding-right:2.0pt;">72.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.10.5.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.11.6">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.11.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14 PruMerge+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.11.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.11.6.3" style="padding-left:2.0pt;padding-right:2.0pt;">40</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.11.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.11.6.5" style="padding-left:2.0pt;padding-right:2.0pt;">68.3</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.11.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">57.1</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.11.6.7" style="padding-left:2.0pt;padding-right:2.0pt;">84.0</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.11.6.8" style="padding-left:2.0pt;padding-right:2.0pt;">76.8</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.11.6.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T5.2.2.2">
<td class="ltx_td ltx_align_left" id="S3.T5.2.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14 M<sup class="ltx_sup" id="S3.T5.2.2.2.1.1">3</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.2.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.2.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">36</td>
<td class="ltx_td ltx_align_center" id="S3.T5.2.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">60.3</td>
<td class="ltx_td ltx_align_center" id="S3.T5.2.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T5.2.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T5.2.2.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">85.5</td>
<td class="ltx_td ltx_align_center" id="S3.T5.2.2.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T5.2.2.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">58.0</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.12.7">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.12.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">FastV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.12.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.12.7.3" style="padding-left:2.0pt;padding-right:2.0pt;">64</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.12.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">46.1</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.12.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">51.1</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.12.7.6" style="padding-left:2.0pt;padding-right:2.0pt;">47.8</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.12.7.7" style="padding-left:2.0pt;padding-right:2.0pt;">48.0</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.12.7.8" style="padding-left:2.0pt;padding-right:2.0pt;">55.0</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.12.7.9" style="padding-left:2.0pt;padding-right:2.0pt;">51.9</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.13.8">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.13.8.1" style="padding-left:2.0pt;padding-right:2.0pt;">SparseVLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib89" title=""><span class="ltx_text" style="font-size:90%;">89</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.13.8.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.13.8.3" style="padding-left:2.0pt;padding-right:2.0pt;">64</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.13.8.4" style="padding-left:2.0pt;padding-right:2.0pt;">52.7</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.13.8.5" style="padding-left:2.0pt;padding-right:2.0pt;">62.2</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.13.8.6" style="padding-left:2.0pt;padding-right:2.0pt;">51.8</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.13.8.7" style="padding-left:2.0pt;padding-right:2.0pt;">75.1</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.13.8.8" style="padding-left:2.0pt;padding-right:2.0pt;">68.2</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.13.8.9" style="padding-left:2.0pt;padding-right:2.0pt;">51.1</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.14.9">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.14.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">VisionZip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.14.9.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.14.9.3" style="padding-left:2.0pt;padding-right:2.0pt;">64</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.14.9.4" style="padding-left:2.0pt;padding-right:2.0pt;">55.1</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.14.9.5" style="padding-left:2.0pt;padding-right:2.0pt;">69.0</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.14.9.6" style="padding-left:2.0pt;padding-right:2.0pt;">55.5</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.14.9.7" style="padding-left:2.0pt;padding-right:2.0pt;">77.0</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.14.9.8" style="padding-left:2.0pt;padding-right:2.0pt;">62.9</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.14.9.9" style="padding-left:2.0pt;padding-right:2.0pt;">52.2</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.15.10">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.15.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">VisionZip‡ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.15.10.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.15.10.3" style="padding-left:2.0pt;padding-right:2.0pt;">64</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.15.10.4" style="padding-left:2.0pt;padding-right:2.0pt;">57.0</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.15.10.5" style="padding-left:2.0pt;padding-right:2.0pt;">68.8</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.15.10.6" style="padding-left:2.0pt;padding-right:2.0pt;">56.0</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.15.10.7" style="padding-left:2.0pt;padding-right:2.0pt;">80.9</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.15.10.8" style="padding-left:2.0pt;padding-right:2.0pt;">74.2</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.15.10.9" style="padding-left:2.0pt;padding-right:2.0pt;">53.4</td>
</tr>
<tr class="ltx_tr" id="S3.T5.3.3.3">
<td class="ltx_td ltx_align_left" id="S3.T5.3.3.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">DynamicLLaVA<sub class="ltx_sub" id="S3.T5.3.3.3.1.1"><span class="ltx_text ltx_font_italic" id="S3.T5.3.3.3.1.1.1">I</span></sub> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">115</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">61.4</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.3.3.3.5.1">69.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.3.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">57.0</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.3.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">85.0</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.3.3.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.3.3.3.8.1">78.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.3.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T5.4.4.4">
<td class="ltx_td ltx_align_left" id="S3.T5.4.4.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">DynamicLLaVA<sub class="ltx_sub" id="S3.T5.4.4.4.1.1"><span class="ltx_text ltx_font_italic" id="S3.T5.4.4.4.1.1.1">I|T</span></sub> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.4.4.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.4.4.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">115</td>
<td class="ltx_td ltx_align_center" id="S3.T5.4.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">61.3</td>
<td class="ltx_td ltx_align_center" id="S3.T5.4.4.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">68.6</td>
<td class="ltx_td ltx_align_center" id="S3.T5.4.4.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">56.5</td>
<td class="ltx_td ltx_align_center" id="S3.T5.4.4.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">85.9</td>
<td class="ltx_td ltx_align_center" id="S3.T5.4.4.4.8" style="padding-left:2.0pt;padding-right:2.0pt;">77.9</td>
<td class="ltx_td ltx_align_center" id="S3.T5.4.4.4.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.16.11">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.16.11.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.16.11.1.1">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.16.11.2" style="padding-left:2.0pt;padding-right:2.0pt;">512</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.16.11.3" style="padding-left:2.0pt;padding-right:2.0pt;">64</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.16.11.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.16.11.4.1">63.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.16.11.5" style="padding-left:2.0pt;padding-right:2.0pt;">68.9</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.16.11.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.16.11.6.1">59.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.16.11.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.16.11.7.1">86.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.16.11.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.16.11.8.1">78.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.16.11.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.16.11.9.1">61.8</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T5.5.5.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14 M<sup class="ltx_sup" id="S3.T5.5.5.5.1.1">3</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">144</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">61.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">87.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.5.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.5.9" style="padding-left:2.0pt;padding-right:2.0pt;">59.7</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.17.12">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.17.12.1" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14 MQT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.17.12.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.17.12.3" style="padding-left:2.0pt;padding-right:2.0pt;">144</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.17.12.4" style="padding-left:2.0pt;padding-right:2.0pt;">61.4</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.17.12.5" style="padding-left:2.0pt;padding-right:2.0pt;">67.6</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.17.12.6" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.17.12.7" style="padding-left:2.0pt;padding-right:2.0pt;">83.9</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.17.12.8" style="padding-left:2.0pt;padding-right:2.0pt;">76.4</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.17.12.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.18.13">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.18.13.1" style="padding-left:2.0pt;padding-right:2.0pt;">FastV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.18.13.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.18.13.3" style="padding-left:2.0pt;padding-right:2.0pt;">192</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.18.13.4" style="padding-left:2.0pt;padding-right:2.0pt;">52.7</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.18.13.5" style="padding-left:2.0pt;padding-right:2.0pt;">67.3</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.18.13.6" style="padding-left:2.0pt;padding-right:2.0pt;">52.5</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.18.13.7" style="padding-left:2.0pt;padding-right:2.0pt;">64.8</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.18.13.8" style="padding-left:2.0pt;padding-right:2.0pt;">67.1</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.18.13.9" style="padding-left:2.0pt;padding-right:2.0pt;">57.1</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.19.14">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.19.14.1" style="padding-left:2.0pt;padding-right:2.0pt;">SparseVLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib89" title=""><span class="ltx_text" style="font-size:90%;">89</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.19.14.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.19.14.3" style="padding-left:2.0pt;padding-right:2.0pt;">192</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.19.14.4" style="padding-left:2.0pt;padding-right:2.0pt;">57.6</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.19.14.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.19.14.5.1">69.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.19.14.6" style="padding-left:2.0pt;padding-right:2.0pt;">56.1</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.19.14.7" style="padding-left:2.0pt;padding-right:2.0pt;">83.6</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.19.14.8" style="padding-left:2.0pt;padding-right:2.0pt;">75.6</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.19.14.9" style="padding-left:2.0pt;padding-right:2.0pt;">55.8</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.20.15">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.20.15.1" style="padding-left:2.0pt;padding-right:2.0pt;">VisionZip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.20.15.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.20.15.3" style="padding-left:2.0pt;padding-right:2.0pt;">192</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.20.15.4" style="padding-left:2.0pt;padding-right:2.0pt;">59.3</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.20.15.5" style="padding-left:2.0pt;padding-right:2.0pt;">68.9</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.20.15.6" style="padding-left:2.0pt;padding-right:2.0pt;">57.3</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.20.15.7" style="padding-left:2.0pt;padding-right:2.0pt;">85.3</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.20.15.8" style="padding-left:2.0pt;padding-right:2.0pt;">76.8</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.20.15.9" style="padding-left:2.0pt;padding-right:2.0pt;">56.4</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.21.16">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.21.16.1" style="padding-left:2.0pt;padding-right:2.0pt;">VisionZip‡ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.21.16.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.21.16.3" style="padding-left:2.0pt;padding-right:2.0pt;">192</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.21.16.4" style="padding-left:2.0pt;padding-right:2.0pt;">60.1</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.21.16.5" style="padding-left:2.0pt;padding-right:2.0pt;">68.2</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.21.16.6" style="padding-left:2.0pt;padding-right:2.0pt;">57.8</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.21.16.7" style="padding-left:2.0pt;padding-right:2.0pt;">84.9</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.21.16.8" style="padding-left:2.0pt;padding-right:2.0pt;">77.4</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.21.16.9" style="padding-left:2.0pt;padding-right:2.0pt;">57.1</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.22.17">
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.22.17.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.22.17.1.1">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.22.17.2" style="padding-left:2.0pt;padding-right:2.0pt;">768</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.22.17.3" style="padding-left:2.0pt;padding-right:2.0pt;">144</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.22.17.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.22.17.4.1">62.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.22.17.5" style="padding-left:2.0pt;padding-right:2.0pt;">67.6</td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.22.17.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.22.17.6.1">62.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.22.17.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.22.17.7.1">87.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.22.17.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.22.17.8.1">78.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.5.5.22.17.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.22.17.9.1">62.5</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.23.18">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T5.5.5.23.18.1" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14 MQT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.23.18.2" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.23.18.3" style="padding-left:2.0pt;padding-right:2.0pt;">256</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.23.18.4" style="padding-left:2.0pt;padding-right:2.0pt;">61.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.23.18.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.23.18.5.1">67.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.23.18.6" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.23.18.7" style="padding-left:2.0pt;padding-right:2.0pt;">84.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.23.18.8" style="padding-left:2.0pt;padding-right:2.0pt;">76.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.5.5.23.18.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5.24.19">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T5.5.5.24.19.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.24.19.1.1">FastViTHD</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.5.5.24.19.2" style="padding-left:2.0pt;padding-right:2.0pt;">1024</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.5.5.24.19.3" style="padding-left:2.0pt;padding-right:2.0pt;">256</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.5.5.24.19.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.24.19.4.1">63.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.5.5.24.19.5" style="padding-left:2.0pt;padding-right:2.0pt;">67.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.5.5.24.19.6" style="padding-left:2.0pt;padding-right:2.0pt;">64.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.5.5.24.19.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.24.19.7.1">88.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.5.5.24.19.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.5.5.24.19.8.1">79.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.5.5.24.19.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T5.14.3.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S3.T5.9.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.T5.9.2.1">FastViTHD more effectively reduces tokens compared with token pruning methods.</span>
The models are grouped based on total number of visual tokens. “-” indicates that performance was not reported in the respective paper. All models presented in this table are trained using LLaVA-1.5 setup with Vicuna 7B. ‡- indicates further finetuning as reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>. <sub class="ltx_sub" id="S3.T5.9.2.2"><span class="ltx_text ltx_font_italic" id="S3.T5.9.2.2.1">I</span></sub> - indicates vision only sparsification and <sub class="ltx_sub" id="S3.T5.9.2.3"><span class="ltx_text ltx_font_italic" id="S3.T5.9.2.3.1">I|T</span></sub> indicates vision-language sparsification, as reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>.
</span></figcaption>
</figure>
<figure class="ltx_table" id="S3.T6">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T6.8" style="width:451.0pt;height:468.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-215.9pt,224.5pt) scale(0.510798405187229,0.510798405187229) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T6.8.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T6.8.8.9.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T6.8.8.9.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Row</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.2" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.9.1.2.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">Vision</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.4" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.9.1.4.1">LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T6.8.8.9.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">Data (M)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">Input</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T6.8.8.9.1.7" style="padding-left:2.0pt;padding-right:2.0pt;">#Visual</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">Vis. Enc.</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T6.8.8.9.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">TTFT</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.10" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.9.1.10.1">GQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.11" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.9.1.11.1">SQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.12" style="padding-left:2.0pt;padding-right:2.0pt;">Text</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.13" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.9.1.13.1">POPE</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.14" style="padding-left:2.0pt;padding-right:2.0pt;">LLaVA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.15" style="padding-left:2.0pt;padding-right:2.0pt;">MM-</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.16" style="padding-left:2.0pt;padding-right:2.0pt;">VQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.17" style="padding-left:2.0pt;padding-right:2.0pt;">Doc</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.18" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.9.1.18.1">MMMU</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T6.8.8.9.1.19" style="padding-left:2.0pt;padding-right:2.0pt;">Seed</td>
</tr>
<tr class="ltx_tr" id="S3.T6.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.4.4.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">Ann.</td>
<td class="ltx_td ltx_align_center" id="S3.T6.4.4.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">Encoder</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.4.4.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">(PT+IT)</td>
<td class="ltx_td ltx_align_center" id="S3.T6.4.4.4.8" style="padding-left:2.0pt;padding-right:2.0pt;">Res.</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.4.4.4.9" style="padding-left:2.0pt;padding-right:2.0pt;">Tokens</td>
<td class="ltx_td ltx_align_center" id="S3.T6.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Size(M)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T6.1.1.1.1.m1.1"><semantics id="S3.T6.1.1.1.1.m1.1a"><mo id="S3.T6.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T6.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T6.1.1.1.1.m1.1b"><ci id="S3.T6.1.1.1.1.m1.1.1.cmml" xref="S3.T6.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T6.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.2.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">(ms)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T6.2.2.2.2.m1.1"><semantics id="S3.T6.2.2.2.2.m1.1a"><mo id="S3.T6.2.2.2.2.m1.1.1" stretchy="false" xref="S3.T6.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T6.2.2.2.2.m1.1b"><ci id="S3.T6.2.2.2.2.m1.1.1.cmml" xref="S3.T6.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T6.2.2.2.2.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.4.4.4.10" style="padding-left:2.0pt;padding-right:2.0pt;">VQA</td>
<td class="ltx_td ltx_align_center" id="S3.T6.3.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">Bench<sup class="ltx_sup" id="S3.T6.3.3.3.3.1"><span class="ltx_text ltx_font_italic" id="S3.T6.3.3.3.3.1.1">W</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.4.4.4.11" style="padding-left:2.0pt;padding-right:2.0pt;">Vet</td>
<td class="ltx_td ltx_align_center" id="S3.T6.4.4.4.12" style="padding-left:2.0pt;padding-right:2.0pt;">v2</td>
<td class="ltx_td ltx_align_center" id="S3.T6.4.4.4.13" style="padding-left:2.0pt;padding-right:2.0pt;">VQA</td>
<td class="ltx_td ltx_align_center" id="S3.T6.4.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">Bench<sup class="ltx_sup" id="S3.T6.4.4.4.4.1"><span class="ltx_text ltx_font_italic" id="S3.T6.4.4.4.4.1.1">I</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.10.2">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="19" id="S3.T6.8.8.10.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">0.5B Model Comparison</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.11.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T6.8.8.11.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">R1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">nanoLLaVA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-SO400M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">Qw.1.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.11.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">384</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.11.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">729</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">430</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.11.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">535</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.10" style="padding-left:2.0pt;padding-right:2.0pt;">54.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.11" style="padding-left:2.0pt;padding-right:2.0pt;">59.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.12" style="padding-left:2.0pt;padding-right:2.0pt;">46.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.13" style="padding-left:2.0pt;padding-right:2.0pt;">84.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.16" style="padding-left:2.0pt;padding-right:2.0pt;">70.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.18" style="padding-left:2.0pt;padding-right:2.0pt;">30.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.11.3.19" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T6.5.5.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.5.5.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">R2</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">LLaVAOV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite><sup class="ltx_sup" id="S3.T6.5.5.5.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-SO400M</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">Qw.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.5.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">4.5+3.2</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">1152</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.5.5.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">7290</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.8" style="padding-left:2.0pt;padding-right:2.0pt;">430</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.5.5.5.9" style="padding-left:2.0pt;padding-right:2.0pt;">14124</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.11" style="padding-left:2.0pt;padding-right:2.0pt;">67.2</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.15" style="padding-left:2.0pt;padding-right:2.0pt;">29.1</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.5.5.5.17.1">70.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.18" style="padding-left:2.0pt;padding-right:2.0pt;">31.4</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.5.5.19" style="padding-left:2.0pt;padding-right:2.0pt;">65.5</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.12.4" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.12.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.1.1" style="background-color:#E3EFFB;">R3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.12.4.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.4.1" style="background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.12.4.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.5.1" style="background-color:#E3EFFB;">15+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.6.1" style="background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.12.4.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.7.1" style="background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.12.4.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.12.4.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.12.4.9.1" style="background-color:#E3EFFB;">166</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.10.1" style="background-color:#E3EFFB;">61.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.11.1" style="background-color:#E3EFFB;">61.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.12.1" style="background-color:#E3EFFB;">57.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.12.4.13.1" style="background-color:#E3EFFB;">87.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.14.1" style="background-color:#E3EFFB;">56.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.12.4.15.1" style="background-color:#E3EFFB;">31.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.16.1" style="background-color:#E3EFFB;">77.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.17.1" style="background-color:#E3EFFB;">61.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.18.1" style="background-color:#E3EFFB;">30.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.12.4.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.12.4.19.1" style="background-color:#E3EFFB;">65.6</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.13.5" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.13.5.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.13.5.1.1" style="background-color:#E3EFFB;">R4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.13.5.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.13.5.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.13.5.4.1" style="background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.13.5.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.13.5.5.1" style="background-color:#E3EFFB;">15+11.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.13.5.6.1" style="background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.13.5.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.13.5.7.1" style="background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.13.5.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.13.5.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.13.5.9.1" style="background-color:#E3EFFB;">166</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.13.5.10.1" style="background-color:#E3EFFB;">62.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.13.5.11.1" style="background-color:#E3EFFB;">80.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.13.5.12.1" style="background-color:#E3EFFB;">61.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.13.5.13.1" style="background-color:#E3EFFB;">87.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.13.5.14.1" style="background-color:#E3EFFB;">63.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.13.5.15.1" style="background-color:#E3EFFB;">31.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.13.5.16.1" style="background-color:#E3EFFB;">78.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.13.5.17.1" style="background-color:#E3EFFB;">66.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.13.5.18.1" style="background-color:#E3EFFB;">31.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.13.5.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.13.5.19.1" style="background-color:#E3EFFB;">68.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.14.6">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="19" id="S3.T6.8.8.14.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">1-2B Model Comparison</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.15.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T6.8.8.15.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">R5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">MobileVLMv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">ML.</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.15.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">1.2+3.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.6" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.15.7.7" style="padding-left:2.0pt;padding-right:2.0pt;">144</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.8" style="padding-left:2.0pt;padding-right:2.0pt;">304</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.15.7.9" style="padding-left:2.0pt;padding-right:2.0pt;">458</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.10" style="padding-left:2.0pt;padding-right:2.0pt;">59.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.11" style="padding-left:2.0pt;padding-right:2.0pt;">66.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.12" style="padding-left:2.0pt;padding-right:2.0pt;">52.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.13" style="padding-left:2.0pt;padding-right:2.0pt;">84.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.15.7.19" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.16.8" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.16.8.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.1.1" style="background-color:#E3EFFB;">R6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.16.8.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.4.1" style="background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.16.8.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.5.1" style="background-color:#E3EFFB;">15+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.6.1" style="background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.16.8.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.7.1" style="background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.16.8.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.16.8.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.16.8.9.1" style="background-color:#E3EFFB;">152</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.16.8.10.1" style="background-color:#E3EFFB;">63.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.11.1" style="background-color:#E3EFFB;">75.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.12.1" style="background-color:#E3EFFB;">64.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.13.1" style="background-color:#E3EFFB;">87.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.14.1" style="background-color:#E3EFFB;">65.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.15.1" style="background-color:#E3EFFB;">35.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.16.1" style="background-color:#E3EFFB;">79.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.17.1" style="background-color:#E3EFFB;">61.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.18.1" style="background-color:#E3EFFB;">34.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.16.8.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.16.8.19.1" style="background-color:#E3EFFB;">71.7</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.17.9" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.17.9.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.17.9.1.1" style="background-color:#E3EFFB;">R7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.17.9.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.17.9.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.17.9.4.1" style="background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.17.9.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.17.9.5.1" style="background-color:#E3EFFB;">15+11.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.17.9.6.1" style="background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.17.9.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.17.9.7.1" style="background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.17.9.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.17.9.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.17.9.9.1" style="background-color:#E3EFFB;">152</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.17.9.10.1" style="background-color:#E3EFFB;">63.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.17.9.11.1" style="background-color:#E3EFFB;">89.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.17.9.12.1" style="background-color:#E3EFFB;">66.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.17.9.13.1" style="background-color:#E3EFFB;">87.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.17.9.14.1" style="background-color:#E3EFFB;">67.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.17.9.15.1" style="background-color:#E3EFFB;">39.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.17.9.16.1" style="background-color:#E3EFFB;">80.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.17.9.17.1" style="background-color:#E3EFFB;">65.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.17.9.18.1" style="background-color:#E3EFFB;">37.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.17.9.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.17.9.19.1" style="background-color:#E3EFFB;">72.4</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.18.10">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T6.8.8.18.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">R8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.2" style="padding-left:2.0pt;padding-right:2.0pt;">DeepSeekVL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-SO400M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.4" style="padding-left:2.0pt;padding-right:2.0pt;">DS.</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.18.10.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.6" style="padding-left:2.0pt;padding-right:2.0pt;">384</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.18.10.7" style="padding-left:2.0pt;padding-right:2.0pt;">576</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.8" style="padding-left:2.0pt;padding-right:2.0pt;">430</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.18.10.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.13" style="padding-left:2.0pt;padding-right:2.0pt;">87.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.15" style="padding-left:2.0pt;padding-right:2.0pt;">34.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.18" style="padding-left:2.0pt;padding-right:2.0pt;">32.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.18.10.19" style="padding-left:2.0pt;padding-right:2.0pt;">66.7</td>
</tr>
<tr class="ltx_tr" id="S3.T6.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.6.6.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">R9</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">MM1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite><sup class="ltx_sup" id="S3.T6.6.6.6.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-H</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.6.6.6.5" style="padding-left:2.0pt;padding-right:2.0pt;">3000+1.5</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">1344</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.6.6.6.7" style="padding-left:2.0pt;padding-right:2.0pt;">720</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.8" style="padding-left:2.0pt;padding-right:2.0pt;">632</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.6.6.6.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.11" style="padding-left:2.0pt;padding-right:2.0pt;">62.3</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.12" style="padding-left:2.0pt;padding-right:2.0pt;">68.2</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.13" style="padding-left:2.0pt;padding-right:2.0pt;">87.4</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.14" style="padding-left:2.0pt;padding-right:2.0pt;">67.5</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.15" style="padding-left:2.0pt;padding-right:2.0pt;">39.4</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.17" style="padding-left:2.0pt;padding-right:2.0pt;">68.4</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.18" style="padding-left:2.0pt;padding-right:2.0pt;">33.2</td>
<td class="ltx_td ltx_align_center" id="S3.T6.6.6.6.19" style="padding-left:2.0pt;padding-right:2.0pt;">65.6</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.19.11" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.19.11.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.1.1" style="background-color:#E3EFFB;">R10</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.19.11.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.4.1" style="background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.19.11.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.5.1" style="background-color:#E3EFFB;">15+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.6.1" style="background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.19.11.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.7.1" style="background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.19.11.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.19.11.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.9.1" style="background-color:#E3EFFB;">233</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.19.11.10.1" style="background-color:#E3EFFB;">64.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.11.1" style="background-color:#E3EFFB;">74.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.12.1" style="background-color:#E3EFFB;">66.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.19.11.13.1" style="background-color:#E3EFFB;">88.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.14.1" style="background-color:#E3EFFB;">66.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.15.1" style="background-color:#E3EFFB;">37.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.16.1" style="background-color:#E3EFFB;">79.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.17.1" style="background-color:#E3EFFB;">67.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.18.1" style="background-color:#E3EFFB;">33.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.19.11.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.19.11.19.1" style="background-color:#E3EFFB;">71.4</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.20.12" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.20.12.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.20.12.1.1" style="background-color:#E3EFFB;">R11</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.20.12.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.20.12.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.20.12.4.1" style="background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.20.12.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.20.12.5.1" style="background-color:#E3EFFB;">15+11.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.20.12.6.1" style="background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.20.12.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.20.12.7.1" style="background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.20.12.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.20.12.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.20.12.9.1" style="background-color:#E3EFFB;">233</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.20.12.10.1" style="background-color:#E3EFFB;">63.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.20.12.11.1" style="background-color:#E3EFFB;">90.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.20.12.12.1" style="background-color:#E3EFFB;">68.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.20.12.13.1" style="background-color:#E3EFFB;">87.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.20.12.14.1" style="background-color:#E3EFFB;">72.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.20.12.15.1" style="background-color:#E3EFFB;">41.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.20.12.16.1" style="background-color:#E3EFFB;">80.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.20.12.17.1" style="background-color:#E3EFFB;">72.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.20.12.18.1" style="background-color:#E3EFFB;">38.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.20.12.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.20.12.19.1" style="background-color:#E3EFFB;">73.0</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.21.13">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="19" id="S3.T6.8.8.21.13.1" style="padding-left:2.0pt;padding-right:2.0pt;">7B Model Comparison</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.22.14">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T6.8.8.22.14.1" style="padding-left:2.0pt;padding-right:2.0pt;">R12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.2" style="padding-left:2.0pt;padding-right:2.0pt;">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-g/14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.4" style="padding-left:2.0pt;padding-right:2.0pt;">Vic.</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.22.14.5" style="padding-left:2.0pt;padding-right:2.0pt;">129+1.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.6" style="padding-left:2.0pt;padding-right:2.0pt;">224</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.22.14.7" style="padding-left:2.0pt;padding-right:2.0pt;">32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.8" style="padding-left:2.0pt;padding-right:2.0pt;">1012</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.22.14.9" style="padding-left:2.0pt;padding-right:2.0pt;">302</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.10" style="padding-left:2.0pt;padding-right:2.0pt;">49.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.11" style="padding-left:2.0pt;padding-right:2.0pt;">60.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.12" style="padding-left:2.0pt;padding-right:2.0pt;">50.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.14" style="padding-left:2.0pt;padding-right:2.0pt;">60.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.15" style="padding-left:2.0pt;padding-right:2.0pt;">26.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.18" style="padding-left:2.0pt;padding-right:2.0pt;">30.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.22.14.19" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.23.15" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.23.15.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.1.1" style="background-color:#E3EFFB;">R13</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.23.15.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.4.1" style="background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.23.15.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.5.1" style="background-color:#E3EFFB;">0.5+0.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.6.1" style="background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.23.15.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.7.1" style="background-color:#E3EFFB;">16</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.23.15.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.23.15.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.23.15.9.1" style="background-color:#E3EFFB;">150</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.10.1" style="background-color:#E3EFFB;">60.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.11.1" style="background-color:#E3EFFB;">69.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.12.1" style="background-color:#E3EFFB;">53.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.13.1" style="background-color:#E3EFFB;">82.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.14.1" style="background-color:#E3EFFB;">60.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.15.1" style="background-color:#E3EFFB;">27.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.16.1" style="background-color:#E3EFFB;">74.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.17.1" style="background-color:#E3EFFB;">17.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.18.1" style="background-color:#E3EFFB;">36.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.23.15.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.23.15.19.1" style="background-color:#E3EFFB;">63.7</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.24.16" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.24.16.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.24.16.1.1" style="background-color:#E3EFFB;">R14</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.24.16.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.24.16.4.1" style="background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.24.16.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.24.16.5.1" style="background-color:#E3EFFB;">15+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.24.16.6.1" style="background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.24.16.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.24.16.7.1" style="background-color:#E3EFFB;">16</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.24.16.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.9.1" style="background-color:#E3EFFB;">150</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.10.1" style="background-color:#E3EFFB;">62.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.11.1" style="background-color:#E3EFFB;">75.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.12.1" style="background-color:#E3EFFB;">57.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.13.1" style="background-color:#E3EFFB;">83.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.14.1" style="background-color:#E3EFFB;">64.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.15.1" style="background-color:#E3EFFB;">31.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.16.1" style="background-color:#E3EFFB;">77.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.17.1" style="background-color:#E3EFFB;">29.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.18.1" style="background-color:#E3EFFB;">37.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.24.16.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.24.16.19.1" style="background-color:#E3EFFB;">68.8</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.25.17">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T6.8.8.25.17.1" style="padding-left:2.0pt;padding-right:2.0pt;">R15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.2" style="padding-left:2.0pt;padding-right:2.0pt;">MobileVLMv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.4" style="padding-left:2.0pt;padding-right:2.0pt;">Vic.</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.25.17.5" style="padding-left:2.0pt;padding-right:2.0pt;">1.2+3.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.6" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.25.17.7" style="padding-left:2.0pt;padding-right:2.0pt;">144</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.8" style="padding-left:2.0pt;padding-right:2.0pt;">304</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.25.17.9" style="padding-left:2.0pt;padding-right:2.0pt;">460</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.10" style="padding-left:2.0pt;padding-right:2.0pt;">62.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.11" style="padding-left:2.0pt;padding-right:2.0pt;">74.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.12" style="padding-left:2.0pt;padding-right:2.0pt;">62.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.13" style="padding-left:2.0pt;padding-right:2.0pt;">85.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.25.17.19" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.26.18">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.26.18.1" style="padding-left:2.0pt;padding-right:2.0pt;">R16</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.2" style="padding-left:2.0pt;padding-right:2.0pt;">ConvLLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.3" style="padding-left:2.0pt;padding-right:2.0pt;">ConvNeXT-L</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.4" style="padding-left:2.0pt;padding-right:2.0pt;">Vic.</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.26.18.5" style="padding-left:2.0pt;padding-right:2.0pt;">4.9+0.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.6" style="padding-left:2.0pt;padding-right:2.0pt;">768</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.26.18.7" style="padding-left:2.0pt;padding-right:2.0pt;">144</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.8" style="padding-left:2.0pt;padding-right:2.0pt;">200</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.26.18.9" style="padding-left:2.0pt;padding-right:2.0pt;">496</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.12" style="padding-left:2.0pt;padding-right:2.0pt;">59.1</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.13" style="padding-left:2.0pt;padding-right:2.0pt;">87.3</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.15" style="padding-left:2.0pt;padding-right:2.0pt;">44.8</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.17" style="padding-left:2.0pt;padding-right:2.0pt;">44.8</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.18" style="padding-left:2.0pt;padding-right:2.0pt;">36.3</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.26.18.19" style="padding-left:2.0pt;padding-right:2.0pt;">68.8</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.27.19" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.27.19.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.1.1" style="background-color:#E3EFFB;">R17</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.27.19.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.4.1" style="background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.27.19.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.5.1" style="background-color:#E3EFFB;">0.5+0.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.6.1" style="background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.27.19.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.7.1" style="background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.27.19.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.27.19.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.27.19.9.1" style="background-color:#E3EFFB;">387</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.10.1" style="background-color:#E3EFFB;">62.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.11.1" style="background-color:#E3EFFB;">67.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.12.1" style="background-color:#E3EFFB;">62.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.13.1" style="background-color:#E3EFFB;">87.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.14.1" style="background-color:#E3EFFB;">63.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.15.1" style="background-color:#E3EFFB;">31.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.16.1" style="background-color:#E3EFFB;">78.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.17.1" style="background-color:#E3EFFB;">32.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.18.1" style="background-color:#E3EFFB;">34.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.27.19.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.27.19.19.1" style="background-color:#E3EFFB;">68.2</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.28.20" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.28.20.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.1.1" style="background-color:#E3EFFB;">R18</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.28.20.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.4.1" style="background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.28.20.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.5.1" style="background-color:#E3EFFB;">0.5+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.6.1" style="background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.28.20.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.7.1" style="background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.28.20.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.28.20.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.28.20.9.1" style="background-color:#E3EFFB;">387</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.10.1" style="background-color:#E3EFFB;">63.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.11.1" style="background-color:#E3EFFB;">73.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.12.1" style="background-color:#E3EFFB;">67.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.13.1" style="background-color:#E3EFFB;">86.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.14.1" style="background-color:#E3EFFB;">63.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.15.1" style="background-color:#E3EFFB;">33.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.16.1" style="background-color:#E3EFFB;">79.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.17.1" style="background-color:#E3EFFB;">57.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.18.1" style="background-color:#E3EFFB;">36.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.28.20.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.28.20.19.1" style="background-color:#E3EFFB;">69.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.29.21" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.29.21.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.1.1" style="background-color:#E3EFFB;">R19</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.29.21.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.4.1" style="background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.29.21.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.5.1" style="background-color:#E3EFFB;">15+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.6.1" style="background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.29.21.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.7.1" style="background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.29.21.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.29.21.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.29.21.9.1" style="background-color:#E3EFFB;">387</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.10.1" style="background-color:#E3EFFB;">65.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.11.1" style="background-color:#E3EFFB;">78.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.12.1" style="background-color:#E3EFFB;">69.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.13.1" style="background-color:#E3EFFB;">87.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.14.1" style="background-color:#E3EFFB;">67.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.15.1" style="background-color:#E3EFFB;">42.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.16.1" style="background-color:#E3EFFB;">81.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.17.1" style="background-color:#E3EFFB;">65.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.18.1" style="background-color:#E3EFFB;">37.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.29.21.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.29.21.19.1" style="background-color:#E3EFFB;">73.7</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.30.22" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.30.22.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.1.1" style="background-color:#E3EFFB;">R20</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.30.22.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.4.1" style="background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.30.22.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.5.1" style="background-color:#E3EFFB;">15+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.6.1" style="background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.30.22.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.7.1" style="background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.30.22.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.30.22.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.9.1" style="background-color:#E3EFFB;">446</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.30.22.10.1" style="background-color:#E3EFFB;">65.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.11.1" style="background-color:#E3EFFB;">85.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.12.1" style="background-color:#E3EFFB;">69.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.13.1" style="background-color:#E3EFFB;">87.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.14.1" style="background-color:#E3EFFB;">73.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.15.1" style="background-color:#E3EFFB;">41.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.16.1" style="background-color:#E3EFFB;">81.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.17.1" style="background-color:#E3EFFB;">66.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.30.22.18.1" style="background-color:#E3EFFB;">43.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.30.22.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.30.22.19.1" style="background-color:#E3EFFB;">75.3</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.31.23" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.31.23.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.31.23.1.1" style="background-color:#E3EFFB;">R21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.31.23.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.31.23.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.31.23.4.1" style="background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.31.23.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.31.23.5.1" style="background-color:#E3EFFB;">15+11.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.31.23.6.1" style="background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.31.23.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.31.23.7.1" style="background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.31.23.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.31.23.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.31.23.9.1" style="background-color:#E3EFFB;">446</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.31.23.10.1" style="background-color:#E3EFFB;">64.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.31.23.11.1" style="background-color:#E3EFFB;">96.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.31.23.12.1" style="background-color:#E3EFFB;">72.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.31.23.13.1" style="background-color:#E3EFFB;">87.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.31.23.14.1" style="background-color:#E3EFFB;">73.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.31.23.15.1" style="background-color:#E3EFFB;">45.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.31.23.16.1" style="background-color:#E3EFFB;">81.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.31.23.17.1" style="background-color:#E3EFFB;">72.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.31.23.18.1" style="background-color:#E3EFFB;">48.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.31.23.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.31.23.19.1" style="background-color:#E3EFFB;">74.7</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.32.24">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T6.8.8.32.24.1" style="padding-left:2.0pt;padding-right:2.0pt;">R22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.2" style="padding-left:2.0pt;padding-right:2.0pt;">Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-G/14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.4" style="padding-left:2.0pt;padding-right:2.0pt;">Qw.</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.32.24.5" style="padding-left:2.0pt;padding-right:2.0pt;">1400+50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.6" style="padding-left:2.0pt;padding-right:2.0pt;">448</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.32.24.7" style="padding-left:2.0pt;padding-right:2.0pt;">256</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.8" style="padding-left:2.0pt;padding-right:2.0pt;">1844</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.32.24.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.10" style="padding-left:2.0pt;padding-right:2.0pt;">59.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.11" style="padding-left:2.0pt;padding-right:2.0pt;">67.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.12" style="padding-left:2.0pt;padding-right:2.0pt;">63.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.16" style="padding-left:2.0pt;padding-right:2.0pt;">79.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.17" style="padding-left:2.0pt;padding-right:2.0pt;">65.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.32.24.19" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.33.25">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.33.25.1" style="padding-left:2.0pt;padding-right:2.0pt;">R23</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.2" style="padding-left:2.0pt;padding-right:2.0pt;">Qwen-VL-Chat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-G/14</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.4" style="padding-left:2.0pt;padding-right:2.0pt;">Qw.</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.33.25.5" style="padding-left:2.0pt;padding-right:2.0pt;">1400+50</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.6" style="padding-left:2.0pt;padding-right:2.0pt;">448</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.33.25.7" style="padding-left:2.0pt;padding-right:2.0pt;">256</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.8" style="padding-left:2.0pt;padding-right:2.0pt;">1844</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.33.25.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.10" style="padding-left:2.0pt;padding-right:2.0pt;">57.5</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.11" style="padding-left:2.0pt;padding-right:2.0pt;">68.2</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.12" style="padding-left:2.0pt;padding-right:2.0pt;">61.5</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.16" style="padding-left:2.0pt;padding-right:2.0pt;">78.2</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.17" style="padding-left:2.0pt;padding-right:2.0pt;">62.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.33.25.19" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.34.26">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.34.26.1" style="padding-left:2.0pt;padding-right:2.0pt;">R24</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.2" style="padding-left:2.0pt;padding-right:2.0pt;">ConvLLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.3" style="padding-left:2.0pt;padding-right:2.0pt;">ConvNeXT-L</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.4" style="padding-left:2.0pt;padding-right:2.0pt;">Vic.</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.34.26.5" style="padding-left:2.0pt;padding-right:2.0pt;">4.9+0.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.6" style="padding-left:2.0pt;padding-right:2.0pt;">1024</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.34.26.7" style="padding-left:2.0pt;padding-right:2.0pt;">256</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.8" style="padding-left:2.0pt;padding-right:2.0pt;">200</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.34.26.9" style="padding-left:2.0pt;padding-right:2.0pt;">1157</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.12" style="padding-left:2.0pt;padding-right:2.0pt;">62.5</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.13" style="padding-left:2.0pt;padding-right:2.0pt;">87.7</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.34.26.15.1">44.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.17" style="padding-left:2.0pt;padding-right:2.0pt;">48.5</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.18" style="padding-left:2.0pt;padding-right:2.0pt;">35.1</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.34.26.19" style="padding-left:2.0pt;padding-right:2.0pt;">69.3</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.35.27" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.35.27.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.1.1" style="background-color:#E3EFFB;">R25</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.35.27.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.4.1" style="background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.35.27.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.5.1" style="background-color:#E3EFFB;">0.5+0.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.6.1" style="background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.35.27.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.7.1" style="background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.35.27.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.35.27.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.35.27.9.1" style="background-color:#E3EFFB;">577</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.10.1" style="background-color:#E3EFFB;">63.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.11.1" style="background-color:#E3EFFB;">67.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.12.1" style="background-color:#E3EFFB;">64.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.35.27.13.1" style="background-color:#E3EFFB;">88.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.14.1" style="background-color:#E3EFFB;">64.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.15.1" style="background-color:#E3EFFB;">31.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.16.1" style="background-color:#E3EFFB;">79.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.17.1" style="background-color:#E3EFFB;">35.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.18.1" style="background-color:#E3EFFB;">35.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.35.27.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.35.27.19.1" style="background-color:#E3EFFB;">68.5</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.36.28" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.36.28.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.1.1" style="background-color:#E3EFFB;">R26</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.36.28.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.4.1" style="background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.36.28.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.5.1" style="background-color:#E3EFFB;">0.5+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.6.1" style="background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.36.28.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.7.1" style="background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.36.28.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.36.28.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.36.28.9.1" style="background-color:#E3EFFB;">577</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.10.1" style="background-color:#E3EFFB;">63.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.11.1" style="background-color:#E3EFFB;">74.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.12.1" style="background-color:#E3EFFB;">67.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.13.1" style="background-color:#E3EFFB;">87.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.14.1" style="background-color:#E3EFFB;">66.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.15.1" style="background-color:#E3EFFB;">32.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.16.1" style="background-color:#E3EFFB;">79.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.17.1" style="background-color:#E3EFFB;">62.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.18.1" style="background-color:#E3EFFB;">37.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.36.28.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.36.28.19.1" style="background-color:#E3EFFB;">69.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.37.29" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.37.29.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.1.1" style="background-color:#E3EFFB;">R27</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.37.29.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.4.1" style="background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.37.29.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.5.1" style="background-color:#E3EFFB;">15+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.6.1" style="background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.37.29.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.7.1" style="background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.37.29.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.37.29.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.37.29.9.1" style="background-color:#E3EFFB;">577</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.10.1" style="background-color:#E3EFFB;">65.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.11.1" style="background-color:#E3EFFB;">80.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.12.1" style="background-color:#E3EFFB;">70.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.13.1" style="background-color:#E3EFFB;">87.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.14.1" style="background-color:#E3EFFB;">71.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.15.1" style="background-color:#E3EFFB;">40.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.16.1" style="background-color:#E3EFFB;">81.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.17.1" style="background-color:#E3EFFB;">72.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.18.1" style="background-color:#E3EFFB;">36.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.37.29.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.37.29.19.1" style="background-color:#E3EFFB;">73.5</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.38.30" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.38.30.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.38.30.1.1" style="background-color:#E3EFFB;">R28</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.38.30.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.38.30.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.38.30.4.1" style="background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.38.30.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.38.30.5.1" style="background-color:#E3EFFB;">15+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.38.30.6.1" style="background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.38.30.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.38.30.7.1" style="background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.38.30.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.38.30.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.38.30.9.1" style="background-color:#E3EFFB;">641</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.38.30.10.1" style="background-color:#E3EFFB;">65.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.38.30.11.1" style="background-color:#E3EFFB;">84.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.38.30.12.1" style="background-color:#E3EFFB;">72.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.38.30.13.1" style="background-color:#E3EFFB;">87.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.38.30.14.1" style="background-color:#E3EFFB;">75.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.38.30.15.1" style="background-color:#E3EFFB;">44.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.38.30.16.1" style="background-color:#E3EFFB;">81.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.38.30.17.1" style="background-color:#E3EFFB;">73.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.38.30.18.1" style="background-color:#E3EFFB;">46.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.38.30.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.38.30.19.1" style="background-color:#E3EFFB;">75.1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.39.31">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T6.8.8.39.31.1" style="padding-left:2.0pt;padding-right:2.0pt;">R29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.2" style="padding-left:2.0pt;padding-right:2.0pt;">LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.4" style="padding-left:2.0pt;padding-right:2.0pt;">Vic.</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.39.31.5" style="padding-left:2.0pt;padding-right:2.0pt;">0.5+0.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.6" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.39.31.7" style="padding-left:2.0pt;padding-right:2.0pt;">576</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.8" style="padding-left:2.0pt;padding-right:2.0pt;">304</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.8.8.39.31.9" style="padding-left:2.0pt;padding-right:2.0pt;">1297</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.10" style="padding-left:2.0pt;padding-right:2.0pt;">62.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.11" style="padding-left:2.0pt;padding-right:2.0pt;">70.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.12" style="padding-left:2.0pt;padding-right:2.0pt;">58.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.13" style="padding-left:2.0pt;padding-right:2.0pt;">85.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.14" style="padding-left:2.0pt;padding-right:2.0pt;">59.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.15" style="padding-left:2.0pt;padding-right:2.0pt;">31.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.16" style="padding-left:2.0pt;padding-right:2.0pt;">76.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.17" style="padding-left:2.0pt;padding-right:2.0pt;">28.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.18" style="padding-left:2.0pt;padding-right:2.0pt;">35.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.39.31.19" style="padding-left:2.0pt;padding-right:2.0pt;">66.1</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.40.32">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.40.32.1" style="padding-left:2.0pt;padding-right:2.0pt;">R30</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.2" style="padding-left:2.0pt;padding-right:2.0pt;">MobileVLMv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.4" style="padding-left:2.0pt;padding-right:2.0pt;">Vic.</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.40.32.5" style="padding-left:2.0pt;padding-right:2.0pt;">1.2+3.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.6" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.40.32.7" style="padding-left:2.0pt;padding-right:2.0pt;">576</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.8" style="padding-left:2.0pt;padding-right:2.0pt;">304</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.40.32.9" style="padding-left:2.0pt;padding-right:2.0pt;">1297</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.10" style="padding-left:2.0pt;padding-right:2.0pt;">64.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.11" style="padding-left:2.0pt;padding-right:2.0pt;">74.8</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.12" style="padding-left:2.0pt;padding-right:2.0pt;">66.8</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.13" style="padding-left:2.0pt;padding-right:2.0pt;">86.1</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.40.32.19" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.41.33">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.41.33.1" style="padding-left:2.0pt;padding-right:2.0pt;">R31</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.2" style="padding-left:2.0pt;padding-right:2.0pt;">ShareGPT4V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.4" style="padding-left:2.0pt;padding-right:2.0pt;">Vic.</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.41.33.5" style="padding-left:2.0pt;padding-right:2.0pt;">1.2+0.7</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.6" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.41.33.7" style="padding-left:2.0pt;padding-right:2.0pt;">576</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.8" style="padding-left:2.0pt;padding-right:2.0pt;">304</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.41.33.9" style="padding-left:2.0pt;padding-right:2.0pt;">1297</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.10" style="padding-left:2.0pt;padding-right:2.0pt;">63.3</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.11" style="padding-left:2.0pt;padding-right:2.0pt;">68.4</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.12" style="padding-left:2.0pt;padding-right:2.0pt;">60.4</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.13" style="padding-left:2.0pt;padding-right:2.0pt;">85.7</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.14" style="padding-left:2.0pt;padding-right:2.0pt;">72.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.15" style="padding-left:2.0pt;padding-right:2.0pt;">37.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.16" style="padding-left:2.0pt;padding-right:2.0pt;">80.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.41.33.19" style="padding-left:2.0pt;padding-right:2.0pt;">69.7</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.42.34">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.42.34.1" style="padding-left:2.0pt;padding-right:2.0pt;">R32</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.2" style="padding-left:2.0pt;padding-right:2.0pt;">ViTamin <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViTamin-L</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.4" style="padding-left:2.0pt;padding-right:2.0pt;">Vic.</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.42.34.5" style="padding-left:2.0pt;padding-right:2.0pt;">0.5+0.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.6" style="padding-left:2.0pt;padding-right:2.0pt;">384</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.42.34.7" style="padding-left:2.0pt;padding-right:2.0pt;">576</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.8" style="padding-left:2.0pt;padding-right:2.0pt;">333</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.42.34.9" style="padding-left:2.0pt;padding-right:2.0pt;">1308</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.10" style="padding-left:2.0pt;padding-right:2.0pt;">61.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.11" style="padding-left:2.0pt;padding-right:2.0pt;">67.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.12" style="padding-left:2.0pt;padding-right:2.0pt;">59.8</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.13" style="padding-left:2.0pt;padding-right:2.0pt;">85.5</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.14" style="padding-left:2.0pt;padding-right:2.0pt;">66.1</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.15" style="padding-left:2.0pt;padding-right:2.0pt;">33.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.16" style="padding-left:2.0pt;padding-right:2.0pt;">78.9</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.42.34.19" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.43.35">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.43.35.1" style="padding-left:2.0pt;padding-right:2.0pt;">R33</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.2" style="padding-left:2.0pt;padding-right:2.0pt;">ConvLLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.3" style="padding-left:2.0pt;padding-right:2.0pt;">ConvNeXT-L</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.4" style="padding-left:2.0pt;padding-right:2.0pt;">Vic.</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.43.35.5" style="padding-left:2.0pt;padding-right:2.0pt;">4.9+0.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.6" style="padding-left:2.0pt;padding-right:2.0pt;">1536</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.43.35.7" style="padding-left:2.0pt;padding-right:2.0pt;">576</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.8" style="padding-left:2.0pt;padding-right:2.0pt;">200</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.43.35.9" style="padding-left:2.0pt;padding-right:2.0pt;">2740</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.12" style="padding-left:2.0pt;padding-right:2.0pt;">65.8</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.13" style="padding-left:2.0pt;padding-right:2.0pt;">87.3</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.15" style="padding-left:2.0pt;padding-right:2.0pt;">45.9</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.17" style="padding-left:2.0pt;padding-right:2.0pt;">59.0</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.18" style="padding-left:2.0pt;padding-right:2.0pt;">35.8</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.43.35.19" style="padding-left:2.0pt;padding-right:2.0pt;">70.2</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.44.36">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.44.36.1" style="padding-left:2.0pt;padding-right:2.0pt;">R34</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.2" style="padding-left:2.0pt;padding-right:2.0pt;">VILA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.4" style="padding-left:2.0pt;padding-right:2.0pt;">L-2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.44.36.5" style="padding-left:2.0pt;padding-right:2.0pt;">50+1</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.6" style="padding-left:2.0pt;padding-right:2.0pt;">336</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.44.36.7" style="padding-left:2.0pt;padding-right:2.0pt;">576</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.8" style="padding-left:2.0pt;padding-right:2.0pt;">304</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.44.36.9" style="padding-left:2.0pt;padding-right:2.0pt;">1297</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.10" style="padding-left:2.0pt;padding-right:2.0pt;">62.3</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.11" style="padding-left:2.0pt;padding-right:2.0pt;">68.2</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.12" style="padding-left:2.0pt;padding-right:2.0pt;">64.4</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.13" style="padding-left:2.0pt;padding-right:2.0pt;">85.5</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.14" style="padding-left:2.0pt;padding-right:2.0pt;">69.7</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.15" style="padding-left:2.0pt;padding-right:2.0pt;">34.9</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.16" style="padding-left:2.0pt;padding-right:2.0pt;">79.9</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.44.36.19" style="padding-left:2.0pt;padding-right:2.0pt;">62.8</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.45.37">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.45.37.1" style="padding-left:2.0pt;padding-right:2.0pt;">R35</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.2" style="padding-left:2.0pt;padding-right:2.0pt;">LLaVA-FlexAttn <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.4" style="padding-left:2.0pt;padding-right:2.0pt;">Vic.</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.45.37.5" style="padding-left:2.0pt;padding-right:2.0pt;">0.5+0.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.6" style="padding-left:2.0pt;padding-right:2.0pt;">1008</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.45.37.7" style="padding-left:2.0pt;padding-right:2.0pt;">576</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.8" style="padding-left:2.0pt;padding-right:2.0pt;">304</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.45.37.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.10" style="padding-left:2.0pt;padding-right:2.0pt;">62.2</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.12" style="padding-left:2.0pt;padding-right:2.0pt;">48.9</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.13" style="padding-left:2.0pt;padding-right:2.0pt;">85.9</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.15" style="padding-left:2.0pt;padding-right:2.0pt;">29.4</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.16" style="padding-left:2.0pt;padding-right:2.0pt;">78.7</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.45.37.19" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T6.7.7.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.7.7.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">R36</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">MM1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite><sup class="ltx_sup" id="S3.T6.7.7.7.1.1">∗</sup>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-H</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.7.7.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">3000+1.5</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.6" style="padding-left:2.0pt;padding-right:2.0pt;">1344</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.7.7.7.7" style="padding-left:2.0pt;padding-right:2.0pt;">720</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.8" style="padding-left:2.0pt;padding-right:2.0pt;">632</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.7.7.7.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.11" style="padding-left:2.0pt;padding-right:2.0pt;">72.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.12" style="padding-left:2.0pt;padding-right:2.0pt;">72.8</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.13" style="padding-left:2.0pt;padding-right:2.0pt;">86.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.7.7.7.14.1">81.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.15" style="padding-left:2.0pt;padding-right:2.0pt;">42.1</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.7.7.7.16.1">82.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.17" style="padding-left:2.0pt;padding-right:2.0pt;">76.8</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.18" style="padding-left:2.0pt;padding-right:2.0pt;">37.0</td>
<td class="ltx_td ltx_align_center" id="S3.T6.7.7.7.19" style="padding-left:2.0pt;padding-right:2.0pt;">69.9</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.8.2" style="padding-left:2.0pt;padding-right:2.0pt;">R37</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.1" style="padding-left:2.0pt;padding-right:2.0pt;">LLaVA-NeXT<sup class="ltx_sup" id="S3.T6.8.8.8.1.1">†</sup><sup class="ltx_sup" id="S3.T6.8.8.8.1.2">∗</sup>
</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.3" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-L/14</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.4" style="padding-left:2.0pt;padding-right:2.0pt;">L-3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.8.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.6" style="padding-left:2.0pt;padding-right:2.0pt;">672</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.8.7" style="padding-left:2.0pt;padding-right:2.0pt;">2880</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.8" style="padding-left:2.0pt;padding-right:2.0pt;">304</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.8.9" style="padding-left:2.0pt;padding-right:2.0pt;">20347</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.10" style="padding-left:2.0pt;padding-right:2.0pt;">65.2</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.11" style="padding-left:2.0pt;padding-right:2.0pt;">72.8</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.12" style="padding-left:2.0pt;padding-right:2.0pt;">64.6</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.14" style="padding-left:2.0pt;padding-right:2.0pt;">80.1</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.17" style="padding-left:2.0pt;padding-right:2.0pt;">78.2</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.18" style="padding-left:2.0pt;padding-right:2.0pt;">41.7</td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.8.19" style="padding-left:2.0pt;padding-right:2.0pt;">72.7</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.46.38" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.46.38.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.1.1" style="background-color:#E3EFFB;">R38</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.46.38.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.4.1" style="background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.46.38.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.5.1" style="background-color:#E3EFFB;">15+6.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.6.1" style="background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.46.38.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.7.1" style="background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.46.38.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.46.38.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.46.38.9.1" style="background-color:#E3EFFB;">641</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.46.38.10.1" style="background-color:#E3EFFB;">66.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.11.1" style="background-color:#E3EFFB;">87.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.12.1" style="background-color:#E3EFFB;">73.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.46.38.13.1" style="background-color:#E3EFFB;">87.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.14.1" style="background-color:#E3EFFB;">72.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.15.1" style="background-color:#E3EFFB;">47.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.16.1" style="background-color:#E3EFFB;">82.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.17.1" style="background-color:#E3EFFB;">78.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.46.38.18.1" style="background-color:#E3EFFB;">42.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.46.38.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.46.38.19.1" style="background-color:#E3EFFB;">75.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.47.39" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.47.39.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.47.39.1.1" style="background-color:#E3EFFB;">R39</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.47.39.2.1" style="background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.47.39.3.1" style="background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.47.39.4.1" style="background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.47.39.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.47.39.5.1" style="background-color:#E3EFFB;">15+11.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.47.39.6.1" style="background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.47.39.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.47.39.7.1" style="background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.47.39.8.1" style="background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.47.39.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.47.39.9.1" style="background-color:#E3EFFB;">641</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.47.39.10.1" style="background-color:#E3EFFB;">64.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.47.39.11.1" style="background-color:#E3EFFB;">96.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.47.39.12.1" style="background-color:#E3EFFB;">74.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.47.39.13.1" style="background-color:#E3EFFB;">87.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.47.39.14.1" style="background-color:#E3EFFB;">73.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.47.39.15.1" style="background-color:#E3EFFB;">49.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.47.39.16.1" style="background-color:#E3EFFB;">82.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.47.39.17.1" style="background-color:#E3EFFB;">78.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.8.8.47.39.18.1" style="background-color:#E3EFFB;">46.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.47.39.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.47.39.19.1" style="background-color:#E3EFFB;">75.1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.48.40">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="19" id="S3.T6.8.8.48.40.1" style="padding-left:2.0pt;padding-right:2.0pt;">VLMs with Multiple Vision Encoders and 8B LLM</td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.49.41">
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T6.8.8.49.41.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.49.41.2" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.49.41.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.49.41.3.1" style="background-color:#F2F2F2;">ConvNeXT-L</span></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.49.41.4" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T6.8.8.49.41.5" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.49.41.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.49.41.6.1" style="background-color:#F2F2F2;">1536</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T6.8.8.49.41.7" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.49.41.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.49.41.8.1" style="background-color:#F2F2F2;">200</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T6.8.8.49.41.9" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.49.41.10" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.49.41.11" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.49.41.12" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.49.41.13" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.49.41.14" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.49.41.15" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.49.41.16" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.49.41.17" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.49.41.18" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.49.41.19" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.50.42" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.8.8.50.42.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.1.1" style="background-color:#F2F2F2;">R40</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.2.1" style="background-color:#F2F2F2;">MiniGemini-HD<sup class="ltx_sup" id="S3.T6.8.8.50.42.2.1.1">†</sup></span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.3.1" style="background-color:#F2F2F2;">ViT-L/14</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.4.1" style="background-color:#F2F2F2;">L-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.50.42.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.5.1" style="background-color:#F2F2F2;">1.5+1.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.6.1" style="background-color:#F2F2F2;">672</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.50.42.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.7.1" style="background-color:#F2F2F2;">2880</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.8.1" style="background-color:#F2F2F2;">304</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.8.8.50.42.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.9.1" style="background-color:#F2F2F2;">21832</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.10.1" style="background-color:#F2F2F2;">64.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.11.1" style="background-color:#F2F2F2;">75.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.12.1" style="background-color:#F2F2F2;">70.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.13.1" style="background-color:#F2F2F2;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.14.1" style="background-color:#F2F2F2;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.15.1" style="background-color:#F2F2F2;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.16.1" style="background-color:#F2F2F2;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.17.1" style="background-color:#F2F2F2;">74.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.18.1" style="background-color:#F2F2F2;">37.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.50.42.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.50.42.19.1" style="background-color:#F2F2F2;">73.2</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.51.43">
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T6.8.8.51.43.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.51.43.2" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.51.43.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.51.43.3.1" style="background-color:#F2F2F2;">ViT-SO400M</span></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.51.43.4" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T6.8.8.51.43.5" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.51.43.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.51.43.6.1" style="background-color:#F2F2F2;">384</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T6.8.8.51.43.7" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.8.8.51.43.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.51.43.8.1" style="background-color:#F2F2F2;">430</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T6.8.8.51.43.9" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.51.43.10" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.51.43.11" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.51.43.12" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.51.43.13" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.51.43.14" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.51.43.15" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.51.43.16" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.51.43.17" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.51.43.18" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T6.8.8.51.43.19" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.52.44">
<td class="ltx_td ltx_border_r" id="S3.T6.8.8.52.44.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.52.44.2" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.52.44.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.52.44.3.1" style="background-color:#F2F2F2;">ConvNeXt-XXL</span></td>
<td class="ltx_td" id="S3.T6.8.8.52.44.4" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_r" id="S3.T6.8.8.52.44.5" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.52.44.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.52.44.6.1" style="background-color:#F2F2F2;">1024</span></td>
<td class="ltx_td ltx_border_r" id="S3.T6.8.8.52.44.7" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.52.44.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.52.44.8.1" style="background-color:#F2F2F2;">846</span></td>
<td class="ltx_td ltx_border_r" id="S3.T6.8.8.52.44.9" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.52.44.10" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.52.44.11" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.52.44.12" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.52.44.13" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.52.44.14" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.52.44.15" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.52.44.16" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.52.44.17" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.52.44.18" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.52.44.19" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.53.45">
<td class="ltx_td ltx_border_r" id="S3.T6.8.8.53.45.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.53.45.2" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.53.45.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.53.45.3.1" style="background-color:#F2F2F2;">DINOv2-ViT-L/14</span></td>
<td class="ltx_td" id="S3.T6.8.8.53.45.4" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_r" id="S3.T6.8.8.53.45.5" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.53.45.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.53.45.6.1" style="background-color:#F2F2F2;">518</span></td>
<td class="ltx_td ltx_border_r" id="S3.T6.8.8.53.45.7" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T6.8.8.53.45.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.53.45.8.1" style="background-color:#F2F2F2;">304</span></td>
<td class="ltx_td ltx_border_r" id="S3.T6.8.8.53.45.9" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.53.45.10" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.53.45.11" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.53.45.12" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.53.45.13" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.53.45.14" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.53.45.15" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.53.45.16" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.53.45.17" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.53.45.18" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T6.8.8.53.45.19" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T6.8.8.54.46" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T6.8.8.54.46.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.1.1" style="background-color:#F2F2F2;">R41</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.2.1" style="background-color:#F2F2F2;">Cambrian-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.3.1" style="background-color:#F2F2F2;">ViT-L/14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.4.1" style="background-color:#F2F2F2;">L-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T6.8.8.54.46.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.5.1" style="background-color:#F2F2F2;">2.5+7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.6.1" style="background-color:#F2F2F2;">336</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T6.8.8.54.46.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.7.1" style="background-color:#F2F2F2;">576</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.8.1" style="background-color:#F2F2F2;">304</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T6.8.8.54.46.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.9.1" style="background-color:#F2F2F2;">5085</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.10.1" style="background-color:#F2F2F2;">64.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.11.1" style="background-color:#F2F2F2;">80.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.12.1" style="background-color:#F2F2F2;">71.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.13.1" style="background-color:#F2F2F2;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.14.1" style="background-color:#F2F2F2;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.15.1" style="background-color:#F2F2F2;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.16" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.16.1" style="background-color:#F2F2F2;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.17" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.17.1" style="background-color:#F2F2F2;">77.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.18" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.18.1" style="background-color:#F2F2F2;">42.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.8.8.54.46.19" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T6.8.8.54.46.19.1" style="background-color:#F2F2F2;">74.7</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T6.14.2.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S3.T6.10.1" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.T6.10.1.1">VLM evaluations and comparison with recent methods.</span> The models are grouped based on total number of visual tokens. “-” indicates that performance was not reported in the respective paper. For the dataset column, “-” indicates that the dataset size for pretraining (“PT”) or instruction tuning (“IT”) is not explicitly mentioned in the respective paper. For methods that have more than 2 stages of training, we report the total samples used for all the pretraining stages as part of “PT”. “TTFT” means time to first token (the sum of the vision encoder latency and the LLM prefilling time), we report latency only for models that are publicly available and in a format favorable to MLX <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> “Vic.” refers to Vicuna <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite>, “Qw.2” refers to Qwen2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite> and “Qw.” refers to Qwen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. “L-2” refers to LLaMA-2. “L-3” refers to LLaMA-3. “ML.” refers to MobileLLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>. “DS.” refers to DeepSeek LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>. <sup class="ltx_sup" id="S3.T6.10.1.2">∗</sup> For input resolution and visual tokens, we report the highest supported resolution by the respective models as some models like LLaVA-OneVision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> and MM1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> use dynamic input resolution. †- performance numbers reported from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite>. For MiniGeminiHD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>, the dataset sizes is inferred from the pretrain and instruction tuning JSON files. For VLMs that use multiple vision encoders, the size of each encoder is listed independently, for TTFT, the latency from each encoder is summed up.
</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we present our training setup and results.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Training Setup.</span>
For all the ablations presented in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3" title="3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we follow the 2-stage setup described in LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> with Vicuna-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite> as the LLM decoder, unless mentioned otherwise. During the first stage, only the projector is trained using LLaVA-558K alignment dataset for one epoch, with a batch size of 256 and a learning rate of <math alttext="10^{-3}" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><msup id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mn id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">10</mn><mrow id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml"><mo id="S4.p2.1.m1.1.1.3a" xref="S4.p2.1.m1.1.1.3.cmml">−</mo><mn id="S4.p2.1.m1.1.1.3.2" xref="S4.p2.1.m1.1.1.3.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1">superscript</csymbol><cn id="S4.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.p2.1.m1.1.1.2">10</cn><apply id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3"><minus id="S4.p2.1.m1.1.1.3.1.cmml" xref="S4.p2.1.m1.1.1.3"></minus><cn id="S4.p2.1.m1.1.1.3.2.cmml" type="integer" xref="S4.p2.1.m1.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">10^{-3}</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT</annotation></semantics></math>. At this stage, the input image resolution matches the backbone pretraining resolution (e.g., 256 for FastViT and 224 for FastViTHD). In the second stage, we use LLaVA-665K supervised finetuning dataset, training the models for one epoch and tuning all the modules, i.e., vision encoder, projector and the LLM. At this stage, the input image resolution is set to the target resolution.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">In <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4" title="4 Experiments ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we present results with different LLM decoders, primarily with Qwen2-0.5B/1.5B/7B model family <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite> (chat variant) and Vicuna-7B model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite>. We report results in two training setups, the first one is the 2-Stage setup introduced in LLaVA-1.5. We additionally scale the dataset used in Stage 2 from 665k samples to 1.1 million samples, which is a subset of the instruction tuning dataset used in InternVL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> (more details in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4a" title="D Datasets ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">D</span></a>). For the second training setup, we follow the current trend in literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> of training the VLMs in 3 stages, i.e. Stage 1 for training the connector, Stage 1.5 for resolution scaling and Stage 2 for visual instruction tuning. In Stage 1.5, we use the densely captioned CC3M <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite> and CC12M <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>. For the final stage we use 1.1 million instruction tuning dataset. In this setup, the input image resolution is set to the backbone pretraining resolution for Stage 1 and adjusted to the target resolution for the following two stages. In both setups, the vision encoder and LLM are frozen only in stage 1, while all modules are finetuned in the remaining stages.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">All FastVLM models reported in the paper are trained on a single node with 8×
NVIDIA H100-80GB GPUs. Stage 1 training of VLM is quick, taking roughly 30 minutes to train with a Qwen2-7B decoder. Stage 1.5 and Stage 2 training runs are dependent on input resolution. For an input resolution of 1024<math alttext="\times" class="ltx_Math" display="inline" id="S4.p4.1.m1.1"><semantics id="S4.p4.1.m1.1a"><mo id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><times id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.p4.1.m1.1d">×</annotation></semantics></math>1024, Stage 1.5 takes 77 hours and Stage 2 takes 8 hours. The reported wall clock times correspond to the following datasets used in these stages: 15 million samples in Stage 1.5 and 1.1 million samples in Stage 2.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.p5.1.1">Evaluation.</span>
We evaluate the models on the mainstream benchmarks of GQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>, ScienceQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>, TextVQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite>, POPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>, LLaVA-in-the-wild <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>, VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, MMVet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib84" title=""><span class="ltx_text" style="font-size:90%;">84</span></a>]</cite>, MMMU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib86" title=""><span class="ltx_text" style="font-size:90%;">86</span></a>]</cite>, DocVQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite> and SeedBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>. For GQA, ScienceQA, TextVQA, POPE and LLaVA-in-the-wild benchmarks, we use the official evaluation from LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>. For the remaining evaluations we use lmms-eval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib88" title=""><span class="ltx_text" style="font-size:90%;">88</span></a>]</cite> library v0.2.2. We use the default settings for all the evaluations and lmms-eval defaults to 0613 version of GPT for evaluations that rely on GPT as a judge.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">For ablations presented in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3" title="3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we report GQA, TextVQA, POPE, DocVQA and SeedBench. GQA and SeedBench are general knowledge benchmarks, DocVQA and TextVQA represent text-rich evaluations and POPE is a hallucination benchmark. Together these benchmarks provide diversity and are quick to evaluate for ablations. Most importantly, they exhibit lower variance to different initializations and under probabilistic decoding setting. We report the variance for all the evals for different initialization in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4.SS3" title="D.3 Evaluations ‣ D Datasets ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">D.3</span></a>. The standard deviation across the 5 selected metrics is less than 0.5. We call the average of these 5 benchmarks Avg-5, and use it as a reliable signal for our analysis. Our empirical estimate of the standard deviation for Avg-5 is <math alttext="\simeq" class="ltx_Math" display="inline" id="S4.p6.1.m1.1"><semantics id="S4.p6.1.m1.1a"><mo id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml">≃</mo><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><csymbol cd="latexml" id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1">similar-to-or-equals</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">\simeq</annotation><annotation encoding="application/x-llamapun" id="S4.p6.1.m1.1d">≃</annotation></semantics></math>0.1.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1"><span class="ltx_text ltx_font_bold" id="S4.p7.1.1">Benchmarking.</span>
We benchmark all the models on a MacBook Pro with the M1 Max chip and 32GB RAM. The image encoder is converted to a Core ML package file using coremltools v7.2 and benchmarked on the neural engine using XCode 15.4 (15F31d). The LLM is benchmarked on the MacBook Pro GPU using MLX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>. The model is first converted using <span class="ltx_text ltx_font_typewriter" id="S4.p7.1.2">mlx_lm.convert</span> tool, which converts the models on huggingface to the MLX format and casts the tensors to FP16. The prefilling latency is estimated using <span class="ltx_text ltx_font_typewriter" id="S4.p7.1.3">mlx_lm.cache_prompt</span> tool <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>. Time-To-First-Token (TTFT) is estimated by adding the image encoding latency at a specific resolution to the LLM prefilling latency for the associated visual tokens.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Comparison with state-of-the-art</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T6" title="In 3.2.3 Comparison with Token Pruning &amp; Downsampling ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, we compare FastVLM with recently published methods. The training setup can vary widely between works. For each, we report the LLM decoder and the sizes of the instruction tuning and pretraining datasets used to train the respective VLMs, to facilitate a fair comparison.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Hierarchical Backbones.</span>
When we compare FastVLM (R18) with ConvLLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> (R16), with the same LLM and similar training data size, our model obtains +8.4% better performance on TextVQA and +12.5% better performance on DocVQA while being 22% faster. The gap widens at higher resolution, where FastVLM (R26 and R27) achieves superior performance on wide range of benchmarks while being 2<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><times id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">×</annotation></semantics></math> faster than ConvLLaVA (R24), with the same LLM decoder.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.4"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.4.1">Dataset Scaling.</span>
When designing a new architecture, it is crucial to consider how effectively the model scales with training data. We demonstrate the performance of FastVLM when scaling the pretraining and instruction tuning datasets. By increasing the instruction tuning dataset from 0.6M to 1.1M samples (R18), FastVLM outperforms prior works like MobileVLMv2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> (R15), which was trained on larger instruction tuning and pretraining datasets. Additionally, when scaling the pretraining dataset by incorporating an intermediate pretraining stage for resolution scaling with 15M samples, FastVLM (R19) matches or surpasses MM1 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> (R36) across a wide range of benchmarks, including MMMU, MMVet, SQA, POPE, and SeedBench. Remarkably, FastVLM achieves this performance while generating 5<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mo id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><times id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">×</annotation></semantics></math> fewer visual tokens. With an input resolution of 1024<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mo id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><times id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">×</annotation></semantics></math>1024 and a larger instruction tuning dataset of size 11.9M, FastVLM (R39) outperforms MM1 (R36) and LLaVA-NeXT (R37) across various benchmarks. Even on text-rich evaluations, like TextVQA and DocVQA, which are sensitive to input resolution and number of visual tokens, our model achieves better performance with 2.8<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p3.3.m3.1"><semantics id="S4.SS1.p3.3.m3.1a"><mo id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><times id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.3.m3.1d">×</annotation></semantics></math> and 11.3<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p3.4.m4.1"><semantics id="S4.SS1.p3.4.m4.1a"><mo id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><times id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.4.m4.1d">×</annotation></semantics></math> less visual tokens than MM1 and LLaVA-NeXT respectively. We provide details of the dataset splits in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4a" title="D Datasets ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.3"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.3.1">Multiple Vision Encoders.</span> Recently, MiniGemini <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> and Cambrian-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite> introduced models that rely on multiple vision encoders. In <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T6" title="In 3.2.3 Comparison with Token Pruning &amp; Downsampling ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, we compare FastVLM (R38), which uses a single vision encoder with methods that use multiple encoders and trained on similarly scaled visual instruction tuning dataset. In Cambrian-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite> (R41), vision encoding contributes 3.2<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.1"><semantics id="S4.SS1.p4.1.m1.1a"><mo id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><times id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.1d">×</annotation></semantics></math> more than LLM prefilling to the total time-to-first-token of approximately 5 seconds (detailed breakdown is provided in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S2.T9" title="In B.1 Naive Scaling ‣ B Architecture Details ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">9</span></a>). FastVLM (R38) outperforms Cambrian-1 (R41) when trained on a similar visual instruction tuning dataset, while being 7.9<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p4.2.m2.1"><semantics id="S4.SS1.p4.2.m2.1a"><mo id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><times id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.2.m2.1d">×</annotation></semantics></math> faster. By scaling the instruction tuning dataset to 11.9M, FastVLM (R39) achieves superior performance over Cambrian-1 (R41) with 2.3<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p4.3.m3.1"><semantics id="S4.SS1.p4.3.m3.1a"><mo id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><times id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.3.m3.1d">×</annotation></semantics></math> fewer visual tokens, even on text-rich evaluations (see <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T10" title="In C Additional Results ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">10</span></a>) that are sensitive to the number of visual tokens.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.3"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.3.1">Effect of Decoder.</span>
VLM performance also depends on the quality of LLM, as demonstrated in prior studies, like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>. By switching from Vicuna-7B (R19, R27) to Qwen2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">71</span></a>]</cite> models (R20, R28), we see a good improvement in performance across all the benchmarks. The improvements are significant on MMVet, LLaVA-in-the-wild and MMMU benchmarks. With Qwen2-0.5B as the LLM decoder, FastVLM (R3) matches the performance of LLaVA-OneVision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> (R2) on key benchmarks such as SeedBench, MMMU, and MMVet, while being 85<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p5.1.m1.1"><semantics id="S4.SS1.p5.1.m1.1a"><mo id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><times id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.1.m1.1d">×</annotation></semantics></math> faster and trained on 2.9<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p5.2.m2.1"><semantics id="S4.SS1.p5.2.m2.1a"><mo id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.1b"><times id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.2.m2.1d">×</annotation></semantics></math> fewer instruction tuning samples. This result underscores the quality of our vision encoder, as both models use the same LLM decoder, while FastViTHD is 3.4<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p5.3.m3.1"><semantics id="S4.SS1.p5.3.m3.1a"><mo id="S4.SS1.p5.3.m3.1.1" xref="S4.SS1.p5.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.3.m3.1b"><times id="S4.SS1.p5.3.m3.1.1.cmml" xref="S4.SS1.p5.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.3.m3.1d">×</annotation></semantics></math> smaller compared to SigLIP-SO400M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib87" title=""><span class="ltx_text" style="font-size:90%;">87</span></a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this work, we introduced FastVLM, which leverages FastViTHD image encoders designed for enhanced resolution scaling while maintaining efficiency. By strategically trading off the costly self-attention in ViTs with a purpose-built hybrid architecture, FastViTHD processes high-resolution images efficiently, and outputs a substantially reduced number of visual tokens. The design of FastVLM enables competitive performance with prior works across a wide range of VLM benchmarks, while improving efficiency in both time-to-first-token and the number of parameters in the vision backbone. Rigorous benchmarking on an M1 MacBook Pro demonstrates that FastVLM achieves a state-of-the-art resolution-latency-accuracy trade-off compared to existing works.


<span class="ltx_text" id="S5.p1.1.1" style="font-size:90%;"></span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">Alayrac et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">Flamingo: a visual language model for few-shot learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.9.1" style="font-size:90%;">Advances in neural information processing systems</em><span class="ltx_text" id="bib.bib1.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Awadalla et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">Openflamingo: An open-source framework for training large autoregressive vision-language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.9.1" style="font-size:90%;">arXiv preprint arXiv:2308.01390</em><span class="ltx_text" id="bib.bib2.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Bai et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">Qwen technical report.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.9.1" style="font-size:90%;">arXiv preprint arXiv:2309.16609</em><span class="ltx_text" id="bib.bib3.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Bai et al. [202k]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.9.1" style="font-size:90%;">arXiv preprint arXiv:2308.12966</em><span class="ltx_text" id="bib.bib4.10.2" style="font-size:90%;">, 202k.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Bavishi et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Introducing our multimodal models, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">Beyer et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">Paligemma: A versatile 3b vlm for transfer, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Cai et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
Mu Cai, Jianwei Yang, Jianfeng Gao, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Matryoshka multimodal models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.9.1" style="font-size:90%;">arXiv preprint arXiv:2405.17430</em><span class="ltx_text" id="bib.bib7.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.4.4.1" style="font-size:90%;">”Cao and Xiao [”2022”]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.6.1" style="font-size:90%;">
Jie ”Cao and Jing” Xiao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">”an augmented benchmark dataset for geometric question answering through dual parallel text encoding”.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.9.2" style="font-size:90%;">”Proceedings of the 29th International Conference on Computational Linguistics”</em><span class="ltx_text" id="bib.bib8.10.3" style="font-size:90%;">, ”2022”.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Cha et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">Honeybee: Locality-enhanced projector for multimodal llm.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib9.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Changpinyo et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib10.11.3" style="font-size:90%;">, pages 3558–3568, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Chen et al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">Vitamin: Designing scalable vision models in the vision-language era.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib11.11.3" style="font-size:90%;">, 2024a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.5.5.1" style="font-size:90%;">Chen et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">Sharegpt4v: Improving large multi-modal models with better captions.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.9.1" style="font-size:90%;">arXiv preprint arXiv:2311.12793</em><span class="ltx_text" id="bib.bib12.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">Chen et al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Chen et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.9.1" style="font-size:90%;">arXiv preprint arXiv:2312.14238</em><span class="ltx_text" id="bib.bib14.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Chen et al. [2024c]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.9.1" style="font-size:90%;">arXiv preprint arXiv:2404.16821</em><span class="ltx_text" id="bib.bib15.10.2" style="font-size:90%;">, 2024c.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.4.4.1" style="font-size:90%;">Chenglin Yang et al [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.6.1" style="font-size:90%;">
Chenglin Yang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">MOAT: Alternating mobile convolution and attention brings strong vision models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.9.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib16.10.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Chu et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.9.1" style="font-size:90%;">arXiv preprint arXiv:2312.16886</em><span class="ltx_text" id="bib.bib17.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">Chu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">Mobilevlm v2: Faster and stronger baseline for vision language model.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.03766</em><span class="ltx_text" id="bib.bib18.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Dai et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.4.4.1" style="font-size:90%;">DeepSeek-AI [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.6.1" style="font-size:90%;">
DeepSeek-AI.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">Deepseek llm: Scaling open-source language models with longtermism.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.8.1" style="font-size:90%;">arXiv preprint arXiv:2401.02954</em><span class="ltx_text" id="bib.bib20.9.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Diao et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">Unveiling encoder-free vision-language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.9.1" style="font-size:90%;">arXiv preprint arXiv:2406.11832</em><span class="ltx_text" id="bib.bib21.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Dosovitskiy et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at scale.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.9.1" style="font-size:90%;">arXiv preprint arXiv:2010.11929</em><span class="ltx_text" id="bib.bib22.10.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Fang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Data filtering networks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.9.1" style="font-size:90%;">arXiv preprint arXiv:2309.17425</em><span class="ltx_text" id="bib.bib23.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Gadre et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">Datacomp: In search of the next generation of multimodal datasets.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.14108</em><span class="ltx_text" id="bib.bib24.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Ge et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Chunjiang Ge, Sijie Cheng, Ziming Wang, Jiale Yuan, Yuan Gao, Jun Song, Shiji Song, Gao Huang, and Bo Zheng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">Convllava: Hierarchical backbones as visual encoder for large multimodal models, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Goyal et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib26.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib26.11.3" style="font-size:90%;">, pages 6904–6913, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Hannun et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Awni Hannun, Jagrit Digani, Angelos Katharopoulos, and Ronan Collobert.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">MLX: Efficient and flexible machine learning on apple silicon, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Hu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
Wenbo Hu, Zi-Yi Dou, Liunian Harold Li, Amita Kamath, Nanyun Peng, and Kai-Wei Chang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">Matryoshka query transformer for large vision-language models, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Huang et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaoshen Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, and Shaohui Lin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">Dynamic-llava: Efficient multimodal large language models via dynamic vision-language context sparsification, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.4.4.1" style="font-size:90%;">Hudson and Manning [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.6.1" style="font-size:90%;">
Drew A Hudson and Christopher D Manning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">Gqa: A new dataset for real-world visual reasoning and compositional question answering.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib30.9.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib30.10.3" style="font-size:90%;">, pages 6700–6709, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Kafle et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Kushal Kafle, Scott Cohen, Brian Price, and Christopher Kanan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">Dvqa: Understanding data visualizations via question answering.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib31.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib31.11.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">Karamcheti et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.8.1" style="font-size:90%;">Prismatic vlms: Investigating the design space of visually-conditioned language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib32.10.2" style="font-size:90%;">International Conference on Machine Learning (ICML)</em><span class="ltx_text" id="bib.bib32.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Kembhavi et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">A diagram is worth a dozen images, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Kim et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">Ocr-free document understanding transformer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib34.10.2" style="font-size:90%;">European Conference on Computer Vision (ECCV)</em><span class="ltx_text" id="bib.bib34.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Kirillov et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">Segment anything.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.02643</em><span class="ltx_text" id="bib.bib35.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">Krishna et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.9.1" style="font-size:90%;">International Journal of Computer Vision</em><span class="ltx_text" id="bib.bib36.10.2" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Li et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">Seed-bench: Benchmarking multimodal llms with generative comprehension.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.9.1" style="font-size:90%;">arXiv preprint arXiv:2307.16125</em><span class="ltx_text" id="bib.bib37.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Li et al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Bo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan Zhang, Renrui Zhang, Feng Li, Ziwei Liu, and Chunyuan Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">Llava-next: What else influences visual instruction tuning beyond data?, 2024a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.5.5.1" style="font-size:90%;">Li et al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">
Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.8.1" style="font-size:90%;">Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.5.5.1" style="font-size:90%;">Li et al. [2024c]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.8.1" style="font-size:90%;">Llava-onevision: Easy visual task transfer.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.9.1" style="font-size:90%;">arXiv preprint arXiv:2408.03326</em><span class="ltx_text" id="bib.bib40.10.2" style="font-size:90%;">, 2024c.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">Li et al. [2025]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
Junyan Li, Delin Chen, Tianle Cai, Peihao Chen, Yining Hong, Zhenfang Chen, Yikang Shen, and Chuang Gan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">Flexattention for efficient high-resolution vision-language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib41.10.2" style="font-size:90%;">European Conference on Computer Vision</em><span class="ltx_text" id="bib.bib41.11.3" style="font-size:90%;">, pages 286–302. Springer, 2025.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="font-size:90%;">Li et al. [2024d]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
Kevin Y. Li, Sachin Goyal, Joao D. Semedo, and J. Zico Kolter.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.8.1" style="font-size:90%;">Inference optimal vlms need only one visual token but larger models, 2024d.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.5.5.1" style="font-size:90%;">Li et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.7.1" style="font-size:90%;">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.8.1" style="font-size:90%;">Evaluating object hallucination in large vision-language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.10355</em><span class="ltx_text" id="bib.bib43.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.5.5.1" style="font-size:90%;">Li et al. [2023c]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.7.1" style="font-size:90%;">
Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.8.1" style="font-size:90%;">Mini-gemini: Mining the potential of multi-modality vision language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.9.1" style="font-size:90%;">arXiv preprint arXiv:2403.18814</em><span class="ltx_text" id="bib.bib44.10.2" style="font-size:90%;">, 2023c.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.5.5.1" style="font-size:90%;">Lin et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.7.1" style="font-size:90%;">
Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.8.1" style="font-size:90%;">Vila: On pre-training for visual language models, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.5.5.1" style="font-size:90%;">Lin et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.7.1" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.8.1" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib46.10.2" style="font-size:90%;">European Conference on Computer Vision</em><span class="ltx_text" id="bib.bib46.11.3" style="font-size:90%;">, pages 740–755, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.5.5.1" style="font-size:90%;">Lin et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.7.1" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.8.1" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib47.10.2" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib47.11.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib48.5.5.1" style="font-size:90%;">Lin et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.7.1" style="font-size:90%;">
Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.8.1" style="font-size:90%;">Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.9.1" style="font-size:90%;">arXiv preprint arXiv:2311.07575</em><span class="ltx_text" id="bib.bib48.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib49.5.5.1" style="font-size:90%;">Liu et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.7.1" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.8.1" style="font-size:90%;">Improved baselines with visual instruction tuning, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib50.5.5.1" style="font-size:90%;">Liu et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.7.1" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.8.1" style="font-size:90%;">Visual instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib50.10.2" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</em><span class="ltx_text" id="bib.bib50.11.3" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib51.5.5.1" style="font-size:90%;">Liu et al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.7.1" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.8.1" style="font-size:90%;">Llava-next: Improved reasoning, ocr, and world knowledge, 2024a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib52.5.5.1" style="font-size:90%;">Liu et al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.7.1" style="font-size:90%;">
Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xucheng Yin, Cheng lin Liu, Lianwen Jin, and Xiang Bai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.8.1" style="font-size:90%;">Ocrbench: On the hidden mystery of ocr in large multimodal models, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib53.5.5.1" style="font-size:90%;">Liu et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.7.1" style="font-size:90%;">
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.8.1" style="font-size:90%;">A convnet for the 2020s.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.9.1" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib53.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib54.5.5.1" style="font-size:90%;">Lu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.7.1" style="font-size:90%;">
Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.8.1" style="font-size:90%;">Deepseek-vl: Towards real-world vision-language understanding, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib55.5.5.1" style="font-size:90%;">Lu et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.7.1" style="font-size:90%;">
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.8.1" style="font-size:90%;">Learn to explain: Multimodal reasoning via thought chains for science question answering.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib55.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib56.5.5.1" style="font-size:90%;">Luo et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.7.1" style="font-size:90%;">
Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.8.1" style="font-size:90%;">Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.9.1" style="font-size:90%;">arXiv preprint arXiv:2403.03003</em><span class="ltx_text" id="bib.bib56.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib57.5.5.1" style="font-size:90%;">Masry et al. [”2022”]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.7.1" style="font-size:90%;">
Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.8.1" style="font-size:90%;">ChartQA: A benchmark for question answering about charts with visual and logical reasoning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib57.10.2" style="font-size:90%;">”Findings of the Association for Computational Linguistics: ACL 2022”</em><span class="ltx_text" id="bib.bib57.11.3" style="font-size:90%;">, ”2022”.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib58.5.5.1" style="font-size:90%;">Mathew et al. [2021a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.7.1" style="font-size:90%;">
Minesh Mathew, Viraj Bagal, Rubèn Pérez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.8.1" style="font-size:90%;">Infographicvqa, 2021a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib59.5.5.1" style="font-size:90%;">Mathew et al. [2021b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.7.1" style="font-size:90%;">
Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.8.1" style="font-size:90%;">Docvqa: A dataset for vqa on document images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib59.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF winter conference on applications of computer vision</em><span class="ltx_text" id="bib.bib59.11.3" style="font-size:90%;">, pages 2200–2209, 2021b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib60.5.5.1" style="font-size:90%;">McKinzie et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.7.1" style="font-size:90%;">
Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.8.1" style="font-size:90%;">Mm1: Methods, analysis &amp; insights from multimodal llm pre-training, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib61.5.5.1" style="font-size:90%;">Mishra et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.7.1" style="font-size:90%;">
Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.8.1" style="font-size:90%;">Ocr-vqa: Visual question answering by reading text in images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib61.10.2" style="font-size:90%;">ICDAR</em><span class="ltx_text" id="bib.bib61.11.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib62.4.4.1" style="font-size:90%;">OpenAI [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.6.1" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.7.1" style="font-size:90%;">Gpt-4 technical report.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.8.1" style="font-size:90%;">arXiv preprint arXiv:2303.08774</em><span class="ltx_text" id="bib.bib62.9.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib63.5.5.1" style="font-size:90%;">Radford et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.7.1" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.8.1" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib63.10.2" style="font-size:90%;">International conference on machine learning</em><span class="ltx_text" id="bib.bib63.11.3" style="font-size:90%;">, pages 8748–8763. PMLR, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib64.5.5.1" style="font-size:90%;">Shang et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.7.1" style="font-size:90%;">
Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.8.1" style="font-size:90%;">Llava-prumerge: Adaptive token reduction for efficient large multimodal models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.9.1" style="font-size:90%;">arXiv preprint arXiv:2403.15388</em><span class="ltx_text" id="bib.bib64.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib65.5.5.1" style="font-size:90%;">Sharma et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.7.1" style="font-size:90%;">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.8.1" style="font-size:90%;">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib65.10.2" style="font-size:90%;">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em><span class="ltx_text" id="bib.bib65.11.3" style="font-size:90%;">, pages 2556–2565, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib66.5.5.1" style="font-size:90%;">Shi et al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.7.1" style="font-size:90%;">
Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.8.1" style="font-size:90%;">When do we not need larger vision models?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib66.10.2" style="font-size:90%;">European Conference on Computer Vision (ECCV)</em><span class="ltx_text" id="bib.bib66.11.3" style="font-size:90%;">, 2024a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib67.5.5.1" style="font-size:90%;">Shi et al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.7.1" style="font-size:90%;">
Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, and Guilin Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.8.1" style="font-size:90%;">Eagle: Exploring the design space for multimodal llms with mixture of encoders.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.9.1" style="font-size:90%;">arXiv:2408.15998</em><span class="ltx_text" id="bib.bib67.10.2" style="font-size:90%;">, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib68.5.5.1" style="font-size:90%;">Singh et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.7.1" style="font-size:90%;">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.8.1" style="font-size:90%;">Towards vqa models that can read.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib68.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib68.11.3" style="font-size:90%;">, pages 8317–8326, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib69.5.5.1" style="font-size:90%;">Sun et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.7.1" style="font-size:90%;">
Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.8.1" style="font-size:90%;">Eva-clip: Improved training techniques for clip at scale.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.9.1" style="font-size:90%;">arXiv preprint arXiv:2303.15389</em><span class="ltx_text" id="bib.bib69.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib70.4.4.1" style="font-size:90%;">Team [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.6.1" style="font-size:90%;">
Chameleon Team.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.7.1" style="font-size:90%;">Chameleon: Mixed-modal early-fusion foundation models, 2024a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib71.4.4.1" style="font-size:90%;">Team [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.6.1" style="font-size:90%;">
Qwen Team.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.7.1" style="font-size:90%;">Qwen2.5: A party of foundation models, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib72.5.5.1" style="font-size:90%;">Tong et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.7.1" style="font-size:90%;">
Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.8.1" style="font-size:90%;">Cambrian-1: A fully open, vision-centric exploration of multimodal llms, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib73.5.5.1" style="font-size:90%;">Touvron et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.7.1" style="font-size:90%;">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.8.1" style="font-size:90%;">Llama: Open and efficient foundation language models, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib74.5.5.1" style="font-size:90%;">Tsimpoukelli et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.7.1" style="font-size:90%;">
Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.8.1" style="font-size:90%;">Multimodal few-shot learning with frozen language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.9.1" style="font-size:90%;">Conference on Neural Information Processing Systems (NeurIPS)</em><span class="ltx_text" id="bib.bib74.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib75.5.5.1" style="font-size:90%;">Vasu et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.7.1" style="font-size:90%;">
Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.8.1" style="font-size:90%;">Mobileone: An improved one millisecond mobile backbone.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib75.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib75.11.3" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib76.5.5.1" style="font-size:90%;">Vasu et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.7.1" style="font-size:90%;">
Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.8.1" style="font-size:90%;">Fastvit: A fast hybrid vision transformer using structural reparameterization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib76.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib76.11.3" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib77.5.5.1" style="font-size:90%;">Vasu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.7.1" style="font-size:90%;">
Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, and Oncel Tuzel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.8.1" style="font-size:90%;">Mobileclip: Fast image-text models through multi-modal reinforced training.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib77.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib77.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib78.5.5.1" style="font-size:90%;">Xue et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.7.1" style="font-size:90%;">
Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S. Ryoo, Shrikant Kendre, Jieyu Zhang, Can Qin, Shu Zhang, Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Manoj Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, and Ran Xu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.8.1" style="font-size:90%;">xgen-mm (BLIP-3): A family of open large multimodal models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.9.1" style="font-size:90%;">CoRR</em><span class="ltx_text" id="bib.bib78.10.2" style="font-size:90%;">, abs/2408.08872, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib79.5.5.1" style="font-size:90%;">Yang et al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.7.1" style="font-size:90%;">
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.8.1" style="font-size:90%;">Qwen2 technical report.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.9.1" style="font-size:90%;">arXiv preprint arXiv:2407.10671</em><span class="ltx_text" id="bib.bib79.10.2" style="font-size:90%;">, 2024a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib80.5.5.1" style="font-size:90%;">Yang et al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.7.1" style="font-size:90%;">
Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.8.1" style="font-size:90%;">Visionzip: Longer is better but not necessary in vision language models, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib81.5.5.1" style="font-size:90%;">Ye et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.7.1" style="font-size:90%;">
Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.8.1" style="font-size:90%;">mplug-owl3: Towards long image-sequence understanding in multi-modal large language models, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib82.5.5.1" style="font-size:90%;">Ye et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.7.1" style="font-size:90%;">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.8.1" style="font-size:90%;">mplug-owl: Modularization empowers large language models with multimodality, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib83.5.5.1" style="font-size:90%;">Ye et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.7.1" style="font-size:90%;">
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.8.1" style="font-size:90%;">mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib84.5.5.1" style="font-size:90%;">Yu et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.7.1" style="font-size:90%;">
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.8.1" style="font-size:90%;">Mm-vet: Evaluating large multimodal models for integrated capabilities.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.9.1" style="font-size:90%;">arXiv preprint arXiv:2308.02490</em><span class="ltx_text" id="bib.bib84.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib85.5.5.1" style="font-size:90%;">Yu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.7.1" style="font-size:90%;">
Weihao Yu, Chenyang Si, Pan Zhou, Mi Luo, Yichen Zhou, Jiashi Feng, Shuicheng Yan, and Xinchao Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.8.1" style="font-size:90%;">Metaformer baselines for vision.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.9.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span class="ltx_text" id="bib.bib85.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib86.5.5.1" style="font-size:90%;">Yue et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.7.1" style="font-size:90%;">
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.8.1" style="font-size:90%;">Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib86.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib86.11.3" style="font-size:90%;">, pages 9556–9567, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib87.5.5.1" style="font-size:90%;">Zhai et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.7.1" style="font-size:90%;">
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.8.1" style="font-size:90%;">Sigmoid loss for language image pre-training.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.9.1" style="font-size:90%;">International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib87.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib88.5.5.1" style="font-size:90%;">Zhang et al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.7.1" style="font-size:90%;">
Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.8.1" style="font-size:90%;">Lmms-eval: Reality check on the evaluation of large multimodal models, 2024a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib89.5.5.1" style="font-size:90%;">Zhang et al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.7.1" style="font-size:90%;">
Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.8.1" style="font-size:90%;">Sparsevlm: Visual token sparsification for efficient vision-language model inference.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.9.1" style="font-size:90%;">arXiv preprint arXiv:2410.04417</em><span class="ltx_text" id="bib.bib89.10.2" style="font-size:90%;">, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib90.5.5.1" style="font-size:90%;">Zheng et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.7.1" style="font-size:90%;">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.8.1" style="font-size:90%;">Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib91.5.5.1" style="font-size:90%;">Zhu et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.7.1" style="font-size:90%;">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.8.1" style="font-size:90%;">Minigpt-4: Enhancing vision-language understanding with advanced large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.10592</em><span class="ltx_text" id="bib.bib91.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\thetitle</span>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.2"><span class="ltx_text" id="p1.2.1" style="font-size:144%;">Supplementary Material 
<br class="ltx_break"/></span></p>
</div>
<section class="ltx_section" id="S1a">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">A </span>Training Setup</h2>
<div class="ltx_para" id="S1a.p1">
<p class="ltx_p" id="S1a.p1.1"><span class="ltx_text" id="S1a.p1.1.1" style="font-size:144%;">For experiments presented in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T1" style="font-size:144%;" title="In 3.1 FastViT as VLM Image Encoder ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S1a.p1.1.2" style="font-size:144%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T2" style="font-size:144%;" title="In 3.1.1 Multi-Scale Features ‣ 3.1 FastViT as VLM Image Encoder ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S1a.p1.1.3" style="font-size:144%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T4" style="font-size:144%;" title="In 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S1a.p1.1.4" style="font-size:144%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T5" style="font-size:144%;" title="In 3.2.3 Comparison with Token Pruning &amp; Downsampling ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text" id="S1a.p1.1.5" style="font-size:144%;">, we perform 2-stage training with the hyperparameters listed in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S1.T7" style="font-size:144%;" title="In A Training Setup ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a><span class="ltx_text" id="S1a.p1.1.6" style="font-size:144%;">. The model is trained for a single epoch in all the stages. Note, in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T5" style="font-size:144%;" title="In 3.2.3 Comparison with Token Pruning &amp; Downsampling ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text" id="S1a.p1.1.7" style="font-size:144%;">, we do not re-train other token pruning works, we simply report the performance of the respective methods as they adhere to the 2-stage training setup described in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S1.T7" style="font-size:144%;" title="In A Training Setup ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a><span class="ltx_text" id="S1a.p1.1.8" style="font-size:144%;">, which was originally introduced in LLaVA-1.5 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1a.p1.1.9.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a><span class="ltx_text" id="S1a.p1.1.10.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S1a.p1.1.11" style="font-size:144%;">.</span></p>
</div>
<div class="ltx_para" id="S1a.p2">
<p class="ltx_p" id="S1a.p2.1"><span class="ltx_text" id="S1a.p2.1.1" style="font-size:144%;">To showcase our model’s performance in the presence of additional dataset, we scale both pretraining and instruction tuning datasets in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4" style="font-size:144%;" title="4 Experiments ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S1a.p2.1.2" style="font-size:144%;">. For results presented in R13, R17, R18, R25, R26 in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T6" style="font-size:144%;" title="In 3.2.3 Comparison with Token Pruning &amp; Downsampling ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text" id="S1a.p2.1.3" style="font-size:144%;">, we still perform 2-stage training described in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S1.T7" style="font-size:144%;" title="In A Training Setup ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a><span class="ltx_text" id="S1a.p2.1.4" style="font-size:144%;">, for R18 and R26, we use instruction tuning dataset of size 1.1 million samples in Stage-2. For results presented in R3, R4, R6, R7, R10, R11, R14, R19, R20, R21, R27, R28, R38 and R39, we scale-up both instruction tuning dataset and pretraining dataset. We also introduce and additional stage of pretraining with the scaled-up dataset as described in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S1.T8" style="font-size:144%;" title="In A Training Setup ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text" id="S1a.p2.1.5" style="font-size:144%;">. Details of 1.1 million, 6.5 million and 11.9 million instruction tuning dataset is presented in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4a" style="font-size:144%;" title="D Datasets ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">D</span></a><span class="ltx_text" id="S1a.p2.1.6" style="font-size:144%;">.</span></p>
</div>
<figure class="ltx_table" id="S1.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.T7.2" style="width:169.1pt;height:75.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-96.7pt,43.2pt) scale(0.466539319524596,0.466539319524596) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S1.T7.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T7.2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S1.T7.2.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T7.2.1.1.1.2"><span class="ltx_text" id="S1.T7.2.1.1.1.2.1" style="font-size:144%;">Stage-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T7.2.1.1.1.3"><span class="ltx_text" id="S1.T7.2.1.1.1.3.1" style="font-size:144%;">Stage-2</span></td>
</tr>
<tr class="ltx_tr" id="S1.T7.2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S1.T7.2.1.2.2.1"><span class="ltx_text" id="S1.T7.2.1.2.2.1.1" style="font-size:144%;">Data</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T7.2.1.2.2.2"><span class="ltx_text" id="S1.T7.2.1.2.2.2.1" style="font-size:144%;">LLaVA-1.5 558K</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T7.2.1.2.2.3"><span class="ltx_text" id="S1.T7.2.1.2.2.3.1" style="font-size:144%;">LLaVA-1.5 665k</span></td>
</tr>
<tr class="ltx_tr" id="S1.T7.2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S1.T7.2.1.3.3.1"><span class="ltx_text" id="S1.T7.2.1.3.3.1.1" style="font-size:144%;">Learning Rate</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T7.2.1.3.3.2"><span class="ltx_text" id="S1.T7.2.1.3.3.2.1" style="font-size:144%;">1e-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T7.2.1.3.3.3"><span class="ltx_text" id="S1.T7.2.1.3.3.3.1" style="font-size:144%;">2e-5</span></td>
</tr>
<tr class="ltx_tr" id="S1.T7.2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T7.2.1.4.4.1"><span class="ltx_text" id="S1.T7.2.1.4.4.1.1" style="font-size:144%;">Batch size</span></th>
<td class="ltx_td ltx_align_center" id="S1.T7.2.1.4.4.2"><span class="ltx_text" id="S1.T7.2.1.4.4.2.1" style="font-size:144%;">256</span></td>
<td class="ltx_td ltx_align_center" id="S1.T7.2.1.4.4.3"><span class="ltx_text" id="S1.T7.2.1.4.4.3.1" style="font-size:144%;">128</span></td>
</tr>
<tr class="ltx_tr" id="S1.T7.2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T7.2.1.5.5.1"><span class="ltx_text" id="S1.T7.2.1.5.5.1.1" style="font-size:144%;">LR. schedule</span></th>
<td class="ltx_td ltx_align_center" id="S1.T7.2.1.5.5.2"><span class="ltx_text" id="S1.T7.2.1.5.5.2.1" style="font-size:144%;">cosine decay</span></td>
<td class="ltx_td ltx_align_center" id="S1.T7.2.1.5.5.3"><span class="ltx_text" id="S1.T7.2.1.5.5.3.1" style="font-size:144%;">cosine decay</span></td>
</tr>
<tr class="ltx_tr" id="S1.T7.2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T7.2.1.6.6.1"><span class="ltx_text" id="S1.T7.2.1.6.6.1.1" style="font-size:144%;">LR. warmup ratio</span></th>
<td class="ltx_td ltx_align_center" id="S1.T7.2.1.6.6.2"><span class="ltx_text" id="S1.T7.2.1.6.6.2.1" style="font-size:144%;">0.03</span></td>
<td class="ltx_td ltx_align_center" id="S1.T7.2.1.6.6.3"><span class="ltx_text" id="S1.T7.2.1.6.6.3.1" style="font-size:144%;">0.03</span></td>
</tr>
<tr class="ltx_tr" id="S1.T7.2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T7.2.1.7.7.1"><span class="ltx_text" id="S1.T7.2.1.7.7.1.1" style="font-size:144%;">Optimizer</span></th>
<td class="ltx_td ltx_align_center" id="S1.T7.2.1.7.7.2"><span class="ltx_text" id="S1.T7.2.1.7.7.2.1" style="font-size:144%;">AdamW</span></td>
<td class="ltx_td ltx_align_center" id="S1.T7.2.1.7.7.3"><span class="ltx_text" id="S1.T7.2.1.7.7.3.1" style="font-size:144%;">AdamW</span></td>
</tr>
<tr class="ltx_tr" id="S1.T7.2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S1.T7.2.1.8.8.1"><span class="ltx_text" id="S1.T7.2.1.8.8.1.1" style="font-size:144%;">Trainable</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T7.2.1.8.8.2" rowspan="2"><span class="ltx_text" id="S1.T7.2.1.8.8.2.1" style="font-size:144%;">Projector</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T7.2.1.8.8.3"><span class="ltx_text" id="S1.T7.2.1.8.8.3.1" style="font-size:144%;">Full</span></td>
</tr>
<tr class="ltx_tr" id="S1.T7.2.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S1.T7.2.1.9.9.1"><span class="ltx_text" id="S1.T7.2.1.9.9.1.1" style="font-size:144%;">modules</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T7.2.1.9.9.2"><span class="ltx_text" id="S1.T7.2.1.9.9.2.1" style="font-size:144%;">Model</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S1.T7.6.1.1" style="font-size:63%;">Table 7</span>: </span><span class="ltx_text" id="S1.T7.7.2" style="font-size:63%;">2-Stage training setup used in ablations for <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3" title="3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</span></figcaption>
</figure>
<figure class="ltx_table" id="S1.T8">
<div class="ltx_inline-block ltx_transformed_outer" id="S1.T8.2" style="width:214.6pt;height:53.6pt;vertical-align:-0.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-254.9pt,63.3pt) scale(0.296306003741833,0.296306003741833) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S1.T8.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T8.2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S1.T8.2.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T8.2.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.1.1.2.1" style="font-size:144%;">Stage-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T8.2.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.1.1.3.1" style="font-size:144%;">Stage-1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T8.2.1.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.1.1.4.1" style="font-size:144%;">Stage-2</span></td>
</tr>
<tr class="ltx_tr" id="S1.T8.2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S1.T8.2.1.2.2.1" rowspan="2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.2.2.1.1" style="font-size:144%;">Data</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T8.2.1.2.2.2" rowspan="2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.2.2.2.1" style="font-size:144%;">LLaVA-1.5 558K</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T8.2.1.2.2.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.2.2.3.1" style="font-size:144%;">Recap-CC3M +</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T8.2.1.2.2.4" rowspan="2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.2.2.4.1" style="font-size:144%;">1.1M / 6.5M / 11.9M</span></td>
</tr>
<tr class="ltx_tr" id="S1.T8.2.1.3.3">
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.3.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S1.T8.2.1.3.3.1.1" style="font-size:144%;">Recap-CC12M </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.T8.2.1.3.3.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a><span class="ltx_text" id="S1.T8.2.1.3.3.1.3.2" style="font-size:144%;">]</span></cite>
</td>
</tr>
<tr class="ltx_tr" id="S1.T8.2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S1.T8.2.1.4.4.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.4.4.1.1" style="font-size:144%;">Learning Rate</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T8.2.1.4.4.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.4.4.2.1" style="font-size:144%;">1e-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T8.2.1.4.4.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.4.4.3.1" style="font-size:144%;">2e-5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T8.2.1.4.4.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.4.4.4.1" style="font-size:144%;">2e-5</span></td>
</tr>
<tr class="ltx_tr" id="S1.T8.2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T8.2.1.5.5.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.5.5.1.1" style="font-size:144%;">Batch size</span></th>
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.5.5.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.5.5.2.1" style="font-size:144%;">256</span></td>
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.5.5.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.5.5.3.1" style="font-size:144%;">128</span></td>
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.5.5.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.5.5.4.1" style="font-size:144%;">128</span></td>
</tr>
<tr class="ltx_tr" id="S1.T8.2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T8.2.1.6.6.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.6.6.1.1" style="font-size:144%;">LR. schedule</span></th>
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.6.6.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.6.6.2.1" style="font-size:144%;">cosine decay</span></td>
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.6.6.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.6.6.3.1" style="font-size:144%;">cosine decay</span></td>
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.6.6.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.6.6.4.1" style="font-size:144%;">cosine decay</span></td>
</tr>
<tr class="ltx_tr" id="S1.T8.2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T8.2.1.7.7.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.7.7.1.1" style="font-size:144%;">LR. warmup ratio</span></th>
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.7.7.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.7.7.2.1" style="font-size:144%;">0.03</span></td>
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.7.7.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.7.7.3.1" style="font-size:144%;">0.03</span></td>
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.7.7.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.7.7.4.1" style="font-size:144%;">0.03</span></td>
</tr>
<tr class="ltx_tr" id="S1.T8.2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T8.2.1.8.8.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.8.8.1.1" style="font-size:144%;">Optimizer</span></th>
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.8.8.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.8.8.2.1" style="font-size:144%;">AdamW</span></td>
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.8.8.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.8.8.3.1" style="font-size:144%;">AdamW</span></td>
<td class="ltx_td ltx_align_center" id="S1.T8.2.1.8.8.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.8.8.4.1" style="font-size:144%;">AdamW</span></td>
</tr>
<tr class="ltx_tr" id="S1.T8.2.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S1.T8.2.1.9.9.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.9.9.1.1" style="font-size:144%;">Trainable</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T8.2.1.9.9.2" rowspan="2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.9.9.2.1" style="font-size:144%;">Projector</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T8.2.1.9.9.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.9.9.3.1" style="font-size:144%;">Full</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T8.2.1.9.9.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.9.9.4.1" style="font-size:144%;">Full</span></td>
</tr>
<tr class="ltx_tr" id="S1.T8.2.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S1.T8.2.1.10.10.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.10.10.1.1" style="font-size:144%;">modules</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T8.2.1.10.10.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.10.10.2.1" style="font-size:144%;">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T8.2.1.10.10.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S1.T8.2.1.10.10.3.1" style="font-size:144%;">Model</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S1.T8.6.1.1" style="font-size:63%;">Table 8</span>: </span><span class="ltx_text" id="S1.T8.7.2" style="font-size:63%;">3-Stage training setup used for results with scaled-up data in <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T6" title="In 3.2.3 Comparison with Token Pruning &amp; Downsampling ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a>.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2a">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">B </span>Architecture Details</h2>
<div class="ltx_para" id="S2a.p1">
<p class="ltx_p" id="S2a.p1.4"><span class="ltx_text" id="S2a.p1.4.1" style="font-size:144%;">The patch embedding layers shown in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.F2" style="font-size:144%;" title="In 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S2a.p1.4.2" style="font-size:144%;">, consists of 7</span><math alttext="\times" class="ltx_Math" display="inline" id="S2a.p1.1.m1.1"><semantics id="S2a.p1.1.m1.1a"><mo id="S2a.p1.1.m1.1.1" mathsize="144%" xref="S2a.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2a.p1.1.m1.1b"><times id="S2a.p1.1.m1.1.1.cmml" xref="S2a.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2a.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2a.p1.1.m1.1d">×</annotation></semantics></math><span class="ltx_text" id="S2a.p1.4.3" style="font-size:144%;">7 depthwise convolutions with </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2a.p1.4.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">75</span></a><span class="ltx_text" id="S2a.p1.4.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2a.p1.4.6" style="font-size:144%;"> style train-time overparameterization, followed by 1</span><math alttext="\times" class="ltx_Math" display="inline" id="S2a.p1.2.m2.1"><semantics id="S2a.p1.2.m2.1a"><mo id="S2a.p1.2.m2.1.1" mathsize="144%" xref="S2a.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2a.p1.2.m2.1b"><times id="S2a.p1.2.m2.1.1.cmml" xref="S2a.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2a.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2a.p1.2.m2.1d">×</annotation></semantics></math><span class="ltx_text" id="S2a.p1.4.7" style="font-size:144%;">1 pointwise convolution. The stride for 7</span><math alttext="\times" class="ltx_Math" display="inline" id="S2a.p1.3.m3.1"><semantics id="S2a.p1.3.m3.1a"><mo id="S2a.p1.3.m3.1.1" mathsize="144%" xref="S2a.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2a.p1.3.m3.1b"><times id="S2a.p1.3.m3.1.1.cmml" xref="S2a.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2a.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2a.p1.3.m3.1d">×</annotation></semantics></math><span class="ltx_text" id="S2a.p1.4.8" style="font-size:144%;">7 depthwise convolution is set to 2 in order to downsample the input tensor. In </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2a.p1.4.9.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a><span class="ltx_text" id="S2a.p1.4.10.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2a.p1.4.11" style="font-size:144%;">, squeeze-excite layers were incorporated into this block; however, we found them to negatively impact inference latency, especially for high image resolutions, so we opted not to include them in our model. We use the same ConvFFN layer defined in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2a.p1.4.12.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a><span class="ltx_text" id="S2a.p1.4.13.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2a.p1.4.14" style="font-size:144%;">, i.e. 7</span><math alttext="\times" class="ltx_Math" display="inline" id="S2a.p1.4.m4.1"><semantics id="S2a.p1.4.m4.1a"><mo id="S2a.p1.4.m4.1.1" mathsize="144%" xref="S2a.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2a.p1.4.m4.1b"><times id="S2a.p1.4.m4.1.1.cmml" xref="S2a.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2a.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2a.p1.4.m4.1d">×</annotation></semantics></math><span class="ltx_text" id="S2a.p1.4.15" style="font-size:144%;">7 depthwise convolutions preceding a typical FFN layer. The stem downsamples the input tensor by factor of 4 on each side, and each patch embedding layer downsamples the input tensor by a factor 2. Although recent architectures like ViTamin </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2a.p1.4.16.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a><span class="ltx_text" id="S2a.p1.4.17.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2a.p1.4.18" style="font-size:144%;"> recommend an overall downsampling factor of only 16, FastViTHD incorporates an additional patch embedding layer compared to FastViT, resulting in an overall downsampling factor of 64× for the input tensor. In each stage, we increase the number of channels by a factor of 2 as done in FastViT and other convolutional and hybrid transformer architectures. This results in a Stage-5 with the widest MLP layers in the architecture, performing self-attention on an input tensor which is downsampled by a factor of 64.</span></p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Naive Scaling</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><span class="ltx_text" id="S2.SS1.p1.1.1" style="font-size:144%;">In order to scale the model size of FastViT, we simply increased the embedding dimensions per stage to </span><span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.1.2" style="font-size:144%;">[128, 256, 512, 1024]</span><span class="ltx_text" id="S2.SS1.p1.1.3" style="font-size:144%;">, and set the number of layers per stage to </span><span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.1.4" style="font-size:144%;">[2, 12, 16, 6]</span><span class="ltx_text" id="S2.SS1.p1.1.5" style="font-size:144%;">. Patch embedding layers in each stage use squeeze-excite layers and the MLP expansion ratio is set to 3.0, following the design in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.6.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a><span class="ltx_text" id="S2.SS1.p1.1.7.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.8" style="font-size:144%;">.</span></p>
</div>
<figure class="ltx_table" id="S2.T9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T9.7" style="width:325.2pt;height:223.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-322.6pt,221.4pt) scale(0.335111192493926,0.335111192493926) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T9.7.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T9.7.7.8.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S2.T9.7.7.8.1.1"><span class="ltx_text" id="S2.T9.7.7.8.1.1.1" style="font-size:144%;">Row</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T9.7.7.8.1.2" rowspan="2"><span class="ltx_text" id="S2.T9.7.7.8.1.2.1" style="font-size:144%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T9.7.7.8.1.3"><span class="ltx_text" id="S2.T9.7.7.8.1.3.1" style="font-size:144%;">Vision</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S2.T9.7.7.8.1.4" rowspan="2"><span class="ltx_text" id="S2.T9.7.7.8.1.4.1" style="font-size:144%;">LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T9.7.7.8.1.5"><span class="ltx_text" id="S2.T9.7.7.8.1.5.1" style="font-size:144%;">Input</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S2.T9.7.7.8.1.6"><span class="ltx_text" id="S2.T9.7.7.8.1.6.1" style="font-size:144%;">#Visual</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T9.7.7.8.1.7"><span class="ltx_text" id="S2.T9.7.7.8.1.7.1" style="font-size:144%;">Vis. Enc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T9.7.7.8.1.8"><span class="ltx_text" id="S2.T9.7.7.8.1.8.1" style="font-size:144%;">Vision Enc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T9.7.7.8.1.9"><span class="ltx_text" id="S2.T9.7.7.8.1.9.1" style="font-size:144%;">LLM</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.3.3.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.3.3.3.4"><span class="ltx_text" id="S2.T9.3.3.3.4.1" style="font-size:144%;">Ann.</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.3.3.3.5"><span class="ltx_text" id="S2.T9.3.3.3.5.1" style="font-size:144%;">Encoder</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.3.3.3.6"><span class="ltx_text" id="S2.T9.3.3.3.6.1" style="font-size:144%;">Res.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.3.3.3.7"><span class="ltx_text" id="S2.T9.3.3.3.7.1" style="font-size:144%;">Tokens</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.1.1.1.1">
<span class="ltx_text" id="S2.T9.1.1.1.1.1" style="font-size:144%;">Size(M)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T9.1.1.1.1.m1.1"><semantics id="S2.T9.1.1.1.1.m1.1a"><mo id="S2.T9.1.1.1.1.m1.1.1" mathsize="144%" stretchy="false" xref="S2.T9.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T9.1.1.1.1.m1.1b"><ci id="S2.T9.1.1.1.1.m1.1.1.cmml" xref="S2.T9.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T9.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T9.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.2.2.2.2">
<span class="ltx_text" id="S2.T9.2.2.2.2.1" style="font-size:144%;">Latency(ms)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T9.2.2.2.2.m1.1"><semantics id="S2.T9.2.2.2.2.m1.1a"><mo id="S2.T9.2.2.2.2.m1.1.1" mathsize="144%" stretchy="false" xref="S2.T9.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T9.2.2.2.2.m1.1b"><ci id="S2.T9.2.2.2.2.m1.1.1.cmml" xref="S2.T9.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T9.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T9.2.2.2.2.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.3.3.3.3">
<span class="ltx_text" id="S2.T9.3.3.3.3.1" style="font-size:144%;">Prefilling(ms)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T9.3.3.3.3.m1.1"><semantics id="S2.T9.3.3.3.3.m1.1a"><mo id="S2.T9.3.3.3.3.m1.1.1" mathsize="144%" stretchy="false" xref="S2.T9.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T9.3.3.3.3.m1.1b"><ci id="S2.T9.3.3.3.3.m1.1.1.cmml" xref="S2.T9.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T9.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T9.3.3.3.3.m1.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.9.2">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="9" id="S2.T9.7.7.9.2.1"><span class="ltx_text" id="S2.T9.7.7.9.2.1.1" style="font-size:144%;">0.5B Model Comparison</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.10.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T9.7.7.10.3.1"><span class="ltx_text" id="S2.T9.7.7.10.3.1.1" style="font-size:144%;">R1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.10.3.2"><span class="ltx_text" id="S2.T9.7.7.10.3.2.1" style="font-size:144%;">nanoLLaVA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.10.3.3"><span class="ltx_text" id="S2.T9.7.7.10.3.3.1" style="font-size:144%;">ViT-SO400M</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T9.7.7.10.3.4"><span class="ltx_text" id="S2.T9.7.7.10.3.4.1" style="font-size:144%;">Qw.1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.10.3.5"><span class="ltx_text" id="S2.T9.7.7.10.3.5.1" style="font-size:144%;">384</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T9.7.7.10.3.6"><span class="ltx_text" id="S2.T9.7.7.10.3.6.1" style="font-size:144%;">729</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.10.3.7"><span class="ltx_text" id="S2.T9.7.7.10.3.7.1" style="font-size:144%;">430</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.10.3.8"><span class="ltx_text" id="S2.T9.7.7.10.3.8.1" style="font-size:144%;">272.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.10.3.9"><span class="ltx_text" id="S2.T9.7.7.10.3.9.1" style="font-size:144%;">263.3</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.4.4.4.2"><span class="ltx_text" id="S2.T9.4.4.4.2.1" style="font-size:144%;">R2</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.4.4.4.1">
<span class="ltx_text" id="S2.T9.4.4.4.1.1" style="font-size:144%;">LLaVAOV </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.4.4.4.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a><span class="ltx_text" id="S2.T9.4.4.4.1.3.2" style="font-size:144%;">]</span></cite><sup class="ltx_sup" id="S2.T9.4.4.4.1.4"><span class="ltx_text" id="S2.T9.4.4.4.1.4.1" style="font-size:144%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.4.4.4.3"><span class="ltx_text" id="S2.T9.4.4.4.3.1" style="font-size:144%;">ViT-SO400M</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.4.4.4.4"><span class="ltx_text" id="S2.T9.4.4.4.4.1" style="font-size:144%;">Qw.2</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.4.4.4.5"><span class="ltx_text" id="S2.T9.4.4.4.5.1" style="font-size:144%;">1152</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.4.4.4.6"><span class="ltx_text" id="S2.T9.4.4.4.6.1" style="font-size:144%;">7290</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.4.4.4.7"><span class="ltx_text" id="S2.T9.4.4.4.7.1" style="font-size:144%;">430</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.4.4.4.8"><span class="ltx_text" id="S2.T9.4.4.4.8.1" style="font-size:144%;">2721.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.4.4.4.9"><span class="ltx_text" id="S2.T9.4.4.4.9.1" style="font-size:144%;">11402.4</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.11.4" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.11.4.1"><span class="ltx_text" id="S2.T9.7.7.11.4.1.1" style="font-size:144%;background-color:#E3EFFB;">R3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.11.4.2"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.11.4.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.11.4.3"><span class="ltx_text" id="S2.T9.7.7.11.4.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.11.4.4"><span class="ltx_text" id="S2.T9.7.7.11.4.4.1" style="font-size:144%;background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.11.4.5"><span class="ltx_text" id="S2.T9.7.7.11.4.5.1" style="font-size:144%;background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.11.4.6"><span class="ltx_text" id="S2.T9.7.7.11.4.6.1" style="font-size:144%;background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.11.4.7"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.11.4.7.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.11.4.8"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.11.4.8.1" style="font-size:144%;background-color:#E3EFFB;">116.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.11.4.9"><span class="ltx_text" id="S2.T9.7.7.11.4.9.1" style="font-size:144%;background-color:#E3EFFB;">50.5</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.12.5">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="9" id="S2.T9.7.7.12.5.1"><span class="ltx_text" id="S2.T9.7.7.12.5.1.1" style="font-size:144%;">1-2B Model Comparison</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.13.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T9.7.7.13.6.1"><span class="ltx_text" id="S2.T9.7.7.13.6.1.1" style="font-size:144%;">R4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.13.6.2">
<span class="ltx_text" id="S2.T9.7.7.13.6.2.1" style="font-size:144%;">MobileVLMv2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.7.7.13.6.2.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a><span class="ltx_text" id="S2.T9.7.7.13.6.2.3.2" style="font-size:144%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.13.6.3"><span class="ltx_text" id="S2.T9.7.7.13.6.3.1" style="font-size:144%;">ViT-L/14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T9.7.7.13.6.4"><span class="ltx_text" id="S2.T9.7.7.13.6.4.1" style="font-size:144%;">ML.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.13.6.5"><span class="ltx_text" id="S2.T9.7.7.13.6.5.1" style="font-size:144%;">336</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T9.7.7.13.6.6"><span class="ltx_text" id="S2.T9.7.7.13.6.6.1" style="font-size:144%;">144</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.13.6.7"><span class="ltx_text" id="S2.T9.7.7.13.6.7.1" style="font-size:144%;">304</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.13.6.8"><span class="ltx_text" id="S2.T9.7.7.13.6.8.1" style="font-size:144%;">127.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.13.6.9"><span class="ltx_text" id="S2.T9.7.7.13.6.9.1" style="font-size:144%;">458</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.14.7" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.14.7.1"><span class="ltx_text" id="S2.T9.7.7.14.7.1.1" style="font-size:144%;background-color:#E3EFFB;">R5</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.14.7.2"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.14.7.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.14.7.3"><span class="ltx_text" id="S2.T9.7.7.14.7.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.14.7.4"><span class="ltx_text" id="S2.T9.7.7.14.7.4.1" style="font-size:144%;background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.14.7.5"><span class="ltx_text" id="S2.T9.7.7.14.7.5.1" style="font-size:144%;background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.14.7.6"><span class="ltx_text" id="S2.T9.7.7.14.7.6.1" style="font-size:144%;background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.14.7.7"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.14.7.7.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.14.7.8"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.14.7.8.1" style="font-size:144%;background-color:#E3EFFB;">54.8</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.14.7.9"><span class="ltx_text" id="S2.T9.7.7.14.7.9.1" style="font-size:144%;background-color:#E3EFFB;">97.1</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.15.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T9.7.7.15.8.1"><span class="ltx_text" id="S2.T9.7.7.15.8.1.1" style="font-size:144%;">R6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.15.8.2">
<span class="ltx_text" id="S2.T9.7.7.15.8.2.1" style="font-size:144%;">DeepSeekVL </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.7.7.15.8.2.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a><span class="ltx_text" id="S2.T9.7.7.15.8.2.3.2" style="font-size:144%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.15.8.3"><span class="ltx_text" id="S2.T9.7.7.15.8.3.1" style="font-size:144%;">ViT-SO400M</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T9.7.7.15.8.4"><span class="ltx_text" id="S2.T9.7.7.15.8.4.1" style="font-size:144%;">DS.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.15.8.5"><span class="ltx_text" id="S2.T9.7.7.15.8.5.1" style="font-size:144%;">384</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T9.7.7.15.8.6"><span class="ltx_text" id="S2.T9.7.7.15.8.6.1" style="font-size:144%;">576</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.15.8.7"><span class="ltx_text" id="S2.T9.7.7.15.8.7.1" style="font-size:144%;">430</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.15.8.8"><span class="ltx_text" id="S2.T9.7.7.15.8.8.1" style="font-size:144%;">272.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.15.8.9"><span class="ltx_text" id="S2.T9.7.7.15.8.9.1" style="font-size:144%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.5.5.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.5.5.5.2"><span class="ltx_text" id="S2.T9.5.5.5.2.1" style="font-size:144%;">R7</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.5.5.5.1">
<span class="ltx_text" id="S2.T9.5.5.5.1.1" style="font-size:144%;">MM1 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.5.5.5.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a><span class="ltx_text" id="S2.T9.5.5.5.1.3.2" style="font-size:144%;">]</span></cite><sup class="ltx_sup" id="S2.T9.5.5.5.1.4"><span class="ltx_text" id="S2.T9.5.5.5.1.4.1" style="font-size:144%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.5.5.5.3"><span class="ltx_text" id="S2.T9.5.5.5.3.1" style="font-size:144%;">ViT-H</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.5.5.5.4"><span class="ltx_text" id="S2.T9.5.5.5.4.1" style="font-size:144%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.5.5.5.5"><span class="ltx_text" id="S2.T9.5.5.5.5.1" style="font-size:144%;">1344</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.5.5.5.6"><span class="ltx_text" id="S2.T9.5.5.5.6.1" style="font-size:144%;">720</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.5.5.5.7"><span class="ltx_text" id="S2.T9.5.5.5.7.1" style="font-size:144%;">632</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.5.5.5.8"><span class="ltx_text" id="S2.T9.5.5.5.8.1" style="font-size:144%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.5.5.5.9"><span class="ltx_text" id="S2.T9.5.5.5.9.1" style="font-size:144%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.16.9" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.16.9.1"><span class="ltx_text" id="S2.T9.7.7.16.9.1.1" style="font-size:144%;background-color:#E3EFFB;">R8</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.16.9.2"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.16.9.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.16.9.3"><span class="ltx_text" id="S2.T9.7.7.16.9.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.16.9.4"><span class="ltx_text" id="S2.T9.7.7.16.9.4.1" style="font-size:144%;background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.16.9.5"><span class="ltx_text" id="S2.T9.7.7.16.9.5.1" style="font-size:144%;background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.16.9.6"><span class="ltx_text" id="S2.T9.7.7.16.9.6.1" style="font-size:144%;background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.16.9.7"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.16.9.7.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.16.9.8"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.16.9.8.1" style="font-size:144%;background-color:#E3EFFB;">116.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.16.9.9"><span class="ltx_text" id="S2.T9.7.7.16.9.9.1" style="font-size:144%;background-color:#E3EFFB;">116.1</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.17.10">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="9" id="S2.T9.7.7.17.10.1"><span class="ltx_text" id="S2.T9.7.7.17.10.1.1" style="font-size:144%;">7B Model Comparison</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.18.11">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T9.7.7.18.11.1"><span class="ltx_text" id="S2.T9.7.7.18.11.1.1" style="font-size:144%;">R9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.18.11.2">
<span class="ltx_text" id="S2.T9.7.7.18.11.2.1" style="font-size:144%;">InstructBLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.7.7.18.11.2.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a><span class="ltx_text" id="S2.T9.7.7.18.11.2.3.2" style="font-size:144%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.18.11.3"><span class="ltx_text" id="S2.T9.7.7.18.11.3.1" style="font-size:144%;">ViT-g/14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T9.7.7.18.11.4"><span class="ltx_text" id="S2.T9.7.7.18.11.4.1" style="font-size:144%;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.18.11.5"><span class="ltx_text" id="S2.T9.7.7.18.11.5.1" style="font-size:144%;">224</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T9.7.7.18.11.6"><span class="ltx_text" id="S2.T9.7.7.18.11.6.1" style="font-size:144%;">32</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.18.11.7"><span class="ltx_text" id="S2.T9.7.7.18.11.7.1" style="font-size:144%;">1012</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.18.11.8"><span class="ltx_text" id="S2.T9.7.7.18.11.8.1" style="font-size:144%;">149.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.18.11.9"><span class="ltx_text" id="S2.T9.7.7.18.11.9.1" style="font-size:144%;">152.1</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.19.12" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.19.12.1"><span class="ltx_text" id="S2.T9.7.7.19.12.1.1" style="font-size:144%;background-color:#E3EFFB;">R11</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.19.12.2"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.19.12.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.19.12.3"><span class="ltx_text" id="S2.T9.7.7.19.12.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.19.12.4"><span class="ltx_text" id="S2.T9.7.7.19.12.4.1" style="font-size:144%;background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.19.12.5"><span class="ltx_text" id="S2.T9.7.7.19.12.5.1" style="font-size:144%;background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.19.12.6"><span class="ltx_text" id="S2.T9.7.7.19.12.6.1" style="font-size:144%;background-color:#E3EFFB;">16</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.19.12.7"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.19.12.7.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.19.12.8"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.19.12.8.1" style="font-size:144%;background-color:#E3EFFB;">6.8</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.19.12.9"><span class="ltx_text" id="S2.T9.7.7.19.12.9.1" style="font-size:144%;background-color:#E3EFFB;">143.4</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.20.13">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T9.7.7.20.13.1"><span class="ltx_text" id="S2.T9.7.7.20.13.1.1" style="font-size:144%;">R12</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.20.13.2">
<span class="ltx_text" id="S2.T9.7.7.20.13.2.1" style="font-size:144%;">MobileVLMv2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.7.7.20.13.2.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a><span class="ltx_text" id="S2.T9.7.7.20.13.2.3.2" style="font-size:144%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.20.13.3"><span class="ltx_text" id="S2.T9.7.7.20.13.3.1" style="font-size:144%;">ViT-L/14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T9.7.7.20.13.4"><span class="ltx_text" id="S2.T9.7.7.20.13.4.1" style="font-size:144%;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.20.13.5"><span class="ltx_text" id="S2.T9.7.7.20.13.5.1" style="font-size:144%;">336</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T9.7.7.20.13.6"><span class="ltx_text" id="S2.T9.7.7.20.13.6.1" style="font-size:144%;">144</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.20.13.7"><span class="ltx_text" id="S2.T9.7.7.20.13.7.1" style="font-size:144%;">304</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.20.13.8"><span class="ltx_text" id="S2.T9.7.7.20.13.8.1" style="font-size:144%;">127.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.20.13.9"><span class="ltx_text" id="S2.T9.7.7.20.13.9.1" style="font-size:144%;">332.1</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.21.14">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.21.14.1"><span class="ltx_text" id="S2.T9.7.7.21.14.1.1" style="font-size:144%;">R13</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.21.14.2">
<span class="ltx_text" id="S2.T9.7.7.21.14.2.1" style="font-size:144%;">ConvLLaVA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.7.7.21.14.2.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a><span class="ltx_text" id="S2.T9.7.7.21.14.2.3.2" style="font-size:144%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.21.14.3"><span class="ltx_text" id="S2.T9.7.7.21.14.3.1" style="font-size:144%;">ConvNeXT-L</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.21.14.4"><span class="ltx_text" id="S2.T9.7.7.21.14.4.1" style="font-size:144%;">Vic.</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.21.14.5"><span class="ltx_text" id="S2.T9.7.7.21.14.5.1" style="font-size:144%;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.21.14.6"><span class="ltx_text" id="S2.T9.7.7.21.14.6.1" style="font-size:144%;">144</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.21.14.7"><span class="ltx_text" id="S2.T9.7.7.21.14.7.1" style="font-size:144%;">200</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.21.14.8"><span class="ltx_text" id="S2.T9.7.7.21.14.8.1" style="font-size:144%;">164.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.21.14.9"><span class="ltx_text" id="S2.T9.7.7.21.14.9.1" style="font-size:144%;">332.1</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.22.15" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.22.15.1"><span class="ltx_text" id="S2.T9.7.7.22.15.1.1" style="font-size:144%;background-color:#E3EFFB;">R14</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.22.15.2"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.22.15.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.22.15.3"><span class="ltx_text" id="S2.T9.7.7.22.15.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.22.15.4"><span class="ltx_text" id="S2.T9.7.7.22.15.4.1" style="font-size:144%;background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.22.15.5"><span class="ltx_text" id="S2.T9.7.7.22.15.5.1" style="font-size:144%;background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.22.15.6"><span class="ltx_text" id="S2.T9.7.7.22.15.6.1" style="font-size:144%;background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.22.15.7"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.22.15.7.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.22.15.8"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.22.15.8.1" style="font-size:144%;background-color:#E3EFFB;">54.8</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.22.15.9"><span class="ltx_text" id="S2.T9.7.7.22.15.9.1" style="font-size:144%;background-color:#E3EFFB;">332.1</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.23.16" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.23.16.1"><span class="ltx_text" id="S2.T9.7.7.23.16.1.1" style="font-size:144%;background-color:#E3EFFB;">R17</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.23.16.2"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.23.16.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.23.16.3"><span class="ltx_text" id="S2.T9.7.7.23.16.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.23.16.4"><span class="ltx_text" id="S2.T9.7.7.23.16.4.1" style="font-size:144%;background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.23.16.5"><span class="ltx_text" id="S2.T9.7.7.23.16.5.1" style="font-size:144%;background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.23.16.6"><span class="ltx_text" id="S2.T9.7.7.23.16.6.1" style="font-size:144%;background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.23.16.7"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.23.16.7.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.23.16.8"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.23.16.8.1" style="font-size:144%;background-color:#E3EFFB;">54.8</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.23.16.9"><span class="ltx_text" id="S2.T9.7.7.23.16.9.1" style="font-size:144%;background-color:#E3EFFB;">391.2</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.24.17">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T9.7.7.24.17.1"><span class="ltx_text" id="S2.T9.7.7.24.17.1.1" style="font-size:144%;">R20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.24.17.2">
<span class="ltx_text" id="S2.T9.7.7.24.17.2.1" style="font-size:144%;">ConvLLaVA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.7.7.24.17.2.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a><span class="ltx_text" id="S2.T9.7.7.24.17.2.3.2" style="font-size:144%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.24.17.3"><span class="ltx_text" id="S2.T9.7.7.24.17.3.1" style="font-size:144%;">ConvNeXT-L</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T9.7.7.24.17.4"><span class="ltx_text" id="S2.T9.7.7.24.17.4.1" style="font-size:144%;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.24.17.5"><span class="ltx_text" id="S2.T9.7.7.24.17.5.1" style="font-size:144%;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T9.7.7.24.17.6"><span class="ltx_text" id="S2.T9.7.7.24.17.6.1" style="font-size:144%;">256</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.24.17.7"><span class="ltx_text" id="S2.T9.7.7.24.17.7.1" style="font-size:144%;">200</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.24.17.8"><span class="ltx_text" id="S2.T9.7.7.24.17.8.1" style="font-size:144%;">696.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.24.17.9"><span class="ltx_text" id="S2.T9.7.7.24.17.9.1" style="font-size:144%;">461.1</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.25.18">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.25.18.1"><span class="ltx_text" id="S2.T9.7.7.25.18.1.1" style="font-size:144%;">R26</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.25.18.2">
<span class="ltx_text" id="S2.T9.7.7.25.18.2.1" style="font-size:144%;">LLaVA-1.5 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.7.7.25.18.2.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a><span class="ltx_text" id="S2.T9.7.7.25.18.2.3.2" style="font-size:144%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.25.18.3" rowspan="3"><span class="ltx_text" id="S2.T9.7.7.25.18.3.1" style="font-size:144%;">ViT-L/14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.25.18.4" rowspan="3"><span class="ltx_text" id="S2.T9.7.7.25.18.4.1" style="font-size:144%;">Vic.</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.25.18.5"><span class="ltx_text" id="S2.T9.7.7.25.18.5.1" style="font-size:144%;">336</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.25.18.6"><span class="ltx_text" id="S2.T9.7.7.25.18.6.1" style="font-size:144%;">576</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.25.18.7"><span class="ltx_text" id="S2.T9.7.7.25.18.7.1" style="font-size:144%;">304</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.25.18.8"><span class="ltx_text" id="S2.T9.7.7.25.18.8.1" style="font-size:144%;">127.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.25.18.9"><span class="ltx_text" id="S2.T9.7.7.25.18.9.1" style="font-size:144%;">1170.0</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.26.19">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.26.19.1"><span class="ltx_text" id="S2.T9.7.7.26.19.1.1" style="font-size:144%;">R27</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.26.19.2">
<span class="ltx_text" id="S2.T9.7.7.26.19.2.1" style="font-size:144%;">MobileVLMv2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.7.7.26.19.2.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a><span class="ltx_text" id="S2.T9.7.7.26.19.2.3.2" style="font-size:144%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.26.19.3"><span class="ltx_text" id="S2.T9.7.7.26.19.3.1" style="font-size:144%;">336</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.26.19.4"><span class="ltx_text" id="S2.T9.7.7.26.19.4.1" style="font-size:144%;">576</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.26.19.5"><span class="ltx_text" id="S2.T9.7.7.26.19.5.1" style="font-size:144%;">304</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.26.19.6"><span class="ltx_text" id="S2.T9.7.7.26.19.6.1" style="font-size:144%;">127.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.26.19.7"><span class="ltx_text" id="S2.T9.7.7.26.19.7.1" style="font-size:144%;">1170.0</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.27.20">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.27.20.1"><span class="ltx_text" id="S2.T9.7.7.27.20.1.1" style="font-size:144%;">R28</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.27.20.2">
<span class="ltx_text" id="S2.T9.7.7.27.20.2.1" style="font-size:144%;">ShareGPT4V </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.7.7.27.20.2.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a><span class="ltx_text" id="S2.T9.7.7.27.20.2.3.2" style="font-size:144%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.27.20.3"><span class="ltx_text" id="S2.T9.7.7.27.20.3.1" style="font-size:144%;">336</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.27.20.4"><span class="ltx_text" id="S2.T9.7.7.27.20.4.1" style="font-size:144%;">576</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.27.20.5"><span class="ltx_text" id="S2.T9.7.7.27.20.5.1" style="font-size:144%;">304</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.27.20.6"><span class="ltx_text" id="S2.T9.7.7.27.20.6.1" style="font-size:144%;">127.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.27.20.7"><span class="ltx_text" id="S2.T9.7.7.27.20.7.1" style="font-size:144%;">1170.0</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.28.21">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.28.21.1"><span class="ltx_text" id="S2.T9.7.7.28.21.1.1" style="font-size:144%;">R29</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.28.21.2">
<span class="ltx_text" id="S2.T9.7.7.28.21.2.1" style="font-size:144%;">ViTamin </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.7.7.28.21.2.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a><span class="ltx_text" id="S2.T9.7.7.28.21.2.3.2" style="font-size:144%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.28.21.3"><span class="ltx_text" id="S2.T9.7.7.28.21.3.1" style="font-size:144%;">ViTamin-L</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.28.21.4"><span class="ltx_text" id="S2.T9.7.7.28.21.4.1" style="font-size:144%;">Vic.</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.28.21.5"><span class="ltx_text" id="S2.T9.7.7.28.21.5.1" style="font-size:144%;">384</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.28.21.6"><span class="ltx_text" id="S2.T9.7.7.28.21.6.1" style="font-size:144%;">576</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.28.21.7"><span class="ltx_text" id="S2.T9.7.7.28.21.7.1" style="font-size:144%;">333</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.28.21.8"><span class="ltx_text" id="S2.T9.7.7.28.21.8.1" style="font-size:144%;">137.6</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.28.21.9"><span class="ltx_text" id="S2.T9.7.7.28.21.9.1" style="font-size:144%;">1170.0</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.29.22">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.29.22.1"><span class="ltx_text" id="S2.T9.7.7.29.22.1.1" style="font-size:144%;">R30</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.29.22.2">
<span class="ltx_text" id="S2.T9.7.7.29.22.2.1" style="font-size:144%;">ConvLLaVA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.7.7.29.22.2.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a><span class="ltx_text" id="S2.T9.7.7.29.22.2.3.2" style="font-size:144%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.29.22.3"><span class="ltx_text" id="S2.T9.7.7.29.22.3.1" style="font-size:144%;">ConvNeXT-L</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.29.22.4"><span class="ltx_text" id="S2.T9.7.7.29.22.4.1" style="font-size:144%;">Vic.</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.29.22.5"><span class="ltx_text" id="S2.T9.7.7.29.22.5.1" style="font-size:144%;">1536</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.29.22.6"><span class="ltx_text" id="S2.T9.7.7.29.22.6.1" style="font-size:144%;">576</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.29.22.7"><span class="ltx_text" id="S2.T9.7.7.29.22.7.1" style="font-size:144%;">200</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.29.22.8"><span class="ltx_text" id="S2.T9.7.7.29.22.8.1" style="font-size:144%;">1569.7</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.29.22.9"><span class="ltx_text" id="S2.T9.7.7.29.22.9.1" style="font-size:144%;">1170.0</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.30.23">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.30.23.1"><span class="ltx_text" id="S2.T9.7.7.30.23.1.1" style="font-size:144%;">R31</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.30.23.2">
<span class="ltx_text" id="S2.T9.7.7.30.23.2.1" style="font-size:144%;">VILA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.7.7.30.23.2.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a><span class="ltx_text" id="S2.T9.7.7.30.23.2.3.2" style="font-size:144%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.30.23.3"><span class="ltx_text" id="S2.T9.7.7.30.23.3.1" style="font-size:144%;">ViT-L/14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.30.23.4"><span class="ltx_text" id="S2.T9.7.7.30.23.4.1" style="font-size:144%;">L-2</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.30.23.5"><span class="ltx_text" id="S2.T9.7.7.30.23.5.1" style="font-size:144%;">336</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.30.23.6"><span class="ltx_text" id="S2.T9.7.7.30.23.6.1" style="font-size:144%;">576</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.30.23.7"><span class="ltx_text" id="S2.T9.7.7.30.23.7.1" style="font-size:144%;">304</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.30.23.8"><span class="ltx_text" id="S2.T9.7.7.30.23.8.1" style="font-size:144%;">127.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.30.23.9"><span class="ltx_text" id="S2.T9.7.7.30.23.9.1" style="font-size:144%;">1169.5</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.6.6.6.2"><span class="ltx_text" id="S2.T9.6.6.6.2.1" style="font-size:144%;">R33</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.6.6.6.1">
<span class="ltx_text" id="S2.T9.6.6.6.1.1" style="font-size:144%;">MM1 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T9.6.6.6.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a><span class="ltx_text" id="S2.T9.6.6.6.1.3.2" style="font-size:144%;">]</span></cite><sup class="ltx_sup" id="S2.T9.6.6.6.1.4"><span class="ltx_text" id="S2.T9.6.6.6.1.4.1" style="font-size:144%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.6.6.6.3"><span class="ltx_text" id="S2.T9.6.6.6.3.1" style="font-size:144%;">ViT-H</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.6.6.6.4"><span class="ltx_text" id="S2.T9.6.6.6.4.1" style="font-size:144%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.6.6.6.5"><span class="ltx_text" id="S2.T9.6.6.6.5.1" style="font-size:144%;">1344</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.6.6.6.6"><span class="ltx_text" id="S2.T9.6.6.6.6.1" style="font-size:144%;">720</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.6.6.6.7"><span class="ltx_text" id="S2.T9.6.6.6.7.1" style="font-size:144%;">632</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.6.6.6.8"><span class="ltx_text" id="S2.T9.6.6.6.8.1" style="font-size:144%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.6.6.6.9"><span class="ltx_text" id="S2.T9.6.6.6.9.1" style="font-size:144%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.7.2"><span class="ltx_text" id="S2.T9.7.7.7.2.1" style="font-size:144%;">R34</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.7.1">
<span class="ltx_text" id="S2.T9.7.7.7.1.1" style="font-size:144%;">LLaVA-NeXT</span><sup class="ltx_sup" id="S2.T9.7.7.7.1.2"><span class="ltx_text" id="S2.T9.7.7.7.1.2.1" style="font-size:144%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.7.3"><span class="ltx_text" id="S2.T9.7.7.7.3.1" style="font-size:144%;">ViT-L/14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.7.4"><span class="ltx_text" id="S2.T9.7.7.7.4.1" style="font-size:144%;">L-3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.7.5"><span class="ltx_text" id="S2.T9.7.7.7.5.1" style="font-size:144%;">672</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.7.6"><span class="ltx_text" id="S2.T9.7.7.7.6.1" style="font-size:144%;">2880</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.7.7"><span class="ltx_text" id="S2.T9.7.7.7.7.1" style="font-size:144%;">304</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.7.8"><span class="ltx_text" id="S2.T9.7.7.7.8.1" style="font-size:144%;">637.0</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.7.9"><span class="ltx_text" id="S2.T9.7.7.7.9.1" style="font-size:144%;">19709.7</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.31.24" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.31.24.1"><span class="ltx_text" id="S2.T9.7.7.31.24.1.1" style="font-size:144%;background-color:#E3EFFB;">R21</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.31.24.2"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.31.24.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.31.24.3"><span class="ltx_text" id="S2.T9.7.7.31.24.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.31.24.4"><span class="ltx_text" id="S2.T9.7.7.31.24.4.1" style="font-size:144%;background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.31.24.5"><span class="ltx_text" id="S2.T9.7.7.31.24.5.1" style="font-size:144%;background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.31.24.6"><span class="ltx_text" id="S2.T9.7.7.31.24.6.1" style="font-size:144%;background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.31.24.7"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.31.24.7.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.31.24.8"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.31.24.8.1" style="font-size:144%;background-color:#E3EFFB;">116.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.31.24.9"><span class="ltx_text" id="S2.T9.7.7.31.24.9.1" style="font-size:144%;background-color:#E3EFFB;">461.1</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.32.25" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.32.25.1"><span class="ltx_text" id="S2.T9.7.7.32.25.1.1" style="font-size:144%;background-color:#E3EFFB;">R36</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.32.25.2"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.32.25.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.32.25.3"><span class="ltx_text" id="S2.T9.7.7.32.25.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.32.25.4"><span class="ltx_text" id="S2.T9.7.7.32.25.4.1" style="font-size:144%;background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.32.25.5"><span class="ltx_text" id="S2.T9.7.7.32.25.5.1" style="font-size:144%;background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.32.25.6"><span class="ltx_text" id="S2.T9.7.7.32.25.6.1" style="font-size:144%;background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.32.25.7"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.32.25.7.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.32.25.8"><span class="ltx_text ltx_font_bold" id="S2.T9.7.7.32.25.8.1" style="font-size:144%;background-color:#E3EFFB;">116.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.32.25.9"><span class="ltx_text" id="S2.T9.7.7.32.25.9.1" style="font-size:144%;background-color:#E3EFFB;">524.5</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.33.26">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="9" id="S2.T9.7.7.33.26.1"><span class="ltx_text" id="S2.T9.7.7.33.26.1.1" style="font-size:144%;">VLMs with Multiple Vision Encoders and 8B LLM</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.34.27">
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T9.7.7.34.27.1"></td>
<td class="ltx_td ltx_border_t" id="S2.T9.7.7.34.27.2"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.34.27.3"><span class="ltx_text" id="S2.T9.7.7.34.27.3.1" style="font-size:144%;background-color:#F2F2F2;">ConvNeXT-L</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T9.7.7.34.27.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.34.27.5"><span class="ltx_text" id="S2.T9.7.7.34.27.5.1" style="font-size:144%;background-color:#F2F2F2;">1536</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T9.7.7.34.27.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.34.27.7"><span class="ltx_text" id="S2.T9.7.7.34.27.7.1" style="font-size:144%;background-color:#F2F2F2;">200</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T9.7.7.34.27.8"><span class="ltx_text" id="S2.T9.7.7.34.27.8.1" style="font-size:144%;background-color:#F2F2F2;">1569.7</span></td>
<td class="ltx_td ltx_border_t" id="S2.T9.7.7.34.27.9"></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.35.28" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T9.7.7.35.28.1"><span class="ltx_text" id="S2.T9.7.7.35.28.1.1" style="font-size:144%;background-color:#F2F2F2;">R35</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.35.28.2"><span class="ltx_text" id="S2.T9.7.7.35.28.2.1" style="font-size:144%;background-color:#F2F2F2;">MiniGemini-HD</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.35.28.3"><span class="ltx_text" id="S2.T9.7.7.35.28.3.1" style="font-size:144%;background-color:#F2F2F2;">ViT-L/14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.35.28.4"><span class="ltx_text" id="S2.T9.7.7.35.28.4.1" style="font-size:144%;background-color:#F2F2F2;">L-3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.35.28.5"><span class="ltx_text" id="S2.T9.7.7.35.28.5.1" style="font-size:144%;background-color:#F2F2F2;">672</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T9.7.7.35.28.6"><span class="ltx_text" id="S2.T9.7.7.35.28.6.1" style="font-size:144%;background-color:#F2F2F2;">2880</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.35.28.7"><span class="ltx_text" id="S2.T9.7.7.35.28.7.1" style="font-size:144%;background-color:#F2F2F2;">304</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.35.28.8"><span class="ltx_text" id="S2.T9.7.7.35.28.8.1" style="font-size:144%;background-color:#F2F2F2;">552.6</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.35.28.9"><span class="ltx_text" id="S2.T9.7.7.35.28.9.1" style="font-size:144%;background-color:#F2F2F2;">19709.7</span></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.36.29">
<td class="ltx_td ltx_border_r" id="S2.T9.7.7.36.29.1"></td>
<td class="ltx_td" id="S2.T9.7.7.36.29.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.36.29.3"><span class="ltx_text" id="S2.T9.7.7.36.29.3.1" style="font-size:144%;background-color:#F2F2F2;">ViT-SO400M</span></td>
<td class="ltx_td ltx_border_r" id="S2.T9.7.7.36.29.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.36.29.5"><span class="ltx_text" id="S2.T9.7.7.36.29.5.1" style="font-size:144%;background-color:#F2F2F2;">384</span></td>
<td class="ltx_td ltx_border_r" id="S2.T9.7.7.36.29.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.36.29.7"><span class="ltx_text" id="S2.T9.7.7.36.29.7.1" style="font-size:144%;background-color:#F2F2F2;">430</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.36.29.8"><span class="ltx_text" id="S2.T9.7.7.36.29.8.1" style="font-size:144%;background-color:#F2F2F2;">272.1</span></td>
<td class="ltx_td" id="S2.T9.7.7.36.29.9"></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.37.30">
<td class="ltx_td ltx_border_r" id="S2.T9.7.7.37.30.1"></td>
<td class="ltx_td" id="S2.T9.7.7.37.30.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.37.30.3"><span class="ltx_text" id="S2.T9.7.7.37.30.3.1" style="font-size:144%;background-color:#F2F2F2;">ConvNeXt-XXL</span></td>
<td class="ltx_td ltx_border_r" id="S2.T9.7.7.37.30.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.37.30.5"><span class="ltx_text" id="S2.T9.7.7.37.30.5.1" style="font-size:144%;background-color:#F2F2F2;">1024</span></td>
<td class="ltx_td ltx_border_r" id="S2.T9.7.7.37.30.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.37.30.7"><span class="ltx_text" id="S2.T9.7.7.37.30.7.1" style="font-size:144%;background-color:#F2F2F2;">846</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.37.30.8"><span class="ltx_text" id="S2.T9.7.7.37.30.8.1" style="font-size:144%;background-color:#F2F2F2;">2290.4</span></td>
<td class="ltx_td" id="S2.T9.7.7.37.30.9"></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.38.31">
<td class="ltx_td ltx_border_r" id="S2.T9.7.7.38.31.1"></td>
<td class="ltx_td" id="S2.T9.7.7.38.31.2"></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.38.31.3"><span class="ltx_text" id="S2.T9.7.7.38.31.3.1" style="font-size:144%;background-color:#F2F2F2;">DINOv2-ViT-L/14</span></td>
<td class="ltx_td ltx_border_r" id="S2.T9.7.7.38.31.4"></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.38.31.5"><span class="ltx_text" id="S2.T9.7.7.38.31.5.1" style="font-size:144%;background-color:#F2F2F2;">518</span></td>
<td class="ltx_td ltx_border_r" id="S2.T9.7.7.38.31.6"></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.38.31.7"><span class="ltx_text" id="S2.T9.7.7.38.31.7.1" style="font-size:144%;background-color:#F2F2F2;">304</span></td>
<td class="ltx_td ltx_align_center" id="S2.T9.7.7.38.31.8"><span class="ltx_text" id="S2.T9.7.7.38.31.8.1" style="font-size:144%;background-color:#F2F2F2;">1171.5</span></td>
<td class="ltx_td" id="S2.T9.7.7.38.31.9"></td>
</tr>
<tr class="ltx_tr" id="S2.T9.7.7.39.32" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S2.T9.7.7.39.32.1"><span class="ltx_text" id="S2.T9.7.7.39.32.1.1" style="font-size:144%;background-color:#F2F2F2;">R36</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T9.7.7.39.32.2"><span class="ltx_text" id="S2.T9.7.7.39.32.2.1" style="font-size:144%;background-color:#F2F2F2;">Cambrian-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T9.7.7.39.32.3"><span class="ltx_text" id="S2.T9.7.7.39.32.3.1" style="font-size:144%;background-color:#F2F2F2;">ViT-L/14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T9.7.7.39.32.4"><span class="ltx_text" id="S2.T9.7.7.39.32.4.1" style="font-size:144%;background-color:#F2F2F2;">L-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T9.7.7.39.32.5"><span class="ltx_text" id="S2.T9.7.7.39.32.5.1" style="font-size:144%;background-color:#F2F2F2;">336</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T9.7.7.39.32.6"><span class="ltx_text" id="S2.T9.7.7.39.32.6.1" style="font-size:144%;background-color:#F2F2F2;">576</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T9.7.7.39.32.7"><span class="ltx_text" id="S2.T9.7.7.39.32.7.1" style="font-size:144%;background-color:#F2F2F2;">304</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T9.7.7.39.32.8"><span class="ltx_text" id="S2.T9.7.7.39.32.8.1" style="font-size:144%;background-color:#F2F2F2;">127.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T9.7.7.39.32.9"><span class="ltx_text" id="S2.T9.7.7.39.32.9.1" style="font-size:144%;background-color:#F2F2F2;">1223.6</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T9.38.2.1" style="font-size:63%;">Table 9</span>: </span><span class="ltx_text" id="S2.T9.9.1" style="font-size:63%;">
<span class="ltx_text ltx_font_bold" id="S2.T9.9.1.1">Breakdown of prefilling latencies for recent methods.</span> The models are grouped based on total number of visual tokens. For models that were difficult to export or unavailable, we mark them as ’-’ in the table. “Vic.” refers to Vicuna <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite>, “Qw.2” refers to Qwen2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite> and “Qw.” refers to Qwen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. “L-2” refers to LLaMA-2. “L-3” refers to LLaMA-3. “ML.” refers to MobileLLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>. “DS.” refers to DeepSeek LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>. <sup class="ltx_sup" id="S2.T9.9.1.2">∗</sup> For input resolution and visual tokens, we report the highest supported resolution by the respective models as some models like LLaVA-OneVision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> and MM1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> use dynamic input resolution. For VLMs that use multiple vision encoders, the size of each encoder is listed independently, for TTFT, the latency from each encoder is summed up.
</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3a">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">C </span>Additional Results</h2>
<div class="ltx_para" id="S3a.p1">
<p class="ltx_p" id="S3a.p1.2"><span class="ltx_text" id="S3a.p1.2.1" style="font-size:144%;">We present the performance of FastVLM on text-rich benchmarks under various training settings in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T10" style="font-size:144%;" title="In C Additional Results ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">10</span></a><span class="ltx_text" id="S3a.p1.2.2" style="font-size:144%;">. FastVLM surpasses MM1 and Cambrian-1 across a wide range of benchmarks by scaling up pretraining and instruction tuning datasets. This result highlights the quality of visual tokens produced by FastViTHD, as FastVLM is able to achieve these improvements with 2.8</span><math alttext="\times" class="ltx_Math" display="inline" id="S3a.p1.1.m1.1"><semantics id="S3a.p1.1.m1.1a"><mo id="S3a.p1.1.m1.1.1" mathsize="144%" xref="S3a.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3a.p1.1.m1.1b"><times id="S3a.p1.1.m1.1.1.cmml" xref="S3a.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3a.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3a.p1.1.m1.1d">×</annotation></semantics></math><span class="ltx_text" id="S3a.p1.2.3" style="font-size:144%;"> less visual tokens than MM1 and with a vision encoder that is 5.1</span><math alttext="\times" class="ltx_Math" display="inline" id="S3a.p1.2.m2.1"><semantics id="S3a.p1.2.m2.1a"><mo id="S3a.p1.2.m2.1.1" mathsize="144%" xref="S3a.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3a.p1.2.m2.1b"><times id="S3a.p1.2.m2.1.1.cmml" xref="S3a.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3a.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3a.p1.2.m2.1d">×</annotation></semantics></math><span class="ltx_text" id="S3a.p1.2.4" style="font-size:144%;"> smaller.</span></p>
</div>
<figure class="ltx_table" id="S3.T10">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T10.4" style="width:451.0pt;height:135.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-343.8pt,103.3pt) scale(0.396101410911012,0.396101410911012) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T10.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T10.4.4.5.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T10.4.4.5.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.1.1" style="font-size:144%;">Row</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T10.4.4.5.1.2" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.2.1" style="font-size:144%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T10.4.4.5.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.3.1" style="font-size:144%;">Vision</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T10.4.4.5.1.4" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.4.1" style="font-size:144%;">LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T10.4.4.5.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.5.1" style="font-size:144%;">Data (M)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T10.4.4.5.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.6.1" style="font-size:144%;">Input</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T10.4.4.5.1.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.7.1" style="font-size:144%;">#Visual</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T10.4.4.5.1.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.8.1" style="font-size:144%;">Vis. Enc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T10.4.4.5.1.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.9.1" style="font-size:144%;">TTFT</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T10.4.4.5.1.10" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.10.1" style="font-size:144%;">ChartQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T10.4.4.5.1.11" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.11.1" style="font-size:144%;">OCRBench</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T10.4.4.5.1.12" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.12.1" style="font-size:144%;">TextVQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T10.4.4.5.1.13" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.13.1" style="font-size:144%;">DocVQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T10.4.4.5.1.14" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.5.1.14.1" style="font-size:144%;">InfoVQA</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T10.2.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.2.2.2.3.1" style="font-size:144%;">Ann.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.2.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.2.2.2.4.1" style="font-size:144%;">Encoder</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.2.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.2.2.2.5.1" style="font-size:144%;">(PT+IT)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.2.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.2.2.2.6.1" style="font-size:144%;">Res.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.2.2.2.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.2.2.2.7.1" style="font-size:144%;">Tokens</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.T10.1.1.1.1.1" style="font-size:144%;">Size(M)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T10.1.1.1.1.m1.1"><semantics id="S3.T10.1.1.1.1.m1.1a"><mo id="S3.T10.1.1.1.1.m1.1.1" mathsize="144%" stretchy="false" xref="S3.T10.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T10.1.1.1.1.m1.1b"><ci id="S3.T10.1.1.1.1.m1.1.1.cmml" xref="S3.T10.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T10.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T10.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.2.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.T10.2.2.2.2.1" style="font-size:144%;">(ms)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T10.2.2.2.2.m1.1"><semantics id="S3.T10.2.2.2.2.m1.1a"><mo id="S3.T10.2.2.2.2.m1.1.1" mathsize="144%" stretchy="false" xref="S3.T10.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T10.2.2.2.2.m1.1b"><ci id="S3.T10.2.2.2.2.m1.1.1.cmml" xref="S3.T10.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T10.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T10.2.2.2.2.m1.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S3.T10.3.3.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T10.3.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.2.1" style="font-size:144%;">R1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.3.3.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.T10.3.3.3.1.1" style="font-size:144%;">MM1 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T10.3.3.3.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a><span class="ltx_text" id="S3.T10.3.3.3.1.3.2" style="font-size:144%;">]</span></cite><sup class="ltx_sup" id="S3.T10.3.3.3.1.4"><span class="ltx_text" id="S3.T10.3.3.3.1.4.1" style="font-size:144%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.3.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.3.1" style="font-size:144%;">ViT-H</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.3.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.4.1" style="font-size:144%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T10.3.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.5.1" style="font-size:144%;">3000+1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.3.3.3.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.6.1" style="font-size:144%;">1344</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T10.3.3.3.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.7.1" style="font-size:144%;">720</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.3.3.3.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.8.1" style="font-size:144%;">632</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T10.3.3.3.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.9.1" style="font-size:144%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.3.3.3.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.10.1" style="font-size:144%;">72.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.3.3.3.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.11.1" style="font-size:144%;">62.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.3.3.3.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.12.1" style="font-size:144%;">72.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.3.3.3.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.13.1" style="font-size:144%;">76.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.3.3.3.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.3.3.3.14.1" style="font-size:144%;">45.5</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T10.4.4.4.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.2.1" style="font-size:144%;">R2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S3.T10.4.4.4.1.1" style="font-size:144%;">LLaVA-NeXT</span><sup class="ltx_sup" id="S3.T10.4.4.4.1.2"><span class="ltx_text" id="S3.T10.4.4.4.1.2.1" style="font-size:144%;">†</span></sup><sup class="ltx_sup" id="S3.T10.4.4.4.1.3"><span class="ltx_text" id="S3.T10.4.4.4.1.3.1" style="font-size:144%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.4.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.3.1" style="font-size:144%;">ViT-L/14</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.4.1" style="font-size:144%;">L-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.4.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.5.1" style="font-size:144%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.6.1" style="font-size:144%;">672</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.4.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.7.1" style="font-size:144%;">2880</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.4.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.8.1" style="font-size:144%;">304</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.4.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.9.1" style="font-size:144%;">20347</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.4.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.10.1" style="font-size:144%;">69.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.4.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.11.1" style="font-size:144%;">49.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.4.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.12.1" style="font-size:144%;">64.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.4.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.13.1" style="font-size:144%;">72.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.4.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.4.14.1" style="font-size:144%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.6.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T10.4.4.6.2.1" rowspan="4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.1.1" style="font-size:144%;">R3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.6.2.2" rowspan="4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.2.1" style="font-size:144%;">Cambrian-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.6.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.3.1" style="font-size:144%;">ViT-L/14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.6.2.4" rowspan="4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.4.1" style="font-size:144%;">L-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T10.4.4.6.2.5" rowspan="4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.5.1" style="font-size:144%;">2.5+7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.6.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.6.1" style="font-size:144%;">336</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T10.4.4.6.2.7" rowspan="4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.7.1" style="font-size:144%;">576</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.6.2.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.8.1" style="font-size:144%;">304</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T10.4.4.6.2.9" rowspan="4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.9.1" style="font-size:144%;">5085</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.6.2.10" rowspan="4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.10.1" style="font-size:144%;">73.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.6.2.11" rowspan="4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.11.1" style="font-size:144%;">62.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.6.2.12" rowspan="4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.12.1" style="font-size:144%;">71.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.6.2.13" rowspan="4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.13.1" style="font-size:144%;">77.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.6.2.14" rowspan="4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.6.2.14.1" style="font-size:144%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.7.3">
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.7.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.7.3.1.1" style="font-size:144%;">ViT-SO400M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.7.3.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.7.3.2.1" style="font-size:144%;">384</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.7.3.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.7.3.3.1" style="font-size:144%;">430</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.8.4">
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.8.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.8.4.1.1" style="font-size:144%;">ConvNeXt-XXL</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.8.4.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.8.4.2.1" style="font-size:144%;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.8.4.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.8.4.3.1" style="font-size:144%;">846</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.9.5">
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.9.5.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.9.5.1.1" style="font-size:144%;">DINOv2-ViT-L/14</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.9.5.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.9.5.2.1" style="font-size:144%;">518</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.9.5.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.9.5.3.1" style="font-size:144%;">304</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.10.6" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T10.4.4.10.6.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.10.6.1.1" style="font-size:144%;background-color:#E3EFFB;">R4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.10.6.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.10.6.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.10.6.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.10.6.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.10.6.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.10.6.4.1" style="font-size:144%;background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T10.4.4.10.6.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.10.6.5.1" style="font-size:144%;background-color:#E3EFFB;">0.5+0.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.10.6.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.10.6.6.1" style="font-size:144%;background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T10.4.4.10.6.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.10.6.7.1" style="font-size:144%;background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.10.6.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.10.6.8.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T10.4.4.10.6.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.10.6.9.1" style="font-size:144%;background-color:#E3EFFB;">387</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.10.6.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.10.6.10.1" style="font-size:144%;background-color:#E3EFFB;">17.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.10.6.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.10.6.11.1" style="font-size:144%;background-color:#E3EFFB;">30.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.10.6.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.10.6.12.1" style="font-size:144%;background-color:#E3EFFB;">62.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.10.6.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.10.6.13.1" style="font-size:144%;background-color:#E3EFFB;">32.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.10.6.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.10.6.14.1" style="font-size:144%;background-color:#E3EFFB;">28.7</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.11.7" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T10.4.4.11.7.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.11.7.1.1" style="font-size:144%;background-color:#E3EFFB;">R5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.11.7.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.11.7.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.11.7.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.11.7.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.11.7.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.11.7.4.1" style="font-size:144%;background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.11.7.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.11.7.5.1" style="font-size:144%;background-color:#E3EFFB;">0.5+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.11.7.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.11.7.6.1" style="font-size:144%;background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.11.7.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.11.7.7.1" style="font-size:144%;background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.11.7.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.11.7.8.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.11.7.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.11.7.9.1" style="font-size:144%;background-color:#E3EFFB;">387</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.11.7.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.11.7.10.1" style="font-size:144%;background-color:#E3EFFB;">59.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.11.7.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.11.7.11.1" style="font-size:144%;background-color:#E3EFFB;">38.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.11.7.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.11.7.12.1" style="font-size:144%;background-color:#E3EFFB;">67.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.11.7.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.11.7.13.1" style="font-size:144%;background-color:#E3EFFB;">57.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.11.7.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.11.7.14.1" style="font-size:144%;background-color:#E3EFFB;">29.7</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.12.8" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T10.4.4.12.8.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.12.8.1.1" style="font-size:144%;background-color:#E3EFFB;">R6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.12.8.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.12.8.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.12.8.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.12.8.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.12.8.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.12.8.4.1" style="font-size:144%;background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.12.8.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.12.8.5.1" style="font-size:144%;background-color:#E3EFFB;">15+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.12.8.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.12.8.6.1" style="font-size:144%;background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.12.8.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.12.8.7.1" style="font-size:144%;background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.12.8.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.12.8.8.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.12.8.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.12.8.9.1" style="font-size:144%;background-color:#E3EFFB;">387</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.12.8.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.12.8.10.1" style="font-size:144%;background-color:#E3EFFB;">65.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.12.8.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.12.8.11.1" style="font-size:144%;background-color:#E3EFFB;">45.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.12.8.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.12.8.12.1" style="font-size:144%;background-color:#E3EFFB;">69.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.12.8.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.12.8.13.1" style="font-size:144%;background-color:#E3EFFB;">65.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.12.8.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.12.8.14.1" style="font-size:144%;background-color:#E3EFFB;">32.0</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.13.9" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T10.4.4.13.9.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.13.9.1.1" style="font-size:144%;background-color:#E3EFFB;">R7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.13.9.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.13.9.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.13.9.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.13.9.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.13.9.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.13.9.4.1" style="font-size:144%;background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.13.9.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.13.9.5.1" style="font-size:144%;background-color:#E3EFFB;">15+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.13.9.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.13.9.6.1" style="font-size:144%;background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.13.9.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.13.9.7.1" style="font-size:144%;background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.13.9.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.13.9.8.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.13.9.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.13.9.9.1" style="font-size:144%;background-color:#E3EFFB;">446</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.13.9.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.13.9.10.1" style="font-size:144%;background-color:#E3EFFB;">69.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.13.9.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.13.9.11.1" style="font-size:144%;background-color:#E3EFFB;">45.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.13.9.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.13.9.12.1" style="font-size:144%;background-color:#E3EFFB;">69.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.13.9.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.13.9.13.1" style="font-size:144%;background-color:#E3EFFB;">66.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.13.9.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.13.9.14.1" style="font-size:144%;background-color:#E3EFFB;">34.3</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.14.10" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T10.4.4.14.10.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.14.10.1.1" style="font-size:144%;background-color:#E3EFFB;">R8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.14.10.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.14.10.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.14.10.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.14.10.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.14.10.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.14.10.4.1" style="font-size:144%;background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.14.10.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.14.10.5.1" style="font-size:144%;background-color:#E3EFFB;">15+11.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.14.10.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.14.10.6.1" style="font-size:144%;background-color:#E3EFFB;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.14.10.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.14.10.7.1" style="font-size:144%;background-color:#E3EFFB;">144</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.14.10.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.14.10.8.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.14.10.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.14.10.9.1" style="font-size:144%;background-color:#E3EFFB;">446</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.14.10.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.14.10.10.1" style="font-size:144%;background-color:#E3EFFB;">74.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.14.10.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.14.10.11.1" style="font-size:144%;background-color:#E3EFFB;">59.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.14.10.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.14.10.12.1" style="font-size:144%;background-color:#E3EFFB;">72.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.14.10.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.14.10.13.1" style="font-size:144%;background-color:#E3EFFB;">72.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.14.10.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.14.10.14.1" style="font-size:144%;background-color:#E3EFFB;">44.3</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.15.11" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T10.4.4.15.11.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.15.11.1.1" style="font-size:144%;background-color:#E3EFFB;">R9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.15.11.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.15.11.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.15.11.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.15.11.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.15.11.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.15.11.4.1" style="font-size:144%;background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T10.4.4.15.11.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.15.11.5.1" style="font-size:144%;background-color:#E3EFFB;">0.5+0.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.15.11.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.15.11.6.1" style="font-size:144%;background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T10.4.4.15.11.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.15.11.7.1" style="font-size:144%;background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.15.11.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.15.11.8.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T10.4.4.15.11.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.15.11.9.1" style="font-size:144%;background-color:#E3EFFB;">577</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.15.11.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.15.11.10.1" style="font-size:144%;background-color:#E3EFFB;">19.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.15.11.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.15.11.11.1" style="font-size:144%;background-color:#E3EFFB;">29.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.15.11.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.15.11.12.1" style="font-size:144%;background-color:#E3EFFB;">64.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.15.11.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.15.11.13.1" style="font-size:144%;background-color:#E3EFFB;">35.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T10.4.4.15.11.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.15.11.14.1" style="font-size:144%;background-color:#E3EFFB;">28.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.16.12" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T10.4.4.16.12.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.16.12.1.1" style="font-size:144%;background-color:#E3EFFB;">R10</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.16.12.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.16.12.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.16.12.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.16.12.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.16.12.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.16.12.4.1" style="font-size:144%;background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.16.12.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.16.12.5.1" style="font-size:144%;background-color:#E3EFFB;">0.5+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.16.12.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.16.12.6.1" style="font-size:144%;background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.16.12.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.16.12.7.1" style="font-size:144%;background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.16.12.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.16.12.8.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.16.12.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.16.12.9.1" style="font-size:144%;background-color:#E3EFFB;">577</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.16.12.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.16.12.10.1" style="font-size:144%;background-color:#E3EFFB;">61.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.16.12.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.16.12.11.1" style="font-size:144%;background-color:#E3EFFB;">38.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.16.12.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.16.12.12.1" style="font-size:144%;background-color:#E3EFFB;">67.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.16.12.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.16.12.13.1" style="font-size:144%;background-color:#E3EFFB;">62.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.16.12.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.16.12.14.1" style="font-size:144%;background-color:#E3EFFB;">32.0</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.17.13" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T10.4.4.17.13.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.17.13.1.1" style="font-size:144%;background-color:#E3EFFB;">R11</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.17.13.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.17.13.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.17.13.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.17.13.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.17.13.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.17.13.4.1" style="font-size:144%;background-color:#E3EFFB;">Vic.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.17.13.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.17.13.5.1" style="font-size:144%;background-color:#E3EFFB;">15+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.17.13.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.17.13.6.1" style="font-size:144%;background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.17.13.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.17.13.7.1" style="font-size:144%;background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.17.13.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.17.13.8.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.17.13.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.17.13.9.1" style="font-size:144%;background-color:#E3EFFB;">577</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.17.13.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.17.13.10.1" style="font-size:144%;background-color:#E3EFFB;">66.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.17.13.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.17.13.11.1" style="font-size:144%;background-color:#E3EFFB;">47.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.17.13.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.17.13.12.1" style="font-size:144%;background-color:#E3EFFB;">70.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.17.13.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.17.13.13.1" style="font-size:144%;background-color:#E3EFFB;">72.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.17.13.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.17.13.14.1" style="font-size:144%;background-color:#E3EFFB;">34.7</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.18.14" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T10.4.4.18.14.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.18.14.1.1" style="font-size:144%;background-color:#E3EFFB;">R12</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.18.14.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.18.14.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.18.14.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.18.14.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.18.14.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.18.14.4.1" style="font-size:144%;background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.18.14.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.18.14.5.1" style="font-size:144%;background-color:#E3EFFB;">15+1.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.18.14.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.18.14.6.1" style="font-size:144%;background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.18.14.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.18.14.7.1" style="font-size:144%;background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.18.14.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.18.14.8.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.18.14.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.18.14.9.1" style="font-size:144%;background-color:#E3EFFB;">641</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.18.14.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.18.14.10.1" style="font-size:144%;background-color:#E3EFFB;">71.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.18.14.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.18.14.11.1" style="font-size:144%;background-color:#E3EFFB;">49.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.18.14.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.18.14.12.1" style="font-size:144%;background-color:#E3EFFB;">72.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.18.14.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.18.14.13.1" style="font-size:144%;background-color:#E3EFFB;">73.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.18.14.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.18.14.14.1" style="font-size:144%;background-color:#E3EFFB;">37.5</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.19.15" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T10.4.4.19.15.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.19.15.1.1" style="font-size:144%;background-color:#E3EFFB;">R13</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.19.15.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.19.15.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.19.15.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.19.15.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.19.15.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.19.15.4.1" style="font-size:144%;background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.19.15.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.19.15.5.1" style="font-size:144%;background-color:#E3EFFB;">15+6.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.19.15.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.19.15.6.1" style="font-size:144%;background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.19.15.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.19.15.7.1" style="font-size:144%;background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.19.15.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.19.15.8.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T10.4.4.19.15.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.19.15.9.1" style="font-size:144%;background-color:#E3EFFB;">641</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.19.15.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.19.15.10.1" style="font-size:144%;background-color:#E3EFFB;">76.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.19.15.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.19.15.11.1" style="font-size:144%;background-color:#E3EFFB;">52.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.19.15.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.19.15.12.1" style="font-size:144%;background-color:#E3EFFB;">73.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.19.15.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.19.15.13.1" style="font-size:144%;background-color:#E3EFFB;">78.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T10.4.4.19.15.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.19.15.14.1" style="font-size:144%;background-color:#E3EFFB;">44.2</span></td>
</tr>
<tr class="ltx_tr" id="S3.T10.4.4.20.16" style="background-color:#E3EFFB;">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T10.4.4.20.16.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.20.16.1.1" style="font-size:144%;background-color:#E3EFFB;">R14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T10.4.4.20.16.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.20.16.2.1" style="font-size:144%;background-color:#E3EFFB;">FastVLM (Ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T10.4.4.20.16.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.20.16.3.1" style="font-size:144%;background-color:#E3EFFB;">FastViTHD</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T10.4.4.20.16.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.20.16.4.1" style="font-size:144%;background-color:#E3EFFB;">Qw.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T10.4.4.20.16.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.20.16.5.1" style="font-size:144%;background-color:#E3EFFB;">15+11.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T10.4.4.20.16.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.20.16.6.1" style="font-size:144%;background-color:#E3EFFB;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T10.4.4.20.16.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.20.16.7.1" style="font-size:144%;background-color:#E3EFFB;">256</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T10.4.4.20.16.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.20.16.8.1" style="font-size:144%;background-color:#E3EFFB;">125</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T10.4.4.20.16.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T10.4.4.20.16.9.1" style="font-size:144%;background-color:#E3EFFB;">641</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T10.4.4.20.16.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.20.16.10.1" style="font-size:144%;background-color:#E3EFFB;">77.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T10.4.4.20.16.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.20.16.11.1" style="font-size:144%;background-color:#E3EFFB;">63.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T10.4.4.20.16.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.20.16.12.1" style="font-size:144%;background-color:#E3EFFB;">74.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T10.4.4.20.16.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.20.16.13.1" style="font-size:144%;background-color:#E3EFFB;">78.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T10.4.4.20.16.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T10.4.4.20.16.14.1" style="font-size:144%;background-color:#E3EFFB;">49.7</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T10.25.1.1" style="font-size:63%;">Table 10</span>: </span><span class="ltx_text" id="S3.T10.26.2" style="font-size:63%;">
<span class="ltx_text ltx_font_bold" id="S3.T10.26.2.1">Comparison with recent methods on text-rich benchmarks.</span> The models are grouped based on total number of visual tokens. “-” indicates that performance was not reported in the respective paper. For the dataset column, “-” indicates that the dataset size for pretraining (“PT”) or instruction tuning (“IT”) is not explicitly mentioned in the respective paper. For methods that have more than 2 stages of training, we report the total samples used for all the pretraining stages as part of “PT”. “TTFT” means time to first token (the sum of the vision encoder latency and the LLM prefilling time), we report latency only for models that are publicly available and in a format favorable to MLX <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> “Vic.” refers to Vicuna <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite>, “Qw.2” refers to Qwen2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>. “L-3” refers to LLaMA-3. * - For input resolution and visual tokens, we report the highest supported resolution by the respective models as some models like MM1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> use dynamic input resolution. †- performance numbers reported from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite>. For VLMs that use multiple vision encoders, the size of each encoder is listed independently, for TTFT, the latency from each encoder is summed up.
</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S4a">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">D </span>Datasets</h2>
<section class="ltx_subsection" id="S4.SS1a">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Pretraining Datasets</h3>
<div class="ltx_para" id="S4.SS1a.p1">
<p class="ltx_p" id="S4.SS1a.p1.1"><span class="ltx_text" id="S4.SS1a.p1.1.1" style="font-size:144%;">For Stage-1 training, we only use LLaVA-1.5 558K </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1a.p1.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a><span class="ltx_text" id="S4.SS1a.p1.1.3.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS1a.p1.1.4" style="font-size:144%;"> dataset. For Stage-1.5 training, we use densely captioned versions of CC3M </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1a.p1.1.5.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">65</span></a><span class="ltx_text" id="S4.SS1a.p1.1.6.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS1a.p1.1.7" style="font-size:144%;"> and CC12M </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1a.p1.1.8.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a><span class="ltx_text" id="S4.SS1a.p1.1.9.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS1a.p1.1.10" style="font-size:144%;"> introduced in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1a.p1.1.11.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a><span class="ltx_text" id="S4.SS1a.p1.1.12.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS1a.p1.1.13" style="font-size:144%;">. The total size of this dataset is 15 million image-text pairs.
We generated 300 generic questions, such as “What is in this photo?”. For each (image, dense-caption) pair, we randomly selected a generic question to form a triplet of (question, image, dense-caption). With a 0.5 probability, we placed the image’s special token </span><span class="ltx_text ltx_font_typewriter" id="S4.SS1a.p1.1.14" style="font-size:144%;">&lt;image&gt;</span><span class="ltx_text" id="S4.SS1a.p1.1.15" style="font-size:144%;"> either before or after the question.
From recent works like </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1a.p1.1.16.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a><span class="ltx_text" id="S4.SS1a.p1.1.17.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS1a.p1.1.18" style="font-size:144%;"> and our results in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T6" style="font-size:144%;" title="In 3.2.3 Comparison with Token Pruning &amp; Downsampling ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text" id="S4.SS1a.p1.1.19" style="font-size:144%;">, scaling dataset in Scale-1.5 is beneficial to improve the performance of VLM across a wide range of evaluations. Even though FastViTHD is smaller than ViT-L/14 and ViT-H used in  </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1a.p1.1.20.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a><span class="ltx_text" id="S4.SS1a.p1.1.21.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS1a.p1.1.22" style="font-size:144%;"> respectively, we see similar scaling trends.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Visual Instruction Tuning Datasets</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text" id="S4.SS2.p1.1.1" style="font-size:144%;">We use 3 different version of instruction tuning datasets. The smallest scale is LLaVA-1.5 665K dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a><span class="ltx_text" id="S4.SS2.p1.1.3.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.4" style="font-size:144%;">. We further scale up this dataset by including training splits of the following datasets; AI2D </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.5.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a><span class="ltx_text" id="S4.SS2.p1.1.6.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.7" style="font-size:144%;">, ScienceQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.8.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">55</span></a><span class="ltx_text" id="S4.SS2.p1.1.9.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.10" style="font-size:144%;">, ChartQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.11.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a><span class="ltx_text" id="S4.SS2.p1.1.12.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.13" style="font-size:144%;">, COCO </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.14.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a><span class="ltx_text" id="S4.SS2.p1.1.15.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.16" style="font-size:144%;">, DocVQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.17.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a><span class="ltx_text" id="S4.SS2.p1.1.18.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.19" style="font-size:144%;">, DVQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.20.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a><span class="ltx_text" id="S4.SS2.p1.1.21.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.22" style="font-size:144%;">, GeoQA+ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.23.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a><span class="ltx_text" id="S4.SS2.p1.1.24.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.25" style="font-size:144%;">, OCRVQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.26.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a><span class="ltx_text" id="S4.SS2.p1.1.27.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.28" style="font-size:144%;">, SegmentAnything </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.29.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a><span class="ltx_text" id="S4.SS2.p1.1.30.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.31" style="font-size:144%;">, SynthDoG-EN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.32.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a><span class="ltx_text" id="S4.SS2.p1.1.33.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.34" style="font-size:144%;">, TextVQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.35.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a><span class="ltx_text" id="S4.SS2.p1.1.36.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.37" style="font-size:144%;"> and Visual Genome </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.38.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a><span class="ltx_text" id="S4.SS2.p1.1.39.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.40" style="font-size:144%;">. The conversational data for the listed datasets is sourced from </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.41.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a><span class="ltx_text" id="S4.SS2.p1.1.42.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.43" style="font-size:144%;">. The total number of samples in this dataset is 1.1 million and is referred to as “1.1M” in all the tables. We further scale-up instruction tuning dataset using image-based conversational data from Cambrian-7M </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.44.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a><span class="ltx_text" id="S4.SS2.p1.1.45.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.46" style="font-size:144%;">, which amounts to 5.4 million samples. Filtered Cambrian-7M </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.47.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a><span class="ltx_text" id="S4.SS2.p1.1.48.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.49" style="font-size:144%;"> is merged with “1.1M ” dataset to obtain “6.5M” instruction tuning dataset. We then append all available single-image instruction tuning data open-sourced by LLaVA-OneVision </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.50.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a><span class="ltx_text" id="S4.SS2.p1.1.51.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.52" style="font-size:144%;"> to “6.5M” to obtain “11.9M” instruction tuning dataset. From </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T6" style="font-size:144%;" title="In 3.2.3 Comparison with Token Pruning &amp; Downsampling ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text" id="S4.SS2.p1.1.53" style="font-size:144%;">, we see further improvements in VLM benchmarks when instruction tuning dataset is scaled, following trends exhibited by image encoders much bigger than FastViTHD.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">D.3 </span>Evaluations</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text" id="S4.SS3.p1.1.1" style="font-size:144%;">In addition to evaluations listed in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4" style="font-size:144%;" title="4 Experiments ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S4.SS3.p1.1.2" style="font-size:144%;">, we report performance of FastVLM on ChartQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS3.p1.1.3.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a><span class="ltx_text" id="S4.SS3.p1.1.4.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS3.p1.1.5" style="font-size:144%;">, OCRBench </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS3.p1.1.6.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a><span class="ltx_text" id="S4.SS3.p1.1.7.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS3.p1.1.8" style="font-size:144%;"> and InfoVQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS3.p1.1.9.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">58</span></a><span class="ltx_text" id="S4.SS3.p1.1.10.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S4.SS3.p1.1.11" style="font-size:144%;"> to compare FastVLM against recent methods on text-rich benchmarks. In </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4.T11" style="font-size:144%;" title="In D.3 Evaluations ‣ D Datasets ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">11</span></a><span class="ltx_text" id="S4.SS3.p1.1.12" style="font-size:144%;">, report performance of FastViT model (with architectural interventions) from multiple training runs and compute the standard deviation of metrics reported in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3.T6" style="font-size:144%;" title="In 3.2.3 Comparison with Token Pruning &amp; Downsampling ‣ 3.2 FastViTHD: High Resolution Encoder for VLM ‣ 3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text" id="S4.SS3.p1.1.13" style="font-size:144%;">. As described in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4" style="font-size:144%;" title="4 Experiments ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S4.SS3.p1.1.14" style="font-size:144%;">, for ablations we are interested in benchmarks that are quick to evaluate and exhibit lower variance to different initializations. From </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S4.T11" style="font-size:144%;" title="In D.3 Evaluations ‣ D Datasets ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">11</span></a><span class="ltx_text" id="S4.SS3.p1.1.15" style="font-size:144%;">, GQA, TextVQA, POPE, DocVQA and SeedBench fit the criteria. While VQAv2 also exhibits lower variance it is substantially larger and takes long time to evaluate. The standard deviation across the selected metrics is below 0.5, so we use the average of these metrics as a reliable indicator for our analysis in </span><a class="ltx_ref" href="https://arxiv.org/html/2412.13303v1#S3" style="font-size:144%;" title="3 Architecture ‣ FastVLM: Efficient Vision Encoding for Vision Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S4.SS3.p1.1.16" style="font-size:144%;">.</span></p>
</div>
<figure class="ltx_table" id="S4.T11">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T11.2" style="width:214.6pt;height:47.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-136.9pt,30.3pt) scale(0.439379090428355,0.439379090428355) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T11.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T11.2.2.3.1">
<td class="ltx_td ltx_border_tt" id="S4.T11.2.2.3.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T11.2.2.3.1.2" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.3.1.2.1" style="font-size:144%;">GQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T11.2.2.3.1.3" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.3.1.3.1" style="font-size:144%;">SQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T11.2.2.3.1.4" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.3.1.4.1" style="font-size:144%;">TextVQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T11.2.2.3.1.5" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.3.1.5.1" style="font-size:144%;">POPE</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T11.2.2.3.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.3.1.6.1" style="font-size:144%;">LLaVA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T11.2.2.3.1.7" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.3.1.7.1" style="font-size:144%;">MMVet</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T11.2.2.3.1.8" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.3.1.8.1" style="font-size:144%;">VQAv2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T11.2.2.3.1.9" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.3.1.9.1" style="font-size:144%;">DocVQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T11.2.2.3.1.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.3.1.10.1" style="font-size:144%;">Seed</span></th>
</tr>
<tr class="ltx_tr" id="S4.T11.2.2.2">
<td class="ltx_td" id="S4.T11.2.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T11.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T11.1.1.1.1.1" style="font-size:144%;">Bench</span><sup class="ltx_sup" id="S4.T11.1.1.1.1.2"><span class="ltx_text ltx_font_italic" id="S4.T11.1.1.1.1.2.1" style="font-size:144%;">W</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T11.2.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T11.2.2.2.2.1" style="font-size:144%;">Bench</span><sup class="ltx_sup" id="S4.T11.2.2.2.2.2"><span class="ltx_text ltx_font_italic" id="S4.T11.2.2.2.2.2.1" style="font-size:144%;">I</span></sup>
</th>
</tr>
<tr class="ltx_tr" id="S4.T11.2.2.4.2">
<td class="ltx_td ltx_border_t" id="S4.T11.2.2.4.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.2.2.4.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.4.2.2.1" style="font-size:144%;">62.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.2.2.4.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.4.2.3.1" style="font-size:144%;">64.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.2.2.4.2.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.4.2.4.1" style="font-size:144%;">60.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.2.2.4.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.4.2.5.1" style="font-size:144%;">85.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.2.2.4.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.4.2.6.1" style="font-size:144%;">59.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.2.2.4.2.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.4.2.7.1" style="font-size:144%;">29.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.2.2.4.2.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.4.2.8.1" style="font-size:144%;">77.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.2.2.4.2.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.4.2.9.1" style="font-size:144%;">27.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.2.2.4.2.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.4.2.10.1" style="font-size:144%;">53.31</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.2.2.5.3">
<td class="ltx_td" id="S4.T11.2.2.5.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.5.3.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.5.3.2.1" style="font-size:144%;">62.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.5.3.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.5.3.3.1" style="font-size:144%;">64.95</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.5.3.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.5.3.4.1" style="font-size:144%;">60.61</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.5.3.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.5.3.5.1" style="font-size:144%;">86.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.5.3.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.5.3.6.1" style="font-size:144%;">60.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.5.3.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.5.3.7.1" style="font-size:144%;">31.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.5.3.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.5.3.8.1" style="font-size:144%;">77.39</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.5.3.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.5.3.9.1" style="font-size:144%;">28.37</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.5.3.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.5.3.10.1" style="font-size:144%;">53.55</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.2.2.6.4">
<td class="ltx_td" id="S4.T11.2.2.6.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.6.4.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.6.4.2.1" style="font-size:144%;">62.69</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.6.4.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.6.4.3.1" style="font-size:144%;">65.64</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.6.4.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.6.4.4.1" style="font-size:144%;">60.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.6.4.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.6.4.5.1" style="font-size:144%;">85.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.6.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.6.4.6.1" style="font-size:144%;">61.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.6.4.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.6.4.7.1" style="font-size:144%;">31.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.6.4.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.6.4.8.1" style="font-size:144%;">77.31</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.6.4.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.6.4.9.1" style="font-size:144%;">28.26</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.2.2.6.4.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.6.4.10.1" style="font-size:144%;">53.46</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.2.2.7.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T11.2.2.7.5.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.7.5.1.1" style="font-size:144%;">Std.</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.2.2.7.5.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.7.5.2.1" style="font-size:144%;">0.0047</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.2.2.7.5.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.7.5.3.1" style="font-size:144%;">0.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.2.2.7.5.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.7.5.4.1" style="font-size:144%;">0.041</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.2.2.7.5.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.7.5.5.1" style="font-size:144%;">0.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.2.2.7.5.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.7.5.6.1" style="font-size:144%;">0.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.2.2.7.5.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.7.5.7.1" style="font-size:144%;">0.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.2.2.7.5.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.7.5.8.1" style="font-size:144%;">0.049</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.2.2.7.5.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.7.5.9.1" style="font-size:144%;">0.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.2.2.7.5.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T11.2.2.7.5.10.1" style="font-size:144%;">0.099</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T11.6.1.1" style="font-size:63%;">Table 11</span>: </span><span class="ltx_text" id="S4.T11.7.2" style="font-size:63%;">
VLM benchmarks across three independent runs with frozen FastViT image encoder. Training setup is LLaVA-1.5 with Vicuna 7B as LLM. Standard deviation across runs is listed in the bottom row.
</span></figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Dec 17 19:44:57 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
