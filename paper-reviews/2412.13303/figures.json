[{"figure_path": "https://arxiv.org/html/2412.13303/", "caption": "(a) Qwen2-0.5B", "description": "\uadf8\ub9bc 1(a)\ub294 Qwen2-0.5B LLM\uc744 \uc0ac\uc6a9\ud55c VLMs\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ube44\uc804 \uc778\ucf54\ub354\uc758 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. x\ucd95\uc740 \uccab \ud1a0\ud070 \uc0dd\uc131 \uc2dc\uac04(TTFT, milliseconds)\uc744 \ub098\ud0c0\ub0b4\uace0 y\ucd95\uc740 \ud3c9\uade0 5\uac1c VLM \ud3c9\uac00 \uc810\uc218\ub97c \ubc31\ubd84\uc728(%)\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ud574\uc0c1\ub3c4(256x256, 320x320, 512x512, 768x768, 1024x1024)\uc5d0\uc11c FastViTHD(\ubcf8 \ub17c\ubb38\uc758 \uc778\ucf54\ub354), ViT-L/14, SigLIP-SO400M, ConvNeXt-XXL, ConvNeXt-L \ub4f1\uc758 \uc778\ucf54\ub354 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec FastViTHD\uc758 \ud6a8\uc728\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \ub9c8\ucee4 \ud06c\uae30\ub294 \ud30c\ub77c\ubbf8\ud130 \uc218\uc5d0 \ube44\ub840\ud558\uba70,  \ubaa8\ub4e0 \ubaa8\ub378\uc740 LLaVA-1.5 \uc124\uc815\uc744 \uc0ac\uc6a9\ud558\uc5ec CLIP \uc0ac\uc804 \ud6c8\ub828\ub41c \ube44\uc804 \uc778\ucf54\ub354\ub97c \ud559\uc2b5 \uac00\ub2a5\ud558\ub3c4\ub85d \uc124\uc815\ud558\uace0 \ud6c8\ub828\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  M1 Macbook Pro\uc5d0\uc11c \ubca4\uce58\ub9c8\ud0b9\uc744 \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.13303/", "caption": "(b) Vicuna-7B", "description": "\uadf8\ub9bc (b)\ub294 Vicuna-7B\ub97c \uae30\ubc18\uc73c\ub85c \ud55c \ub2e4\uc591\ud55c \ube44\uc804 \uc778\ucf54\ub354\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. x\ucd95\uc740 \uccab \ud1a0\ud070 \uc0dd\uc131\uae4c\uc9c0 \uac78\ub9ac\ub294 \uc2dc\uac04(ms)\uc744 \ub098\ud0c0\ub0b4\uace0, y\ucd95\uc740 \ud3c9\uade0 5\uac1c\uc758 VLM \ud3c9\uac00 \uc810\uc218\ub97c \ubc31\ubd84\uc728(%)\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  FastViTHD(\uc800\uc790 \uc81c\uc548 \ubaa8\ub378), ViT-L/14, SigLIP-SO400M, ConvNeXt-XXL, ConvNeXt-L \ub4f1 \ub2e4\uc591\ud55c \ube44\uc804 \uc778\ucf54\ub354\uc758 \uccab \ud1a0\ud070 \uc0dd\uc131 \uc2dc\uac04\uacfc \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec FastViTHD\uc758 \ud6a8\uc728\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ub9c8\ucee4 \ud06c\uae30\ub294 \ube44\uc804 \uc778\ucf54\ub354\uc758 \ud30c\ub77c\ubbf8\ud130 \uc218\uc5d0 \ube44\ub840\ud569\ub2c8\ub2e4. \uc774 \uadf8\ub798\ud504\ub97c \ud1b5\ud574 \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\uc5d0\uc11c\ub3c4 FastViTHD\uac00 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc5d0 \ube44\ud574 \ud6e8\uc52c \ube60\ub978 \uc18d\ub3c4\ub85c \uccab \ud1a0\ud070\uc744 \uc0dd\uc131\ud558\uba74\uc11c \ub3d9\uc2dc\uc5d0 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud568\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. Architecture"}, {"figure_path": "https://arxiv.org/html/2412.13303/", "caption": "Figure 1: FastVLM is more than 3\u00d7\\times\u00d7 faster than prior work. Comparison of commonly used vision encoders for VLMs with (a) Qwen2\u00a0[79] 0.5B LLM and (b) Vicuna 7B\u00a0[90] LLM. All the vision encoders are CLIP\u00a0[63] pretrained. For a fair comparison all models are trained using LLaVA-1.5\u00a0[49] setup with the vision encoders made trainable for resolution adaptation, see Sec.\u00a04 for more details. Marker size for each model corresponds to number of parameters of the vision encoder. The x\ud835\udc65xitalic_x-axis is the sum of vision encoder latency and LLM prefilling time. All models are benchmarked on an M1 Macbook Pro.", "description": "\uadf8\ub9bc 1\uc740 FastVLM\uc758 \uc18d\ub3c4\uac00 \uae30\uc874 \uc5f0\uad6c\ubcf4\ub2e4 3\ubc30 \uc774\uc0c1 \ube60\ub974\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  (a) Qwen2 [79] 0.5B LLM\uacfc (b) Vicuna 7B [90] LLM\uc744 \uc0ac\uc6a9\ud55c VLM\uc5d0 \ub300\ud574 \uc77c\ubc18\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \ube44\uc804 \uc778\ucf54\ub354\ub4e4\uc744 \ube44\uad50\ud55c \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. \ubaa8\ub4e0 \ube44\uc804 \uc778\ucf54\ub354\ub294 CLIP [63]\uc744 \uc0ac\uc804 \ud6c8\ub828\ud588\uc2b5\ub2c8\ub2e4. \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574 \ubaa8\ub4e0 \ubaa8\ub378\uc740 \ube44\uc804 \uc778\ucf54\ub354\ub97c \ud574\uc0c1\ub3c4 \uc801\uc751\uc744 \uc704\ud574 \ud6c8\ub828 \uac00\ub2a5\ud558\uac8c \ub9cc\ub4e4\uace0 LLaVA-1.5 [49] \uc124\uc815\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub418\uc5c8\uc2b5\ub2c8\ub2e4 (\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 4\uc7a5 \ucc38\uc870). \ub9c8\ucee4 \ud06c\uae30\ub294 \ube44\uc804 \uc778\ucf54\ub354\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. x\ucd95\uc740 \ube44\uc804 \uc778\ucf54\ub354 \uc9c0\uc5f0 \uc2dc\uac04\uacfc LLM \ucc44\uc6b0\uae30 \uc2dc\uac04\uc758 \ud569\uacc4\uc785\ub2c8\ub2e4. \ubaa8\ub4e0 \ubaa8\ub378\uc740 M1 Macbook Pro\uc5d0\uc11c \ubca4\uce58\ub9c8\ud0b9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "3. Architecture"}, {"figure_path": "https://arxiv.org/html/2412.13303/", "caption": "Figure 2: Overview of the FastVLM architecture. FastVLM consists of our novel vision encoder, FastViTHD, trained using the same setup as LLaVa. The FastViTHD architecture is designed and trained for low latency at high resolution,\nutilizing novel multi-scale pooling, additional self-attention layers, and downsampling to generate 4\u00d7\\times\u00d7 fewer tokens than FastViT, and 16\u00d7\\times\u00d7 fewer tokens than ViT-L/14 at resolution 336.", "description": "\uadf8\ub9bc 2\ub294 FastVLM \uc544\ud0a4\ud14d\ucc98\uc758 \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. FastVLM\uc740 LLaVA\uc640 \ub3d9\uc77c\ud55c \uc124\uc815\uc73c\ub85c \ud559\uc2b5\ub41c \uc0c8\ub85c\uc6b4 \ube44\uc804 \uc778\ucf54\ub354\uc778 FastViTHD\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. FastViTHD \uc544\ud0a4\ud14d\ucc98\ub294 \ub192\uc740 \ud574\uc0c1\ub3c4\uc5d0\uc11c \ub0ae\uc740 \ub300\uae30 \uc2dc\uac04\uc744 \uc704\ud574 \uc124\uacc4 \ubc0f \ud559\uc2b5\ub418\uc5c8\uc73c\uba70, \uc0c8\ub85c\uc6b4 \ub2e4\uc911 \uc2a4\ucf00\uc77c \ud480\ub9c1, \ucd94\uac00\uc801\uc778 \uc790\uae30 \uc8fc\uc758 \uacc4\uce35 \ubc0f \ub2e4\uc6b4\uc0d8\ud50c\ub9c1\uc744 \uc0ac\uc6a9\ud558\uc5ec FastViT\ubcf4\ub2e4 4\ubc30, \ud574\uc0c1\ub3c4 336\uc5d0\uc11c\ub294 ViT-L/14\ubcf4\ub2e4 16\ubc30 \uc801\uc740 \ud1a0\ud070\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 FastVLM\uc758 \uc8fc\uc694 \uad6c\uc131 \uc694\uc18c\uc778 FastViTHD \ube44\uc804 \uc778\ucf54\ub354\uc640 \uadf8 \uc791\ub3d9 \ubc29\uc2dd\uc744 \uc790\uc138\ud788 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc911 \uc2a4\ucf00\uc77c \ud480\ub9c1\uc740 \ub2e4\uc591\ud55c \ud574\uc0c1\ub3c4\uc758 \ud2b9\uc9d5\uc744 \ud65c\uc6a9\ud558\uc5ec \uc815\ud655\ub3c4\ub97c \ub192\uc774\uace0, \ucd94\uac00 \uc790\uae30 \uc8fc\uc758 \uacc4\uce35\uc740 \ub354\uc6b1 \uc815\uad50\ud55c \ud2b9\uc9d5 \ud45c\ud604\uc744 \uac00\ub2a5\ud558\uac8c \ud558\uba70, \ub2e4\uc6b4\uc0d8\ud50c\ub9c1\uc740 \uacc4\uc0b0 \ube44\uc6a9\uc744 \uc904\uc774\uba74\uc11c\ub3c4 \uc131\ub2a5 \uc800\ud558\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ub370 \uae30\uc5ec\ud569\ub2c8\ub2e4.  \uacb0\uacfc\uc801\uc73c\ub85c FastViT\ub098 ViT-L/14\ubcf4\ub2e4 \ud6e8\uc52c \ub354 \uc801\uc740 \ud1a0\ud070 \uc218\ub85c \ub3d9\uc77c\ud558\uac70\ub098 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.", "section": "3. Architecture"}, {"figure_path": "https://arxiv.org/html/2412.13303/", "caption": "Figure 3: \nNovel scaling strategy of FastViTHD lowers latency\nat various image resolutions.\nFastViT-Naive, a naive scaling of the FastViT architecture, and our proposed FastViTHD have the same number of parameters. ConvNeXt-L is provided for reference. All models are benchmarked on M1 Macbook Pro and trained with LLaVA-1.5 setup and Vicuna 7B. Note that the y\ud835\udc66yitalic_y-axis is in log scale.", "description": "\uadf8\ub9bc 3\uc740 FastViTHD\uc758 \uc0c8\ub85c\uc6b4 \uc2a4\ucf00\uc77c\ub9c1 \uc804\ub7b5\uc774 \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4\uc5d0\uc11c \uc9c0\uc5f0 \uc2dc\uac04\uc744 \uc904\uc774\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. FastViT \uc544\ud0a4\ud14d\ucc98\ub97c \ub2e8\uc21c\ud788 \ud655\uc7a5\ud55c FastViT-Naive\uc640 \uc81c\uc548\ub41c FastViTHD\ub294 \ub3d9\uc77c\ud55c \ub9e4\uac1c\ubcc0\uc218 \uc218\ub97c \uac00\uc9c0\uba70, \ube44\uad50\ub97c \uc704\ud574 ConvNeXt-L \ubaa8\ub378\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ubaa8\ub4e0 \ubaa8\ub378\uc740 M1 Macbook Pro\uc5d0\uc11c \ubca4\uce58\ub9c8\ud0b9\ub418\uc5c8\uace0, LLaVA-1.5 \uc124\uc815\uacfc Vicuna 7B\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub418\uc5c8\uc2b5\ub2c8\ub2e4. Y\ucd95\uc740 \ub85c\uadf8 \uc2a4\ucf00\uc77c\uc784\uc744 \uc720\uc758\ud574\uc57c \ud569\ub2c8\ub2e4.", "section": "3.2 FastViTHD: \uace0\ud574\uc0c1\ub3c4 VLM\uc6a9 \uc778\ucf54\ub354"}, {"figure_path": "https://arxiv.org/html/2412.13303/", "caption": "Figure 4: \nFastViTHD improves the Pareto-Optimal curve for accuracy versus time to first token compared with FastViT.\nComparison of FastViT and FastViTHD backbones paired with Qwen2\u00a0[79] family (chat variant) LLMs of varying sizes and different image resolutions (annotated for each point). The Pareto-optimal curve is highlighted for the two vision backbones. Training setup is LLaVA-1.5. Note that the x\ud835\udc65xitalic_x-axis is in log scale.", "description": "\uadf8\ub9bc 4\ub294 FastViT\uacfc FastViTHD \ube44\uc804 \ubc31\ubcf8\uc744 \uc11c\ub85c \ub2e4\ub978 \ud06c\uae30\uc640 \ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0\uc640 \ud568\uaed8 Qwen2(\ucc57\ubd07 \ubcc0\ud615) LLM\uacfc \uc9dd\uc9c0\uc5b4 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4.  x\ucd95\uc740 \ub85c\uadf8 \uc2a4\ucf00\uc77c\ub85c \uc2dc\uac04(ms)\uc744 \ub098\ud0c0\ub0b4\uace0 y\ucd95\uc740 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  FastViTHD\ub294 \uc8fc\uc5b4\uc9c4 \uc2dc\uac04 \ub0b4\uc5d0 \ub354 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud558\uc5ec FastViT\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc810\uc740 \uc11c\ub85c \ub2e4\ub978 LLM \ud06c\uae30\uc640 \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4\ub97c \ub098\ud0c0\ub0b4\uba70, \ud30c\ub808\ud1a0 \ucd5c\uc801 \uace1\uc120\uc740 \ub450 \ube44\uc804 \ubc31\ubcf8\uc758 \ucd5c\uc0c1\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LLaVA-1.5 \uc124\uc815\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ud588\uc2b5\ub2c8\ub2e4.", "section": "3.2 FastViTHD: High Resolution Encoder for VLM"}, {"figure_path": "https://arxiv.org/html/2412.13303/", "caption": "Figure 5: \nVision latency dominates at high resolution.\nBreakdown of FastVLM\u2019s time to first token for varying image resolutions. Vision encoder is FastViTHD and LLM is Qwen2-1.5B.", "description": "\uadf8\ub9bc 5\ub294 \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\uc5d0\uc11c \ube44\uc804 \uc778\ucf54\ub354\uc758 \ucc98\ub9ac \uc2dc\uac04\uc774 \uc804\uccb4 \ucc98\ub9ac \uc2dc\uac04\uc744 \uc9c0\ubc30\uc801\uc73c\ub85c \ucc28\uc9c0\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. FastVLM\uc758 \uccab \ud1a0\ud070 \uc0dd\uc131 \uc2dc\uac04\uc744 \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4\ubcc4\ub85c \ubd84\uc11d\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. \ube44\uc804 \uc778\ucf54\ub354\ub294 FastViTHD\ub97c \uc0ac\uc6a9\ud588\uace0, \uc5b8\uc5b4 \ubaa8\ub378\uc740 Qwen2-1.5B\uc785\ub2c8\ub2e4.  \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4\uac00 \ub192\uc544\uc9d0\uc5d0 \ub530\ub77c \ube44\uc804 \uc778\ucf54\ub354\uc758 \ucc98\ub9ac \uc2dc\uac04\uc774 \uc99d\uac00\ud558\uace0, \uc804\uccb4 \ucc98\ub9ac \uc2dc\uac04\uc758 \ub300\ubd80\ubd84\uc744 \ucc28\uc9c0\ud558\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\ub97c \ucc98\ub9ac\ud558\ub294 \ub370 \ube44\uc804 \uc778\ucf54\ub354\uc758 \ud6a8\uc728\uc131\uc774 \uc911\uc694\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3.2.1 Vision Encoder - Language Decoder Interplay"}]