[{"content": "| Image | Input | #Visual | Latency | GQA | TextVQA | POPE | DocVQA | Seed | Avg-5 |\n|---|---|---|---|---|---|---|---|---|---| \n| ViT-L/14 | 336 | 576 | 127.4 | 62.0 | 58.2 | 85.9 | 28.1 | 66.1 | 60.1 |\n| ViT-L/14\u2020 | 336 | 576 | 127.4 | 63.5 | 59.2 | 86.3 | 28.7 | 68.6 | 61.2 |\n| FastViT | 256 | 64 | 3.0 | 60.2 | 51.6 | 82.9 | 15.8 | 61.5 | 54.4 |\n| FastViT | 768 | 576 | 34.5 | 62.7 | 62.3 | 86.5 | 34.4 | 67.1 | 62.6 |", "caption": "Table 1: \nFastViT has higher accuracy than ViT-L/14 at near 4\u00d7\\times\u00d7 lower latency.\nTo scale resolution up to 768, FastViT is made trainable during Stage-2 training of LLaVA-1.5 setup. \u2020To have a fair comparison, we also report the performance of ViT-L/14 finetuned during Stage-2 training of LLaVA-1.5. All latencies are reported in milliseconds. See Sec.\u00a04 for details.", "description": "\ud45c 1\uc740 FastViT\uc640 ViT-L/14\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec FastViT\uac00 ViT-L/14\ubcf4\ub2e4 4\ubc30 \uac00\uae4c\uc774 \ub0ae\uc740 \uc9c0\uc5f0 \uc2dc\uac04\uc73c\ub85c \ub354 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 768 \ud574\uc0c1\ub3c4\ub85c\uc758 \uc2a4\ucf00\uc77c\ub9c1\uc744 \uc704\ud574 FastViT\ub294 LLaVA-1.5 \uc124\uc815\uc758 2\ub2e8\uacc4 \ud6c8\ub828 \uc911\uc5d0 \ud6c8\ub828 \uac00\ub2a5\ud558\ub3c4\ub85d \uc124\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574 2\ub2e8\uacc4 LLaVA-1.5 \ud6c8\ub828 \uc911\uc5d0 \ubbf8\uc138 \uc870\uc815\ub41c ViT-L/14\uc758 \uc131\ub2a5\ub3c4 \ud568\uaed8 \ubcf4\uace0\ud569\ub2c8\ub2e4. \ubaa8\ub4e0 \uc9c0\uc5f0 \uc2dc\uac04\uc740 \ubc00\ub9ac\ucd08 \ub2e8\uc704\ub85c \ud45c\uc2dc\ub418\uba70 \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 4\uc808\uc744 \ucc38\uc870\ud558\uc2ed\uc2dc\uc624.", "section": "3. Architecture"}, {"content": "| Image | Multi | Pool | GQA | TextVQA | POPE | DocVQA | Seed | Avg-5 |\n|---|---|---|---|---|---|---|---|---|\n| FastViT |  | - | 62.7 | 62.3 | 86.5 | 34.4 | 67.1 | 62.6 |\n| FastViT | \u2713 | AvgPool | 63.0 | 62.2 | 86.2 | 35.1 | 66.9 | 62.7 |\n| FastViT | \u2713 | DWConv | 63.0 | 62.5 | 86.8 | 34.7 | 67.4 | 62.9 |", "caption": "Table 2: \nPushing FastViT VLM performance using multi-scale features and pooling strategies.\nThese modifications slightly improve FastViT. Training setup is LLaVA-1.5 with Vicuna 7B.", "description": "\ud45c 2\ub294 \ub2e4\uc911 \uc2a4\ucf00\uc77c \ud2b9\uc9d5\uacfc \ud480\ub9c1 \uc804\ub7b5\uc744 \uc0ac\uc6a9\ud558\uc5ec FastViT VLM\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc2e4\ud5d8 \uacb0\uacfc, \ub2e4\uc911 \uc2a4\ucf00\uc77c \ud2b9\uc9d5\uc744 \ud65c\uc6a9\ud558\uace0 \uae4a\uc774 \ubc29\ud5a5 \ud569\uc131\uacf1(depthwise convolution) \ud480\ub9c1\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ud3c9\uade0\uc801\uc73c\ub85c \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ub3c4\uc6c0\uc774 \ub428\uc744 \ud655\uc778\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc218\uc815\uc740 FastViT\uc758 \uc131\ub2a5\uc744 \uc57d\uac04 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ud6a8\uacfc\ub97c \ubcf4\uc600\uc2b5\ub2c8\ub2e4.  \ud6c8\ub828 \uc124\uc815\uc740 LLaVA-1.5\uc640 Vicuna 7B\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.", "section": "3. Architecture"}, {"content": "| Image | Encoder | Input | Latency | Zero-Shot | Avg Perf. | Avg Perf. |\n|---|---|---|---|---|---|---|\n| Encoder | Size(M)\u2193 | Res. | Enc.(ms)\u2193 | ImageNet | Retrieval | on 38 tasks |\n| ViT-L/14 [24] | 304 | 224 | 47.2 | 79.2 | 60.8 | 66.3 |\n| ViTamin-L [11] | 333 | 224 | 38.1 | 80.8 | 60.3 | 66.7 |\n| ConvNeXt-L | 200 | 320 | 34.4 | 76.8 | 64.8 | 63.9 |\n| **FastViTHD** | 125 | 224 | 6.8 | 78.3 | 67.7 | 66.3 |", "caption": "Table 3: \nFastViTHD achieves competitive results on CLIP benchmarks at significantly lower latency. We follow the same setup described in\u00a0[11] to report average retrieval performance and setup described in\u00a0[24] to report average performance on 38 tasks. All models are benchmarked on M1 Macbook Pro.", "description": "\ud45c 3\uc740 FastViTHD \ubaa8\ub378\uc774 CLIP \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc0c1\ub2f9\ud788 \ub0ae\uc740 \uc9c0\uc5f0 \uc2dc\uac04\uc73c\ub85c \uacbd\uc7c1\ub825 \uc788\ub294 \uacb0\uacfc\ub97c \ub2ec\uc131\ud588\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 ImageNet \uac80\uc0c9 \ubc0f 38\uac00\uc9c0 \uacfc\uc81c\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec FastViTHD\uc758 \ud6a8\uc728\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.  \uc2e4\ud5d8 \uc124\uc815\uc740 \ub17c\ubb38 [11]\uacfc [24]\uc5d0 \uc81c\uc2dc\ub41c \uc124\uc815\uc744 \ub530\ub985\ub2c8\ub2e4. \ubaa8\ub4e0 \ubaa8\ub378\uc740 M1 Macbook Pro\uc5d0\uc11c \ubca4\uce58\ub9c8\ud0b9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "3.2 FastViTHD: \uace0\ud574\uc0c1\ub3c4 VLM\uc6a9 \uc778\ucf54\ub354"}, {"content": "| Image | Input | Latency | #Visual | GQA | TextVQA | POPE | DocVQA | Seed | Avg-5 |\n|---|---|---|---|---|---|---|---|---|---| \n| **FastViTHD** | 256 | 10.1 | 16 | 60.6 | 53.1 | 82.3 | 17.4 | 63.7 | 55.5 |\n| C.N-L | 320 | 34.4 | 100 | 61.9 | 55.5 | 85.3 | 21.3 | 64.6 | 57.7 |\n| C.N-XXL | 256 | 89.9 | 64 | 62.7 | 56.3 | 85.3 | 21.6 | 65.6 | 58.3 |\n| **FastViTHD** | 512 | 33.5 | 64 | 63.0 | 59.3 | 86.4 | 25.7 | 67.1 | **60.4** |\n| **FastViTHD** | 768 | 122.6 | 144 | 62.4 | 62.9 | 87.7 | 32.9 | 68.2 | 62.8 |\n| C.N-L | 512 | 71.9 | 256 | 61.8 | 61.0 | 86.3 | 30.8 | 66.8 | 61.3 |\n| C.N-XXL | 512 | 397.1 | 256 | 62.3 | 65.1 | 87.7 | 36.2 | 68.4 | **63.9** |\n| **FastViTHD** | 1024 | 235.6 | 256 | 63.1 | 64.4 | 88.1 | 35.6 | 68.5 | **63.9** |", "caption": "Table 4: \nFastViTHD achieves higher accuracy than ConvNeXT while having lower latency at a higher resolution.\nThe models are grouped based on the total number of visual tokens produced for the LLM to process. \u201cC.N\u201d stands for ConvNeXT. Training setup is LLaVA-1.5 with Vicuna 7B.", "description": "\ud45c 4\ub294 \uace0\ud574\uc0c1\ub3c4 \uc774\ubbf8\uc9c0\uc5d0\uc11c FastViTHD\uac00 ConvNeXT\ubcf4\ub2e4 \ub354 \ub192\uc740 \uc815\ud655\ub3c4\uc640 \ub0ae\uc740 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \ub2ec\uc131\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubaa8\ub378\uc740 LLM\uc774 \ucc98\ub9ac\ud574\uc57c \ud558\ub294 \ucd1d \uc2dc\uac01 \ud1a0\ud070 \uc218\ub97c \uae30\uc900\uc73c\ub85c \uadf8\ub8f9\ud654\ub429\ub2c8\ub2e4.  'C.N'\uc740 ConvNeXT\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud6c8\ub828 \uc124\uc815\uc740 Vicuna 7B\ub97c \uc0ac\uc6a9\ud55c LLaVA-1.5\uc785\ub2c8\ub2e4.", "section": "3.2 FastViTHD: High Resolution Encoder for VLM"}, {"content": "| Model | Input | #Visual | GQA | SQA | Text- | POPE | VQA | Seed |\n|---|---|---|---|---|---|---|---|---|\n| ViT-L/14 M<sup>3</sup> [7] | 336 | 9 | 58.0 | - | - | **83.4** | - | 55.4 |\n| ViT-L/14 MQT [28] | 336 | 16 | 57.6 | 67.5 | - | 80.8 | 71.1 | - |\n| **FastViTHD** | 256 | 16 | **60.6** | **69.2** | 53.1 | 82.3 | **74.7** | **58.8** |\n| ViT-L/14 PruMerge [64] | 336 | 40 | - | 68.5 | 56.0 | 76.3 | 72.0 | - |\n| ViT-L/14 PruMerge+ [64] | 336 | 40 | - | 68.3 | 57.1 | 84.0 | 76.8 | - |\n| ViT-L/14 M<sup>3</sup> [7] | 336 | 36 | 60.3 | - | - | 85.5 | - | 58.0 |\n| FastV [13] | 336 | 64 | 46.1 | 51.1 | 47.8 | 48.0 | 55.0 | 51.9 |\n| SparseVLM [89] | 336 | 64 | 52.7 | 62.2 | 51.8 | 75.1 | 68.2 | 51.1 |\n| VisionZip [80] | 336 | 64 | 55.1 | 69.0 | 55.5 | 77.0 | 62.9 | 52.2 |\n| VisionZip\u2021 [80] | 336 | 64 | 57.0 | 68.8 | 56.0 | 80.9 | 74.2 | 53.4 |\n| DynamicLLaVA<sub>I</sub> [29] | 336 | 115 | 61.4 | **69.1** | 57.0 | 85.0 | **78.0** | - |\n| DynamicLLaVA<sub>I|T</sub> [29] | 336 | 115 | 61.3 | 68.6 | 56.5 | 85.9 | 77.9 | - |\n| **FastViTHD** | 512 | 64 | **63.0** | 68.9 | **59.3** | **86.4** | **78.0** | **61.8** |\n| ViT-L/14 M<sup>3</sup> [7] | 336 | 144 | 61.3 | - | - | 87.0 | - | 59.7 |\n| ViT-L/14 MQT [28] | 336 | 144 | 61.4 | 67.6 | - | 83.9 | 76.4 | - |\n| FastV [13] | 336 | 192 | 52.7 | 67.3 | 52.5 | 64.8 | 67.1 | 57.1 |\n| SparseVLM [89] | 336 | 192 | 57.6 | **69.1** | 56.1 | 83.6 | 75.6 | 55.8 |\n| VisionZip [80] | 336 | 192 | 59.3 | 68.9 | 57.3 | 85.3 | 76.8 | 56.4 |\n| VisionZip\u2021 [80] | 336 | 192 | 60.1 | 68.2 | 57.8 | 84.9 | 77.4 | 57.1 |\n| **FastViTHD** | 768 | 144 | **62.4** | 67.6 | **62.9** | **87.7** | **78.9** | **62.5** |\n| ViT-L/14 MQT [28] | 336 | 256 | 61.6 | **67.5** | - | 84.4 | 76.8 | - |\n| **FastViTHD** | 1024 | 256 | **63.1** | 67.4 | 64.4 | **88.1** | **79.2** | - |", "caption": "Table 5: \nFastViTHD more effectively reduces tokens compared with token pruning methods.\nThe models are grouped based on total number of visual tokens. \u201c-\u201d indicates that performance was not reported in the respective paper. All models presented in this table are trained using LLaVA-1.5 setup with Vicuna 7B. \u2021- indicates further finetuning as reported in\u00a0[80]. I - indicates vision only sparsification and I|T indicates vision-language sparsification, as reported in\u00a0[29].", "description": "\ud45c 5\ub294 FastViTHD\uac00 \uae30\uc874 \ud1a0\ud070 \uac04\uc18c\ud654 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \ud1a0\ud070 \uc218\ub97c \ub354 \ud6a8\uacfc\uc801\uc73c\ub85c \uc904\uc778\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc740 LLM\uc774 \ucc98\ub9ac\ud574\uc57c \ud558\ub294 \ucd1d \uc2dc\uac01 \ud1a0\ud070 \uc218\ub97c \uae30\uc900\uc73c\ub85c \uadf8\ub8f9\ud654\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uc5ec\ub7ec \ube44\uad50 \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uc77c\ubd80 \ubaa8\ub378\uc758 \uc131\ub2a5\uc740 \ud574\ub2f9 \ub17c\ubb38\uc5d0\uc11c \ubcf4\uace0\ub418\uc9c0 \uc54a\uc544 '-'\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0 \uc81c\uc2dc\ub41c \ubaa8\ub4e0 \ubaa8\ub378\uc740 Vicuna 7B\ub97c \uc0ac\uc6a9\ud558\ub294 LLaVA-1.5 \uc124\uc815\uc73c\ub85c \ud559\uc2b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \u2021 \ud45c\uc2dc\ub294 [80]\uc5d0\uc11c \ucd94\uac00\uc801\uc778 \ubbf8\uc138 \uc870\uc815\uc744 \uc758\ubbf8\ud558\uba70, I\ub294 [29]\uc5d0\uc11c \ubcf4\uace0\ub41c \ub300\ub85c \uc2dc\uac01\uc801 \uac04\uc18c\ud654\ub9cc, I|T\ub294 \uc2dc\uac01-\uc5b8\uc5b4\uc801 \uac04\uc18c\ud654\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "3.2 FastViTHD: High Resolution Encoder for VLM"}, {"content": "Row|Method|Vision|LLM|Data (M)|Input|#Visual|Vis. Enc.|TTFT|GQA|SQA|Text|POPE|LLaVA|MM-|VQA|Doc|MMMU|Seed\n---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---\nAnn.|nanoLLaVA|ViT-SO400M|Qw.1.5|-|384|729|430|535|54.8|59.0|46.7|84.1|-|-|70.8|-|30.4|-\nR1|LLaVAOV [40]<sup>\u2217</sup>|ViT-SO400M|Qw.2|4.5+3.2|1152|7290|430|14124|-|67.2|-|-|-|29.1|-|**70.0**|31.4|65.5\nR2|**FastVLM (Ours)**|**FastViTHD**|**Qw.2**|**15+1.1**|**1024**|**256**|**125**|**166**|**61.6**|**61.4**|**57.4**|**87.4**|**56.0**|**31.8**|**77.0**|**61.0**|**30.9**|**65.6**\nR3|**FastVLM (Ours)**|**FastViTHD**|**Qw.2**|**15+11.9**|**1024**|**256**|**125**|**166**|**62.9**|**80.8**|**61.6**|**87.0**|**63.6**|**31.0**|**78.7**|**66.2**|**31.7**|**68.9**\nR4|MobileVLMv2 [18]|ViT-L/14|ML.|1.2+3.6|336|144|304|458|59.3|66.7|52.1|84.3|-|-|-|-|-|-\nR5|**FastVLM (Ours)**|**FastViTHD**|**Qw.2**|**15+1.1**|**768**|**144**|**125**|**152**|**63.9**|**75.8**|**64.4**|**87.2**|**65.2**|**35.4**|**79.4**|**61.3**|**34.9**|**71.7**\nR6|**FastVLM (Ours)**|**FastViTHD**|**Qw.2**|**15+11.9**|**768**|**144**|**125**|**152**|**63.8**|**89.8**|**66.2**|**87.6**|**67.9**|**39.7**|**80.2**|**65.7**|**37.8**|**72.4**\nR7|DeepSeekVL [54]|ViT-SO400M|DS.| -|384|576|430|-|-|-|-|87.6|-|34.8|-|-|32.2|66.7\nR8|MM1 [60]<sup>\u2217</sup>|ViT-H|-|3000+1.5|1344|720|632|-|-|62.3|68.2|87.4|67.5|39.4|-|68.4|33.2|65.6\nR9|InstructBLIP [19]|ViT-g/14|Vic.|129+1.2|224|32|1012|302|49.2|60.5|50.1|-|60.9|26.2|-|-|30.6|-\nR10|Qwen-VL [4]|ViT-G/14|Qw.|1400+50|448|256|1844|-|59.3|67.1|63.8|-|-|-|79.5|65.1|-|-\nR11|Qwen-VL-Chat [4]|ViT-G/14|Qw.|1400+50|448|256|1844|-|57.5|68.2|61.5|-|-|-|78.2|62.6|-|-\nR12|ConvLLaVA [25]|ConvNeXT-L|Vic.|4.9+0.6|768|144|200|496|-|-|59.1|87.3|-|44.8|-|44.8|36.3|68.8\nR13|MiniGemini-HD<sup>\u2020</sup>|ViT-L/14|L-3|1.5+1.5|672|2880|304|21832|64.5|75.1|70.2|-|-|-|74.6|37.3|73.2\nR14|Cambrian-1 [72]|ViT-L/14|L-3|2.5+7|336|576|304|5085|64.6|80.4|71.7|-|-|-|77.8|42.7|74.7\nR15|LLaVA-1.5 [49]|ViT-L/14|Vic.|0.5+0.6|336|576|304|1297|62.0|70.4|58.2|85.9|59.6|31.1|76.6|28.1|35.3|66.1\nR16|MobileVLMv2 [18]|ViT-L/14|Vic.|1.2+3.6|336|576|304|1297|64.6|74.8|66.8|86.1|-|-|-|-|-|-\nR17|ShareGPT4V [12]|ViT-L/14|Vic.|1.2+0.7|336|576|304|1297|63.3|68.4|60.4|85.7|72.6|37.6|80.6|-|-|69.7\nR18|ViTamin [11]|ViTamin-L|Vic.|0.5+0.6|384|576|333|1308|61.6|67.6|59.8|85.5|66.1|33.6|78.9|-|-|-\nR19|ConvLLaVA [25]|ConvNeXT-L|Vic.|4.9+0.6|1536|576|200|2740|-|-|65.8|87.3|-|45.9|-|59.0|35.8|70.2\nR20|VILA [45]|ViT-L/14|L-2|50+1|336|576|304|1297|62.3|68.2|64.4|85.5|69.7|34.9|79.9|-|-|62.8\nR21|LLaVA-FlexAttn [41]|ViT-L/14|Vic.|0.5+0.6|1008|576|304|-|62.2|-|48.9|85.9|-|29.4|78.7|-|-|-\nR22|MM1 [60]<sup>\u2217</sup>|ViT-H|-|3000+1.5|1344|720|632|-|72.6|72.8|64.6|86.6|**81.5**|42.1|**82.8**|76.8|37.0|69.9\nR23|LLaVA-NeXT<sup>\u2020</sup><sup>\u2217</sup>|ViT-L/14|L-3|-|672|2880|304|20347|65.2|72.8|64.6|-|80.1|-|-|78.2|41.7|72.7", "caption": "Table 6: \nVLM evaluations and comparison with recent methods. The models are grouped based on total number of visual tokens. \u201c-\u201d indicates that performance was not reported in the respective paper. For the dataset column, \u201c-\u201d indicates that the dataset size for pretraining (\u201cPT\u201d) or instruction tuning (\u201cIT\u201d) is not explicitly mentioned in the respective paper. For methods that have more than 2 stages of training, we report the total samples used for all the pretraining stages as part of \u201cPT\u201d. \u201cTTFT\u201d means time to first token (the sum of the vision encoder latency and the LLM prefilling time), we report latency only for models that are publicly available and in a format favorable to MLX\u00a0[27] \u201cVic.\u201d refers to Vicuna\u00a0[90], \u201cQw.2\u201d refers to Qwen2\u00a0[79] and \u201cQw.\u201d refers to Qwen\u00a0[3]. \u201cL-2\u201d refers to LLaMA-2. \u201cL-3\u201d refers to LLaMA-3. \u201cML.\u201d refers to MobileLLaMA\u00a0[17, 18]. \u201cDS.\u201d refers to DeepSeek LLM\u00a0[20]. \u2217 For input resolution and visual tokens, we report the highest supported resolution by the respective models as some models like LLaVA-OneVision\u00a0[40] and MM1\u00a0[60] use dynamic input resolution. \u2020- performance numbers reported from\u00a0[72]. For MiniGeminiHD\u00a0[44], the dataset sizes is inferred from the pretrain and instruction tuning JSON files. For VLMs that use multiple vision encoders, the size of each encoder is listed independently, for TTFT, the latency from each encoder is summed up.", "description": "\ud45c 6\uc740 \ub2e4\uc591\ud55c \ube44\uc804 \uc5b8\uc5b4 \ubaa8\ub378(VLM)\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4. \ubaa8\ub378\ub4e4\uc740 \ucd1d \uc2dc\uac01 \ud1a0\ud070 \uc218\ub97c \uae30\uc900\uc73c\ub85c \uadf8\ub8f9\ud654\ub418\uc5b4 \uc788\uc73c\uba70, \ucd5c\uadfc \uc5f0\uad6c\ub4e4\uacfc\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \uc704\ud574 \uc5ec\ub7ec \uc9c0\ud45c(GQA, SQA, TextVQA, POPE, DocVQA, MM-Vet, VQAv2, MMMU, SeedBench)\uac00 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud45c\uc758 \uac01 \uc5f4\uc740 \ubaa8\ub378 \uc774\ub984, \uc0ac\uc6a9\ub41c \ube44\uc804 \uc778\ucf54\ub354, \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM), \ud559\uc2b5 \ub370\uc774\ud130 \ud06c\uae30(PT, IT), \uc785\ub825 \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4, \uc2dc\uac01 \ud1a0\ud070 \uc218, \ube44\uc804 \uc778\ucf54\ub354 \ud06c\uae30, \ube44\uc804 \uc778\ucf54\ub354 \uc9c0\uc5f0 \uc2dc\uac04, \uccab \ud1a0\ud070 \uc0dd\uc131 \uc2dc\uac04(TTFT), \uadf8\ub9ac\uace0 \uac01 \uc131\ub2a5 \uc9c0\ud45c\uc5d0 \ub300\ud55c \uacb0\uacfc\uac12\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc77c\ubd80 \ubaa8\ub378\ub4e4\uc740 \ub2e4\uc911 \ube44\uc804 \uc778\ucf54\ub354\ub97c \uc0ac\uc6a9\ud558\uac70\ub098, \ud559\uc2b5 \ub2e8\uacc4\uac00 2\ub2e8\uacc4 \uc774\uc0c1\uc778 \uacbd\uc6b0\ub3c4 \uc788\uc5b4, \ud45c\uc5d0\ub294 \uc774\uc5d0 \ub300\ud55c \uc815\ubcf4\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  '-' \uae30\ud638\ub294 \ud574\ub2f9 \uc815\ubcf4\uac00 \ub17c\ubb38\uc5d0 \uba85\uc2dc\uc801\uc73c\ub85c \uc5b8\uae09\ub418\uc9c0 \uc54a\uc558\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud2b9\ud788, \uccab \ud1a0\ud070 \uc0dd\uc131 \uc2dc\uac04(TTFT)\uc740 \uacf5\uac1c\uc801\uc73c\ub85c \uc811\uadfc \uac00\ub2a5\ud558\uace0 MLX[27]\uacfc \ud638\ud658\ub418\ub294 \ubaa8\ub378\uc5d0 \ub300\ud574\uc11c\ub9cc \ubcf4\uace0\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "4. Comparison with state-of-the-art"}, {"content": "|             | Stage-1             | Stage-2             |\n|-------------|----------------------|----------------------|\n| Data        | LLaVA-1.5 558K       | LLaVA-1.5 665k       |\n| Learning Rate | 1e-3                 | 2e-5                 |\n| Batch size   | 256                  | 128                  |\n| LR. schedule | cosine decay          | cosine decay          |\n| LR. warmup ratio | 0.03                 | 0.03                 |\n| Optimizer    | AdamW                | AdamW                |\n| Trainable modules | Projector           | Full                 |\n|             | Model                 |                      |", "caption": "Table 7: 2-Stage training setup used in ablations for Sec.\u00a03.", "description": "\ud45c 7\uc740 \ub17c\ubb38\uc758 3\uc7a5\uc5d0\uc11c \uc218\ud589\ub41c \uc2e4\ud5d8\uc5d0 \ub300\ud55c 2\ub2e8\uacc4 \ud559\uc2b5 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b, \ud559\uc2b5\ub960, \ubc30\uce58 \ud06c\uae30, \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904, \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998, \ud559\uc2b5 \uac00\ub2a5\ud55c \ubaa8\ub4c8 \ub4f1\uc744 \uc790\uc138\ud558\uac8c \uc124\uba85\ud569\ub2c8\ub2e4.  \uc774\ub294 3\uc7a5\uc758 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \uc774\ud574\ud558\ub294 \ub370 \ud544\uc218\uc801\uc778 \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3. Architecture"}, {"content": "|                     | Stage-1             | Stage-1.5          | Stage-2                 |\n|---------------------|----------------------|----------------------|--------------------------|\n| Data                 | LLaVA-1.5 558K        | Recap-CC3M +        | 1.1M / 6.5M / 11.9M     |\n|                     |                      | Recap-CC12M [38]     |                          |\n| Learning Rate        | 1e-3                 | 2e-5                 | 2e-5                     |\n| Batch size           | 256                  | 128                  | 128                      |\n| LR. schedule         | cosine decay          | cosine decay          | cosine decay              |\n| LR. warmup ratio     | 0.03                 | 0.03                 | 0.03                     |\n| Optimizer            | AdamW                | AdamW                | AdamW                    |\n| Trainable modules   | Projector            | Full                 | Full                      |\n|                     | Model                 | Model                 |                          |", "caption": "Table 8: 3-Stage training setup used for results with scaled-up data in Tab.\u00a06.", "description": "\ud45c 8\uc740 \ub17c\ubb38\uc758 \ud45c 6\uc5d0 \uc81c\uc2dc\ub41c \uacb0\uacfc\ub97c \uc5bb\uae30 \uc704\ud574 \ud655\uc7a5\ub41c \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud6c8\ub828\ub41c \ubaa8\ub378\uc5d0 \uc0ac\uc6a9\ub41c 3\ub2e8\uacc4 \ud6c8\ub828 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  1\ub2e8\uacc4\ub294 LLaVA-1.5 558K \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud504\ub85c\uc81d\ud130\ub9cc\uc744 \ud559\uc2b5\uc2dc\ud0a4\ub294 \ub2e8\uacc4\uc785\ub2c8\ub2e4. 1.5\ub2e8\uacc4\ub294 \ucd94\uac00\uc801\uc778 15M\uac1c\uc758 \uc774\ubbf8\uc9c0 \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b(Recap-CC3M \ubc0f Recap-CC12M)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0 \ud574\uc0c1\ub3c4\ub97c \uc870\uc815\ud558\uace0 \uc804\uccb4 \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud558\ub294 \ub2e8\uacc4\uc785\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9 2\ub2e8\uacc4\uc5d0\uc11c\ub294 110\ub9cc/650\ub9cc/1190\ub9cc\uac1c\uc758 \uc9c0\uc2dc\uc5b4 \uc870\uc815 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc804\uccb4 \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud569\ub2c8\ub2e4.  \ud559\uc2b5\ub960, \ubc30\uce58 \ud06c\uae30, \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uacfc \uac19\uc740 \ucd08\ub9e4\uac1c\ubcc0\uc218\ub294 \uac01 \ub2e8\uacc4\ub9c8\ub2e4 \ub2e4\ub974\uac8c \uc124\uc815\ub429\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30 \ubcc0\ud654\uc5d0 \ub530\ub978 \ubaa8\ub378 \uc131\ub2a5\uc758 \ubcc0\ud654\ub97c \ubd84\uc11d\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "3. Architecture"}, {"content": "| Row | Method | Vision | LLM | Input | #Visual | Vis. Enc. | Vision Enc. | LLM |\n|---|---|---|---|---|---|---|---|---|\n| Ann. | nanoLLaVA | ViT-SO400M | Qw.1.5 | 384 | 729 | 430 | 272.1 | 263.3 |\n| R2 | LLaVAOV [40]* | ViT-SO400M | Qw.2 | 1152 | 7290 | 430 | 2721.4 | 11402.4 |\n| R3 | **FastVLM (Ours)** | FastViTHD | Qw.2 | 1024 | 256 | **125** | **116.3** | 50.5 |\n|  | 0.5B Model Comparison |  |  |  |  |  |  |  |\n| R4 | MobileVLMv2 [18] | ViT-L/14 | ML. | 336 | 144 | 304 | 127.4 | 458 |\n| R5 | **FastVLM (Ours)** | FastViTHD | Qw.2 | 768 | 144 | **125** | **54.8** | 97.1 |\n| R6 | DeepSeekVL [54] | ViT-SO400M | DS. | 384 | 576 | 430 | 272.1 | - |\n| R7 | MM1 [60]* | ViT-H | - | 1344 | 720 | 632 | - | - |\n| R8 | **FastVLM (Ours)** | FastViTHD | Qw.2 | 1024 | 256 | **125** | **116.3** | 116.1 |\n|  | 1-2B Model Comparison |  |  |  |  |  |  |  |\n| R9 | InstructBLIP [19] | ViT-g/14 | Vic. | 224 | 32 | 1012 | 149.5 | 152.1 |\n| R11 | **FastVLM (Ours)** | FastViTHD | Vic. | 256 | 16 | **125** | **6.8** | 143.4 |\n| R12 | MobileVLMv2 [18] | ViT-L/14 | Vic. | 336 | 144 | 304 | 127.4 | 332.1 |\n| R13 | ConvLLaVA [25] | ConvNeXT-L | Vic. | 768 | 144 | 200 | 164.3 | 332.1 |\n| R14 | **FastVLM (Ours)** | FastViTHD | Vic. | 768 | 144 | **125** | **54.8** | 332.1 |\n| R17 | **FastVLM (Ours)** | FastViTHD | Qw.2 | 768 | 144 | **125** | **54.8** | 391.2 |\n| R20 | ConvLLaVA [25] | ConvNeXT-L | Vic. | 1024 | 256 | 200 | 696.1 | 461.1 |\n| R26 | LLaVA-1.5 [49] | ViT-L/14 | Vic. | 336 | 576 | 304 | 127.4 | 1170.0 |\n| R27 | MobileVLMv2 [18] |  |  | 336 | 576 | 304 | 127.4 | 1170.0 |\n| R28 | ShareGPT4V [12] |  |  | 336 | 576 | 304 | 127.4 | 1170.0 |\n| R29 | ViTamin [11] | ViTamin-L | Vic. | 384 | 576 | 333 | 137.6 | 1170.0 |\n| R30 | ConvLLaVA [25] | ConvNeXT-L | Vic. | 1536 | 576 | 200 | 1569.7 | 1170.0 |\n| R31 | VILA [45] | ViT-L/14 | L-2 | 336 | 576 | 304 | 127.4 | 1169.5 |\n| R33 | MM1 [60]* | ViT-H | - | 1344 | 720 | 632 | - | - |\n| R34 | LLaVA-NeXT* | ViT-L/14 | L-3 | 672 | 2880 | 304 | 637.0 | 19709.7 |\n| R21 | **FastVLM (Ours)** | FastViTHD | Vic. | 1024 | 256 | **125** | **116.3** | 461.1 |\n| R36 | **FastVLM (Ours)** | FastViTHD | Qw.2 | 1024 | 256 | **125** | **116.3** | 524.5 |\n|  | 7B Model Comparison |  |  |  |  |  |  |  |\n|  |  | ConvNeXT-L |  | 1536 |  | 200 | 1569.7 |  |\n| R35 | MiniGemini-HD | ViT-L/14 | L-3 | 672 | 2880 | 304 | 552.6 | 19709.7 |\n|  |  | ViT-SO400M |  | 384 |  | 430 | 272.1 |  |\n|  |  | ConvNeXT-XXL |  | 1024 |  | 846 | 2290.4 |  |\n|  |  | DINOv2-ViT-L/14 |  | 518 |  | 304 | 1171.5 |  |\n| R36 | Cambrian-1 [72] | ViT-L/14 | L-3 | 336 | 576 | 304 | 127.4 | 1223.6 |", "caption": "Table 9: \nBreakdown of prefilling latencies for recent methods. The models are grouped based on total number of visual tokens. For models that were difficult to export or unavailable, we mark them as \u2019-\u2019 in the table. \u201cVic.\u201d refers to Vicuna\u00a0[90], \u201cQw.2\u201d refers to Qwen2\u00a0[79] and \u201cQw.\u201d refers to Qwen\u00a0[3]. \u201cL-2\u201d refers to LLaMA-2. \u201cL-3\u201d refers to LLaMA-3. \u201cML.\u201d refers to MobileLLaMA\u00a0[17, 18]. \u201cDS.\u201d refers to DeepSeek LLM\u00a0[20]. \u2217 For input resolution and visual tokens, we report the highest supported resolution by the respective models as some models like LLaVA-OneVision\u00a0[40] and MM1\u00a0[60] use dynamic input resolution. For VLMs that use multiple vision encoders, the size of each encoder is listed independently, for TTFT, the latency from each encoder is summed up.", "description": "\ud45c 9\ub294 \ucd5c\uadfc \uc5f0\uad6c\uc5d0\uc11c \uc81c\uc2dc\ub41c \uc5ec\ub7ec \ub2e4\uc591\ud55c \ube44\uc804 \uc5b8\uc5b4 \ubaa8\ub378(VLM)\uc758 \uc0ac\uc804 \ucc44\uc6b0\uae30 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc740 \uc2dc\uac01 \ud1a0\ud070\uc758 \ucd1d \uac1c\uc218\ub97c \uae30\uc900\uc73c\ub85c \uadf8\ub8f9\ud654\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc77c\ubd80 \ubaa8\ub378\uc740 \ub0b4\ubcf4\ub0b4\uae30\uac00 \uc5b4\ub835\uac70\ub098 \uc0ac\uc6a9\ud560 \uc218 \uc5c6\uc5b4 \ud45c\uc5d0\uc11c '-'\ub85c \ud45c\uc2dc\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  Vicuna [90], Qwen2 [79], Qwen [3], LLaMA-2, LLaMA-3, MobileLLaMA [17, 18], DeepSeek LLM [20] \ub4f1 \ub2e4\uc591\ud55c \ubaa8\ub378\uc5d0 \ub300\ud55c \uc57d\uc5b4\uac00 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \ud2b9\ud788 LLaVA-OneVision [40]\uc774\ub098 MM1 [60]\uacfc \uac19\uc774 \ub3d9\uc801 \uc785\ub825 \ud574\uc0c1\ub3c4\ub97c \uc0ac\uc6a9\ud558\ub294 \ubaa8\ub378\uc758 \uacbd\uc6b0 \uac00\uc7a5 \ub192\uc740 \uc9c0\uc6d0 \ud574\uc0c1\ub3c4\uc640 \uc2dc\uac01 \ud1a0\ud070 \uc218\uac00 \ubcf4\uace0\ub429\ub2c8\ub2e4. \uc5ec\ub7ec \ube44\uc804 \uc778\ucf54\ub354\ub97c \uc0ac\uc6a9\ud558\ub294 VLM\uc758 \uacbd\uc6b0 \uac01 \uc778\ucf54\ub354\uc758 \ud06c\uae30\uac00 \uac1c\ubcc4\uc801\uc73c\ub85c \ub098\uc5f4\ub418\uba70, TTFT(\ucc98\uc74c \ud1a0\ud070\uae4c\uc9c0\uc758 \uc2dc\uac04)\ub294 \uac01 \uc778\ucf54\ub354\uc758 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \ud569\uc0b0\ud558\uc5ec \uacc4\uc0b0\ub429\ub2c8\ub2e4.", "section": "3.2.1 Vision Encoder - Language Decoder Interplay"}, {"content": "| Row | Method | Vision | LLM | Data (M) | Input | #Visual | Vis. Enc. | TTFT | ChartQA | OCRBench | TextVQA | DocVQA | InfoVQA |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| Row | Method | Vision | LLM | Data (M) | Input | #Visual | Vis. Enc. | TTFT | ChartQA | OCRBench | TextVQA | DocVQA | InfoVQA |\n| Ann. |  |  |  |  | Size(M)\u2193 | (ms)\u2193 |  |  |  |  |  |  |\n| R1 | MM1 [60]* | ViT-H | - | 3000+1.5 | 1344 | 720 | 632 | - | 72.6 | 62.6 | 72.8 | 76.8 | 45.5 |\n| R2 | LLaVA-NeXT\u2020* | ViT-L/14 | L-3 | - | 672 | 2880 | 304 | 20347 | 69.5 | 49.0 | 64.6 | 72.6 | - |\n| R3 | Cambrian-1 [72] | ViT-L/14 | L-3 | 2.5+7 | 336 | 576 | 304 | 5085 | 73.3 | 62.4 | 71.7 | 77.8 | - |\n|  |  | ViT-SO400M |  |  | 384 |  | 430 |  |  |  |  |  |  |\n|  |  | ConvNeXt-XXL |  |  | 1024 |  | 846 |  |  |  |  |  |  |\n|  |  | DINOv2-ViT-L/14 |  |  | 518 |  | 304 |  |  |  |  |  |  |\n| R4 | **FastVLM (Ours)** | FastViTHD | Vic. | 0.5+0.6 | 768 | 144 | **125** | **387** | 17.1 | 30.0 | 62.9 | 32.9 | 28.7 |\n| R5 | **FastVLM (Ours)** | FastViTHD | Vic. | 0.5+1.1 | 768 | 144 | **125** | **387** | 59.1 | 38.4 | 67.5 | 57.3 | 29.7 |\n| R6 | **FastVLM (Ours)** | FastViTHD | Vic. | 15+1.1 | 768 | 144 | **125** | **387** | 65.4 | 45.3 | 69.4 | 65.5 | 32.0 |\n| R7 | **FastVLM (Ours)** | FastViTHD | Qw.2 | 15+1.1 | 768 | 144 | **125** | 446 | 69.3 | 45.9 | 69.5 | 66.9 | 34.3 |\n| R8 | **FastVLM (Ours)** | FastViTHD | Qw.2 | 15+11.9 | 768 | 144 | **125** | 446 | 74.2 | 59.0 | 72.8 | 72.0 | 44.3 |\n| R9 | **FastVLM (Ours)** | FastViTHD | Vic. | 0.5+0.6 | 1024 | 256 | **125** | **577** | 19.2 | 29.3 | 64.4 | 35.6 | 28.9 |\n| R10 | **FastVLM (Ours)** | FastViTHD | Vic. | 0.5+1.1 | 1024 | 256 | **125** | **577** | 61.0 | 38.3 | 67.4 | 62.8 | 32.0 |\n| R11 | **FastVLM (Ours)** | FastViTHD | Vic. | 15+1.1 | 1024 | 256 | **125** | **577** | 66.9 | 47.1 | 70.6 | 72.4 | 34.7 |\n| R12 | **FastVLM (Ours)** | FastViTHD | Qw.2 | 15+1.1 | 1024 | 256 | **125** | 641 | 71.0 | 49.7 | 72.1 | 73.3 | 37.5 |\n| R13 | **FastVLM (Ours)** | FastViTHD | Qw.2 | 15+6.5 | 1024 | 256 | **125** | 641 | 76.6 | 52.9 | 73.1 | 78.7 | 44.2 |\n| R14 | **FastVLM (Ours)** | FastViTHD | Qw.2 | 15+11.9 | 1024 | 256 | **125** | 641 | **77.0** | **63.3** | **74.8** | **78.9** | **49.7** |", "caption": "Table 10: \nComparison with recent methods on text-rich benchmarks. The models are grouped based on total number of visual tokens. \u201c-\u201d indicates that performance was not reported in the respective paper. For the dataset column, \u201c-\u201d indicates that the dataset size for pretraining (\u201cPT\u201d) or instruction tuning (\u201cIT\u201d) is not explicitly mentioned in the respective paper. For methods that have more than 2 stages of training, we report the total samples used for all the pretraining stages as part of \u201cPT\u201d. \u201cTTFT\u201d means time to first token (the sum of the vision encoder latency and the LLM prefilling time), we report latency only for models that are publicly available and in a format favorable to MLX\u00a0[27] \u201cVic.\u201d refers to Vicuna\u00a0[90], \u201cQw.2\u201d refers to Qwen2\u00a0[79]. \u201cL-3\u201d refers to LLaMA-3. * - For input resolution and visual tokens, we report the highest supported resolution by the respective models as some models like MM1\u00a0[60] use dynamic input resolution. \u2020- performance numbers reported from\u00a0[72]. For VLMs that use multiple vision encoders, the size of each encoder is listed independently, for TTFT, the latency from each encoder is summed up.", "description": "\ud45c 10\uc740 \uc5ec\ub7ec \ucd5c\uc2e0 \ubc29\ubc95\ub4e4\uacfc\uc758 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc774\uba70, \ud14d\uc2a4\ud2b8\uac00 \ud48d\ubd80\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ubaa8\ub378\uc740 \ucd1d \uc2dc\uac01 \ud1a0\ud070 \uc218\ub97c \uae30\uc900\uc73c\ub85c \uadf8\ub8f9\ud654\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \ub2e4\uc591\ud55c \uc9c0\ud45c\uac00 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  '-'\ub294 \ud574\ub2f9 \ub17c\ubb38\uc5d0 \uc131\ub2a5\uc774 \ubcf4\uace0\ub418\uc9c0 \uc54a\uc740 \uacbd\uc6b0\ub97c \ub098\ud0c0\ub0b4\uba70, \ub370\uc774\ud130\uc14b \uc5f4\uc758 '-'\ub294 \uc0ac\uc804 \ud6c8\ub828 \ub610\ub294 \uc9c0\uc2dc \ud29c\ub2dd \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\uac00 \uba85\uc2dc\uc801\uc73c\ub85c \uc5b8\uae09\ub418\uc9c0 \uc54a\uc740 \uacbd\uc6b0\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. 2\ub2e8\uacc4 \uc774\uc0c1\uc758 \ud6c8\ub828\uc744 \uac70\uce5c \ubc29\ubc95\uc758 \uacbd\uc6b0, \ubaa8\ub4e0 \uc0ac\uc804 \ud6c8\ub828 \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ucd1d \uc0d8\ud50c \uc218\uac00 'PT'\uc5d0 \ud3ec\ud568\ub429\ub2c8\ub2e4. TTFT(Time To First Token)\ub294 \uc2dc\uac01 \uc778\ucf54\ub354 \uc9c0\uc5f0 \uc2dc\uac04\uacfc LLM(Large Language Model) \uc0ac\uc804 \ucc44\uc6b0\uae30 \uc2dc\uac04\uc758 \ud569\uc73c\ub85c, \uacf5\uac1c\uc801\uc73c\ub85c \uc0ac\uc6a9 \uac00\ub2a5\ud558\uace0 MLX [27]\uc5d0 \uc801\ud569\ud55c \ud615\uc2dd\uc778 \ubaa8\ub378\uc5d0 \ub300\ud574\uc11c\ub9cc \uc9c0\uc5f0 \uc2dc\uac04\uc774 \ubcf4\uace0\ub429\ub2c8\ub2e4. 'Vic.'\uc740 Vicuna [90], 'Qw.2'\ub294 Qwen2 [79], 'L-3'\ub294 LLaMA-3\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc77c\ubd80 \ubaa8\ub378(\uc608: MM1 [60])\uc740 \ub3d9\uc801 \uc785\ub825 \ud574\uc0c1\ub3c4\ub97c \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0 \uc785\ub825 \ud574\uc0c1\ub3c4\uc640 \uc2dc\uac01 \ud1a0\ud070 \uc218\uc5d0 \ub300\ud574\uc11c\ub294 \ud574\ub2f9 \ubaa8\ub378\uc5d0\uc11c \uc9c0\uc6d0\ud558\ub294 \uac00\uc7a5 \ub192\uc740 \ud574\uc0c1\ub3c4\ub97c \ubcf4\uace0\ud569\ub2c8\ub2e4.  \u2020\ub294 [72]\uc5d0\uc11c \ubcf4\uace0\ub41c \uc131\ub2a5 \uc218\uce58\uc774\uba70, \uc5ec\ub7ec \uc2dc\uac01 \uc778\ucf54\ub354\ub97c \uc0ac\uc6a9\ud558\ub294 VLM\uc758 \uacbd\uc6b0 \uac01 \uc778\ucf54\ub354\uc758 \ud06c\uae30\uac00 \uac1c\ubcc4\uc801\uc73c\ub85c \ub098\uc5f4\ub418\uace0 TTFT\uc5d0\ub294 \uac01 \uc778\ucf54\ub354\uc758 \uc9c0\uc5f0 \uc2dc\uac04\uc774 \ud569\uc0b0\ub429\ub2c8\ub2e4.", "section": "4. \uacb0\uacfc"}, {"content": "|   | GQA | SQA | TextVQA | POPE | LLaVA | MMVet | VQAv2 | DocVQA | Seed |\n|---|---|---|---|---|---|---|---|---|---| \n| Bench<sup>W</sup> | 62.69 | 64.25 | 60.71 | 85.8 | 59.4 | 29.6 | 77.27 | 27.57 | 53.31 |\n| Bench<sup>I</sup> | 62.68 | 64.95 | 60.61 | 86.1 | 60.1 | 31.6 | 77.39 | 28.37 | 53.55 |\n|  | 62.69 | 65.64 | 60.68 | 85.3 | 61.4 | 31.1 | 77.31 | 28.26 | 53.46 |\n| Std. | 0.0047 | 0.57 | 0.041 | 0.33 | 0.83 | 0.85 | 0.049 | 0.35 | 0.099 |", "caption": "Table 11: \nVLM benchmarks across three independent runs with frozen FastViT image encoder. Training setup is LLaVA-1.5 with Vicuna 7B as LLM. Standard deviation across runs is listed in the bottom row.", "description": "\ud45c 11\uc740 \ub3d9\uc77c\ud55c \uc124\uc815(LLaVA-1.5, Vicuna 7B LLM \uc0ac\uc6a9)\uc5d0\uc11c \uc138 \ubc88\uc758 \ub3c5\ub9bd\uc801\uc778 \uc2e4\ud589 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4. FastViT \uc774\ubbf8\uc9c0 \uc778\ucf54\ub354\uac00 \uace0\uc815\ub418\uc5b4 \uc788\uace0, \ud45c\uc758 \uac01 \uc5f4\uc740 Seed, GQA, SQA, TextVQA, POPE, DocVQA, Bench, MMVet, VQAv2 \uc640 \uac19\uc740 \ub2e4\uc591\ud55c VLM \ubca4\uce58\ub9c8\ud06c\uc758 \ud3c9\uac00 \uc9c0\ud45c\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9 \ud589\uc740 \uac01 \uc9c0\ud45c\uc5d0 \ub300\ud55c \ud45c\uc900 \ud3b8\ucc28\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "D.3. Evaluations"}]