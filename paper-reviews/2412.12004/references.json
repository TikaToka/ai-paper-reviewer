{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduced the Transformer architecture, which revolutionized NLP by enabling parallel processing of sequences and capturing long-range dependencies, forming the basis of modern LLMs."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2018-10-11", "reason": "BERT's bidirectional training approach and masked language modeling significantly enhanced performance on various NLP tasks, influencing the development of subsequent LLMs."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and Efficient Foundation Language Models", "publication_date": "2023-02-27", "reason": "LLaMA demonstrated that high performance in LLMs could be achieved with smaller model sizes and open accessibility, challenging the dominance of closed-source models and promoting democratization of LLM technology."}, {"fullname_first_author": "BigScience Workshop", "paper_title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "publication_date": "2022-05-10", "reason": "BLOOM showcased the potential of open-source collaboration in developing large-scale multilingual LLMs, addressing linguistic diversity and promoting inclusivity in NLP research."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models", "publication_date": "2021-06-14", "reason": "LoRA's parameter-efficient fine-tuning technique significantly reduced the computational cost of adapting LLMs to specific tasks, enabling wider adoption and customization of large language models, especially in resource-constrained settings."}]}