{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-02", "reason": "This paper introduces a crucial safety technique, preference tuning, which is directly relevant to the main topic of the current paper about mitigating safety degradation in fine-tuned LLMs."}, {"fullname_first_author": "Yangyi Chen", "paper_title": "Why should adversarial perturbations be imperceptible? Rethink the research paradigm in adversarial NLP", "publication_date": "2022-12-01", "reason": "This paper discusses the challenges of adversarial attacks on LLMs, providing context for the current paper's focus on maintaining safety in fine-tuned models."}, {"fullname_first_author": "Mitchell Wortsman", "paper_title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "publication_date": "2022-07-01", "reason": "This paper introduces the concept of model merging, which is a core technique of the current paper, offering a practical solution for adapting safety-aligned LLMs."}, {"fullname_first_author": "Hyung Won Chung", "paper_title": "Scaling instruction-finetuned language models", "publication_date": "2024-01-01", "reason": "This paper explores instruction-finetuned language models and demonstrates the effectiveness of scaling techniques, which are relevant to the current paper's goal of enhancing downstream task performance."}, {"fullname_first_author": "Seungju Han", "paper_title": "WildGuard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of LLMs", "publication_date": "2024-01-01", "reason": "This paper introduces WildGuard, a safety classifier used in the current paper to assess the safety of the models, offering a way to quantitatively evaluate the impact of different methods on preserving model safety."}]}