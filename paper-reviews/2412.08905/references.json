{"references": [{"fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "publication_date": "2024-04-14", "reason": "This paper introduces Phi-3, a previous model in the Phi family which served as a basis for the development of phi-4."}, {"fullname_first_author": "Suriya Gunasekar", "paper_title": "Textbooks are all you need", "publication_date": "2023-06-11", "reason": "This paper introduces key techniques used in the training of Phi-3, which were further developed in phi-4."}, {"fullname_first_author": "Mojan Javaheripi", "paper_title": "The surprising power of small language models", "publication_date": "2023-07-24", "reason": "This paper demonstrates the performance capabilities of smaller language models, which is relevant to the design choices of phi-4."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-12-01", "reason": "This paper introduces Direct Preference Optimization (DPO), a crucial post-training technique used to refine the outputs of phi-4."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduces the Transformer architecture, which is the foundation of the phi-4 model architecture."}]}