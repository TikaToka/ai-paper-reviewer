[{"content": "| Model | Vision Sensors | Existence | Count | Position | General | Multi-vision | Contextual | Sensory | Multi-vision | Open Source Large-scale Vision-Language Models |  | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | BLIP-2 [25] | Thermal | 59.2 | 32.2 | 57.8 | 65.3 | 53.6 | 74.8 | 42.7 | 58.7 |  | Depth | 60.4 | 40.0 | 52.4 | 71.6 | 56.1 | 71.3 | 26.3 | 48.8 |  | X-ray | 65.2 | 55.0 | 58.6 | 81.7 | 65.1 | 75.8 | 59.3 | 67.5 |  | LLaVA-1.5-7B [26] | Thermal | 60.7 | 27.6 | 65.6 | 60.7 | 53.7 | 74.4 | 41.1 | 57.8 |  | Depth | 73.6 | 22.1 | 61.0 | 77.6 | 58.6 | 73.0 | 22.1 | 47.5 |  | X-ray | 63.2 | 35.3 | 54.6 | 75.0 | 57.0 | 73.9 | 49.6 | 61.7 |  | InternVL2-8B [39] | Thermal | 66.7 | 47.7 | 70.3 | 73.0 | 64.4 | 74.8 | 50.4 | 60.6 |  | Depth | 71.2 | 40.5 | 67.2 | 77.6 | 64.1 | 68.8 | 28.7 | 48.7 |  | X-ray | 69.5 | 39.8 | 64.9 | 82.8 | 64.3 | 75.6 | 65.0 | 70.3 |  | VideoLLaMA2-7B [5] | Thermal | 82.4 | 49.8 | 69.5 | 81.7 | 70.8 | 83.8 | 76.2 | 80.0 |  | Depth | 82.2 | 40.5 | 66.9 | 83.5 | 68.3 | 77.9 | 29.9 | 53.9 |  | X-ray | 70.2 | 49.0 | 60.2 | 85.7 | 66.2 | 80.6 | 72.9 | 76.7 |  | MiniCPM-V-2.5-8B [53] | Thermal | 76.1 | 52.8 | 72.7 | 77.8 | 69.8 | 80.9 | 59.8 | 70.4 |  | Depth | 76.8 | 43.7 | 71.6 | 84.7 | 69.2 | 77.4 | 51.3 | 64.3 |  | X-ray | 75.2 | 51.0 | 72.1 | 85.3 | 70.9 | 85.7 | 81.6 | 83.7 |  | Qwen2-VL-7B [50] | Thermal | 76.1 | 47.7 | 72.7 | 77.6 | 68.5 | 70.6 | 62.8 | 66.7 |  | Depth | 75.1 | 38.4 | 64.1 | 81.6 | 64.8 | 65.0 | 19.3 | 42.1 |  | X-ray | 71.0 | 39.8 | 63.8 | 84.4 | 64.7 | 76.0 | 64.4 | 70.2 |  | Phantom-7B [23] | Thermal | 71.1 | 46.3 | 75.0 | 72.7 | 66.3 | 77.4 | 50.6 | 64.0 |  | Depth | 67.8 | 36.3 | 68.1 | 76.6 | 62.2 | 66.9 | 29.6 | 48.2 |  | X-ray | 69.9 | 44.6 | 64.1 | 82.4 | 65.3 | 76.8 | 67.6 | 72.2 |  | Closed Source Large-scale Vision-Language Models |  |  |  |  |  |  |  |  |  | Gemini-Pro [47] | Thermal | 81.8 | 57.3 | 79.7 | 80.7 | 74.9 | 84.5 | 68.7 | 76.6 |  | Depth | 82.1 | 38.4 | 73.7 | 86.6 | 70.2 | 78.2 | 32.5 | 55.3 |  | X-ray | 76.7 | 49.4 | 66.5 | 89.8 | 70.6 | 86.9 | 76.2 | 81.5 |  | GPT-4o [38] | Thermal | 79.3 | 55.3 | 78.9 | 84.4 | 74.5 | 90.6 | 69.7 | 80.2 |  | Depth | 84.9 | 45.8 | 73.2 | 90.2 | 73.5 | 85.0 | 33.6 | 59.3 |  | X-ray | 78.2 | 41.0 | 72.5 | 90.6 | 70.6 | 85.5 | 79.3 | 82.4 |  | Claude-3.5-Sonnet [1] | Thermal | 75.3 | 46.2 | 64.1 | 67.8 | 63.3 | 65.4 | 64.4 | 64.9 |  | Depth | 63.3 | 30.5 | 52.3 | 73.0 | 54.8 | 53.8 | 44.5 | 49.1 |  | X-ray | 66.8 | 33.1 | 68.1 | 82.4 | 62.6 | 76.9 | 72.9 | 74.9 |  |  |  |  |  |  |  |  |  |  |  |", "caption": "Table 1: Evaluation results of different VLMs on the MS-PR benchmark are reported, using accuracy as the metric. \u201cMulti-vision Perception\u201d shows the average performance on four dimensions (Existence, Count, Position, and General Description) for evaluating visual perception, and \u201cMulti-vision Reasoning\u201d shows the average performance on two dimensions (Contextual Reasoning and Sensory Reasoning) for evaluating vision sensory understanding. VLMs are sorted in ascending order of release date.", "description": "\ud45c 1\uc740 MS-PR \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub2e4\uc591\ud55c VLMs\uc758 \uc131\ub2a5 \ud3c9\uac00 \uacb0\uacfc\ub97c \uc815\ud655\ub3c4\ub97c \uce21\uc815 \uae30\uc900\uc73c\ub85c \ub098\ud0c0\ub0b8 \uac83\uc785\ub2c8\ub2e4. \"\ub2e4\uc911 \ube44\uc804 \uc778\uc2dd\" \uc5f4\uc740 \uc2dc\uac01\uc801 \uc778\uc2dd \ud3c9\uac00\ub97c \uc704\ud55c \ub124 \uac00\uc9c0 \ucc28\uc6d0(\uc874\uc7ac, \uacc4\uc0b0, \uc704\uce58, \uc77c\ubc18 \uc124\uba85)\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uace0, \"\ub2e4\uc911 \ube44\uc804 \ucd94\ub860\" \uc5f4\uc740 \uc2dc\uac01\uc801 \uac10\uac01 \uc774\ud574 \ud3c9\uac00\ub97c \uc704\ud55c \ub450 \uac00\uc9c0 \ucc28\uc6d0(\ub9e5\ub77d \ucd94\ub860 \ubc0f \uac10\uac01 \ucd94\ub860)\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. VLMs\ub294 \ucd9c\uc2dc \ub0a0\uc9dc \uc21c\uc73c\ub85c \uc815\ub82c\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. Multi-Vision Sensor Perception and Reasoning (MS-PR) Benchmark"}, {"content": "| General | Description |\n|---|---|", "caption": "Table 2: Performance is increased by Diverse Negative Attributes (DNA) optimization in the sense of multi-vision reasoning. Highlighted columns show average performance for perception and reasoning capabilities. The best results are denoted in bold.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c VLMs\uc5d0 \ub300\ud55c \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \uc778\uc2dd \ubc0f \ucd94\ub860 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. DNA \ucd5c\uc801\ud654\ub97c \ud1b5\ud574 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \ucd94\ub860 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uacb0\uacfc\uc785\ub2c8\ub2e4.  \uac01 VLMs\uc758 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \uc778\uc2dd(\uc874\uc7ac, \uacc4\uc0b0, \uc704\uce58, \uc77c\ubc18 \uc124\uba85)\uacfc \ucd94\ub860(\ub9e5\ub77d\uc801 \ucd94\ub860, \uac10\uac01\uc801 \ucd94\ub860)\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc815\ud655\ub3c4\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \uc5f4\uc758 \ucd5c\uace0 \uc131\ub2a5\uc740 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \ucd94\ub860 \ud5a5\uc0c1"}, {"content": "| Multi-vision |\n|---|---| \n| Perception |", "caption": "Table 3: Ablation study on multi-vision sensor reasoning performance according to the number of negative sample k\ud835\udc58kitalic_k.", "description": "\ud45c 3\uc740 \ub2e4\uc591\ud55c \uc218\uc758 \ubd80\uc815\uc801 \uc608\uc2dc(k)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c \ucd94\ub860 \uc131\ub2a5\uc5d0 \ub300\ud55c \ucd94\uac00 \ubd84\uc11d \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc5ec\ub7ec \uac1c\uc758 \ubd80\uc815\uc801 \uc608\uc2dc\ub97c \uc0ac\uc6a9\ud568\uc73c\ub85c\uc368 \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \uc0c1\ud669\uc5d0\uc11c \uc13c\uc11c \ub370\uc774\ud130\ub97c \ub354 \uc798 \uc774\ud574\ud558\uace0 \ucd94\ub860\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub418\ub294\uc9c0 \ud655\uc778\ud558\ub294 \uc2e4\ud5d8\uc785\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ud2b9\ud788 \uc0c1\ud669 \ucd94\ub860 \ubc0f \uac10\uac01 \ucd94\ub860 \uacfc\uc81c\uc5d0\uc11c \ubd80\uc815\uc801 \uc608\uc2dc\uc758 \uc218\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \ub2e4\uc591\ud55c \ubd80\uc815\uc801 \uc608\uc2dc\ub294 \ub2e4\uc911 \ube44\uc804 \uc13c\uc11c\uc5d0 \ub300\ud55c \uc774\ud574\ub3c4\ub97c \ub192\uc774\ub294 \ub370 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2. \ub2e4\uc591\ud55c \ubd80\uc815\uc801 \uc18d\uc131 \ucd5c\uc801\ud654"}, {"content": "| Contextual | Reasoning |\n|---|---|", "caption": "Table 4: Ablation study on multi-vision sensor reasoning performance according to the number of training images per sensor n\ud835\udc5bnitalic_n", "description": "\ud45c 4\ub294 \ub2e4\uc591\ud55c \uc13c\uc11c \uc720\ud615(\uc5f4\ud654\uc0c1, \uae4a\uc774, X\uc120)\uc5d0 \ub300\ud574 \ud559\uc2b5 \uc774\ubbf8\uc9c0 \uc218(n)\ub97c \ubcc0\uacbd\ud558\uba74\uc11c \ub2e4\uc591\ud55c \ube44\uc804 \uc5b8\uc5b4 \ubaa8\ub378(VLMs)\uc758 \uba40\ud2f0 \ube44\uc804 \uc13c\uc11c \ucd94\ub860 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc13c\uc11c \uc720\ud615\ubcc4\ub85c 50\uac1c\uc640 100\uac1c\uc758 \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uba70,  SFT(Supervised Fine-Tuning)\uc640 DNA(Diverse Negative Attributes) \ucd5c\uc801\ud654 \uae30\ubc95\uc744 \uc801\uc6a9\ud55c \uacbd\uc6b0\uc640 \uadf8\ub807\uc9c0 \uc54a\uc740 \uacbd\uc6b0\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \uba40\ud2f0 \ube44\uc804 \uc778\uc2dd(Existence, Count, Position, General Description)\uacfc \uba40\ud2f0 \ube44\uc804 \ucd94\ub860(Contextual Reasoning, Sensory Reasoning)\uc758 \ub450 \uac00\uc9c0 \uc8fc\uc694 \uacfc\uc81c\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uc5ec DNA \ucd5c\uc801\ud654\uac00 \uc81c\ud55c\ub41c \ub370\uc774\ud130 \ud658\uacbd\uc5d0\uc11c\ub3c4 \uba40\ud2f0 \ube44\uc804 \uc13c\uc11c \ucd94\ub860 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Enhancing Multi-vision Sensor Reasoning"}, {"content": "| Sensory |\n|---|---| \n| Reasoning |", "caption": "Table 5: Performance comparison of Phantom-7B with and without DNA optimization across various benchmarks.", "description": "\ud45c 5\ub294 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c DNA \ucd5c\uc801\ud654 \uc801\uc6a9 \uc720\ubb34\uc5d0 \ub530\ub978 Phantom-7B \ubaa8\ub378\uc758 \uc131\ub2a5 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e8\uc21c\ud55c \uc131\ub2a5 \ube44\uad50\ub97c \ub118\uc5b4,  DNA \ucd5c\uc801\ud654\uac00 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ubca4\uce58\ub9c8\ud06c \uc791\uc5c5\uc5d0\uc11c \uc5bc\ub9c8\ub098 \uc77c\ubc18\ud654\ub41c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uac00\uc838\uc624\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.  \uac01 \ubca4\uce58\ub9c8\ud06c\uc758 \uc138\ubd80\uc801\uc778 \ud2b9\uc9d5\uacfc Phantom-7B \ubaa8\ub378\uc758 DNA \ucd5c\uc801\ud654 \uc804\ud6c4 \uc131\ub2a5 \ubcc0\ud654\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50\ud558\uc5ec \ubaa8\ub378\uc758 \uacac\uace0\uc131\uacfc \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "5.3 DNA \ucd5c\uc801\ud654\uc758 \uc77c\ubc18\ud654 \uac00\ub2a5\uc131"}]