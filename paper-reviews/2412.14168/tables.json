[{"content": "| Method | CLIP-I\u2191 | DINO\u2191 | CLIP-T\u2191 |\n|---|---|---|---|\n| **Ours** | **77.60** | **40.11** | **27.71** |\n| Emu2 | 69.70 | 35.96 | 20.54 |\n| Collage Diffusion | 67.80 | 34.16 | 22.14 |\n| AnyDoor+ControlNet | 72.40 | 37.94 | 27.00 |\n| Paint-by-example+ControlNet | 64.50 | 34.60 | 23.77 |", "caption": "Table 1: Comparison with multi-object reference generation methods. The first three rows represent one pass multi-reference customization methods and the last two rows represent two stage inpainting pipeline based on pre-generated base images.", "description": "\ud45c 1\uc740 \ub2e4\uc911 \uac1d\uccb4 \ucc38\uc870 \uc0dd\uc131 \ubc29\ubc95\ub4e4\uacfc\uc758 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc0c1\uc704 \uc138 \ud589\uc740 1\ud68c \ud1b5\uacfc \ub2e4\uc911 \ucc38\uc870 \uc0ac\uc6a9\uc790 \uc9c0\uc815 \ubc29\ubc95\ub4e4\uc744 \ub098\ud0c0\ub0b4\uba70, \ud558\uc704 \ub450 \ud589\uc740 \ubbf8\ub9ac \uc0dd\uc131\ub41c \uae30\ubcf8 \uc774\ubbf8\uc9c0\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 2\ub2e8\uacc4 \ud398\uc778\ud305 \ud30c\uc774\ud504\ub77c\uc778\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uc5ec\ub7ec \uac1c\uc758 \uc758\ub958 \uc544\uc774\ud15c\uc744 \ub3d9\uc2dc\uc5d0 \uc0ac\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\ub294 \ub2e4\uc591\ud55c \uae30\ubc95\ub4e4\uc758 \uc131\ub2a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788, \ud55c \ubc88\uc758 \ucc98\ub9ac \uacfc\uc815\uc73c\ub85c \ub2e4\uc911 \ucc38\uc870\ub97c \ucc98\ub9ac\ud558\ub294 \ubc29\ubc95\uacfc, \ubbf8\ub9ac \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\ub97c \uae30\ubc18\uc73c\ub85c \ub450 \ub2e8\uacc4\uc758 \ucc98\ub9ac \uacfc\uc815\uc744 \uac70\uce58\ub294 \ubc29\ubc95\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  CLIP-I, DINO, CLIP-T \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0 \uc720\uc0ac\ub3c4\uc640 \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uc77c\uad00\uc131\uc744 \ud3c9\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Methods | VITON-HD Paired SSIM \u2191 | VITON-HD Paired FID \u2193 | VITON-HD Paired KID \u2193 | VITON-HD Paired LPIPS \u2193 | Unpaired FID \u2193 | Unpaired KID \u2193 |\n|---|---|---|---|---|---|---|\n| DCI-VTON [10] | 0.8620 | 9.408 | 4.547 | 0.0606 | 12.531 | 5.251 |\n| StableVITON [15] | 0.8543 | 6.439 | 0.942 | 0.0905 | 11.054 | 3.914 |\n| StableGarment [30] | 0.8029 | 15.567 | 8.519 | 0.1042 | 17.115 | 8.851 |\n| MV-VTON [29] | 0.8083 | 15.442 | 7.501 | 0.1171 | 17.900 | 8.861 |\n| GP-VTON [32] | 0.8701 | 8.726 | 3.944 | **0.0585** | 11.844 | 4.310 |\n| LaDI-VTON [21] | 0.8603 | 11.386 | 7.248 | 0.0733 | 14.648 | 8.754 |\n| OOTDiffusion [33] | 0.8187 | 9.305 | 4.086 | 0.0876 | 12.408 | 4.689 |\n| **Ours** | **0.8771** | **5.842** | **0.906** | 0.0727 | **9.205** | **1.3606** |", "caption": "Table 2: Quantitative comparison for the standard virtual try-on task on the VITON-HD test dataset.", "description": "\ud45c 2\ub294 VITON-HD \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud55c \ud45c\uc900 \uac00\uc0c1 \ud53c\ud305 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\ub7c9\uc801 \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ucd5c\ucca8\ub2e8 \uac00\uc0c1 \ud53c\ud305 \ubc29\ubc95\ub4e4(DCI-VTON, StableVITON, StableGarment, MV-VTON, GP-VTON, LaDI-VTON, OOTDiffusion)\uacfc \uc81c\uc548\ub41c \ubc29\ubc95(Ours)\uc744 \ube44\uad50\ud558\uc5ec SSIM, FID, KID, LPIPS \uc9c0\ud45c\ub97c \ud1b5\ud574 \uc131\ub2a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  \uc9dd\uc9c0\uc5b4\uc9c4(Paired) \ubc0f \uc9dd\uc9c0\uc5b4\uc9c0\uc9c0 \uc54a\uc740(Unpaired) \uc124\uc815 \ubaa8\ub450\uc5d0\uc11c \uacb0\uacfc\ub97c \uc81c\uc2dc\ud558\uc5ec \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \uc131\ub2a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \uc815\ub7c9\uc801 \uc131\ub2a5\uc744 \uba85\ud655\ud788 \ubcf4\uc5ec\uc8fc\uace0 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \uc6b0\uc218\uc131\uc744 \uc785\uc99d\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "4.3. \uac00\uc0c1 \ud53c\ud305 \ube44\uad50"}, {"content": "| Method | CLIP-I \u2191 | DINO \u2191 | CLIP-T \u2191 |\n|---|---|---|---|\n| DINOv2 Embeddings | 76.80 | 38.22 | 26.17 |\n| ControlNet | 75.94 | 33.47 | 27.10 |\n| Reference UNet | **77.30** | **39.39** | **27.74** |", "caption": "Table 3: Quantitative study for the reference UNet. We compare with other options for the appearance encoders like DINOv2 and ControlNet. Reference UNet shows the best performance.", "description": "\uc774 \ud45c\ub294 FashionComposer \ubaa8\ub378\uc5d0\uc11c \uc0ac\uc6a9\ub41c appearance encoder\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc138 \uac00\uc9c0 \ub2e4\ub978 appearance encoder (Reference UNet, DINOv2, ControlNet)\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud5d8\uc744 \uc9c4\ud589\ud588\uc73c\uba70, CLIP-I, DINO, CLIP-T \uc138 \uac00\uc9c0 \uc9c0\ud45c\ub97c \ud1b5\ud574 \uc131\ub2a5\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. \uadf8 \uacb0\uacfc Reference UNet\uc774 \ub2e4\ub978 \ub450 \uac00\uc9c0 \ubc29\ubc95\ubcf4\ub2e4 \ubaa8\ub4e0 \uc9c0\ud45c\uc5d0\uc11c \uac00\uc7a5 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ud655\uc778\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub294 Reference UNet\uc774 \uc758\ub958\uc758 \uc138\ubd80\uc801\uc778 \ud2b9\uc9d5\uc744 \ub354 \uc798 \uc720\uc9c0\ud558\uba74\uc11c \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc758 \uc815\ud655\ub3c4\uc640 \uc77c\uad00\uc131\uc744 \ub192\uc774\ub294 \ub370 \ud6a8\uacfc\uc801\uc784\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Method | CLIP-I \u2191 | DINO \u2191 | CLIP-T \u2191 | Quality \u2191 | Fidelity \u2191 |\n|---|---|---|---|---|---| \n| w/o Binding | 77.30 | 39.39 | 27.74 | 84 | 74 |\n| Conv-in | 77.60 | 39.39 | 27.86 | 90 | 122 |\n| Bind(1) | 77.20 | 39.42 | **28.10** | **169** | 95 |\n| Bind(1,2,3) | **77.60** | **40.11** | 27.71 | 140 | **192** |", "caption": "Table 4: Quantitative study for subject-binding attention. Bind(1) means only augmenting the self-attention modules of the UNet down and up blocks with the smallest resolution. Conv-in refers to injecting the text embeddings through the Convolution-in layer of the reference UNet.", "description": "\ud45c 4\ub294 \uc81c\uc548\ub41c \uc8fc\uc81c \uc5f0\uacb0 \uc5b4\ud150\uc158\uc758 \uc815\ub7c9\uc801 \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \"Bind(1)\"\uc740 \uac00\uc7a5 \uc791\uc740 \ud574\uc0c1\ub3c4\uc758 UNet \uc0c1\ud558 \ube14\ub85d\uc758 \uc790\uae30 \uc5b4\ud150\uc158 \ubaa8\ub4c8\ub9cc \uc99d\uac15\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud558\uace0, \"Conv-in\"\uc740 \ucc38\uc870 UNet\uc758 \ud569\uc131\uacf1 \uc785\ub825 \uacc4\uce35\uc744 \ud1b5\ud574 \ud14d\uc2a4\ud2b8 \uc784\ubca0\ub529\uc744 \uc8fc\uc785\ud558\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc124\uc815(\uc8fc\uc81c \uc5f0\uacb0 \uc5b4\ud150\uc158 \uc801\uc6a9 \uc5ec\ubd80, \ud14d\uc2a4\ud2b8 \uc784\ubca0\ub529 \uc8fc\uc785 \ubc29\uc2dd \ub4f1)\uc5d0 \ub530\ub978 CLIP-I, DINO, CLIP-T, \ud488\uc9c8, \ucda9\uc2e4\ub3c4 \uc810\uc218\ub97c \ube44\uad50\ud558\uc5ec \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \ud6a8\uacfc\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \uacb0\uacfc(Experiments)"}]