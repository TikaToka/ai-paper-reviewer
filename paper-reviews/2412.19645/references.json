{"references": [{"fullname_first_author": "Aditya Ramesh", "paper_title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models", "publication_date": "2022-08-01", "reason": "This paper is foundational for text-to-image diffusion models, which are crucial for the video generation techniques explored in the current paper."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-01", "reason": "This paper introduces high-resolution image synthesis using latent diffusion models, directly impacting the quality and resolution of video generation methods."}, {"fullname_first_author": "Nataniel Ruiz", "paper_title": "DreamBooth: Fine-tuning text-to-image diffusion models for subject-driven generation", "publication_date": "2023-06-01", "reason": "This work is highly relevant due to its focus on subject-driven generation, a core concept in zero-shot customized video generation."}, {"fullname_first_author": "Yuchao Gu", "paper_title": "Mix-of-Show: Decentralized low-rank adaptation for multi-concept customization of diffusion models", "publication_date": "2024-01-01", "reason": "This paper addresses multi-concept customization in diffusion models, which is directly relevant to the problem of customizing video generation."}, {"fullname_first_author": "Yuwei Guo", "paper_title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning", "publication_date": "2023-07-01", "reason": "This paper introduces AnimateDiff, a crucial base model for the VideoMaker method, which is used to generate videos with high fidelity and quality."}]}