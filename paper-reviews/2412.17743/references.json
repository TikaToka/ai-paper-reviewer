{"references": [{"fullname_first_author": "Kosuke Nishida", "paper_title": "Initialization of large language models via reparameterization to mitigate loss spikes", "publication_date": "2024-11-12", "reason": "This paper introduces a re-parameterization method to stabilize the training process, a key technique used in YuLan-Mini."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper introduces LLaMA, a foundational model architecture that YuLan-Mini builds upon."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The LLaMA 3 herd of models", "publication_date": "2024-07-21", "reason": "This paper details LLaMA 3, a significant model that YuLan-Mini is compared against, providing context for YuLan-Mini's performance."}, {"fullname_first_author": "Mitchell Wortsman", "paper_title": "Small-scale proxies for large-scale transformer training instabilities", "publication_date": "2024-05-07", "reason": "This paper investigates training instabilities in LLMs, a key challenge addressed by YuLan-Mini's training techniques."}, {"fullname_first_author": "Shengding Hu", "paper_title": "MiniCPM: Unveiling the potential of small language models with scalable training strategies", "publication_date": "2024-04-07", "reason": "This paper introduces MiniCPM, another small-scale LLM, enabling comparison of model performance and training strategies."}]}