{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-04", "reason": "This paper introduced the Transformer architecture, which is the foundation of many modern language models, including ModernBERT."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2019-06-02", "reason": "BERT is a seminal work in the field, introducing a highly influential masked language modeling pre-training technique that forms the basis of many subsequent encoder-only models."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-05-01", "reason": "This paper demonstrated that large language models can perform well on many downstream tasks without explicit supervision, influencing the approach to pre-training in ModernBERT."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "publication_date": "2020-07-01", "reason": "This paper introduced the T5 model, a unified text-to-text framework, which influenced ModernBERT's architecture and the training objective."}, {"fullname_first_author": "Vladimir Karpukhin", "paper_title": "Dense Passage Retrieval for Open-Domain Question Answering", "publication_date": "2020-11-16", "reason": "This paper introduced Dense Passage Retrieval (DPR), a method for efficient and effective semantic search that significantly advanced Information Retrieval and has influenced approaches used in ModernBERT."}]}