{"references": [{"fullname_first_author": "Ren\u00e9 Ranftl", "paper_title": "Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer", "publication_date": "2020-00-00", "reason": "This paper is highly relevant due to its focus on robust monocular depth estimation and zero-shot cross-dataset transfer, which are directly related to the core task and challenges addressed in DepthLab."}, {"fullname_first_author": "Angela Dai", "paper_title": "ScanNet: Richly-annotated 3d reconstructions of indoor scenes", "publication_date": "2017-00-00", "reason": "ScanNet is a crucial dataset for training and evaluating 3D scene understanding models, directly impacting the performance and generalizability of DepthLab."}, {"fullname_first_author": "Bingxin Ke", "paper_title": "Repurposing diffusion-based image generators for monocular depth estimation", "publication_date": "2024-00-00", "reason": "This paper introduces a foundational model for depth inpainting leveraging the power of diffusion models, which DepthLab builds upon and extends."}, {"fullname_first_author": "Johannes L. Sch\u00f6nberger", "paper_title": "A multi-view stereo benchmark with high-resolution images and multi-camera videos", "publication_date": "2017-00-00", "reason": "This paper provides a benchmark dataset and evaluation metrics for multi-view stereo, which is relevant for evaluating the performance of DepthLab's downstream applications, particularly in sparse view reconstruction."}, {"fullname_first_author": "Shuzhe Wang", "paper_title": "DUST3R: Geometric 3d vision made easy", "publication_date": "2024-00-00", "reason": "DUST3R is a key component for sparse-view 3D reconstruction, and this paper is highly relevant because DepthLab enhances its capabilities, which is a significant contribution."}]}