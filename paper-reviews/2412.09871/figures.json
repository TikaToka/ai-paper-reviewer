[{"figure_path": "https://arxiv.org/html/2412.09871/x3.png", "caption": "Figure 1: \nScaling trends for fixed inference flop models (fully) trained with varying training budgets.\nIn token-based models, a fixed inference budget determines the model size.\nIn contrast, the BLT architecture provides a new scaling axis allowing simultaneous increases in model and patch size while keeping the same training and inference budget.\nBLT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama\u00a02 and 3. Moving to the larger inference budget makes the larger patch size 8 model more desirable sooner. Both BPE compute-optimal point and crossover point are indicated with vertical lines.", "description": "\uace0\uc815 \ucd94\ub860 FLOP \ubaa8\ub378\uc5d0 \ub300\ud55c \ud559\uc2b5 \uc608\uc0b0 \ub300\ube44 \uc131\ub2a5 \uc2a4\ucf00\uc77c\ub9c1 \ucd94\uc138\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. \ud1a0\ud070 \uae30\ubc18 \ubaa8\ub378\uc5d0\uc11c\ub294 \uace0\uc815 \ucd94\ub860 \uc608\uc0b0\uc5d0 \ub530\ub77c \ubaa8\ub378 \ud06c\uae30\uac00 \uacb0\uc815\ub429\ub2c8\ub2e4. \ubc18\uba74 BLT \uc544\ud0a4\ud14d\ucc98\ub294 \ud559\uc2b5 \ubc0f \ucd94\ub860 \uc608\uc0b0\uc744 \ub3d9\uc77c\ud558\uac8c \uc720\uc9c0\ud558\uba74\uc11c \ubaa8\ub378 \ubc0f \ud328\uce58 \ud06c\uae30\ub97c \ub3d9\uc2dc\uc5d0 \ub298\ub9b4 \uc218 \uc788\ub294 \uc0c8\ub85c\uc6b4 \uc2a4\ucf00\uc77c\ub9c1 \ucd95\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. BLT \ud328\uce58 \ud06c\uae30(ps) 6 \ubc0f 8 \ubaa8\ub378\uc740 BPE Llama 2 \ubc0f 3\uc758 \uc2a4\ucf00\uc77c\ub9c1 \ucd94\uc138\ub97c \ube60\ub974\uac8c \ub530\ub77c\uc7a1\uc2b5\ub2c8\ub2e4. \ub354 \ud070 \ucd94\ub860 \uc608\uc0b0\uc73c\ub85c \uc804\ud658\ud558\uba74 \ub354 \ud070 \ud328\uce58 \ud06c\uae30 8 \ubaa8\ub378\uc774 \ub354 \ube68\ub9ac \ubc14\ub78c\uc9c1\ud574\uc9d1\ub2c8\ub2e4. BPE\uc758 \uacc4\uc0b0 \ucd5c\uc801 \uc9c0\uc810\uacfc \uad50\ucc28 \uc9c0\uc810\uc774 \uc138\ub85c\uc120\uc73c\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. \uc2a4\ucf00\uc77c\ub9c1 \ucd94\uc138"}, {"figure_path": "https://arxiv.org/html/2412.09871/extracted/6066458/assets/patching_types.png", "caption": "Figure 2: \nBLT comprises three modules, a lightweight Local Encoder that encodes input bytes into patch representations, a computationally expensive Latent Transformer over patch representations, and a lightweight Local Decoder to decode the next patch of bytes. BLT incorporates byte n\ud835\udc5bnitalic_n-gram embeddings and a cross-attention mechanism to maximize information flow between the Latent Transformer and the byte-level modules\u00a0(Figure\u00a05). Unlike fixed-vocabulary tokenization, BLT dynamically groups bytes into patches preserving access to the byte-level information.", "description": "BLT\ub294 \uc785\ub825 \ubc14\uc774\ud2b8\ub97c \ud328\uce58 \ud45c\ud604\uc73c\ub85c \uc778\ucf54\ub529\ud558\ub294 \uacbd\ub7c9 \ub85c\uceec \uc778\ucf54\ub354, \ud328\uce58 \ud45c\ud604\uc5d0 \ub300\ud55c \uacc4\uc0b0\ub7c9\uc774 \ub9ce\uc740 \uc7a0\uc7ac \ubcc0\ud658\uae30, \ub2e4\uc74c \ubc14\uc774\ud2b8 \ud328\uce58\ub97c \ub514\ucf54\ub529\ud558\ub294 \uacbd\ub7c9 \ub85c\uceec \ub514\ucf54\ub354\uc758 \uc138 \uac00\uc9c0 \ubaa8\ub4c8\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. BLT\ub294 \ubc14\uc774\ud2b8 n-\uadf8\ub7a8 \uc784\ubca0\ub529\uacfc \uad50\ucc28 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \ud1b5\ud569\ud558\uc5ec \uc7a0\uc7ac \ubcc0\ud658\uae30\uc640 \ubc14\uc774\ud2b8 \ub808\ubca8 \ubaa8\ub4c8 \uac04\uc758 \uc815\ubcf4 \ud750\ub984\uc744 \uadf9\ub300\ud654\ud569\ub2c8\ub2e4(\uadf8\ub9bc 5). \uace0\uc815 \uc5b4\ud718 \ud1a0\ud070\ud654\uc640 \ub2ec\ub9ac BLT\ub294 \ubc14\uc774\ud2b8 \ub808\ubca8 \uc815\ubcf4\uc5d0 \ub300\ud55c \uc561\uc138\uc2a4\ub97c \uc720\uc9c0\ud558\uba74\uc11c \ubc14\uc774\ud2b8\ub97c \ud328\uce58\ub85c \ub3d9\uc801\uc73c\ub85c \uadf8\ub8f9\ud654\ud569\ub2c8\ub2e4.", "section": "3 BLT \uc544\ud0a4\ud14d\ucc98"}, {"figure_path": "https://arxiv.org/html/2412.09871/x4.png", "caption": "Figure 3: \nPatching schemes group bytes in different ways, each leading to a different number of resulting patches.\nSince each patch is processed using a large transformer step, the number of patches directly determines the bulk of the compute expended in terms of flops.\nThese schemes group bytes into patches by (a) striding every four bytes\u00a0(\u00a72.1) as in MegaByte\u00a0(Yu et\u00a0al., 2023), (b) tokenizing with Byte-Pair Encoding (bpe), in this case the Llama-3\u00a0(Dubey et\u00a0al., 2024) tokenizer, (c & d) entropy-based patching as in this work\u00a0(\u00a72.3), (e) patching on space-bytes\u00a0(Slagle, 2024), (f) and patching on entropy using a small CNN byte-level model with 2-byte context.", "description": "\uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ud328\uce6d \ubc29\uc2dd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubc29\uc2dd\uc740 \ubc14\uc774\ud2b8\ub97c \ud328\uce58\ub85c \uadf8\ub8f9\ud654\ud558\ub294 \ubc29\uc2dd\uc774 \ub2e4\ub974\uba70, \uacb0\uacfc\uc801\uc73c\ub85c \ud328\uce58 \uc218\uac00 \ub2ec\ub77c\uc9d1\ub2c8\ub2e4. \uac01 \ud328\uce58\ub294 \ud070 \ubcc0\ud658\uae30 \ub2e8\uacc4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucc98\ub9ac\ub418\ubbc0\ub85c \ud328\uce58 \uc218\ub294 FLOPS \uce21\uba74\uc5d0\uc11c \uc18c\ube44\ub418\ub294 \uacc4\uc0b0\ub7c9\uc758 \ub300\ubd80\ubd84\uc744 \uc9c1\uc811\uc801\uc73c\ub85c \uacb0\uc815\ud569\ub2c8\ub2e4. \ud328\uce6d \ubc29\uc2dd\uc5d0\ub294 (a) MegaByte(Yu et al., 2023)\uc5d0\uc11c\ucc98\ub7fc 4\ubc14\uc774\ud2b8\ub9c8\ub2e4 \uc2a4\ud2b8\ub77c\uc774\ub4dc\ud558\ub294 \ubc29\uc2dd(\u00a72.1), (b) \ubc14\uc774\ud2b8 \ud398\uc5b4 \uc778\ucf54\ub529(BPE)\uc73c\ub85c \ud1a0\ud070\ud654\ud558\ub294 \ubc29\uc2dd(\uc774 \uacbd\uc6b0 Llama-3(Dubey et al., 2024) \ud1a0\ud06c\ub098\uc774\uc800 \uc0ac\uc6a9), (c \ubc0f d) \uc774 \uc5f0\uad6c\uc5d0\uc11c\ucc98\ub7fc \uc5d4\ud2b8\ub85c\ud53c \uae30\ubc18 \ud328\uce6d \ubc29\uc2dd(\u00a72.3), (e) \uacf5\ubc31 \ubc14\uc774\ud2b8\uc5d0\uc11c \ud328\uce6d\ud558\ub294 \ubc29\uc2dd(Slagle, 2024), (f) 2\ubc14\uc774\ud2b8 \ucee8\ud14d\uc2a4\ud2b8\ub97c \uac00\uc9c4 \uc791\uc740 CNN \ubc14\uc774\ud2b8 \ub808\ubca8 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5d4\ud2b8\ub85c\ud53c\uc5d0\uc11c \ud328\uce6d\ud558\ub294 \ubc29\uc2dd\uc774 \uc788\uc2b5\ub2c8\ub2e4.", "section": "Patching: From Individual Bytes to Groups of Bytes"}, {"figure_path": "https://arxiv.org/html/2412.09871/x5.png", "caption": "Figure 4: \nThis figure plots the entropy H\u2062(xi)\ud835\udc3bsubscript\ud835\udc65\ud835\udc56H(x_{i})italic_H ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) of each byte in \u201cDaenerys Targeryen is in Game of Thrones, a fantasy epic by George R.R. Martin.\u201d with spaces shown as underscores.\nPatches end when H\u2062(xi)\ud835\udc3bsubscript\ud835\udc65\ud835\udc56H(x_{i})italic_H ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) exceeds the global threshold \u03b8gsubscript\ud835\udf03\ud835\udc54\\theta_{g}italic_\u03b8 start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, shown as a red horizontal line.\nThe start of new patches are shown with vertical gray lines.\nFor example, the entropies of \u201cG\u201d and \u201ce\u201d in \u201cGeorge R.R. Martin\u201d exceed \u03b8gsubscript\ud835\udf03\ud835\udc54\\theta_{g}italic_\u03b8 start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, so \u201cG\u201d is the start of a single byte patch and \u201ce\u201d of a larger patch extending to the end of the named entity as the entropy H\u2062(xi)\ud835\udc3bsubscript\ud835\udc65\ud835\udc56H(x_{i})italic_H ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) stays low, resulting in no additional patches.", "description": "\uc774 \uadf8\ub9bc\uc740 \ubb38\uc790\uc5f4 \"Daenerys Targeryen is in Game of Thrones, a fantasy epic by George R.R. Martin.\"\uc758 \uac01 \ubc14\uc774\ud2b8\uc5d0 \ub300\ud55c \uc5d4\ud2b8\ub85c\ud53c \uac12\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub744\uc5b4\uc4f0\uae30\ub294 \ubc11\uc904\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ube68\uac04\uc0c9 \uc218\ud3c9\uc120\uc73c\ub85c \ud45c\uc2dc\ub41c \uc804\uc5ed \uc784\uacc4\uac12 \u03b8g\ub97c \ucd08\uacfc\ud558\uba74 \uc0c8 \ud328\uce58\uac00 \uc2dc\uc791\ub429\ub2c8\ub2e4. \uc0c8 \ud328\uce58\uc758 \uc2dc\uc791\uc740 \ud68c\uc0c9 \uc138\ub85c\uc120\uc73c\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \"George R.R. Martin\"\uc5d0\uc11c \"G\"\uc640 \"e\"\uc758 \uc5d4\ud2b8\ub85c\ud53c\ub294 \u03b8g\ub97c \ucd08\uacfc\ud558\ubbc0\ub85c \"G\"\ub294 \ub2e8\uc77c \ubc14\uc774\ud2b8 \ud328\uce58\uc758 \uc2dc\uc791\uc774\uace0 \"e\"\ub294 \ub354 \ud070 \ud328\uce58\uc758 \uc2dc\uc791\uc785\ub2c8\ub2e4. \uc774\ud6c4 \uc5d4\ud2b8\ub85c\ud53c \uac12\uc774 \ub0ae\uac8c \uc720\uc9c0\ub418\ubbc0\ub85c \ucd94\uac00 \ud328\uce58\uac00 \uc0dd\uc131\ub418\uc9c0 \uc54a\uace0, \"e\"\ub85c \uc2dc\uc791\ud558\ub294 \ud328\uce58\ub294 \uc774\ub984\uc788\ub294 \uac1c\uccb4\uc758 \ub05d\uae4c\uc9c0 \ud655\uc7a5\ub429\ub2c8\ub2e4.", "section": "Patching: From Individual Bytes to Groups of Bytes"}, {"figure_path": "https://arxiv.org/html/2412.09871/x6.png", "caption": "Figure 5: The local encoder uses a cross-attention block with patch representations as queries, and byte representations as keys/values to encode byte representations into patch representations. The local decoder uses a similar block but with the roles reversed i.e. byte representations are now the queries and patch representations are the keys/values. Here we use Cross-Attn k=2\ud835\udc582k=2italic_k = 2.", "description": "\uc774 \uadf8\ub9bc\uc740 BLT \uc544\ud0a4\ud14d\ucc98\uc758 \ub85c\uceec \uc778\ucf54\ub354\uc640 \ub85c\uceec \ub514\ucf54\ub354\uac00 \uc5b4\ub5bb\uac8c cross-attention \ube14\ub85d\uc744 \uc0ac\uc6a9\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub85c\uceec \uc778\ucf54\ub354\ub294 \ud328\uce58 \ud45c\ud604\uc744 \ucffc\ub9ac\ub85c, \ubc14\uc774\ud2b8 \ud45c\ud604\uc744 \ud0a4/\uac12\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \ubc14\uc774\ud2b8 \ud45c\ud604\uc744 \ud328\uce58 \ud45c\ud604\uc73c\ub85c \uc778\ucf54\ub529\ud569\ub2c8\ub2e4. \ub85c\uceec \ub514\ucf54\ub354\ub294 \ubc14\uc774\ud2b8 \ud45c\ud604\uc744 \ucffc\ub9ac\ub85c, \ud328\uce58 \ud45c\ud604\uc744 \ud0a4/\uac12\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec cross-attention \ube14\ub85d\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc11c Cross-Attn k=2\ub294 cross-attention \ube14\ub85d\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \ub9e4\uac1c\ubcc0\uc218 k\uac00 2\ub77c\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uc989, \uac01 \ud328\uce58\ub294 \uc774\uc804 \ub808\uc774\uc5b4\uc5d0\uc11c \ud574\ub2f9 \ud328\uce58\uc758 \ubc14\uc774\ud2b8 \ud45c\ud604\uc758 2\ubc30\uc5d0 \ud574\ub2f9\ud558\ub294 \ud0a4\uc640 \uac12\uc5d0 \uc8fc\uc758\ub97c \uae30\uc6b8\uc785\ub2c8\ub2e4.", "section": "3. BLT \uc544\ud0a4\ud14d\ucc98"}, {"figure_path": "https://arxiv.org/html/2412.09871/x7.png", "caption": "Figure 6: Scaling trends for BLT models with different architectural choices, as well as for baseline BPE token-based models. We train models at multiple scales from 1B up to 8B parameters for the optimal number of tokens as computed by\u00a0Dubey et\u00a0al. (2024) and report bits-per-byte on a sample from the training distribution. BLT models perform on par with state-of-the-art tokenizer-based models such as Llama 3, at scale. PS denotes patch size. We illustrate separate architecture improvements on space-patching (left) and combine them with dynamic patching (right).", "description": "\uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \uc544\ud0a4\ud14d\ucc98 \uc120\ud0dd\uc9c0\ub97c \uc0ac\uc6a9\ud55c BLT \ubaa8\ub378\uacfc \uae30\uc900 BPE \ud1a0\ud070 \uae30\ubc18 \ubaa8\ub378\uc758 \uc2a4\ucf00\uc77c\ub9c1 \ucd94\uc138\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubaa8\ub378\ub4e4\uc740 Dubey et al. (2024)\uc5d0\uc11c \uacc4\uc0b0\ub41c \ucd5c\uc801 \ud1a0\ud070 \uc218\uc5d0 \ub530\ub77c 10\uc5b5\uc5d0\uc11c 80\uc5b5 \uac1c\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \uaddc\ubaa8\ub85c \ud559\uc2b5\ub418\uc5c8\uc73c\uba70, \ud559\uc2b5 \ubd84\ud3ec\uc5d0\uc11c \ucd94\ucd9c\ud55c \uc0d8\ud50c\uc5d0 \ub300\ud55c \ube44\ud2b8/\ubc14\uc774\ud2b8\ub97c \ubcf4\uace0\ud569\ub2c8\ub2e4. BLT \ubaa8\ub378\uc740 Llama 3\uc640 \uac19\uc740 \ucd5c\ucca8\ub2e8 \ud1a0\ud06c\ub098\uc774\uc800 \uae30\ubc18 \ubaa8\ub378\uacfc \ub3d9\ub4f1\ud55c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. PS\ub294 \ud328\uce58 \ud06c\uae30\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uacf5\uac04 \ud328\uce6d(\uc67c\ucabd)\uc5d0 \ub300\ud55c \uc544\ud0a4\ud14d\ucc98 \uac1c\uc120 \uc0ac\ud56d\uc744 \ubcc4\ub3c4\ub85c \ubcf4\uc5ec\uc8fc\uace0 \ub3d9\uc801 \ud328\uce6d(\uc624\ub978\ucabd)\uacfc \uacb0\ud569\ud569\ub2c8\ub2e4.", "section": "5. \uc2a4\ucf00\uc77c\ub9c1 \ucd94\uc138"}, {"figure_path": "https://arxiv.org/html/2412.09871/x8.png", "caption": "Figure 7: Output responses from Llama 3 and BLT models for various tasks from CUTE benchmark. BLT model performs better on sequence manipulation tasks compared to the tokenizer-based Llama 3 model. Note that few-shot examples are not shown in the above prompts to maintain clarity.", "description": "\uc774 \uadf8\ub9bc\uc740 CUTE \ubca4\uce58\ub9c8\ud06c\uc758 \ub2e4\uc591\ud55c \uc791\uc5c5\uc5d0 \ub300\ud55c Llama 3 \ubc0f BLT \ubaa8\ub378\uc758 \ucd9c\ub825 \uc751\ub2f5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. BLT \ubaa8\ub378\uc740 \ud1a0\ud06c\ub098\uc774\uc800 \uae30\ubc18 Llama 3 \ubaa8\ub378\uc5d0 \ube44\ud574 \uc2dc\ud000\uc2a4 \uc870\uc791 \uc791\uc5c5\uc5d0\uc11c \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. \uba85\ud655\uc131\uc744 \uc704\ud574 \uc704\uc758 \ud504\ub86c\ud504\ud2b8\uc5d0\ub294 few-shot \uc608\uc81c\uac00 \ud45c\uc2dc\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.", "section": "6.1 Character-Level Tasks"}, {"figure_path": "https://arxiv.org/html/2412.09871/extracted/6066458/assets/patching.png", "caption": "Figure 8: Variation of language modeling performance in bits-per-byte (bpb) with training flops for 400m and 1b BLT models patched with entropy models of different sizes and context windows. Both dimensions improve scaling performance, with diminishing returns beyond 50m parameter entropy models with a context of 512 bytes.", "description": "\uc774 \uadf8\ub9bc\uc740 400m \ubc0f 1b BLT \ubaa8\ub378\uc5d0 \ub300\ud574 \ud559\uc2b5 FLOPS \ub300\ube44 bits-per-byte(bpb) \uc5b8\uc5b4 \ubaa8\ub378\ub9c1 \uc131\ub2a5\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\uba70, \ub2e4\uc591\ud55c \ud06c\uae30\uc640 \ucee8\ud14d\uc2a4\ud2b8 \ucc3d\uc758 \uc5d4\ud2b8\ub85c\ud53c \ubaa8\ub378\ub85c \ud328\uce58\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ub450 \ucc28\uc6d0 \ubaa8\ub450 \uc2a4\ucf00\uc77c\ub9c1 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uace0, \ucee8\ud14d\uc2a4\ud2b8 \ucc3d\uc774 512\ubc14\uc774\ud2b8\uc778 50m \ub9e4\uac1c\ubcc0\uc218 \uc5d4\ud2b8\ub85c\ud53c \ubaa8\ub378\uc744 \ub118\uc5b4\uc11c\uba74 \uac10\uc18c\ud558\ub294 \uc218\uc775\ub960\uc744 \ubcf4\uc785\ub2c8\ub2e4.", "section": "5. \uc2a4\ucf00\uc77c\ub9c1 \ud2b8\ub80c\ub4dc"}]