[{"content": "|                       | Llama 3 (1T Tokens) | BLT-Space (6T Bytes) | BLT-Entropy (4.5T Bytes) |\n| :-------------------- | :------------------: | :-----------------: | :----------------------: |\n| **Arc-E**             |        77.6         |       75.4        |          **79.6**         |\n| **Arc-C**             |        **53.3**         |       49.8        |          52.1         |\n| **HellaSwag**         |        79.1         |       79.6        |          **80.6**         |\n| **PIQA**              |        80.7         |       **81.1**        |          80.6         |\n| **MMLU**              |        **58.1**         |       54.8        |          57.4         |\n| **MBPP**              |        40.2         |       37.6        |          **41.8**         |\n| **HumanEval**         |        31.1         |       27.4        |          **35.4**         |\n| **Average**           |        60.0         |       58.0        |          **61.1**         |\n| **Bytes/Patch on Train Mix** |        4.4         |       **6.1**        |          4.5         |", "caption": "Table 1: Comparison of flop-matched BLT 8B models trained on the BLT-1T dataset comprising high-quality tokens of text and code from publicly available sources, with baseline models using the Llama 3 tokenizer. BLT performs better than Llama 3 on average, and depending on the patching scheme, achieves significant flops savings with a minor reduction in performance.", "description": "BLT-1T \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ud559\uc2b5\ub41c 80\uc5b5 \ud30c\ub77c\ubbf8\ud130 BLT \ubaa8\ub378\uacfc Llama 3 \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc0ac\uc6a9\ud55c \uae30\uc900 \ubaa8\ub378\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. BLT \ubaa8\ub378\uc740 \ud3c9\uade0\uc801\uc73c\ub85c Llama 3\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub6f0\uc5b4\ub098\uba70, \ud328\uce6d \ubc29\uc2dd\uc5d0 \ub530\ub77c \uc131\ub2a5 \uc800\ud558\ub97c \ucd5c\uc18c\ud654\ud558\uba74\uc11c FLOPS\ub97c \ud06c\uac8c \uc808\uac10\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.2 Beyond Compute Optimal Task Evaluations"}, {"content": "| Llama 3 |\n| -------- |\n| 1T Tokens |", "caption": "Table 2: Details of models used in the fixed-inference scaling study. We report non-embedding parameters for each model and their relative number compared to Llama 2. We pick model sizes with equal inference flops per byte. We also indicate BPE\u2019s compute-optimal training data quantity and the crossover point where BLT surpasses BPE as seen in Figure\u00a01 (both expressed in bytes of training data). This point is achieved at much smaller scales compared to many modern training budgets.", "description": "\uc774 \ud45c\ub294 \uace0\uc815 \ucd94\ub860 FLOP \uc2a4\ucf00\uc77c\ub9c1 \uc5f0\uad6c\uc5d0 \uc0ac\uc6a9\ub41c \ubaa8\ub378\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc784\ubca0\ub529 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc81c\uc678\ud55c \ub9e4\uac1c\ubcc0\uc218\uc640 Llama 2 \ub300\ube44 \uc0c1\ub300\uc801\uc778 \uc218\ub97c \ubcf4\uace0\ud569\ub2c8\ub2e4. \ubc14\uc774\ud2b8\ub2f9 \ucd94\ub860 FLOP\uac00 \ub3d9\uc77c\ud55c \ubaa8\ub378 \ud06c\uae30\ub97c \uc120\ud0dd\ud569\ub2c8\ub2e4. \ub610\ud55c \uadf8\ub9bc 1\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774 BPE\uc758 \uacc4\uc0b0 \ucd5c\uc801 \ud559\uc2b5 \ub370\uc774\ud130 \uc591\uacfc BLT\uac00 BPE\ub97c \ub2a5\uac00\ud558\ub294 \uad50\ucc28\uc810\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4(\ub458 \ub2e4 \ud559\uc2b5 \ub370\uc774\ud130\uc758 \ubc14\uc774\ud2b8\ub85c \ud45c\uc2dc\ub428). \uc774 \uc9c0\uc810\uc740 \ub9ce\uc740 \ucd5c\uc2e0 \ud559\uc2b5 \uc608\uc0b0\uc5d0 \ube44\ud574 \ud6e8\uc52c \uc791\uc740 \uaddc\ubaa8\uc5d0\uc11c \ub2ec\uc131\ub429\ub2c8\ub2e4.", "section": "5. \uc2a4\ucf00\uc77c\ub9c1 \ucd94\uc138"}, {"content": "| BLT-Space |\n| :--------: |\n| 6T Bytes |", "caption": "Table 3: We compare our 8B BLT model to 8B BPE Llama 3 trained on 1T tokens on tasks that assess robustness to noise and awareness of the constituents of language (best result bold). We also report the performance of Llama 3.1 on the same tasks and underline best result overall. BLT outperforms the Llama 3 BPE model by a large margin and even improves over Llama 3.1 in many tasks indicating that the byte-level awareness is not something that can easily be obtained with more data.", "description": "\uc774 \ud45c\ub294 \ub178\uc774\uc988\uc5d0 \ub300\ud55c \uac15\uac74\uc131 \ubc0f \uc5b8\uc5b4 \uad6c\uc131 \uc694\uc18c\uc5d0 \ub300\ud55c \uc778\uc2dd\uc744 \ud3c9\uac00\ud558\ub294 \uc791\uc5c5\uc5d0\uc11c 80\uc5b5 \uac1c \ub9e4\uac1c\ubcc0\uc218 BLT \ubaa8\ub378\uc744 1\uc870 \uac1c\uc758 \ud1a0\ud070\uc73c\ub85c \ud559\uc2b5\ub41c 80\uc5b5 \uac1c \ub9e4\uac1c\ubcc0\uc218 BPE Llama 3 \ubaa8\ub378\uacfc \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c \ub3d9\uc77c\ud55c \uc791\uc5c5\uc5d0 \ub300\ud55c Llama 3.1(16\uc870 \uac1c\uc758 \ud1a0\ud070\uc73c\ub85c \ud559\uc2b5)\uc758 \uc131\ub2a5\ub3c4 \ubcf4\uace0\ud569\ub2c8\ub2e4. BLT\ub294 Llama 3 BPE \ubaa8\ub378\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ud6e8\uc52c \ub6f0\uc5b4\ub098\uba70, \ub9ce\uc740 \uc791\uc5c5\uc5d0\uc11c Llama 3.1\ubcf4\ub2e4 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ubc14\uc774\ud2b8 \uc218\uc900 \uc778\uc2dd\uc774 \ub354 \ub9ce\uc740 \ub370\uc774\ud130\ub9cc\uc73c\ub85c\ub294 \uc27d\uac8c \uc5bb\uc744 \uc218 \uc788\ub294 \uac83\uc774 \uc544\ub2d8\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "6. Byte Modeling Improves Robustness"}, {"content": "| BLT-Entropy |\n| -------- |\n| 4.5T Bytes |", "caption": "Table 4: Performance of 8B BLT and 8B Llama 3 trained for 1T tokens on translating into and from six widely-used languages and twenty one lower resource languages with various scripts from the FLORES-101 benchmark (Goyal et\u00a0al., 2022).", "description": "FLORES-101 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c 6\uac1c\uc758 \uc8fc\uc694 \uc5b8\uc5b4\uc640 21\uac1c\uc758 \uc800\uc790\uc6d0 \uc5b8\uc5b4\uc5d0 \ub300\ud55c \ubc88\uc5ed \uc131\ub2a5(BLEU \uc810\uc218)\uc744 1\uc870 \ud1a0\ud070\uc73c\ub85c \ud559\uc2b5\ub41c 80\uc5b5 \ud30c\ub77c\ubbf8\ud130 BLT \ubaa8\ub378\uacfc Llama 3 \ubaa8\ub378\uc744 \ube44\uad50\ud558\uc5ec \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. BLT \ubaa8\ub378\uc740 \ubc14\uc774\ud2b8 \uc218\uc900 \ubaa8\ub378\ub9c1\uc744 \uc0ac\uc6a9\ud558\uace0 Llama 3\ub294 \ud1a0\ud070 \uae30\ubc18 \ubaa8\ub378\uc785\ub2c8\ub2e4.", "section": "Byte Modeling Improves Robustness"}, {"content": "| Llama 2 | Llama 3 | Entropy ps=6 | Entropy ps=8 | Inference <span class=\"ltx_text ltx_font_smallcaps\">flop</span>s | Compute Optimal (Bytes) | Crossover (Bytes) |\n|---|---|---|---|---|---|---| \n| 470m | 450m | 610m (1.2x) | 760m (1.6x) | 3.1E8 | 50B | 150B |\n| 3.6B | 3.9B | 5.2B (1.3x) | 6.6B (1.7x) | 2.1E9 | 400B | 1T |", "caption": "Table 5: Initializing the global transformer model of BLT from the non-embedding parameters of Llama 3 improves performance on several benchmark tasks. First three models trained on the Llama 2 data for compute-optimal steps.", "description": "\ud45c 5\ub294 BLT \ubaa8\ub378\uc758 \uc804\uc5ed \ubcc0\ud658\uae30 \ub9e4\uac1c\ubcc0\uc218\ub97c Llama 3\uc758 \ube44 \uc784\ubca0\ub529 \ub9e4\uac1c\ubcc0\uc218\ub85c \ucd08\uae30\ud654\ud558\uba74 \uc5ec\ub7ec \ubca4\uce58\ub9c8\ud06c \uc791\uc5c5\uc5d0\uc11c \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. BLT, Llama 3, Llama 3.1 \ubaa8\ub378\uc740 Llama 2 \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac01 \ubaa8\ub378 \ud06c\uae30\uc5d0 \ub300\ud574 \uacc4\uc0b0\uc801\uc73c\ub85c \ucd5c\uc801\uc758 \ub2e8\uacc4 \uc218\ub9cc\ud07c \ud6c8\ub828\ub418\uc5c8\uc2b5\ub2c8\ub2e4. Llama 3.1 \ubaa8\ub378\uc740 15T \ud1a0\ud070\uc73c\ub85c \ud6c8\ub828\ub418\uc5c8\uc73c\uba70, Llama 3\uc640 BLT\ub294 220B \ud1a0\ud070\uc73c\ub85c \ud6c8\ub828\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "5.2. Beyond Compute Optimal Task Evaluations"}, {"content": "|                               | Llama 3 (1T tokens) | Llama 3.1 (16T tokens) | BLT (1T tokens) |\n| :---------------------------- | :------------------: | :-------------------: | :--------------: |\n| **HellaSwag Original**       |        79.1         |         *80.7*        |      **80.6**   |\n| **HellaSwag Noise Avg.**      |        56.9         |         64.3         |     ***64.3***  |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- AntSpeak** |        45.6         |         *61.3*        |      **57.9**   |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Drop**    |        53.8         |         57.3         |     ***58.2***  |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- RandomCase** |        55.3         |         65.0         |     ***65.7***  |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Repeat**   |        57.0         |         61.5         |     ***66.6***  |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- UpperCase** |        72.9         |         76.5         |     ***77.3***  |\n| **Phonology-G2P**            |        11.8         |         *18.9*        |      **13.0**   |\n| **CUTE**                     |        27.5         |         20.0         |     ***54.1***  |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Contains Char** |        0.0          |         0.0          |     ***55.9***  |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Contains Word** |        55.1         |         21.6         |     ***73.5***  |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Del Char**    |        34.6         |         34.3         |     ***35.9***  |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Del Word**    |        **75.5**       |         *84.5*        |      56.1       |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Ins Char**    |        7.5          |         0.0          |      ***7.6***   |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Ins Word**    |        **33.5**       |         *63.3*        |      31.2       |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Orthography**  |        43.1         |         0.0          |     ***52.4***  |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Semantic**   |        65           |         0.0          |     ***90.5***  |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Spelling**    |        1.1          |         -            |     ***99.9***  |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Spelling Inverse** |  30.1 | 3.6 | ***99.9*** |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Substitute Char** | 0.4 | 1.2 | ***48.7*** |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Substitute Word** | 16.4 | 6.8 | ***72.8*** |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Swap Char** | 2.6 | 2.4 | ***11.5*** |\n| &nbsp;&nbsp;&nbsp;&nbsp;**- Swap Word** | 20.1 | 4.1 | ***21*** |", "caption": "Table 6: Benchmark evaluations of two patching schemes for 8b BLT models and BPE Llama3 baseline. These models are trained on the Llama 2 data for the optimal number of steps as determined by Dubey et\u00a0al. (2024).", "description": "\uc774 \ud45c\ub294 \ub450 \uac00\uc9c0 \ud328\uce6d \ubc29\uc2dd(\uc2a4\ud398\uc774\uc2a4 \ud328\uce6d\uacfc \uc5d4\ud2b8\ub85c\ud53c \ud328\uce6d)\uc744 \uc0ac\uc6a9\ud558\ub294 80\uc5b5 \ud30c\ub77c\ubbf8\ud130 BLT \ubaa8\ub378\uacfc BPE \uae30\ubc18 Llama 3 \ubaa8\ub378\uc758 \ubca4\uce58\ub9c8\ud06c \ud3c9\uac00 \uacb0\uacfc\ub97c \ube44\uad50\ud569\ub2c8\ub2e4. \ubaa8\ub4e0 \ubaa8\ub378\uc740 Llama 2 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec Dubey \ub4f1(2024)\uc5d0\uc11c \uc81c\uc2dc\ub41c \ucd5c\uc801\uc758 \ud559\uc2b5 \ub2e8\uacc4 \uc218\ub9cc\ud07c \ud559\uc2b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc989, \uc8fc\uc5b4\uc9c4 \ucef4\ud4e8\ud305 \uc608\uc0b0 \ub0b4\uc5d0\uc11c \ucd5c\uc0c1\uc758 \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\ub3c4\ub85d \uc124\uacc4\ub41c \uc124\uc815\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 BLT \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uae30\uc874 \ud1a0\ud06c\ub098\uc774\uc800 \uae30\ubc18 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uace0, \uc11c\ub85c \ub2e4\ub978 \ud328\uce6d \ubc29\uc2dd\uc758 \ud6a8\uacfc\ub97c \ud3c9\uac00\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "5. Scaling Trends"}, {"content": "| Llama 3 |\n| -------- |\n| (1T tokens) |", "caption": "Table 7: Ablations on the use of Cross Attention for a 1B BLT model trained on 100B bytes. We report bits-per-byte (bpb) on different datasets. We also report bpb on a random sample of the training data (denoted as Train Dist.) The Cross Attn. Enc. and Dec. columns denote which transformer layers the cross-attention block is applied after (or before for the decoder) in the local encoder and decoder respectively.", "description": "\uc774 \ud45c\ub294 10\uc5b5 \ubc14\uc774\ud2b8\ub85c \ud559\uc2b5\ub41c 10\uc5b5 \ub9e4\uac1c\ubcc0\uc218 BLT \ubaa8\ub378\uc5d0 \ub300\ud574 \uad50\ucc28 \uc8fc\uc758\ub825 \uc0ac\uc6a9 \uc5ec\ubd80\uc5d0 \ub530\ub978 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. bits-per-byte (bpb)\ub294 \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b\uacfc \ud559\uc2b5 \ub370\uc774\ud130\uc758 \ub79c\ub364 \uc0d8\ud50c(Train Dist.)\uc5d0\uc11c \uce21\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. 'Cross Attn. Enc.'\uc640 'Cross Attn. Dec.' \uc5f4\uc740 \uad50\ucc28 \uc8fc\uc758\ub825 \ube14\ub85d\uc774 \uc9c0\uc5ed \uc778\ucf54\ub354\uc640 \uc9c0\uc5ed \ub514\ucf54\ub354\uc758 \uc5b4\ub5a4 \ubcc0\ud658\uae30 \ub808\uc774\uc5b4 \ub2e4\uc74c\uc5d0 \uc801\uc6a9\ub418\uc5c8\ub294\uc9c0 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc9c0\uc5ed \ub514\ucf54\ub354\uc758 \uacbd\uc6b0, \uad50\ucc28 \uc8fc\uc758\ub825 \ube14\ub85d\uc740 \ubcc0\ud658\uae30 \ub808\uc774\uc5b4 *\uc55e*\uc5d0 \uc801\uc6a9\ub429\ub2c8\ub2e4.", "section": "7. Ablations and Discussion"}, {"content": "| Llama 3.1 |\n| --- |\n| (16T tokens) |", "caption": "Table 8: Ablations on the use of n-gram hash embedding tables for a 1B BLT model trained on 100B bytes. We find that hash n-gram embeddings are very effective with very large improvements in BPB.\nThe most significant parameter is the per-ngram vocab size and that smaller ngram sizes are more impactful than larger ones.", "description": "\uc774 \ud45c\ub294 10\uc5b5 \ubc14\uc774\ud2b8\ub85c \ud559\uc2b5\ub41c 10\uc5b5 \ud30c\ub77c\ubbf8\ud130 BLT \ubaa8\ub378\uc5d0 \ub300\ud574 n-gram \ud574\uc2dc \uc784\ubca0\ub529 \ud14c\uc774\ube14\uc744 \uc0ac\uc6a9\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud574\uc2dc n-gram \uc784\ubca0\ub529\uc740 BPB\ub97c \ud06c\uac8c \uac1c\uc120\ud558\ub294 \ub9e4\uc6b0 \ud6a8\uacfc\uc801\uc778 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4. \uac00\uc7a5 \uc911\uc694\ud55c \ud30c\ub77c\ubbf8\ud130\ub294 n-gram\ub2f9 \uc5b4\ud718 \ud06c\uae30\uc774\uba70, \uc791\uc740 n-gram \ud06c\uae30\uac00 \ud070 n-gram \ud06c\uae30\ubcf4\ub2e4 \ub354 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce69\ub2c8\ub2e4. \ud45c\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774 Wikipedia, Common Crawl, Github \ub370\uc774\ud130\uc14b\uacfc \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 bits-per-byte(bpb) \uc131\ub2a5\uc744 n-gram \ud06c\uae30(Ngram Sizes), n-gram\ub2f9 \uc5b4\ud718 \ud06c\uae30(Per Ngram Vocab), \ucd1d \uc5b4\ud718 \ud06c\uae30(Total Vocab)\ub97c \ubc14\uafd4\uac00\uba70 \uce21\uc815\ud588\uc2b5\ub2c8\ub2e4. BLT \ubaa8\ub378\uc740 \ud574\uc2dc n-gram \uc784\ubca0\ub529\uc744 \ud1b5\ud574 \uc774\uc804 \ubc14\uc774\ud2b8 \uc815\ubcf4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ud1b5\ud569\ud558\uc5ec \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.", "section": "7. Ablations and Discussion"}, {"content": "| BLT |\n| --- |\n| (1T tokens) |", "caption": "Table 9: \nWhen paired with hash n-gram embeddings, a light-weight local encoder is sufficient. More layers can then be allocated to the decoder for the same cost.", "description": "\uc774 \ud45c\ub294 BLT \ubaa8\ub378\uc5d0\uc11c \ud574\uc2dc n-\uadf8\ub7a8 \uc784\ubca0\ub529\uc744 \uc0ac\uc6a9\ud560 \ub54c \ub85c\uceec \uc778\ucf54\ub354\uc640 \ub514\ucf54\ub354\uc758 \ub808\uc774\uc5b4 \uc218\ub97c \ubcc0\uacbd\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud574\uc2dc n-\uadf8\ub7a8 \uc784\ubca0\ub529\uacfc \ud568\uaed8 \uc0ac\uc6a9\ud558\uba74 \uac00\ubcbc\uc6b4 \ub85c\uceec \uc778\ucf54\ub354(\uc608: \ub2e8\uc77c \ub808\uc774\uc5b4)\ub85c\ub3c4 \ucda9\ubd84\ud558\uba70, \ub354 \ub9ce\uc740 \ub808\uc774\uc5b4\ub97c \ub514\ucf54\ub354\uc5d0 \ud560\ub2f9\ud558\uc5ec \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "7. Ablations and Discussion"}, {"content": "| Language | Language -> English | | English -> Language | |\n|---|---|---|---|---| \n|  | Llama 3 | BLT | Llama 3 | BLT |\n| **Arabic** | 22.3 | 24.6 | 10.4 | 8.8 |\n| **German** | 41.3 | 42.0 | 29.8 | 31.2 |\n| **Hindi** | 20.7 | 20.9 | 7.8 | 7.2 |\n| **Italian** | 34.0 | 33.9 | 24.4 | 26.2 |\n| **Vietnamese** | 31.2 | 31.0 | 28.4 | 23.7 |\n| **Thai** | 17.9 | 18.1 | 10.5 | 7.7 |\n| **Armenian** | 1.7 | 6.3 | 0.6 | 0.9 |\n| **Amharic** | 1.3 | 3.1 | 0.4 | 0.5 |\n| **Assamese** | 2.7 | 5.4 | 0.8 | 1.6 |\n| **Bengali** | 4.7 | 12.7 | 1.7 | 4.1 |\n| **Bosnian** | 36.0 | 37.3 | 16.9 | 19.6 |\n| **Cebuano** | 18.2 | 20.6 | 5.8 | 9.1 |\n| **Georgian** | 1.7 | 7.4 | 1.0 | 2.5 |\n| **Gujarati** | 2.0 | 5.8 | 1.0 | 2.2 |\n| **Hausa** | 5.75 | 5.9 | 1.2 | 1.3 |\n| **Icelandic** | 16.1 | 17.9 | 4.8 | 5.3 |\n| **Kannada** | 1.6 | 3.9 | 0.7 | 1.7 |\n| **Kazakh** | 5.6 | 7.0 | 1.0 | 2.6 |\n| **Kabuverdianu** | 20.3 | 20.9 | 5.1 | 6.8 |\n| **Khmer** | 4.4 | 9.5 | 0.8 | 0.8 |\n| **Kyrgyz** | 4.6 | 5.1 | 0.9 | 2.0 |\n| **Malayalam** | 1.8 | 3.5 | 0.7 | 1.4 |\n| **Odia** | 1.6 | 2.7 | 0.8 | 1.1 |\n| **Somali** | 5.0 | 5.0 | 1.1 | 1.4 |\n| **Swahili** | 10.1 | 12.0 | 1.4 | 2.3 |\n| **Urdu** | 9.3 | 9.5 | 2.0 | 1.4 |\n| **Zulu** | 4.7 | 5.0 | 0.6 | 0.5 |\n| **Overall Average** | 12.1 | **14.0** | 5.9 | **6.4** |", "caption": "Table 10: Architectural hyper-parameters for different BLT model sizes that we train for flop-controlled experiments described in this paper.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc5d0\uc11c FLOP \uc81c\uc5b4 \uc2e4\ud5d8\uc5d0 \uc0ac\uc6a9\ub41c \ub2e4\uc591\ud55c BLT \ubaa8\ub378 \ud06c\uae30\uc5d0 \ub300\ud55c \uc544\ud0a4\ud14d\ucc98 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ubaa8\ub378 \ud06c\uae30\uc5d0 \ub300\ud574 \ub85c\uceec \uc778\ucf54\ub354 \ub808\uc774\uc5b4 \uc218(le), \ub85c\uceec \uc778\ucf54\ub354 \ud5e4\ub4dc \uc218, \ub85c\uceec \uc778\ucf54\ub354\uc758 hidden size, \ub85c\uceec \uc778\ucf54\ub354 \ud30c\ub77c\ubbf8\ud130 \uc218, \uc804\uccb4 \ub808\uc774\uc5b4 \uc218(lg), \uae00\ub85c\ubc8c latent transformer\uc758 \ud5e4\ub4dc \uc218, hidden size, \ud30c\ub77c\ubbf8\ud130 \uc218, \ub85c\uceec \ub514\ucf54\ub354 \ub808\uc774\uc5b4 \uc218, \ud5e4\ub4dc \uc218, hidden size, \ud30c\ub77c\ubbf8\ud130 \uc218, cross-attention \ud5e4\ub4dc \uc218, k \uac12\uc774 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "Appendix"}, {"content": "| Task | Prompt | Llama 3 | BLT | \n|---|---|---|---| \n| Substitute Word | Question: Substitute \" and \" with \" internet \" in \" She went to the kitchen and saw two cereals. \". Answer: | She went to the kitchen and saw two cereals. | She went to the kitchen internet saw two cereals. | \n| Swap Char | Question: Swap \" h \" and \" a \" in \" that \". Answer: | that | taht | \n| Substitute Char | Question: Substitute \" a \" with \" m \" in \" page \". Answer: | - | pmge | \n| Semantic Similarity | Question: More semantically related to \" are \": \" seem \", \" acre \". Answer: | acre | seem | \n| Orthographic Similarity | Question: Closer in Levenshtein distance to \" time \": \" timber \", \" period \". Answer: | period | timber | \n| Insert Char | Question: Add an \" z \" after every \" n \" in \" not \". Answer: | znotz | nzot |", "caption": "Table 11: flops for operations used in transformer and BLT models. l\ud835\udc59litalic_l corresponds to layers, h\u210ehitalic_h is the hidden dimension (hksubscript\u210e\ud835\udc58h_{k}italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT with nh\u2062e\u2062a\u2062d\u2062ssubscript\ud835\udc5b\u210e\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc60n_{heads}italic_n start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d italic_s end_POSTSUBSCRIPT heads), m\ud835\udc5amitalic_m is the context length, df\u2062f=4subscript\ud835\udc51\ud835\udc53\ud835\udc534d_{ff}=4italic_d start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT = 4 is the feed-forward dimension multiplier, p\ud835\udc5dpitalic_p is the patch size, and r\ud835\udc5fritalic_r is the ratio of queries to keys.", "description": "\uc774 \ud45c\ub294 \ud2b8\ub79c\uc2a4\ud3ec\uba38\uc640 BLT \ubaa8\ub378\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \uc5f0\uc0b0\uc5d0 \ub300\ud55c FLOPS(\ubd80\ub3d9 \uc18c\uc218\uc810 \uc5f0\uc0b0) \uacc4\uc0b0\uc2dd\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc5ec\uae30\uc11c *l*\uc740 \ub808\uc774\uc5b4 \uc218, *h*\ub294 \uc740\ub2c9 \ucc28\uc6d0 \ud06c\uae30(*hk*\ub294 \uc5b4\ud150\uc158 \ud5e4\ub4dc \uc218\uac00 *nheads*\uc778 \uacbd\uc6b0 \ud5e4\ub4dc \ucc28\uc6d0), *m*\uc740 \ubb38\ub9e5 \uae38\uc774, *dff*\ub294 \ud53c\ub4dc\ud3ec\uc6cc\ub4dc \ub124\ud2b8\uc6cc\ud06c\uc758 \ucc28\uc6d0 \ubc30\uc728(\ubcf4\ud1b5 4), *p*\ub294 \ud328\uce58 \ud06c\uae30, *r*\uc740 \ucffc\ub9ac\uc640 \ud0a4\uc758 \ube44\uc728\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc774 \ud45c\ub294 BLT \ubaa8\ub378\uc5d0\uc11c \uc11c\ub85c \ub2e4\ub978 \uad6c\uc131 \uc694\uc18c\uc758 \uacc4\uc0b0 \ube44\uc6a9\uc744 \ucd94\uc815\ud558\uace0 \ud1a0\ud070 \uae30\ubc18 \ubaa8\ub378\uacfc \ube44\uad50\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "Appendix B"}, {"content": "|                          | Llama 3 8B (220B tokens) | BLT 8B (220B tokens) | BLT from Llama 3.1 8B (220B tokens) | Llama 3.1 8B (15T tokens) |\n|--------------------------|---------------------------|-----------------------|------------------------------------|---------------------------|\n| **Arc-E**                | 67.4                     | 66.8                 | 66.6                                 | 83.4                     |\n| **Arc-C**                | 40.4                     | 38.8                 | 45.8                                 | 55.2                     |\n| **HellaSwag**            | 71.2                     | 72.2                 | 76.1                                 | 80.7                     |\n| **PIQA**                 | 77.0                     | 78.2                 | 77.4                                 | 80.7                     |\n| **MMLU**                 | 26.5                     | 25.2                 | 63.7                                 | 66.3                     |\n| **MBPP**                 | 11.8                     | 10.0                 | 38.2                                 | 47.2                     |\n| **HumanEval**            | 9.2                      | 7.3                  | 34.2                                 | 37.2                     |", "caption": "Table 12: Ablations on the use of frequency-based as well as hash-based n-gram embedding tables for a 1B BLT model trained on 100B bytes.", "description": "\uc774 \ud45c\ub294 10\uc5b5 \ubc14\uc774\ud2b8\ub85c \ud559\uc2b5\ub41c 10\uc5b5 \ub9e4\uac1c\ubcc0\uc218 BLT \ubaa8\ub378\uc5d0 \ub300\ud55c \ube48\ub3c4 \uae30\ubc18 n-gram \uc784\ubca0\ub529 \ud14c\uc774\ube14\uacfc \ud574\uc2dc \uae30\ubc18 n-gram \uc784\ubca0\ub529 \ud14c\uc774\ube14 \uc0ac\uc6a9\uc5d0 \ub300\ud55c ablation \uc5f0\uad6c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud574\uc2dc \uae30\ubc18 n-gram \uc784\ubca0\ub529\uc774 \ubaa8\ub4e0 \ub3c4\uba54\uc778, \ud2b9\ud788 Wikipedia\uc640 Github\uc5d0\uc11c \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ub3c4\uc6c0\uc774 \ub41c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uac00\uc7a5 \uc911\uc694\ud55c \ub9e4\uac1c\ubcc0\uc218\ub294 n-gram\ub2f9 \uc5b4\ud718 \ud06c\uae30\uc774\uba70, \uc791\uc740 n-gram \ud06c\uae30\uac00 \ud070 n-gram \ud06c\uae30\ubcf4\ub2e4 \ub354 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce5c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "7. Ablations and Discussion"}]