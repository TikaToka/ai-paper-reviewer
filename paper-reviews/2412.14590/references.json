{"references": [{"fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: accurate post-training quantization for generative pre-trained transformers", "publication_date": "2022-10-17", "reason": "This paper is highly influential as it introduces GPTQ, a widely adopted technique for post-training quantization of LLMs, significantly improving accuracy and efficiency."}, {"fullname_first_author": "Sehoon Kim", "paper_title": "Squeezellm: Dense-and-sparse quantization", "publication_date": "2024-07-21", "reason": "This paper presents SqueezeLLM, a novel method that effectively combines dense and sparse quantization techniques, achieving high accuracy with reduced computation costs."}, {"fullname_first_author": "Zhewei Yao", "paper_title": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers", "publication_date": "2022-11-28", "reason": "The authors introduce ZeroQuant, a method focused on addressing the accuracy limitations of low-bit quantization, demonstrating a strong improvement in efficiency while mitigating accuracy loss."}, {"fullname_first_author": "Tim Dettmers", "paper_title": "Spqr: A sparse-quantized representation for near-lossless LLM weight compression", "publication_date": "2024-05-07", "reason": "The authors introduce SpQR, a method that efficiently combines sparse and quantized representations of LLM weights, improving compression while preserving accuracy."}, {"fullname_first_author": "Xiaoxia Wu", "paper_title": "Zero-quant(4+2): Redefining LLMs quantization with a new fp6-centric strategy for diverse generative tasks", "publication_date": "2023-12-08", "reason": "This paper proposes ZeroQuant(4+2), which improves accuracy through a novel FP6-centric strategy, demonstrating significant advancements in handling diverse generative tasks."}]}