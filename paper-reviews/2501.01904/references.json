{"references": [{"fullname_first_author": "OpenAI", "paper_title": "Learning to reason with large language models", "publication_date": "2024-09-00", "reason": "This paper introduces the concept of slow-thinking reasoning systems, which is the central focus of the current research."}, {"fullname_first_author": "DeepSeek Team", "paper_title": "DeepSeek-R1-lite-preview is now live: unleashing supercharged reasoning power!", "publication_date": "2024-11-00", "reason": "This paper presents a slow-thinking reasoning system that serves as a key comparison point for the current research."}, {"fullname_first_author": "Qwen Team", "paper_title": "QWQ: Reflect deeply on the boundaries of the unknown", "publication_date": "2024-11-00", "reason": "This paper introduces another slow-thinking reasoning system, providing an additional benchmark for comparison."}, {"fullname_first_author": "Wayne Xin Zhao", "paper_title": "A survey of large language models", "publication_date": "2023-03-00", "reason": "This paper provides background information on large language models, establishing a foundational understanding for the current research."}, {"fullname_first_author": "Yingqian Min", "paper_title": "Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems", "publication_date": "2024-12-00", "reason": "This paper offers insights into the methodology of reproducing slow-thinking reasoning systems, which is relevant to the current research's approach."}]}