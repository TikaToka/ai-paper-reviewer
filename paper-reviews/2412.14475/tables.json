[{"content": "Task|Zero-shot|Zero-shot|Zero-shot|Zero-shot|Zero-shot|Zero-shot|Zero-shot|Zero-shot|Fine-Tune|Fine-Tune\n---|---|---|---|---|---|---|---|---|---|---\n|CLIP|OpenCLIP|SigLIP|BLIP2|MagicLens|E5-V|UniIR|MMRet|VLM2Vec|MMRet|\nClassification (10 tasks)|ImageNet-1K|N24News|HatefulMemes|VOC2007|SUN397|Place365|ImageNet-A|ImageNet-R|ObjectNet|Country-211\n---|---|---|---|---|---|---|---|---|---|---\n|55.8|34.7|51.1|50.7|43.4|28.5|25.5|75.6|43.4|19.2\n|63.5|38.6|51.7|52.4|68.8|37.8|14.2|83.0|51.4|16.8\n|45.4|13.9|47.2|64.3|39.6|20.0|42.6|75.0|40.3|14.2\n|10.3|36.0|49.6|52.1|34.5|21.5|3.2|39.7|20.6|2.5\n|48.0|33.7|49.0|51.6|57.0|31.5|8.0|70.9|31.6|6.2\n|9.6|23.4|49.7|49.9|33.1|8.6|2.0|30.8|7.5|3.1\n|53.7|33.9|51.0|62.7|61.7|38.0|12.9|61.6|37.1|8.8\n|49.1|45.8|51.0|74.6|60.1|35.3|31.6|66.2|49.2|9.3\n|65.6|79.5|67.1|88.6|72.7|42.6|19.3|70.2|29.5|13.0\n|58.8|71.3|53.7|85.0|70.0|43.0|36.1|71.6|55.8|14.7\nAll Classification|42.8|47.8|40.3|27.0|38.8|21.8|42.1|47.2|54.8|56.0\nVQA (10 tasks)|OK-VQA|A-OKVQA|DocVQA|InfographicsVQA|ChartQA|Visual7W|ScienceQA|VizWiz|GQA|TextVQA\n---|---|---|---|---|---|---|---|---|---|---\n|7.5|3.8|4.0|4.6|1.4|4.0|9.4|8.2|41.3|7.0\n|11.5|3.3|5.3|4.6|1.5|2.6|10.2|6.6|52.5|10.9\n|2.4|1.5|4.2|2.7|3.0|1.2|7.9|2.3|57.5|1.0\n|8.7|3.2|2.6|2.0|0.5|1.3|6.8|4.0|9.7|3.3\n|12.7|2.9|3.0|5.9|0.9|2.5|5.2|1.7|43.5|4.6\n|8.9|5.9|1.7|2.3|2.4|5.8|3.6|2.6|7.8|3.2\n|24.5|10.6|5.6|5.0|1.8|12.3|11.6|19.2|49.3|10.6\n|28.0|11.6|12.6|10.6|2.4|9.0|23.3|25.9|41.3|18.9\n|63.2|50.2|78.4|40.8|59.0|47.7|43.4|39.2|60.7|66.1\n|73.3|56.7|78.5|39.3|41.7|49.5|45.2|51.7|59.0|79.0\nAll VQA|9.1|10.9|8.4|4.2|8.3|4.9|15.0|18.4|54.9|57.4\nRetrieval (12 tasks)|VisDial|CIRR|VisualNews_t2i|VisualNews_i2t|MSCOCO_t2i|MSCOCO_i2t|NIGHTS|WebQA|FashionIQ|Wiki-SS-NQ\n---|---|---|---|---|---|---|---|---|---|---\n|30.7|12.6|78.9|79.6|59.5|57.7|60.4|67.5|11.4|55.0\n|25.4|15.4|74.0|78.0|63.6|62.1|66.1|62.1|13.8|44.6\n|21.5|15.1|51.0|52.4|58.3|55.0|62.9|58.1|20.1|55.1\n|18.0|9.8|48.1|13.5|53.7|20.3|56.5|55.4|9.3|28.7\n|24.8|39.1|50.7|21.1|54.1|40.0|58.1|43.0|11.2|18.7\n|9.2|6.1|13.5|8.1|20.7|14.0|4.2|17.7|2.8|8.6\n|37.6|53.2|63.6|68.8|72.0|74.1|69.7|86.3|39.3|11.3\n|62.6|65.7|45.7|53.4|68.7|56.7|59.4|76.3|31.5|25.4\n|73.3|47.8|67.2|70.7|70.6|66.5|66.1|88.1|12.9|56.6\n|83.0|61.4|74.2|78.1|78.6|72.4|68.3|90.2|54.9|24.9\nAll Retrieval|53.0|52.3|31.6|33.9|35.4|11.5|60.1|56.5|62.3|69.9\nVisual Grounding (4 tasks)|MSCOCO|RefCOCO|RefCOCO-matching|Visual7W-pointing\n---|---|---|---|---\n|33.8|56.9|61.3|55.1\n|34.5|54.2|68.3|56.3\n|46.4|70.8|50.8|70.1\n|28.9|47.4|59.5|52.0\n|22.1|22.8|35.6|23.4\n|10.8|11.9|38.9|14.3\n|46.6|67.8|62.9|71.3\n|42.7|69.3|63.2|73.5\n|67.3|84.7|79.2|86.8\n|76.8|89.8|90.6|77.0\nAll Visual Grounding|51.8|53.3|59.5|47.0|26.0|19.0|62.2|62.2|79.5|83.6\nFinal Score (36 tasks)|All|All IND|All OOD\n---|---|---|---\n|37.8|37.1|38.7\n|39.7|39.3|40.2\n|34.8|32.3|38.0\n|25.2|25.3|25.1\n|27.8|31.0|23.7\n|13.3|14.9|11.5\n|42.8|44.7|40.4\n|44.0|43.5|44.3\n|60.1|66.5|52.0\n|64.1|59.1|68.0", "caption": "Table 1: Zero-shot retrieval performance on various CIR benchmarks. \u2217 denotes the previous best performance for each benchmark prior to MMRet. \u2020 indicates methods with multiple components (e.g., GPT-3.5, Qwen1.5-32B); we report # parameters of components with known sizes. The CoCa-based MagicLens\u2021 models are proprietary. Results in bold and underline denote the best and second-best performances for each model scale, respectively. Our MMRet model achieves state-of-the-art results across different model sizes and benchmarks, surpassing the previous SOTA by 8.1% on the main benchmark CIRCO, significantly advancing zero-shot CIR methods.", "description": "\ud45c 1\uc740 \ub2e4\uc591\ud55c \uad6c\uc131 \uc774\ubbf8\uc9c0 \uac80\uc0c9(CIR) \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc81c\ub85c\uc0f7 \uac80\uc0c9 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MMRet \uc774\uc804 \ucd5c\uace0 \uc131\ub2a5\uc740 \ubcc4\ud45c(*)\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, GPT-3.5\ub098 Qwen-1.5-32B \uc640 \uac19\uc774 \uc5ec\ub7ec \uad6c\uc131 \uc694\uc18c\uac00 \uc788\ub294 \ubc29\ubc95\uc740 \u2020 \ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uc54c\ub824\uc9c4 \ud06c\uae30\uc758 \uad6c\uc131 \uc694\uc18c \ub9e4\uac1c\ubcc0\uc218 \uc218\ub97c \ubcf4\uace0\ud569\ub2c8\ub2e4. CoCa \uae30\ubc18 MagicLens \ubaa8\ub378\uc740 \ub3c5\uc810\uc801\uc774\ubbc0\ub85c \u2021\uc73c\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4.  \uad75\uc740 \ubc11\uc904\uc740 \uac01 \ubaa8\ub378 \uaddc\ubaa8\uc5d0 \ub300\ud55c \ucd5c\uace0 \ubc0f \ub450 \ubc88\uc9f8 \ucd5c\uace0 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. MMRet \ubaa8\ub378\uc740 \ub2e4\uc591\ud55c \ubaa8\ub378 \ud06c\uae30\uc640 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ucd5c\ucca8\ub2e8 \uacb0\uacfc\ub97c \ub2ec\uc131\ud558\uba70, \uc8fc\uc694 \ubca4\uce58\ub9c8\ud06c\uc778 CIRCO\uc5d0\uc11c \uc774\uc804 \ucd5c\uace0 \uc131\ub2a5\uc744 8.1% \uc0c1\ud68c\ud569\ub2c8\ub2e4. \uc774\ub294 \uc81c\ub85c\uc0f7 CIR \ubc29\ubc95\uc744 \ud06c\uac8c \ubc1c\uc804\uc2dc\ud0a8 \uac83\uc785\ub2c8\ub2e4.", "section": "4.1 Zero-shot Performance on CIR tasks"}]