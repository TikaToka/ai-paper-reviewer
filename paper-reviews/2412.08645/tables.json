[{"content": "| Method | Composition | Identity | \n|---|---|---| \n| | CLIP-I | DINO | IR |\n| Paint-by-Example | 0.898 | 0.800 | 0.544 |\n| ObjectStitch | 0.905 | 0.793 | 0.564 |\n| AnyDoor | 0.916 | 0.822 | 0.738 |\n| Ours - 1 Ref. | 0.934 | 0.868 | 0.803 |\n| Ours - 3 Ref. | **0.940** | **0.885** | **0.858** |", "caption": "Table 1: Object insertion: baseline comparison. Our method achieves better composition and identity preservation.", "description": "\uac1d\uccb4 \uc0bd\uc785\uc5d0 \ub300\ud55c \uc5ec\ub7ec \uae30\uc900 \ubaa8\ub378\uacfc ObjectMate\ub97c \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. ObjectMate\ub294 \uad6c\uc131 \ubc0f ID \ubcf4\uc874 \uce21\uba74\uc5d0\uc11c \ub2e4\ub978 \ubaa8\ub4e0 \uae30\uc900\uc120\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub6f0\uc5b4\ub0a9\ub2c8\ub2e4.", "section": "6. \uc2e4\ud5d8"}, {"content": "| Task | CLIP-I | DINO | IR |\n|---|---|---|---| \n| Subject Generation | 64.7% | 68.4% | **72.9%** |\n| Object Insertion | 60.4% | 71.8% | **79.5%** |", "caption": "Table 2: Subject-driven generation: baseline comparison. While many methods perform well on semantic similarity (CLIP-I, DINO), our method performs the best at identity presentation (IR) and alignment to the text prompt (CLIP-T).", "description": "\uc774 \ud45c\ub294 \uc8fc\uccb4 \uae30\ubc18 \uc0dd\uc131(Subject-driven generation)\uc5d0\uc11c \ub2e4\uc591\ud55c \uc0dd\uc131 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. \uc758\ubbf8\ub860\uc801 \uc720\uc0ac\uc131(CLIP-I, DINO)\uc5d0\uc11c\ub294 \ub9ce\uc740 \ubaa8\ub378\ub4e4\uc774 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uc9c0\ub9cc, \uac1d\uccb4 \uc77c\uce58(IR) \ubc0f \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8 \uc815\ub82c(CLIP-T)\uc5d0\uc11c\ub294 \ubcf8 \uc5f0\uad6c\uc758 ObjectMate\uac00 \uac00\uc7a5 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "6.3. Subject-driven generation"}, {"content": "| Dataset | # Images | # Objects | Detection type | 1 NN | 3 NNs |\n|---|---|---|---|---|---| \n| COCO | 108,151 | 362,684 | Human annotations | 31,445 (8.7%) | 17,119 (4.7%) |\n| Open Images | 1,743,042 | 8,067,907 | Human annotations | 471,091 (5.8%) | 64,991 (2.4%) |\n| Web-based | 47,992,480 | 55,232,441 | Object detection model | 9,947,017 (18%) | 4,550,770 (8.2%) |", "caption": "Table 3: Identity metric comparison. Accuracy of metrics in predicting user responses. IR is the most accurate.", "description": "\uc0ac\uc6a9\uc790 \uc751\ub2f5 \uc608\uce21\uc5d0\uc11c \ub2e4\uc591\ud55c \uc720\uc0ac\ub3c4 \uce21\uc815 \uc9c0\ud45c(CLIP-I, DINO, IR)\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4. IR \uc9c0\ud45c\uac00 \uac00\uc7a5 \uc815\ud655\ud55c \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4.", "section": "6. \uc2e4\ud5d8"}]