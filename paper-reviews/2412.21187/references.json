{"references": [{"fullname_first_author": "Maciej Besta", "paper_title": "Graph of thoughts: Solving elaborate problems with large language models", "publication_date": "2024", "reason": "This paper introduces a novel approach to enhance problem-solving capabilities of LLMs by exploring multiple reasoning strategies in a graph structure, which is relevant to the paper's focus on efficient reasoning in LLMs."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-14", "reason": "This paper introduces a dataset of high-quality math word problems, providing a benchmark dataset for evaluating the reasoning capabilities of LLMs, which is directly relevant to the paper's experimentation."}, {"fullname_first_author": "Mehul Damani", "paper_title": "Learning how hard to think: Input-adaptive allocation of LM computation", "publication_date": "2024", "reason": "This paper explores efficient resource allocation for LLMs by dynamically adjusting computation based on problem difficulty which is directly related to the paper's work on reducing overthinking."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the MATH dataset", "publication_date": "2021", "reason": "This paper introduces a challenging benchmark dataset for mathematical problem solving which is used in the paper's empirical evaluation of the overthinking issue and efficiency metrics."}, {"fullname_first_author": "Namgyu Ho", "paper_title": "Large language models are reasoning teachers", "publication_date": "2023", "reason": "This paper explores the use of LLMs as reasoning teachers, providing insights into the training methods for LLMs, a topic that relates to the paper's discussion of self-training and improving reasoning efficiency."}]}