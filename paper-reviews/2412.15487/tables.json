[{"content": "| Multi-LLM Summarization |  |  |\n|---|---|---|\n| **Framework** | **General Mechanism** | **Stage** |\n| **Centralized** (Sec. 4) | **Single-Round** (Sec. 4.1) | Generation (\u00a7 4.1.1) |\n|  |  | Evaluation (\u00a7 4.1.2) |\n|  | **Conversational** (Sec. 4.2) | Generation (\u00a7 4.2.1) |\n|  |  | Evaluation (\u00a7 4.2.2) |\n| **Decentralized** (Sec. 5) | **Single-Round** (Sec. 5.1) | Generation (\u00a7 5.1.1) |\n|  |  | Evaluation (\u00a7 5.1.2) |\n|  | **Conversational** (Sec. 5.2) | Generation (\u00a7 5.2.1) |\n|  |  | Evaluation (\u00a7 5.2.2) |", "caption": "Table 1: \nOverview of Multi-LLM Summarization Framework (Sections\u00a04-5).", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 4\uc7a5\uacfc 5\uc7a5\uc5d0 \uac78\uccd0 \uc124\uba85\ud558\ub294 \ub2e4\uc591\ud55c Multi-LLM \uc694\uc57d \ud504\ub808\uc784\uc6cc\ud06c\ub4e4\uc744 \uac1c\uad04\uc801\uc73c\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ud504\ub808\uc784\uc6cc\ud06c\ub294 \uc694\uc57d \uc0dd\uc131\uacfc \ud3c9\uac00 \ub2e8\uacc4\ub97c \ud3ec\ud568\ud558\uba70, \uc911\uc559 \uc9d1\uc911\uc2dd \ubc0f \ubd84\uc0b0\uc2dd \uc804\ub7b5\uc5d0 \ub530\ub77c \uc138\ubd84\ud654\ub429\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \uc804\ub7b5\uc758 \ub2e8\uc77c \ub77c\uc6b4\ub4dc \ubc0f \ub300\ud654\ud615 \uc811\uadfc \ubc29\uc2dd, \uadf8\ub9ac\uace0 \uc0ac\uc6a9\ub41c \uc0dd\uc131 \ubc0f \ud3c9\uac00 \uba54\ucee4\ub2c8\uc998\uc774 \uc694\uc57d\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3 Multi-LLM Summarization Framework"}, {"content": "|               |                     | ROUGE-1 \u2191 | ROUGE-L \u2191 |       | BLEU-1 \u2191 | BLEU-4 \u2191 |       | ROUGE-1 \u2191 | ROUGE-L \u2191 |       | BLEU-1 \u2191 | BLEU-4 \u2191 |       |\n| :------------ | :------------------ | :--------: | :--------: | :----: | :--------: | :--------: | :----: | :--------: | :--------: | :----: | :--------: | :--------: | :----: |\n|               |                     |  ArXiv    |  ArXiv    |       |  ArXiv    |  ArXiv    |       | GovReport | GovReport |       | GovReport | GovReport |       |\n|               | LLaMA3-8B           |   0.180   |   0.106   |       |   0.084   |   0.021   |       |   0.403   |   0.177   |       |   0.242   |   0.079   |       |\n|               | GPT-3.5             |   0.193   |   0.114   |       |   0.093   |   0.026   |       |   0.390   |   0.178   |       |   0.226   |   0.084   |       |\n|               | GPT-4o mini         |   0.217   |   0.118   |       |   0.108   |   0.020   |       |   0.384   |   0.156   |       |   0.224   |   0.058   |       |\n|               | GPT-4o              |   0.165   |   0.095   |       |   0.073   |   0.015   |       |   0.372   |   0.155   |       |   0.211   |   0.059   |       |\n| **Decentralized** | Multi-LLM 3 round max |   0.313   |   0.163   |       |   0.200   |   0.029   |       |   0.447   |   0.180   |       |   0.458   |   0.098   |       |\n|               | Multi-LLM 1 round max |   0.339   |   0.180   |       |   0.224   |   0.043   |       |   0.468   |   0.190   |       |   0.477   |   0.112   |       |\n| **Centralized**  | Multi-LLM 3 round max |   0.329   |   0.168   |       |   0.217   |   0.031   |       |   0.468   |   0.189   |       |   0.470   |   0.109   |       |\n|               | Multi-LLM 1 round max |   0.333   |   0.173   |       |   0.219   |   0.036   |       |   0.479   |   0.197   |       |   0.485   |   0.121   |       |", "caption": "Table 2: \nResults for the decentralized and centralized Multi-LLM approaches. For the multi-LLM pipelines participating models are GPT-3.5 and GPT-4o mini.\nThe results use GPT-3.5 for the evaluator in the centralized approach, and summaries from GPT-3.5 are chosen in tie-breaking for both centralized and de-centralized approaches.", "description": "\ud45c 2\ub294 \uc911\uc559 \uc9d1\uc911\uc2dd \ubc0f \ubd84\uc0b0\ud615 Multi-LLM \uc811\uadfc \ubc29\uc2dd\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Multi-LLM \ud30c\uc774\ud504\ub77c\uc778\uc5d0 \ucc38\uc5ec\ud558\ub294 \ubaa8\ub378\uc740 GPT-3.5\uc640 GPT-4o mini\uc785\ub2c8\ub2e4. \uc911\uc559 \uc9d1\uc911\uc2dd \ubc29\ubc95\uc758 \uacbd\uc6b0 \ud3c9\uac00\uc790\ub85c GPT-3.5\ub97c \uc0ac\uc6a9\ud558\uace0, \uc911\uc559 \uc9d1\uc911\uc2dd \ubc0f \ubd84\uc0b0\ud615 \ubc29\ubc95 \ubaa8\ub450\uc5d0\uc11c GPT-3.5 \uc694\uc57d\ubb38\uc744 \ub3d9\uc810\uc790 \uacb0\uc815\uc5d0 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 ArXiv\uc640 GovReport \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c ROUGE-1, ROUGE-L, BLEU-1, BLEU-4 \uc810\uc218\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc989, \ub2e8\uc77c LLM \uae30\ubc18 \ubc29\ubc95 \ub300\ube44 Multi-LLM \ubc29\ubc95\uc758 \uc131\ub2a5 \ud5a5\uc0c1 \uc815\ub3c4\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "6. Experiments"}, {"content": "|                       | Max Rounds | Multi-LLM Model Combination | ROUGE-1 \u2191 | ROUGE-L \u2191 |  | BLEU-1 \u2191 | BLEU-4 \u2191 | \n| :-------------------- | :---------- | :----------------------------- | :-------- | :-------- | :- | :-------- | :-------- | \n| **Decentralized**     | **3 Rounds** | **GPT-3.5** & GPT-4o mini       | **0.328**  | **0.167**  |   | **0.217**  | **0.030**  | \n|                       |             | GPT-4o & **GPT-3.5**           | 0.313     | 0.159     |   | 0.197     | 0.025     | \n|                       |             | GPT-4o & **GPT-4o mini**       | 0.302     | 0.152     |   | 0.185     | 0.022     | \n|                       | **1 Rounds** | **GPT-3.5** & GPT-4o mini       | **0.333**  | **0.173**  |   | **0.218**  | **0.036**  | \n|                       |             | GPT-4o & **GPT-3.5**           | 0.328     | 0.170     |   | 0.212     | 0.033     | \n|                       |             | GPT-4o & **GPT-4o mini**       | 0.305     | 0.153     |   | 0.189     | 0.023     | \n| **Centralized**      | **3 Rounds** | **GPT-3.5** & GPT-4o mini       | 0.312     | 0.163     |   | 0.199     | **0.029**  | \n|                       |             | GPT-4o & **GPT-3.5**           | **0.325**  | **0.166**  |   | **0.214**  | **0.029**  | \n|                       |             | GPT-4o & **GPT-4o mini**       | 0.304     | 0.153     |   | 0.188     | 0.022     | \n|                       | **1 Rounds** | **GPT-3.5** & GPT-4o mini       | 0.338     | **0.180**  |   | 0.224     | **0.042**  | \n|                       |             | GPT-4o & **GPT-3.5**           | **0.339**  | 0.177     |   | **0.228**  | 0.039     | \n|                       |             | GPT-4o & **GPT-4o mini**       | 0.306     | 0.155     |   | 0.190     | 0.022     | ", "caption": "Table 3: Varying the combination of models in our Multi-LLM approaches.\nNote rounds is the max number of rounds allowed and all results are for ArXiv.\nBolded numbers are best scores for each round-model combination. Underlined numbers are overall best scores for each metric in this table.\nFurthermore, the central LLM is highlighted in blue and for the decentralized multi-LLM approaches, we highlight the LLM used for tie-breaking in green.", "description": "\ud45c 3\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ub41c \ub2e4\uc911 LLM \uc811\uadfc \ubc29\uc2dd\uc5d0\uc11c \ubaa8\ub378 \uc870\ud569\uc744 \ub2e4\ub974\uac8c \uc0ac\uc6a9\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  'rounds'\ub294 \ud5c8\uc6a9\ub418\ub294 \ucd5c\ub300 \ub77c\uc6b4\ub4dc \uc218\ub97c \ub098\ud0c0\ub0b4\uba70, \ubaa8\ub4e0 \uacb0\uacfc\ub294 ArXiv \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uac83\uc785\ub2c8\ub2e4.  \uad75\uc740 \uc22b\uc790\ub294 \uac01 \ub77c\uc6b4\ub4dc-\ubaa8\ub378 \uc870\ud569\uc5d0\uc11c \uac00\uc7a5 \uc88b\uc740 \uc810\uc218\ub97c \ub098\ud0c0\ub0b4\uace0, \ubc11\uc904 \uce5c \uc22b\uc790\ub294 \ud45c\uc758 \uac01 \uc9c0\ud45c\uc5d0 \ub300\ud55c \uc804\ubc18\uc801\uc73c\ub85c \uac00\uc7a5 \uc88b\uc740 \uc810\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc911\uc559 LLM\uc740 \ud30c\ub780\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uace0, \ubd84\uc0b0\ud615 \ub2e4\uc911 LLM \uc811\uadfc \ubc29\uc2dd\uc758 \uacbd\uc6b0 \ub3d9\uc810 \ud574\uacb0\uc5d0 \uc0ac\uc6a9\ub41c LLM\uc740 \ub179\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub429\ub2c8\ub2e4.", "section": "6.3 Ablation Studies"}, {"content": "|                   |                               | Input Tokens | Output Tokens | Average Tokens | Total Tokens |\n|-------------------|-------------------------------|---------------|----------------|-----------------|---------------|\n| Decentralized     | Multi-LLM 3 round max          | 383.73M       | 25.63M         | 14.62M          | 409.37M       |\n|                   | Multi-LLM 1 round max          | 129.36M       | 11.89M         | 11.77M          | 141.25M       |\n| Centralized      | Multi-LLM 3 round max          | 216.65M       | 19.55M         | 14.76M          | 236.2M        |\n|                   | Multi-LLM 1 round max          | 77.69M        | 6.77M          | 10.56M          | 84.46M        |", "caption": "Table 4: Cost Analysis of our Multi-LLM Decentralized and Centralized Summarization Methods. Note M\ud835\udc40Mitalic_M= millions of tokens.", "description": "\ud45c 4\ub294 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ub41c \ub2e4\uc911 LLM \uc694\uc57d \ubc29\ubc95\uc758 \ube44\uc6a9 \ubd84\uc11d\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc911 LLM \uc911\uc559 \uc9d1\uc911\uc2dd \ubc0f \ubd84\uc0b0\uc2dd \uc811\uadfc \ubc29\uc2dd \ubaa8\ub450\uc5d0 \ub300\ud574 \uc785\ub825 \ud1a0\ud070, \ucd9c\ub825 \ud1a0\ud070, \ud3c9\uade0 \ud1a0\ud070, \ucd1d \ud1a0\ud070 \uc218\ub97c M(\ubc31\ub9cc) \ub2e8\uc704\ub85c \ubd84\uc11d\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uac01 \uc694\uc57d \ubc29\ubc95\uc758 \uacc4\uc0b0 \ube44\uc6a9\uc744 \ube44\uad50\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  \ud2b9\ud788, \uc911\uc559 \uc9d1\uc911\uc2dd \uc811\uadfc \ubc29\uc2dd\uacfc \ubd84\uc0b0\uc2dd \uc811\uadfc \ubc29\uc2dd \uac04\uc758 \ube44\uc6a9 \ucc28\uc774\ub97c \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6. Experiments"}, {"content": "|               |                       |                               | ROUGE-1 \u2191 | ROUGE-L \u2191 |       | BLEU-1 \u2191 | BLEU-4 \u2191 | ROUGE-1 \u2191 | ROUGE-L \u2191 |       | BLEU-1 \u2191 | BLEU-4 \u2191 |\n| :------------ | :-------------------- | :----------------------------- | :-------: | :-------: | :----: | :-------: | :-------: | :-------: | :-------: | :----: | :-------: | :-------: |\n| **GPT-4o mini Evaluator** | **Decentralized** | Multi-LLM 3 round max         |  0.317   |  0.160   |       |  0.206   |  0.026   |  **0.445** |  **0.178** |       |  **0.452** |  **0.094** |\n|               |                       | Multi-LLM 1 round max         | **0.326** | **0.163** |       | **0.221** | **0.027** |  0.438   |  0.175   |       |  0.446   |  0.089   |\n|               | **Centralized**      | Multi-LLM 3 round max         |  0.315   |  0.158   |       |  0.201   |  0.027   |  **0.441** |  **0.176** |       |  **0.447** |  **0.092** |\n|               |                       | Multi-LLM 1 round max         | **0.330** | **0.165** |       | **0.222** | **0.028** |  0.439   |  0.175   |       |  0.446   |  0.090   |\n| **GPT-3.5 Evaluator**     | **Decentralized** | Multi-LLM 3 round max         |  0.313   |  0.163   |       |  0.200   |  0.029   |  0.447   |  0.180   |       |  0.458   |  0.098   |\n|               |                       | Multi-LLM 1 round max         | **0.339** | **0.180** |       | **0.224** | **0.043** |  **0.468** |  **0.190** |       |  **0.477** |  **0.112** |\n|               | **Centralized**      | Multi-LLM 3 round max         |  0.329   |  0.168   |       |  0.217   |  0.031   |  0.468   |  0.189   |       |  0.470   |  0.109   |\n|               |                       | Multi-LLM 1 round max         | **0.333** | **0.173** |       | **0.219** | **0.036** | **0.479** | **0.197** |       | **0.485** | **0.121** |\n| **GPT-4o Evaluator**     | **Decentralized** | Multi-LLM 3 round max         | **0.326** | **0.166** |       | **0.214** | 0.030   |  0.446   |  0.179   |       |  0.456   |  0.098   |\n|               |                       | Multi-LLM 1 round max         |  0.325   |  0.165   |       |  0.211   |  0.030   |  **0.456** |  **0.183** |       |  **0.461** |  **0.100** |\n|               | **Centralized**      | Multi-LLM 3 round max         |  0.318   |  0.162   |       |  0.206   |  0.027   |  0.449   |  0.181   |       |  0.452   |  0.096   |\n|               |                       | Multi-LLM 1 round max         | **0.327** | **0.167** |       | **0.215** | **0.031** |  **0.461** |  **0.186** |       |  **0.467** |  **0.105** |", "caption": "Table 5: \nResults for different evaluating and tie-breaking models for Multi-LLM approaches. The choice of the tie-breaker models is the same as the choice of evaluator model. We bold the best results for each combination of the experimental variables, and we underline the best results overall. For ease of comparison, we reproduce the best-performing 2-LLM results obtained in Table\u00a02", "description": "\ud45c 5\ub294 Multi-LLM \uc811\uadfc \ubc29\uc2dd\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ud3c9\uac00 \ubc0f \ub3d9\uc810\uc790 \uacb0\uc815 \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub3d9\uc810\uc790 \uacb0\uc815 \ubaa8\ub378\uc740 \ud3c9\uac00 \ubaa8\ub378\uacfc \ub3d9\uc77c\ud558\uac8c \uc120\ud0dd\ub429\ub2c8\ub2e4. \uc2e4\ud5d8 \ubcc0\uc218\uc758 \uac01 \uc870\ud569\uc5d0 \ub300\ud574 \ucd5c\uc0c1\uc758 \uacb0\uacfc\ub294 \uad75\uac8c \ud45c\uc2dc\ud558\uace0, \uc804\ubc18\uc801\uc73c\ub85c \ucd5c\uc0c1\uc758 \uacb0\uacfc\ub294 \ubc11\uc904\ub85c \ud45c\uc2dc\ud569\ub2c8\ub2e4. \ube44\uad50\ub97c \uc6a9\uc774\ud558\uac8c \ud558\uae30 \uc704\ud574 \ud45c 2\uc5d0\uc11c \uac00\uc7a5 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc778 2-LLM \uacb0\uacfc\ub97c \ub2e4\uc2dc \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "6.3 Ablation Studies"}, {"content": "|   |   |   | ArXiv ROUGE-1 \u2191 | ArXiv ROUGE-L \u2191 |   | ArXiv BLEU-1 \u2191 | ArXiv BLEU-4 \u2191 |   | GovReport ROUGE-1 \u2191 | GovReport ROUGE-L \u2191 |   | GovReport BLEU-1 \u2191 | GovReport BLEU-4 \u2191 |   |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **2-LLMs** <br> GPT-3.5 Evaluator | **Decentralized** | 3 rounds | 0.313 | 0.163 |  | 0.200 | 0.029 |  | 0.447 | 0.180 |  | 0.458 | 0.098 |  |\n|  |  | 1 rounds | **0.339** | **0.180** |  | **0.224** | **0.043** |  | 0.468 | 0.190 |  | 0.477 | 0.112 |  |\n|  | **Centralized** | 3 rounds | 0.329 | 0.168 |  | 0.217 | 0.031 |  | 0.468 | 0.189 |  | 0.470 | 0.109 |  |\n|  |  | 1 rounds | 0.333 | 0.173 |  | 0.219 | 0.036 |  | **0.479** | **0.197** |  | **0.485** | **0.121** |  |\n| **3-LLMs** <br> GPT-4o mini Evaluator | **Decentralized** | 3 rounds | **0.301** | **0.154** |  | 0.184 | **0.024** |  | 0.445 | 0.178 |  | 0.449 | 0.095 |  |\n|  |  | 1 rounds | 0.299 | 0.152 |  | 0.184 | 0.023 |  | 0.442 | 0.178 |  | 0.447 | 0.094 |  |\n|  | **Centralized** | 3 rounds | 0.300 | **0.153** |  | 0.185 | 0.023 |  | **0.443** | 0.178 |  | 0.447 | **0.094** |  |\n|  |  | 1 rounds | 0.300 | 0.152 |  | **0.186** | 0.023 |  | 0.442 | 0.178 |  | **0.449** | 0.093 |  |\n| **3-LLMs** <br> GPT-3.5 Evaluator | **Decentralized** | 3 rounds | 0.300 | 0.154 |  | 0.184 | 0.024 |  | 0.446 | 0.179 |  | 0.443 | 0.094 |  |\n|  |  | 1 rounds | **0.309** | **0.159** |  | **0.193** | **0.027** |  | **0.451** | **0.182** |  | **0.459** | **0.099** |  |\n|  | **Centralized** | 3 rounds | 0.294 | 0.151 |  | 0.177 | 0.023 |  | 0.451 | 0.181 |  | 0.440 | 0.095 |  |\n|  |  | 1 rounds | **0.329** | **0.172** |  | **0.214** | **0.036** |  | **0.460** | **0.189** |  | **0.451** | **0.104** |  |", "caption": "Table 6: Multi-LLM framework with three models. We bold the best results for each combination of the experimental variables, and we underline the best results overall. For ease of comparison, we reproduce the best-performing 2-LLM results obtained in Table\u00a02", "description": "\ud45c 6\uc740 \uc138 \uac1c\uc758 \uc5b8\uc5b4 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\ub294 \ub2e4\uc911 LLM \ud504\ub808\uc784\uc6cc\ud06c\uc758 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc2e4\ud5d8 \ubcc0\uc218(\ubaa8\ub378 \uc870\ud569, \ub77c\uc6b4\ub4dc \uc218, \uc911\uc559/\ubd84\uc0b0 \ubc29\uc2dd \ub4f1)\uc758 \uac01 \uc870\ud569\uc5d0 \ub300\ud574 \uac00\uc7a5 \uc88b\uc740 \uacb0\uacfc\ub97c \uad75\uac8c \ud45c\uc2dc\ud558\uace0, \uc804\ubc18\uc801\uc73c\ub85c \uac00\uc7a5 \uc88b\uc740 \uacb0\uacfc\ub294 \ubc11\uc904\ub85c \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4.  \ube44\uad50\ub97c \uc6a9\uc774\ud558\uac8c \ud558\uae30 \uc704\ud574 \ud45c 2\uc5d0\uc11c \uc5bb\uc740 \ucd5c\uace0 \uc131\ub2a5\uc758 2-LLM \uacb0\uacfc\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc2e4\ud5d8 \uc124\uc815\uc5d0\uc11c \uc138 \uac1c\uc758 LLM\uc744 \uc0ac\uc6a9\ud558\ub294 \ubaa8\ub378\uc774 2\uac1c\uc758 LLM\uc744 \uc0ac\uc6a9\ud558\ub294 \ubaa8\ub378\uc5d0 \ube44\ud574 \uc5bc\ub9c8\ub098 \ub354 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "6. Experiments"}, {"content": "|                       |                         |               | ROUGE-1 \u2191 | ROUGE-L \u2191 |   | BLEU-1 \u2191 | BLEU-4 \u2191 |   | ROUGE-1 \u2191 | ROUGE-L \u2191 |   | BLEU-1 \u2191 | BLEU-4 \u2191 |   |\n|-----------------------|-------------------------|---------------|----------:|----------:|---|----------:|----------:|---|----------:|----------:|---|----------:|----------:|---|\n| **Baseline Prompts** | **Decentralized**       | 3 round max   |   0.313   |   0.163   |   |   0.200   |   0.029   |   |   0.447   |   0.180   |   |   0.458   |   0.098   |   |\n|                       |                         | 1 round max   |  **0.339** |  **0.180** |   |  **0.224** |  **0.043** |   |  **0.468** |  **0.190** |   |  **0.477** |  **0.112** |   |\n|                       | **Centralized**         | 3 round max   |   0.329   |   0.168   |   |   0.217   |   0.031   |   |   0.468   |   0.189   |   |   0.470   |   0.109   |   |\n|                       |                         | 1 round max   |  **0.333** |  **0.173** |   |  **0.219** |  **0.036** |   |  **0.479** |  **0.197** |   |  **0.485** |  **0.121** |   |\n| **Specialized Prompts** | **Decentralized**       | 3 round max   |   0.300   |   0.155   |   |   0.201   |   0.025   |   |   0.464   |   0.174   |   |   0.441   |   0.093   |   |\n|                       |                         | 1 round max   |  **0.338** |  **0.175** |   |  **0.236** |  **0.040** |   |  **0.469** |  **0.181** |   |  **0.486** |  **0.104** |   |\n|                       | **Centralized**         | 3 round max   |   0.316   |   0.162   |   |   0.215   |   0.032   |   |   0.473   |   0.177   |   |   0.452   |   0.101   |   |\n|                       |                         | 1 round max   |  **0.355** |  **0.181** |   |  **0.251** |  **0.049** |   |  **0.482** |  **0.185** |   |  **0.494** |  **0.115** |   |", "caption": "Table 7: Results on the use of 2 specialized prompts on where the only change in the pipeline is that 4 total specialized baseline summaries are fed in initially instead of the 2 simple prompts fed in the methodology used to curate Table\u00a02. Note that these results use GPT-3.5 for the evaluator in the centralized approach, and for breaking ties in the decentralized multi-LLM approaches. This is for a 15 sample size for both datasets. Refer to Figure\u00a07 and Figure\u00a08 for the prompts used for initial generation. We bold the best results for each combination of the experimental variables, and we underline the best results overall.", "description": "\ud45c 7\uc740 \uae30\uc874 \ubc29\ubc95(\ud45c 2)\uc5d0\uc11c \uc0ac\uc6a9\ub41c 2\uac1c\uc758 \ub2e8\uc21c \ud504\ub86c\ud504\ud2b8 \ub300\uc2e0, 4\uac1c\uc758 \ud2b9\uc218\ud654\ub41c \uae30\ubcf8 \uc694\uc57d\uc744 \ucd08\uae30 \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc911\uc559 \uc9d1\uc911\uc2dd \ubc29\ubc95\uc5d0\uc11c\ub294 \ud3c9\uac00\uc790\ub85c GPT-3.5\ub97c, \ubd84\uc0b0 \ubc29\uc2dd\uc5d0\uc11c\ub294 \ub3d9\uc810 \ud574\uc18c\ub97c \uc704\ud574 GPT-3.5\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \ub450 \ub370\uc774\ud130\uc14b \ubaa8\ub450\uc5d0 \ub300\ud574 \uc0d8\ud50c \ud06c\uae30\ub294 15\uc785\ub2c8\ub2e4. \ucd08\uae30 \uc0dd\uc131\uc5d0 \uc0ac\uc6a9\ub41c \ud504\ub86c\ud504\ud2b8\ub294 \uadf8\ub9bc 7\uacfc \uadf8\ub9bc 8\uc744 \ucc38\uc870\ud558\uc2ed\uc2dc\uc624. \uc2e4\ud5d8 \ubcc0\uc218 \uc870\ud569\ubcc4 \ucd5c\uc0c1\uc758 \uacb0\uacfc\ub294 \uad75\uac8c \ud45c\uc2dc\ud558\uace0, \uc804\ubc18\uc801\uc73c\ub85c \ucd5c\uc0c1\uc758 \uacb0\uacfc\ub294 \ubc11\uc904\ub85c \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4.", "section": "6.3 \ud2b9\uc9d5 \ubd84\uc11d \uc5f0\uad6c"}, {"content": "| Text Length | Model Type | Round | ROUGE-1 \u2191 | ROUGE-L \u2191 | BLEU-1 \u2191 | BLEU-4 \u2191 |\n|---|---|---|---|---|---|---|\n| Long Text | Decentralized | Multi-LLM 3 round max | 0.329 | 0.168 | 0.217 | 0.031 |\n|  |  | Multi-LLM 1 round max | **0.333** | **0.173** | **0.219** | **0.036** |\n|  | Centralized | Multi-LLM 3 round max | 0.313 | 0.163 | 0.200 | 0.029 |\n|  |  | Multi-LLM 1 round max | **0.338** | **0.180** | **0.224** | **0.043** |\n| Short Text | Decentralized | Multi-LLM 3 round max | 0.360 | 0.188 | **0.328** | 0.038 |\n|  |  | Multi-LLM 1 round max | **0.369** | **0.198** | 0.309 | **0.044** |\n|  | Centralized | Multi-LLM 3 round max | 0.367 | 0.194 | **0.321** | 0.041 |\n|  |  | Multi-LLM 1 round max | **0.379** | **0.206** | 0.305 | **0.049** |", "caption": "Table 8: Results on short summarization tasks using the ArXiv dataset for the decentralized and centralized Multi-LLM approaches. Note that these results use\nGPT-3.5 for the evaluator in the centralized approach, and for breaking ties in the decentralized multi-LLM\napproaches.", "description": "\ud45c 8\uc740 ArXiv \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubd84\uc0b0 \ubc0f \uc911\uc559 \uc9d1\uc911\uc2dd Multi-LLM \uc811\uadfc \ubc29\uc2dd\uc5d0 \ub300\ud55c \uc9e7\uc740 \uc694\uc57d \uc791\uc5c5\uc758 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uae34 \ubb38\uc11c\uc5d0 \ub300\ud55c \uc694\uc57d\uacfc \uc9e7\uc740 \ubb38\uc11c\uc5d0 \ub300\ud55c \uc694\uc57d \ubaa8\ub450\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uba70, ROUGE-1, ROUGE-L, BLEU-1, BLEU-4 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc694\uc57d \ud488\uc9c8\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uc911\uc559 \uc9d1\uc911\uc2dd \uc811\uadfc \ubc29\uc2dd\uc5d0\uc11c\ub294 GPT-3.5\ub97c \ud3c9\uac00\uc790\ub85c \uc0ac\uc6a9\ud558\uace0, \ubd84\uc0b0\ub41c Multi-LLM \uc811\uadfc \ubc29\uc2dd\uc5d0\uc11c\ub294 \ub3d9\uc810\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574 GPT-3.5\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 \uc811\uadfc \ubc29\uc2dd\uacfc \uc694\uc57d \uae38\uc774\uc5d0 \ub530\ub978 \uc5ec\ub7ec \uce21\uc815\ud56d\ubaa9(ROUGE, BLEU)\uc758 \uc810\uc218\uac00 \ub098\uc640\uc788\uc5b4, \ubaa8\ub378 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294\ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4. \ud2b9\ud788, \uae34 \ubb38\uc11c\uc640 \uc9e7\uc740 \ubb38\uc11c\uc5d0 \ub300\ud55c \uc131\ub2a5 \ucc28\uc774\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc810\uc774 \uc911\uc694\ud569\ub2c8\ub2e4.", "section": "6 Experiments"}]