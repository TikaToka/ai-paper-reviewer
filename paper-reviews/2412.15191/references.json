{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a foundational model for cross-modal understanding that heavily influences the current work's approach to audio-video generation."}, {"fullname_first_author": "Rohit Girdhar", "paper_title": "ImageBind: One embedding space to bind them all", "publication_date": "2023-00-00", "reason": "ImageBind provides a unified embedding space for various modalities, inspiring the paper's approach to cross-modal conditioning using diffusion model activations."}, {"fullname_first_author": "Adam Polyak", "paper_title": "Movie Gen: A Cast of Media Foundation Models", "publication_date": "2024-00-00", "reason": "Movie Gen serves as a strong baseline and direct comparison for the proposed AV-Link model, particularly in its approach to cross-modal generation."}, {"fullname_first_author": "Simian Luo", "paper_title": "Diff-Foley: Synchronized video-to-audio synthesis with latent diffusion models", "publication_date": "2024-00-00", "reason": "Diff-Foley is a key competitor to AV-Link, focusing on video-to-audio generation and providing a direct point of comparison for evaluating the method's performance."}, {"fullname_first_author": "Yazhou Xing", "paper_title": "Seeing-and-Hearing: Open-domain visual-audio generation with diffusion latent aligners", "publication_date": "2024-00-00", "reason": "This paper tackles both video-to-audio and audio-to-video tasks, offering valuable insights into cross-modal alignment and serving as a strong comparison point for AV-Link."}]}