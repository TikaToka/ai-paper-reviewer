{"references": [{"fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners.", "publication_date": "2020-05-28", "reason": "This paper introduces the concept of few-shot learning in large language models, highlighting their ability to perform tasks with limited examples, which serves as a core foundation for prompt engineering and optimization."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.", "publication_date": "2022-06-27", "reason": "This work introduces chain-of-thought prompting as a technique for eliciting reasoning in large language models, a critical aspect that GREATER builds upon by leveraging gradients over the reasoning process."}, {"fullname_first_author": "Reid Pryzant", "paper_title": "Automatic prompt optimization with \"gradient descent\" and beam search.", "publication_date": "2023-05-03", "reason": "This paper formalizes the concept of \"textual gradient\" for prompt optimization, establishing a key baseline and framework for automated prompt refinement that GREATER compares against."}, {"fullname_first_author": "Yongchao Zhou", "paper_title": "Large language models are human-level prompt engineers.", "publication_date": "2022-11-02", "reason": "This research demonstrates the potential of LLMs as prompt optimizers for less powerful models, initiating a significant line of work in automated prompt engineering that GREATER contributes to by offering a gradient-based approach."}, {"fullname_first_author": "Qinyuan Ye", "paper_title": "Prompt engineering a prompt engineer.", "publication_date": "2023-11-06", "reason": "This paper introduces a methodology for prompt engineering using prompt engineering, showcasing advanced techniques for improving prompt quality and serving as a comparative benchmark for GREATER."}]}