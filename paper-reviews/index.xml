<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Paper Reviews by AI on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/</link><description>Recent content in Paper Reviews by AI on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2025 AI Paper Reviews by AI</copyright><lastBuildDate>Tue, 29 Oct 2024 20:55:37 +0100</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/index.xml" rel="self" type="application/rss+xml"/><item><title>Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03225/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03225/</guid><description>AutoConverter는 오픈엔드 방식의 VQA 질문을 다지선다형 질문으로 자동 변환하는 시스템입니다. 이를 통해 VLM(Vision Language Model) 평가의 객관성과 재현성을 높일 수 있습니다. 연구진은 AutoConverter를 사용하여 20개의 기존 VQA 데이터셋을 통합한 VMCBench라는 새로운 벤치마크를 구축했습니다. VMCBen&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03225/cover.png"/></item><item><title>BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/</guid><description>BoostStep: 단계별 추론으로 LLMs의 수학적 능력 향상!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/cover.png"/></item><item><title>Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03218/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03218/</guid><description>Dispider: 실시간 상호작용을 위해 분리된 인식, 결정, 반응을 사용하는 비디오 LLM을 가능하게 합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03218/cover.png"/></item><item><title>Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured State-Space Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02832/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02832/</guid><description>Mamba 아키텍처 기반의 Samba-ASR은 효율적인 상태 공간 모델을 이용, 기존 Transformer 모델의 한계를 극복하고 음성 인식 분야에서 최첨단 성능을 달성했습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02832/cover.png"/></item><item><title>STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02976/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02976/</guid><description>STAR: T2V 모델 기반 실세계 비디오 초고해상도 기술로 현실적인 공간적 세부 정보와 견고한 시간적 일관성을 달성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02976/cover.png"/></item><item><title>Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03059/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03059/</guid><description>마스크 기반 모션 경로를 이용한 2단계 이미지-비디오 생성 프레임워크인 THROUGH-THE-MASK가 다중 객체의 정확한 애니메이션을 가능하게 합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03059/cover.png"/></item><item><title>TransPixar: Advancing Text-to-Video Generation with Transparency</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/</guid><description>TransPixar: 제한된 데이터로도 고품질 투명 비디오 생성</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/cover.png"/></item><item><title>DepthMaster: Taming Diffusion Models for Monocular Depth Estimation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02576/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02576/</guid><description>DepthMaster는 단일 단계 확산 모델을 이용, 생성적 특징을 활용하여 모노큘러 깊이 추정의 정확도와 속도를 획기적으로 향상시켰습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02576/cover.png"/></item><item><title>GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02690/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02690/</guid><description>GS-DiT: 효율적인 3D 점 추적으로 의사 4D 가우스 필드를 활용, 4D 비디오 제어 가능한 혁신적 비디오 생성 모델</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02690/cover.png"/></item><item><title>Scaling Laws for Floating Point Quantization Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02423/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02423/</guid><description>부동 소수점 양자화 훈련의 새로운 scaling law 발견: 지수, 맨티사 비트 및 스케일링 인자 계산 정밀도가 LLM 성능에 미치는 영향을 정량적으로 규명</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02423/cover.png"/></item><item><title>Test-time Computing: from System-1 Thinking to System-2 Thinking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02497/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02497/</guid><description>테스트 시간 컴퓨팅을 활용하여 대규모 언어 모델의 추론 능력을 시스템 1 사고에서 시스템 2 사고 수준으로 향상시키는 방법을 제시하는 획기적인 연구!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02497/cover.png"/></item><item><title>ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02506/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02506/</guid><description>ToolHop: 대규모 언어 모델의 다중 단계 도구 사용 능력을 엄격히 평가하는 새로운 벤치마크</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02506/cover.png"/></item><item><title>Personalized Graph-Based Retrieval for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02157/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02157/</guid><description>개인화된 그래프 기반 검색 증강 생성(PGraphRAG) 프레임워크를 통해 희소 데이터 문제를 해결하고, LLM의 개인화 성능을 크게 향상시켰습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02157/cover.png"/></item><item><title>Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01830/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01830/</guid><description>AUTO-RT: 자동화된 재밍 전략 탐색으로 LLM 취약점 효율적으로 발견!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01830/cover.png"/></item><item><title>EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01895/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01895/</guid><description>EnerVerse: 로봇 조작을 위한 미래 공간 생성 프레임워크가 장기간 작업에서 성능 향상을 달성했습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01895/cover.png"/></item><item><title>Ingredients: Blending Custom Photos with Video Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01790/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01790/</guid><description>고품질 다중 ID 맞춤형 비디오 생성을 위한 혁신적인 프레임워크, Ingredients 소개!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01790/cover.png"/></item><item><title>METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02045/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02045/</guid><description>70억 개 매개변수를 가진 메타유전체 기반 대규모 언어 모델(METAGENE-1)이 폐수 데이터로 훈련되어 병원균 탐지 및 유전체 서열 임베딩 작업에서 최첨단 성능을 달성했습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02045/cover.png"/></item><item><title>Virgo: A Preliminary Exploration on Reproducing o1-like MLLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01904/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01904/</guid><description>Virgo: 텍스트 기반 장문 사고 데이터를 활용, 다양한 멀티모달 벤치마크에서 최첨단 성능 달성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01904/cover.png"/></item><item><title>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01957/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01957/</guid><description>VITA-1.5: 실시간 시각 및 음성 상호작용을 위한 GPT-40 수준의 다중 모달 LLM</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01957/cover.png"/></item><item><title>A3: Android Agent Arena for Mobile GUI Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01149/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01149/</guid><description>Android Agent Arena(A3): 실제 모바일 앱에서 AI 에이전트의 동적 성능 평가를 위한 혁신 플랫폼</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01149/cover.png"/></item><item><title>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01540/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01540/</guid><description>BoxingGym: LLM 기반 과학적 에이전트의 실험 설계 및 모델 발견 능력 종합 평가 벤치마크</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01540/cover.png"/></item><item><title>CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01257/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01257/</guid><description>CODEELO 벤치마크: 인간 수준의 Elo 등급으로 LLM의 경쟁적 코드 생성 능력 평가</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01257/cover.png"/></item><item><title>Dynamic Scaling of Unit Tests for Code Reward Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01054/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01054/</guid><description>단위 테스트의 수를 늘려 코드 보상 모델의 정확성을 높이는 방법을 제시하는 연구!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01054/cover.png"/></item><item><title>Graph Generative Pre-trained Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01073/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01073/</guid><description>G2PT: 그래프를 시퀀스로 효율적으로 인코딩하고 Transformer로 학습시켜 그래프 생성 및 예측 성능을 획기적으로 향상시킨 새로운 모델!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01073/cover.png"/></item><item><title>Nested Attention: Semantic-aware Attention Values for Concept Personalization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01407/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01407/</guid><description>중첩 주의 메커니즘을 사용하여 텍스트-이미지 모델의 개인화 성능을 향상시킨 Nested Attention 기법 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01407/cover.png"/></item><item><title>Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01423/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01423/</guid><description>고차원 잠재 공간에서의 최적화 딜레마를 해결하는 VA-VAE를 통해, 고해상도 이미지 생성에서 최첨단 성능을 달성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01423/cover.png"/></item><item><title>SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01320/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01320/</guid><description>SeedVR: 무한한 확산 트랜스포머로 일반적인 비디오 복원 향상</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01320/cover.png"/></item><item><title>SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01245/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01245/</guid><description>SeFAR: 제한된 데이터로도 정밀 동작 인식의 성능을 획기적으로 향상시키는 새로운 세미-슈퍼바이즈드 학습 프레임워크!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01245/cover.png"/></item><item><title>VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/</guid><description>VideoAnydoor: 정밀한 모션 제어를 갖춘 고품질 영상 객체 삽입</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/cover.png"/></item><item><title>2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00958/</link><pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00958/</guid><description>2.5년 분량의 교육 비디오를 활용, 고품질 다중 모달 텍스트북 코퍼스 구축 및 VLMs 사전 학습 성능 향상</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00958/cover.png"/></item><item><title>AutoPresent: Designing Structured Visuals from Scratch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00912/</link><pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00912/</guid><description>AUTOPRESENT: 자연어 명령어로 완벽한 프레젠테이션 슬라이드 자동 생성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00912/cover.png"/></item><item><title>Population Aware Diffusion for Time Series Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00910/</link><pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00910/</guid><description>인구 수준 특징 보존 시계열 생성을 위한 새로운 확산 모델 PaD-TS 제안</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00910/cover.png"/></item><item><title>Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00712/</link><pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00712/</guid><description>TAPE(conTextualized equivAriant Position Embedding) 프레임워크를 통해 문맥 정보를 활용한 동적 위치 인코딩으로 트랜스포머의 위치 기반 주소 지정 성능을 향상시켰습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00712/cover.png"/></item><item><title>MLLM-as-a-Judge for Image Safety without Human Labeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00192/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00192/</guid><description>인간 라벨링 없이 사전 정의된 안전 규칙을 사용하여 사전 훈련된 다중 모달 대형 언어 모델(MLLM)을 통해 이미지 안전성을 판단하는 새로운 제로샷 방법을 제시합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00192/cover.png"/></item><item><title>Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00658/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00658/</guid><description>심층 신경망의 장기 의존성을 모델링하는 구조적 상태 공간 모델(SSM)의 한계를 극복! 최신 연구에서 SSM의 &lt;strong>최근 편향(recency bias)&lt;/strong> 및 &lt;strong>과도한 평활화(over-smoothing)&lt;/strong> 문제를 규명하고, 이를 해결하는 **극성화 기법(polarization)**을 제시하여 장기 토큰 상관관계 정확도를 높였습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00658/cover.png"/></item><item><title>VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00599/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00599/</guid><description>VideoRefer Suite는 &lt;strong>정교한 공간-시간적 개체 이해를 위한 새로운 비디오 LLM(VideoRefer)과 대규모 고품질 데이터셋(VideoRefer-700K), 종합적인 벤치마크(VideoRefer-Bench)를 제시&lt;/strong>합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00599/cover.png"/></item><item><title>Are Vision-Language Models Truly Understanding Multi-vision Sensor?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20750/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20750/</guid><description>멀티 비전 센서 데이터에 대한 VLMs의 이해도 향상을 위한 새로운 벤치마크(MS-PR)와 DNA 최적화 기법 제시</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20750/cover.png"/></item><item><title>Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21187/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21187/</guid><description>대규모 언어 모델의 과도한 연산 문제 해결: 효율적인 추론을 위한 새로운 지표 및 자기 학습 전략 제시</description></item><item><title>Edicho: Consistent Image Editing in the Wild</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/</guid><description>Edicho: 이미지 간 일관성 유지하며 제로샷 이미지 편집 가능!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/cover.png"/></item><item><title>Efficiently Serving LLM Reasoning Programs with Certaindex</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20993/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20993/</guid><description>Dynasor은 LLM 추론 프로그램의 자원 사용을 최적화하는 시스템으로, &lt;strong>certaindex&lt;/strong>라는 새로운 지표를 활용하여 어려운 질의에는 더 많은 연산을, 간단한 질의에는 적은 연산을 할당하고, 전망이 없는 질의는 조기에 종료함으로써 정확도, 지연 시간 및 비용을 균형 있게 맞춥니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20993/cover.png"/></item><item><title>Facilitating large language model Russian adaptation with Learned Embedding Propagation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21140/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21140/</guid><description>LEP(Learned Embedding Propagation)는 적은 양의 학습 데이터만으로도 다국어 대규모 언어 모델을 효율적으로 적응시키는 새로운 기법입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21140/cover.png"/></item><item><title>HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21199/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21199/</guid><description>LLM의 점진적 추론 및 문제 해결 능력을 평가하기 위한 새로운 벤치마크 HumanEval Pro, MBPP Pro, BigCodeBench-Lite Pro 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21199/cover.png"/></item><item><title>HUNYUANPROVER: A Scalable Data Synthesis Framework and Guided Tree Search for Automated Theorem Proving</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20735/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20735/</guid><description>HunyuanProver: 대규모 언어 모델 기반의 확장 가능한 데이터 합성 프레임워크와 안내 트리 탐색을 통해 최첨단 자동 정리 증명 성능 달성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20735/cover.png"/></item><item><title>LTX-Video: Realtime Video Latent Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00103/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00103/</guid><description>LTX-Video: 초고속 실시간 고해상도 비디오 생성 모델</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00103/cover.png"/></item><item><title>MapQaTor: A System for Efficient Annotation of Map Query Datasets</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21015/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21015/</guid><description>MAPQATOR: 플러그앤플레이 방식의 지리공간 질의응답 데이터셋 생성 시스템</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21015/cover.png"/></item><item><title>Slow Perception: Let's Perceive Geometric Figures Step-by-step</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20631/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20631/</guid><description>느린 지각(Slow Perception): 단계별 기하학적 도형 인식으로 정확도 향상</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20631/cover.png"/></item><item><title>TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21037/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21037/</guid><description>TANGOFLUX: 적은 매개변수로 초고속, 고품질 텍스트 음성 변환</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21037/cover.png"/></item><item><title>Training Software Engineering Agents and Verifiers with SWE-Gym</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21139/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21139/</guid><description>SWE-Gym: 현실 세계 소프트웨어 엔지니어링 에이전트 훈련을 위한 최초의 환경</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21139/cover.png"/></item><item><title>VMix: Improving Text-to-Image Diffusion Model with Cross-Attention Mixing Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20800/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20800/</guid><description>VMix: 크로스 어텐션 믹싱 제어를 통한 텍스트-이미지 확산 모델 개선</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20800/cover.png"/></item><item><title>Bringing Objects to Life: 4D generation from 3D objects</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20422/</link><pubDate>Sun, 29 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20422/</guid><description>3to4D: 텍스트 프롬프트로 사용자 제공 3D 객체를 실감나게 애니메이션화!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20422/cover.png"/></item><item><title>On the Compositional Generalization of Multimodal LLMs for Medical Imaging</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20070/</link><pubDate>Sat, 28 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20070/</guid><description>의료 영상에 대한 다중 모드 거대 언어 모델의 일반화 능력 향상에 구성적 일반화(CG)가 핵심 역할을 수행하며, 제한된 데이터에서도 효과적임을 밝힘.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20070/cover.png"/></item><item><title>OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/</link><pubDate>Sat, 28 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/</guid><description>OneKE: 도커 기반, 다중 에이전트 LLM 지식 추출 시스템으로 웹, PDF에서 다양한 도메인 지식 추출 가능</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/cover.png"/></item><item><title>From Elements to Design: A Layered Approach for Automatic Graphic Design Composition</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19712/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19712/</guid><description>LaDeCo: 계층적 접근 방식을 사용한 자동 그래픽 디자인 합성</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19712/cover.png"/></item><item><title>OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19723/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19723/</guid><description>OS-Genesis는 역방향 작업 합성을 통해 GUI 에이전트 궤적 생성 자동화 문제를 해결하는 혁신적인 파이프라인입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19723/cover.png"/></item><item><title>Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19512/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19512/</guid><description>미세 조정으로 안전성이 저하된 LLM의 성능을 향상시키는 동시에 안전성을 유지하는 간편하고 효과적인 모델 결합 방법 제시!</description></item><item><title>VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/</guid><description>VideoMaker: 영상 확산 모델의 고유한 힘을 이용한 제로샷 맞춤형 영상 생성</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/cover.png"/></item><item><title>Xmodel-2 Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19638/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19638/</guid><description>Xmodel-2: 12억 매개변수의 추론 전문 대규모 언어 모델로, 효율적인 설계와 훈련 전략을 통해 최첨단 성능 달성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19638/cover.png"/></item><item><title>Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19326/</link><pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19326/</guid><description>시각적 과제 정렬을 통한 작업 선호도 최적화(TPO)로 멀티모달 대규모 언어 모델의 성능을 획기적으로 향상시켰습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19326/cover.png"/></item><item><title>1.58-bit FLUX</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18653/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18653/</guid><description>1.58-bit FLUX: 99.5%의 파라미터를 1.58-bit로 양자화하여 모델 크기 7.7배, 추론 메모리 5.1배 감소, 고품질 이미지 생성 유지!</description></item><item><title>3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18450/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18450/</guid><description>3DGraphLLM: 의미론적 그래프와 거대 언어 모델을 결합하여 3D 장면 이해 성능을 획기적으로 향상시킨 최첨단 연구!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18450/cover.png"/></item><item><title>CypherBench: Towards Precise Retrieval over Full-scale Modern Knowledge Graphs in the LLM Era</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18702/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18702/</guid><description>본 연구는 대규모 현대 지식 그래프에서 LLM을 이용한 정확한 정보 검색을 위한 새로운 벤치마크인 CypherBench를 제시합니다. 기존의 RDF 기반 지식 그래프는 과도하게 큰 스키마와 리소스 식별자 사용으로 LLM에 비효율적이라는 문제점을 분석합니다. 특히, Wikidata와 같은 현대 지식 그래프는 LLM의 문맥 창 크기를 초과하는 경우가 많습니&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18702/cover.png"/></item><item><title>DepthLab: From Partial to Complete</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18153/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18153/</guid><description>DepthLab: 부분 깊이 정보로 완전한 3D 시각 정보 복원</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18153/cover.png"/></item><item><title>DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18597/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18597/</guid><description>DiTCtrl: 튜닝 없이 다중 프롬프트로 매끄러운 장시간 비디오 생성</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18597/cover.png"/></item><item><title>How "Real" is Your Real-Time Simultaneous Speech-to-Text Translation System?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18495/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18495/</guid><description>실시간 동시 통역 시스템의 현실적인 한계를 규명하고, 표준화된 용어와 체계를 제시하여 연구 발전을 촉진하는 논문.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18495/cover.png"/></item><item><title>MMFactory: A Universal Solution Search Engine for Vision-Language Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18072/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18072/</guid><description>MMFactory: 사용자 맞춤형 비전-언어 작업 솔루션 검색 엔진</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18072/cover.png"/></item><item><title>Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18176/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18176/</guid><description>Molar: 멀티모달 LLM과 협업 필터링을 결합하여 시퀀셜 추천 성능을 획기적으로 향상시킨 혁신적인 프레임워크!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18176/cover.png"/></item><item><title>Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18319/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18319/</guid><description>Mulberry는 집단 몬테 카를로 트리 탐색(CoMCTS)을 이용, 단계적 추론 및 반성 능력을 갖춘 다중 모드 대규모 언어 모델(MLLM)을 개발한 연구입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18319/cover.png"/></item><item><title>Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/</guid><description>단일 이미지에서 객체 방향 추정의 정확도를 크게 높이는 &amp;lsquo;Orient Anything&amp;rsquo; 모델 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/cover.png"/></item><item><title>PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18608/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18608/</guid><description>PartGen: 다중 뷰 확산 모델을 이용, 텍스트, 이미지, 기존 3D 객체로부터 의미있는 부분으로 구성된 고품질 3D 객체 생성 및 재구성.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18608/cover.png"/></item><item><title>Token-Budget-Aware LLM Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18547/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18547/</guid><description>토큰 예산 인식 LLM 추론 프레임워크(TALE)를 통해 LLM 추론의 토큰 비용을 크게 줄이면서 성능 저하를 최소화했습니다!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18547/cover.png"/></item><item><title>Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18609/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18609/</guid><description>Video-Panda: 초경량 인코더 없는 비디오-언어 모델로, 계산 비용을 획기적으로 줄이면서 최첨단 성능을 달성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18609/cover.png"/></item><item><title>B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/</guid><description>B-STAR: 자기 학습 추론자에서 탐색과 활용의 균형을 모니터링하고 조정하여 성능을 향상시키는 새로운 프레임워크</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/cover.png"/></item><item><title>Deliberation in Latent Space via Differentiable Cache Augmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/</guid><description>대규모 언어 모델의 추론 성능을 향상시키는 새로운 방법인 ‘차별 가능한 캐시 증강’ 기법 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/cover.png"/></item><item><title>Diving into Self-Evolving Training for Multimodal Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/</guid><description>M-STAR: 다모달 추론을 위한 자기 진화 훈련의 새로운 프레임워크를 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/cover.png"/></item><item><title>DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17498/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17498/</guid><description>DRT-01 모델은 장문의 사고 과정을 활용하여 문학 번역의 정확도와 유창성을 크게 향상시켰습니다.</description></item><item><title>Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17739/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17739/</guid><description>FoPE: 주파수 영역 특징 개선으로 긴 문맥 길이 일반화 달성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17739/cover.png"/></item><item><title>Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17295/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17295/</guid><description>Friends-MMC: 방대한 비디오 데이터와 주석을 포함한 새로운 다중 모달 다중 참여 대화 데이터셋을 통해 실제 세계의 대화 이해를 위한 새로운 가능성을 제시합니다!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17295/cover.png"/></item><item><title>In Case You Missed It: ARC 'Challenge' Is Not That Challenging</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17758/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17758/</guid><description>기존 다중 선택 문제 평가 방식의 오류를 지적하고, 모든 옵션을 함께 고려하는 새로운 평가 방식을 제안하여 모델 성능 평가의 정확성을 높였습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17758/cover.png"/></item><item><title>Large Motion Video Autoencoding with Cross-modal Video VAE</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17805/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17805/</guid><description>고품질 영상 생성 및 효율적 압축을 위한 혁신적인 크로스 모달 비디오 VAE!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17805/cover.png"/></item><item><title>PC Agent: While You Sleep, AI Works -- A Cognitive Journey into Digital World</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17589/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17589/</guid><description>PC Agent는 인간의 인지 과정을 AI 에 전이하여 복잡한 디지털 작업을 자동화하는 혁신적인 시스템입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17589/cover.png"/></item><item><title>ResearchTown: Simulator of Human Research Community</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17767/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17767/</guid><description>RESEARCHTOWN: LLM 기반 인간 연구 공동체 시뮬레이터로, 다양한 연구 활동을 현실적으로 모방하며 학제 간 연구 아이디어 생성 가능</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17767/cover.png"/></item><item><title>SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17606/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17606/</guid><description>SBS Figures: 100만 개의 합성 이미지와 QA 쌍으로 사전 학습된, 효율적인 Figure QA 모델!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17606/cover.png"/></item><item><title>VidTwin: Video VAE with Decoupled Structure and Dynamics</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17726/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17726/</guid><description>VidTwin: 구조와 동역학을 분리하여 비디오 압축 및 생성의 새로운 기준을 제시하는 혁신적인 비디오 자동 인코더!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17726/cover.png"/></item><item><title>WavePulse: Real-time Content Analytics of Radio Livestreams</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17998/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17998/</guid><description>WavePulse: 실시간 라디오 방송 콘텐츠 분석 프레임워크가 정치적 담론, 미디어 유통, 여론 동향을 실시간 분석하여 정치 과학 및 미디어 연구에 새로운 가능성을 열었습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17998/cover.png"/></item><item><title>YuLan-Mini: An Open Data-efficient Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17743/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17743/</guid><description>YuLan-Mini: 24억 개 매개변수를 가진 데이터 효율적인 개방형 LLM</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17743/cover.png"/></item><item><title>Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17153/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17153/</guid><description>단일 단계 샘플링으로 이미지 자동 회귀 모델 속도를 획기적으로 향상시킨 증류 디코딩(DD) 기법 제안!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17153/cover.png"/></item><item><title>OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/</guid><description>OpenRFT는 제한된 도메인 특정 데이터를 사용하여 일반적인 추론 모델을 미세 조정하는 새로운 방법을 제시합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/cover.png"/></item><item><title>Revisiting In-Context Learning with Long Context Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/</guid><description>장문 컨텍스트 언어 모델에서 정교한 샘플 선택 전략보다 &lt;strong>무작위 샘플링&lt;/strong>이 ICL 성능 향상에 더 효과적이며, &lt;strong>데이터 증강&lt;/strong>을 통해 저자원 작업 성능을 5% 향상시켰다는 놀라운 연구 결과를 발표!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/cover.png"/></item><item><title>LearnLM: Improving Gemini for Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16429/</link><pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16429/</guid><description>LearnLM은 교육적 맥락에서 생성형 AI의 페다고지(Pedagogy)를 향상시킨 모델입니다. &lt;strong>교사나 개발자가 원하는 페다고지적 특성을 모델에 주입하는 새로운 프레임워크&lt;/strong>를 통해 기존 모델보다 학습 효과를 31% 향상시켰습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16429/cover.png"/></item><item><title>NILE: Internal Consistency Alignment in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/</link><pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/</guid><description>NILE 프레임워크는 LLM의 내부 지식과 IFT 데이터셋의 세계 지식 간 일관성을 높여 LLM 성능을 최대 68.5%까지 향상시킵니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/cover.png"/></item><item><title>CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16112/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16112/</guid><description>CLEAR: 선형화된 어텐션으로 고해상도 이미지 생성 속도를 획기적으로 높이다!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16112/cover.png"/></item><item><title>Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15797/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15797/</guid><description>대규모 언어 모델들의 앙상블을 통해 복잡한 추론 문제를 더욱 효과적으로 해결하는 새로운 프레임워크, LE-MCTS를 제안합니다!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15797/cover.png"/></item><item><title>MotiF: Making Text Count in Image Animation with Motion Focal Loss</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16153/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16153/</guid><description>MotiF: 움직임에 초점을 맞춘 손실 함수로 텍스트 기반 이미지 애니메이션 개선</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16153/cover.png"/></item><item><title>Multi-LLM Text Summarization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15487/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15487/</guid><description>다수의 거대 언어 모델(LLM)을 활용한 혁신적인 장문 요약 프레임워크가 제시되어 요약 품질을 최대 3배 향상시켰습니다!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15487/cover.png"/></item><item><title>Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15484/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15484/</guid><description>초정밀 이미지 캡션 생성의 환각 문제 해결을 위해, LLM-MLLM 협업 기반의 다중 에이전트 시스템(CapMAS)을 제안하여 사실성과 포괄성을 높였습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15484/cover.png"/></item><item><title>AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/</guid><description>AceMath는 사전 훈련 및 보상 모델링을 통해 최첨단 수학 추론 능력을 달성한 프런티어급 모델 시리즈입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/cover.png"/></item><item><title>Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14462/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14462/</guid><description>Affordance-Aware Object Insertion: 배경과 전경의 상호작용을 고려한 현실적인 이미지 합성 기술!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14462/cover.png"/></item><item><title>AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15191/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15191/</guid><description>AV-Link: 시간 정렬 확산 기능을 통한 크로스 모달 오디오-비디오 생성의 획기적인 발전!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15191/cover.png"/></item><item><title>DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15200/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15200/</guid><description>DI-PCG는 이미지 조건으로부터 고품질 3D 자산을 효율적으로 생성하기 위해 경량화된 확산 변환기 모델을 활용한 혁신적인 역방향 절차적 콘텐츠 생성 방법론입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15200/cover.png"/></item><item><title>Fietje: An open, efficient LLM for Dutch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/</guid><description>Fietje: 오픈소스 소형 네덜란드어 LLM 공개!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/cover.png"/></item><item><title>Flowing from Words to Pixels: A Framework for Cross-Modality Evolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15213/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15213/</guid><description>CrossFlow: 모달리티 간 직접적 변환 가능한 혁신적 프레임워크!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15213/cover.png"/></item><item><title>How to Synthesize Text Data without Model Collapse?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/</guid><description>합성 데이터 기반 언어 모델 학습의 붕괴 문제 해결: 토큰 편집 기법 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/cover.png"/></item><item><title>IDOL: Instant Photorealistic 3D Human Creation from a Single Image</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14963/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14963/</guid><description>단일 이미지에서 초고속, 고품질, 애니메이션 가능한 3D 아바타를 생성하는 IDOL 모델 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14963/cover.png"/></item><item><title>LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/</guid><description>LeviTor: 사용자의 간편한 3D 궤적 입력만으로 사실적인 비디오 합성이 가능한 혁신적인 모델!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/cover.png"/></item><item><title>LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/</guid><description>M-ALERT는 다국어 LLM의 안전성을 평가하기 위한 새로운 벤치마크입니다. 영어, 프랑스어, 독일어, 이탈리아어, 스페인어 5개 언어의 75,000개 프롬프트를 포함하며, 다양한 언어 및 범주에서 LLM의 안전성 불일치를 밝혀냈습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/cover.png"/></item><item><title>MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14475/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14475/</guid><description>MegaPairs는 VLM과 공개 도메인 이미지를 활용, 2600만 개 이상의 고품질 다중 모달 학습 데이터를 생성하여 범용 다중 모달 검색 성능을 획기적으로 향상시켰습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14475/cover.png"/></item><item><title>MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/</guid><description>MixLLM: 출력 특징 간의 전역 혼합 정밀도 양자화와 고효율 시스템 설계를 통해 LLM의 정확도와 효율성을 동시에 향상시키는 획기적인 양자화 방법</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/cover.png"/></item><item><title>Outcome-Refining Process Supervision for Code Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/</guid><description>복잡한 알고리즘 추론이 필요한 코드 생성 과제에서 기존의 한계를 극복하는 새로운 방법론, Outcome-Refining Process Supervision (ORPS) 제시</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/cover.png"/></item><item><title>Parallelized Autoregressive Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/</guid><description>본 연구는 토큰 의존성을 고려한 병렬화 전략을 통해 자동 회귀 시각적 생성의 속도를 최대 9.5배까지 향상시켰습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/cover.png"/></item><item><title>Progressive Multimodal Reasoning via Active Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14835/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14835/</guid><description>AR-MCTS: 능동적 검색과 몬테 카를로 트리 탐색으로 멀티모달 추론 향상</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14835/cover.png"/></item><item><title>ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14711/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14711/</guid><description>ReLU 라우팅을 사용하는 완전 미분 가능한 MoE 아키텍처 ReMoE를 통해 대규모 언어 모델의 확장성과 효율성을 획기적으로 개선했습니다!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14711/cover.png"/></item><item><title>RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/</guid><description>ROBUSTFT는 잡음이 포함된 응답 아래에서 대규모 언어 모델의 강건한 지도 학습 미세 조정을 위한 프레임워크로, 잡음 감지 및 재라벨링을 통해 하류 작업 성능을 향상시킵니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/cover.png"/></item><item><title>Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15322/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15322/</guid><description>고품질 비디오-오디오 합성을 위한 혁신적인 다중 모드 조인트 학습 프레임워크 MMAudio 제안!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15322/cover.png"/></item><item><title>TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14642/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14642/</guid><description>TOMG-Bench: LLM 기반 오픈 분자 생성 벤치마크 제시! 25개 LLM 평가 및 새로운 instruction tuning 데이터셋 OpenMolIns 공개로, 오픈소스 LLM의 성능 향상 및 분자 발견의 새로운 가능성 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14642/cover.png"/></item><item><title>UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15216/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15216/</guid><description>비지도 학습 기반 순환 편집 일관성(CEC) 활용, 지시어 기반 이미지 편집의 새로운 지평을 열다!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15216/cover.png"/></item><item><title>AniDoc: Animation Creation Made Easier</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/</guid><description>AniDoc: 희소 스케치와 참조 이미지를 활용, 2D 애니메이션 자동 채색 및 보간을 구현하는 혁신적 AI 모델!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/cover.png"/></item><item><title>AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/</guid><description>AntiLeak-Bench: 자동화된 벤치마킹으로 LLM 데이터 오염 방지</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/cover.png"/></item><item><title>Autoregressive Video Generation without Vector Quantization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14169/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14169/</guid><description>벡터 양자화 없이도 효율적이고 유연한 자기회귀 비디오 생성 모델, NOVA 개발!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14169/cover.png"/></item><item><title>Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14233/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14233/</guid><description>시각 전문가 모델을 활용한 이미지 캡션 향상으로 다중 모달 모델 성능 개선</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14233/cover.png"/></item><item><title>FashionComposer: Compositional Fashion Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14168/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14168/</guid><description>FashionComposer: 다양한 입력(텍스트, 의상 이미지, 3D 모델)을 활용해 사실적인 패션 이미지를 합성하는 혁신적인 프레임워크!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14168/cover.png"/></item><item><title>GUI Agents: A Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13501/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13501/</guid><description>대규모 언어 모델 기반 GUI 에이전트 기술의 최신 동향을 종합적으로 분석하고, 벤치마크, 평가 지표, 아키텍처, 학습 방법을 체계적으로 분류하여 통합 프레임워크를 제시합니다.</description></item><item><title>LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13871/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13871/</guid><description>LLaVA-UHD v2는 계층적 윈도우 변환기를 이용, 고해상도 특징 피라미드를 통합하여 다양한 시각적 세부 정보를 포착하는 혁신적인 다중 모달 언어 모델입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13871/cover.png"/></item><item><title>PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14283/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14283/</guid><description>PixelMan은 픽셀 조작 및 생성을 통해 훈련 없이도 일관성 있는 객체 편집을 16단계 만에 달성하는 혁신적인 확산 모델 기반 방법입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14283/cover.png"/></item><item><title>Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/</guid><description>저렴한 라이다 프롬프트를 사용한 4K 고해상도 정확한 계량적 깊이 추정을 위한 새로운 패러다임, Prompt Depth Anything 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/cover.png"/></item><item><title>RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/</guid><description>RAG-RewardBench: RAG 환경에서 보상 모델 평가를 위한 최초의 벤치마크 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/cover.png"/></item><item><title>Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13663/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13663/</guid><description>ModernBERT: 빠르고 메모리 효율적인 장문 컨텍스트 미세 조정 및 추론을 위한 최첨단 양방향 인코더!</description></item><item><title>TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/</guid><description>TheAgentCompany 벤치마크는 실제 소프트웨어 회사 환경을 모방하여 LLM 에이전트의 실제 업무 수행 능력을 평가하며, AI 에이전트의 현실 세계 적용 가능성과 한계를 보여줍니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/cover.png"/></item><item><title>Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14171/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14171/</guid><description>MLLM의 시각-공간 지능 향상에 도움이 되는 새로운 비디오 기반 벤치마크 VSI-Bench 발표!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14171/cover.png"/></item><item><title>ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12571/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12571/</guid><description>ChatDiT: 제로샷 방식으로 사전 훈련된 확산 변환기를 활용, 자연어로 다양한 시각적 과제 해결!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12571/cover.png"/></item><item><title>DateLogicQA: Benchmarking Temporal Biases in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13377/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13377/</guid><description>DateLogicQA: LLM의 시간적 추론 편향 벤치마크 제시! 토큰화, 표상 및 논리 수준 편향 분석으로 시간적 데이터 처리 개선 방안 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13377/cover.png"/></item><item><title>Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12953/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12953/</guid><description>MoDE: 효율적인 다중 작업 학습을 위한 전문가 혼합 잡음 제거기를 사용한 확산 트랜스포머 정책</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12953/cover.png"/></item><item><title>Move-in-2D: 2D-Conditioned Human Motion Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13185/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13185/</guid><description>Move-in-2D: 2D 이미지와 텍스트 프롬프트로 현실적인 인간 동작 생성</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13185/cover.png"/></item><item><title>VidTok: A Versatile and Open-Source Video Tokenizer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13061/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13061/</guid><description>VidTok: 오픈소스 고성능 비디오 토크나이저가 연속 및 이산 토큰화에서 최첨단 성능을 달성하며, 효율적인 학습 전략과 혁신적인 양자화 기법을 통해 영상 생성 및 이해 연구에 새로운 가능성을 열었습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13061/cover.png"/></item><item><title>Causal Diffusion Transformers for Generative Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12095/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12095/</guid><description>CausalFusion은 확산 및 자기 회귀 모델을 결합하여 생성 모델링에서 최첨단 결과를 달성하고 새로운 기능을 가능하게 합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12095/cover.png"/></item><item><title>ColorFlow: Retrieval-Augmented Image Sequence Colorization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11815/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11815/</guid><description>만화 채색 자동화: ColorFlow는 ID 일관성을 유지하면서 흑백 만화 시퀀스를 채색합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11815/cover.png"/></item><item><title>GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11863/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11863/</guid><description>GeoX: MLLM보다 뛰어난 기하학적 문제 해결사!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11863/cover.png"/></item><item><title>IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12083/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12083/</guid><description>IDArb: Decomposition under varied lights.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12083/cover.png"/></item><item><title>Just a Simple Transformation is Enough for Data Protection in Vertical Federated Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11689/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11689/</guid><description>간단한 변환만으로 수직 연합 학습에서 데이터 보호 가능.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11689/cover.png"/></item><item><title>MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12098/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12098/</guid><description>정보 이득으로 강화 학습 탐색을 강화.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12098/cover.png"/></item><item><title>MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11457/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11457/</guid><description>MOVIS는 실내 장면에 대한 멀티-객체 novel view synthesis에서 구조적 인식을 향상시켜 일관성 있고 사실적인 novel view를 생성합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11457/cover.png"/></item><item><title>Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11423/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11423/</guid><description>실시간 이미지 보호, 딥페이크 대비책.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11423/cover.png"/></item><item><title>RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/</guid><description>RetroLLM: 검색과 생성을 통합한 RAG 시스템</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/cover.png"/></item><item><title>SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/</guid><description>SepLLM은 특수 토큰의 중요성을 활용하여 LLM 추론을 가속화하고 긴 시퀀스를 효율적으로 처리합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/cover.png"/></item><item><title>Sequence Matters: Harnessing Video Models in 3D Super-Resolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/</guid><description>비디오 초해상도 모델을 이용한 혁신적인 3D 초해상도 기법으로, 정렬 과정 없이도 최첨단 성능 달성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/cover.png"/></item><item><title>SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/</guid><description>Self-play with refinement boosts instruction-following in LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/cover.png"/></item><item><title>StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11586/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11586/</guid><description>&amp;rsquo;&amp;rsquo; StrandHead: 텍스트만으로 사실적인 3D 헤드 아바타와 섬세한 헤어스타일까지 생성.''</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11586/cover.png"/></item><item><title>The Open Source Advantage in Large Language Models (LLMs)</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12004/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12004/</guid><description>오픈소스 LLM, 폐쇄형 LLM 대비 투명성과 접근성은 높지만, 성능은 낮음. 하이브리드 전략이 미래.</description></item><item><title>Whisper-GPT: A Hybrid Representation Audio Large Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11449/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11449/</guid><description>Whisper-GPT: 하이브리드 음성 및 음악 LLM으로, 연속 오디오와 이산 토큰을 결합하여 향상된 성능을 제공합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11449/cover.png"/></item><item><title>Wonderland: Navigating 3D Scenes from a Single Image</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12091/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12091/</guid><description>단일 이미지로 고품질 3D 장면을 생성하는 효율적이고 확장 가능한 프레임워크</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12091/cover.png"/></item><item><title>DynamicScaler: Seamless and Scalable Video Generation for Panoramic Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11100/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11100/</guid><description>DynamicScaler는 텍스트나 이미지에서 긴 &lt;strong>끊김 없는 파노라마 비디오&lt;/strong>를 생성하며, &lt;strong>해상도와 종횡비에 관계없이 일관된 움직임을 유지&lt;/strong>합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11100/cover.png"/></item><item><title>GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11258/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11258/</guid><description>GaussianProperty는 LMM을 사용하여 3D 가우시안에 물리적 속성을 통합하는 훈련 없는 프레임워크로, 물리 기반 시뮬레이션 및 로봇 쥐기와 같은 다운스트림 작업을 가능하게 합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11258/cover.png"/></item><item><title>Reliable, Reproducible, and Really Fast Leaderboards with Evalica</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11314/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11314/</guid><description>Evalica: 벤치마킹을 쉽고 빠르고 신뢰할 수 있게 만드는 툴킷</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11314/cover.png"/></item><item><title>Smaller Language Models Are Better Instruction Evolvers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/</guid><description>소형 언어 모델이 더 나은 명령 생성자!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/cover.png"/></item><item><title>VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video Face Swapping</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11279/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11279/</guid><description>VividFace: 첫 번째 확산 기반 비디오 얼굴 바꾸기 프레임워크로 고충실도 결과 제공.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11279/cover.png"/></item><item><title>Apollo: An Exploration of Video Understanding in Large Multimodal Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10360/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10360/</guid><description>Apollo: 대규모 멀티모달 모델의 비디오 이해를 위한 심층 탐구.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10360/cover.png"/></item><item><title>BrushEdit: All-In-One Image Inpainting and Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10316/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10316/</guid><description>BrushEdit: All-in-One Image Inpainting &amp;amp; Editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10316/cover.png"/></item><item><title>Byte Latent Transformer: Patches Scale Better Than Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/</guid><description>BLT: 바이트 기반 LLM, 토큰보다 패치 우선.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/cover.png"/></item><item><title>Efficient Generative Modeling with Residual Vector Quantization-Based Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10208/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10208/</guid><description>ResGen, 고품질 생성과 빠른 샘플링 속도를 모두 달성하는 효율적인 RVQ 기반 생성 모델.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10208/cover.png"/></item><item><title>Large Action Models: From Inception to Implementation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/</guid><description>LLM에서 LAM으로: 실제 작업을 수행하는 AI 에이전트 구축.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/cover.png"/></item><item><title>LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09856/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09856/</guid><description>LinGen: 분 단위 고해상도 텍스트-투-비디오 생성, 선형 계산 복잡도로 효율성 극대화</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09856/cover.png"/></item><item><title>Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on Breast Ultrasound Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09910/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09910/</guid><description>P2P: 텍스트 기반의 새로운 적대적 공격으로 의료 영상 DNN의 취약성 공략</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09910/cover.png"/></item><item><title>RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09858/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09858/</guid><description>RLDG는 강화 학습을 통해 생성된 고품질 데이터로 범용 로봇 정책의 성능을 향상시키는 획기적인 방법입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09858/cover.png"/></item><item><title>SCBench: A KV Cache-Centric Analysis of Long-Context Methods</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/</guid><description>SCBench는 멀티턴 및 멀티리퀘스트 시나리오에서 장문 맥락 메서드를 평가하는 새로운 벤치마크입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/cover.png"/></item><item><title>SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09982/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09982/</guid><description>SplineGS: 실시간 동적 3D 장면을 위한 강력한 모션 적응형 스플라인.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09982/cover.png"/></item><item><title>TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10345/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10345/</guid><description>TraceVLA: 과거의 움직임을 시각적으로 보여줌으로써 로봇의 시공간적 인식을 향상시킵니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10345/cover.png"/></item><item><title>FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09611/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09611/</guid><description>''</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09611/cover.png"/></item><item><title>FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09626/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09626/</guid><description>FreeScale로 튜닝 없이 8K 이미지 생성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09626/cover.png"/></item><item><title>GenEx: Generating an Explorable World</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09624/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09624/</guid><description>GenEx: 단일 이미지로 탐색 가능한 3D 세계 생성.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09624/cover.png"/></item><item><title>GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong Prompt Optimizers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09722/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09722/</guid><description>GREATER는 추론에 대한 그레이디언트를 활용하여 소규모 언어 모델의 프롬프트를 최적화하여 대규모 LLM 없이도 성능을 향상시킵니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09722/cover.png"/></item><item><title>InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09283/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09283/</guid><description>InstanceCap: 인스턴스 인식 구조화 캡션을 통해 텍스트-비디오 생성을 개선합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09283/cover.png"/></item><item><title>InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09596/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09596/</guid><description>InternLM-XComposer2.5-OmniLive: 실시간 스트리밍 비디오 및 오디오 상호작용을 위한 인간의 인지능력을 모방한 혁신적 다중 모드 AI 시스템</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09596/cover.png"/></item><item><title>Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09428/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09428/</guid><description>VMB는 텍스트 및 음악 브리지를 활용하여 멀티모달 음악 생성을 위한 새롭고 제어 가능한 프레임워크를 제시합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09428/cover.png"/></item><item><title>Phi-4 Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/</guid><description>Phi-4: 140억 매개변수 언어 모델은 &lt;strong>데이터 품질에 중점을 둔 훈련 레시피&lt;/strong>로 개발되어 추론 능력을 대폭 향상시켰습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/cover.png"/></item><item><title>SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09604/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09604/</guid><description>SynerGen-VL: 간단한 구조로 이미지 이해 및 생성을 동시에 수행하는 강력한 MLLM.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09604/cover.png"/></item><item><title>ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08645/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08645/</guid><description>객체 합성의 새 시대: ObjectMate로 튜닝 없이 사실적인 결과를 얻으세요.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08645/cover.png"/></item><item><title>SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/</guid><description>Smaller language models reason better with fine-tuned training recipes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/cover.png"/></item><item><title>TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10447/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10447/</guid><description>TidyBot++: 저비용, 홀로노믹 이동 조작기 &amp;amp; 핸드폰 텔레오퍼레이션 인터페이스 공개</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10447/cover.png"/></item><item><title>BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07769/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07769/</guid><description>BiMediX2: 아랍어-영어 이중 언어 의료 전문가 LMM 출시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07769/cover.png"/></item><item><title>Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/</guid><description>Evaluation Agent: 더 빠르고, 유연하며, 설명 가능한 시각적 생성 모델 평가 프레임워크.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/cover.png"/></item><item><title>NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/</guid><description>NeuZip dynamically compresses neural network weights, achieving memory-efficient training and inference without performance loss, significantly reducing the memory footprint of large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/cover.png"/></item><item><title>PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.20083/</link><pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.20083/</guid><description>POLIFORMER: 대규모 트랜스포머 기반 강화학습으로 실제 환경에서도 탁월한 내비게이션 성능을 달성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.20083/cover.png"/></item><item><title>Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.14703/</link><pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.14703/</guid><description>LLM의 개성을 정량적으로 평가하는 새로운 벤치마크 TRAIT 제시: 신뢰성 및 타당성 높은 8,000개의 질문으로 구성, LLM 개성의 독특성과 일관성 규명, 모델 정렬 과정의 영향 분석 및 제한점 제시.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.14703/cover.png"/></item><item><title>Background-aware Moment Detection for Video Moment Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2306.02728/</link><pubDate>Mon, 05 Jun 2023 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2306.02728/</guid><description>BM-DETR: 배경 정보 활용으로 비디오 순간 검색의 약한 정렬 문제 해결!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2306.02728/cover.png"/></item></channel></rss>