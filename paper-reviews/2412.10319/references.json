{"references": [{"fullname_first_author": "Yucheng Li", "paper_title": "SCBENCH: A KV CACHE-CENTRIC ANALYSIS OF LONG-CONTEXT METHODS", "publication_date": "2024-12-13", "reason": "This paper introduces SCBench, a novel benchmark designed to evaluate long-context methods, addressing a key gap in existing evaluations by focusing on real-world scenarios with KV cache reuse."}, {"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention-2: Faster attention with better parallelism and work partitioning", "publication_date": "2024-00-00", "reason": "FlashAttention-2 is a critical optimization technique used in the implementation and evaluation of several long-context methods in the study, significantly influencing performance results."}, {"fullname_first_author": "Huiqiang Jiang", "paper_title": "MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention", "publication_date": "2024-00-00", "reason": "MInference, a state-of-the-art dynamic sparse attention method, is extensively evaluated in the benchmark and serves as a key comparison point for other long-context methods."}, {"fullname_first_author": "Guangxuan Xiao", "paper_title": "Efficient streaming language models with attention sinks", "publication_date": "2024-00-00", "reason": "StreamingLLM, a KV cache dropping method, is a key representative of the class of long-context approaches that exhibit limitations in multi-turn scenarios with KV cache reuse, as highlighted by SCBench."}, {"fullname_first_author": "Yuhong Li", "paper_title": "SnapKV: LLM knows what you are looking for before generation", "publication_date": "2024-00-00", "reason": "SnapKV, another KV cache compression technique, provides valuable comparison data and insights into the performance trade-offs of different approaches in the context of SCBench."}]}