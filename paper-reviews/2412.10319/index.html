<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>SCBench: A KV Cache-Centric Analysis of Long-Context Methods &#183; AI Paper Reviews by AI</title>
<meta name=title content="SCBench: A KV Cache-Centric Analysis of Long-Context Methods &#183; AI Paper Reviews by AI"><meta name=description content="SCBench는 멀티턴 및 멀티리퀘스트 시나리오에서 장문 맥락 메서드를 평가하는 새로운 벤치마크입니다."><meta name=keywords content="Natural Language Processing,Large Language Models,🏢 Microsoft Corporation,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="SCBench: A KV Cache-Centric Analysis of Long-Context Methods"><meta property="og:description" content="SCBench는 멀티턴 및 멀티리퀘스트 시나리오에서 장문 맥락 메서드를 평가하는 새로운 벤치마크입니다."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-13T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="🏢 Microsoft Corporation"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/cover.png"><meta name=twitter:title content="SCBench: A KV Cache-Centric Analysis of Long-Context Methods"><meta name=twitter:description content="SCBench는 멀티턴 및 멀티리퀘스트 시나리오에서 장문 맥락 메서드를 평가하는 새로운 벤치마크입니다."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods","headline":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods","abstract":"SCBench는 멀티턴 및 멀티리퀘스트 시나리오에서 장문 맥락 메서드를 평가하는 새로운 벤치마크입니다.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.10319\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-12-13T00:00:00\u002b00:00","datePublished":"2024-12-13T00:00:00\u002b00:00","dateModified":"2024-12-13T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","🏢 Microsoft Corporation"],"mainEntityOfPage":"true","wordCount":"4642"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.10319/cover_hu955220518808701655.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.10319/>SCBench: A KV Cache-Centric Analysis of Long-Context Methods</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">SCBench: A KV Cache-Centric Analysis of Long-Context Methods</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-13T00:00:00+00:00>13 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>4642 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">22 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.10319/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.10319/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🤗 Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-microsoft-corporation/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🏢 Microsoft Corporation</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#kv-cache-focus>KV Cache Focus</a></li><li><a href=#scbench-design>SCBench Design</a></li><li><a href=#long-ctx-analysis>Long-Ctx Analysis</a></li><li><a href=#multi-turn-limits>Multi-Turn Limits</a></li><li><a href=#sparsity-insights>Sparsity Insights</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#kv-cache-focus>KV Cache Focus</a></li><li><a href=#scbench-design>SCBench Design</a></li><li><a href=#long-ctx-analysis>Long-Ctx Analysis</a></li><li><a href=#multi-turn-limits>Multi-Turn Limits</a></li><li><a href=#sparsity-insights>Sparsity Insights</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.10319</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Yucheng Li et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>🤗 2024-12-16</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.10319 target=_self role=button>↗ arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.10319 target=_self role=button>↗ Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/scbench-a-kv-cache-centric-analysis-of-long target=_self role=button>↗ Papers with Code</a></p><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p><strong>장문 맥락 LLM은 긴 텍스트를 처리하지만 추론에 필요한 계산 및 메모리 비용이 많이 듭니다.</strong> 기존 벤치마크는 단일 요청에 중점을 두고 실제 애플리케이션에서의 KV 캐시 재사용을 무시하여 문제가 됩니다. <strong>KV 캐시 재사용은 vLLM 및 SGLang과 같은 프레임워크와 OpenAI, Microsoft, Google, Anthropic과 같은 LLM 제공업체에서 널리 사용</strong>됩니다. 기존 벤치마크는 멀티턴 및 멀티리퀘스트 시나리오에서 장문 맥락 메서드를 완전히 평가하지 못합니다. <strong>멀티턴 대화 및 멀티단계 추론에서 컨텍스트가 여러 턴 또는 요청에 걸쳐 공유</strong>될 때 KV 캐시 재사용이 중요해집니다. 하위 O(n) 메모리 방식은 이러한 시나리오에서 어려움을 겪습니다. 이러한 문제는 이전 정보의 압축으로 인해 후속 쿼리에 대한 응답이 어려워진다는 보고로 이어졌습니다. 이러한 한계를 해결하기 위해 <strong>실제 장문 맥락 시나리오를 반영하는 포괄적인 벤치마크가 필요</strong>합니다.</p><p>SCBench는 <strong>멀티라운드 및 멀티리퀘스트 시나리오를 포함하는 현실적인 KV 캐시 재사용</strong>을 평가합니다. 이 벤치마크는 문자열 검색, 의미 검색, 전역 정보, 멀티태스킹과 같은 <strong>네 가지 주요 장문 맥락 기능을 평가</strong>합니다. 또한 두 가지 컨텍스트 공유 모드인 멀티턴 및 멀티리퀘스트를 통합합니다. SCBench는 Llama, Qwen, GLM과 같은 8개의 오픈 소스 장문 맥락 LLM과 Codestal Mamba, Jamba와 같은 게이트 선형 RNN을 포함하여 13개의 다른 장문 맥락 메서드를 평가합니다. <strong>KV 캐시 생성, 압축, 검색 및 로드의 네 가지 핵심 단계로 분류</strong>된 이러한 메서드를 분석합니다. SCBench를 통해 연구자들은 <strong>다양한 희소성 기법, 작업 복잡성 및 멀티턴 상호 작용의 영향에 대한 중요한 통찰력</strong>을 얻을 수 있습니다. 이는 궁극적으로 실제 애플리케이션에서 장문 맥락 LLM의 효율성을 개선하고 향후 아키텍처 설계에 대한 정보를 제공하는 것을 목표로 합니다.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-f7de923691a62792d9ee19bde6340477></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-f7de923691a62792d9ee19bde6340477",{strings:[" O(n) 메모리 방식은 장문 디코딩에 필수적이며, 하위 O(n) 방식은 멀티턴 성능이 저하됩니다. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-05e34e1c934a8995bece72d92c18f68c></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-05e34e1c934a8995bece72d92c18f68c",{strings:[" 희소 인코딩은 멀티리퀘스트 시나리오에서 효과적이며 동적 희소성이 고정 패턴보다 우수합니다. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-a82e197e50e6a12ba52aacdbf2ac5aab></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-a82e197e50e6a12ba52aacdbf2ac5aab",{strings:[" 장문 생성은 중요도 분포 변화로 인해 성능 저하가 발생할 수 있습니다. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p><strong>LLM 연구자들에게 SCBench는 장문 맥락 방법 평가를 위한 중요한 벤치마크를 제공</strong>합니다. <strong>멀티턴 및 멀티리퀘스트 시나리오에서 KV 캐시 재사용에 중점</strong>을 두어 기존 벤치마크의 한계를 해결합니다. 이를 통해 현실적인 애플리케이션을 위한 장문 맥락 모델의 성능에 대한 <strong>새로운 통찰력을 제공</strong>하고, 장문 맥락 LLM의 <strong>효율적인 개발 및 배포</strong>를 위한 귀중한 도구가 됩니다.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x1.png alt></figure></p><blockquote><p>🔼 이 그림은 KV 캐시의 수명 주기를 보여줍니다. 기존 벤치마크는 단일 요청에 중점을 두는 반면 실제 애플리케이션에서는 여러 요청에 걸쳐 KV 캐시를 재사용합니다. SCBench는 KV 캐시 생성, 압축, 검색 및 로드의 네 가지 단계로 장문 컨텍스트 메서드를 분류합니다.</p><details><summary>read the caption</summary>Figure 1: KV Cache lifecycle. Prior benchmarks focus on single-request, while real-world applications reuse KV cache across requests. We propose SCBench and categorize long-context methods into KV Cache Generation, Compression, Retrieval, and Loading from a KV-cache-centric perspective.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th>Taxonomy</th><th>Stage</th><th>P-stage Efficient</th><th>D-stage Efficient</th><th>KV Cache Size</th><th>Prefilling Complexity</th><th>Decoding Complexity</th></tr></thead><tbody><tr><td>Codestral Mamba (team, 2024)</td><td>Gated Linear RNN</td><td>❶</td><td>✓</td><td>✓</td><td>O(k)</td><td>O(kn)</td><td>O(km)</td></tr><tr><td>Jamba (Lieber et al., 2024)</td><td>Gated Linear RNN + Full Attention</td><td>❶</td><td>✓</td><td>✓</td><td>O(n)</td><td>O(n²)</td><td>O(nm)</td></tr><tr><td>LLMLingua-2 (Pan et al., 2024)</td><td>Prompt Compression</td><td>❶</td><td>✓</td><td>✗</td><td>O(αn)</td><td>O(α²n²)</td><td>O(αnm)</td></tr><tr><td>A-shape (Xiao et al., 2024b)</td><td>Sparse Attention</td><td>❶</td><td>✓</td><td>✗</td><td>O(n)</td><td>O(kn)</td><td>O(nm)</td></tr><tr><td>Tri-shape</td><td>Sparse Attention</td><td>❶</td><td>✓</td><td>✗</td><td>O(n)</td><td>O(kn)</td><td>O(nm)</td></tr><tr><td>MInference (Jiang et al., 2024)</td><td>Sparse Attention</td><td>❶</td><td>✓</td><td>✗</td><td>O(n)</td><td>O(kn)</td><td>O(nm)</td></tr><tr><td>StreamingLLM (Xiao et al., 2024b)</td><td>KV Cache Dropping</td><td>❷</td><td>✗</td><td>✓</td><td>O(k)</td><td>O(n²)</td><td>O(km)</td></tr><tr><td>SnapKV (Li et al., 2024c)</td><td>KV Cache Dropping</td><td>❷</td><td>✗</td><td>✓</td><td>O(k)</td><td>O(n²)</td><td>O(km)</td></tr><tr><td>PyramidKV (Cai et al., 2024)</td><td>KV Cache Dropping</td><td>❷</td><td>✗</td><td>✓</td><td>O(k)</td><td>O(n²)</td><td>O(km)</td></tr><tr><td>KIVI (Liu et al., 2024e)</td><td>KV Cache Quantitation</td><td>❷</td><td>✗</td><td>✓</td><td>O(n)</td><td>O(n²)</td><td>O(nm)</td></tr><tr><td>CacheBlend (Yao et al., 2024a)</td><td>KV Cache Retrieval</td><td>❸</td><td>✓</td><td>✗</td><td>O(n)</td><td>O(n²)</td><td>O(nm)</td></tr><tr><td>Quest (Tang et al., 2024)</td><td>KV Cache Loading</td><td>❹</td><td>✗</td><td>✓</td><td>O(n)</td><td>O(n²)</td><td>O(km)</td></tr><tr><td>RetrievalAttention (Liu et al., 2024b)</td><td>KV Cache Loading</td><td>❹</td><td>✗</td><td>✓</td><td>O(n)</td><td>O(n²)</td><td>O(km)</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 SCBench에서 평가된 다양한 장문 맥락(long-context) 메서드들을 보여줍니다. 입력 프롬프트의 토큰 크기를 <em>n</em>, 생성 토큰 크기를 <em>m</em>으로 표시하며, <em>n</em>은 <em>m</em>보다 훨씬 큽니다(<em>n</em> &#187; <em>m</em>). 표는 각 메서드의 분류, pre-filling 및 decoding 단계의 효율성, KV 캐시 크기, pre-filling 및 decoding 단계의 계산 복잡도, 그리고 pre-filling 및 decoding 단계에서 효율적인 연산 수행 여부를 보여줍니다. 게이트 선형 RNN, SSM-어텐션 하이브리드 모델, Sparse Attention, KV 캐시 삭제, KV 캐시 양자화, KV 캐시 검색, KV 캐시 로딩 등 다양한 메서드들이 포함되어 있습니다. 이 표는 논문의 섹션 2에서 다양한 장문 맥락 메서드에 대한 KV 캐시 중심적 관점을 소개하는 데 사용됩니다.</p><details><summary>read the caption</summary>Table 1: We evaluated long-context methods on SCBench, where n𝑛nitalic_n represents the token size of the input prompt and m𝑚mitalic_m represents the generation token size, with n≫mmuch-greater-than𝑛𝑚n\gg mitalic_n ≫ italic_m.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">KV Cache Focus<div id=kv-cache-focus class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kv-cache-focus aria-label=Anchor>#</a></span></h4><p><strong>KV 캐시</strong>에 중점을 둔 접근 방식은 <strong>긴 컨텍스트 LLM</strong>의 성능과 효율성을 향상시키는 데 매우 중요합니다. KV 캐시는 이전 토큰의 표현을 저장하여 모델이 긴 텍스트를 효과적으로 처리할 수 있도록 합니다. <strong>캐시 생성, 압축, 검색 및 로딩</strong>을 포함한 KV 캐시 수명 주기의 각 단계를 최적화하면 LLM의 기능을 크게 향상시킬 수 있습니다. 예를 들어, 효율적인 캐시 생성 기술은 초기 처리 비용을 줄이는 반면 지능적인 압축 방법은 메모리 사용량을 최소화합니다. 또한 효과적인 검색 및 로딩 전략은 모델이 이전 정보에 빠르게 액세스하여 신속한 응답을 생성할 수 있도록 합니다. 이러한 모든 최적화는 전체적으로 더 빠른 추론, 더 긴 컨텍스트 처리 및 더 나은 성능으로 이어집니다.</p><h4 class="relative group">SCBench Design<div id=scbench-design class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#scbench-design aria-label=Anchor>#</a></span></h4><p><strong>SCBench</strong>는 <strong>KV 캐시</strong> 중심의 롱 컨텍스트 메서드 평가를 위한 벤치마크입니다. <strong>멀티 라운드 및 멀티 요청 시나리오</strong>에서 KV 캐시 <strong>재사용</strong>에 중점을 두어 실제 애플리케이션을 더 잘 반영합니다. 문자열 검색, 의미 검색, 전역 정보 처리, 멀티태스킹 등 네 가지 주요 <strong>롱 컨텍스트 기능</strong>을 평가하는 12가지 작업을 포함합니다. 벤치마크는 <strong>멀티 턴 모드와 멀티 요청 모드</strong>의 두 가지 공유 컨텍스트 모드에서 이러한 작업을 평가합니다. 이 설계를 통해 SCBench는 다양한 시나리오에서 롱 컨텍스트 메서드의 강점과 약점에 대한 <strong>포괄적인 분석</strong>을 제공하여 실제 환경에서 성능을 보다 정확하게 평가할 수 있도록 합니다.</p><h4 class="relative group">Long-Ctx Analysis<div id=long-ctx-analysis class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#long-ctx-analysis aria-label=Anchor>#</a></span></h4><p>**긴 컨텍스트 분석(Long-Ctx Analysis)**은 대규모 언어 모델(LLM)에서 긴 입력 시퀀스를 처리하는 능력에 대한 심층적인 조사입니다. 이 분석은 <strong>KV 캐시 사용 최적화</strong>에 중점을 두어 컨텍스트 창을 확장하는 방법을 모색합니다. <strong>핵심 과제</strong>는 긴 시퀀스의 계산 및 메모리 비용 증가를 해결하는 것입니다. Long-Ctx Analysis는 <strong>다양한 전략</strong>을 평가합니다. 여기에는 <strong>희소 주의 기법</strong>과 KV 캐시 압축, 검색 및 로드와 같은 메모리 관리 전략이 포함됩니다. 또한 멀티턴 대화 및 다중 요청과 같은 <strong>공유 컨텍스트</strong>에서 이러한 방법의 성능을 분석하여 성능 저하 문제를 조사합니다. 목표는 서로 다른 긴 컨텍스트 방법을 비교하여 다양한 시나리오에서 <strong>장점과 단점</strong>을 강조하는 것입니다. 또한 이 분석은 모델이 긴 컨텍스트 내에서 <strong>글로벌 정보를 효과적으로 처리</strong>하는 능력을 고려합니다. 궁극적으로 Long-Ctx Analysis는 성능을 개선하고 메모리 효율적인 <strong>긴 컨텍스트 LLM 설계를 위한 통찰력</strong>을 제공하는 것을 목표로 합니다.</p><h4 class="relative group">Multi-Turn Limits<div id=multi-turn-limits class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multi-turn-limits aria-label=Anchor>#</a></span></h4><p><strong>멀티턴 대화에서의 한계점</strong>은 현재 LLM 연구의 중요한 과제입니다. 컨텍스트 창 크기 제한, 이전 대화 기억 유지 어려움, 누적되는 계산 비용 증가 등 여러 요인이 복합적으로 작용합니다. 특히, 긴 대화에서 <strong>정보 손실</strong>이 발생하고, <strong>일관성 유지</strong>가 어려워지며, <strong>새로운 정보 통합</strong> 능력도 저하됩니다. 또한, <strong>대화 맥락에 따른 반응 생성</strong> 능력과 <strong>사용자 의도 파악</strong> 능력 향상도 중요한 연구 주제입니다. 이러한 한계를 극복하기 위해 다양한 연구가 진행 중이며, 메모리 효율적인 아키텍처, 지식 증강 기법, 효과적인 컨텍스트 관리 전략 등이 활발히 개발되고 있습니다.</p><h4 class="relative group">Sparsity Insights<div id=sparsity-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#sparsity-insights aria-label=Anchor>#</a></span></h4><p><strong>희소성</strong>은 길고 복잡한 입력을 처리할 때 계산 및 메모리 효율성을 개선하는 데 중요한 역할을 합니다. <strong>희소 인코딩</strong>을 사용하면 전체 입력을 처리하지 않고도 중요한 정보를 포착할 수 있습니다. 디코딩 단계에서 <strong>희소성</strong>을 적용하면 생성된 텍스트의 품질과 일관성이 떨어질 수 있습니다. <strong>동적 희소성</strong>은 정적 패턴보다 유연성이 높으며 성능을 향상시키는 데 도움이 될 수 있습니다. 하이브리드 아키텍처에서 계층 수준 희소성을 사용하면 메모리 사용량을 줄이면서 성능을 향상시킬 수 있습니다. 다중 요청 시나리오의 경우 입력에서 중요한 정보를 추출하는 <strong>희소 인코딩</strong>이 유용할 수 있습니다. 하지만 <strong>희소 디코딩</strong>은 각 요청에 대해 중요한 토큰이 다를 수 있으므로 성능이 떨어질 수 있습니다. 따라서 희소성 기반 방법의 효율성과 효과를 최적화하려면 인코딩 및 디코딩 단계에서 <strong>희소성 패턴</strong>을 신중하게 설계하는 것이 중요합니다.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x2.png alt></figure></p><blockquote><p>🔼 이 그림은 두 가지 일반적인 공유 컨텍스트 패턴, 즉 다중 턴 모드와 힌트된 KV 캐시 다중 요청 모드를 보여줍니다. 다중 턴 모드에서 컨텍스트는 단일 세션 내에 캐시되고, 힌트된 KV 캐시 다중 요청 모드에서는 여러 세션에 걸쳐 캐시됩니다. 각 모드는 공유 컨텍스트와 여러 후속 쿼리로 구성됩니다.</p><details><summary>read the caption</summary>(a) Two Shared Context Modes</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x3.png alt></figure></p><blockquote><p>🔼 SCBench는 공유 컨텍스트와 다중 라운드 상호 작용에 중점을 둔 효율적인 긴 컨텍스트 방법을 평가하도록 설계된 벤치마크입니다. 그림 2b에서 볼 수 있듯이, SCBench는 공유 컨텍스트 모드 두 가지에서 12가지 작업에 대한 네 가지 주요 긴 컨텍스트 기능을 평가합니다. 각 테스트 예시에는 공유 컨텍스트와 여러 후속 쿼리가 포함됩니다. 네 가지 긴 컨텍스트 기능에는 문자열 검색 기능(NIAH 및 Multi-NIAH와 같은 이전 검색 작업을 확장하여 포괄적인 문자열 검색 작업 3가지를 도입), 의미 검색 기능(다양한 도메인에서 다양한 의미 검색 시나리오를 고려하여 네 가지 고유한 테스트 구축), 글로벌 정보 기능(다중 샷 인컨텍스트 학습, 요약 및 긴 배열 통계와 같은 세 가지 작업을 통해 긴 컨텍스트 LLM의 글로벌 정보 처리 및 집계 기능 평가), 다중 작업 기능(NIAH가 포함된 RepoQA 및 KV 검색이 포함된 요약이라는 두 가지 작업을 통해 공유 긴 컨텍스트 입력으로 여러 작업을 처리하는 LLM의 기능 평가)이 포함됩니다. 또한 벤치마크에는 다중 턴 모드와 다중 요청 모드라는 두 가지 일반적인 공유 컨텍스트 모드가 포함됩니다.</p><details><summary>read the caption</summary>(b) Overview of SCBench</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x4.png alt></figure></p><blockquote><p>🔼 이 그림은 두 부분으로 구성되어 있습니다. (a)는 두 가지 일반적인 공유 컨텍스트 패턴, 즉 다중 턴 모드와 힌트된 KV 캐시 다중 요청 모드를 보여줍니다. 다중 턴 모드에서는 컨텍스트가 단일 세션 내에 캐시되고, 힌트된 KV 캐시 다중 요청 모드에서는 여러 세션에 걸쳐 캐시됩니다. (b)는 벤치마크에서 다루는 작업과 시나리오의 개요를 보여줍니다. 문자열 검색, 의미 검색, 전역 정보, 다중 작업의 네 가지 범주의 장문 컨텍스트 기능과 두 가지 공유 컨텍스트 모드(다중 턴 및 다중 요청)가 포함됩니다.</p><details><summary>read the caption</summary>Figure 2: Long-context tasks often involve contexts sharing, e.g., multi-turn dialogues, multi-step reasoning, and repository-level tasks. (a) Illustration of two common shared-context patterns. (b) Overview of tasks and scenarios covered by our benchmark, encompassing four categories of long-context abilities and two shared-context modes.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x5.png alt></figure></p><blockquote><p>🔼 이 그림은 다양한 장문 컨텍스트 기법들이 여러 요청에 걸쳐 어떤 성능 추세를 보이는지 나타냅니다. 디코딩 시 O(n) 메모리 비용이 드는 기법들은 요청이 증가함에 따라 성능이 향상되는 경향이 있습니다. 반대로, 디코딩 시 sub-O(n) KV 캐시를 사용하는 기법들, 예를 들어 KV 캐시 삭제 기법들은 첫 번째 요청에서만 좋은 성능을 보입니다.</p><details><summary>read the caption</summary>(a) Performance Across Different Requests</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x6.png alt></figure></p><blockquote><p>🔼 이 그림은 다양한 롱 컨텍스트 메서드가 SCBench에서 여러 롱 컨텍스트 기능(문자열 검색, 의미 검색, 전역 정보, 멀티태스킹)에서 어떻게 수행되는지 보여줍니다. 모든 롱 컨텍스트 메서드는 검색 기능에서 어느 정도 성능 저하를 보이는 반면, 전역 정보 처리 기능에서는 성능을 대체로 유지합니다.</p><details><summary>read the caption</summary>(b) Performance in Different Abilities</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x7.png alt></figure></p><blockquote><p>🔼 SCBench 성능 결과에 대한 개요입니다. (a)는 여러 요청에 걸친 다양한 장문 맥락 방식의 성능 추세를 보여줍니다. 디코딩 시 O(n) 메모리 비용이 드는 방식은 요청이 증가함에 따라 성능이 향상되는 것을 보여줍니다. 반대로, KV 캐시 삭제 방식과 같이 sub-O(n) KV 캐시를 디코딩에 사용하는 방식은 첫 번째 요청에서만 좋은 성능을 보입니다. (b)는 다양한 장문 맥락 기능 작업에서 서로 다른 장문 맥락 방식의 구체적인 성능을 보여줍니다. 평가된 모든 장문 맥락 방식은 검색 기능에서 약간의 손실을 보이지만, 전역 정보 처리 기능은 대체로 유지합니다. sub-O(n) 방식은 여러 차례의 디코딩에서 비실용적이며, O(n) 메모리를 가진 희소 인코딩이 여러 쿼리에서 전체 어텐션 정확도에 근접할 수 있음을 보여줍니다. O(n) 메모리 방식은 정확한 일치 검색 작업에 필수적입니다. 모든 장문 맥락 방식은 예산이 감소함에 따라 성능이 저하되지만, sub-O(n) 메모리 방식은 더 큰 성능 저하를 보입니다. 장문 생성 시나리오에서는 분포 변화 문제가 발생합니다.</p><details><summary>read the caption</summary>Figure 3: Overview of performance results for SCBench. (a) Performance trends of various long-context methods across multiple requests. Methods with O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) memory cost in decoding show improving performance as requests increase. In contrast, methods with sub-O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) KV cache in decoding, like KV cache dropping methods, perform well only in the first request. (b) Specific performance of different long-context methods across various long-context capability tasks. All evaluated long-context methods exhibit some loss in Retrieval capability while largely maintaining Global Information processing capability.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x8.png alt></figure></p><blockquote><p>🔼 이 그림은 다양한 압축률에서 여러가지 Long-context 메서드의 성능을 Llama-3.1-8B 모델을 사용하여 SCBench에서 평가한 결과를 보여줍니다. 압축률이 낮을수록(예: 1/32) 메모리 사용량은 줄어들지만 성능 저하가 더 커집니다. 반대로, 압축률이 높을수록(예: 1) 성능은 좋아지지만 메모리 사용량은 늘어납니다. 이 그림은 압축률과 성능 사이의 trade-off 관계를 보여주며, MInference와 같이 더 정확한 sparse 메서드는 더 높은 압축률에서도 좋은 성능을 유지할 수 있음을 보여줍니다. 또한, RetreivalAttention 및 KIVI와 같은 O(n) 메모리를 유지하는 메서드는 높은 압축률에서도 상대적으로 높은 성능을 유지함을 알 수 있습니다.</p><details><summary>read the caption</summary>Figure 4: Performance of various long-context methods at different compression rates on SCBench using Llama-3.1-8B (Dubey et al., 2024).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x9.png alt></figure></p><blockquote><p>🔼 이 그림은 A-shape와 Tri-shape라는 두 가지희소 어텐션 방법의 프레임워크를 보여줍니다. A-shape는 싱크 토큰과 로컬 윈도우 영역을 유지하는 반면, Tri-shape는 마지막 윈도우 쿼리 영역도 유지하여 사전 채우기 단계에서 삼각형 패턴을 형성합니다. 이러한 추가는 첫 번째 턴 성능을 향상시키는 것으로 나타났습니다.</p><details><summary>read the caption</summary>Figure 5: The sparse attention methods framework.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x10.png alt></figure></p><blockquote><p>🔼 이 그림은 문자열 검색 능력에 대한 다양한 장문 맥락 메서드의 성능을 여러 턴에 걸쳐 보여줍니다. 결과는 테스트된 모든 기본 LLM에서 평균을 낸 값입니다. 다중 작업 작업에 대한 결과는 그림 10에 나와 있으며, 자세한 내용은 4절에 설명되어 있습니다. 이 그림은 다양한 장문 맥락 방법의 성능이 쿼리가 반복됨에 따라 어떻게 변화하는지, 특히 문자열 검색 작업에서 보여줍니다. O(n) 메모리 방법이 일반적으로 여러 턴에 걸쳐 더 나은 성능을 유지하는 반면, sub-O(n) 방법은 성능이 저하되는 경향이 있음을 알 수 있습니다. 이 그림은 장문 맥락 방법의 강점과 약점에 대한 추가적인 맥락을 제공하며, 특히 메모리 효율성과 다중 턴 성능 간의 균형을 맞추는 방법에 중점을 둡니다.</p><details><summary>read the caption</summary>(a) String Retrieval</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x11.png alt></figure></p><blockquote><p>🔼 이 그림은 다양한 장문 맥락 메서드들이 시맨틱 검색 능력에서 여러 턴에 걸쳐 어떤 성능을 보이는지 비교하고 있습니다. 결과는 테스트된 모든 기본 LLM에 대해 평균화되었습니다.</p><details><summary>read the caption</summary>(b) Semantic Retrieval</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x12.png alt></figure></p><blockquote><p>🔼 이 그림은 다양한 장문 컨텍스트 기법들이 전역 정보 처리 능력을 얼마나 잘 수행하는지 비교하고 있습니다. 여러 턴에 걸쳐 성능을 비교하여, 동적 희소 어텐션 기법(MInference)이 전역 정보를 잘 활용하는 작업에서 우수한 성능을 보이는 반면, KV 캐시 압축 기법(StreamingLLM, SnapKV)은 성능이 저하되는 것을 보여줍니다.</p><details><summary>read the caption</summary>(c) Global Information</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.10319/x13.png alt></figure></p><blockquote><p>🔼 이 그림은 다양한 작업과 턴에 따른 여러 장문 컨텍스트 메서드의 성능을 보여줍니다. 문자열 검색, 의미 검색, 전역 정보와 같은 작업 유형별로 하위 그림이 나뉩니다. 각 하위 그림은 다양한 장문 컨텍스트 메서드(FullAttention, Tri-shape, MInference, A-shape, StreamingLLM, SnapKV, LLMLingua-2, Quest)의 5개 턴에 걸친 성능 변화를 보여줍니다. 결과는 테스트된 모든 기본 LLM에서 평균을 낸 것입니다. 멀티태스킹 작업의 결과는 그림 10에 나와 있습니다.</p><details><summary>read the caption</summary>Figure 6: Performance of different long-context methods across various tasks and turns. The results for multi-tasking tasks are shown in Fig. 10, and the results are averaged across all tested base LLMs.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Task</th><th>Description</th><th>Capability</th><th>Avg. Input Length</th><th>Avg. Output Length</th><th>#Sessions / #Turns</th></tr></thead><tbody><tr><td>Retr.KV</td><td>Key-value retrieval from many key-value pairs</td><td>String Retrieval</td><td>125K</td><td>943</td><td>100/500</td></tr><tr><td>Retr.Prefix-Suffix</td><td>Find string with specific prefix and suffix in a dict</td><td>String Retrieval</td><td>112K</td><td>914</td><td>100/500</td></tr><tr><td>Retr.MultiHop</td><td>Tracking variables assignment in a long input</td><td>String Retrieval</td><td>124K</td><td>410</td><td>90/450</td></tr><tr><td>Code.RepoQA</td><td>Functions retrieval from a GitHub repo</td><td>Semantic Retrieval</td><td>65K</td><td>6,058</td><td>88/440</td></tr><tr><td>En.QA</td><td>English Question Answering</td><td>Semantic Retrieval</td><td>198K</td><td>272</td><td>69/351</td></tr><tr><td>Zh.QA</td><td>Chinese Question Answering</td><td>Semantic Retrieval</td><td>1.5M</td><td>322</td><td>35/189</td></tr><tr><td>En.MultiChoice</td><td>English Multi-Choice Questions</td><td>Semantic Retrieval</td><td>188K</td><td>215</td><td>58/299</td></tr><tr><td>Math.Find</td><td>Math computation tasks within long sequence arrays</td><td>Global Information</td><td>120K</td><td>172</td><td>100/240</td></tr><tr><td>ICL.ManyShot</td><td>Hundreds-shot in-context learning</td><td>Global Information</td><td>22K</td><td>975</td><td>54/270</td></tr><tr><td>En.Sum</td><td>Summarize a doc given multiple docs as input</td><td>Global Information</td><td>104K</td><td>1,170</td><td>79/350</td></tr><tr><td>Mix.Sum+NIAH</td><td>Multi-tasking of En.Sum and Needle in A Haystack</td><td>Multi-tasking</td><td>105K</td><td>3,441</td><td>70/560</td></tr><tr><td>Mix.RepoQA+KV</td><td>Multi-tasking of RepoQA and KV retrieval</td><td>Multi-tasking</td><td>68K</td><td>5,318</td><td>88/704</td></tr><tr><td><strong>Total</strong></td><td>-</td><td>-</td><td><strong>227K</strong></td><td><strong>1,684</strong></td><td><strong>931/4,853</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 SCBench 벤치마크에 포함된 작업들의 개요를 보여주는 표입니다. 각 작업에 대한 설명, 측정되는 능력, 평균 입력 길이, 평균 출력 길이, 세션 수 및 턴 수가 표시되어 있습니다.</p><details><summary>read the caption</summary>Table 2: Overview of SCBench tasks.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Task</th><th>Source</th><th>Configuration</th><th>Example</th></tr></thead><tbody><tr><td>Retr.KV</td><td>Lost in the Middle<br>(Liu et al., 2024d)</td><td>num kv pairs = 2500<br>len of key & value = 36<br>metric = Accuracy</td><td>Input: {<code>&lt;key #1></code>: <code>&lt;value #1></code>, …, <code>&lt;key #100></code>: <code>&lt;value #100></code>}<br>Turn 1: The value of the <code>&lt;key #1></code> is? Answer 1: …<code>&lt;value #1></code>…<br>Turn 2: The value of the <code>&amp;lt;key #20&amp;gt;</code> is? Answer 2: …<code>&amp;lt;value #20&amp;gt;</code>…<br>Turn 3: The value of the <code>&amp;lt;key #40&amp;gt;</code> is? Answer 3: …<code>&amp;lt;value #40&amp;gt;</code>…</td></tr><tr><td>Retr.Prefix-Suffix</td><td>Ours</td><td>size of dict = 6000<br>len of string = [65, 123)<br>metric = Accuracy</td><td>Input: Dictionary = [<code>&lt;str #1></code>, <code>&lt;str #2></code>, …, <code>&lt;str #100></code>]<br>Turn 1: Prefix: <code>&lt;px #1></code>; Suffix: <code>&lt;sx #1></code>. The word with both prefix and suffix from the dict is? Answer: <code>&lt;str></code><br>Turn 2: Prefix: <code>&lt;px #2></code>; Suffix: <code>&lt;sx #2></code>. Answer: <code>&lt;str></code></td></tr><tr><td>Retr.MultiHop</td><td>RULER<br>(Hsieh et al., 2024)</td><td>num chains = 2<br>num hops = 2<br>metric = Accuracy</td><td>Input: VAR <code>X1</code> = <code>12345</code> …… VAR Y1 = 54321 …..<code>&lt;noise></code><br>VAR <code>X2</code> = X1 …… VAR Y2 = Y1 ……<code>&lt;noise></code><br>VAR <code>X3</code> = X2 …… VAR Y3 = Y2 ……<code>&lt;noise></code><br>Turn 1: Variables that are assigned to <code>12345</code>? Answer 1: <code>X1 X2 X3</code><br>Turn 2: Variables that are assigned to 54321? Answer 1: Y1 Y2 Y3</td></tr><tr><td>Code.RepoQA</td><td>RepoQA<br>(Liu et al., 2024c)</td><td>func description from GPT-4<br>metric = Pass@1</td><td>Input: <code>&lt;func 1></code> + <code>&lt;func 2></code> + … + <code>&lt;func 100></code><br>Turn 1: <code>&lt;description of func 1></code>. Answer 1: <code>&lt;func 1></code><br>Turn 2: <code>&lt;description of func 20></code>. Answer 2: <code>&lt;func 20></code></td></tr><tr><td>En.QA<br>Zh.QA</td><td>InfiniteBench<br>(Zhang et al., 2024a)</td><td>ground_truth from human<br>metric = Accuracy</td><td>Input: Read the book below and answer a question. <code>&lt;context></code><br>Turn 1: <code>&lt;question></code> Be very concise. Answer 1: …<code>&lt;ans></code>…<br>Turn 2: <code>&lt;question></code> Be very concise. Answer 2: …<code>&lt;ans></code>…</td></tr><tr><td>En.MultiChoice</td><td>InfiniteBench<br>(Zhang et al., 2024a)</td><td>ground_truth from human<br>metric = Accuracy</td><td>Input: Read the book and answer the question. <code>&lt;context></code><br>Turn 1: <code>&lt;question></code> + <code>&lt;Option A,B,C,D></code>. Answer 1: …<code>&lt;ans></code>…<br>Turn 2: <code>&lt;question></code> + <code>&lt;Option A,B,C,D></code>. Answer 2: …<code>&lt;ans></code>…</td></tr><tr><td>Math.Find</td><td>Ours</td><td>len_array=30000<br>num_digits=3<br>metric = Accuracy</td><td>Input: <code>&lt;a large array of number></code><br>Turn 1: The <code>max number</code> in the array is? Answer 1: …<code>&lt;max number></code>…<br>Turn 2: The <code>max number</code> in the array is? Answer 2: …<code>&lt;max number></code>…</td></tr><tr><td>ICL.ManyShot</td><td>ManyShotICL<br>(Srivastava et al., 2023)</td><td>num_examples = ~150<br>Tasks = date, salient, tracking7<br>metric = Accuracy</td><td>Input: ICL Demo. 1 + Demo. 2 + ….. + Demo. 1000<br>Turn 1: <code>&lt;question></code>. Answer 1: …<code>&lt;ans></code>…<br>Turn 2: <code>&lt;question></code>. Answer 2: …<code>&lt;ans></code>…</td></tr><tr><td>En.Sum</td><td>Ours</td><td>Concatenated arXiv papers<br>ground_truth from GPT-4<br>num document = ~8<br>metric = ROUGE</td><td>Input: <code>Doc 1</code> + Doc 2 + Doc 3 + … + Doc 10.<br>Turn 1: Please summarize <code>Doc 1</code>. Answer 1: … <code>&lt;summary of Doc 1></code>…<br>Turn 2: Please summarize Doc 3. Answer 2: … <code>&lt;summary of Doc 3></code>…<br>Turn 3: Please summarize Doc 5. Answer 2: … <code>&lt;summary of Doc 5></code>…</td></tr><tr><td>Mix.Sum+NIAH</td><td>Ours</td><td>num needle = 5<br>num document = ~8<br>metric = ROUGE + Acc</td><td>Input: <code>Doc 1</code> + <code>&lt;Passkeys></code> + Doc 2 + … + <code>&lt;Passkeys></code> + Doc 10.<br>Turn 1: Please summarize <code>Doc 1</code>. Answer 1: …<code>&lt;summary of Doc 1></code>…<br>Turn 2: What is the needle? Answer 2: ..<code>&lt;needle></code>…</td></tr><tr><td>Mix.RepoQA+KV</td><td>Ours</td><td>num KV pairs = ~100<br>metric = Pass@1 + Acc</td><td>Input: <code>&lt;func 1></code> + KV pairs + <code>&lt;func 2></code> + … + KV pairs + <code>&lt;func 100></code><br>Turn 1: <code>&lt;description of func 1></code>. Answer 1: <code>&lt;func 1></code><br>Turn 2: The value of the <code>&lt;key #1></code> is? Answer 2: …<code>&lt;value #1></code>..</td></tr></tbody></table></table></figure><blockquote><p>🔼 SCBench의 작업 예시와 설정을 보여주는 표입니다. 질문, 답변, 오답은 각각 다른 색깔로 강조되어 있습니다.</p><details><summary>read the caption</summary>Table 3: Task examples and configurations in SCBench. We use different colors to highlight the questions, answers, and distractors in our examples.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Retr.KV</th></tr></thead><tbody></tbody></table></table></figure><blockquote><p>🔼 이 표는 다양한 기본 모델과 두 가지 공유 컨텍스트 모드(멀티턴 및 멀티요청)에서 다양한 장문 컨텍스트 메서드의 SCBench에 대한 평균 성능을 보여줍니다. Llama-3.1-70B, Qwen2.5-32B 및 Llama-3-8B-262K와 같은 기본 모델에 대한 추가 결과는 §D의 표 10을 참조하십시오. 여기서 τ는 목표 압축률을 나타냅니다.</p><details><summary>read the caption</summary>Table 4: Average performance of various long-context methods across different base models in two shared context modes on SCBench. For additional results on base models such as Llama-3.1-70B, Qwen2.5-32B, and Llama-3-8B-262K, see Table 10 in §D. Here, τ𝜏\tauitalic_τ denotes the target compression rate.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Lost in the Middle</th></tr></thead><tbody><tr><td>(Liu et al., 2024d)</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 쿼리 인식 및 비인식 롱 컨텍스트 메서드(SnapKV, Tri-shape, MInference)의 성능 결과를 보여줍니다. 쿼리 인식은 첫 번째 결과에 해당하고, 쿼리 비인식은 두 번째 결과에 해당하며, 밑줄은 쿼리 부재 시 성능 저하를 나타냅니다.</p><details><summary>read the caption</summary>Table 5: Results of query-awareness long-context methods. w/ (first) and w/o (later) query.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>num kv pairs = 2500</th></tr></thead><tbody><tr><td>len of key & value = 36</td></tr><tr><td>metric = Accuracy</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 6은 다양한 롱 컨텍스트 벤치마크를 비교하고 있습니다. 평가되는 롱 컨텍스트 기능, 고려되는 요청 유형 및 구현된 사항에 따라 벤치마크를 비교합니다.</p><details><summary>read the caption</summary>Table 6: Comparison of Long-Context Benchmarks.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Retr.Prefix-Suffix</th></tr></thead><tbody></tbody></table></table></figure><blockquote><p>🔼 이 표는 요약 능력을 평가하는 다양한 벤치마크에서 효율적인 장문 컨텍스트 메서드의 성능을 비교합니다. 이전 벤치마크(InfiniteBench 및 LongBench)와 SCBench에서 Llama-3.1-8B-Inst 모델에 대해 A-Shape, Tri-shape, MInference, StreamingLLM, SnapKV, LLMLingua와 같은 여러 메서드의 성능을 비교하여 SCBench가 다중 요청 시나리오에서 장문 컨텍스트 메서드의 약점을 더 잘 식별할 수 있음을 보여줍니다.</p><details><summary>read the caption</summary>Table 7: Comparing the summarization capability of efficient long-context methods on prior benchmarks and our SCBench.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption>| Ours |</table></figure><blockquote><p>🔼 이 표는 다양한 효율적인 장문 맥락 메서드의 검색 능력을 기존 벤치마크(InfiniteBench, LongBench)와 SCBench에서 비교하여 보여줍니다. SCBench는 특히 다중 요청 및 다중 턴 시나리오에서 장문 맥락 방법의 약점을 더 잘 식별할 수 있습니다.</p><details><summary>read the caption</summary>Table 8: Comparing the retrieval capability of efficient long-context methods on prior benchmarks and our SCBench.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>size of dict = 6000</th></tr></thead><tbody><tr><td>len of string = [65, 123)</td></tr><tr><td>metric = Accuracy</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 SCBench에서 사용되는 다양한 장문 맥락 메서드에 대한 구성을 자세히 설명합니다. SSM(State Space Model), 하이브리드 모델, 희소 주의(Sparse Attention), KV 캐시 압축, 양자화, 검색 및 로딩, 프롬프트 압축을 포함한 여러 범주의 방법에 대한 특정 매개변수와 설정이 표에 요약되어 있습니다. 각 방법에 대한 구성 세부 정보에는 청크 크기, 커널 크기, 은닉 크기, 레이어 수, 주의 헤드 수, 희소성 예산, 로컬 및 초기 토큰 크기, 관측 창, 최대 용량, 커널 크기 등이 포함됩니다. 이 표는 다양한 장문 맥락 메서드의 구현과 평가에 사용되는 특정 설정에 대한 포괄적인 개요를 제공합니다.</p><details><summary>read the caption</summary>Table 9: Configurations of long-context methods in SCBench.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption>| Retr.MultiHop |</table></figure><blockquote><p>🔼 이 표는 Llama-3.1-70B, Qwen2.5-32B, Llama-3-8B-262K 모델에서 다양한 장문 맥락(long-context) 메서드의 SCBench에서의 평균 성능 결과를 보여줍니다. 두 가지 공유 맥락 모드(multi-turn 및 multi-request)에서 Retr.String, Retr.Semantic, Global, Multi-task 작업에 대한 각 메서드의 평균 정확도가 표시되어 있습니다. 이를 통해 서로 다른 모델과 작업에서 다양한 장문 맥락 메서드의 효과를 비교할 수 있습니다.</p><details><summary>read the caption</summary>Table 10: The average results of various long-context methods on Llama-3.1-70B, Qwen2.5-32B, and Llama-3-8B-262K with two shared context modes on SCBench.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>RULER</th></tr></thead><tbody><tr><td><a href=https://arxiv.org/html/2412.10319/2412.10319v1#bib.bib34 target=_blank>Hsieh et al. (2024)</a></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 다중 턴 모드에서 모든 하위 작업에 대한 SCBench의 세부 결과를 보여줍니다. 다양한 언어 모델과 효율적인 장문 컨텍스트 접근 방식에서 En.Sum 작업에 대한 사례 연구를 제시합니다. 요약의 품질은 모델 규모와 양의 상관관계가 있는 것으로 보입니다. 예를 들어 Llama-3.1-70B 및 Qwen2.5-72B는 다른 모델에 비해 더 포괄적이고 세분화된 요약을 제공합니다. 효율적인 장문 컨텍스트 접근 방식의 경우, Tri-Shape 및 MInference와 같은 고밀도 디코딩을 사용하는 희소 인코딩 방법은 세부적인 내용을 포착하는 데 탁월한 성능을 보입니다. 반대로 StreamingLLM과 같은 희소 디코딩 방법은 실패하여 임의적이고 일관성 없는 결과를 생성합니다.Retr.Prefix-Suffix 작업의 결과를 제시합니다. 흥미롭게도 Mamba-Attention 하이브리드 아키텍처 Jamba가 가장 정확한 성능을 달성했습니다. Retr.Prefix-Suffix 작업에는 상당히 큰 공간과 시간 복잡도가 필요하며 Mamba 레이어는 이러한 차원에서 성능이 좋지 않다고 보고되었기 때문에 이는 중요한 결과입니다. 반대로, Llama 및 Qwen 시리즈 모델과 같은 전체 주의 LLMs는 모두 이 작업에서 실패했습니다. 대부분의 모델은 여전히 가변 길이의 접두사를 기억할 수 있지만 종종 전체 문자열을 재현하지 못합니다. 예를 들어 MInference를 사용하는 Llama-70B는 거의 전체 문자열을 검색할 수 있지만 중간에 있는 여러 문자의 철자가 틀립니다. 이는 Transformer 어텐션 헤드에서 유도 헤드(Olsson et al., 2022)의 약점 때문일 수 있으며, 이러한 효율적인 장문 컨텍스트 방법에 대한 희소 입력으로 인해 발생할 수도 있습니다.또한, 다중 작업 테스트, 즉 표 16의 Mix.RepoQA+KV에 대한 일부 장문 컨텍스트 방법의 결과를 제시합니다. 정답은 KV 검색의 답변 하나와 reporqa의 답변 하나를 제공합니다. Llama-3.1-70B와 MInference를 사용하는 변형은 모두 값을 정확하게 검색하여 키-값 검색에서 좋은 성능을 보였습니다. 그러나 Python 함수를 재현한 결과는 흥미로운 차이점을 보여줍니다. 두 모델 모두 전반적인 구조와 들여쓰기를 유지하면서 함수 로직에 여러 수정 사항을 도입합니다. Llama-3.1-70B는 잘못된 함수 이름을 재현하고 새로운 알고리즘을 구현하지만 원래 요소는 제한적으로만 유지합니다. MInference 변형은 기본 모델의 출력과 매우 유사하며 Python 코드 블록 식별자 추가와 같은 사소한 차이점만 있습니다. 특히 두 모델 모두 정답 함수를 정확하게 복제하지 않아 정확한 함수 재현에 어려움이 있음을 시사합니다. 하지만 MInference의 결과는 인코딩 방식의 희소 특성보다는 기본 Llama 모델의 제한된 장문 컨텍스트 기능 때문이라고 생각합니다.표 17에서는 Retr.KV에서 A-shape 및 Tri-shape 모델의 성능을 강조합니다. 특히 Tri-shape는 첫 번째 턴에서도 강력한 성능을 보이며 모델의 지침 준수 기능을 효과적으로 유지합니다. 반대로 A-shape는 모델의 초기 응답을 방해하여 전반적인 작업 성능을 저하시키는 경향이 있습니다. 이러한 차이점은 Tri-shape가 처음부터 작업 구조와 이해를 유지하는 데 유리함을 보여줍니다. 마지막으로, 이러한 결과가 이전 벤치마크의 결과와 어떻게 다른지 설명하고, 희소 인코딩과 디코딩 방법의 성능 차이, 다양한 모델의 작업 적합성, 그리고 오류 전파 및 모델 생성의 영향과 같은 몇 가지 중요한 질문을 다룹니다.</p><details><summary>read the caption</summary>Table 11: The results breakdown of SCBench for all sub-tasks in multi-turn mode.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>num chains = 2</th></tr></thead><tbody><tr><td>num hops = 2</td></tr><tr><td>metric = Accuracy</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 테이블은 다양한 하위 작업에 대한 여러 효율적인 장문 컨텍스트 방법의 성능을 다중 요청 모드에서 비교하여 보여줍니다. Retr.KV 및 Retr.PS와 같은 검색 작업, En.QA 및 Zh.QA와 같은 QA, En.Sum과 같은 요약, RepoQA와 같은 코드 이해, 수학 및 ICL과 같은 문맥 내 학습을 포함합니다. 각 방법은 이러한 영역에서 다양한 강점과 약점을 보여줍니다. 특히 StreamingLLM 및 SnapKV와 같은 일부 방법은 여러 모드에서 검색 및 수학 작업에서 거의 또는 전혀 성능을 보이지 않는 반면 GLM-4-1M 및 MInference와 같은 다른 방법은 검색, QA 및 ICL에서 지속적으로 잘 수행됩니다.</p><details><summary>read the caption</summary>Table 12: The results breakdown of SCBench for all sub-tasks in multi-requests mode.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption>| Code.RepoQA |</table></figure><blockquote><p>🔼 이 표는 이전 질문에 대한 응답을 다음 질문의 컨텍스트로 사용하는 경우(즉, 정답을 컨텍스트로 사용하지 않는 경우)의 결과를 보여줍니다. 표의 두 번째 숫자는 정답을 컨텍스트로 사용하는 경우와 비교한 차이를 나타냅니다.</p><details><summary>read the caption</summary>Table 13: Results when disabling golden answer as context. The later number indicate the gap compared to golden-answer-as-context.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>RepoQA</th></tr></thead><tbody><tr><td><a href=https://arxiv.org/html/2412.10319/bib.bib52 target=_blank>Liu et al., 2024c</a></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 다양한 언어 모델과 장문 맥락 접근 방식을 사용한 En.Sum 과제에 대한 요약 사례 연구를 보여줍니다. 표에서 파란색은 정보가 누락되었음을 나타내고 주황색은 모델이 환각을 일으켰을 가능성이 있음을 나타냅니다. 요약의 품질은 모델 크기와 양의 상관관계가 있는 것으로 보입니다. 예를 들어 Llama-3.1-70B와 Qwen2.5-72B는 다른 모델에 비해 더 포괄적이고 세분화된 요약을 제공합니다. 효율적인 장문 맥락 접근 방식의 경우, Tri-Shape 및 MInference와 같은 dense 디코딩을 사용한 sparse 인코딩 방법은 세부적인 내용을 파악하는 데 뛰어난 성능을 보입니다. 반대로 StreamingLLM과 같은 sparse 디코딩 방법은 실패하여 무작위적이고 일관성 없는 출력을 생성합니다.</p><details><summary>read the caption</summary>Table 14: Case Study of En.Sum. We use blue to indicate mising informaiton, and orange to mark potential hallucination.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>func description from GPT-4</th></tr></thead><tbody><tr><td>metric = Pass@1</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 문자열 검색 능력을 평가하는 Retr.Prefix-Suffix 과제에 대한 다양한 모델의 성능을 보여주는 사례 연구입니다. 각 모델은 주어진 접두사와 접미사를 가진 문자열을 검색해야 하며, 예시 응답은 정답과 비교하여 다른 부분을 주황색으로 강조 표시합니다. 이를 통해 각 모델이 접두사와 접미사를 정확하게 일치시키는 능력과 문자열의 나머지 부분을 올바르게 재현하는 능력을 자세히 분석할 수 있습니다. 특히, Jamba-1.5-Mini 모델은 가장 정확한 성능을 보이는 반면, Llama 및 Qwen 시리즈와 같은 Full-attention LLM은 이 작업에 실패하는 것을 볼 수 있습니다. 또한, 효율적인 장문 컨텍스트 접근 방식 중에서 Sparse Encoding with Dense Decoding 방식인 Tri-Shape 및 MInference가 세부 정보를 잘 캡처하는 우수한 성능을 보이는 반면, StreamingLLM과 같은 Sparse Decoding 방식은 실패하여 무작위적이고 일관성 없는 결과를 생성하는 것을 확인할 수 있습니다.</p><details><summary>read the caption</summary>Table 15: Case Study of Retr.Prefix-Suffix. Orange is used to mark the difference of model response compared to the ground truth.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>En.QA</th></tr></thead><tbody><tr><td>Zh.QA</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 Mix.RepoQA + KV 작업에 대한 사례 연구를 보여줍니다. 주황색은 모델의 잠재적 환각을 나타냅니다. Llama-3.1-70B와 MInference 변형 모두 KV 검색에서 정확하게 값을 검색하여 우수한 성능을 보여주지만, Python 함수를 재현할 때는 차이를 보입니다. 두 모델 모두 전체 구조와 들여쓰기를 유지하지만 함수 로직에 몇 가지 수정 사항을 도입합니다. Llama-3.1-70B는 잘못된 함수 이름을 재현하고 새로운 알고리즘을 구현하면서 원본 요소만 제한적으로 유지합니다. MInference 변형은 기본 모델의 출력과 거의 유사하며 Python 코드 블록 식별자 추가와 같은 사소한 차이만 있습니다. 특히 두 모델 모두 정확하게 함수를 복제하지 못하여 정확한 함수 재현에 어려움이 있음을 시사합니다. MInference 결과는 인코딩 접근 방식의 희소 특성이 아닌 기본 Llama 모델의 제한된 장기 문맥 기능 때문인 것으로 보입니다.</p><details><summary>read the caption</summary>Table 16: Case Study of Mix.RepoQA + KV. Orange indicate the potential model hallucination.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>InfiniteBench</th></tr></thead><tbody><tr><td><a href=https://arxiv.org/html/2412.10319/2412.10319v1#bib.bib93 target=_blank>Zhang et al., 2024a</a></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 Retr.KV 작업에서 A-shape와 Tri-shape를 비교한 케이스 스터디를 보여줍니다. Tri-shape는 첫 번째 턴에서도 강력한 성능을 보여 모델의 지시 따르기 기능을 효과적으로 유지하는 반면, A-shape는 모델의 초기 응답을 방해하여 전반적인 작업 성능을 저하시키는 경향이 있음을 보여줍니다.</p><details><summary>read the caption</summary>Table 17: Case Study of Retr.KV to compare A-shape and Tri-shape.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-8675299dc21592b0449186c136ce9617 class=gallery><img src=paper_images/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/&amp;title=SCBench:%20A%20KV%20Cache-Centric%20Analysis%20of%20Long-Context%20Methods" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/&amp;text=SCBench:%20A%20KV%20Cache-Centric%20Analysis%20of%20Long-Context%20Methods" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/&amp;subject=SCBench:%20A%20KV%20Cache-Centric%20Analysis%20of%20Long-Context%20Methods" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.10319/index.md",oid_likes="likes_paper-reviews/2412.10319/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.09982/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-13T00:00:00+00:00>13 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.09858/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-13T00:00:00+00:00>13 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>