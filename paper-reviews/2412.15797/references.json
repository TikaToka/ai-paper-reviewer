{"references": [{"fullname_first_author": "Richard Bellman", "paper_title": "A Markovian decision process", "publication_date": "1957-00-00", "reason": "This paper introduces the concept of Markov Decision Processes (MDPs), the foundation of the LE-MCTS algorithm's step-by-step reasoning model."}, {"fullname_first_author": "R\u00e9mi Coulom", "paper_title": "Efficient selectivity and backup operators in Monte-Carlo tree search", "publication_date": "2006-00-00", "reason": "This paper details the Monte Carlo Tree Search (MCTS) algorithm, which LE-MCTS uses to efficiently explore the space of reasoning paths."}, {"fullname_first_author": "David Silver", "paper_title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm", "publication_date": "2017-00-00", "reason": "This paper introduces AlphaZero, a groundbreaking reinforcement learning agent that uses MCTS and inspired the LE-MCTS algorithm's architecture."}, {"fullname_first_author": "Hunter Lightman", "paper_title": "Let's verify step by step", "publication_date": "2023-00-00", "reason": "This paper introduces the Process Reward Model (PRM), which LE-MCTS uses to guide the tree search and evaluate the quality of reasoning steps."}, {"fullname_first_author": "Peiyi Wang", "paper_title": "Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations", "publication_date": "2024-00-00", "reason": "This paper provides the Math-Shepherd PRM, a crucial component of LE-MCTS, which evaluates reasoning steps and guides the MCTS algorithm towards more accurate solutions."}]}