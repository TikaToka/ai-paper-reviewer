<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces &#183; AI Paper Reviews by AI</title>
<meta name=title content="Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces &#183; AI Paper Reviews by AI"><meta name=description content="MLLM의 시각-공간 지능 향상에 도움이 되는 새로운 비디오 기반 벤치마크 VSI-Bench 발표!"><meta name=keywords content="Computer Vision,Visual Question Answering,🏢 Stanford University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14171/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14171/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces"><meta property="og:description" content="MLLM의 시각-공간 지능 향상에 도움이 되는 새로운 비디오 기반 벤치마크 VSI-Bench 발표!"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-18T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-18T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Visual Question Answering"><meta property="article:tag" content="🏢 Stanford University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14171/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14171/cover.png"><meta name=twitter:title content="Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces"><meta name=twitter:description content="MLLM의 시각-공간 지능 향상에 도움이 되는 새로운 비디오 기반 벤치마크 VSI-Bench 발표!"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces","headline":"Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces","abstract":"MLLM의 시각-공간 지능 향상에 도움이 되는 새로운 비디오 기반 벤치마크 VSI-Bench 발표!","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.14171\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-12-18T00:00:00\u002b00:00","datePublished":"2024-12-18T00:00:00\u002b00:00","dateModified":"2024-12-18T00:00:00\u002b00:00","keywords":["Computer Vision","Visual Question Answering","🏢 Stanford University"],"mainEntityOfPage":"true","wordCount":"4794"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.14171/cover_hu4044863466548767981.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.14171/>Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-18T00:00:00+00:00>18 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>4794 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">23 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.14171/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.14171/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🤗 Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/visual-question-answering/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Visual Question Answering
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-stanford-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🏢 Stanford University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#spatial-intelligence>Spatial Intelligence</a></li><li><a href=#vsi-bench>VSI-Bench</a></li><li><a href=#mllm-reasoning>MLLM Reasoning</a></li><li><a href=#cognitive-maps>Cognitive Maps</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#spatial-intelligence>Spatial Intelligence</a></li><li><a href=#vsi-bench>VSI-Bench</a></li><li><a href=#mllm-reasoning>MLLM Reasoning</a></li><li><a href=#cognitive-maps>Cognitive Maps</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.14171</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Jihan Yang et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>🤗 2024-12-19</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.14171 target=_self role=button>↗ arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.14171 target=_self role=button>↗ Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/thinking-in-space-how-multimodal-large target=_self role=button>↗ Papers with Code</a></p><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>최근 **다중 모드 대규모 언어 모델(MLLM)**이 다양한 분야에서 괄목할 만한 성과를 거두고 있지만, 시각-공간 지능은 아직 미개척 분야로 남아 있습니다. 기존의 연구들은 주로 정지된 이미지를 사용하여 시각-공간 지능을 평가했지만, 실제 환경에서는 시간에 따른 변화를 포착하는 것이 중요합니다. 따라서, 동영상 데이터를 활용한 시각-공간 지능 평가의 필요성이 대두되고 있습니다.</p><p>본 연구에서는 <strong>MLLM의 시각-공간 지능을 평가하기 위한 새로운 비디오 기반 벤치마크인 VSI-Bench</strong>를 제시합니다. VSI-Bench는 다양한 실내 환경에서 촬영된 5,000개 이상의 질문-답변 쌍을 포함하며, <strong>객체 계수, 상대 거리, 방향, 크기, 경로 계획</strong> 등 다양한 시각-공간 지능 과제를 다룹니다. 연구진은 VSI-Bench를 이용하여 다양한 MLLM 모델을 평가하고, 그 결과를 분석하여 MLLM의 시각-공간 지능의 강점과 약점을 밝혔습니다. 특히, 기존의 언어적 추론 기법은 성능 향상에 도움이 되지 않지만, <strong>인지 지도 생성</strong>을 통해 공간적 거리 추론 능력이 향상됨을 발견하였습니다.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-f466e5af2b461fcf055f8dcd4d29df7b></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-f466e5af2b461fcf055f8dcd4d29df7b",{strings:[" 새로운 비디오 기반 벤치마크 VSI-Bench를 통해 다중 모드 대규모 언어 모델(MLLM)의 시각-공간 지능을 정량적으로 평가 "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-72c89050d83a42e7818a41221af11e93></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-72c89050d83a42e7818a41221af11e93",{strings:[" MLLM이 공간적 추론에 어려움을 겪는다는 것을 밝히고, 공간적 추론 능력 향상을 위한 새로운 방법 제시 "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-4bd72d1484537ee6870944e4a2364ebb></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-4bd72d1484537ee6870944e4a2364ebb",{strings:[" MLLM의 공간 지각 능력은 **국지적 모델**을 기반으로 하며, **글로벌 모델** 구축에는 어려움을 겪는다는 사실 발견 "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>본 논문은 **다중 모드 대규모 언어 모델(MLLM)**의 공간 지각 능력을 평가하기 위한 새로운 벤치마크인 <strong>VSI-Bench</strong>를 제시하고, MLLM이 공간을 어떻게 이해하고 기억하는지에 대한 심층적인 분석을 제공합니다. 이는 <strong>로봇 공학, 자율 주행, 증강 현실/가상 현실</strong> 분야에서의 잠재적 응용과 더불어 <strong>시각-공간 지능</strong> 연구에 중요한 영향을 미칩니다. 또한, 제시된 벤치마크와 분석 방법은 향후 연구자들의 시각-공간 지능 연구에 귀중한 자료가 될 것입니다.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x5.png alt></figure></p><blockquote><p>🔼 그림 1은 시각적 공간 지능의 핵심 요소인 공간 인지, 공간 배치 기억, 그리고 요구시 공간 정보를 떠올려 질문에 답하는 능력을 보여줍니다. 최근 다중 모드 거대 언어 모델(Multimodal LLMs)은 일반적인 비디오를 이해할 수 있지만, 환경의 비디오 녹화를 보여주면 &lsquo;공간적으로 생각&rsquo;할 수 있을까요? 모델이 공간에 대한 질문에 답할 수 있도록 정확하고 암묵적인 &lsquo;인지 지도(cognitive map)&lsquo;를 만들 수 있을까요? 그리고 공간 지능을 향상시키기 위해 다중 모드 거대 언어 모델을 사용하는 것의 강점과 한계는 무엇일까요? 본 논문에서는 다중 모드 거대 언어 모델이 시청할 비디오 데이터를 설정하고, 모델의 기억을 확인하기 위한 VQA 벤치마크를 구축하고, 다중 모드 거대 언어 모델이 실제로 무엇을 기억하고 이해하는지 조사함으로써 이러한 질문들을 탐구합니다.</p><details><summary>read the caption</summary>Figure 1: Whether at home, in the workplace, or elsewhere, the ability to perceive a space, remember its layout, and retrieve this spatial information to answer questions on demand is a key aspect of visual-spatial intelligence. Recent Multimodal LLMs can understand general videos, but can they “think spatially” when presented with a video recording of an environment? Can they build an accurate, implicit “cognitive map” that allows them to answer questions about a space? What are the strengths and limitations of using MLLMs to enhance spatial intelligence? We dig into these questions by setting up video data for MLLMs to watch, building a VQA benchmark to check their recall, and examining what the MLLMs actually remember and understand.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th>Rank</th><th>Avg.</th><th>Obj. Count</th><th>Abs. Dist.</th><th>Obj. Size</th><th>Room Size</th><th>Rel. Dist.</th><th>Rel. Dir.</th><th>Route Plan</th><th>Appr. Order</th></tr></thead><tbody><tr><td>Baseline</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Chance Level (Random)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>25.0</td><td>36.1</td><td>28.3</td><td>25.0</td></tr><tr><td>Chance Level (Frequency)</td><td>-</td><td>34.0</td><td>62.1</td><td>32.0</td><td>29.9</td><td>33.1</td><td>25.1</td><td>47.9</td><td>28.4</td><td>25.2</td></tr><tr><td>VSI-Bench (tiny) Perf.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Human Level<sup>†</sup></td><td>-</td><td>79.2</td><td>94.3</td><td>47.0</td><td>60.4</td><td>45.9</td><td>94.7</td><td>95.8</td><td>95.8</td><td>100.0</td></tr><tr><td>Gemini-1.5 Flash<sup>†</sup></td><td>-</td><td>45.7</td><td>50.8</td><td>33.6</td><td>56.5</td><td>45.2</td><td>48.0</td><td>39.8</td><td>32.7</td><td>59.2</td></tr><tr><td>Gemini-1.5 Pro<sup>†</sup></td><td>-</td><td>48.8</td><td>49.6</td><td>28.8</td><td>58.6</td><td>49.4</td><td>46.0</td><td>48.1</td><td>42.0</td><td>68.0</td></tr><tr><td>Gemini-2.0 Flash<sup>†</sup></td><td>-</td><td>45.4</td><td>52.4</td><td>30.6</td><td>66.7</td><td>31.8</td><td>56.0</td><td>46.3</td><td>24.5</td><td>55.1</td></tr><tr><td>Proprietary Models (API)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4o</td><td>3</td><td>34.0</td><td>46.2</td><td>5.3</td><td>43.8</td><td>38.2</td><td>37.0</td><td>41.3</td><td>31.5</td><td>28.5</td></tr><tr><td>Gemini-1.5 Flash</td><td>2</td><td>42.1</td><td>49.8</td><td>30.8</td><td>53.5</td><td>54.4</td><td>37.7</td><td>41.0</td><td>31.5</td><td>37.8</td></tr><tr><td>Gemini-1.5 Pro</td><td>1</td><td>45.4</td><td>56.2</td><td>30.9</td><td>64.1</td><td>43.6</td><td>51.3</td><td>46.3</td><td>36.0</td><td>34.6</td></tr><tr><td>Open-source Models</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InternVL2-2B</td><td>11</td><td>27.4</td><td>21.8</td><td>24.9</td><td>22.0</td><td>35.0</td><td>33.8</td><td>44.2</td><td>30.5</td><td>7.1</td></tr><tr><td>InternVL2-8B</td><td>5</td><td>34.6</td><td>23.1</td><td>28.7</td><td>48.2</td><td>39.8</td><td>36.7</td><td>30.7</td><td>29.9</td><td>39.6</td></tr><tr><td>InternVL2-40B</td><td>3</td><td>36.0</td><td>34.9</td><td>26.9</td><td>46.5</td><td>31.8</td><td>42.1</td><td>32.2</td><td>34.0</td><td>39.6</td></tr><tr><td>LongVILA-8B</td><td>12</td><td>21.6</td><td>29.1</td><td>9.1</td><td>16.7</td><td>0.0</td><td>29.6</td><td>30.7</td><td>32.5</td><td>25.5</td></tr><tr><td>VILA-1.5-8B</td><td>9</td><td>28.9</td><td>17.4</td><td>21.8</td><td>50.3</td><td>18.8</td><td>32.1</td><td>34.8</td><td>31.0</td><td>24.8</td></tr><tr><td>VILA-1.5-40B</td><td>7</td><td>31.2</td><td>22.4</td><td>24.8</td><td>48.7</td><td>22.7</td><td>40.5</td><td>25.7</td><td>31.5</td><td>32.9</td></tr><tr><td>LongVA-7B</td><td>8</td><td>29.2</td><td>38.0</td><td>16.6</td><td>38.9</td><td>22.2</td><td>33.1</td><td>43.3</td><td>25.4</td><td>15.7</td></tr><tr><td>LLaVA-NeXT-Video-7B</td><td>4</td><td>35.6</td><td>48.5</td><td>14.0</td><td>47.8</td><td>24.2</td><td>43.5</td><td>42.4</td><td>34.0</td><td>30.6</td></tr><tr><td>LLaVA-NeXT-Video-72B</td><td>1</td><td>40.9</td><td>48.9</td><td>22.8</td><td>57.4</td><td>35.3</td><td>42.4</td><td>36.7</td><td>35.0</td><td>48.6</td></tr><tr><td>LLaVA-OneVision-0.5B</td><td>10</td><td>28.0</td><td>46.1</td><td>28.4</td><td>15.4</td><td>28.3</td><td>28.9</td><td>36.9</td><td>34.5</td><td>5.8</td></tr><tr><td>LLaVA-OneVision-7B</td><td>6</td><td>32.4</td><td>47.7</td><td>20.2</td><td>47.4</td><td>12.3</td><td>42.5</td><td>35.2</td><td>29.4</td><td>24.4</td></tr><tr><td>LLaVA-OneVision-72B</td><td>2</td><td>40.2</td><td>43.5</td><td>23.9</td><td>57.6</td><td>37.5</td><td>42.5</td><td>39.9</td><td>32.5</td><td>44.6</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 2는 VideoMME라는 비디오 이해 벤치마크의 500개 질문 하위 집합에 대한 Gemini-1.5 Pro 모델의 Chain-of-Thought(CoT) 프롬프팅 성능을 보여줍니다. CoT 프롬프팅을 사용했을 때와 사용하지 않았을 때의 성능 차이를 보여주어, CoT 프롬프팅이 VideoMME 작업에서 Gemini-1.5 Pro 모델의 성능 향상에 미치는 영향을 평가하는 데 도움이 됩니다.</p><details><summary>read the caption</summary>Table 1: Gemini-1.5 Pro CoT performance on a 500-questions subset in VideoMME.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Spatial Intelligence<div id=spatial-intelligence class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#spatial-intelligence aria-label=Anchor>#</a></span></h4><p>본 논문에서 논의된 공간 지능에 대한 심층적인 생각은 <strong>다양한 모달리티(시각, 언어)를 통합하는 대규모 언어 모델(MLLM)의 공간적 사고 능력</strong>에 초점을 맞춥니다. 인간의 공간 지능은 단순히 공간적 정보를 인지하는 것을 넘어, <strong>공간적 관계를 이해하고 조작하는 능력</strong>을 포함합니다. MLLM은 비디오 데이터를 통해 공간을 학습하지만, <strong>인간 수준의 공간 지능에는 미치지 못하며, 특히 공간적 추론과 배치(allocentric) 및 자기중심적(egocentric) 관점 전환에 어려움을 겪는 것</strong>으로 나타났습니다. 흥미롭게도, 언어적 추론 기법은 공간 지능 향상에 도움이 되지 않지만, <strong>인지적 지도 생성은 MLLM의 공간 거리 추론 능력을 향상</strong>시키는 것으로 보입니다. 이는 <strong>MLLM이 공간을 국소적 모델로 표상하고, 전역적 모델 생성에는 어려움을 겪는다는 점</strong>을 시사합니다. 결론적으로, MLLM의 공간 지능은 여전히 발전의 여지가 크며, 향후 연구는 <strong>국소적 모델을 전역적 모델로 통합하는 방법론</strong>에 초점을 맞춰야 할 것입니다.</p><h4 class="relative group">VSI-Bench<div id=vsi-bench class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#vsi-bench aria-label=Anchor>#</a></span></h4><p>VSI-Bench는 <strong>비디오 기반 시각적 공간 지능 벤치마크</strong>로, 다양한 환경의 실내 공간을 묘사하는 288개의 실제 영상과 5,000개 이상의 질의응답 쌍으로 구성됩니다. <strong>실제 환경 데이터</strong>를 사용하여 <strong>다양한 시각적 공간 지능 과제</strong>를 평가할 수 있다는 점이 핵심입니다. 이는 정적 이미지 기반 벤치마크보다 더욱 풍부한 공간 이해와 추론을 가능하게 합니다. <strong>다양한 유형의 질문</strong> (객체 개수 세기, 상대적 거리, 방향, 경로 계획 등)을 포함하며, 모델의 공간적 추론 능력을 포괄적으로 평가합니다. <strong>정량적 성능 평가</strong>를 위한 명확한 지표를 제공함으로써, 다양한 다중 모달 대규모 언어 모델의 시각적 공간 지능 수준을 비교 분석하고, 향후 개선 방향을 제시하는 데 중요한 역할을 수행합니다. 특히, <strong>인간의 성능과의 비교</strong>를 통해 모델의 강점과 한계를 명확히 드러내어, <strong>시각적 공간 지능 향상</strong>을 위한 연구 방향을 제시합니다.</p><h4 class="relative group">MLLM Reasoning<div id=mllm-reasoning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#mllm-reasoning aria-label=Anchor>#</a></span></h4><p>본 논문에서는 **MLLM(다중 모드 대규모 언어 모델)**의 추론 능력에 대한 심층적인 분석을 제시합니다. 특히, 시공간적 지능(visual-spatial intelligence) 측면에서 MLLM이 공간을 어떻게 인지하고, 기억하고, 상기하는지에 초점을 맞춥니다. <strong>비디오 데이터를 기반으로 구축된 VSI-Bench 벤치마크</strong>를 통해 MLLM의 성능을 평가하고, <strong>인간 수준의 시공간적 추론 능력과의 차이점을 분석</strong>합니다. 흥미롭게도, <strong>언어적 추론 기법(CoT, self-consistency, ToT)은 MLLM의 공간 추론 능력 향상에 큰 효과가 없다는 점</strong>을 발견하였습니다. 반면, <strong>인지 지도(cognitive maps) 생성을 통해 MLLM의 공간 거리 추정 능력이 향상</strong>되었음을 확인하였습니다. 이는 <strong>MLLM이 국지적인 공간 모델은 잘 구축하지만, 전반적인 공간 모델 구축에는 어려움</strong>을 겪는다는 것을 시사합니다. 따라서, <strong>MLLM의 시공간적 추론 능력 향상을 위해서는 국지적 모델에서 전반적인 공간 모델로의 확장이 중요</strong>한 과제임을 강조합니다.</p><h4 class="relative group">Cognitive Maps<div id=cognitive-maps class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#cognitive-maps aria-label=Anchor>#</a></span></h4><p>본 논문에서 인용된 &lsquo;인지 지도(Cognitive Maps)&lsquo;는 **다중모드 대규모 언어 모델(MLLM)**이 공간적 정보를 어떻게 표상하고 기억하는지 이해하는 데 중요한 개념입니다. 연구는 MLLM이 <strong>단편적인 비디오 프레임</strong>에서 <strong>전체적인 공간 지도</strong>를 생성하기보다는 <strong>국소적인 공간 모델</strong>을 만들어 연속적인 공간 경험을 재구성한다는 것을 보여줍니다. 이러한 국소적 모델은 인접한 사물들의 위치 관계는 정확하게 파악하지만, 거리가 멀어질수록 정확도가 떨어집니다. <strong>인지 지도 생성</strong>은 MLLM의 <strong>공간적 추론 능력</strong>을 향상시키는 데 도움이 되는 것으로 나타났습니다. 특히, 거리 추정과 같은 과제에서 <strong>인지 지도 활용</strong>은 성능 개선으로 이어집니다. 이는 MLLM이 공간적 정보를 처리하는 방식에 대한 중요한 통찰력을 제공하며, <strong>보다 정교한 공간적 이해 능력</strong>을 갖춘 모델 개발을 위한 방향을 제시합니다.</p><h4 class="relative group">Future Work<div id=future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-work aria-label=Anchor>#</a></span></h4><p>본 논문의 &ldquo;미래 연구&rdquo; 부분은 시각적 공간 지능 향상을 위한 <strong>다양한 방향</strong>을 제시합니다. <strong>MLLM의 공간 추론 능력 향상</strong>을 위해 특정 작업에 대한 파인튜닝, 자기 지도 학습 기법 도입, 그리고 시각적 공간 추론에 맞춤화된 프롬프팅 기법 개발 등을 제안합니다. 또한, <strong>비디오 데이터를 활용한 MLLM의 공간 이해 능력</strong>에 대한 심층 연구를 통해, <strong>지도 학습과 비지도 학습의 강점을 결합</strong>한 새로운 학습 전략을 모색해야 합니다. <strong>공간적 추론 과정의 투명성을 높이는 방법</strong>도 중요한 연구 과제입니다. <strong>실제 로봇과의 상호 작용</strong>을 통해 MLLM의 공간 지능을 평가하고 발전시키는 연구가 필요합니다. 궁극적으로, <strong>인간 수준의 시각적 공간 지능을 가진 MLLM</strong>을 개발하기 위한 연구가 지속되어야 합니다.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x6.png alt></figure></p><blockquote><p>🔼 그림 2는 시각-공간 지능 능력의 계층 구조를 보여줍니다. 시각적 지각, 공간 추론, 시간적 처리, 언어적 지능의 네 가지 주요 영역이 있습니다. 공간 추론은 관계적 추론과 자기중심-타중심 변환이라는 두 가지 주요 기능으로 나뉩니다. 관계적 추론은 거리와 방향을 통해 객체 간의 관계를 파악하는 능력을 의미합니다. 또한, 크기, 거리 등의 시각적 상식에 기반하여 객체 사이의 거리를 추론하는 것을 포함합니다. 자기중심-타중심 변환은 자기중심적 관점(자신의 위치를 중심으로 한 관점)과 타중심적 관점(환경을 중심으로 한 관점)을 전환하는 능력입니다. 이러한 전환은 다양한 관점에서 공간을 이해하고 새로운 관점을 시각화하고, 경로 계획과 같은 작업에 필수적인 공간적 정신 지도를 만드는 데 필요합니다. 시각적 작업 기억은 정보를 처리하고 사용할 수 있는 능력을 나타내며, 원근 시각화는 객체의 위치와 방향을 이해하는 데 도움이 됩니다. 또한, 시각-공간 지능은 거리, 방향, 시각적 공간 상식과 같은 시각적 공간 상식에 대한 이해를 포함합니다.</p><details><summary>read the caption</summary>Figure 2: A taxonomy of visual-spatial intelligence capabilities.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x7.png alt></figure></p><blockquote><p>🔼 그림 3은 VSI-Bench의 8가지 과제를 보여줍니다. 각 과제는 다양한 유형의 시공간 추론 능력을 평가하도록 설계되었습니다. 예를 들어, &lsquo;물체 개수 세기&rsquo;는 공간 내 물체의 수를 파악하는 능력을, &lsquo;상대 거리 측정&rsquo;은 물체 간의 상대적 거리를 추론하는 능력을, &lsquo;상대 방향 파악&rsquo;은 물체의 상대적 위치를 파악하는 능력을, &lsquo;외관 순서&rsquo;는 시간적 순서에 따른 물체의 출현 순서를 기억하는 능력을, &lsquo;상대적 방향&rsquo;은 주어진 위치에서 다른 물체의 방향을 파악하는 능력을, &lsquo;절대 거리 측정&rsquo;은 물체 간의 절대적 거리를 측정하는 능력을, &lsquo;방 크기&rsquo;는 방의 크기를 추정하는 능력을, &lsquo;경로 계획&rsquo;은 주어진 환경에서 목표 위치까지의 경로를 계획하는 능력을 평가합니다. 각 과제에 대한 질문은 명확성과 간결성을 위해 약간 간략화되었습니다.</p><details><summary>read the caption</summary>Figure 3: Tasks demonstration of VSI-Bench. Note: the questions above are simplified slightly for clarity and brevity.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x8.png alt></figure></p><blockquote><p>🔼 그림 4는 VSI-Bench 데이터셋 제작 과정을 보여줍니다. 다양한 데이터셋들을 표준화된 형식과 의미 공간으로 통합하여 일관된 처리가 가능하도록 합니다. QA 쌍은 사람의 주석과 질문 템플릿을 통해 생성됩니다. 품질을 보장하기 위해 저품질 비디오, 주석 및 모호한 QA 쌍을 걸러내기 위해 모든 단계에서 사람의 검증이 이루어집니다.</p><details><summary>read the caption</summary>Figure 4: Benchmark curation pipeline. The pipeline first unifies diverse datasets into a standardized format and semantic space for consistent processing. QA pairs are then generated through both human annotation and question templates. To ensure quality, human verification is implemented at all key stages for filtering low-quality videos, annotations, and ambiguous QA pairs.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x10.png alt></figure></p><blockquote><p>🔼 그림 5는 VSI-Bench 데이터셋의 통계를 보여줍니다. 위쪽 그래프는 세 가지 주요 범주(구성, 측정, 시공간)에 걸쳐 작업의 분포를 보여주는 막대 그래프입니다. 각 범주 내에는 여러 하위 작업이 포함되어 있으며, 각 하위 작업의 데이터셋 내 비율이 표시됩니다. 아래쪽 그래프는 각 데이터셋(ScanNet, ScanNet++, ARKitScenes)에 따른 비디오 길이 분포를 나타내는 히스토그램입니다. 이 히스토그램을 통해 각 데이터셋의 비디오 길이 분포 특징을 파악할 수 있으며, 데이터셋의 다양성을 평가하는 데 도움이 됩니다.</p><details><summary>read the caption</summary>Figure 5: Benchmark Statistics. Top: The distribution of tasks across three main categories. Bottom: The video length statistic.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x11.png alt></figure></p><blockquote><p>🔼 그림 6은 VSI-Bench에 대한 평가 결과를 보여줍니다. 왼쪽 그래프는 모든 모델 중 최고 성능을 어두운 회색으로, 오픈소스 모델 중 최고 성능을 밝은 회색으로 표시합니다. †는 축소된 VSI-Bench(tiny) 데이터셋에 대한 결과를 나타냅니다. 오른쪽 그래프는 상위 3개의 오픈소스 모델을 포함한 결과를 보여줍니다. 각 과제에 대한 모델의 성능을 정량적으로 비교하여 시각적 공간 지능의 강점과 약점을 보여줍니다.</p><details><summary>read the caption</summary>Figure 6: Evaluation on VSI-Bench. Left: Dark gray indicates the best result among all models and light gray indicates the best result among open-source models. † indicates results on VSI-Bench (tiny) set. Right: Results including the top-3 open-source models.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x12.png alt></figure></p><blockquote><p>🔼 그림 6은 비전 활성화(비디오 포함), 비전 비활성화(비디오 없음), 그리고 우연 수준(빈도) 간의 성능 비교를 보여줍니다. 활성화-비활성화는 비전 활성화와 비전 비활성화 간의 차이를 나타내고, 비활성화-우연은 비전 비활성화와 우연 수준(빈도) 간의 차이를 보여줍니다. 과제는 활성화-비활성화에 따라 정렬되어 이해도를 높였습니다. 이 그림은 다양한 시각적 공간 지능 과제에서 비디오 데이터의 중요성과 모델의 한계를 보여줍니다.</p><details><summary>read the caption</summary>Figure 7: Performance comparisons between Vision Enabled (w/ video), Vision Disabled (w/o video) and Chance Level (Freq.). Enabled−--Disabled indicates the gap between Vision Enabled and Vision Disabled, and Disabled−--Chance betokens the gap between Vision Disabled and Chance Level (Freq.). Tasks are sorted by Enable−--Disable for better understanding.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x13.png alt></figure></p><blockquote><p>🔼 그림 7은 다중 모드 대규모 언어 모델(MLLM)이 자체 설명에서 어떻게 생각하는지를 보여주는 예시입니다. MLLM은 비디오 이해 및 언어적 추론 능력이 뛰어나지만, 공간적 추론 능력은 아직 개발 중임을 보여줍니다. 즉, 그림은 MLLM이 질문에 답변하기 위해 사용하는 사고 과정을 보여주는 자체 설명의 예시를 제시합니다. 각 예시는 시각적 정보(비디오)와 언어적 정보(질문, 답변, 추론 과정)를 함께 보여줌으로써 MLLM의 사고 과정을 자세히 분석하고, 강점과 약점을 파악하는 데 도움을 줍니다. 특히, MLLM의 시각적 정보 처리 능력과 언어적 추론 능력은 뛰어나지만, 공간적인 관계나 위치를 정확하게 이해하고 추론하는 데는 어려움이 있음을 보여줍니다.</p><details><summary>read the caption</summary>Figure 8: Examples of how a MLLM thinks as seen in self-explanations. While a MLLM exhibits strong video understanding and linguistic reasoning capabilities, its spatial reasoning capabilities are still developing.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x14.png alt></figure></p><blockquote><p>🔼 그림 8은 다양한 유형의 실수에 대한 인간이 수행한 분석을 보여줍니다. 각 과제 유형별로 모델이 어떤 종류의 오류를 범했는지 시각적으로 보여주는 막대 그래프와 원 그래프가 함께 제시됩니다. 분석 결과에 따르면, 70%가 넘는 오류가 공간 추론 능력의 결함에서 비롯된다는 것을 알 수 있습니다. 이는 모델이 공간적 관계를 이해하고 이를 사용하여 질문에 답하는 데 어려움을 겪는다는 것을 시사합니다. 이는 단순히 개체를 인식하는 것 이상으로 공간적 사고 능력이 부족하다는 점을 강조합니다.</p><details><summary>read the caption</summary>Figure 9: Human-conducted analysis of errors by type. Over 70% of errors stem from faulty spatial reasoning capabilities.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x15.png alt></figure></p><blockquote><p>🔼 그림 10은 제시된 벤치마크(VSI-Bench)에서 세 가지 주요 언어적 프롬프팅 기법(제로샷 체인 오브 스레드, 자기 일관성, 트리 오브 스레드)의 성능 향상 정도를 기준 성능과 비교하여 보여줍니다. 세 가지 방법 모두 평균적으로 벤치마크에서 실패했으며, 경우에 따라 적용 후 작업 성능이 훨씬 저하되는 경우도 있었습니다. 이는 VSI-Bench가 단순히 언어적 능력만 향상시켜서는 해결할 수 없다는 점을 시사합니다. 즉, 시각적 공간 지능은 언어적 추론만으로는 해결될 수 없으며, 시각적 정보 처리 및 공간적 추론 능력의 향상 또한 필요하다는 것을 보여줍니다.</p><details><summary>read the caption</summary>Figure 10: Relative improvements of CoT, self-consistency and Tree-of-Thought compared to the baseline. All three prevailing prompting techniques fail on average on our benchmark, and, in some cases, task performance becomes much worse after applying them. This implies that VSI-Bench cannot be solved by solely improving linguistic capabilities.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x16.png alt></figure></p><blockquote><p>🔼 그림 11은 다양한 실내 환경에 대한 MLLM(다중 모드 대규모 언어 모델)과 GT(Ground Truth)의 인지 지도를 시각적으로 비교한 것입니다. 각 지도는 방 안의 물체들의 위치를 10x10 격자 좌표로 표현하여, MLLM이 실제 공간을 얼마나 정확하게 이해하고 있는지를 보여줍니다. MLLM의 예측 결과는 GT와 비교하여, MLLM이 공간적 관계를 얼마나 잘 파악하는지, 그리고 어떤 오차가 발생하는지를 보여줍니다. 특히, 가까운 물체들의 위치는 상대적으로 정확하게 예측하지만, 먼 물체일수록 정확도가 떨어지는 경향이 그림에서 나타납니다.</p><details><summary>read the caption</summary>Figure 11: Visualization of cognitive maps from MLLM and GT.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x17.png alt></figure></p><blockquote><p>🔼 본 그림은 MLLM이 예측한 인지 지도에서 거리 정확도가 객체 간의 거리가 증가함에 따라 크게 감소함을 보여줍니다. 즉, MLLM은 가까운 객체들의 위치는 상대적으로 정확하게 예측하지만, 멀리 떨어진 객체들의 위치는 정확도가 급격히 떨어짐을 시각적으로 보여줍니다. 이는 MLLM이 공간을 표현할 때, 전체 공간에 대한 하나의 통합된 지도를 생성하는 것이 아니라, 국부적인 영역에 대한 여러 개의 부분적인 지도를 형성하는 경향이 있음을 시사합니다.</p><details><summary>read the caption</summary>Figure 12: Locality of the MLLM’s predicted cognitive maps. The MLLM’s map-distance accuracy decreases dramatically with increasing object distance.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x18.png alt></figure></p><blockquote><p>🔼 그림 13은 VSI-Bench의 질문 유형 예시를 보여줍니다. 다양한 유형의 질문 (물체 개수 세기, 상대적 거리, 방향, 외형 순서, 크기, 절대적 거리, 방 크기, 경로 계획) 이 제시되며 각 질문 유형에 대한 여러가지 예시 질문과 답변이 함께 제공됩니다. 이를 통해 모델이 공간적 지각, 기억, 상기 능력을 어떻게 평가하는지 보여줍니다. 각각의 예시는 다양한 실내 공간을 보여주는 비디오 클립과 연관되어 있어, 모델이 실제 환경에서 얼마나 잘 작동하는지 이해하는 데 도움이 됩니다.</p><details><summary>read the caption</summary>Figure 13: VSI-Bench Examples (Part 1).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x19.png alt></figure></p><blockquote><p>🔼 그림 14는 VSI-Bench의 질문 유형 예시 중 일부를 보여줍니다. 각 질문 유형(개체 수 세기, 상대 거리, 개체 크기, 방 크기, 상대 방향, 경로 계획, 외관 순서)에 대해 2개의 예시를 제공하여 다양한 시각적 공간적 추론 능력을 평가하는 방법을 보여줍니다. 각 예시는 질문과 그에 해당하는 이미지, 정답을 포함합니다. 이 그림은 논문의 3장, VSI-Bench 벤치마크 소개 부분에 포함되어 있습니다.</p><details><summary>read the caption</summary>Figure 14: VSI-Bench Examples (Part 2).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x20.png alt></figure></p><blockquote><p>🔼 그림 15는 모델의 오류 분석 사례들을 추가적으로 보여줍니다. 시각적 인식 오류, 언어적 지능 오류, 관계적 추론 오류, 그리고 자기 중심적-타중심적 변환 오류 등 네 가지 주요 오류 유형을 보여주는 다양한 질문과 답변 예시들이 제시됩니다. 각 오류 유형에 대한 설명과 함께, 모델이 어떤 부분에서 오류를 범했는지에 대한 자세한 분석이 포함되어 있습니다. 특히, 모델이 질문에 대한 답변을 생성하는 과정에서 시각적 정보를 어떻게 처리하고 해석하는지, 그리고 어떤 유형의 추론 과정을 거치는지에 대한 통찰력을 제공합니다.</p><details><summary>read the caption</summary>Figure 15: Additional Error Analysis Examples.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x21.png alt></figure></p><blockquote><p>🔼 그림 16은 제로샷 체인 오브 스레드(Zero-Shot Chain of Thought) 기법을 사용한 질의응답 예시를 보여줍니다. 세 가지 다른 유형의 질문(개체 수 세기, 개체 크기, 방 크기)에 대한 모델의 응답과 추론 과정을 단계별로 보여줍니다. 각 질문 유형에 대해 모델이 어떻게 질문을 이해하고, 관련 정보를 추출하고, 최종 답변에 도달하는지 자세히 설명합니다. 이는 모델의 추론 과정을 시각적으로 보여주어 모델의 성능과 한계를 이해하는 데 도움을 줍니다.</p><details><summary>read the caption</summary>Figure 16: Zero-Shot CoT Examples.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.14171/x22.png alt></figure></p><blockquote><p>🔼 그림 17은 &lsquo;Self-Consistency with Chain-of-Thought&rsquo; 프롬프팅 기법을 사용한 모델의 추론 과정을 보여줍니다. 세 가지 과제(개체 수 세기, 개체 크기 추정, 방 크기 추정)에 대해, 모델이 질문에 대한 답을 도출하는 다섯 가지 다른 시도의 예시를 보여줍니다. 각 시도는 중간 단계의 추론 과정과 최종 답변을 포함하며, 각 과제에 대해 다수결 투표로 최종 답변을 결정하는 과정을 보여줍니다. 이는 모델이 주어진 비디오 데이터를 기반으로 어떻게 추론하고 답변을 생성하는지, 그리고 Self-Consistency 기법이 모델의 성능에 어떻게 영향을 미치는지를 보여주는 시각적인 설명입니다.</p><details><summary>read the caption</summary>Figure 17: Self-Consistency w/ CoT Examples.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Case</th><th>Performance</th></tr></thead><tbody><tr><td>Gemini-1.5 Pro (w/o CoT)</td><td>77.2</td></tr><tr><td>Gemini-1.5 Pro (w/ CoT)</td><td>79.8</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 (a)는 인지 지도 프롬프팅에 대한 실험 결과를 보여줍니다. &lsquo;Cog. Map Src&rsquo;는 인지 지도 생성에 사용된 소스(MLLM 또는 GT)를 나타내고, &lsquo;Size&rsquo;는 인지 지도의 크기를 나타냅니다. &lsquo;Rel. Dist Acc&rsquo;는 상대 거리 정확도를 의미하며, 인지 지도를 사용했을 때와 사용하지 않았을 때의 상대 거리 질문에 대한 MLLM의 정확도를 비교합니다. 결과는 인지 지도를 사용하면 MLLM의 상대 거리 추론 능력이 향상됨을 보여줍니다.</p><details><summary>read the caption</summary>(a) Cognitive map prompting.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Case</th><th>Rel. Dist Acc.</th></tr></thead><tbody><tr><td>w/o Cog. map</td><td>46.0</td></tr><tr><td>w/ Cog. map</td><td>56.0</td></tr><tr><td>w/ Cog. map (GT)</td><td>66.0</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 (b)는 MLLM이 공간을 기억하는 방식을 조사하기 위해 사용된 인지 지도의 크기가 성능에 미치는 영향을 보여줍니다. 10x10 크기의 격자와 20x20 크기의 격자를 비교하여, MLLM의 상대적 거리 추론 정확도에 대한 영향을 분석합니다. 즉, MLLM이 공간을 표현하는 데 사용하는 격자 크기가 다를 때 상대적 거리 인식 성능이 어떻게 변하는지 보여주는 실험 결과입니다.</p><details><summary>read the caption</summary>(b) Cognitive map canvas size.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Cog. Map Src.</th><th>Size</th><th>Rel. Dist Acc.</th></tr></thead><tbody><tr><td>MLLM</td><td>10 × 10</td><td>56.0</td></tr><tr><td>MLLM</td><td>20 × 20</td><td>54.0</td></tr><tr><td>GT</td><td>10 × 10</td><td>66.0</td></tr><tr><td>GT</td><td>20 × 20</td><td>78.0</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 2는 모델이 공간적 정보를 기억하는 방식을 평가하기 위해 사용된 &lsquo;인지 지도&rsquo; 접근법에 대한 실험 결과를 보여줍니다. 특히, 인지 지도를 활용했을 때 상대적 거리 추론 과제에서 모델의 성능 향상 여부를 보여줍니다. &lsquo;인지 지도 생성&rsquo; 크기(10x10 또는 20x20)를 달리하여 실험한 결과도 포함되어 있습니다. 기준 모델(MLLM)과 인지 지도를 사용한 모델의 성능을 비교하여 인지 지도의 효과를 정량적으로 분석합니다.</p><details><summary>read the caption</summary>Table 2: Relative distance task with cognitive map.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Task</th><th>Question Template</th></tr></thead><tbody><tr><td>Object Counting</td><td>How many {category}(s) are in this room?</td></tr><tr><td>Relative Distance</td><td>Measuring from the closest point of each object, which of these objects ({choice a}, {choice b}, {choice c}, {choice d}) is the closest to the {category}?</td></tr><tr><td>Relative Direction</td><td>To create a comprehensive test of relative direction, three difficulty levels were created:</td></tr></tbody></table><ul><li><strong>Easy:</strong> If I am standing by the {positioning object} and facing the {orienting object}, is the {querying object} to the left or the right of the {orienting object}?</li><li><strong>Medium:</strong> If I am standing by the {positioning object} and facing the {orienting object}, is the {querying object} to my left, right, or back? An object is to my back if I would have to turn at least 135 degrees in order to face it.</li><li><strong>Hard:</strong> If I am standing by the {positioning object} and facing the {orienting object}, is the {querying object} to my front-left, front-right, back-left, or back-right? Directions refer to the quadrants of a Cartesian plane (assuming I am at the origin and facing the positive y-axis). |
| Appearance Order | What will be the first-time appearance order of the following categories in the video: {choice a}, {choice b}, {choice c}, {choice d}? |
| Object Size | What is the length of the longest dimension (length, width, or height) of the {category}, measured in centimeters? |
| Absolute Distance | Measuring from the closest point of each object, what is the direct distance between the {object 1} and the {object 2} (in meters)? |
| Room Size | What is the size of this room (in square meters)? If multiple rooms are shown, estimate the size of the combined space. |
| Route Plan | You are a robot beginning at {the bed facing the tv}. You want to navigate to {the toilet}. You will perform the following actions (Note: for each [please fill in], choose either ‘turn back,’ ‘turn left,’ or ‘turn right.’): {1. Go forward until the TV 2. [please fill in] 3. Go forward until the shower 4. [please fill in] 5. Go forward until the toilet.}. You have reached the final destination.|</li></ul></table></figure><blockquote><p>🔼 이 표는 VSI-Bench(Video-based Spatial Intelligence Benchmark)의 각 과제에 대한 질문 템플릿을 보여줍니다. VSI-Bench는 다양한 실내 환경의 비디오를 사용하여 다중 모달 대규모 언어 모델(MLLM)의 시각-공간 지능을 평가하기 위한 벤치마크입니다. 표에는 개체 계수, 상대 거리, 상대 방향, 경로 계획, 외형 순서, 개체 크기, 절대 거리, 방 크기 등 8가지 과제에 대한 질문 템플릿이 나열되어 있습니다. 각 템플릿에는 특정 요소(예: 개체 범주, 선택지)를 해당 장면에 맞게 바꿔서 사용하도록 되어 있습니다. 경로 계획 과제의 경우 완벽한 예시 질문이 제공됩니다. 이 표는 VSI-Bench 데이터셋을 구성하는 방법과 MLLM 평가 방식에 대한 이해를 돕습니다.</p><details><summary>read the caption</summary>Table 3: Question Templates for tasks in VSI-Bench. We replace the highlighted part in the question template from scene to scene to construct our benchmark. Note that a complete example question is provided for Route Plan.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Order</th><th>Avg.</th></tr></thead><tbody><tr><td>Video first</td><td><strong>48.8</strong></td></tr><tr><td>Question first</td><td>46.3</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 5는 비디오 입력 순서와 반복에 따른 비교 실험 결과를 보여줍니다. (a) 비디오 입력 순서는 비디오를 먼저 보여주는 방식과 질문을 먼저 보여주는 방식을 비교합니다. (b) 비디오 반복 횟수는 비디오를 한 번 보여주는 것과 두 번 보여주는 것을 비교합니다. 실험 결과, 비디오를 먼저 보여주는 방식이 질문을 먼저 보여주는 방식보다 평균 정확도가 약 2.5% 높았고, 비디오를 두 번 보여주는 방식이 한 번 보여주는 방식보다 평균 정확도가 약 2.1% 높았습니다. 이는 사람이 시각적 정보를 여러 번 검토하여 문제 해결 능력을 향상시키는 것과 유사합니다. 이러한 결과는 MLLM이 비디오 이해에 있어 단순히 시각적 정보를 처리하는 것을 넘어, 반복적인 검토를 통해 추론 능력을 향상시킬 수 있음을 시사합니다.</p><details><summary>read the caption</summary>(a) Input Sequence</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th># Times</th><th>Avg.</th></tr></thead><tbody><tr><td>1</td><td>48.8</td></tr><tr><td>2</td><td><strong>50.9</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 표 5(b)는 비디오 반복 횟수에 따른 모델 성능 변화를 보여줍니다. 비디오를 한 번만 보여주었을 때와 두 번 보여주었을 때의 Gemini-1.5 Pro 모델 성능을 비교하여, 비디오를 반복해서 보여주는 것이 모델 성능 향상에 미치는 영향을 분석한 결과입니다. 구체적으로는, 평균 정확도를 비교하여 비디오 반복이 모델 성능 향상에 어떤 영향을 주는지 확인합니다. 단순히 비디오를 여러 번 보여주는 것만으로도 성능이 향상될 수 있는지, 그리고 그 정도는 어느 정도인지 보여주는 실험 결과입니다.</p><details><summary>read the caption</summary>(b) Video Repetition Times</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th># of Frames</th></tr></thead><tbody><tr><td><em>Proprietary Models (API)</em></td><td></td></tr><tr><td>GPT-4o</td><td>16</td></tr><tr><td>Gemini-1.5 Flash</td><td>-</td></tr><tr><td>Gemini-1.5 Pro</td><td>-</td></tr><tr><td><em>Open-source Models</em></td><td></td></tr><tr><td>InternVL2-2B</td><td>8</td></tr><tr><td>InternVL2-8B</td><td>8</td></tr><tr><td>InternVL2-40B</td><td>8</td></tr><tr><td>LongVILA-8B</td><td>32</td></tr><tr><td>VILA-1.5-8B</td><td>32</td></tr><tr><td>VILA-1.5-40B</td><td>32</td></tr><tr><td>LongVA-7B</td><td>32</td></tr><tr><td>LLaVA-NeXT-Video-7B</td><td>32</td></tr><tr><td>LLaVA-NeXT-Video-72B</td><td>32</td></tr><tr><td>LLaVA-OneVision-0.5B</td><td>32</td></tr><tr><td>LLaVA-OneVision-7B</td><td>32</td></tr><tr><td>LLaVA-OneVision-72B</td><td>32</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 5는 비디오 입력 순서 및 반복에 대한 추가 실험 결과를 보여줍니다. 먼저, 비디오를 먼저 보여주는 방식(Video first)과 질문을 먼저 보여주는 방식(Question first)을 비교한 결과, 비디오 먼저 방식이 평균 2.5% 더 높은 성능을 보였습니다. 이는 시각적 정보를 먼저 제공하는 것이 모델의 이해도 향상에 도움이 된다는 것을 시사합니다. 다음으로, 비디오 반복 횟수에 따른 성능 변화를 분석한 결과, 비디오를 두 번 반복해서 보여준 경우 평균 2.1% 더 높은 성능을 기록했습니다. 이는 모델이 비디오를 여러 번 분석할 수 있는 기회를 제공하면 더 나은 성능을 낼 수 있음을 보여주는 결과입니다. 즉, 시각 정보의 순서와 반복 횟수가 모델의 시각적 추론 능력에 영향을 미침을 보여주는 실험입니다.</p><details><summary>read the caption</summary>Table 4: Ablations on the video input sequence and repetition.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Models</th><th>QA. Type</th><th>Prompt</th></tr></thead><tbody><tr><td>Pre-Prompt</td><td>-</td><td><em>These are frames of a video.</em></td></tr><tr><td>Post-Prompt</td><td>Open-source Models</td><td>NA</td></tr><tr><td></td><td></td><td>MCA</td></tr><tr><td>Post-Prompt</td><td>Proprietary Models</td><td>NA</td></tr><tr><td></td><td></td><td>MCA</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 6은 평가에 사용된 비디오 프레임 수를 보여줍니다. 각 모델에 대해 비디오의 전체 프레임 수가 아니라, 실제 평가에 사용된 프레임의 수를 나타냅니다. 이는 모델의 성능 평가에 사용된 비디오 데이터의 양적 차이를 보여주는 정보입니다. 특히, 일부 모델의 경우 비디오 전체를 사용하지 않고 일부만 사용했음을 알 수 있습니다. 비디오 프레임의 수는 모델의 유형과 매개변수 크기 등에 따라 다릅니다. 이 표는 모델별로 사용된 비디오 데이터 양의 차이를 고려해야 할 필요가 있음을 시사합니다.</p><details><summary>read the caption</summary>Table 5: Number of frames used in evaluation.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th>Avg.</th><th>Obj. Count</th><th>Abs. Dist.</th><th>Obj. Size</th><th>Room Size</th><th>Rel. Dist.</th><th>Rel. Dir.</th><th>Route Plan</th><th>Appr. Order</th></tr></thead><tbody><tr><td>Proprietary Models (API)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4o</td><td>35.6</td><td>36.2</td><td>4.6</td><td>47.2</td><td>40.4</td><td>40.0</td><td>46.2</td><td>32.0</td><td>38.0</td></tr><tr><td>Gemini-1.5 Flash</td><td>45.7</td><td>50.8</td><td>33.6</td><td>56.5</td><td>45.2</td><td>48.0</td><td>39.8</td><td>32.7</td><td>59.2</td></tr><tr><td>Gemini-1.5 Pro</td><td>48.8</td><td>49.6</td><td>28.8</td><td>58.6</td><td>49.4</td><td>46.0</td><td>48.1</td><td>42.0</td><td>68.0</td></tr><tr><td>Gemini-2.0 Flash</td><td>45.4</td><td>52.4</td><td>30.6</td><td>66.7</td><td>31.8</td><td>56.0</td><td>46.3</td><td>24.5</td><td>55.1</td></tr><tr><td>Open-source Models</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InternVL2-2B</td><td>25.5</td><td>30.6</td><td>20.4</td><td>26.0</td><td>29.6</td><td>28.0</td><td>39.2</td><td>28.0</td><td>2.0</td></tr><tr><td>InternVL2-8B</td><td>32.9</td><td>26.4</td><td>25.4</td><td>43.8</td><td>41.6</td><td>30.0</td><td>32.2</td><td>20.0</td><td>44.0</td></tr><tr><td>InternVL2-40B</td><td>37.6</td><td>40.8</td><td>23.8</td><td>48.0</td><td>26.0</td><td>46.0</td><td>30.1</td><td>42.0</td><td>44.0</td></tr><tr><td>LongVILA-8B</td><td>19.1</td><td>23.4</td><td>10.8</td><td>11.4</td><td>0.0</td><td>20.0</td><td>33.1</td><td>28.0</td><td>26.0</td></tr><tr><td>VILA-1.5-8B</td><td>31.4</td><td>12.2</td><td>23.4</td><td>51.4</td><td>18.6</td><td>36.0</td><td>41.5</td><td>42.0</td><td>26.0</td></tr><tr><td>VILA-1.5-40B</td><td>32.3</td><td>14.6</td><td>21.0</td><td>48.0</td><td>20.6</td><td>42.0</td><td>22.0</td><td>40.0</td><td>50.0</td></tr><tr><td>LongVA-7B</td><td>31.8</td><td>41.2</td><td>17.4</td><td>39.6</td><td>25.4</td><td>30.0</td><td>52.8</td><td>34.0</td><td>14.0</td></tr><tr><td>LLaVA-NeXT-Video-7B</td><td>35.7</td><td>49.0</td><td>12.8</td><td>48.6</td><td>21.4</td><td>40.0</td><td>43.5</td><td>34.0</td><td>36.0</td></tr><tr><td>LLaVA-NeXT-Video-72B</td><td>39.3</td><td>41.4</td><td>26.6</td><td>55.6</td><td>31.6</td><td>36.0</td><td>25.6</td><td>42.0</td><td>56.0</td></tr><tr><td>LLaVA-OneVision-0.5B</td><td>27.7</td><td>44.0</td><td>23.0</td><td>18.8</td><td>28.4</td><td>30.0</td><td>33.4</td><td>36.0</td><td>8.0</td></tr><tr><td>LLaVA-OneVision-7B</td><td>33.8</td><td>48.2</td><td>22.0</td><td>44.4</td><td>14.0</td><td>44.0</td><td>31.9</td><td>34.0</td><td>32.0</td></tr><tr><td>LLaVA-OneVision-72B</td><td>41.6</td><td>38.0</td><td>31.6</td><td>54.4</td><td>35.2</td><td>44.0</td><td>39.7</td><td>32.0</td><td>58.0</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 7은 본 논문의 실험에서 사용된 프롬프트들을 보여줍니다. 각 모델(오픈소스 모델과 독점 모델)과 질문 유형(수치형 답변, 다중 선택형 답변)에 따라 다른 프롬프트가 사용되었음을 알 수 있습니다. 수치형 답변의 경우 숫자만으로 답변하도록 지시하고, 다중 선택형 답변의 경우 제시된 선택지 중 하나의 문자를 답으로 제출하도록 지시합니다. 이는 모델의 응답 형식을 일관성 있게 유지하고, 결과 분석의 정확성을 높이기 위한 것입니다.</p><details><summary>read the caption</summary>Table 6: Prompts used in evaluation. NA and MAC indicates questions with Numerical Answer and Multiple Choice Answer respectively.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th>Avg.</th><th>Obj. Count</th><th>Abs. Dist.</th><th>Obj. Size</th><th>Room Size</th><th>Rel. Dist.</th><th>Rel. Dir.</th><th>Route Plan</th><th>Appr. Order</th></tr></thead><tbody><tr><td>Proprietary Models (API)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4o</td><td>14.5</td><td>0.1</td><td>5.2</td><td>36.7</td><td>0.0</td><td>10.8</td><td>23.2</td><td>26.9</td><td>13.1</td></tr><tr><td>Gemini-1.5 Flash</td><td>19.9</td><td>25.0</td><td>30.3</td><td>52.5</td><td>0.0</td><td>0.0</td><td>21.2</td><td>29.9</td><td>0.2</td></tr><tr><td>Gemini-1.5 Pro</td><td>32.3</td><td>30.6</td><td>11.5</td><td>51.5</td><td>33.1</td><td>33.8</td><td>44.6</td><td>33.5</td><td>20.2</td></tr><tr><td>Open-source Models</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InternVL2-2B</td><td>17.8</td><td>5.4</td><td>23.7</td><td>9.2</td><td>0.0</td><td>26.9</td><td>41.2</td><td>27.9</td><td>7.9</td></tr><tr><td>InternVL2-8B</td><td>27.6</td><td>31.9</td><td>26.8</td><td>38.3</td><td>0.7</td><td>27.1</td><td>39.2</td><td>33.0</td><td>23.6</td></tr><tr><td>InternVL2-40B</td><td>24.4</td><td>5.4</td><td>29.1</td><td>39.2</td><td>0.7</td><td>30.3</td><td>37.7</td><td>27.9</td><td>24.7</td></tr><tr><td>LongVILA-8B</td><td>20.2</td><td>47.4</td><td>12.6</td><td>8.7</td><td>0.6</td><td>24.3</td><td>27.0</td><td>27.4</td><td>13.9</td></tr><tr><td>VILA-1.5-8B</td><td>21.5</td><td>7.4</td><td>7.6</td><td>45.7</td><td>0.0</td><td>25.4</td><td>39.1</td><td>29.4</td><td>17.6</td></tr><tr><td>VILA-1.5-40B</td><td>25.5</td><td>5.3</td><td>27.6</td><td>46.5</td><td>0.7</td><td>30.2</td><td>37.1</td><td>31.5</td><td>25.0</td></tr><tr><td>LongVA-7B</td><td>21.9</td><td>5.1</td><td>18.1</td><td>27.4</td><td>26.1</td><td>23.4</td><td>39.8</td><td>26.9</td><td>8.7</td></tr><tr><td>LLaVA-NeXT-Video-7B</td><td>25.2</td><td>14.8</td><td>14.6</td><td>32.5</td><td>26.1</td><td>26.8</td><td>45.0</td><td>33.0</td><td>8.5</td></tr><tr><td>LLaVA-NeXT-Video-72B</td><td>29.1</td><td>19.0</td><td>25.4</td><td>46.3</td><td>26.1</td><td>29.0</td><td>38.8</td><td>33.0</td><td>15.5</td></tr><tr><td>LLaVA-OneVision-0.5B</td><td>28.6</td><td>38.4</td><td>30.1</td><td>32.0</td><td>24.3</td><td>22.0</td><td>41.8</td><td>34.5</td><td>5.4</td></tr><tr><td>LLaVA-OneVision-7B</td><td>25.3</td><td>13.8</td><td>8.5</td><td>45.5</td><td>26.1</td><td>28.6</td><td>41.2</td><td>27.9</td><td>11.1</td></tr><tr><td>LLaVA-OneVision-72B</td><td>28.9</td><td>8.2</td><td>23.8</td><td>54.1</td><td>26.1</td><td>30.4</td><td>38.1</td><td>33.0</td><td>17.1</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 7은 본 논문에서 제시된 VSI-Bench (tiny) 데이터셋에 대한 15가지 다양한 비디오 지원 다중 모달 대규모 언어 모델(MLLM)의 성능 평가 결과를 보여줍니다. 표에는 각 모델의 평균 정확도와 함께, 개체 수 세기, 상대 거리, 개체 크기, 방 크기, 상대 방향, 경로 계획, 외관 순서 등 8가지 시각적 공간 지능 작업에 대한 세부 정확도 점수가 포함되어 있습니다. 이 표는 다양한 MLLM의 시각적 공간 추론 능력을 비교하고, 강점과 약점을 분석하는 데 도움이 됩니다.</p><details><summary>read the caption</summary>Table 7: Complete VSI-Bench (tiny) evaluation results.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th>Avg.</th><th>Obj. Count</th><th>Abs. Dist.</th><th>Obj. Size</th><th>Room Size</th><th>Rel. Dist.</th><th>Rel. Dir.</th><th>Route Plan</th><th>Appr. Order</th></tr></thead><tbody><tr><td>Proprietary Models (API)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4o</td><td>19.5</td><td>46.1</td><td>0.1</td><td>7.1</td><td>38.2</td><td>26.2</td><td>18.0</td><td>4.6</td><td>15.4</td></tr><tr><td>Gemini-1.5 Flash</td><td>22.2</td><td>24.9</td><td>0.5</td><td>1.0</td><td>54.4</td><td>37.7</td><td>19.9</td><td>1.5</td><td>37.7</td></tr><tr><td>Gemini-1.5 Pro</td><td>13.0</td><td>25.5</td><td>19.5</td><td>12.6</td><td>10.6</td><td>17.5</td><td>1.7</td><td>2.5</td><td>14.4</td></tr><tr><td>Open-source Models</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InternVL2-2B</td><td>9.6</td><td>16.4</td><td>1.2</td><td>12.8</td><td>35.0</td><td>6.9</td><td>3.0</td><td>2.5</td><td>-0.8</td></tr><tr><td>InternVL2-8B</td><td>7.0</td><td>-8.8</td><td>1.9</td><td>9.9</td><td>39.1</td><td>9.7</td><td>-8.5</td><td>-3.0</td><td>16.0</td></tr><tr><td>InternVL2-40B</td><td>11.6</td><td>29.6</td><td>-2.2</td><td>7.3</td><td>31.1</td><td>11.8</td><td>-5.5</td><td>6.1</td><td>14.9</td></tr><tr><td>LongVILA-8B</td><td>1.4</td><td>-18.2</td><td>-3.5</td><td>7.9</td><td>-0.6</td><td>5.3</td><td>3.7</td><td>5.1</td><td>11.5</td></tr><tr><td>VILA-1.5-8B</td><td>7.3</td><td>10.0</td><td>14.2</td><td>4.6</td><td>18.8</td><td>6.7</td><td>-4.4</td><td>1.5</td><td>7.2</td></tr><tr><td>VILA-1.5-40B</td><td>5.7</td><td>17.1</td><td>-2.8</td><td>2.2</td><td>22.0</td><td>10.4</td><td>-11.4</td><td>0.0</td><td>7.9</td></tr><tr><td>LongVA-7B</td><td>7.2</td><td>32.9</td><td>-1.5</td><td>11.5</td><td>-3.9</td><td>9.7</td><td>3.5</td><td>-1.5</td><td>7.1</td></tr><tr><td>LLaVA-NeXT-Video-7B</td><td>10.5</td><td>33.8</td><td>-0.6</td><td>15.2</td><td>-1.9</td><td>16.7</td><td>-2.7</td><td>1.0</td><td>22.1</td></tr><tr><td>LLaVA-NeXT-Video-72B</td><td>11.7</td><td>29.9</td><td>-2.6</td><td>11.1</td><td>9.2</td><td>13.3</td><td>-2.0</td><td>2.0</td><td>33.0</td></tr><tr><td>LLaVA-OneVision-0.5B</td><td>-0.5</td><td>7.8</td><td>-1.7</td><td>-16.6</td><td>4.0</td><td>6.9</td><td>-5.0</td><td>0.0</td><td>0.3</td></tr><tr><td>LLaVA-OneVision-7B</td><td>7.0</td><td>33.9</td><td>11.7</td><td>1.9</td><td>-13.9</td><td>13.9</td><td>-6.0</td><td>1.5</td><td>13.3</td></tr><tr><td>LLaVA-OneVision-72B</td><td>11.4</td><td>35.4</td><td>0.1</td><td>3.5</td><td>11.4</td><td>12.1</td><td>1.8</td><td>-0.5</td><td>27.4</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 8은 비전 데이터를 사용하지 않고(즉, 비전이 비활성화된 상태에서) 다양한 모델이 VSI-Bench(tiny) 데이터셋에서 달성한 성능을 보여줍니다. 다양한 모델의 평균 정확도와 각 작업(개체 수 세기, 절대 거리, 개체 크기, 방 크기, 상대 거리, 상대 방향, 경로 계획, 외관 순서)에 대한 세부 정확도를 보여주어 모델의 시각적 공간 지능 능력을 비교 분석하는 데 도움이 됩니다. 특히, 다양한 모델 유형과 크기(매개변수 수) 간의 성능 차이를 파악하고, 개방형 모델과 독점 모델 간의 성능을 비교 분석할 수 있습니다.</p><details><summary>read the caption</summary>Table 8: Complete blind evaluation results.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-fee0531563e56017c1a3401a151e77fc class=gallery><img src=paper_images/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14171/&amp;title=Thinking%20in%20Space:%20How%20Multimodal%20Large%20Language%20Models%20See,%20Remember,%20and%20Recall%20Spaces" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14171/&amp;text=Thinking%20in%20Space:%20How%20Multimodal%20Large%20Language%20Models%20See,%20Remember,%20and%20Recall%20Spaces" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14171/&amp;subject=Thinking%20in%20Space:%20How%20Multimodal%20Large%20Language%20Models%20See,%20Remember,%20and%20Recall%20Spaces" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.14171/index.md",oid_likes="likes_paper-reviews/2412.14171/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.12571/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-17T00:00:00+00:00>17 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.14161/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-18T00:00:00+00:00>18 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>