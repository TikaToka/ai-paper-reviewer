[{"content": "|   | Model | Vision Size | Modality | Pretrain | Finetune | MSVD-QA | MSVD-QA | MSRVTT-QA | MSRVTT-QA | TGIF-QA* | TGIF-QA* | Activity Net-QA | Activity Net-QA |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Different Datasets** | LLaMA Adapter [43] | 404.3M | I | 567K | 52K | 54.9 | 3.1 | 43.8 | 2.7 | - | - | 34.2 | 2.7 |\n|  | VideoChat [18] | 1.2B | V | 25M | 18K | 56.3 | 2.8 | 45.0 | 2.5 | 21.3 | 1.9 | 26.5 | 2.2 |\n|  | Video-LLaMA [42] | 1.1B | V | 3.1M | 164K | 51.6 | 2.5 | 29.6 | 1.8 | - | - | 12.4 | 1.1 |\n|  | ChatUniVi [15] | 307M | V+I | 1.6M | 649K | 65.0 | 3.6 | 54.6 | 3.1 | 38.2 | 3.0 | 45.8 | 3.2 |\n|  | LLaMA-VID [21] | 1B | V+I | 790K | 763K | 69.7 | 3.7 | 57.7 | 3.2 | - | - | 47.4 | 3.3 |\n|  | Video-LLaVA [22] | 425M | V+I | 1.26M | 765K | 70.7 | 3.9 | 59.2 | 3.5 | 47.0 | 3.3 | 45.3 | 3.3 |\n|  | VideoChat2 [20] | 496M | V+I | 37M | 2M | 70.0 | 3.9 | 54.1 | 3.3 | - | - | 49.1 | 3.3 |\n| **Same Dataset** | Video-ChatGPT [26] | 307M | V | - | 100K | 64.9 | 3.3 | 49.3 | 2.8 | 40.7 | 3.1 | 35.2 | 2.8 |\n|  | Video-LLaVA [22] | 425M | V | 702K | 100K | 64.8 | - | 58.3 | - | 41.7 | - | 40.7 | - |\n|  | EVE* | 30M | V | 702K | 100K | 60.5 | 3.3 | 49.7 | 3.0 | 39.2 | 2.9 | 38.1 | 3.0 |\n|  | Video-Panda (ours) | 45M | V | 702K | 100K | 64.7 | 3.8 | 54.8 | 3.4 | 42.9 | 3.2 | 40.0 | 3.3 |", "caption": "Table 1: Comparison with other video-language models that use LLMs with 7B parameters on open-ended video question answering. Vision Size refers to #parameters of vision encoder and alignment modules. Modality indicates whether videos and/or images are used as training data. For TGIF-QA*, we re-evaluated the results since the performance depends on the current version of GPT-3.5 which changes over time and highly impacts the evaluation. EVE* is our extension of EVE [11] to video data.", "description": "\ud45c 1\uc740 7B \ub9e4\uac1c\ubcc0\uc218\ub97c \uac00\uc9c4 LLMs\ub97c \uc0ac\uc6a9\ud558\ub294 \ube44\ub514\uc624-\uc5b8\uc5b4 \ubaa8\ub378\ub4e4\uacfc \uc81c\uc548\ub41c \ube44\ub514\uc624 \ud310\ub2e4 \ubaa8\ub378\uc758 \uc624\ud508 \uc5d4\ub4dc \ube44\ub514\uc624 \uc9c8\ubb38 \uc751\ub2f5 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \ud45c\uc785\ub2c8\ub2e4.  Vision Size \uc5f4\uc740 \ube44\uc804 \uc778\ucf54\ub354\uc640 \uc815\ub82c \ubaa8\ub4c8\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc218\ub97c \ub098\ud0c0\ub0b4\uace0, Modality \uc5f4\uc740 \ud559\uc2b5 \ub370\uc774\ud130\ub85c \ube44\ub514\uc624\uc640 \uc774\ubbf8\uc9c0 \uc911 \ubb34\uc5c7\uc744 \uc0ac\uc6a9\ud588\ub294\uc9c0 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. TGIF-QA*\uc758 \uacbd\uc6b0 GPT-3.5\uc758 \ubc84\uc804 \ubcc0\ud654\uc5d0 \ub530\ub978 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubc18\uc601\ud558\uc5ec \uacb0\uacfc\ub97c \uc7ac\ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. EVE*\ub294 \uc774\ubbf8\uc9c0 \uc804\uc6a9 \ubaa8\ub378\uc778 EVE\ub97c \ube44\ub514\uc624 \ub370\uc774\ud130\uc5d0 \uc801\uc6a9\ud55c \ud655\uc7a5 \ubaa8\ub378\uc785\ub2c8\ub2e4.  \ud45c\ub294 \uac01 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\uc640 \uc810\uc218\ub97c \uc81c\uc2dc\ud558\uc5ec \ube44\ub514\uc624 \ud310\ub2e4 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "|       | Model                                      | Correctness | Detail | Context | Temporal | Consistency | AVG |\n| :---- | :------------------------------------------ | :-----------: | :----: | :------: | :-------: | :----------: | :-: |\n|       | <span class=\"ltx_font_italic\">Encoder-based Vision-Language Models</span> |             |        |         |          |             |     |\n| <span class=\"ltx_p\">Different Datasets</span> | VideoChat [<a href=\"https://arxiv.org/html/2412.18609v1#bib.bib18\">18</a>] |     2.23      |   2.50    |   2.53    |    1.94    |     2.24     | 2.29 |\n|       | LLaMA Adapter [<a href=\"https://arxiv.org/html/2412.18609v1#bib.bib43\">43</a>] |     2.03      |   2.32    |   2.30    |    1.98    |     2.15     | 2.16 |\n|       | Video-LLaMA [<a href=\"https://arxiv.org/html/2412.18609v1#bib.bib42\">42</a>] |     1.96      |   2.18    |   2.16    |    1.82    |     1.79     | 1.98 |\n|       | ChatUniVi [<a href=\"https://arxiv.org/html/2412.18609v1#bib.bib15\">15</a>] |     2.89      |   2.91    |   3.46    |    2.39    |     2.81     | 2.89 |\n|       | LLaMA-VID [<a href=\"https://arxiv.org/html/2412.18609v1#bib.bib21\">21</a>] |     2.96      |   3.00    |   3.53    |    2.46    |     2.51     | 2.89 |\n|       | Video-LLaVA [<a href=\"https://arxiv.org/html/2412.18609v1#bib.bib22\">22</a>] |     2.84      |   2.86    |   3.44    |    2.46    |     2.57     | 2.81 |\n|       | VideoChat2 [<a href=\"https://arxiv.org/html/2412.18609v1#bib.bib20\">20</a>] |     3.02      |   2.88    |   3.51    |    2.66    |     2.81     | 2.98 |\n| <span class=\"ltx_p\">Same Datasets</span> | <span class=\"ltx_font_italic\">Encoder-based Vision-Language Models</span> |             |        |         |          |             |     |\n|       | Video-ChatGPT [<a href=\"https://arxiv.org/html/2412.18609v1#bib.bib26\">26</a>] |     2.40      |   2.52    |   2.62    |    1.98    |     2.37     | 2.38 |\n|       | Video-LLaVA* [<a href=\"https://arxiv.org/html/2412.18609v1#bib.bib22\">22</a>] |     2.46      |   2.37    |   2.89    |    2.12    |     2.17     | 2.40 |\n|       | <span class=\"ltx_font_italic\">Encoder-free Vision-Language Models</span> |             |        |         |          |             |     |\n|       | Video-Panda (ours)                         |     2.74      |   2.47    |   3.01    |    2.26    |     2.36     | 2.57 |", "caption": "Table 2: Comparison of video-language models on fine-grained video question answering metrics (scale 1-5) across correctness, detail, context, temporal reasoning, and consistency. Video-LLaVA*: trained with video-only datasets for fair comparison.", "description": "\ud45c 2\ub294 \ube44\ub514\uc624 \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc138\ubd84\ud654\ub41c \ube44\ub514\uc624 \uc9c8\ubb38 \ub2f5\ubcc0 \uba54\ud2b8\ub9ad\uc5d0 \ub300\ud55c \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc815\ud655\uc131, \uc0c1\uc138 \uc815\ubcf4, \ub9e5\ub77d, \uc2dc\uac04\uc801 \ucd94\ub860 \ubc0f \uc77c\uad00\uc131 \ub4f1 \ub2e4\uc12f \uac00\uc9c0 \uce21\uba74\uc5d0\uc11c 1~5\uc810 \ucc99\ub3c4\ub85c \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. Video-LLaVA*\ub294 \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574 \ube44\ub514\uc624 \uc804\uc6a9 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4.  \uac01 \uc9c0\ud45c\ub294 \ubaa8\ub378\uc774 \ube44\ub514\uc624\uc758 \ub0b4\uc6a9\uc744 \uc5bc\ub9c8\ub098 \uc815\ud655\ud558\uac8c \uc774\ud574\ud558\uace0, \uc790\uc138\ud55c \uc815\ubcf4\ub97c \uc5bc\ub9c8\ub098 \uc798 \uc81c\uacf5\ud558\uace0, \ubb38\ub9e5\uc744 \uc5bc\ub9c8\ub098 \uc798 \ud30c\uc545\ud558\uba70, \uc2dc\uac04\uc801 \ud750\ub984\uc744 \uc5bc\ub9c8\ub098 \uc798 \ucd94\ub860\ud558\uace0, \uc751\ub2f5\uc758 \uc77c\uad00\uc131\uc744 \uc5bc\ub9c8\ub098 \uc798 \uc720\uc9c0\ud558\ub294\uc9c0\ub97c \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "4.2. \uc815\ub7c9\uc801 \ud3c9\uac00"}, {"content": "| Model | #Param.(M) | Inference time (ms) |\n|---|---|---|\n| VideoChatGPT [26] | 307 | 171 |\n| Video-LLaVA [22] | 425 | 125 |\n| Video-Panda | 45 | 41 |", "caption": "Table 3: Comparison of the number of parameters and inference time of the vision part.", "description": "\ubcf8 \ud45c\ub294 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c \ube44\ub514\uc624 \uc774\ud574 \ubaa8\ub378\uc758 \uc131\ub2a5 \ube44\uad50\ub97c \uc704\ud55c \ud45c\uc785\ub2c8\ub2e4.  'Vision part'\ub294 \uc774\ubbf8\uc9c0 \ub610\ub294 \ube44\ub514\uc624\ub97c \ucc98\ub9ac\ud558\ub294 \ubd80\ubd84\uc744 \uc758\ubbf8\ud558\uba70, \ubaa8\ub378\uc758 \ud30c\ub77c\ubbf8\ud130 \uc218\uc640 \ucd94\ub860 \uc18d\ub3c4\ub97c \ube44\uad50\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.  VideoChatGPT\uc640 Video-LLaVA \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec Video-Panda \ubaa8\ub378\uc758 \ud30c\ub77c\ubbf8\ud130 \uc218\uac00 \ud6e8\uc52c \uc801\uc73c\uba74\uc11c\ub3c4 \ucd94\ub860 \uc18d\ub3c4\uac00 \ube60\ub984\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 Video-Panda \ubaa8\ub378\uc758 \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Model | MSVD-QA | Activity Net-QA |\n|---|---|---|\n| **Spatial** |\n| w/o <row> | 63.2/3.7 | 39.5/3.3 |\n| w/o FSRA | 63.4/3.7 | 39.2/3.3 |\n| w/o LSD (avg pool) | 58.0/3.6 | 38.1/3.2 |\n| **Temporal** |\n| w/o LSTE | 63.6/3.7 | 39.4/3.3 |\n| w/o GSTRA | 63.0/3.7 | 38.2/3.2 |\n| w/o GSTRA & LSTE | 62.2/3.7 | 38.1/3.2 |\n| Video-Panda | **64.7/3.8** | **40.0/3.3** |", "caption": "Table 4: Ablation study on the impact of removing different spatial and temporal modules used in our design.", "description": "\ud45c 4\ub294 \uc81c\uc548\ub41c Spatio-Temporal Alignment Block (STAB)\uc758 \uacf5\uac04 \ubc0f \uc2dc\uac04\uc801 \ubaa8\ub4c8 \uc81c\uac70\uc5d0 \ub530\ub978 \uc601\ud5a5\uc744 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub4c8(LSTE, GSTRA, FSRA, LSD, <row>)\uc744 \uc81c\uac70\ud588\uc744 \ub54c MSVD-QA \ubc0f Activity Net-QA \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \uc815\ud655\ub3c4\uc640 \uc810\uc218 \ubcc0\ud654\ub97c \ube44\uad50\ud558\uc5ec \uac01 \ubaa8\ub4c8\uc758 \uc5ed\ud560\uacfc \uc911\uc694\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uacb0\uacfc\uc801\uc73c\ub85c \uac01 \ubaa8\ub4c8\uc774 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \uc81c\uc2dc\ud558\uc5ec STAB \uc124\uacc4\uc758 \ud0c0\ub2f9\uc131\uc744 \ub4b7\ubc1b\uce68\ud569\ub2c8\ub2e4.", "section": "3. Method"}, {"content": "| Distillation Loss | MSVD-QA | Activity Net-QA |\n|---|---|---|\n| w/o Distillation | 63.1/3.7 | 39.8/3.3 |\n| Mean Squared Error | 63.5/3.7 | 38.2/3.2 |\n| Negative Cosine Similarity | **64.7/3.8** | **40.0/3.3** |", "caption": "Table 5: Ablation study on the impact of not using distillation loss or a different (MSE) loss.", "description": "\ud45c 5\ub294 \uc99d\ub958 \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uac70\ub098 \ub2e4\ub978 \uc190\uc2e4 \ud568\uc218(MSE)\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\uc785\ub2c8\ub2e4.  \ube44\uad50\ub97c \uc704\ud574 \uc99d\ub958 \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0\uc640 MSE \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0, \uadf8\ub9ac\uace0 \uc99d\ub958 \uc190\uc2e4 \uc790\uccb4\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0\uc758 MSVD-QA\uc640 Activity Net-QA \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc131\ub2a5 \uc9c0\ud45c(\uc815\ud655\ub3c4/\uc810\uc218)\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \uac01 \ubc29\ubc95\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. \ubd84\uc11d \ubc0f \uc808\uc81c \uc2e4\ud5d8"}, {"content": "| Hyperparameter | Stage-1 | Stage-2 | Stage-3 |\n|---|---|---|---| \n| Batch Size | 2048 | 2048 | 1024 |\n| Learning Rate (lr) | 4e-4 | 4e-5 | 2e-5 |\n| LR Schedule | cosine decay | cosine decay | cosine decay |\n| LR Warmup Ratio | 0.03 | 0.01 | 0.01 |\n| Weight Decay | 0 | 0 | 0 |\n| Epoch | 1 | 1 | 1 |\n| Optimizer | AdamW | AdamW | AdamW |\n| DeepSpeed Stage | 2 | 2 | 2 |\n| LLM | Frozen | Trainable | Trainable |\n| STAB | Trainable | Trainable | Trainable |", "caption": "Table 6: Hyperparameter Settings", "description": "\ud45c 6\uc740 Video-Panda \ubaa8\ub378\uc758 \uc138 \uac00\uc9c0 \ud6c8\ub828 \ub2e8\uacc4(Stage-1, Stage-2, Stage-3)\uc5d0 \ub300\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ub2e8\uacc4\ubcc4\ub85c \ubc30\uce58 \ud06c\uae30, \ud559\uc2b5\ub960, \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904, \uac00\uc911\uce58 \uac10\uc1e0, \uc635\ud2f0\ub9c8\uc774\uc800, \uadf8\ub9ac\uace0 STAB(Spatio-Temporal Alignment Block)\uc5d0 \ub300\ud55c \ud6c8\ub828 \uc124\uc815 \ub4f1\uc774 \uc790\uc138\ud558\uac8c \ub098\uc5f4\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 Video-Panda \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \ub2e4\uc591\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.", "section": "3. Method"}, {"content": "| #Samples for Initial Alignment | MSVD-QA | Activity Net-QA |\n|---|---|---|\n| 702K Video-Text Pairs (full) | 63.7/3.8 | 39.7/3.3 |\n| 351K Video-Text Pairs (half) | 64.7/3.8 | 40.0/3.3 |", "caption": "Table 7: Ablation study on amount of data for the first training stage.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 \uccab \ubc88\uc9f8 \ud6c8\ub828 \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \ub370\uc774\ud130 \uc591\uc5d0 \ub530\ub978 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  351K \ube44\ub514\uc624-\ud14d\uc2a4\ud2b8 \uc30d(\ub370\uc774\ud130\uc14b\uc758 \uc808\ubc18)\uacfc \uc804\uccb4 702K \ube44\ub514\uc624-\ud14d\uc2a4\ud2b8 \uc30d\uc744 \uac01\uac01 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\uc2dc\ucf30\uc744 \ub54c\uc758 MSVD-QA\uc640 Activity Net-QA \uc131\ub2a5 \uc9c0\ud45c(\uc815\ud655\ub3c4 \ubc0f \uc810\uc218)\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \ucd5c\uc801\uc758 \ub370\uc774\ud130 \uc591\uc744 \ucc3e\ub294 \uc2e4\ud5d8 \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Model | MSVD-QA | MSRVTT-QA | TGIF-QA | Activity Net-QA |\n|---|---|---|---|---|\n| Before LSTE | 64.2/3.8 | 54.6/3.4 | 42.7/3.2 | **42.3/3.3** |\n| After LSTE (Ours) | **64.7/3.8** | **54.8/3.4** | **42.9/3.2** | 40.0/3.3 |", "caption": "Table 8: Ablation study on downsampling positions of LSD.", "description": "\ud45c 8\uc740 LSD(Local Spatial Downsampling)\uc758 \ub2e4\uc6b4\uc0d8\ud50c\ub9c1 \uc704\uce58\ub97c \ubcc0\uacbd\ud588\uc744 \ub54c \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  LSTE(Local Spatio-Temporal Encoding) \uc804\uacfc \ud6c4 \ub450 \uac00\uc9c0 \uc704\uce58\uc5d0\uc11c LSD\ub97c \uc801\uc6a9\ud55c \uacb0\uacfc\ub97c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uac01 \uc704\uce58\uc5d0\uc11c\uc758 \uc131\ub2a5 \ubcc0\ud654\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \uc81c\uc2dc\ud569\ub2c8\ub2e4.  MSVD-QA, MSRVTT-QA, TGIF-QA, Activity Net-QA \ub124 \uac00\uc9c0 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4 \uc810\uc218\uc640 \uc2e0\ub8b0\ub3c4 \uc810\uc218\ub97c \uc81c\uc2dc\ud558\uc5ec, LSD\uc758 \ub2e4\uc6b4\uc0d8\ud50c\ub9c1 \uc704\uce58\uac00 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.", "section": "4.2 \uc815\ub7c9\uc801 \ud3c9\uac00"}, {"content": "| Model | MSVD-QA | Activity Net-QA |\n|---|---|---|\n| w/o LSD (half-resolution) | 48.2/3.3 | 38.5/3.2 |\n| w/o LSD (avg pool) | 58.0/3.6 | 38.1/3.2 |\n| w/o LSD (PR) | 43.4/3.2 | 27.8/2.9 |\n| Video-Panda (LSD) | **64.7/3.8** | **40.0/3.3** |", "caption": "Table 9: Ablation study on downsampling methods. PR stands for Perceiver Resampler\u00a0[2].", "description": "\ud45c 9\ub294 \ub2e4\uc591\ud55c \ub2e4\uc6b4\uc0d8\ud50c\ub9c1 \uae30\ubc95\uc758 \uc601\ud5a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \ucd94\uac00 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ubcf8 \uc5f0\uad6c\uc5d0\uc11c\ub294 \ube44\ub514\uc624 \ud504\ub808\uc784\uc758 \uacf5\uac04\uc801 \ud574\uc0c1\ub3c4\ub97c \uc904\uc774\uae30 \uc704\ud574 \ud559\uc2b5 \uae30\ubc18\uc758 \uc9c0\uc5ed\uc801 \uacf5\uac04 \ub2e4\uc6b4\uc0d8\ud50c\ub9c1(LSD) \uae30\ubc95\uc744 \uc81c\uc548\ud569\ub2c8\ub2e4.  \uc774 \ud45c\ub294 LSD\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \uacbd\uc6b0, \ud3c9\uade0 \ud480\ub9c1\uc744 \uc0ac\uc6a9\ud55c \uacbd\uc6b0, \uadf8\ub9ac\uace0 Perceiver Resampler [2]\ub97c \uc0ac\uc6a9\ud55c \uacbd\uc6b0\uc758 \uc138 \uac00\uc9c0 \ub2e4\ub978 \ub2e4\uc6b4\uc0d8\ud50c\ub9c1 \ubc29\ubc95\uc5d0 \ub300\ud55c \ube44\uad50 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubc29\ubc95\uc5d0 \ub530\ub978 MSVD-QA\uc640 Activity Net-QA \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \uc815\ud655\ub3c4\uc640 \uc810\uc218\ub97c \ube44\uad50\ud558\uc5ec LSD \uae30\ubc95\uc758 \ud6a8\uacfc\ub97c \uac80\uc99d\ud569\ub2c8\ub2e4. Perceiver Resampler\ub294 \ube44\uad50 \ub300\uc0c1 \uae30\ubc95 \uc911 \ud558\ub098\uc785\ub2c8\ub2e4.", "section": "4.2 \uc815\ub7c9\uc801 \ud3c9\uac00"}, {"content": "| Model | MSVD-QA | Activity Net-QA |\n|---|---|---|\n| CLIP | 60.3/3.5 | 38.6/3.2 |\n| InternVideo | 62.5/3.6 | 39.6/3.2 |\n| DINOv2 | 61.7/3.5 | 38.1/3.2 |\n| LanguageBind (Video-Panda) | **64.7/3.8** | **40.0/3.3** |", "caption": "Table 10: Ablation study different teacher encoders.", "description": "\ud45c 10\uc740 \ub2e4\uc591\ud55c teacher \uc778\ucf54\ub354\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \ube44\uad50 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Video-Panda \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 teacher \uc778\ucf54\ub354\uac00 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud655\uc778\ud558\uae30 \uc704\ud574 LanguageBind, InternVideo, CLIP, DINOv2 \ub124 \uac00\uc9c0 \uc778\ucf54\ub354\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud5d8\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uac01 teacher \uc778\ucf54\ub354\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c MSVD-QA\uc640 Activity Net-QA \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uc5ec \ub098\ud0c0\ub0b4\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 Video-Panda \ubaa8\ub378\uc5d0\uc11c \uac00\uc7a5 \uc801\ud569\ud55c teacher \uc778\ucf54\ub354\ub97c \ud655\uc778\ud558\uace0,  \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud558\ub294 \uc694\uc778\uc744 \ubd84\uc11d\ud558\uace0\uc790 \ud569\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8"}]