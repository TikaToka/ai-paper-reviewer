{"references": [{"fullname_first_author": "Yihan Cao", "paper_title": "Instruction mining: Instruction data selection for tuning large language models", "publication_date": "2024-XX-XX", "reason": "This paper introduces the concept of instruction mining, a crucial technique in improving the quality of instruction tuning datasets for LLMs, which is the core focus of the current paper."}, {"fullname_first_author": "Yann Dubois", "paper_title": "Length-controlled alpacaeval: A simple way to debias automatic evaluators", "publication_date": "2024-04-04", "reason": "This paper presents AlpacaEval V2, the main evaluation benchmark used in the current paper to assess the performance of LLMs, providing a crucial metric for evaluating the impact of the proposed method."}, {"fullname_first_author": "Katie Kang", "paper_title": "Unfamiliar finetuning examples control how language models hallucinate", "publication_date": "2024-03-05", "reason": "This paper reveals the critical relationship between the alignment of world knowledge from IFT datasets and LLMs' internal knowledge, a key finding that directly motivates the current research."}, {"fullname_first_author": "Ming Li", "paper_title": "From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning", "publication_date": "2024-XX-XX", "reason": "This paper explores data selection techniques for instruction tuning, a related area of research that shares the goal of improving the effectiveness of IFT datasets, which the current paper also addresses."}, {"fullname_first_author": "Yizhong Wang", "paper_title": "Self-instruct: Aligning language models with self-generated instructions", "publication_date": "2023-XX-XX", "reason": "This paper introduces the Self-Instruct method which is used to generate the Alpaca dataset, one of the key datasets used in the experiments of the current paper, making it foundational to the experimental results."}]}