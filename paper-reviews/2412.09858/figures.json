[{"figure_path": "https://arxiv.org/html/2412.09858/x1.png", "caption": "Figure 1: RLDG improves generalist robot policies like OpenVLA and Octo by training with specialist RL policies and using them to generate high-quality fine-tuning datasets. It has the flexibility to distill knowledge from multiple RL policies trained on individual narrowly scoped tasks into a single generalist. It can also be applied to the most critical sub-task of a long-horizon manipulation task, improving the success rate at the \u201cbottleneck\" while leveraging human demonstrations on parts of the task where it suffices.", "description": "RLDG\ub294 OpenVLA \ubc0f Octo\uc640 \uac19\uc740 \uc77c\ubc18 \ub85c\ubd07 \uc815\ucc45\uc744 \uc804\ubb38 RL \uc815\ucc45\uc73c\ub85c \ud559\uc2b5\uc2dc\ud0a4\uace0 \uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec \uace0\ud488\uc9c8 \ubbf8\uc138 \uc870\uc815 \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uc0dd\uc131\ud558\uc5ec \uac1c\uc120\ud569\ub2c8\ub2e4. \uac1c\ubcc4\uc801\uc73c\ub85c \ubc94\uc704\uac00 \uc881\uc740 \uc791\uc5c5\uc5d0 \ub300\ud574 \ud559\uc2b5\ub41c \uc5ec\ub7ec RL \uc815\ucc45\uc5d0\uc11c \uc5bb\uc740 \uc9c0\uc2dd\uc744 \ub2e8\uc77c \uc77c\ubc18 \uc815\ucc45\uc73c\ub85c \ucd94\ucd9c\ud560 \uc218 \uc788\ub294 \uc720\uc5f0\uc131\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \ub610\ud55c \uc7a5\uae30 \uc870\uc791 \uc791\uc5c5\uc758 \uac00\uc7a5 \uc911\uc694\ud55c \ud558\uc704 \uc791\uc5c5\uc5d0 \uc801\uc6a9\ud558\uc5ec \"\ubcd1\ubaa9 \ud604\uc0c1\"\uc758 \uc131\uacf5\ub960\uc744 \uac1c\uc120\ud558\ub294 \ub3d9\uc2dc\uc5d0 \ucda9\ubd84\ud55c \uc791\uc5c5 \ubd80\ubd84\uc5d0 \ub300\ud55c \uc778\uac04 \ub370\ubaa8\ub97c \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. \uac15\ud654 \ud559\uc2b5 \uae30\ubc18 \uc77c\ubc18 \uc815\ucc45 \ucd94\ucd9c (Reinforcement Learning Distilled Generalist)"}, {"figure_path": "https://arxiv.org/html/2412.09858/x2.png", "caption": "Figure 2: We use a Franka Emika Panda arm with a parallel jaw gripper teleoperated by a 3Dconnexion SpaceMouse device. There is a single RealSense D405 camera mounted on the robot\u2019s wrist for image observations.", "description": "\uc774 \uadf8\ub9bc\uc740 RLDG \uc2e4\ud5d8\uc5d0 \uc0ac\uc6a9\ub41c \ub85c\ubd07 \uc124\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Franka Emika Panda \ud314, \ud3c9\ud589 \ud131 \uadf8\ub9ac\ud37c, 3Dconnexion SpaceMouse, \uc190\ubaa9\uc5d0 \uc7a5\ucc29\ub41c RealSense D405 \uce74\uba54\ub77c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. SpaceMouse\ub294 \ub85c\ubd07 \ud314\uc744 \uc6d0\uaca9 \uc870\uc791\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\uba70, RealSense \uce74\uba54\ub77c\ub294 \ub85c\ubd07\uc758 \uc190\ubaa9 \uad00\uc810\uc5d0\uc11c \uc774\ubbf8\uc9c0 \uad00\uce21\uac12\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4.1. Experimental Setup and Tasks"}, {"figure_path": "https://arxiv.org/html/2412.09858/x3.png", "caption": "Figure 3: Illustrations of tasks used to evaluate RLDG. (A) Precise Connector Insertion includes three training objects and four unseen test objects for evaluating policy generalization. (B) Pick and Place involves an unseen scenario that tests the policy\u2019s visual robustness to different backgrounds and objects. (C) FMB Insertion involves inserting a pre-grasped object in a moving board while (D) FMB Assembly starts with the object on the table and involves an additional grasping phase.", "description": "\uc774 \uadf8\ub9bc\uc740 RLDG\ub97c \ud3c9\uac00\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub41c \uc791\uc5c5\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (A) \uc815\ubc00 \ucee4\ub125\ud130 \uc0bd\uc785\uc740 \uc815\ucc45 \uc77c\ubc18\ud654\ub97c \ud3c9\uac00\ud558\uae30 \uc704\ud55c 3\uac1c\uc758 \ud6c8\ub828 \uac1d\uccb4\uc640 4\uac1c\uc758 \ubcf4\uc774\uc9c0 \uc54a\ub294 \ud14c\uc2a4\ud2b8 \uac1d\uccb4\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. (B) \ud53d \uc564 \ud50c\ub808\uc774\uc2a4\ub294 \uc815\ucc45\uc758 \ub2e4\uc591\ud55c \ubc30\uacbd \ubc0f \uac1d\uccb4\uc5d0 \ub300\ud55c \uc2dc\uac01\uc801 \uacac\uace0\uc131\uc744 \ud14c\uc2a4\ud2b8\ud558\ub294 \ubcf4\uc774\uc9c0 \uc54a\ub294 \uc2dc\ub098\ub9ac\uc624\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. (C) FMB \uc0bd\uc785\uc740 \uc6c0\uc9c1\uc774\ub294 \ubcf4\ub4dc\uc5d0 \ubbf8\ub9ac \uc7a1\uc740 \ubb3c\uccb4\ub97c \uc0bd\uc785\ud558\ub294 \uc791\uc5c5\uc744 \ud3ec\ud568\ud558\ub294 \ubc18\uba74 (D) FMB \uc870\ub9bd\uc740 \ud14c\uc774\ube14 \uc704\uc758 \ubb3c\uccb4\uc5d0\uc11c \uc2dc\uc791\ud558\uc5ec \ucd94\uac00\uc801\uc778 \uc7a1\uae30 \ub2e8\uacc4\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. \uc989, RLDG\ub294 RL \uc815\ucc45\uc5d0\uc11c \uc0dd\uc131\ub41c \uace0\ud488\uc9c8 \ub370\uc774\ud130\ub85c \uc77c\ubc18\ud654 \uc815\ucc45\uc744 \ubbf8\uc138 \uc870\uc815\ud558\ub294 \uac04\ub2e8\ud55c \ubc29\ubc95\uc785\ub2c8\ub2e4. RL\ub85c \ud6c8\ub828\ub41c \uc815\ucc45\uc740 \ud2b9\uc815 \uc791\uc5c5\uc5d0\uc11c \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc9c0\ub9cc \uc81c\ub85c\uc0f7 \uc77c\ubc18\ud654 \ubc0f \uad50\ub780\uc5d0 \ub300\ud55c \uacac\uace0\uc131\uc774 \ubd80\uc871\ud55c \uacbd\uc6b0\uac00 \ub9ce\uc2b5\ub2c8\ub2e4. \ubc18\ub300\ub85c \uc77c\ubc18\ud654 \uc815\ucc45\uc740 \uc77c\ubc18\ud654\uc5d0\ub294 \ud0c1\uc6d4\ud558\uc9c0\ub9cc \uc0ac\ub78c\uc758 \ub370\ubaa8\ub85c \ud6c8\ub828\ud560 \ub54c \ub192\uc740 \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\uae30 \uc5b4\ub824\uc6b8 \uc218 \uc788\uc2b5\ub2c8\ub2e4. RLDG\ub294 \uc9c0\uc2dd \uc99d\ub958\ub97c \ud1b5\ud574 \uc774\ub7ec\ud55c \ucc28\uc774\ub97c \ud574\uc18c\ud558\uc5ec \uc0ac\ub78c\uc758 \ub370\ubaa8\ub85c \ubbf8\uc138 \uc870\uc815\ud558\ub294 \uac83\ubcf4\ub2e4 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uc77c\ubc18\ud654 \uc815\ucc45\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.", "section": "4. Experiment and Results"}, {"figure_path": "https://arxiv.org/html/2412.09858/extracted/6066371/figures/success_rate_comparison_ood.png", "caption": "Figure 4: Success rate comparison of OpenVLA and Octo policies fine-tuned with RLDG versus conventional methods using human demonstrations.\nBoth generalists trained with RLDG consistently outperform their counterparts trained with the same number of successful expert human demonstrations in both training and unseen scenarios.", "description": "\uc774 \uadf8\ub9bc\uc740 RLDG\ub85c \ubbf8\uc138 \uc870\uc815\ub41c OpenVLA \ubc0f Octo \uc815\ucc45\uacfc \uc0ac\ub78c \ub370\ubaa8\ub97c \uc0ac\uc6a9\ud558\ub294 \uae30\uc874 \ubc29\ubc95\uc758 \uc131\uacf5\ub960 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. RLDG\ub85c \ud6c8\ub828\ub41c \ub450 \uc77c\ubc18 \uc815\ucc45 \ubaa8\ub450 \uad50\uc721 \ubc0f \ubbf8\uacf5\uac1c \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \ub3d9\uc77c\ud55c \uc218\uc758 \uc131\uacf5\uc801\uc778 \uc804\ubb38\uac00 \uc778\uac04 \ub370\ubaa8\ub85c \ud6c8\ub828\ub41c \uc815\ucc45\ubcf4\ub2e4 \uc9c0\uc18d\uc801\uc73c\ub85c \uc131\ub2a5\uc774 \ub6f0\uc5b4\ub0a9\ub2c8\ub2e4.", "section": "4. Experiment and Results"}, {"figure_path": "https://arxiv.org/html/2412.09858/extracted/6066371/figures/success_rate_vs_demos.png", "caption": "Figure 5: Success rate of OpenVLA policies fine-tuned on different sizes of RL-generated and human-collected datasets. When evaluated on seen (VGA) and unseen (Type C) Connector Insertion tasks, RLDG shows superior sample efficiency, requiring significantly fewer demonstrations to achieve perfect success rate in both scenarios while the performance of conventional method saturates in the unseen case.", "description": "\uc774 \uadf8\ub9bc\uc740 OpenVLA \uc815\ucc45\uc744 \uc11c\ub85c \ub2e4\ub978 \ud06c\uae30\uc758 RL \uc0dd\uc131 \ubc0f \uc0ac\ub78c\uc774 \uc218\uc9d1\ud55c \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \ubbf8\uc138 \uc870\uc815\ud55c \uc131\uacf5\ub960\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ubcf8(VGA) \ubc0f \ubbf8\uacf5\uac1c(Type C) \ucee4\ub125\ud130 \uc0bd\uc785 \uc791\uc5c5\uc5d0\uc11c \ud3c9\uac00\ud588\uc744 \ub54c RLDG\ub294 \ub6f0\uc5b4\ub09c \uc0d8\ud50c \ud6a8\uc728\uc131\uc744 \ubcf4\uc5ec\uc8fc\uba70 \ub450 \uc2dc\ub098\ub9ac\uc624 \ubaa8\ub450\uc5d0\uc11c \uc644\ubcbd\ud55c \uc131\uacf5\ub960\uc744 \ub2ec\uc131\ud558\ub294 \ub370 \ud6e8\uc52c \uc801\uc740 \ub370\ubaa8\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. \ubc18\uba74 \uae30\uc874 \ubc29\ubc95\uc758 \uc131\ub2a5\uc740 \ubbf8\uacf5\uac1c \uc0ac\ub840\uc5d0\uc11c \ud3ec\ud654 \uc0c1\ud0dc\uc785\ub2c8\ub2e4.", "section": "4.2. RLDG vs. Conventional Fine-tuning"}, {"figure_path": "https://arxiv.org/html/2412.09858/extracted/6066371/figures/cycle_time_comparison_ood.png", "caption": "Figure 6: Cycle time comparison between policies trained with RL data versus human demonstrations. N/A for RL in FMB Assembly denotes policy not trained on the whole task, while N/A for fine-tuned policies denotes no successes recorded. The RL-trained policies generally achieve faster execution times across tasks, demonstrating the efficiency benefits of using RL-generated data for policy training.", "description": "\uc774 \uadf8\ub9bc\uc740 \uac15\ud654 \ud559\uc2b5(RL) \ub370\uc774\ud130\uc640 \uc778\uac04 \ub370\ubaa8 \ub370\uc774\ud130\ub85c \ud559\uc2b5\ub41c \uc815\ucc45 \uac04\uc758 \uc791\uc5c5 \uc644\ub8cc \uc8fc\uae30 \uc2dc\uac04\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. FMB \uc870\ub9bd\uc5d0\uc11c RL\uc5d0 \ub300\ud574 \"N/A\"\ub294 \uc815\ucc45\uc774 \uc804\uccb4 \uc791\uc5c5\uc5d0 \ub300\ud574 \ud559\uc2b5\ub418\uc9c0 \uc54a\uc558\uc74c\uc744 \ub098\ud0c0\ub0b4\uace0, \ubbf8\uc138 \uc870\uc815\ub41c \uc815\ucc45\uc5d0 \ub300\ud55c \"N/A\"\ub294 \uc131\uacf5\uc774 \uae30\ub85d\ub418\uc9c0 \uc54a\uc558\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. RL\ub85c \ud559\uc2b5\ub41c \uc815\ucc45\uc740 \uc77c\ubc18\uc801\uc73c\ub85c \uc791\uc5c5\uc5d0\uc11c \ub354 \ube60\ub978 \uc2e4\ud589 \uc2dc\uac04\uc744 \ub2ec\uc131\ud558\uc5ec \uc815\ucc45 \ud559\uc2b5\uc5d0 RL \uc0dd\uc131 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud560 \ub54c\uc758 \ud6a8\uc728\uc131 \uc774\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4. Experiment and Results"}, {"figure_path": "https://arxiv.org/html/2412.09858/extracted/6066371/figures/fmb_scaling.png", "caption": "Figure 7: Fine-tuning success rate on the FMB insertion task with different fine-tuning data sources and varied dataset sizes (from 25 trajectories to 300 trajectories). Human: demo trajectories collected by human teleoperators. Human + RL actions: the same human demo trajectories but with all the actions relabeled by a trained RL agent. RL: rollouts collected by the RL agent. RL data consistently provide better fine-tuning performance than human data. Human + RL actions closes the gap mostly, suggesting that most of the benefits of RL data come from it having better action quality.", "description": "\uc774 \uadf8\ub9bc\uc740 FMB \uc0bd\uc785 \uc791\uc5c5\uc5d0\uc11c \ub2e4\uc591\ud55c \ub370\uc774\ud130 \uc18c\uc2a4\uc640 \ub370\uc774\ud130\uc138\ud2b8 \ud06c\uae30\uc5d0 \ub530\ub978 \uc77c\ubc18\ud654 \uc815\ucc45\uc758 \ubbf8\uc138 \uc870\uc815 \uc131\uacf5\ub960\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc778\uac04 \uc2dc\uc5f0\uc790\uc758 \ub370\ubaa8 \uada4\uc801, RL \uc5d0\uc774\uc804\ud2b8\uac00 \ub2e4\uc2dc \ub808\uc774\ube14\uc744 \uc9c0\uc815\ud55c \ub3d9\uc77c\ud55c \uc778\uac04 \ub370\ubaa8 \uada4\uc801, RL \uc5d0\uc774\uc804\ud2b8\uac00 \uc218\uc9d1\ud55c \ub864\uc544\uc6c3\uc758 \uc138 \uac00\uc9c0 \ub370\uc774\ud130 \uc18c\uc2a4\uac00 \ube44\uad50\ub429\ub2c8\ub2e4. RL \ub370\uc774\ud130\ub294 \uc778\uac04 \ub370\uc774\ud130\ubcf4\ub2e4 \uc77c\uad00\ub418\uac8c \ub354 \ub098\uc740 \ubbf8\uc138 \uc870\uc815 \uc131\ub2a5\uc744 \uc81c\uacf5\ud558\uba70, RL \ub370\uc774\ud130\uac00 \ub354 \ub098\uc740 \ud589\ub3d9 \ud488\uc9c8\uc744 \uac00\uc9c0\uace0 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4. \uc778\uac04 + RL \uc791\uc5c5\uc740 \uc778\uac04\uacfc RL \ub370\uc774\ud130 \uac04\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ub300\ubd80\ubd84 \uc904\uc5ec RL \ub370\uc774\ud130\uc758 \uc774\uc810\uc774 \ub354 \ub098\uc740 \uc791\uc5c5 \ud488\uc9c8\uc5d0\uc11c \ube44\ub86f\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5. Analysis: why is RL data better than human data?"}, {"figure_path": "https://arxiv.org/html/2412.09858/extracted/6066371/figures/multimodality.png", "caption": "Figure 8: Action distribution visualization for RL data and human demo data for the FMB insertion task. We visualize the first two dimensions of the dataset actions after filtering all the transitions in the dataset where the end-effector positions are close to the position shown in the image on the left (x\ud835\udc65xitalic_x/y\ud835\udc66yitalic_y coordinates are both within 4444mm and z\ud835\udc67zitalic_z coordinate is within 10101010mm). The robot arm needs to move in the -x\ud835\udc65xitalic_x direction and in the -y\ud835\udc66yitalic_y direction to reach the insertion point. The first two dimensions of the action space corresponds to the control of the x\ud835\udc65xitalic_x and y\ud835\udc66yitalic_y position of the end-effector position correspondingly. Human actions are clustered around the center of the action space whereas the RL actions are more optimized, and mostly found near the correct corner (bottom-left) of the action space.", "description": "RL \ub370\uc774\ud130\uc640 \uc0ac\ub78c \ub370\ubaa8 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \ud589\ub3d9 \ubd84\ud3ec\ub97c \uc2dc\uac01\ud654\ud55c \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. FMB \uc0bd\uc785 \uc791\uc5c5\uc5d0\uc11c \uc5d4\ub4dc \uc774\ud399\ud130 \uc704\uce58\uac00 \uc67c\ucabd \uc774\ubbf8\uc9c0\uc5d0 \ud45c\uc2dc\ub41c \uc704\uce58\uc5d0 \uac00\uae4c\uc6b4 \ub370\uc774\ud130\uc14b\uc758 \ud589\ub3d9\ub9cc \ud544\ud130\ub9c1\ud558\uc5ec \uc2dc\uac01\ud654\ud588\uc2b5\ub2c8\ub2e4. \ub85c\ubd07 \ud314\uc740 \uc0bd\uc785 \uc9c0\uc810\uc5d0 \ub3c4\ub2ec\ud558\uae30 \uc704\ud574 -x \ubc0f -y \ubc29\ud5a5\uc73c\ub85c \uc6c0\uc9c1\uc5ec\uc57c \ud569\ub2c8\ub2e4. \ud589\ub3d9 \uacf5\uac04\uc758 \ucc98\uc74c \ub450 \ucc28\uc6d0\uc740 \uc5d4\ub4dc \uc774\ud399\ud130 \uc704\uce58\uc758 x \ubc0f y \uc704\uce58 \uc81c\uc5b4\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4. \uc0ac\ub78c\uc758 \ud589\ub3d9\uc740 \ud589\ub3d9 \uacf5\uac04\uc758 \uc911\uc559\uc5d0 \uc9d1\uc911\ub418\uc5b4 \uc788\ub294 \ubc18\uba74, RL \ud589\ub3d9\uc740 \ub354 \ucd5c\uc801\ud654\ub418\uc5b4 \uc788\uc73c\uba70 \ub300\ubd80\ubd84 \ud589\ub3d9 \uacf5\uac04\uc758 \uc62c\ubc14\ub978 \ubaa8\uc11c\ub9ac(\uc67c\ucabd \uc544\ub798) \uadfc\ucc98\uc5d0\uc11c \ubc1c\uacac\ub429\ub2c8\ub2e4.", "section": "5. Analysis: why is RL data better than human data?"}]