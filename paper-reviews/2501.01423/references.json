{"references": [{"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-01", "reason": "This paper introduces the foundation of latent diffusion models, a core concept of the current work."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Scaling rectified flow transformers for high-resolution image synthesis", "publication_date": "2024-01-01", "reason": "This paper addresses a similar optimization dilemma in high-resolution image generation, providing a comparative context for the current research."}, {"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2021-11-01", "reason": "This paper introduces a vision foundation model that is leveraged in this paper for the core VA-VAE methodology."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a vision foundation model used in ablation studies to demonstrate the impact of different foundation models on performance."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-10-01", "reason": "This paper introduces the Diffusion Transformer architecture, which is the main generative model used in this research."}]}