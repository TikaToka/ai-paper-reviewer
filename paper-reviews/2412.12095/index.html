<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Causal Diffusion Transformers for Generative Modeling &#183; AI Paper Reviews by AI</title>
<meta name=title content="Causal Diffusion Transformers for Generative Modeling &#183; AI Paper Reviews by AI"><meta name=description content="CausalFusion은 확산 및 자기 회귀 모델을 결합하여 생성 모델링에서 최첨단 결과를 달성하고 새로운 기능을 가능하게 합니다."><meta name=keywords content="Computer Vision,Image Generation,🏢 ByteDance Research,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12095/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12095/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="Causal Diffusion Transformers for Generative Modeling"><meta property="og:description" content="CausalFusion은 확산 및 자기 회귀 모델을 결합하여 생성 모델링에서 최첨단 결과를 달성하고 새로운 기능을 가능하게 합니다."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-16T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-16T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Image Generation"><meta property="article:tag" content="🏢 ByteDance Research"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12095/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12095/cover.png"><meta name=twitter:title content="Causal Diffusion Transformers for Generative Modeling"><meta name=twitter:description content="CausalFusion은 확산 및 자기 회귀 모델을 결합하여 생성 모델링에서 최첨단 결과를 달성하고 새로운 기능을 가능하게 합니다."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Causal Diffusion Transformers for Generative Modeling","headline":"Causal Diffusion Transformers for Generative Modeling","abstract":"CausalFusion은 확산 및 자기 회귀 모델을 결합하여 생성 모델링에서 최첨단 결과를 달성하고 새로운 기능을 가능하게 합니다.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.12095\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-12-16T00:00:00\u002b00:00","datePublished":"2024-12-16T00:00:00\u002b00:00","dateModified":"2024-12-16T00:00:00\u002b00:00","keywords":["Computer Vision","Image Generation","🏢 ByteDance Research"],"mainEntityOfPage":"true","wordCount":"4953"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.12095/cover_hu7496459625794239137.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.12095/>Causal Diffusion Transformers for Generative Modeling</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Causal Diffusion Transformers for Generative Modeling</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-16T00:00:00+00:00>16 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>4953 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">24 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.12095/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.12095/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🤗 Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/image-generation/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Image Generation
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-bytedance-research/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🏢 ByteDance Research</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#causal-ar-diffusion>Causal AR Diffusion</a></li><li><a href=#dual-factorization>Dual Factorization</a></li><li><a href=#task-difficulty>Task Difficulty</a></li><li><a href=#multimodal-fusion>Multimodal Fusion</a></li><li><a href=#zero-shot-editing>Zero-Shot Editing</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#causal-ar-diffusion>Causal AR Diffusion</a></li><li><a href=#dual-factorization>Dual Factorization</a></li><li><a href=#task-difficulty>Task Difficulty</a></li><li><a href=#multimodal-fusion>Multimodal Fusion</a></li><li><a href=#zero-shot-editing>Zero-Shot Editing</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.12095</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Chaorui Deng et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>🤗 2024-12-17</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.12095 target=_self role=button>↗ arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.12095 target=_self role=button>↗ Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/causal-diffusion-transformers-for-generative target=_self role=button>↗ Papers with Code</a></p><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p><strong>생성 모델</strong>은 이미지 생성에서 상당한 발전을 이루었지만 자기 회귀(AR)와 확산이라는 두 가지 주요 패러다임은 <strong>서로 다른 데이터 분해 접근 방식</strong>을 사용합니다. AR 모델은 <strong>순차적 토큰 예측</strong>에 탁월하지만 확산 모델은 고품질 이미지 합성에 널리 사용됩니다. 그러나 이러한 모델을 단일 프레임워크에 효과적으로 통합하는 데 어려움이 있으며, 두 가지 접근 방식의 고유한 장점을 모두 활용할 수 있는 잠재력이 제한됩니다.</p><p>이 논문에서는 순차적 토큰 및 노이즈 레벨 데이터 분해를 모두 통합하는 새로운 생성 모델링 프레임워크인 <strong>CausalFusion</strong>을 제시합니다. 이 <strong>이중 분해</strong> 접근 방식을 통해 AR 및 확산 생성 모드 간의 <strong>원활한 전환</strong>이 가능하며 두 가지 패러다임의 장점을 활용할 수 있습니다. CausalFusion은 <strong>디코더 전용 트랜스포머</strong>로 구현되어 <strong>이미지 생성 벤치마크에서 최첨단 결과</strong>를 달성하고 <strong>맥락 내 추론을 위한 무제한 토큰 생성</strong>을 가능하게 합니다. 또한 이 모델은 <strong>이미지 생성 및 캡션 작업을 공동으로 수행</strong>하는 멀티모달 기능을 보여줍니다. 즉, 단일 모델에서 <strong>텍스트 기반 이미지 편집</strong>이 가능합니다.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-3c316fee3666a99f0b0dc60944adb348></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-3c316fee3666a99f0b0dc60944adb348",{strings:[" CausalFusion은 확산과 자기 회귀 모델의 장점을 결합하여 최첨단 이미지 생성 성능을 달성합니다. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-23826b06df381e7712cf00b51f45fd91></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-23826b06df381e7712cf00b51f45fd91",{strings:[" CausalFusion은 멀티모달 기능을 보여 주며 텍스트 기반 zero-shot 이미지 편집과 같은 새로운 기능을 가능하게 합니다. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-545eff57eedde11a2aeaf62cfc2dd285></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-545eff57eedde11a2aeaf62cfc2dd285",{strings:[" CausalFusion은 향상된 표현 학습 능력을 보여줍니다. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p><strong>CausalFusion</strong>은 생성 모델링 분야, 특히 <strong>확산 및 자기 회귀 모델의 통합</strong>에 중요한 의미를 지닙니다. 이 연구는 두 패러다임의 장점을 결합한 새로운 프레임워크를 제시하고, <strong>이미지 생성, 멀티모달 생성 및 표현 학습</strong>에서 최첨단 결과를 달성했습니다. CausalFusion은 <strong>두 가지 방법의 장점을 모두 활용</strong>하여 단일 이미지 생성 및 텍스트 기반 이미지 편집과 같은 새로운 기능을 가능하게 하며, <strong>멀티모달 모델링 및 zero-shot 이미지 조작</strong>에 대한 새로운 길을 열어줍니다.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/x1.png alt></figure></p><blockquote><p>🔼 이 그림은 CausalFusion이라는 새로운 이미지 생성 모델의 작동 방식을 보여줍니다. CausalFusion은 &lsquo;이중 인수분해(Dual-Factorization)&lsquo;라는 개념을 사용하는데, 이는 이미지 생성 과정을 순차적 토큰 생성(AR)과 노이즈 레벨 감소(Diffusion)라는 두 가지 축으로 나누어 진행하는 것을 의미합니다. 기존 DiT 모델과 비교하여, In-context DiT는 더 적은 파라미터로 성능을 크게 향상시켰습니다. CausalFusion은 이 아키텍처를 변경하지 않고도 성능을 더욱 향상시킵니다. CausalFusion은 각 단계에서 이미지의 일부 토큰만 확산시켜 계산 복잡도를 낮으면서도 자유로운 AR 단계를 사용할 수 있습니다. 그림에서 화살표는 생성 경로를 나타내며, 각 단계에서 순차적 토큰과 노이즈 레벨 차원을 따라 생성이 어떻게 진행되는지 보여줍니다. IN1K 데이터셋에서 240 epoch 동안 훈련된 결과를 사용했습니다.</p><details><summary>read the caption</summary>Figure 1: Illustration of Dual-Factorization. The arrow line indicates CausalFusion’s generation path, moving from one state to the next by jointly generating along the sequential and noise-level dimension at each step. Compared to DiT, our In-context DiT substantially improves results with fewer parameters. CausalFusion further enhances performance without changing the architecture or parameter count. Results were trained on IN1K for 240 epochs. CausalFusion adopts arbitrary AR steps for image generation, but each step only diffuses partial tokens, resulting in similar (or slightly lower) computational complexity.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Params (M)</th><th>FID10k↓</th></tr></thead><tbody><tr><td>DiT [44]</td><td>458</td><td>18.24</td></tr><tr><td>- AdaLN-zero [44]</td><td>305</td><td>26.71</td></tr><tr><td>+ new recipe</td><td>305</td><td>21.94</td></tr><tr><td>    + T embedding</td><td>308</td><td>20.68</td></tr><tr><td>      + QK-norm</td><td>308</td><td>18.66</td></tr><tr><td>      + lr warmup</td><td>308</td><td>17.11</td></tr><tr><td>+ All (In-context DiT)</td><td>308</td><td><strong>13.78</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 In-context DiT(Diffusion Transformer) 모델의 성능을 ImageNet 256x256 데이터셋에서 240 epoch 훈련 후 FID 점수로 비교합니다. DiT 모델은 기존의 AdaLN-zero 구성 요소 대신, 클래스 및 타임스텝 조건을 토큰으로 취급하여 입력 시퀀스에 직접 추가하는 in-context 디자인을 적용했습니다. 표에서 밑줄은 기본 설정을, 회색으로 강조 표시된 부분은 선택된 설정을 나타냅니다. 본 표는 QK 정규화, 학습률 워밍업 등 다양한 설정을 통해 In-context DiT 모델의 성능 향상을 보여줍니다. 특히, 제안된 In-context DiT-L/2 모델은 단순하지만 DiT-XL/2 모델에 필적하는 FID-10k 점수인 13.78을 달성했습니다.</p><details><summary>read the caption</summary>Table 1: In-context DiT baseline. ImageNet 256×\times×256, 240 epoch. Baseline settings are marked by underlines and selected settings highlighted in gray.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Causal AR Diffusion<div id=causal-ar-diffusion class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#causal-ar-diffusion aria-label=Anchor>#</a></span></h4><p>**인과적 AR 확산(Causal AR Diffusion)**은 자기 회귀적 토큰 예측과 확산 모델의 노이즈 레벨 기반 정규화를 결합한 생성 모델링 접근 방식입니다. 이는 순차적 토큰 생성과 노이즈 레벨에서의 점진적 개선을 동시에 활용합니다. AR 모델은 <strong>장문 추론과 문맥 내 생성</strong>에 탁월하지만, 확산 모델은 <strong>확장 가능한 추론 연산</strong>과 반복적인 품질 향상에 뛰어납니다. 인과적 AR 확산은 이러한 장점을 결합하여 <strong>다양한 생성 작업에 대한 유연하고 강력한 프레임워크</strong>를 제공합니다. <strong>이중 인수분해</strong>는 AR 및 확산 생성 모드 간의 원활한 전환을 가능하게 하며, 순차적 토큰과 노이즈 레벨 모두에서 데이터를 처리합니다. <strong>인과적 AR 확산은 이산 및 연속 데이터 모두에서 멀티모달 모델을 훈련</strong>시키는 새로운 관점을 제공하며, 이미지 생성, 텍스트 생성, 이미지 편집 및 비전-언어 공동 모델링과 같은 작업에서 유망한 결과를 보여줍니다.</p><h4 class="relative group">Dual Factorization<div id=dual-factorization class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#dual-factorization aria-label=Anchor>#</a></span></h4><p><strong>이중 인수분해</strong>는 순차적 토큰과 확산 노이즈 레벨에서 데이터를 이중으로 인수분해하는 <strong>CausalFusion</strong>이라는 디코더 전용 트랜스포머를 제안합니다. 이 접근 방식은 순차적 토큰과 노이즈 레벨 축을 따라 데이터 분포를 인수분해하여 AR 및 확산 생성 모드 간의 원활한 전환을 가능하게 합니다. AR 모델은 순차적 축을 따라 데이터를 인수분해하여 각 토큰의 확률이 이전 토큰에 따라 달라지도록 합니다. 확산 모델은 노이즈 레벨 축을 따라 데이터를 인수분해하여 각 단계의 토큰이 이전 단계에서 자체적으로 정제된 버전이 되도록 합니다. CausalFusion은 이러한 두 가지 패러다임의 장점을 결합하여 <strong>이미지 생성</strong> 및 <strong>멀티모달 생성</strong> 시나리오에서 최첨단 결과를 달성합니다. 또한 <strong>제로샷 이미지 조작</strong> 및 <strong>맥락 내 추론</strong>과 같은 AR의 이점도 누릴 수 있습니다. CausalFusion은 이중 인수분해 평면 내에서 AR 및 확산 패러다임을 공동으로 탐색하는 유연한 프레임워크를 제공하여 생성 모델링 작업의 어려움을 해결하고 훈련 신호 영향을 조정합니다.</p><h4 class="relative group">Task Difficulty<div id=task-difficulty class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#task-difficulty aria-label=Anchor>#</a></span></h4><p><strong>CausalFusion은 AR 및 확산 모델의 이중 요소화로 인해 작업 난이도가 다양해지는 문제에 직면했습니다.</strong> 확산 모델에서는 노이즈 레벨이 높을수록 학습이 어렵고, AR 모델에서는 초기 단계 예측의 가시 컨텍스트가 제한되어 오류가 누적될 수 있습니다. 또한 AR 단계 수에 따라 AR과 확산 사이의 보간이 제어되므로 훈련 난이도에 영향을 미칩니다. AR 단계가 많을수록 훈련 작업이 단순해지고, AR 단계 샘플링이 균일하면 훈련 신호가 토큰 수가 적은 AR 단계에 편향되어 모델이 가시 컨텍스트에 과도하게 의존하게 됩니다. 이러한 문제를 해결하기 위해 CausalFusion은 AR 단계 샘플링에 지수적 감쇠를 사용하고 AR 축을 따라 손실 가중치를 적용하여 작업 난이도를 조정합니다. 이를 통해 훈련 신호의 영향을 균형 있게 조정하고 요소화 공간을 철저하게 탐색하여 성능을 향상시킵니다.</p><h4 class="relative group">Multimodal Fusion<div id=multimodal-fusion class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multimodal-fusion aria-label=Anchor>#</a></span></h4><p><strong>CausalFusion은 이미지 생성과 캡셔닝을 결합한 멀티모달 모델 학습에 효과적</strong>입니다. 텍스트와 이미지를 순차적으로 입력받아 <strong>두 모달리티 간의 관계를 학습</strong>하며, 단일 모델로 텍스트-이미지 생성과 이미지 캡셔닝 모두 수행 가능합니다. 기존 멀티모달 생성 모델(TransFusion)보다 <strong>뛰어난 성능</strong>을 보이며, <strong>제로샷 이미지 편집과 같은 다양한 작업</strong>에도 적용 가능합니다. <strong>CausalFusion의 듀얼 팩토라이제이션 디자인</strong>은 텍스트와 이미지의 의미적 연결을 효과적으로 포착하여 <strong>멀티모달 추론 능력 향상</strong>에 기여합니다. 하지만, <strong>학습 과정에서 텍스트와 이미지 손실 가중치 균형 조정</strong> 등 추가 연구가 필요합니다. <strong>향후, 멀티모달 생성 모델의 새로운 가능성</strong>을 제시하는 CausalFusion의 발전이 기대됩니다.</p><h4 class="relative group">Zero-Shot Editing<div id=zero-shot-editing class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#zero-shot-editing aria-label=Anchor>#</a></span></h4><p><strong>CausalFusion은 제로샷 이미지 편집 기능을 자연스럽게 지원합니다.</strong> 토큰의 임의 하위 집합을 예측하도록 훈련되었기 때문에 작업별 미세 조정 없이 국소 편집을 수행할 수 있습니다. 그림 2(b)에서 볼 수 있듯이, ImageNet 클래스 조건부 생성 작업에서만 사전 훈련된 모델도 고품질 편집 결과를 생성할 수 있으며, 편집 작업에 대한 견고성과 적응성을 보여줍니다. 또한 CausalFusion의 이중 인수분해 설계를 통해 맥락 일관성과 고충실도 업데이트 간의 균형을 유지하여 편집된 영역이 주변 콘텐츠와 매끄럽게 조화를 이루도록 합니다. 모델이 다양한 편집 시나리오를 처리하는 기능을 보여주는 추가 시각화는 부록 D를 참조하십시오.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/x2.png alt></figure></p><blockquote><p>🔼 (a) CausalFusion-XL/2 모델로 생성된 샘플 이미지들입니다. ImageNet 512x512 해상도 데이터셋으로 800 에포크 동안 학습되었고, DDPM 250 스텝, CFG=4.0 설정으로 생성되었습니다. 이 그림은 CausalFusion 모델의 이미지 생성 능력을 보여주는 다양한 샘플들을 제시합니다. 고품질의 다양한 이미지들이 생성되었음을 확인할 수 있으며, 이는 CausalFusion이 복잡한 이미지 분포를 학습했음을 시사합니다. 샘플들은 사실적인 동물, 사물, 풍경 등 다양한 범주를 포괄하고 있습니다. 이러한 결과는 CausalFusion이 ImageNet 데이터셋의 다양한 클래스에 걸쳐 효과적으로 일반화될 수 있음을 보여줍니다.</p><details><summary>read the caption</summary>(a) Samples generated by CausalFusion-XL/2, ImageNet 512×\times×512, 800 epoch, DDPM 250 steps, CFG=4.0</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/x3.png alt></figure></p><blockquote><p>🔼 CausalFusion-XL/2 모델을 사용하여 ImageNet 512x512 해상도 이미지에서 800 epoch 동안 훈련한 제로샷 이미지 편집 결과입니다. 왼쪽에 있는 원본 이미지를 생성한 후, 이미지의 중앙, 상반부 또는 하반부를 마스킹하고 새로운 클래스 조건을 사용하여 이미지를 재생성합니다. 6장에서 자세한 내용을 확인할 수 있습니다. 이 그림은 중앙, 상반부, 하반부 마스킹과 같은 다양한 마스킹 기법과 새로운 클래스 조건을 적용하여 이미지가 어떻게 편집되는지를 보여줍니다. 제로샷 이미지 편집은 모델이 특정 편집 작업에 대해 명시적으로 학습되지 않았음에도 불구하고 편집을 수행할 수 있음을 의미합니다.</p><details><summary>read the caption</summary>(b) Zero-shot image editing results generated by CausalFusion-XL/2, ImageNet 512×\times×512, 800 epoch. We first generate the original image (those on the left), then mask out its centre region, top-half, or bottom-half, and regenerate the image with new class conditions. Details are discussed in Sec 6.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/x4.png alt></figure></p><blockquote><p>🔼 이 그림은 CausalFusion 모델의 시각화 결과를 보여줍니다. 모든 샘플은 ImageNet-1K 클래스 조건부 생성 작업으로만 훈련된 모델에 의해 생성되었으며, CausalFusion의 제로샷 이미지 조작 능력을 보여줍니다. (a)는 CausalFusion-XL/2 모델을 사용하여 ImageNet 512x512 해상도에서 800 epoch, DDPM 250 step, CFG=4.0으로 생성한 샘플들을 보여줍니다. (b)는 CausalFusion-XL/2 모델을 사용하여 ImageNet 512x512 해상도에서 800 epoch으로 생성한 제로샷 이미지 편집 결과를 보여줍니다. 먼저 원본 이미지(왼쪽)를 생성한 다음 중앙 영역, 상반부 또는 하반부를 마스크하고 새로운 클래스 조건으로 이미지를 다시 생성합니다. 자세한 내용은 섹션 6에서 설명합니다. 추가 시각화 결과는 부록 D를 참조하십시오.</p><details><summary>read the caption</summary>Figure 2: Visualization results. All samples are generated by models trained only on ImageNet-1K class-conditional generation task, demonstrating CausalFusion’s zero-shot image manipulation ability. See more visualization results in Appendix D.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/x5.png alt></figure></p><blockquote><p>🔼 이 그림은 DiT와 CausalFusion 아키텍처의 차이점을 보여줍니다. (a) DiT는 전체 이미지 토큰을 입력으로 받아 어댑티브 레이어 정규화를 통해 조건화를 통합합니다. 모든 노이즈 토큰 xt는 전체 어텐션 관찰과 함께 DiT에 입력되어 처리 중 입력에 대한 포괄적인 모델링이 가능합니다. (b) CausalFusion은 모든 입력 양식을 동일하게 처리하고, 이전에 디노이징된 토큰과 기타 문맥 입력을 조건으로 사용하면서 각 단계에서 이미지 토큰의 무작위 하위 집합 xt,κs를 디노이징합니다. 이 방법은 마스크된 특징 예측 모델의 정신을 구현하여 부분 관찰을 통해 이미지를 재구성하도록 모델을 강제합니다.</p><details><summary>read the caption</summary>Figure 3: Conceptual comparison between the DiT and CausalFusion architectures. a) DiT incorporates conditioning via adaptive layer normalization, processing a fixed-size set of entire image tokens as input. All the noise tokens xtsubscript𝑥𝑡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are fed into DiT with full attention observation, enabling comprehensive modeling of the input during processing. b) CausalFusion treats all input modalities equally in an in-context manner, denoising a random subset of image tokens xt,κssubscript𝑥𝑡subscript𝜅𝑠x_{t,\kappa_{s}}italic_x start_POSTSUBSCRIPT italic_t , italic_κ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT at each step while causally conditioning on previously denoised tokens x0,1:κs−1subscript𝑥:01subscript𝜅𝑠1x_{0,1:\kappa_{s-1}}italic_x start_POSTSUBSCRIPT 0 , 1 : italic_κ start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT, and other contextual inputs. This approach enforces the model to reconstruct the image with partial observation, embodying the spirit of masked feature prediction models [24, 67, 35].</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/x6.png alt></figure></p><blockquote><p>🔼 (a) CausalFusion에서 생성된 샘플들. ImageNet 512x512, 800 에포크, DDPM 250 스텝, CFG=4.0. 모든 샘플은 ImageNet-1K 클래스 조건부 생성 작업으로만 훈련된 모델에 의해 생성되었으며, CausalFusion의 제로샷 이미지 조작 기능을 보여줍니다. 부록 D에서 더 많은 시각화 결과를 참조하세요.</p><details><summary>read the caption</summary>(a)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/x7.png alt></figure></p><blockquote><p>🔼 (b) CausalFusion-XL/2를 사용하여 생성된 zero-shot 이미지 편집 결과. ImageNet 512x512, 800 epoch. 먼저 원본 이미지(왼쪽)를 생성한 다음, 중앙 영역, 상반부 또는 하반부를 마스킹하고 새로운 클래스 조건으로 이미지를 다시 생성합니다. 자세한 내용은 섹션 6에서 설명합니다. 이 예시들은 CausalFusion이 이미지의 특정 부분을 마스킹하고 새로운 클래스 조건을 제공함으로써 이미지를 수정할 수 있음을 보여줍니다. 예를 들어, 꽃 이미지의 중앙을 마스킹하고 &lsquo;곰인형&rsquo; 조건을 제공하면, 마스킹된 영역이 곰인형으로 채워진 이미지가 생성됩니다.</p><details><summary>read the caption</summary>(b)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/x8.png alt></figure></p><blockquote><p>🔼 (c) AR 손실 가중치는 어려운 샘플에서 더 나은 학습을 용이하게 함으로써 성능을 향상시킵니다. 표에서 밑줄 친 항목은 기준 설정이고 회색 블록으로 강조 표시된 설정은 선택된 설정입니다. AR 손실 가중치 λ가 증가하면 Seval=1과 Seval=2에서 성능이 향상됩니다.</p><details><summary>read the caption</summary>(c)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/x9.png alt></figure></p><blockquote><p>🔼 이 그림은 CausalFusion 모델 학습 과정에서 AR 단계 수, 토큰 분포, 검증 손실 간의 관계를 보여줍니다. (a)는 AR 단계 수에 따른 학습 손실을 나타냅니다. AR 단계 수가 증가할수록 학습 손실이 감소하는 경향을 보입니다. (b)는 각 AR 단계에서 예측되는 토큰 수의 분포를 보여줍니다. 균일 샘플링을 사용하는 경우, 적은 수의 토큰이 예측되는 AR 단계가 지배적으로 나타납니다. (c)는 AR 단계 수에 따른 검증 손실을 나타냅니다. 학습 손실과 마찬가지로, 후반 AR 단계의 검증 손실이 초반 AR 단계보다 낮습니다. 이는 AR 단계가 진행될수록 학습 난이도가 낮아짐을 시사하며, AR 단계 수, 토큰 분포, 학습 및 검증 손실을 조정하여 CausalFusion 모델의 성능을 향상시킬 수 있음을 보여줍니다.</p><details><summary>read the caption</summary>Figure 4: (a) Training loss using different number of AR steps. (b) Distribution of |κs|subscript𝜅𝑠|\kappa_{s}|| italic_κ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT |. (c) Validation loss at difference AR steps.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/x10.png alt></figure></p><blockquote><p>🔼 (a) CausalFusion으로 생성된 샘플들입니다. ImageNet 512x512, 800 epoch, DDPM 250 steps, CFG=4.0로 훈련되었습니다. CausalFusion은 ImageNet-1K 클래스 조건부 생성 작업에서만 훈련되었음에도 불구하고 제로샷 이미지 조작 능력을 보여줍니다. 부록 D에서 더 많은 시각화 결과를 확인할 수 있습니다.</p><details><summary>read the caption</summary>(a)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/x11.png alt></figure></p><blockquote><p>🔼 이 이미지는 CausalFusion-XL/2 모델을 사용하여 ImageNet 512x512 해상도에서 생성된 제로샷 이미지 편집 결과를 보여줍니다. 모델은 800 epoch 동안 학습되었으며, 왼쪽의 이미지들은 원본 이미지입니다. 편집은 이미지의 중앙, 상반부 또는 하반부를 마스킹하고 새로운 클래스 조건으로 이미지를 다시 생성하여 수행되었습니다. 이 그림은 CausalFusion 모델이 사전 학습만으로도 다양한 제로샷 이미지 편집 작업을 수행할 수 있음을 보여주는 예시입니다. 6장에서 자세한 내용을 확인할 수 있습니다.</p><details><summary>read the caption</summary>(b)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/x12.png alt></figure></p><blockquote><p>🔼 (c) AR 손실 가중치는 어려운 샘플에서 더 나은 학습을 용이하게 함으로써 성능을 향상시킵니다. 표 3(c)는 AR 손실 가중치를 다르게 설정하여 실험한 결과를 보여줍니다. AR 단계가 진행될수록 가시 컨텍스트의 지역성이 높아지기 때문에 생성 작업이 더 쉬워집니다. 반대로, 초기 AR 단계에서의 예측은 시각적 컨텍스트 내에서 비-지역적 종속성 학습을 용이하게 하여 생성 모델링에 도움이 됩니다. 따라서 초기 AR 단계의 손실 가중치를 높게 설정하는 것이 생성 모델링 성능 향상에 도움이 될 수 있습니다.</p><details><summary>read the caption</summary>(c)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl512_360_cfg4.0.jpg alt></figure></p><blockquote><p>🔼 (a) 텍스트-이미지 생성 샘플입니다. 주어진 텍스트 프롬프트에 따라 CausalFusion-XL 모델이 생성한 이미지 샘플들을 보여줍니다. 각 샘플 아래에는 해당 이미지 생성에 사용된 텍스트 프롬프트가 적혀 있습니다. 이 그림은 CausalFusion 모델이 텍스트 프롬프트를 얼마나 잘 이해하고 그에 맞는 이미지를 생성하는지 보여주는 예시입니다.</p><details><summary>read the caption</summary>(a) Samples on Text-to-Image generation.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl512_387_cfg4.0.jpg alt></figure></p><blockquote><p>🔼 이 그림들은 CausalFusion XL 모델이 이미지 캡셔닝 작업을 수행한 결과를 보여줍니다. 모델은 ImageNet 캡션 데이터로 훈련되었으며, 주어진 이미지에 대해 묘사적인 캡션을 생성합니다. 예시로, &lsquo;빨간색과 흰색의 스테인리스 스틸 식기세척기와 나무 캐비닛이 있는 주방&rsquo;, &lsquo;의자에 앉아 있는 고양이&rsquo; 와 같은 캡션이 생성되었습니다.</p><details><summary>read the caption</summary>(b) Samples on Image Caption generation.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl512_817_cfg4.0.jpg alt></figure></p><blockquote><p>🔼 이 그림은 CausalFusion XL 모델의 multimodal 생성 능력을 보여줍니다. 모델은 ImageNet 재캡션 데이터로 학습되었습니다. (a)는 Text-to-Image 생성 결과로, 텍스트 프롬프트를 기반으로 이미지를 생성한 것을 보여줍니다. 각 샘플 아래에는 해당 텍스트 프롬프트가 표시됩니다. (b)는 Image Captioning 결과를 보여주며, 주어진 이미지에 대해 모델이 생성한 캡션이 이미지 아래에 표시됩니다. 단일 CausalFusion XL 모델이 이미지 생성과 캡션 생성 모두에서 우수한 성능을 보이는 것을 확인할 수 있습니다.</p><details><summary>read the caption</summary>Figure 6: Multimodal generation. Results are generated by a single CausalFusion XL model trained on ImageNet recaption data.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl512_972_cfg4.0.jpg alt></figure></p><blockquote><p>🔼 (a) CausalFusion에 의해 생성된 샘플. ImageNet 512x512, 800 epoch, DDPM 250 steps, CFG=4.0. CausalFusion-XL/2 모델로 생성된 이미지들입니다. 다양한 종류의 고품질 이미지 샘플들을 보여주고 있습니다.</p><details><summary>read the caption</summary>(a)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl512_279_cfg2.0.jpg alt></figure></p><blockquote><p>🔼 이 그림은 CausalFusion-XL/2 모델을 사용하여 ImageNet 512x512 해상도와 800 에포크로 학습하여 생성한 제로샷 이미지 편집 결과를 보여줍니다. 왼쪽에 있는 이미지는 원본 이미지이고, 가운데와 오른쪽 이미지는 원본 이미지의 중앙, 상단 절반 또는 하단 절반을 마스킹하고 새로운 클래스 조건으로 이미지를 재생성한 결과입니다. 이는 CausalFusion 모델이 작업별 미세 조정 없이도 국소 편집을 수행할 수 있는 능력을 보여줍니다. 자세한 내용은 6장을 참조하세요.</p><details><summary>read the caption</summary>(b)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl512_975_cfg2.0.jpg alt></figure></p><blockquote><p>🔼 이 그림은 CausalFusion 모델의 일반화된 인과 어텐션 마스크를 보여줍니다. 입력 시퀀스는 κ₁, κ₂, κ₃ 세 개의 AR 단계로 구성되며, 각각 2개, 2개, 3개의 토큰을 포함합니다. x₀,κ₁과 x₀,κ₂는 처음 두 AR 단계의 클린 토큰이고, xt,κ₁, xt,κ₂, xt,κ₃는 노이즈가 추가된 토큰입니다. 흰색 블록은 마스크된 어텐션을, 회색 블록은 마스크되지 않은 어텐션을 나타냅니다. 각 xt,κs는 자기 자신과 이전 AR 단계의 클린 토큰 x₀,κ₁:s-₁에만 집중합니다.</p><details><summary>read the caption</summary>Figure 7: Generalized causal mask. In this case, the input sequence is organized to have 3 AR-steps κ1subscript𝜅1\kappa_{1}italic_κ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, κ2subscript𝜅2\kappa_{2}italic_κ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and κ3subscript𝜅3\kappa_{3}italic_κ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, containing 2, 2, and 3 tokens, respectively. 𝐱0,κ1subscript𝐱0subscript𝜅1\mathbf{x}_{0,\kappa_{1}}bold_x start_POSTSUBSCRIPT 0 , italic_κ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT and 𝐱0,κ2subscript𝐱0subscript𝜅2\mathbf{x}_{0,\kappa_{2}}bold_x start_POSTSUBSCRIPT 0 , italic_κ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT are the clean tokens at the first two AR steps, while 𝐱t,κ1subscript𝐱𝑡subscript𝜅1\mathbf{x}_{t,\kappa_{1}}bold_x start_POSTSUBSCRIPT italic_t , italic_κ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT, 𝐱t,κ2subscript𝐱𝑡subscript𝜅2\mathbf{x}_{t,\kappa_{2}}bold_x start_POSTSUBSCRIPT italic_t , italic_κ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT, and 𝐱t,κ3subscript𝐱𝑡subscript𝜅3\mathbf{x}_{t,\kappa_{3}}bold_x start_POSTSUBSCRIPT italic_t , italic_κ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUBSCRIPT are noised tokens. White and gray blocks denote the masked and unmasked attention, respectively. Note that, each 𝐱t,κssubscript𝐱𝑡subscript𝜅𝑠\mathbf{x}_{t,\kappa_{s}}bold_x start_POSTSUBSCRIPT italic_t , italic_κ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT only attends to itself and the clean tokens from previous AR steps 𝐱0,κ1:s−1subscript𝐱0subscript𝜅:1𝑠1\mathbf{x}_{0,\kappa_{1:s-1}}bold_x start_POSTSUBSCRIPT 0 , italic_κ start_POSTSUBSCRIPT 1 : italic_s - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl512_89_cfg1.5.jpg alt></figure></p><blockquote><p>🔼 이 그림은 CausalFusion-XL 모델을 사용하여 제로샷 이미지 편집을 수행한 결과를 보여줍니다. 모델은 512x512 해상도의 ImageNet 데이터셋으로 800 epoch 동안 훈련되었으며, Classifier-free guidance scale은 3.0으로 설정되었습니다. 제로샷 이미지 편집은 원본 이미지의 일부를 마스킹하고, 마스킹되지 않은 영역과 새로운 클래스 레이블을 조건으로 이미지를 재생성하는 방식으로 수행됩니다. 그림에서 볼 수 있듯이, 모델은 &lsquo;화산&rsquo; 이미지에서 &lsquo;텔레비전&rsquo;, &lsquo;미닫이문&rsquo;, &lsquo;자동차 백미러&rsquo;와 같은 다양한 레이블로 편집된 고품질 결과물을 생성합니다.</p><details><summary>read the caption</summary>Figure 8: Zero-shot editing samples. CausalFusion-XL, resolution 512×\times×512, 800 epoch, Classifier-free guidance scale = 3.0.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl512_974_cfg1.5.jpg alt></figure></p><blockquote><p>🔼 이 그림은 CausalFusion-XL 모델을 사용하여 제로샷 이미지 편집을 수행한 결과를 보여줍니다. 모델은 먼저 초기 클래스 레이블을 사용하여 원본 이미지를 생성한 다음, 이미지의 일부를 가리고, 가리지 않은 영역과 새로운 클래스 레이블을 조건으로 이미지를 다시 생성합니다. 예를 들어 첫 번째 예시에서 &lsquo;화산&rsquo; 이미지가 생성된 후 이미지의 바깥 부분이 가려지고 &lsquo;텔레비전&rsquo;, &lsquo;미닫이문&rsquo;, &lsquo;자동차 백미러&rsquo;와 같은 새로운 레이블로 이미지가 다시 생성되었습니다. 이러한 결과는 CausalFusion-XL 모델이 작업별 미세 조정 없이 국소적인 편집을 수행할 수 있음을 보여줍니다. 또한 이중 인수분해 설계를 통해 문맥적 일관성과 높은 충실도 업데이트 사이의 균형을 유지하여 편집된 영역이 주변 콘텐츠에 자연스럽게 혼합되도록 합니다. 해상도는 256x256, 학습 에포크는 800, Classifier-free guidance scale은 1.5입니다.</p><details><summary>read the caption</summary>Figure 9: Zero-shot editing samples. CausalFusion-XL, resolution 256×\times×256, 800 epoch, Classifier-free guidance scale = 1.5.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl256_88_cfg4.0.jpg alt></figure></p><blockquote><p>🔼 이 그림은 논문의 섹션 6, &lsquo;성능 비교&rsquo;에 나오는 그림 10입니다. 512x512 해상도의 CausalFusion-XL 모델에서 생성된 큐레이션되지 않은 이미지 샘플들을 보여줍니다. 이미지들은 &lsquo;수달&rsquo;(클래스 레이블 360)이라는 단일 클래스 레이블을 조건으로 하여 생성되었으며, 분류기 없는 안내 척도는 4.0으로 설정되었습니다. 이 그림은 CausalFusion-XL 모델이 다양한 포즈, 표정, 배경의 수달 이미지들을 생성할 수 있음을 보여주며, 고품질 이미지 합성 능력을 보여줍니다.</p><details><summary>read the caption</summary>Figure 10: Uncurated 512×512512512512\times 512512 × 512 CausalFusion-XL samples. Classifier-free guidance scale = 4.0 Class label = “otter” (360)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl256_980_cfg4.0.jpg alt></figure></p><blockquote><p>🔼 이 그림은 논문의 6장 &lsquo;성능 비교&rsquo; 부분에 나오는 추가 샘플 중 하나입니다. 512x512 해상도의 이미지를 생성하는 CausalFusion-XL 모델의 결과물을 보여주고 있습니다. Classifier-free guidance scale은 4.0으로 설정되었고, &lsquo;레서판다(387)&lsquo;라는 클래스 레이블을 사용하여 생성되었습니다. 그림에서 여러 레서판다 이미지들이 다양한 포즈와 구도로 생성된 것을 확인할 수 있습니다. 이는 모델이 단일 클래스 레이블을 기반으로 다양한 이미지를 생성할 수 있음을 보여줍니다.</p><details><summary>read the caption</summary>Figure 11: Uncurated 512×512512512512\times 512512 × 512 CausalFusion-XL samples. Classifier-free guidance scale = 4.0 Class label = “red panda” (387)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl256_388_cfg2.0.jpg alt></figure></p><blockquote><p>🔼 이 그림은 CausalFusion-XL 모델을 사용하여 생성된 512x512 해상도의 스포츠카 이미지 샘플들을 보여줍니다. Classifier-free guidance scale은 4.0으로 설정되었고, 이미지의 클래스 레이블은 &lsquo;스포츠카&rsquo;(817)입니다. 샘플들은 다양한 각도와 색상의 스포츠카들을 보여주며, 모델이 스포츠카의 특징을 잘 학습했음을 알 수 있습니다. 일부 샘플에서는 약간의 노이즈나 왜곡이 보일 수 있지만, 전반적으로 높은 품질의 이미지를 생성합니다.</p><details><summary>read the caption</summary>Figure 12: Uncurated 512×512512512512\times 512512 × 512 CausalFusion-XL samples. Classifier-free guidance scale = 4.0 Class label = “sports car” (817)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl256_979_cfg2.0.jpg alt></figure></p><blockquote><p>🔼 이 그림은 논문의 부록 D에 있는 그림 13으로, 512x512 해상도의 이미지를 생성하는 CausalFusion-XL 모델의 결과물들을 보여줍니다. 이 샘플들은 특정 클래스 라벨(&lsquo;cliff&rsquo;, 972)에 대해 생성되었으며, Classifier-free guidance scale은 4.0으로 설정되었습니다. 여러 개의 샘플들을 통해 모델이 다양한 절벽 이미지를 생성할 수 있음을 알 수 있습니다.</p><details><summary>read the caption</summary>Figure 13: Uncurated 512×512512512512\times 512512 × 512 CausalFusion-XL samples. Classifier-free guidance scale = 4.0 Class label = “cliff” (972)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl256_928_cfg1.5.jpg alt></figure></p><blockquote><p>🔼 이 그림은 논문의 섹션 6, &lsquo;성능 비교&rsquo;에 있는 그림 13입니다. 512x512 해상도의 북극여우 이미지 여러 개를 보여줍니다. 이 이미지들은 CausalFusion-XL 모델로 생성되었으며, classifier-free guidance scale은 4.0으로 설정되었습니다. 클래스 레이블은 &lsquo;북극여우(279)&lsquo;입니다. 이 그림들은 모델이 북극여우의 다양한 포즈와 모습을 생성할 수 있음을 보여줍니다.</p><details><summary>read the caption</summary>Figure 14: Uncurated 512×512512512512\times 512512 × 512 CausalFusion-XL samples. Classifier-free guidance scale = 4.0 Class label = “arctic fox” (279)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12095/extracted/6075434/figs/xl256_973_cfg1.5.jpg alt></figure></p><blockquote><p>🔼 512x512 해상도의 ImageNet &rsquo;lakeshore&rsquo; 클래스에 대한 CausalFusion-XL의 큐레이팅되지 않은 샘플들입니다. Classifier-free guidance scale은 4.0으로 설정되었습니다. 이 그림은 CausalFusion-XL 모델이 &rsquo;lakeshore&rsquo; 클래스의 이미지를 얼마나 잘 생성하는지 보여줍니다. 다양한 호숫가 풍경이 생성되었으며, 일부 샘플에서는 물, 나무, 산과 같은 세부 사항을 명확하게 볼 수 있습니다.</p><details><summary>read the caption</summary>Figure 15: Uncurated 512×512512512512\times 512512 × 512 CausalFusion-XL samples. Classifier-free guidance scale = 4.0 Class label = “lakeshore” (975)</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>#AR steps</th><th>$S_{\text{eval}}$ = 1</th><th>$S_{\text{eval}}$ = 2</th><th>$S_{\text{eval}}$ = 4</th><th>$S_{\text{eval}}$ = 8</th></tr></thead><tbody><tr><td>$S_{\text{train}}$ = 1</td><td><strong>13.78</strong></td><td>356.69</td><td>404.67</td><td>390.18</td></tr><tr><td>$S_{\text{train}}$ = 2</td><td>16.69</td><td><strong>14.77</strong></td><td>47.49</td><td>136.04</td></tr><tr><td>$S_{\text{train}}$ = 4</td><td>24.14</td><td>15.37</td><td><strong>18.13</strong></td><td>33.14</td></tr><tr><td>$S_{\text{train}}$ = 8</td><td>54.08</td><td>24.49</td><td>22.66</td><td><strong>20.01</strong></td></tr><tr><td>$S_{\text{train}}$ = 256</td><td>313.28</td><td>321.62</td><td>261.26</td><td>192.25</td></tr><tr><td>random</td><td>21.31</td><td>22.17</td><td>23.54</td><td>25.05</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 AR 단계 수를 고정하여 훈련 및 추론하는 CausalFusion 모델에 대한 실험 결과를 보여줍니다. S_train은 훈련 중에 사용된 고정 AR 단계 수를 나타내고, S_eval은 추론 중에 사용된 고정 AR 단계 수를 나타냅니다. 밑줄은 기준 설정을 나타내고, 회색으로 강조 표시된 것은 선택된 설정을 나타냅니다. 표에서 볼 수 있듯이, 고정된 AR 단계로 훈련된 CausalFusion은 다른 추론 설정으로 강건하게 전달될 수 없습니다. 예를 들어, 모든 모델은 추론 설정이 훈련과 일치하지 않을 때 상당히 나쁜 성능을 보입니다. 각 훈련 설정의 최상의 평가 결과를 비교하면 AR 단계 수를 늘리면 성능이 크게 저하되는 것을 알 수 있습니다. 특히, 8단계 CausalFusion은 FID 20.01을 산출하여 In-context DiT가 달성한 13.78 FID보다 명확하게 뒤떨어집니다. 그러나 그림 4(a)의 손실 곡선에서 AR 단계가 더 많은 모델이 AR 단계가 더 적은 모델보다 지속적으로 더 낮은 손실 값을 나타내는 반대 추세가 관찰됩니다. 이는 AR 단계 수가 증가함에 따라 학습 과제가 지나치게 단순화됨을 시사합니다.</p><details><summary>read the caption</summary>Table 2: Ablations on AR steps. Strainsubscript𝑆trainS_{\text{train}}italic_S start_POSTSUBSCRIPT train end_POSTSUBSCRIPT and Sevalsubscript𝑆evalS_{\text{eval}}italic_S start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT indicates the fixed AR steps used during training and inference, respectively. Baseline settings are marked by underlines and selected settings highlighted in gray.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>ratio</th><th>$S_{\text{eval}}$ = 1</th><th>$S_{\text{eval}}$ = 2</th><th>$S_{\text{eval}}$ = 4</th><th>$S_{\text{eval}}$ = 8</th></tr></thead><tbody><tr><td><strong>1.0</strong></td><td>21.31</td><td>22.17</td><td>23.54</td><td>25.05</td></tr><tr><td>0.95</td><td>14.49</td><td>17.78</td><td>19.79</td><td>23.93</td></tr><tr><td>0.9</td><td>12.89</td><td>15.57</td><td><strong>18.83</strong></td><td><strong>22.72</strong></td></tr><tr><td>0.85</td><td>12.94</td><td>15.54</td><td>19.12</td><td>23.46</td></tr><tr><td>0.8</td><td><strong>12.78</strong></td><td><strong>15.42</strong></td><td>19.38</td><td>23.78</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 AR 단계 감쇠, 순서 지정 및 AR 가중치에 대한 절제 연구를 제시합니다. 표에서 밑줄은 기준 설정을 나타내고 회색 블록은 선택한 설정을 강조 표시합니다. AR 단계 감쇠에 대한 절제 연구는 감쇠 비율이 고정된 AR 단계를 사용하는 것보다 모든 추론 설정에서 경쟁력 있는 성능이나 더 나은 성능을 제공한다는 것을 보여줍니다. 토큰 순서 지정에 대한 절제 연구는 이미지 토큰의 지역성이 훈련 난이도에 영향을 미친다는 것을 보여줍니다. AR 손실 가중치에 대한 절제 연구는 어려운 샘플에서 더 나은 학습을 용이하게 함으로써 성능을 향상시키는 것을 보여줍니다.</p><details><summary>read the caption</summary>Table 3: Ablations on AR step decay, ordering, and AR weighting. Baseline settings are marked by underlines and selected settings highlighted in gray.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Patch order</th><th>FID10k↓</th></tr></thead><tbody><tr><td>raster order</td><td>14.46</td></tr><tr><td>block-wise raster (8x8)</td><td>14.76</td></tr><tr><td>block-wise raster (4x4)</td><td>14.62</td></tr><tr><td>dilated order</td><td>15.54</td></tr><tr><td><strong>random order</strong></td><td><strong>12.89</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 ImageNet 데이터셋에서 클래스 조건부 이미지 생성에 대한 다양한 모델의 성능 비교를 보여줍니다. FID(Fréchet Inception Distance), IS(Inception Score), Pre.(Precision), Rec.(Recall)과 같은 메트릭을 사용하여 성능을 평가합니다. 256x256 및 512x512 해상도에서 CFG(Classifier-Free Guidance)를 사용했는지 여부를 명시합니다. 회색 블록으로 표시된 숫자는 추론 중에 온도 샘플링을 사용했음을 나타냅니다.</p><details><summary>read the caption</summary>Table 4: System performance comparison on ImageNet class-conditioned generation. Numbers marked with gray blocks use temperature sampling during inference.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:center>weight</th><th style=text-align:center>FID10k ↓</th><th style=text-align:center></th><th style=text-align:center></th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center>$S_{\text{eval}}$ = 1</td><td style=text-align:center>$S_{\text{eval}}$ = 2</td><td style=text-align:center>$S_{\text{eval}}$ = 4</td></tr><tr><td style=text-align:center><strong>1 → 1</strong></td><td style=text-align:center>12.89</td><td style=text-align:center>15.57</td><td style=text-align:center>18.83</td></tr><tr><td style=text-align:center>1.5 → 1</td><td style=text-align:center>12.61</td><td style=text-align:center>15.49</td><td style=text-align:center>18.32</td></tr><tr><td style=text-align:center><strong>2 → 1</strong></td><td style=text-align:center><strong>12.13</strong></td><td style=text-align:center><strong>15.15</strong></td><td style=text-align:center>18.09</td></tr><tr><td style=text-align:center>2.5 → 1</td><td style=text-align:center>12.32</td><td style=text-align:center>15.22</td><td style=text-align:center>17.99</td></tr><tr><td style=text-align:center>3 → 1</td><td style=text-align:center>12.50</td><td style=text-align:center>15.28</td><td style=text-align:center><strong>17.92</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 ImageNet 256x256 이미지 생성 벤치마크에서 CausalFusion 및 다른 최첨단 모델의 성능을 비교한 것입니다. FID(Fréchet Inception Distance), IS(Inception Score), FID30k, CIDEr 등 다양한 지표를 사용하여 모델을 평가합니다. 또한 토큰화 방법, 매개변수 수, 훈련 에포크 수, 샘플링 전략 및 샘플링 트릭과 같은 세부 정보도 제공합니다.</p><details><summary>read the caption</summary>Table 5: System performance comparison on 256×\times×256 ImageNet generation, compared with previously reported large models.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>Params</th><th>256×256, w/o CFG</th><th>256×256, w/ CFG</th><th>512×512, w/ CFG</th><th></th></tr></thead><tbody><tr><td></td><td></td><td>FID↓</td><td>IS↑</td><td>Pre.↑</td><td>Rec.↑</td></tr><tr><td>GIVT [63]</td><td>304M</td><td>5.67</td><td>-</td><td>0.75</td><td>0.59</td></tr><tr><td>MAR-B [34]</td><td>208M</td><td>3.48</td><td>192.4</td><td>0.78</td><td>0.58</td></tr><tr><td>LDM-4 [50]</td><td>400M</td><td>10.56</td><td>103.5</td><td>0.71</td><td>0.62</td></tr><tr><td>CausalFusion-L</td><td>368M</td><td><strong>5.12</strong></td><td><strong>166.1</strong></td><td><strong>0.73</strong></td><td><strong>0.66</strong></td></tr><tr><td>MAR-L [34]</td><td>479M</td><td>2.6</td><td>221.4</td><td>0.79</td><td>0.60</td></tr><tr><td>ADM [13]</td><td>554M</td><td>10.94</td><td>-</td><td>0.69</td><td>0.63</td></tr><tr><td>DiT-XL [44]</td><td>675M</td><td>9.62</td><td>121.5</td><td>0.67</td><td>0.67</td></tr><tr><td>SiT-XL [42]</td><td>675M</td><td>8.3</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ViT-XL [22]</td><td>451M</td><td>8.10</td><td>-</td><td>-</td><td>-</td></tr><tr><td>U-ViT-H/2 [1]</td><td>501M</td><td>6.58</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MaskDiT [73]</td><td>675M</td><td>5.69</td><td>178.0</td><td>0.74</td><td>0.60</td></tr><tr><td>RDM [59]</td><td>553M</td><td>5.27</td><td>153.4</td><td>0.75</td><td>0.62</td></tr><tr><td>CausalFusion-XL</td><td>676M</td><td><strong>3.61</strong></td><td><strong>180.9</strong></td><td><strong>0.75</strong></td><td><strong>0.66</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 CausalFusion 모델과 다른 최신 multimodal 모델인 TransFusion [75] 및 DiT [44]와의 성능 비교를 보여줍니다. (a) 부분은 TransFusion과의 비교로, 인식 및 생성 벤치마크에서의 성능을 보여주고, 두 모델 모두 동일한 사전 훈련 데이터를 사용하여 동일한 설정으로 훈련되었습니다. (b) 부분은 DiT와의 비교로, 인식 및 생성 벤치마크에서의 성능을 나타냅니다. 여기서 ††로 표시된 모델은 [34]에서 제안된 VAE를 사용하여 잡음이 아닌 잠재 변수를 예측하는 손실 함수로 훈련되었습니다.</p><details><summary>read the caption</summary>Table 6: (a) Comparison with Transfusion [75] on perception and generation benchmarks. All models are trained under the same settings using the same pretraining data. (b) Comparison with DiT [44] on perception and generation benchmarks. The model marked with ††\dagger† is trained with a VAE from [34], using a loss function to predict latent variables rather than noise.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:left>Type</th><th style=text-align:left>Tokenizer</th><th style=text-align:left>Params</th><th style=text-align:left>Training Epoch</th><th style=text-align:left>Sampler (Steps)</th><th style=text-align:left>Sampling tricks</th><th style=text-align:left>FID↓</th></tr></thead><tbody><tr><td style=text-align:left>Open-MAGVIT2-L [41]</td><td style=text-align:left>AR</td><td style=text-align:left>MAGVIT2</td><td style=text-align:left>800M</td><td style=text-align:left>300</td><td style=text-align:left>AR(256)</td><td style=text-align:left>N/A</td><td style=text-align:left>2.51</td></tr><tr><td style=text-align:left>Open-MAGVIT2-XL [41]</td><td style=text-align:left>AR</td><td style=text-align:left>MAGVIT2</td><td style=text-align:left>1.5B</td><td style=text-align:left>300</td><td style=text-align:left>AR(256)</td><td style=text-align:left>N/A</td><td style=text-align:left>2.33</td></tr><tr><td style=text-align:left>LlamaGen-3B [56]</td><td style=text-align:left>AR</td><td style=text-align:left>custom</td><td style=text-align:left>3.1B</td><td style=text-align:left>-</td><td style=text-align:left>AR(256)</td><td style=text-align:left>N/A</td><td style=text-align:left>2.18</td></tr><tr><td style=text-align:left>VAR-d24 [60]</td><td style=text-align:left>VAR</td><td style=text-align:left>custom</td><td style=text-align:left>1B</td><td style=text-align:left>350</td><td style=text-align:left>VAR</td><td style=text-align:left>N/A</td><td style=text-align:left>2.09</td></tr><tr><td style=text-align:left>VAR-d30 [60]</td><td style=text-align:left>VAR</td><td style=text-align:left>custom</td><td style=text-align:left>2B</td><td style=text-align:left>350</td><td style=text-align:left>VAR</td><td style=text-align:left>reject sampling</td><td style=text-align:left>1.73</td></tr><tr><td style=text-align:left>Simple-diffusion [27]</td><td style=text-align:left>Diffusion</td><td style=text-align:left>N/A</td><td style=text-align:left>2B</td><td style=text-align:left>800</td><td style=text-align:left>DDPM</td><td style=text-align:left>N/A</td><td style=text-align:left>2.44</td></tr><tr><td style=text-align:left>FiTv2-3B [66]</td><td style=text-align:left>Diffusion</td><td style=text-align:left>SD</td><td style=text-align:left>3B</td><td style=text-align:left>256</td><td style=text-align:left>DDPM(250)</td><td style=text-align:left>N/A</td><td style=text-align:left>2.15</td></tr><tr><td style=text-align:left>VDM++ [30]</td><td style=text-align:left>Diffusion</td><td style=text-align:left>N/A</td><td style=text-align:left>2B</td><td style=text-align:left>-</td><td style=text-align:left>EDM</td><td style=text-align:left>-</td><td style=text-align:left>2.12</td></tr><tr><td style=text-align:left>Large-DiT-7B [20]</td><td style=text-align:left>Diffusion</td><td style=text-align:left>SD</td><td style=text-align:left>3B</td><td style=text-align:left>435</td><td style=text-align:left>DDPM(250)</td><td style=text-align:left>N/A</td><td style=text-align:left>2.10</td></tr><tr><td style=text-align:left>Flag-DiT-3B [20]</td><td style=text-align:left>Diffusion</td><td style=text-align:left>SD</td><td style=text-align:left>3B</td><td style=text-align:left>256</td><td style=text-align:left>adaptive Dopri-5</td><td style=text-align:left>N/A</td><td style=text-align:left>1.96</td></tr><tr><td style=text-align:left>DiT-MoE-XL/2-8E2A [18]</td><td style=text-align:left>Diffusion</td><td style=text-align:left>SD</td><td style=text-align:left>16B</td><td style=text-align:left>≈1000</td><td style=text-align:left>DDPM(250)</td><td style=text-align:left>N/A</td><td style=text-align:left>1.72</td></tr><tr><td style=text-align:left>DiMR-G/2R [38]</td><td style=text-align:left>Diffusion</td><td style=text-align:left>SD</td><td style=text-align:left>1.1B</td><td style=text-align:left>800</td><td style=text-align:left>DPM-solver(250)</td><td style=text-align:left>N/A</td><td style=text-align:left>1.63</td></tr><tr><td style=text-align:left>DART-XL [21]</td><td style=text-align:left>AR+Diffusion</td><td style=text-align:left>LDM</td><td style=text-align:left>812M</td><td style=text-align:left>-</td><td style=text-align:left>AR(256)+FM(100)</td><td style=text-align:left>τ sampling</td><td style=text-align:left>3.98</td></tr><tr><td style=text-align:left>MonoFormer [72]</td><td style=text-align:left>AR+Diffusion</td><td style=text-align:left>SD</td><td style=text-align:left>1.1B</td><td style=text-align:left>-</td><td style=text-align:left>DDPM(250)</td><td style=text-align:left>N/A</td><td style=text-align:left>2.57</td></tr><tr><td style=text-align:left>BiGR-XL-d24 [23]</td><td style=text-align:left>AR+Diffusion</td><td style=text-align:left>custom</td><td style=text-align:left>799M</td><td style=text-align:left>400</td><td style=text-align:left>AR(256)+DDPM(100)</td><td style=text-align:left>τ sampling</td><td style=text-align:left>2.49</td></tr><tr><td style=text-align:left>BiGR-XXL-d32 [23]</td><td style=text-align:left>AR+Diffusion</td><td style=text-align:left>custom</td><td style=text-align:left>1.5B</td><td style=text-align:left>400</td><td style=text-align:left>AR(256)+DDPM(100)</td><td style=text-align:left>τ sampling</td><td style=text-align:left>2.36</td></tr><tr><td style=text-align:left>MAR-H [34]</td><td style=text-align:left>AR+Diffusion</td><td style=text-align:left>custom</td><td style=text-align:left>943M</td><td style=text-align:left>800</td><td style=text-align:left>AR(256)+DDPM(100)</td><td style=text-align:left>τ sampling</td><td style=text-align:left>1.55</td></tr><tr><td style=text-align:left>CausalFusion-H</td><td style=text-align:left>Diffusion</td><td style=text-align:left>custom</td><td style=text-align:left>1B</td><td style=text-align:left>800</td><td style=text-align:left>DDPM(250)</td><td style=text-align:left>N/A</td><td style=text-align:left>1.64</td></tr><tr><td style=text-align:left>CausalFusion-H</td><td style=text-align:left>Diffusion</td><td style=text-align:left>custom</td><td style=text-align:left>1B</td><td style=text-align:left>800</td><td style=text-align:left>DDPM(250)</td><td style=text-align:left>CFG interval</td><td style=text-align:left>1.57</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 CausalFusion 모델 학습 시 확산 시간 단계 샘플링 전략을 비교하여 성능에 미치는 영향을 분석한 결과를 보여줍니다. 각 AR 단계에 대해 동일한 확산 시간 단계를 사용하는 기본 설정과 다른 AR 단계에 대해 서로 다른 시간 단계를 사용하는 설정, 그리고 각 AR 단계에 대해 여러 개의 시간 단계를 샘플링하는 설정을 비교합니다. 결과적으로, 어떤 샘플링 전략을 사용하더라도 성능 차이가 크지 않다는 것을 알 수 있습니다. 즉, CausalFusion 모델은 확산 시간 단계 샘플링 전략에 robust 하다는 것을 시사합니다.</p><details><summary>read the caption</summary>Table 7: Diffusion time steps sampling strategy does not affect the performance. The default setting is underlined.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:center>Source</th><th style=text-align:center>Size</th><th style=text-align:center>FID30k↓</th><th style=text-align:center>CIDEr↑</th></tr></thead><tbody><tr><td style=text-align:left>Transfusion-L [75]</td><td style=text-align:center>IN1KCap</td><td style=text-align:center>1M</td><td style=text-align:center>8.1</td><td style=text-align:center>34.5</td></tr><tr><td style=text-align:left>CausalFusion-L</td><td style=text-align:center>IN1KCap</td><td style=text-align:center>1M</td><td style=text-align:center><strong>7.1</strong></td><td style=text-align:center><strong>47.9</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 클래스 조건 토큰의 수가 CausalFusion 모델의 성능과 매개변수 수에 미치는 영향을 보여줍니다. 클래스 조건 토큰은 모델에 클래스 정보를 제공하는 데 사용됩니다. 표에서 볼 수 있듯이 클래스 토큰 수를 늘리면 성능이 약간 향상되지만 매개변수 수도 증가합니다. 이는 클래스 조건화에 할당된 계산량이 클래스 조건화에 사용되는 매개변수 수보다 더 중요함을 시사합니다.</p><details><summary>read the caption</summary>Table 8: #Class tokens offers a trade-off between performance and number of parameters. The default setting is underlined.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>Params</th><th>Data</th><th>Size</th><th>FID10k↓</th><th>Acc↑</th><th>CIDEr↑</th></tr></thead><tbody><tr><td>DiT [44]</td><td>458M</td><td>IN1K</td><td>1M</td><td>18.2</td><td>83.5</td><td>94.4</td></tr><tr><td>CausalFusion</td><td>368M</td><td>IN1K</td><td>1M</td><td>11.8</td><td>84.2</td><td>98.0</td></tr><tr><td>CausalFusion†</td><td>368M</td><td>IN1K</td><td>1M</td><td><strong>9.3</strong></td><td><strong>84.7</strong></td><td><strong>103.2</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 4장과 5장의 클래스 조건부 이미지 생성에 대한 CausalFusion 모델의 세부 설정을 제공합니다. 이미지 해상도, 은닉 차원, 헤드 수, 레이어 수, cls 토큰 수, 패치 크기, 위치 임베딩, VAE, VAE 다운샘플링, 잠재 채널, 옵티마이저, 기본 학습률, 가중치 감쇠, 옵티마이저 모멘텀, 배치 크기, 학습률 스케줄, 웜업 에포크, 학습 에포크, 증강, 확산 샘플러, 확산 단계, 평가 슈트, 평가 메트릭 등의 설정값을 보여줍니다.</p><details><summary>read the caption</summary>Table 9: Ablation study configuration.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>FID10k</th></tr></thead><tbody><tr><td>shared <em>t</em> for different AR steps</td><td>12.13</td></tr><tr><td>random <em>t</em> for different AR steps</td><td>12.27</td></tr><tr><td>4× <em>t</em> for each AR step</td><td>12.19</td></tr><tr><td>8× <em>t</em> for each AR step</td><td>12.23</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 논문 6장에 나오는 표 4와 표 5의 CausalFusion 모델에 대한 자세한 설정값들을 보여줍니다. 표에는 CausalFusion-L, XL, H 모델의 hidden dimension, head 개수, layer 개수, class token 개수, positional embedding 종류, VAE 종류, latent channel 개수, optimizer, learning rate, batch size, learning rate schedule, epoch, augmentation, diffusion sampler, step, evaluation suite, evaluation metric 등의 설정값이 나와 있습니다.</p><details><summary>read the caption</summary>Table 10: System-level comparison configuration.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>#class tokens</th><th>params (M)</th><th>FID10k</th></tr></thead><tbody><tr><td>4</td><td>308 (+3.9)</td><td>12.13</td></tr><tr><td>16</td><td>320 (+15.6)</td><td>12.04</td></tr><tr><td>64</td><td>368 (+62.5)</td><td>11.84</td></tr><tr><td>1 (repeat 64 ×)</td><td>305 (+1.0)</td><td>12.29</td></tr><tr><td>4 (repeat 16 ×)</td><td>308 (+ 3.9)</td><td>11.75</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 CausalFusion과 Transfusion 모델 모두에 대한 다중 모달 실험 설정을 자세히 설명합니다. 두 모델 모두 이미지와 텍스트 데이터를 공동으로 모델링하도록 훈련되었으며, 텍스트-이미지 생성과 이미지 캡션 생성 작업을 동시에 수행합니다. 표에는 이미지 해상도, 숨겨진 차원, 헤드 수, 레이어 수, 최대 텍스트 토큰, 패치 크기, 위치 임베딩, VAE, 잠재 채널, 최적화 도구, 기본 학습률, 텍스트 손실 계수, 가중치 감쇠, 최적화 도구 모멘텀, 배치 크기, 학습률 스케줄, 웜업 에포크, 훈련 에포크, 증강, 확산 샘플러, 확산 단계, 생성 평가 지표, 캡션 평가 지표 등 다양한 설정 세부 정보가 나와 있습니다.</p><details><summary>read the caption</summary>Table 11: Multi-modal experiment configuration for both CausalFusion and Transfusion.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>config</th><th>value</th></tr></thead><tbody><tr><td>image resolution</td><td>256 × 256</td></tr><tr><td>hidden dimension</td><td>1024</td></tr><tr><td>#heads</td><td>16</td></tr><tr><td>#layers</td><td>24</td></tr><tr><td>#cls tokens</td><td>4</td></tr><tr><td>patch size</td><td>2</td></tr><tr><td>positional embedding</td><td>sinusoidal</td></tr><tr><td>VAE</td><td>SD [55]</td></tr><tr><td>VAE donwsample</td><td>8 ×</td></tr><tr><td>latent channel</td><td>4</td></tr><tr><td>optimizer</td><td>AdamW [39]</td></tr><tr><td>base learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>0.0</td></tr><tr><td>optimizer momentum</td><td>β1, β2 = 0.9, 0.95</td></tr><tr><td>batch size</td><td>2048</td></tr><tr><td>learning rate schedule</td><td>constant</td></tr><tr><td>warmup epochs</td><td>40</td></tr><tr><td>training epochs</td><td>240</td></tr><tr><td>augmentation</td><td>horizontal flip, center crop</td></tr><tr><td>diffusion sampler</td><td>DDPM [26]</td></tr><tr><td>diffusion steps</td><td>250</td></tr><tr><td>evaluation suite</td><td>ADM [13]</td></tr><tr><td>evaluation metric</td><td>FID-10k</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 ImageNet 분류 작업을 위한 미세 조정 설정을 보여줍니다. 사전 훈련된 CausalFusion 모델을 ImageNet 데이터셋에서 미세 조정하기 위한 최적화 프로그램, 학습률, 배치 크기, 학습률 스케줄, 증강 및 정규화 방법과 같은 다양한 하이퍼파라미터를 지정합니다.</p><details><summary>read the caption</summary>Table 12: ImageNet classification end-to-end fine-tuning setting.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>config</th><th>value</th></tr></thead><tbody><tr><td>hidden dimension</td><td>1024 (L), 1280 (XL), 1408 (H)</td></tr><tr><td>#heads</td><td>16 (L), 20 (XL), 22 (H)</td></tr><tr><td>#layers</td><td>24 (L), 32 (XL), 40 (H)</td></tr><tr><td>#cls tokens</td><td>64</td></tr><tr><td>positional embedding</td><td>learnable</td></tr><tr><td>VAE</td><td>mar [34]</td></tr><tr><td>VAE donwsample</td><td>16x</td></tr><tr><td>latent channel</td><td>16</td></tr><tr><td>optimizer</td><td>AdamW [39]</td></tr><tr><td>base learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>0.0</td></tr><tr><td>optimizer momentum</td><td>β1,β2=0.9,0.95</td></tr><tr><td>batch size</td><td>2048</td></tr><tr><td>learning rate schedule</td><td>constant</td></tr><tr><td>warmup epochs</td><td>40</td></tr><tr><td>training epochs</td><td>800</td></tr><tr><td>augmentation</td><td>horizontal flip, center crop</td></tr><tr><td>diffusion sampler</td><td>DDPM [26]</td></tr><tr><td>diffusion steps</td><td>250</td></tr><tr><td>evaluation suite</td><td>ADM [13]</td></tr><tr><td>evaluation metric</td><td>FID-50k</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 MSCOCO 캡셔닝 작업을 위한 미세 조정 설정을 제공합니다. 여기에는 최적화 프로그램, 학습률, 가중치 감쇠, 드롭아웃, 최적화 프로그램 모멘텀, 배치 크기, 학습률 스케줄, 웜업 에포크 및 훈련 에포크와 같은 하이퍼파라미터가 포함됩니다.</p><details><summary>read the caption</summary>Table 13: MSCOCO captioning end-to-end fine-tuning setting</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-6481784bd78f9792885317ca423d9f1b class=gallery><img src=paper_images/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12095/&amp;title=Causal%20Diffusion%20Transformers%20for%20Generative%20Modeling" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12095/&amp;text=Causal%20Diffusion%20Transformers%20for%20Generative%20Modeling" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12095/&amp;subject=Causal%20Diffusion%20Transformers%20for%20Generative%20Modeling" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.12095/index.md",oid_likes="likes_paper-reviews/2412.12095/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.11815/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">ColorFlow: Retrieval-Augmented Image Sequence Colorization</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-16T00:00:00+00:00>16 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.13061/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">VidTok: A Versatile and Open-Source Video Tokenizer</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-17T00:00:00+00:00>17 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>