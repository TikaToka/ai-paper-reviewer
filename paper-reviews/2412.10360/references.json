{"references": [{"fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "Building and better understanding vision-language models: insights and future directions.", "publication_date": "2024-08-26", "reason": "This paper provides valuable insights into the design and evaluation of vision-language models, which are crucial for understanding the advancements and limitations of video-LMMs."}, {"fullname_first_author": "Chaoyou Fu", "paper_title": "Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal LLMs in video analysis.", "publication_date": "2024-05-08", "reason": "Video-MME is a crucial benchmark for evaluating video-LMMs, and this paper details its construction and significance."}, {"fullname_first_author": "Yi Wang", "paper_title": "InternVideo2: Scaling video foundation models for multimodal video understanding.", "publication_date": "2024-03-28", "reason": "This work introduces InternVideo2, a high-performing video encoder crucial for achieving state-of-the-art results in video-LMMs."}, {"fullname_first_author": "Zuyan Liu", "paper_title": "Oryx MLLM: On-demand spatial-temporal understanding at arbitrary resolution.", "publication_date": "2024-09-25", "reason": "Oryx is a state-of-the-art video-LMM, and this paper details its architecture and performance, serving as a key comparison point for Apollo."}, {"fullname_first_author": "Xiaohua Zhai", "paper_title": "Sigmoid loss for language image pre-training.", "publication_date": "2023-01-01", "reason": "This work introduces SigLIP, a powerful image encoder found to be extremely effective for video-LMMs, as discussed extensively in the paper."}]}