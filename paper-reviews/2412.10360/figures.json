[{"figure_path": "https://arxiv.org/html/2412.10360/x1.png", "caption": "Figure 1: Apollo exploration. Schematic illustrating our comprehensive exploration of video-specific design choices; critically evaluating the existing conceptions in the field, from video sampling and model architecture to training schedules and data compositions.\nFor example, we found that the SigLIP encoder is the best single encoder for video-LMMs but can be combined with additional encoders to improve temporal perception, and that keeping a \u223c10%similar-toabsentpercent10\\sim 10\\%\u223c 10 % text data during fine-tuning is critical for video understanding performance. More insights can be found in Sec.\u00a04 &\u00a0LABEL:sec:training.", "description": "\uc774 \uadf8\ub9bc\uc740 \ube44\ub514\uc624 \uc774\ud574\ub97c \uc704\ud55c \uac70\ub300 \uba40\ud2f0\ubaa8\ub2ec \ubaa8\ub378(LMM) \uc124\uacc4 \uacf5\uac04 \ud0d0\uc0c9\uc5d0 \ub300\ud55c \uac1c\uc694\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube44\ub514\uc624 \uc0d8\ud50c\ub9c1, \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98, \ud6c8\ub828 \uc77c\uc815 \ubc0f \ub370\uc774\ud130 \uad6c\uc131\uacfc \uac19\uc740 \ube44\ub514\uc624 \uad00\ub828 \uc124\uacc4 \uc120\ud0dd \ud56d\ubaa9\uc744 \ud3ec\uad04\uc801\uc73c\ub85c \ud0d0\uad6c\ud558\ub294 \uc544\ud3f4\ub85c \ud504\ub808\uc784\uc6cc\ud06c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 SigLIP \uc778\ucf54\ub354\uac00 \ube44\ub514\uc624 LMM\uc5d0 \uac00\uc7a5 \uc801\ud569\ud55c \ub2e8\uc77c \uc778\ucf54\ub354\uc774\uc9c0\ub9cc \uc2dc\uac04\uc801 \uc778\uc2dd\uc744 \uac1c\uc120\ud558\uae30 \uc704\ud574 \ucd94\uac00 \uc778\ucf54\ub354\uc640 \uacb0\ud569\ud560 \uc218 \uc788\uace0, \ubbf8\uc138 \uc870\uc815 \uc911 \uc57d 10%\uc758 \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\ub97c \uc720\uc9c0\ud558\ub294 \uac83\uc774 \ube44\ub514\uc624 \uc774\ud574 \uc131\ub2a5\uc5d0 \uc911\uc694\ud558\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.10360/x2.png", "caption": "Figure 2:  Benchmark Analysis. (Left) Accuracy of the open-source LMMs on various video question-answering benchmarks when provided with different input modalities: full video (green bars), a single frame from the video (red bars), and text-only input without any visual content (blue bars). The light blue shaded areas represent the difference in accuracy between video and text inputs, highlighting the extent to which video perception enhances performance over text comprehension alone. The yellow shaded areas indicate the difference between video and image inputs, quantifying the additional benefit of temporal information from videos compared to static images.\n(Right) The correlation matrix shows the redundancy among benchmarks by illustrating the correlation coefficients between model performances on different benchmarks. Each cell in the matrix represents how closely the two benchmarks are related in terms of model performance. Our proposed benchmark, ApolloBench, is highly correlated with all tested benchmarks, suggesting that it offers an equally effective evaluation while being more computationally efficient.", "description": "\uc774 \uadf8\ub9bc\uc740 \ube44\ub514\uc624 \uc9c8\uc758\uc751\ub2f5 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc624\ud508\uc18c\uc2a4 LMM\uc758 \uc815\ud655\ub3c4\uc640 \ubca4\uce58\ub9c8\ud06c \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubd84\uc11d\ud55c \uac83\uc785\ub2c8\ub2e4. \uc67c\ucabd \uadf8\ub798\ud504\ub294 \ube44\ub514\uc624, \ub2e8\uc77c \ud504\ub808\uc784, \ud14d\uc2a4\ud2b8 \uc785\ub825\ubcc4 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\uace0, \ube44\ub514\uc624 \uc778\uc2dd\uc774 \ud14d\uc2a4\ud2b8 \uc774\ud574\ubcf4\ub2e4 \uc5bc\ub9c8\ub098 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud558\ub294\uc9c0, \uadf8\ub9ac\uace0 \ube44\ub514\uc624\uc758 \uc2dc\uac04\uc801 \uc815\ubcf4\uac00 \uc815\uc801 \uc774\ubbf8\uc9c0 \ub300\ube44 \uc5bc\ub9c8\ub098 \ucd94\uac00\uc801\uc778 \uc774\uc810\uc744 \uc81c\uacf5\ud558\ub294\uc9c0\ub97c \uac15\uc870\ud569\ub2c8\ub2e4. \uc624\ub978\ucabd\uc758 \uc0c1\uad00 \ud589\ub82c\uc740 \uac01 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ubaa8\ub378 \uc131\ub2a5 \uac04\uc758 \uc0c1\uad00 \uacc4\uc218\ub97c \ubcf4\uc5ec\uc90c\uc73c\ub85c\uc368 \ubca4\uce58\ub9c8\ud06c \uac04\uc758 \uc911\ubcf5\uc131\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc81c\uc548\ub41c ApolloBench\ub294 \ub2e4\ub978 \ubca4\uce58\ub9c8\ud06c\uc640 \ub192\uc740 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc774\uba70 \ud6a8\uc728\uc801\uc778 \ud3c9\uac00\ub97c \uc81c\uacf5\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "2 How effective are existing video question-answering benchmarks?"}, {"figure_path": "https://arxiv.org/html/2412.10360/x3.png", "caption": "Figure 3: \nScaling Consistency. We discover Scaling Consistency, where design decisions made with smaller models on smaller datasets carry over to larger models on larger datasets. (Left) R2superscript\ud835\udc452R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT values of\n7B and 0.5B versus other LLM sizes show an increasing correlation with larger LLM sizes for the 7B model. The same trend is not seen in the 0.50.50.50.5B model.\nInterestingly, while the Qwen1.51.51.51.5-4444B model variants have lower/similar performance to their smaller Qwen2\u22121.521.52-1.52 - 1.5B counterparts, the correlation to larger models is still higher (See App. Fig.\u00a0LABEL:sup:fig:scaling_consistency).\n(Right) R2superscript\ud835\udc452R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT of 0.5/1.5/40.51.540.5/1.5/40.5 / 1.5 / 4B models to 7777B vs dataset size. R2superscript\ud835\udc452R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT to larger datasets starts to plateau at around 500500500500K samples.", "description": "\uc774 \uadf8\ub9bc\uc740 LMM\uc758 \ud06c\uae30 \ubc0f \ub370\uc774\ud130\uc14b \ud06c\uae30\uc5d0 \ub530\ub978 \ub514\uc790\uc778 \uacb0\uc815\uc758 \uc77c\uad00\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd \uadf8\ub798\ud504\ub294 \ub354 \ud070 LLM(7B)\uc758 \uacbd\uc6b0 \ub354 \uc791\uc740 LLM\uacfc\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \uc99d\uac00\ud568\uc744 \ubcf4\uc5ec\uc8fc\uc9c0\ub9cc, \ub354 \uc791\uc740 LLM(0.5B)\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \uacbd\ud5a5\uc774 \ub098\ud0c0\ub098\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc624\ub978\ucabd \uadf8\ub798\ud504\ub294 \ub370\uc774\ud130\uc14b \ud06c\uae30\uac00 \uc57d 500K \uc0d8\ud50c\uc77c \ub54c \ub354 \ud070 \ubaa8\ub378(7B)\uacfc\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \uc548\uc815\ud654\ub428\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc989, \ud2b9\uc815 \ud06c\uae30 \uc774\uc0c1\uc758 \ubaa8\ub378\uacfc \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uba74 \ub514\uc790\uc778 \uacb0\uc815\uc774 \uc77c\uad00\ub418\uac8c \ub098\ud0c0\ub098\ubbc0\ub85c, \ub354 \uc791\uc740 \ubaa8\ub378\uacfc \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud6a8\uc728\uc801\uc778 \uc2e4\ud5d8 \uc124\uacc4\uac00 \uac00\ub2a5\ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3 Scaling Consistency: How small can you go during model design?"}, {"figure_path": "https://arxiv.org/html/2412.10360/x4.png", "caption": "Figure 4: Video sampling. We compare different sampling strategies and their effect on performance.\n(Left) Models were trained and tested using uniform sampling. Increasing the number of frames improves overall performance but does not reach fps sampling performance.\n(Middle) Models trained with uniform sampling but tested with fps sampling. Differences in performance are not explained by the number of frames sampled at test time.\n(Right) Analysis of the effect of frames per second (fps) and tokens per second (tps) on overall performance. The dotted red lines (-\u00a0-) indicate the tokens per frame. For a per-metric breakdown, please see App. Fig.\u00a0LABEL:sup:fig:full_sampling.", "description": "\uc774 \uadf8\ub9bc\uc740 \ube44\ub514\uc624 \uc0d8\ud50c\ub9c1 \uc804\ub7b5\uc744 \ube44\uad50\ud558\uace0 LMM(Large Multimodal Model) \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4. \uc67c\ucabd \uadf8\ub9bc\uc740 \uade0\uc77c \uc0d8\ud50c\ub9c1\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828 \ubc0f \ud14c\uc2a4\ud2b8\ud55c \ubaa8\ub378\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud504\ub808\uc784 \uc218\ub97c \ub298\ub9ac\uba74 \uc804\ubc18\uc801\uc778 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\uc9c0\ub9cc FPS \uc0d8\ud50c\ub9c1 \uc131\ub2a5\uc5d0\ub294 \ubbf8\uce58\uc9c0 \ubabb\ud569\ub2c8\ub2e4. \uac00\uc6b4\ub370 \uadf8\ub9bc\uc740 \uade0\uc77c \uc0d8\ud50c\ub9c1\uc73c\ub85c \ud6c8\ub828\ub418\uc5c8\uc9c0\ub9cc FPS \uc0d8\ud50c\ub9c1\uc73c\ub85c \ud14c\uc2a4\ud2b8\ub41c \ubaa8\ub378\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud14c\uc2a4\ud2b8 \uc2dc \uc0d8\ud50c\ub9c1\ub41c \ud504\ub808\uc784 \uc218\ub85c\ub294 \uc131\ub2a5 \ucc28\uc774\ub97c \uc124\uba85\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. \uc624\ub978\ucabd \uadf8\ub9bc\uc5d0\uc11c\ub294 \ucd08\ub2f9 \ud504\ub808\uc784 \uc218(FPS)\uc640 \ucd08\ub2f9 \ud1a0\ud070 \uc218(TPS)\uac00 \uc804\ubc18\uc801\uc778 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4. \uc810\uc120 \ube68\uac04\uc0c9 \uc120\uc740 \ud504\ub808\uc784\ub2f9 \ud1a0\ud070 \uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc790\uc138\ud55c \ubd84\uc11d\uc740 \ubd80\ub85d \uadf8\ub9bc LABEL:sup:fig:full_sampling\uc744 \ucc38\uc870\ud558\uc2ed\uc2dc\uc624.", "section": "4. Exploring the video-LMM design space: what influences effective model design?"}, {"figure_path": "https://arxiv.org/html/2412.10360/x5.png", "caption": "Figure 5: Vision encoders. In our study, we tested InternVideo2\u00a0(internvideo2), LanguageBind-Image/Video\u00a0(languagebind),V-JEPA\u00a0(vjepa), Video-MAE\u00a0(videomae), SigLIP-SO400400400400M\u00a0(siglip), and DINOv2\u00a0(dinov2), and their combinations. (Left) SigLIP-SO-400400400400M emerges as the best overall among single encoders. We also find that image encoders underperform in temporal perception compared to video encoders. (Right) Performance of dual-encoder configurations. Language-supervised encoders outperformed their self-supervised counterparts. Combining InternVideo2 and SigLIP-SO-400400400400M leads to the best overall performance.", "description": "\uc774 \uadf8\ub9bc\uc740 \ub2e4\uc591\ud55c \ube44\uc804 \uc778\ucf54\ub354\ub97c \ub2e8\ub3c5 \ub610\ub294 \uc870\ud569\ud558\uc5ec \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. \uc67c\ucabd \uadf8\ub798\ud504\ub294 \ub2e8\uc77c \uc778\ucf54\ub354\ub97c \uc0ac\uc6a9\ud55c \uacb0\uacfc\uc774\uba70, SigLIP-SO400M\uc774 \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. \ub610\ud55c \uc774\ubbf8\uc9c0 \uc778\ucf54\ub354\ub294 \uc2dc\uac04\uc801 \uc778\uc2dd \ub2a5\ub825\uc774 \ube44\ub514\uc624 \uc778\ucf54\ub354\ubcf4\ub2e4 \ub5a8\uc5b4\uc9c0\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc624\ub978\ucabd \uadf8\ub798\ud504\ub294 \ub450 \uac1c\uc758 \uc778\ucf54\ub354\ub97c \uc870\ud569\ud558\uc5ec \uc0ac\uc6a9\ud55c \uacb0\uacfc\uc774\uba70, \uc5b8\uc5b4 \uac10\ub3c5 \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ub41c \uc778\ucf54\ub354\uac00 \uc790\uae30 \uc9c0\ub3c4 \ud559\uc2b5 \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ub41c \uc778\ucf54\ub354\ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc6b0\uc218\ud569\ub2c8\ub2e4. \ud2b9\ud788, InternVideo2\uc640 SigLIP-SO400M\uc744 \uacb0\ud569\ud588\uc744 \ub54c \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "4.2 Video representation"}]