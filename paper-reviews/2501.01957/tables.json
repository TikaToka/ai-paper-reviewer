[{"content": "| Data Scenario | QA Type | Dataset Name | Questions (K) | Language |\n|---|---|---|---|---|\n| General Image | Description | ShareGPT4V | 99.50 | Eng |\n|  |  | ALLaVA-Caption | 697.40 | Eng |\n|  |  | ShareGTP4o-Image | 55.50 | Eng |\n|  |  | Synthetic Data | 593.70 | CN |\n|  | QA | LLaVA-150K | 218.36 | CN |\n|  |  | LLaVA-Mixture-sample | 1872.10 | Eng |\n|  |  | LVIS-Instruct | 939.36 | Eng |\n|  |  | ScienceQA | 12.72 | Eng |\n|  |  | ChatQA | 7.39 | Eng |\n|  |  | LLaVA-OV General | 1754.65 | Eng |\n|  |  | LLaVA-OV Math Reasoning | 1140.92 | Eng |\n|  |  | Synthetic Data | 212.68 | CN |\n| OCR & Diagram | Description | Anyword-3M | 1709.30 | CN |\n|  |  | ICDAR2019-LSVT | 366.30 | CN |\n|  |  | UReader | 100.00 | Eng |\n|  |  | SynDOG-EN | 100.00 | Eng |\n|  |  | SynDOG-CN | 101.90 | CN |\n|  | QA | ICDAR2019-LSVT-QA | 630.08 | CN |\n|  |  | LLaVA-OV Doc Chart Screen | 4431.50 | Eng |\n|  |  | LLaVA-OV General OCR | 404.20 | Eng |\n| General Video | Description | ShareGemini | 205.70 | CN |\n|  |  | Synthetic Data | 569.40 | CN & Eng |\n|  | QA | Synthetic Data | 4336.30 | CN & Eng |\n| Pure Text | QA | Synthetic Data | 1574.20 | CN & Eng |\n| Total |  |  | 22133.16 | CN & Eng |", "caption": "Table 1: Training data of multimodal instruction tuning. The images of the synthetic data come from open-source datasets like Wukong\u00a0[19], LAION\u00a0[46], and CC12M\u00a0[5].", "description": "\ud45c 1\uc740 \ub2e4\uc911 \ubaa8\ub4dc \uc9c0\uc2dc \uc870\uc815\uc744 \uc704\ud55c \ud6c8\ub828 \ub370\uc774\ud130\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\uc5d0\ub294 \uc774\ubbf8\uc9c0 \ucea1\uc158 \ub370\uc774\ud130, \uc774\ubbf8\uc9c0 \uc9c8\uc758\uc751\ub2f5 \ub370\uc774\ud130, OCR \ubc0f \ub2e4\uc774\uc5b4\uadf8\ub7a8 \ub370\uc774\ud130, \ube44\ub514\uc624 \ub370\uc774\ud130, \uc21c\uc218 \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130 \ub4f1 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ub370\uc774\ud130\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ud569\uc131 \ub370\uc774\ud130\uc758 \uc774\ubbf8\uc9c0\ub294 Wukong [19], LAION [46], CC12M [5]\uacfc \uac19\uc740 \uc624\ud508\uc18c\uc2a4 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uac00\uc838\uc654\uc2b5\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130 \uc720\ud615\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uc124\uba85\uacfc \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b, \uc9c8\ubb38 \uc218, \uc5b8\uc5b4 \ub4f1\uc758 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.2 \ud6c8\ub828 \ub370\uc774\ud130"}, {"content": "| Questions (K) |\n|---|---|", "caption": "Table 2: Evaluation on Image Understanding Benchmarks. VITA-1.5 shows performance comparable to the leading open-source models and advanced closed-source counterparts. MMB refers to MMBench, MMS to MMStar, Hal to HallusionBench, MathV to MathVista, and OCR to OCRBench. Note that after the training of Stages 2 (Audio Input Tuning) and 3 (Audio Output Tuning), VITA-1.5 retains almost its original visual-language capabilities in Stage 1 (Vision-Language Training).", "description": "\ud45c 2\ub294 \uc774\ubbf8\uc9c0 \uc774\ud574 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. VITA-1.5\uc758 \uc131\ub2a5\uc740 \ucd5c\uace0 \uc218\uc900\uc758 \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378 \ubc0f \uace0\uae09 \ud074\ub85c\uc988\ub4dc\uc18c\uc2a4 \ubaa8\ub378\uacfc \uc720\uc0ac\ud569\ub2c8\ub2e4.  MMB\ub294 MMBench, MMS\ub294 MMStar, Hal\uc740 HallusionBench, MathV\ub294 MathVista, OCR\uc740 OCRBench\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.  2\ub2e8\uacc4(\uc624\ub514\uc624 \uc785\ub825 \uc870\uc815)\uc640 3\ub2e8\uacc4(\uc624\ub514\uc624 \ucd9c\ub825 \uc870\uc815) \ud559\uc2b5 \ud6c4\uc5d0\ub3c4 VITA-1.5\ub294 1\ub2e8\uacc4(\ube44\uc804-\uc5b8\uc5b4 \ud559\uc2b5)\uc758 \uc6d0\ub798 \uc2dc\uac01-\uc5b8\uc5b4 \uae30\ub2a5\uc744 \uac70\uc758 \uadf8\ub300\ub85c \uc720\uc9c0\ud569\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0 \uc774\ud574 \ubca4\uce58\ub9c8\ud06c(MMBench, MMStar, MM-MU, MathVista, HallucinationBench, AI-2D, OCRBench \ub4f1)\uc5d0\uc11c\uc758 VITA-1.5\uc640 \ub2e4\ub978 \uc8fc\uc694 \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud558\uc5ec \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uac01 \ubca4\uce58\ub9c8\ud06c \uc810\uc218\ub97c \ud1b5\ud574 VITA-1.5\uc758 \uc774\ubbf8\uc9c0 \uc774\ud574 \ub2a5\ub825\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ud3c9\uac00\ud558\uace0 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uacfc\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uba85\ud655\ud558\uac8c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud2b9\ud788 \uc624\ub514\uc624 \uad00\ub828 \ub2e8\uacc4 \ud559\uc2b5 \ud6c4\uc5d0\ub3c4 \uc774\ubbf8\uc9c0 \uc774\ud574 \ub2a5\ub825\uc774 \uc720\uc9c0\ub418\ub294 \uc810\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.", "section": "4.1 Vision-Language Evaluation"}, {"content": "| Method | LLM | MMB | MMB | MMS | MMMU | MathV | Hal | AI2D | OCR | MMVet | MME | Avg |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| VILA-1.5 | Vicuna-v1.5-13B | 68.5 | 44.2 | 41.1 | 42.5 | 39.3 | 69.9 | 460.0 | 45.0 | 1718.2 | 52.1 |\n| LLaVA-Next | Yi-34b | 77.8 | 51.6 | 48.8 | 40.4 | 34.8 | 78.9 | 574.0 | 50.7 | 2006.5 | 58.3 |\n| CogVLM2 | Llama3-8B-Instruct | 70.7 | 50.5 | 42.6 | 38.6 | 41.3 | 73.4 | 757.0 | 57.8 | 1869.5 | 58.8 |\n| InternLM-Xcomposer2 | InternLM2-7B | 77.6 | 56.2 | 41.4 | 59.5 | 41.0 | 81.2 | 532.0 | 46.7 | 2220.4 | 61.2 |\n| Cambrian | Nous-Hermes-2-Yi-34B | 77.8 | 54.2 | 50.4 | 50.3 | 41.6 | 79.5 | 591.0 | 53.2 | 2049.9 | 61.4 |\n| InternVL-Chat-1.5 | InternLM2-20B | 79.7 | 57.1 | 46.8 | 54.7 | 47.4 | 80.6 | 720.0 | 55.4 | 2189.6 | 65.1 |\n| Ovis1.5 | Gemma2-9B-It | 77.3 | 58.1 | 49.7 | 65.6 | 48.2 | 84.5 | 752.0 | 53.8 | 2125.2 | 66.9 |\n| InternVL2 | InternLM2.5-7b | 79.4 | 61.5 | 51.2 | 58.3 | 45.0 | 83.6 | 794.0 | 54.3 | 2215.1 | 67.3 |\n| MiniCPM-V 2.6 | Qwen2-7B | 78.0 | 57.5 | 49.8 | 60.6 | 48.1 | 82.1 | 852.0 | 60.0 | 2268.7 | 68.5 |\n| Proprietary |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4V | - | 65.5 | 50.4 | 59.3 | 48.2 | 39.3 | 71.4 | 678.0 | 49.0 | 1790.3 | 58.5 |\n| GPT-4o mini | - | 76.0 | 54.8 | 60.0 | 52.4 | 46.1 | 77.8 | 785.0 | 66.9 | 2003.4 | 66.3 |\n| Gemini 1.5 Pro | - | 73.9 | 59.1 | 60.6 | 57.7 | 45.6 | 79.1 | 754.0 | 64.0 | 2110.6 | 67.2 |\n| GPT-4o | - | 82.8 | 61.6 | 62.8 | 56.5 | 51.7 | 77.4 | 663.0 | 66.5 | 2328.7 | 69.3 |\n| Claude3.5 Sonnet | - | 78.5 | 62.2 | 65.9 | 61.6 | 49.9 | 80.2 | 788.0 | 66.0 | 1920.0 | 69.3 |\n| Ours |  |  |  |  |  |  |  |  |  |  |  |\n| VITA-1.0 | Mixtral-8x7B | 71.8 | 46.4 | 47.3 | 44.9 | 39.7 | 73.1 | 678.0 | 41.6 | 2097.0 | 57.8 |\n| VITA-1.5 (Stage 1) | Qwen2-7B | 77.1 | 59.1 | 53.1 | 66.2 | 44.1 | 80.3 | 752.0 | 51.1 | 2311.0 | 67.1 |\n| VITA-1.5-Audio (Stage 3) | Qwen2-7B | 76.7 | 59.9 | 52.1 | 66.2 | 44.9 | 79.3 | 732.0 | 49.6 | 2352.0 | 66.8 |", "caption": "Table 3: Evaluation on Video Understanding Benchmarks. Although VITA-1.5 still lags behind models like GPT-4o and Gemini-1.5-Pro, it performs comparably to many open-source models. Note that after the training of Stages 2 (Audio Input Tuning) and 3 (Audio Output Tuning), VITA-1.5 retains almost its original visual-language capabilities in Stage 1 (Vision-Language Training).", "description": "\ud45c 3\uc740 \ube44\ub514\uc624 \uc774\ud574 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c VITA-1.5\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  VITA-1.5\ub294 GPT-40\uc774\ub098 Gemini-1.5-Pro\uc640 \uac19\uc740 \ucd5c\ucca8\ub2e8 \ubaa8\ub378\uc5d0\ub294 \ubbf8\uce58\uc9c0 \ubabb\ud558\uc9c0\ub9cc, \uc5ec\ub7ec \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378\ub4e4\uacfc \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4.  \ud2b9\ud788 2\ub2e8\uacc4(\uc624\ub514\uc624 \uc785\ub825 \ubbf8\uc138 \uc870\uc815)\uc640 3\ub2e8\uacc4(\uc624\ub514\uc624 \ucd9c\ub825 \ubbf8\uc138 \uc870\uc815) \ud6c8\ub828 \ud6c4\uc5d0\ub3c4 VITA-1.5\ub294 1\ub2e8\uacc4(\ube44\ub514\uc624-\uc5b8\uc5b4 \ud6c8\ub828)\uc758 \uc6d0\ub798 \ube44\ub514\uc624-\uc5b8\uc5b4 \uae30\ub2a5\uc744 \uac70\uc758 \uc720\uc9c0\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ube44\ub514\uc624 \uc774\ud574 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c VITA-1.5\uc758 \uc131\ub2a5\uc744 \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.1 Vision-Language Evaluation"}, {"content": "| Method | LLM | Video-MME w/o sub | Video-MME w/ sub | MVBench | TempCompass |\n|---|---|---|---|---|---| \n| Video-LLaVA | Vicuna-v1.5-13B | 39.9 | 41.6 |  | 49.8 |\n| SliME | Llama3-8B-Instruct | 45.3 | 47.2 | - | - |\n| LongVA | Qwen2-7B | 52.6 | 54.3 | - | 57.0 |\n| VILA-1.5 | Llama3-8B-Instruct | - | - | - | 58.8 |\n| InternLM-XComposer-2.5 | InternLM2-7B | - | - | - | 62.1 |\n| LLaVA-OneVision | Qwen2-7B | 58.2 | 61.5 | 56.7 | 64.2 |\n| InternVL-2 | InternLM2.5-7b | - | - | - | 66.0 |\n| MiniCPM-V-2.6 | Qwen2-7B | 60.9 | 63.7 | - | 66.3 |\n|  |  |  |  |  |  |\n| GPT-4o-mini | - | 64.8 | 68.9 | - |  |\n| Gemini-1.5-Pro | - | 75.0 | 81.3 | - | 67.1 |\n| GPT-4o | - | 71.9 | 77.2 | - | 73.8 |\n|  |  |  |  |  |  |\n| VITA-1.0 | Mixtral-8x7B | 55.8 | 59.2 | - | 62.3 |\n| VITA-1.5 (Stage 1) | Qwen2-7B | 56.8 | 59.5 | 56.8 | 65.5 |\n| VITA-1.5 (Stage 3) | Qwen2-7B | 56.1 | 58.7 | 55.4 | 66.7 |", "caption": "Table 4: Evaluation on ASR Benchmarks. VITA-1.5 has demonstrated strong performance in both Mandarin and English ASR tasks. It outperforms specialized speech models, achieving better results in both languages.", "description": "\ud45c 4\ub294 \uc74c\uc131 \uc778\uc2dd(ASR) \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. VITA-1.5\ub294 \uc911\uad6d\uc5b4\uc640 \uc601\uc5b4 \uc74c\uc131 \uc778\uc2dd \uc791\uc5c5 \ubaa8\ub450\uc5d0\uc11c \uac15\ub825\ud55c \uc131\ub2a5\uc744 \uc785\uc99d\ud588\uc2b5\ub2c8\ub2e4. \ud2b9\ud788 \uae30\uc874\uc758 \uc804\ubb38\uc801\uc778 \uc74c\uc131 \ubaa8\ub378\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub6f0\uc5b4\ub098 \ub450 \uc5b8\uc5b4 \ubaa8\ub450\uc5d0\uc11c \ub354 \ub098\uc740 \uacb0\uacfc\ub97c \uc5bb\uc5c8\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b(aishell-1, test net, test meeting, dev clean, dev other, test clean, test other)\uc5d0 \ub300\ud55c \ubb38\uc790 \uc624\ub958\uc728(CER)\uacfc \ub2e8\uc5b4 \uc624\ub958\uc728(WER)\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4 \ud3c9\uac00"}]