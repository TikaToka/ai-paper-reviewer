{"references": [{"fullname_first_author": "Lin Chen", "paper_title": "ShareGPT4Video: Improving video understanding and generation with better captions.", "publication_date": "2024-06-04", "reason": "This paper introduces ShareGPT4Video, a state-of-the-art video captioning model that enriches textual content but suffers from hallucination issues, serving as a key comparison point and motivation for the proposed InstanceCap method."}, {"fullname_first_author": "Xuan Ju", "paper_title": "MiraData: A large-scale video dataset with long durations and structured captions.", "publication_date": "2024-07-06", "reason": "MiraData provides coarse-level structured captions, which serves as a baseline and comparison point for InstanceCap's instance-aware structured captions."}, {"fullname_first_author": "Tsai-Shien Chen", "paper_title": "Panda-70M: Captioning 70M videos with multiple cross-modality teachers.", "publication_date": "2024-02-19", "reason": "This paper presents Panda-70M, a method generating short captions with limited content, highlighting the need for more detailed captioning methods like InstanceCap."}, {"fullname_first_author": "Kepan Nan", "paper_title": "OpenVid-1M: A large-scale high-quality dataset for text-to-video generation.", "publication_date": "2024-07-02", "reason": "OpenVid-1M serves as the primary dataset for evaluating video captioning and T2V generation methods, including InstanceCap."}, {"fullname_first_author": "Zhuoyi Yang", "paper_title": "CogVideoX: Text-to-video diffusion models with an expert transformer.", "publication_date": "2024-08-06", "reason": "CogVideoX, a latent text-to-video generation model, is used to evaluate the effectiveness of InstanceCap by comparing the reconstruction quality of generated videos from different captioning methods."}]}