{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-04", "reason": "This paper introduced InstructGPT, establishing the core supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF) workflow that is foundational to most instruction-following language models today."}, {"fullname_first_author": "Nathan Lambert", "paper_title": "T\u00fclu 3: Pushing frontiers in open language model post-training", "publication_date": "2024-01-01", "reason": "This work details the specific training and optimization recipes used and modified by the current paper for its SmolTulu model."}, {"fullname_first_author": "Loubna Ben Allal", "paper_title": "Smollm2 - with great data, comes great performance", "publication_date": "2024-01-01", "reason": "This paper introduced the SmolLM family of language models, on which the SmolTulu model is based."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-01-01", "reason": "This paper details Direct Preference Optimization (DPO), a novel, simplified approach to preference learning employed for training SmolTulu."}, {"fullname_first_author": "Dominic Masters", "paper_title": "Revisiting small batch training for deep neural networks", "publication_date": "2018-01-01", "reason": "This work offers theoretical arguments in favor of smaller batch sizes in specific cases, motivating SmolTulu's exploration of higher learning rate to batch size ratios."}]}