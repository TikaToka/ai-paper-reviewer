{"references": [{"fullname_first_author": "Patrick S. H. Lewis", "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks", "publication_date": "2020-12-06", "reason": "This paper introduced the concept of Retrieval-Augmented Generation (RAG), which has become a cornerstone in addressing hallucination problems in large language models by incorporating external knowledge sources."}, {"fullname_first_author": "Xiaoxi Li", "paper_title": "UniGen: A unified generative framework for retrieval and question answering with large language models", "publication_date": "2024-02-20", "reason": "This paper presents UniGen, a unified generative framework for retrieval and question answering with LLMs, which enables joint optimization and eliminates the need for separate retrievers, paving the way for more streamlined and efficient RAG systems."}, {"fullname_first_author": "Xiaoxi Li", "paper_title": "CorpusLM: Towards a unified language model on corpus for knowledge-intensive tasks", "publication_date": "2024-07-14", "reason": "CorpusLM proposes using a unified language model directly on the corpus, streamlining retrieval and generation by allowing the LLM to generate evidence and answers directly from the corpus."}, {"fullname_first_author": "Palak Jain", "paper_title": "From RAG to RICHES: retrieval interlaced with sequence generation", "publication_date": "2024-07-01", "reason": "This work introduces RICHES, a method that interlaces retrieval and generation by constraining decoding with FM-Index, improving evidence generation accuracy and efficiency in RAG systems."}, {"fullname_first_author": "Akari Asai", "paper_title": "Self-RAG: Learning to retrieve, generate, and critique through self-reflection", "publication_date": "2024-05-07", "reason": "This paper introduced Self-RAG, which leverages self-reflection mechanisms to iteratively refine both retrieval and generation quality in RAG models."}]}