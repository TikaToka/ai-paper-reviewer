<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation &#183; AI Paper Reviews by AI</title>
<meta name=title content="RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation &#183; AI Paper Reviews by AI"><meta name=description content="RetroLLM: 검색과 생성을 통합한 RAG 시스템"><meta name=keywords content="Natural Language Processing,Question Answering,🏢 Renmin University of China,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation"><meta property="og:description" content="RetroLLM: 검색과 생성을 통합한 RAG 시스템"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-16T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-16T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Question Answering"><meta property="article:tag" content="🏢 Renmin University of China"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/cover.png"><meta name=twitter:title content="RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation"><meta name=twitter:description content="RetroLLM: 검색과 생성을 통합한 RAG 시스템"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation","headline":"RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation","abstract":"RetroLLM: 검색과 생성을 통합한 RAG 시스템","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.11919\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-12-16T00:00:00\u002b00:00","datePublished":"2024-12-16T00:00:00\u002b00:00","dateModified":"2024-12-16T00:00:00\u002b00:00","keywords":["Natural Language Processing","Question Answering","🏢 Renmin University of China"],"mainEntityOfPage":"true","wordCount":"3747"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.11919/cover_hu2503229949671907280.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.11919/>RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-16T00:00:00+00:00>16 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>3747 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">18 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.11919/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.11919/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🤗 Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/question-answering/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Question Answering
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-renmin-university-of-china/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🏢 Renmin University of China</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#llm-hallucination>LLM Hallucination</a></li><li><a href=#retrollm-framework>RetroLLM Framework</a></li><li><a href=#joint-optimization>Joint Optimization</a></li><li><a href=#constrained-decoding>Constrained Decoding</a></li><li><a href=#evidence-accuracy>Evidence Accuracy</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#llm-hallucination>LLM Hallucination</a></li><li><a href=#retrollm-framework>RetroLLM Framework</a></li><li><a href=#joint-optimization>Joint Optimization</a></li><li><a href=#constrained-decoding>Constrained Decoding</a></li><li><a href=#evidence-accuracy>Evidence Accuracy</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.11919</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Xiaoxi Li et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>🤗 2024-12-17</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.11919 target=_self role=button>↗ arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.11919 target=_self role=button>↗ Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/retrollm-empowering-large-language-models-to target=_self role=button>↗ Papers with Code</a></p><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>대규모 언어 모델(LLM)은 인상적인 텍스트 생성 기능을 보여주지만, 사실적 오류 또는 &lsquo;환각&rsquo;에 취약합니다. 검색 증강 생성(RAG)은 외부 지식 소스를 통합하여 이러한 한계를 해결하지만, 별도의 검색기 배포 비용, 검색된 텍스트 청크의 중복 입력 토큰 및 검색과 생성 간 공동 최적화 부족과 같은 문제가 있습니다.</p><p>기존 RAG의 문제점을 해결하기 위해, RetroLLM은 검색과 생성을 단일 프로세스로 통합하는 통합 프레임워크를 도입했습니다. 이를 통해 LLM은 제약된 디코딩을 사용하여 코퍼스에서 직접 미세 조정된 증거를 생성하여 별도의 검색 모델에 대한 필요성을 없앨 수 있습니다. 제약된 증거 생성에서 잘못된 가지치기 문제를 완화하기 위해, RetroLLM은 후보 문서의 하위 집합을 식별하기 위해 계층적 FM-Index 제약을 사용하고, 전방탐색 제약 디코딩 전략을 사용하여 미래 시퀀스의 관련성을 고려하여 증거 정확성을 향상시킵니다.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-9d125f7142743af3d98d9a79ba6f6d77></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-9d125f7142743af3d98d9a79ba6f6d77",{strings:[" RetroLLM은 단일 프로세스에서 검색과 생성을 통합하여 기존 RAG 시스템의 한계를 해결합니다. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-975a1c54e5ae0bf62fde7d1822243391></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-975a1c54e5ae0bf62fde7d1822243391",{strings:[" 계층적 FM-Index와 전방탐색 제약 디코딩은 잘못된 가지치기 문제를 완화하여 증거 정확성을 향상시킵니다. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-14cb1270832c225dce918f3e4a0cd1c2></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-14cb1270832c225dce918f3e4a0cd1c2",{strings:[" RetroLLM은 여러 QA 데이터 세트에서 우수한 성능과 토큰 소비 감소를 보여줍니다. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p><strong>RetroLLM은 검색 증강 생성(RAG) 시스템에 상당한 개선을 제공합니다.</strong> 기존 RAG 방식이 별도의 검색기와 과도한 입력 토큰으로 어려움을 겪었던 반면, RetroLLM은 검색과 생성 프로세스를 통합하여 효율성과 정확성을 향상시킵니다. 이러한 통합된 접근 방식은 공동 학습을 가능하게 하고, 미세 조정된 증거 검색을 허용하며, 입력 토큰 소비를 줄여 RAG 연구에 새로운 길을 열어줍니다. RetroLLM은 환각을 줄이는 동시에 사실에 기반한 출력을 생성할 수 있는 LLM의 잠재력을 보여줍니다. 또한, 계층적 FM-Index 및 전방탐색 제약 디코딩과 같은 혁신적인 기술은 추가 탐구를 위한 유망한 길을 제시하며, RAG 개발을 위한 새로운 방향을 제시합니다.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11919/x1.png alt></figure></p><blockquote><p>🔼 이 그림은 다양한 검색 증강 생성(RAG) 프레임워크를 비교합니다. (a) 기존 RAG는 문서 일치에 밀집 검색기를 사용하고, (b) 생성 RAG는 제약된 DocID 생성에 의존합니다. 두 가지 모두 검색된 문서 텍스트를 LLM에 입력하여 답변을 생성해야 합니다. (c) RetroLLM은 검색 및 생성을 단일 자동 회귀 디코딩 프로세스로 통합하여 FM-인덱스 제약 조건을 활용하여 세분화된 증거를 검색합니다.</p><details><summary>read the caption</summary>Figure 1: Comparison of retrieval-augmented generation frameworks. (a) Traditional RAG uses a dense retriever for document matching, while (b) generative RAG relies on constrained DocID generation. Both require feeding retrieved document text into the LLM for answer generation. (c) Our RetroLLM unifies retrieval and generation in a single auto-regressive decoding process, leveraging FM-Index constraints to retrieve fine-grained evidence.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>In-domain Datasets</th><th></th><th></th><th></th><th></th><th></th><th>Out-of-domain Datasets</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>NQ</td><td></td><td></td><td>TriviaQA</td><td></td><td></td><td>HotpotQA</td><td></td><td></td><td>PopQA</td><td></td><td></td><td>2WIKI</td><td></td></tr><tr><td></td><td>Acc</td><td>F1</td><td>Tok</td><td>Acc</td><td>F1</td><td>Tok</td><td>Acc</td><td>F1</td><td>Tok</td><td>Acc</td><td>F1</td><td>Tok</td><td>Acc</td><td>F1</td></tr><tr><td><strong><em>Direct Generation</em></strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llama2-7B</td><td>27.6</td><td>30.1</td><td>50</td><td>56.1</td><td>60.2</td><td>52</td><td>21.2</td><td>26.5</td><td>56</td><td>24.2</td><td>26.4</td><td>43</td><td>20.9</td><td>24.3</td></tr><tr><td>Mistral-7B</td><td>30.4</td><td>25.2</td><td>57</td><td>58.8</td><td>58.6</td><td>57</td><td>27.0</td><td>23.6</td><td>65</td><td>25.8</td><td>25.2</td><td>45</td><td>36.5</td><td>18.7</td></tr><tr><td>Qwen-7B</td><td>21.8</td><td>21.3</td><td>52</td><td>45.1</td><td>48.1</td><td>54</td><td>21.3</td><td>27.5</td><td>57</td><td>17.1</td><td>18.7</td><td>45</td><td>22.4</td><td>28.1</td></tr><tr><td>ChatGPT</td><td>-</td><td>-</td><td>-</td><td><span style=color:gray>77.0</span></td><td><span style=color:gray>52.9</span></td><td>-</td><td><span style=color:gray>33.8</span></td><td><span style=color:gray>24.0</span></td><td>-</td><td><span style=color:gray>26.6</span></td><td><span style=color:gray>13.2</span></td><td>-</td><td><span style=color:gray>38.0</span></td><td><span style=color:gray>21.3</span></td></tr><tr><td><strong><em>Retrieval-augmented Generation</em></strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Naive RAG</td><td><strong>52.4</strong></td><td>41.1</td><td>919</td><td>69.3</td><td>65.9</td><td><strong>915</strong></td><td><strong>37.8</strong></td><td>35.8</td><td><strong>960</strong></td><td>47.7</td><td>38.6</td><td>944</td><td><strong>38.7</strong></td><td>21.7</td></tr><tr><td>REPLUG</td><td>41.6</td><td>41.2</td><td><strong>903</strong></td><td>65.4</td><td>66.5</td><td>939</td><td>27.8</td><td>31.7</td><td>965</td><td>38.2</td><td>37.0</td><td><strong>921</strong></td><td>24.5</td><td>20.8</td></tr><tr><td>Self-RAG</td><td>41.8</td><td>45.2</td><td>1203</td><td>64.1</td><td>53.4</td><td>1267</td><td>32.1</td><td>29.6</td><td>1354</td><td>39.7</td><td>32.7</td><td>1236</td><td>30.3</td><td>25.7</td></tr><tr><td>IRCoT</td><td>49.6</td><td>45.9</td><td>1598</td><td>66.0</td><td>66.1</td><td>1715</td><td>37.3</td><td><strong>41.5</strong></td><td>1842</td><td><strong>59.8</strong></td><td><strong>45.6</strong></td><td>1667</td><td>29.4</td><td><strong>32.4</strong></td></tr><tr><td>Iter-RetGen</td><td>51.7</td><td><strong>48.4</strong></td><td>3002</td><td><strong>71.0</strong></td><td><strong>69.9</strong></td><td>2461</td><td>37.2</td><td>39.0</td><td>2545</td><td>51.7</td><td><strong>47.5</strong></td><td>2509</td><td>29.2</td><td>21.5</td></tr><tr><td>Adaptive-RAG</td><td>50.5</td><td>46.6</td><td>946</td><td>65.1</td><td>65.6</td><td>958</td><td>37.1</td><td>39.1</td><td>2080</td><td>58.3</td><td>40.4</td><td>1681</td><td>32.1</td><td>28.4</td></tr><tr><td><strong><em>Retrieval within Generation</em></strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>RetroLLM (Ours)</td><td><strong>61.6</strong></td><td><strong>49.8</strong></td><td><strong>302</strong></td><td><strong>74.3</strong></td><td><strong>72.8</strong></td><td><strong>287</strong></td><td><strong>61.9</strong></td><td><strong>47.2</strong></td><td><strong>607</strong></td><td><strong>65.7</strong></td><td>43.0</td><td><strong>355</strong></td><td><strong>48.9</strong></td><td><strong>36.2</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 단일 홉 및 다중 홉 QA 작업을 포함한 오픈 도메인 QA 데이터 세트에 대한 전반적인 성능을 보여줍니다. 최상의 결과는 굵게 표시되고 두 번째로 좋은 결과는 밑줄이 그어져 있습니다. 독점이 아닌 모델의 결과는 회색으로 표시됩니다.</p><details><summary>read the caption</summary>Table 1: Overall performance on open-domain QA datasets, including single-hop and multi-hop QA tasks. The best results are in bold and the second are underlined. Results from non-proprietary models are in gray color.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">LLM Hallucination<div id=llm-hallucination class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#llm-hallucination aria-label=Anchor>#</a></span></h4><p><strong>LLM 환각</strong>은 LLM이 <strong>사실과 다른 출력</strong>을 생성하는 현상을 말합니다. 이는 LLM의 <strong>학습 데이터 편향</strong>, <strong>맥락 이해 부족</strong>, <strong>추론 능력 한계</strong> 등 여러 요인이 복합적으로 작용하여 발생합니다. 환각은 LLM의 신뢰도를 떨어뜨리고, 잘못된 정보 확산으로 이어질 수 있어 심각한 문제입니다. 따라서 <strong>환각 완화</strong>는 LLM 연구의 핵심 과제입니다. 최근 연구들은 <strong>외부 지식 활용</strong>, <strong>출력 검증 메커니즘 도입</strong>, <strong>학습 데이터 개선</strong> 등 다양한 방식으로 환각 문제 해결을 시도하고 있습니다. 하지만 아직 완벽한 해결책은 없으며, <strong>지속적인 연구 개발</strong>이 필요합니다. LLM 환각은 단순한 기술적 문제를 넘어, <strong>정보 생태계</strong>와 <strong>사회 전반</strong>에 큰 영향을 미칠 수 있는 중요한 문제입니다.</p><h4 class="relative group">RetroLLM Framework<div id=retrollm-framework class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#retrollm-framework aria-label=Anchor>#</a></span></h4><p><strong>RetroLLM 프레임워크는 검색과 생성을 단일 프로세스로 통합</strong>하여 대규모 언어 모델(LLM)이 FM-Index 제약 조건을 사용하여 코퍼스에서 직접 증거를 생성할 수 있도록 합니다. 이러한 <strong>통합된 접근 방식은 별도의 검색기의 필요성을 없애고</strong> 입력 토큰의 중복성을 줄여 효율성을 향상시킵니다. 또한 검색 및 생성 작업의 <strong>공동 최적화를 가능하게 하여 전반적인 성능 향상</strong>에 기여합니다. RetroLLM은 <strong>계층적 FM-Index 제약 조건과 미래 지향적 제약 조건 디코딩 전략을 활용</strong>하여 증거 정확도를 더욱 향상시킵니다. 계층적 제약 조건은 관련 문서의 하위 집합을 식별하여 관련 없는 디코딩 공간을 줄이고 미래 지향적 디코딩은 미래 시퀀스의 관련성을 고려하여 증거 생성을 안내합니다. 이러한 혁신적인 기능을 통해 RetroLLM은 <strong>기존 RAG(검색 증강 생성) 방법과 복잡한 RAG 전략보다 뛰어난 성능</strong>을 달성하여 생성 검색의 새로운 시대를 열었습니다.</p><h4 class="relative group">Joint Optimization<div id=joint-optimization class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#joint-optimization aria-label=Anchor>#</a></span></h4><p><strong>RetroLLM의 핵심은 검색과 생성을 하나의 프로세스로 통합</strong>하여, 기존 RAG의 분리된 리트리버 운영 및 입력 토큰 증가 문제를 해결하고 <strong>joint optimization을 가능하게</strong> 하는 것입니다. 이러한 통합으로 인해 검색과 생성 간의 관계를 더 깊이 이해하고 전반적인 성능 향상을 도모합니다. 하지만 단순히 FM-Index를 적용하는 방식은 &lsquo;false pruning&rsquo; 문제를 야기할 수 있습니다. RetroLLM은 이를 완화하기 위해 hierarchical FM-Index constraints와 forward-looking constrained decoding 전략을 사용합니다. <strong>Hierarchical FM-Index는 단계적 검색 공간을 줄여줌</strong>으로써 효율적인 검색을 가능케 하고, <strong>Forward-looking constrained decoding은 미래 시퀀스의 관련성을 고려하여 정확도 향상</strong>에 기여합니다. 즉, RetroLLM은 joint optimization을 통해 검색과 생성을 효과적으로 결합하여 성능 및 효율성을 향상시킵니다.</p><h4 class="relative group">Constrained Decoding<div id=constrained-decoding class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#constrained-decoding aria-label=Anchor>#</a></span></h4><p><strong>제약된 디코딩</strong>은 외부 지식을 활용하여 언어 모델의 생성 품질을 향상하는 데 중점을 둡니다. 이 기술은 사실성, 관련성 및 일관성을 보장하기 위해 미리 정의된 제약 조건 내에서 텍스트를 생성합니다. <strong>주요 이점</strong>으로는 환각 감소, 텍스트의 집중도 향상, 특정 기준 충족 등이 있습니다. 그러나 잘못된 가지치기, 즉 유효한 시퀀스가 너무 일찍 제거되는 문제가 발생할 수 있습니다. 이는 <strong>초기 접두사 선택의 과도한 다양성</strong>과 <strong>미래 시퀀스 관련성에 대한 인식 부족</strong>으로 인해 발생합니다. 이러한 문제를 해결하기 위해 <strong>접두사 선택 감소 및 미래 관련성 인식 향상</strong>과 같은 전략을 사용할 수 있습니다. 예를 들어 단서 생성을 사용하여 관련 문서의 하위 집합을 식별하여 접두사 선택을 줄이고 후속 디코딩을 안내할 수 있습니다. 또한 미래 창을 식별하고 점수를 매겨 모델이 더 관련성 높은 증거를 생성하고 잘못된 가지치기 문제를 완화하도록 할 수 있습니다.</p><h4 class="relative group">Evidence Accuracy<div id=evidence-accuracy class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#evidence-accuracy aria-label=Anchor>#</a></span></h4><p><strong>증거 정확도</strong>는 RAG에서 중요합니다. RetroLLM은 <strong>계층적 FM-Index</strong> 및 <strong>미래 예측 디코딩</strong>을 사용하여 이를 향상시킵니다. <strong>계층적 색인</strong>은 관련 문서의 하위 집합을 먼저 식별하여 잘못된 가지치기 문제를 줄입니다. 그런 다음 <strong>미래 예측 디코딩</strong>은 관련성 점수가 높은 미래 윈도우를 기반으로 증거 생성을 안내합니다. 이러한 전략은 정확한 증거 검색을 보장합니다.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11919/x2.png alt></figure></p><blockquote><p>🔼 이 그래프는 생성된 증거 시퀀스의 앞부분 n개 토큰과 쿼리 간의 관련성 점수를 보여줍니다. Corpus FM-Index 제약 조건을 사용하는 경우 처음 13개 토큰 내에서 관련성 점수가 급격히 감소하는 것을 관찰할 수 있는 반면, 관련 문서의 Doc FM-Index 제약 조건을 사용하는 경우 관련성 점수가 감소하지 않고 beam 크기에 따라 정확도가 향상됩니다.</p><details><summary>read the caption</summary>(a) Sequence Relevance</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11919/x3.png alt></figure></p><blockquote><p>🔼 이 그래프는 다양한 빔 크기에서 말뭉치 수준 FM-Index와 문서 수준 FM-Index를 사용한 제약 증강 생성에서의 전반적인 정확도를 비교합니다. 말뭉치 수준 제약은 특히 처음 몇 토큰 내에서 정확도가 크게 저하되는 반면, 문서 수준 제약은 이러한 저하를 완화하고 다양한 빔 크기에서 더 나은 정확도를 보여줍니다.</p><details><summary>read the caption</summary>(b) Overall Accuracy</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11919/x4.png alt></figure></p><blockquote><p>🔼 이 그림은 제한된 증거 생성에서 잘못된 가지치기 문제에 대한 실증적 연구 결과를 보여줍니다. 말뭉치 수준 FM-Index와 문서 수준 FM-Index 접근 방식을 비교하여 생성된 증거 시퀀스의 관련성 점수(bge-reranker-large 기준)가 자동 회귀 디코딩 프로세스 중에 어떻게 변하는지 보여줍니다. 레이블이 지정된 증거 시퀀스와 비교하여 말뭉치 FM-Index 제약 조건에서 접두사 관련성이 크게 감소하는 것을 알 수 있습니다. 특히 처음 13개 토큰 내에서 심각하게 감소합니다. FM-Index 제약 조건을 관련 문서로만 제한하면 이러한 저하가 크게 줄어들고 다양한 빔 크기에 걸쳐 증거 생성 정확도가 향상됩니다.</p><details><summary>read the caption</summary>Figure 2: Empirical Study on false pruning problem in constrained evidence generation, comparing corpus-level and document-level FM-Index approaches.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11919/x5.png alt></figure></p><blockquote><p>🔼 RetroLLM은 계층적이고 미래 지향적인 FM-Index 제약 생성 프로세스를 통해 세분화된 증거를 검색하는 프레임워크입니다. 생성 중에 모델은 현재 컨텍스트의 충분성을 기반으로 추가 증거를 생성할지 아니면 최종 답변을 제공할지 자율적으로 결정합니다. 그림에서 (a)는 RetroLLM의 전체 프로세스 개요, (b)는 계층적 FM-Index 제약 조건 구성, (c)는 미래 지향적 제약 증거 생성 방식을 보여줍니다.</p><details><summary>read the caption</summary>Figure 3: Overview of the RetroLLM Framework, which retrieves fine-grained evidence through a hierarchical, forward-looking FM-Index constrained generation process. During generation, the model autonomously determines whether to generate additional evidence or provide the final answer, based on the sufficiency of the current context.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11919/x6.png alt></figure></p><blockquote><p>🔼 이 그래프는 다양한 매개변수 크기를 가진 여러 기본 LLM에서 RetroLLM의 성능을 보여줍니다. x축은 LLM의 매개변수 크기(1B에서 14B까지)를 나타내고 y축은 NQ, TriviaQA, HotpotQA, PopQA 및 2WIKI의 5개 데이터 세트에 대한 평균 정확도를 나타냅니다. 이 그림은 Llama3, Qwen2.5 및 Mistral의 세 가지 LLM 시리즈를 비교합니다. 매개변수 크기가 증가함에 따라 RetroLLM의 성능이 꾸준히 향상되어 스케일링 법칙과 일치하는 것을 알 수 있습니다. 또한 서로 다른 모델(Mistral, Llama3, Qwen2.5) 간에 약간의 성능 차이가 있으며, Mistral은 일반적으로 Llama3보다 성능이 우수하고, Llama3은 Qwen2.5보다 성능이 우수합니다. 그럼에도 불구하고 모든 모델에서 RetroLLM의 효과가 확인되었으며, Qwen2.5-1.5B와 같은 소규모 모델조차도 상당한 성능(예: NQ에서 50.1% 정확도, TriviaQA에서 57.2% 정확도)을 달성했습니다. 이는 RetroLLM이 다양한 기본 모델 및 매개변수 크기에서 강력함을 보여줍니다.</p><details><summary>read the caption</summary>(a) Parameters vs. Accuracy</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11919/x7.png alt></figure></p><blockquote><p>🔼 이 그래프는 다양한 매개변수 크기의 기본 LLM을 사용하는 RetroLLM의 성능을 보여줍니다. 매개변수 크기가 증가함에 따라 RetroLLM의 성능이 꾸준히 향상되어 스케일링 법칙을 따릅니다. 또한 다양한 모델(Mistral, Llama3, Qwen2.5)에서 약간의 성능 차이가 있습니다. Mistral은 일반적으로 Llama3보다 성능이 우수하고 Llama3은 Qwen2.5보다 성능이 우수합니다. 그럼에도 불구하고 모든 모델은 RetroLLM의 효과를 확인합니다. 작은 모델(예: Qwen2.5-1.5B)도 상당한 성능(예: NQ에서 정확도 50.1%, TriviaQA에서 57.2%)을 달성하여 RetroLLM이 다양한 기본 모델과 매개변수 크기에 대해 강력함을 보여줍니다.</p><details><summary>read the caption</summary>(b) Parameters vs. F1</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.11919/x8.png alt></figure></p><blockquote><p>🔼 이 그림은 다양한 기본 LLM(Llama3, Qwen2.5, Mistral 시리즈)과 매개변수 크기(1B에서 14B까지)를 사용하여 RetroLLM의 성능을 비교합니다. 매개변수 크기가 증가함에 따라 RetroLLM의 성능이 향상되는 것을 보여주고, 다양한 모델 간의 약간의 성능 차이도 보여줍니다. 하지만 모든 모델에서 RetroLLM의 효과를 확인할 수 있습니다.</p><details><summary>read the caption</summary>Figure 4: Impact of performance with different base LLMs, reporting average performance on five datasets.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>Single-hop QA</th><th></th><th></th><th>Multi-hop QA</th><th></th><th></th></tr></thead><tbody><tr><td></td><td>R@1</td><td>R@5</td><td>Num</td><td>R@1</td><td>R@5</td><td>Num</td></tr><tr><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td></tr><tr><td>BM25</td><td>37.8</td><td>56.3</td><td>5</td><td>26.9</td><td>43.1</td><td>5</td></tr><tr><td>SPLADE-v3</td><td>50.6</td><td>69.7</td><td>5</td><td>27.5</td><td>42.9</td><td>5</td></tr><tr><td>E5</td><td>54.3</td><td><strong>74.3</strong></td><td>5</td><td>26.9</td><td>45.9</td><td>5</td></tr><tr><td>BGE</td><td>53.3</td><td>72.8</td><td>5</td><td>27.4</td><td>46.8</td><td>5</td></tr><tr><td>Naive Constrain</td><td>15.7</td><td>31.7</td><td>5</td><td>10.6</td><td>20.3</td><td>5</td></tr><tr><td>RetroLLM</td><td><strong>56.6</strong></td><td>67.9</td><td><strong>3.29</strong></td><td><strong>29.3</strong></td><td><strong>49.6</strong></td><td><strong>4.24</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 RetroLLM의 검색 성능을 희소, 밀집, 생성 검색 방법과 비교하여 분석한 내용입니다. 세 가지 단일 홉 및 두 가지 다중 홉 QA 데이터 세트에 대한 평균 성능을 보여줍니다. RetroLLM은 단일 홉 QA 작업에서 R@1 정확도가 우수하고 다중 홉 QA 작업에서도 다른 모든 방법보다 R@1과 R@5 모두에서 더 나은 정확도를 보입니다. 또한 RetroLLM은 검색된 구절의 평균 개수가 기준선보다 적어 검색 효율성이 더 높습니다.</p><details><summary>read the caption</summary>Table 2: Analysis of retrieval performance of RetroLLM, compared with sparse, dense, and generative retrieval methods. We report average performance on three single-hop and two multi-hop QA datasets.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>In-domain</th><th></th><th>Out-of-domain</th><th></th></tr></thead><tbody><tr><td></td><td>Acc</td><td>F1</td><td>Acc</td><td>F1</td></tr><tr><td>RetroLLM</td><td><strong>66.0</strong></td><td><strong>56.6</strong></td><td><strong>57.3</strong></td><td><strong>39.6</strong></td></tr><tr><td>w/o Future Window</td><td>44.3</td><td>43.2</td><td>40.9</td><td>33.8</td></tr><tr><td>w/o Clue Generation</td><td>60.6</td><td>52.1</td><td>56.4</td><td>38.1</td></tr><tr><td>w/o Clue Expansion</td><td>49.6</td><td>45.1</td><td>44.1</td><td>35.4</td></tr><tr><td>w/ Naive Constraints</td><td>27.2</td><td>28.0</td><td>21.8</td><td>20.7</td></tr><tr><td>w/o Constraints</td><td>41.6</td><td>43.0</td><td>31.6</td><td>28.1</td></tr></tbody></table></table></figure><blockquote><p>🔼 RetroLLM 성능에 대한 ablation study 결과를 보여주는 표입니다. 표에는 in-domain 데이터셋과 out-of-domain 데이터셋에 대한 성능 지표가 포함되어 있습니다. 또한 future window, clue 생성, clue 확장과 같은 RetroLLM의 각 구성 요소가 미치는 영향을 평가하여 이러한 구성 요소의 중요성을 보여줍니다. 마지막으로 순수하게 제약 조건 기반의 생성 검색만 사용했을 때의 성능 저하를 보여줍니다.</p><details><summary>read the caption</summary>Table 3: Ablation Studies of RetroLLM, considering in-domain and out-of-domain performance.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>Latency (ms)</th><th></th><th></th><th>Token Num</th><th></th><th></th><th># P</th></tr></thead><tbody><tr><td></td><td>Retr</td><td>Gen</td><td>Total</td><td>In</td><td>Out</td><td>Total</td><td>F1</td></tr><tr><td>Naive RAG</td><td><strong>54</strong></td><td><strong>528</strong></td><td><strong>582</strong></td><td>902</td><td><strong>17</strong></td><td>919</td><td>41.1</td></tr><tr><td>SelfRAG</td><td>89</td><td>3180</td><td>3269</td><td>1096</td><td>107</td><td>1203</td><td>45.2</td></tr><tr><td>Iter-RetGen</td><td>274</td><td>2058</td><td>2332</td><td>2963</td><td>39</td><td>3002</td><td>48.4</td></tr><tr><td>IRCoT</td><td>83</td><td>1759</td><td>1842</td><td>1535</td><td>63</td><td>1598</td><td>46.6</td></tr><tr><td>RetroLLM</td><td>-</td><td>-</td><td>786</td><td><strong>18</strong></td><td>297</td><td><strong>315</strong></td><td><strong>49.8</strong></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 RetroLLM의 효율성 분석 결과를 보여줍니다. 쿼리 지연 시간, 토큰 수 및 성능을 다른 RAG 메서드들과 비교하여 RetroLLM의 효율성을 평가합니다.</p><details><summary>read the caption</summary>Table 4: Efficiency Analysis of RetroLLM, comparing query latency, number of tokens and performance (# P).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Task</th><th>Dataset</th><th># Train</th><th># Test</th></tr></thead><tbody><tr><td>Single-hop QA</td><td>NQ</td><td>79,168</td><td>3,610</td></tr><tr><td>Single-hop QA</td><td>TriviaQA</td><td>78,785</td><td>11,313</td></tr><tr><td>Single-hop QA</td><td>PopQA</td><td>/</td><td>14,267</td></tr><tr><td>Multi-hop QA</td><td>HotpotQA</td><td>90,447</td><td>7,405</td></tr><tr><td>Multi-hop QA</td><td>2WIKI</td><td>/</td><td>12,576</td></tr><tr><td><strong>Retrieval Corpus</strong></td><td><strong># Passages</strong></td><td><strong># Documents</strong></td><td></td></tr><tr><td>Wikipedia</td><td>21,015,324</td><td>3,232,907</td><td></td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 논문에서 사용된 데이터셋과 검색 코퍼스에 대한 자세한 통계를 제공합니다. 단일 홉 및 다중 홉 추론 능력을 평가하기 위해 다양한 질문 답변(QA) 데이터셋이 사용되었습니다. 단일 홉 QA의 경우, Natural Questions(NQ), TriviaQA, PopQA 데이터셋을 사용하고, 다중 홉 QA의 경우, HotpotQA 및 2WikiMultiHopQA(2WIKI) 데이터셋을 사용합니다. 검색 코퍼스로는 21,015,324개의 구절과 3,232,907개의 문서로 구성된 Wikipedia 데이터셋을 사용합니다.</p><details><summary>read the caption</summary>Table 5: Detailed statistics of datasets and retrieval corpus utilized in our experiments.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption>| Method | NQ | | | TriviaQA | | | HotpotQA | | | PopQA | | | 2WIKI | | |
|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|
| <strong>In-domain Datasets</strong> | <strong>R@1</strong> | <strong>R@5</strong> | <strong>Num</strong> | <strong>R@1</strong> | <strong>R@5</strong> | <strong>Num</strong> | <strong>R@1</strong> | <strong>R@5</strong> | <strong>Num</strong> | <strong>R@1</strong> | <strong>R@5</strong> | <strong>Num</strong> | <strong>R@1</strong> | <strong>R@5</strong> | <strong>Num</strong> |
| <strong>Out-of-domain Datasets</strong> | | | | | | | | | | | | | | | |
| <em>Sparse Retrieval</em> | | | | | | | | | | | | | | | |
| BM25 | 24.1 | 46.2 | 5 | 49.6 | 68.5 | 5 | 31.2 | 48.7 | 5 | 39.6 | 54.3 | 5 | 22.6 | 37.5 | 5 |
| SPLADE-v3 | 45.4 | 68.0 | 5 | 58.8 | 75.9 | 5 | 32.9 | 45.3 | 5 | 47.6 | 65.2 | 5 | 22.2 | 40.6 | 5 |
| <em>Dense Retrieval</em> | | | | | | | | | | | | | | | |
| E5 | <strong>55.7</strong> | <strong>77.3</strong> | 5 | <strong>61.6</strong> | <strong>77.8</strong> | 5 | 32.3 | 52.0 | 5 | 51.7 | <strong>70.9</strong> | 5 | 21.6 | 39.8 | 5 |
| BGE | 50.3 | 73.6 | 5 | 58.7 | 75.1 | 5 | 33.7 | 54.7 | 5 | 50.8 | 69.6 | 5 | 21.1 | 38.9 | 5 |
| <em>Generative Retrieval</em> | | | | | | | | | | | | | | | |
| Naive Constrain | 13.1 | 26.9 | 5 | 23.0 | 46.9 | 5 | 11.8 | 21.6 | 5 | 10.9 | 21.2 | 5 | 9.4 | 19.0 | 5 |
| RetroLLM | 51.6 | 62.5 | <strong>3.20</strong> | 61.1 | 71.0 | <strong>2.80</strong> | <strong>35.6</strong> | <strong>57.3</strong> | <strong>3.86</strong> | <strong>57.0</strong> | 70.1 | <strong>4.07</strong> | <strong>23.0</strong> | <strong>41.8</strong> | <strong>4.40</strong> |</table></figure><blockquote><p>🔼 이 표는 희소, 밀집 및 생성 검색 방식을 비교하여 5개의 개방형 도메인 QA 데이터 세트에 대한 자세한 검색 성능을 보여줍니다. 단일 홉 및 다중 홉 QA 작업 모두에서 RetroLLM이 어떻게 다른 기준선과 비교하여 성능이 우수한지 강조 표시합니다. 또한 순진한 제약 빔 검색 방법이 직면한 잘못된 가지치기 문제를 강조 표시합니다.</p><details><summary>read the caption</summary>Table 6: Detailed retrieval performance on five open-domain QA datasets, comparing sparse, dense, and generative approaches. The best results are highlighted in Bold.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left>Base Model</th><th style=text-align:center>In-domain Datasets</th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center>Out-of-domain Datasets</th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th><th></th></tr></thead><tbody><tr><td style=text-align:left></td><td style=text-align:center>NQ</td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>TriviaQA</td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>HotpotQA</td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>PopQA</td><td style=text-align:center></td></tr><tr><td style=text-align:left></td><td style=text-align:center>Acc</td><td style=text-align:center>F1</td><td style=text-align:center>Tok</td><td style=text-align:center>Acc</td><td style=text-align:center>F1</td><td style=text-align:center>Tok</td><td style=text-align:center>Acc</td><td style=text-align:center>F1</td><td style=text-align:center>Tok</td><td style=text-align:center>Acc</td><td style=text-align:center>F1</td></tr><tr><td style=text-align:left><strong><em>Llama3 Series</em></strong></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Llama3.2-1B</td><td style=text-align:center>54.4</td><td style=text-align:center>35.8</td><td style=text-align:center>260</td><td style=text-align:center>64.4</td><td style=text-align:center>52.9</td><td style=text-align:center>288</td><td style=text-align:center>58.8</td><td style=text-align:center>33.5</td><td style=text-align:center>573</td><td style=text-align:center>63.3</td><td style=text-align:center>32.9</td></tr><tr><td style=text-align:left>Llama3.2-3B</td><td style=text-align:center>58.9</td><td style=text-align:center>45.4</td><td style=text-align:center>278</td><td style=text-align:center>67.8</td><td style=text-align:center>62.1</td><td style=text-align:center>267</td><td style=text-align:center>61.3</td><td style=text-align:center>37.8</td><td style=text-align:center>609</td><td style=text-align:center>64.7</td><td style=text-align:center>40.4</td></tr><tr><td style=text-align:left>Llama3-8B</td><td style=text-align:center>59.2</td><td style=text-align:center>46.4</td><td style=text-align:center>306</td><td style=text-align:center>72.7</td><td style=text-align:center><strong>69.3</strong></td><td style=text-align:center>256</td><td style=text-align:center>62.2</td><td style=text-align:center><strong>47.4</strong></td><td style=text-align:center>575</td><td style=text-align:center>65.2</td><td style=text-align:center>41.4</td></tr><tr><td style=text-align:left><strong><em>Qwen2.5 Series</em></strong></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Qwen2.5-1.5B</td><td style=text-align:center>50.1</td><td style=text-align:center>34.3</td><td style=text-align:center><strong>200</strong></td><td style=text-align:center>57.2</td><td style=text-align:center>51.2</td><td style=text-align:center><strong>170</strong></td><td style=text-align:center>57.0</td><td style=text-align:center>32.6</td><td style=text-align:center><strong>539</strong></td><td style=text-align:center>59.5</td><td style=text-align:center>32.6</td></tr><tr><td style=text-align:left>Qwen2.5-3B</td><td style=text-align:center>52.1</td><td style=text-align:center>36.8</td><td style=text-align:center>236</td><td style=text-align:center>61.4</td><td style=text-align:center>56.3</td><td style=text-align:center>212</td><td style=text-align:center>60.6</td><td style=text-align:center>34.1</td><td style=text-align:center>628</td><td style=text-align:center>64.0</td><td style=text-align:center>34.8</td></tr><tr><td style=text-align:left>Qwen2.5-7B</td><td style=text-align:center>54.9</td><td style=text-align:center>42.3</td><td style=text-align:center>230</td><td style=text-align:center>64.5</td><td style=text-align:center>62.4</td><td style=text-align:center>196</td><td style=text-align:center>61.9</td><td style=text-align:center>42.0</td><td style=text-align:center>549</td><td style=text-align:center>62.8</td><td style=text-align:center>37.1</td></tr><tr><td style=text-align:left>Qwen2.5-14B</td><td style=text-align:center>58.6</td><td style=text-align:center><strong>50.6</strong></td><td style=text-align:center>225</td><td style=text-align:center><strong>72.8</strong></td><td style=text-align:center>69.5</td><td style=text-align:center>186</td><td style=text-align:center><strong>62.6</strong></td><td style=text-align:center>45.9</td><td style=text-align:center>568</td><td style=text-align:center>64.3</td><td style=text-align:center>40.8</td></tr><tr><td style=text-align:left><strong><em>Mistral Series</em></strong></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Mistral-7B</td><td style=text-align:center><strong>61.6</strong></td><td style=text-align:center>49.8</td><td style=text-align:center>302</td><td style=text-align:center>74.3</td><td style=text-align:center>72.8</td><td style=text-align:center>287</td><td style=text-align:center>61.9</td><td style=text-align:center>47.2</td><td style=text-align:center>607</td><td style=text-align:center><strong>65.7</strong></td><td style=text-align:center><strong>43.0</strong></td></tr><tr><td style=text-align:left>2WIKI</td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left></td><td style=text-align:center>44.5</td><td style=text-align:center>28.5</td><td style=text-align:center>583</td><td style=text-align:center>47.3</td><td style=text-align:center>32.2</td><td style=text-align:center>632</td><td style=text-align:center>48.7</td><td style=text-align:center>36.1</td><td style=text-align:center>668</td><td style=text-align:center>47.5</td><td style=text-align:center>26.3</td></tr><tr><td style=text-align:left></td><td style=text-align:center>48.1</td><td style=text-align:center>30.6</td><td style=text-align:center>694</td><td style=text-align:center>48.7</td><td style=text-align:center>32.5</td><td style=text-align:center>634</td><td style=text-align:center><strong>51.3</strong></td><td style=text-align:center><strong>36.9</strong></td><td style=text-align:center>687</td><td style=text-align:center>48.9</td><td style=text-align:center>36.2</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 다양한 기본 LLM을 사용한 RetroLLM의 성능 비교를 보여줍니다. Llama3 시리즈, Qwen-2.5 시리즈, Mistral 시리즈와 같이 매개변수 크기가 1B에서 14B까지인 다양한 LLM을 사용하여 실험을 진행했습니다. 모든 기본 모델은 instruction-tuned 버전을 사용했습니다. RetroLLM은 다양한 기본 모델과 매개변수 크기에서 강력한 성능을 보여줍니다. 매개변수 크기가 증가함에 따라 RetroLLM의 성능이 꾸준히 향상됩니다. 또한 Mistral, Llama3, Qwen2.5와 같은 다양한 모델 간에 약간의 성능 차이가 있습니다. 하지만 모든 모델에서 RetroLLM의 효과가 확인되었으며, 작은 모델(예: Qwen2.5-1.5B)도 상당한 성능을 달성합니다.</p><details><summary>read the caption</summary>Table 7: Detailed performance comparison of RetroLLM using various base models, including the Llama3 series, Qwen-2.5 series, and Mistral series, with parameter sizes ranging from 1B to 14B. All base models we used are the instruction-tuned versions. The best results are highlighted in Bold.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th># Num</th><th>NQ</th><th>TriviaQA</th><th>HotpotQA</th><th>PopQA</th><th>2WIKI</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td><strong>In-domain Datasets</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>Acc</td><td>F1</td><td>Acc</td><td>F1</td><td>Acc</td><td>F1</td><td>Acc</td><td>F1</td><td>Acc</td></tr><tr><td><strong>Out-of-domain Datasets</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>1</strong></td><td>42.2</td><td>40.5</td><td>59.3</td><td>61.6</td><td>50.6</td><td>44.2</td><td>43.9</td><td>40.9</td><td>35.1</td></tr><tr><td><strong>2</strong></td><td>50.6</td><td>42.3</td><td>66.3</td><td>65.9</td><td>59.8</td><td>43.8</td><td>52.8</td><td>45.9</td><td>39.8</td></tr><tr><td><strong>3</strong></td><td>54.4</td><td>42.5</td><td>69.3</td><td>67.2</td><td>61.9</td><td>43.0</td><td>55.7</td><td>45.5</td><td>42.1</td></tr><tr><td><strong>4</strong></td><td>56.7</td><td>43.1</td><td>70.9</td><td>67.6</td><td>64.6</td><td>41.0</td><td>57.7</td><td>45.7</td><td>43.9</td></tr><tr><td><strong>5</strong></td><td>61.5</td><td>49.4</td><td>74.6</td><td>72.9</td><td>66.8</td><td>43.0</td><td>59.4</td><td>46.8</td><td>45.9</td></tr><tr><td><strong>6</strong></td><td>61.7</td><td>49.5</td><td>74.6</td><td>73.0</td><td>67.4</td><td>42.8</td><td>60.1</td><td>47.1</td><td>47.9</td></tr><tr><td><strong>7</strong></td><td>61.7</td><td>49.5</td><td>74.6</td><td>72.9</td><td>67.6</td><td>42.5</td><td>60.8</td><td>47.0</td><td>48.4</td></tr><tr><td><strong>8</strong></td><td>61.7</td><td>49.5</td><td>74.6</td><td>72.9</td><td>68.0</td><td>42.7</td><td>61.2</td><td>46.9</td><td>48.6</td></tr><tr><td><strong>9</strong></td><td>61.7</td><td>49.5</td><td>74.6</td><td>72.9</td><td>68.0</td><td>42.7</td><td>61.6</td><td>47.1</td><td>48.7</td></tr><tr><td><strong>10</strong></td><td>61.7</td><td>49.5</td><td>74.6</td><td>72.9</td><td>68.5</td><td>42.7</td><td>61.9</td><td>47.1</td><td>48.9</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 생성된 근거의 최대 개수를 1에서 10까지 다양하게 변경하면서 RetroLLM의 성능에 미치는 영향을 보여줍니다. 단일 홉 질의응답의 경우, 검색되는 근거가 최대 5개까지 증가함에 따라 성능이 향상되는 경향이 있지만, 다중 홉 질의응답의 경우 근거가 6개를 넘어가면 성능 향상이 제한적입니다. 이는 다중 홉 질의응답의 경우, 너무 많은 근거는 유용한 정보와 함께 방해가 되는 정보를 가져올 수 있어 추가적인 근거가 오히려 성능 향상에 도움이 되지 않을 수 있음을 시사합니다.</p><details><summary>read the caption</summary>Table 8: Detailed performance with different number of generated evidence.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Dataset</th><th>Question</th><th>Labeled Answer</th><th>Model Input</th><th>Model Output</th></tr></thead><tbody><tr><td>NQ Dataset</td><td>when does the movie the star come out?</td><td>[&ldquo;November 17, 2017&rdquo;]</td><td>Question: when does the movie the star come out?</td><td></td></tr><tr><td>Your Response:</td><td>&lt;</td><td>clue</td><td>> The Star &lt;</td><td>/clue</td></tr><tr><td>TriviaQA Dataset</td><td>Who was the man behind The Chipmunks?</td><td>[&ldquo;David Seville&rdquo;]</td><td>Question: Who was the man behind The Chipmunks?</td><td></td></tr><tr><td>Your Response:</td><td>&lt;</td><td>clue</td><td>> The Chipmunks &lt;</td><td>/clue</td></tr><tr><td>PopQA Dataset</td><td>What is Carsten Carlsen’s occupation?</td><td>[&ldquo;pianist&rdquo;, &ldquo;composer&rdquo;]</td><td>Question: What is Carsten Carlsen’s occupation?</td><td></td></tr><tr><td>Your Response:</td><td>&lt;</td><td>clue</td><td>> Carlsen &lt;</td><td>/clue</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 단일 홉 질문 답변 데이터셋에 대한 RetroLLM의 예시를 보여줍니다. 단서 생성 및 증거 생성 단계에 사용된 특수 토큰은 각각 주황색 상자와 파란색 상자로 강조 표시되어 있습니다. 생성된 모든 증거는 Wikipedia에서 가져온 것입니다.</p><details><summary>read the caption</summary>Table 9: Examples from RetroLLM on single-hop QA datasets, with special tokens used in the clue and evidence generation stages highlighted in orange box and blue box, respectively. All generated evidence is from Wikipedia.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Example #1 from HotpotQA Dataset</th><th></th></tr></thead><tbody><tr><td><strong>Question:</strong></td><td>Which American audio engineer and clandestine chemist, who was a key figure in the San Francisco Bay Area hippie movement during the 1960s recorded the album &ldquo;Old and in the Way?&rdquo;</td></tr><tr><td><strong>Labeled Answer:</strong></td><td>[&ldquo;Owsley Stanley&rdquo;]</td></tr><tr><td><strong>Results by RetroLLM</strong></td><td></td></tr><tr><td><strong>Model Input:</strong></td><td>Question: Which American audio engineer and clandestine chemist, who was a key figure in the San Francisco Bay Area hippie movement during the 1960s recorded the album &ldquo;Old and in the Way?&rdquo;</td></tr><tr><td><strong>Model Output:</strong></td><td>&lt;</td></tr><tr><td><strong>Example #2 from 2WikiMultiHopQA Dataset</strong></td><td></td></tr><tr><td><strong>Question:</strong></td><td>Where was the director of film Ronnie Rocket born?</td></tr><tr><td><strong>Labeled Answer:</strong></td><td>[&ldquo;Missoula&rdquo;, &ldquo;Missoula, Montana&rdquo;]</td></tr><tr><td><strong>Results by RetroLLM</strong></td><td></td></tr><tr><td><strong>Model Input:</strong></td><td>Question: Where was the director of film Ronnie Rocket born?</td></tr><tr><td><strong>Model Output:</strong></td><td>&lt;</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 RetroLLM이 다중 홉 질의응답 데이터셋에서 생성한 예시를 보여주며, 단서 생성과 증거 생성 단계에 사용된 특수 토큰은 각각 주황색 상자와 파란색 상자로 강조 표시되어 있습니다. RetroLLM은 단서를 생성하여 관련 문서의 하위 집합을 식별한 다음, 이 하위 집합 내에서 순방향 탐색 제약 증거 생성을 수행합니다. 생성된 모든 증거는 Wikipedia에서 가져온 것입니다.</p><details><summary>read the caption</summary>Table 10: Examples from RetroLLM on multi-hop QA datasets, with special tokens used in the clue and evidence generation stages highlighted in orange box and blue box, respectively. All generated evidence is from Wikipedia.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Example #1 from NQ Dataset</th><th></th></tr></thead><tbody><tr><td>Question:</td><td>who got the first nobel prize in physics?</td></tr><tr><td>Labeled Answer:</td><td>[&ldquo;Wilhelm Conrad Röntgen&rdquo;]</td></tr><tr><td>Results by RetroLLM</td><td></td></tr><tr><td>Model Input:</td><td>Question: who got the first nobel prize in physics?</td></tr><tr><td>Model Output:</td><td>&lt;</td></tr><tr><td>Results by Naive Constrained Beam Search</td><td></td></tr><tr><td>Model Input:</td><td>Question: who got the first nobel prize in physics?</td></tr><tr><td>Model Output (beam_size = 5):</td><td></td></tr><tr><td>Beam 1:</td><td>&lt;</td></tr><tr><td>Beam 2:</td><td>&lt;</td></tr><tr><td>Beam 3:</td><td>&lt;</td></tr><tr><td>Beam 4:</td><td>&lt;</td></tr><tr><td>Beam 5:</td><td>&lt;</td></tr></tbody></table></table></figure><blockquote><p>🔼 RetroLLM과 Naive 제약 빔 검색 방법의 출력을 비교한 예시입니다. 단서 및 증거 생성 단계에서 사용된 특수 토큰은 각각 주황색 상자와 파란색 상자로 강조 표시됩니다. 녹색으로 표시된 내용은 정답(또는 부분적으로 정답)을 나타내고 빨간색으로 표시된 내용은 오답을 나타냅니다. 모든 생성된 증거는 Wikipedia에서 가져온 것입니다. 이 표는 Naive 제약 빔 검색의 잘못된 가지치기 문제점과 RetroLLM이 이 문제를 해결하는 방법을 보여주는 사례 연구 역할을 합니다.</p><details><summary>read the caption</summary>Table 11: An example comparing outputs from RetroLLM and the naive constrained beam search method. Special tokens used during the clue and evidence generation stages are highlighted in orange boxes and blue boxes, respectively. Content colored in green indicates correct (or partially correct) answers, whereas content colored in red indicates incorrect answers. All generated evidence is from Wikipedia.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left>Example #2 from TriviaQA Dataset</th><th style=text-align:left></th></tr></thead><tbody><tr><td style=text-align:left><strong>Question:</strong></td><td style=text-align:left>Who was the man behind The Chipmunks?</td></tr><tr><td style=text-align:left><strong>Labeled Answer:</strong></td><td style=text-align:left>[&ldquo;David Seville&rdquo;]</td></tr><tr><td style=text-align:left><strong>Results by RetroLLM</strong></td><td style=text-align:left></td></tr><tr><td style=text-align:left><strong>Model Input:</strong></td><td style=text-align:left>Question: Who was the man behind The Chipmunks?</td></tr><tr><td style=text-align:left><strong>Model Output:</strong></td><td style=text-align:left>&lt;</td></tr><tr><td style=text-align:left><strong>Results by Naive Constrained Beam Search</strong></td><td style=text-align:left></td></tr><tr><td style=text-align:left><strong>Model Input:</strong></td><td style=text-align:left>Question: Who was the man behind The Chipmunks?</td></tr><tr><td style=text-align:left><strong>Model Output (beam_size = 5):</strong></td><td style=text-align:left></td></tr><tr><td style=text-align:left><strong>Beam 1:</strong></td><td style=text-align:left>&lt;</td></tr><tr><td style=text-align:left><strong>Beam 2:</strong></td><td style=text-align:left>&lt;</td></tr><tr><td style=text-align:left><strong>Beam 3:</strong></td><td style=text-align:left>&lt;</td></tr><tr><td style=text-align:left><strong>Beam 4:</strong></td><td style=text-align:left>&lt;</td></tr><tr><td style=text-align:left><strong>Beam 5:</strong></td><td style=text-align:left>&lt;</td></tr></tbody></table></table></figure><blockquote><p>🔼 RetroLLM과 단순 제약 빔 검색 방법의 출력을 비교한 예시입니다. 단서 및 증거 생성 단계에 사용된 특수 토큰은 각각 주황색 상자와 파란색 상자로 강조 표시되어 있습니다. 녹색으로 표시된 내용은 정답을, 빨간색으로 표시된 내용은 오답을 나타냅니다. 모든 생성된 증거는 Wikipedia에서 가져온 것입니다.</p><details><summary>read the caption</summary>Table 12: An example comparing outputs from RetroLLM and the naive constrained beam search method. Special tokens used during the clue and evidence generation stages are highlighted in orange boxes and blue boxes, respectively. Content colored in green indicates correct answers, whereas content colored in red indicates incorrect answers. All generated evidence is from Wikipedia.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-ec0f34d938528d5bf264805593ced127 class=gallery><img src=paper_images/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/&amp;title=RetroLLM:%20Empowering%20Large%20Language%20Models%20to%20Retrieve%20Fine-grained%20Evidence%20within%20Generation" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/&amp;text=RetroLLM:%20Empowering%20Large%20Language%20Models%20to%20Retrieve%20Fine-grained%20Evidence%20within%20Generation" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/&amp;subject=RetroLLM:%20Empowering%20Large%20Language%20Models%20to%20Retrieve%20Fine-grained%20Evidence%20within%20Generation" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.11919/index.md",oid_likes="likes_paper-reviews/2412.11919/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.12094/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-16T00:00:00+00:00>16 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.11423/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion Models</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-16T00:00:00+00:00>16 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>