{"references": [{"fullname_first_author": "Simone Tedeschi", "paper_title": "ALERT: A comprehensive benchmark for assessing large language models' safety through red teaming", "publication_date": "2024-04-08", "reason": "This paper introduces ALERT, a crucial benchmark for evaluating LLM safety that M-ALERT expands upon for multilingual evaluation."}, {"fullname_first_author": "Felix Friedrich", "paper_title": "Multilingual text-to-image generation magnifies gender stereotypes and prompt engineering may not help you", "publication_date": "2024-01-16", "reason": "This paper highlights the importance of multilingual safety analysis, motivating the creation of a multilingual safety benchmark like M-ALERT."}, {"fullname_first_author": "Devansh Jain", "paper_title": "PolygloToxicityPrompts: Multilingual evaluation of neural toxic degeneration in large language models", "publication_date": "2024-05-09", "reason": "This paper emphasizes the need for cross-lingual safety evaluations, a gap that M-ALERT addresses by providing a multilingual benchmark."}, {"fullname_first_author": "Rishi Bommasani", "paper_title": "On the opportunities and risks of foundation models", "publication_date": "2021-08-07", "reason": "This paper provides a comprehensive overview of the opportunities and risks of foundation models, including safety concerns that motivate research like the development of M-ALERT."}, {"fullname_first_author": "Emily M. Bender", "paper_title": "On the dangers of stochastic parrots: Can language models be too big?", "publication_date": "2021-00-00", "reason": "This foundational paper highlights ethical concerns and potential risks associated with LLMs, underscoring the need for safety evaluations and benchmarks like M-ALERT."}]}