{"references": [{"fullname_first_author": "Alex Graves", "paper_title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "publication_date": "2006-01-01", "reason": "This paper is foundational for sequence modeling using recurrent neural networks, a concept expanded upon in the current research."}, {"fullname_first_author": "Alec Radford", "paper_title": "Robust speech recognition via large-scale weak supervision", "publication_date": "2022-01-01", "reason": "This paper introduced a state-of-the-art speech recognition model, Whisper, which this paper aims to improve on using a different architecture."}, {"fullname_first_author": "Albert Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "publication_date": "2022-01-01", "reason": "This paper introduced the core concept of structured state-space models (SSMs), which forms the basis for the Mamba architecture used in this paper."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2024-01-01", "reason": "This paper details the Mamba architecture, the main innovation in this paper, providing efficient sequence modeling with linear complexity."}, {"fullname_first_author": "Guoguo Chen", "paper_title": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio", "publication_date": "2021-01-01", "reason": "This paper describes a large-scale speech dataset used to benchmark the performance of Samba-ASR, showcasing its effectiveness on a challenging dataset."}]}