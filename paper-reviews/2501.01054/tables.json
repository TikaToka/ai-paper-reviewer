[{"content": "| Method | Llama3-8B | Llama3-70B | GPT-3.5 | GPT-4o-m |\n|---|---|---|---|---|\n| **HumanEval Plus** |  |  |  |  |\n| Vanilla | 53.58 | 73.74 | 67.83 | 82.96 |\n| Grading RM | 62.20<sub> +8.62</sub> | 75.00<sub> +1.26</sub> | 70.12<sub> +2.29</sub> | 83.50<sub> +0.54</sub> |\n| MBR-Exec | 60.30<sub> +6.72</sub> | 75.80<sub> +2.06</sub> | 70.60<sub> +2.77</sub> | 85.20<sub> +2.24</sub> |\n| CodeT | 65.30<sub> +11.72</sub> | 76.20<sub> +2.46</sub> | 73.89<sub> +6.06</sub> | 85.30<sub> +2.34</sub> |\n| MPSC | 59.72<sub> +6.14</sub> | 75.51<sub> +1.77</sub> | 72.76<sub> +4.93</sub> | 84.82<sub> +1.86</sub> |\n| Llama3.1-70B | **72.04**<sub><span style=\"font-weight:normal;\"> +18.46</span></sub> | <span style=\"text-decoration:underline;\">78.54</span><sub> +4.80</sub> | **79.76**<sub><span style=\"font-weight:normal;\"> +11.93</span></sub> | <span style=\"text-decoration:underline;\">85.45</span><sub> +2.49</sub> |\n| CodeRM-8B | <span style=\"text-decoration:underline;\">72.01</span><sub> +18.43</sub> | **78.69**<sub><span style=\"font-weight:normal;\"> +4.95</span></sub> | <span style=\"text-decoration:underline;\">78.01</span><sub> +10.18</sub> | **86.38**<sub><span style=\"font-weight:normal;\"> +3.42</span></sub> |\n| **MBPP Plus** |  |  |  |  |\n| Vanilla | 49.20 | 69.33 | 70.53 | 71.59 |\n| Grading RM | 48.40<sub> -0.80</sub> | 70.60<sub> +1.27</sub> | 66.67<sub> -3.86</sub> | 69.00<sub> -2.59</sub> |\n| MBR-Exec | 50.00<sub> +0.80</sub> | 69.80<sub> +0.47</sub> | 70.53<sub> +0.00</sub> | 72.30<sub> +0.71</sub> |\n| CodeT | 59.20<sub> +10.00</sub> | 69.90<sub> +0.57</sub> | 69.92<sub> -0.61</sub> | 73.40<sub> +1.81</sub> |\n| MPSC | 53.32<sub> +4.12</sub> | 70.91<sub> +1.58</sub> | 71.59<sub> +1.06</sub> | 73.20<sub> +1.61</sub> |\n| Llama3.1-70B | <span style=\"text-decoration:underline;\">65.26</span><sub> +16.06</sub> | <span style=\"text-decoration:underline;\">71.85</span><sub> +2.52</sub> | <span style=\"text-decoration:underline;\">75.72</span><sub> +5.19</sub> | <span style=\"text-decoration:underline;\">74.96</span><sub> +3.37</sub> |\n| CodeRM-8B | **66.71**<sub><span style=\"font-weight:normal;\"> +17.51</span></sub> | **72.44**<sub><span style=\"font-weight:normal;\"> +3.11</span></sub> | **75.96**<sub><span style=\"font-weight:normal;\"> +5.43</span></sub> | **75.20**<sub><span style=\"font-weight:normal;\"> +3.61</span></sub> |\n| **LiveCodeBench** |  |  |  |  |\n| Vanilla | 11.98 | 25.30 | 20.55 | 34.83 |\n| Grading RM | 13.10<sub> +1.12</sub> | 26.19<sub> +0.89</sub> | 20.83<sub> +0.28</sub> | 36.31<sub> +1.48</sub> |\n| MBR-Exec | 12.04<sub> +0.06</sub> | 25.37<sub> +0.07</sub> | 20.52<sub> -0.03</sub> | 34.83<sub> +0.00</sub> |\n| CodeT | 12.61<sub> +0.63</sub> | 25.89<sub> +0.59</sub> | 20.58<sub> +0.03</sub> | 35.13<sub> +0.30</sub> |\n| MPSC | 11.98<sub> +0.00</sub> | 25.30<sub> +0.00</sub> | 20.55<sub> +0.00</sub> | 34.83<sub> +0.00</sub> |\n| Llama3.1-70B | <span style=\"text-decoration:underline;\">13.28</span><sub> +1.30</sub> | **28.46**<sub><span style=\"font-weight:normal;\"> +3.16</span></sub> | **22.80**<sub><span style=\"font-weight:normal;\"> +2.25</span></sub> | <span style=\"text-decoration:underline;\">38.60</span><sub> +3.77</sub> |\n| CodeRM-8B | **15.21**<sub><span style=\"font-weight:normal;\"> +3.23</span></sub> | <span style=\"text-decoration:underline;\">27.73</span><sub> +2.43</sub> | <span style=\"text-decoration:underline;\">21.76</span><sub> +1.21</sub> | **39.20**<sub><span style=\"font-weight:normal;\"> +4.37</span></sub> |", "caption": "Table 1: \nThe main result of our approach and other baselines over three code generation benchmarks.\nGPT-4o-m stands for GPT-4o-mini.\nThe improvements are calculated between methods and vanilla.\nThe top two performances for each dataset and policy model are marked in bold and underlined.", "description": "\ubcf8 \ud45c\ub294 \uc138 \uac00\uc9c0 \ucf54\ub4dc \uc0dd\uc131 \ubca4\uce58\ub9c8\ud06c(HumanEval Plus, MBPP Plus, LiveCodeBench)\uc5d0\uc11c \uc81c\uc548\ub41c \ubc29\ubc95\uacfc \ub2e4\ub978 \uae30\uc900 \ubc29\ubc95\ub4e4\uc758 \uc8fc\uc694 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. GPT-4o-m\uc740 GPT-4o-mini\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.  \uac1c\uc120\uc728\uc740 \uae30\uc900 \ubc29\ubc95(Vanilla)\uacfc\uc758 \ube44\uad50\ub97c \ud1b5\ud574 \uacc4\uc0b0\ub418\uc5c8\uc73c\uba70, \uac01 \ub370\uc774\ud130\uc14b\uacfc \uc815\ucc45 \ubaa8\ub378\uc5d0 \ub300\ud574 \uc0c1\uc704 \ub450 \uac1c\uc758 \uc131\ub2a5\uc774 \uad75\uc740 \uae00\uc528\uc640 \ubc11\uc904\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. \uc2e4\ud5d8 \uacb0\uacfc"}, {"content": "| Model | Acc (<mo stretchy=\"false\">\u2191</mo>) | F1 (<mo stretchy=\"false\">\u2191</mo>) | FAR (<mo stretchy=\"false\">\u2193</mo>) | FRR (<mo stretchy=\"false\">\u2193</mo>) |\n|---|---|---|---|---|\n| Llama3.1-8B | 60.02 | 44.97 | 13.66 | 46.13 |\n| Llama3.1-70B | **73.65** | **70.15** | **11.10** | **34.51** |\n| CodeRM-8B (Ours) | <ins>69.64</ins> | <ins>63.63</ins> | <ins>11.17</ins> | <ins>38.55</ins> |\n|  |  |  |  |  |\n| Llama3.1-8B | 74.21 | 74.35 | 20.44 | 30.55 |\n| Llama3.1-70B | <ins>78.30</ins> | <ins>78.76</ins> | <ins>17.19</ins> | <ins>25.97</ins> |\n| CodeRM-8B (Ours) | **80.46** | **81.27** | **16.48** | **22.71** |", "caption": "Table 2: The quality of individual unit tests and the combination of multiple unit tests on HumanEval Plus, utilizing Llama3.1-8B as the policy model.\nThe top two performances are highlighted using bold and underlining.", "description": "\uc774 \ud45c\ub294 HumanEval Plus \ub370\uc774\ud130\uc14b\uc5d0\uc11c Llama3.1-8B \ubaa8\ub378\uc744 \uc815\ucc45 \ubaa8\ub378\ub85c \uc0ac\uc6a9\ud558\uc5ec \uac1c\ubcc4 \ub2e8\uc704 \ud14c\uc2a4\ud2b8\uc640 \ub2e4\uc218\uc758 \ub2e8\uc704 \ud14c\uc2a4\ud2b8 \uc870\ud569\uc758 \ud488\uc9c8\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e8\uc77c \ub2e8\uc704 \ud14c\uc2a4\ud2b8\uc640 \uc5ec\ub7ec \ub2e8\uc704 \ud14c\uc2a4\ud2b8\ub97c \uacb0\ud569\ud588\uc744 \ub54c\uc758 \uc815\ud655\ub3c4(Accuracy), F1 \uc810\uc218, \uac70\uc9d3 \uc591\uc131\ub960(FAR), \uac70\uc9d3 \uc74c\uc131\ub960(FRR)\uc744 \uce21\uc815\ud558\uc5ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uc0c1\uc704 \ub450 \uac1c\uc758 \uc131\ub2a5\uc740 \uad75\uc740 \uae00\uc528\uc640 \ubc11\uc904\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \ub2e8\uc704 \ud14c\uc2a4\ud2b8\uc758 \ud488\uc9c8\uacfc \ub2e4\uc218\uc758 \ud14c\uc2a4\ud2b8\ub97c \uacb0\ud569\ud558\ub294 \uc804\ub7b5\uc758 \ud6a8\uacfc\ub97c \uc815\ub7c9\uc801\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "4.3 Quality of Generated Unit Tests"}, {"content": "| Method | HumanEval+ | MBPP+ |\n|---|---|---|\n| zero-shot | 66.67 | 63.27 |\n| training wo / quality control | 69.71<sub>+3.04</sub> | 64.96<sub>+1.69</sub> |\n| training w / quality control | 71.09<sub>+4.42</sub> | 66.31<sub>+3.04</sub> |", "caption": "Table 3: The effects of synthetic data quality control.", "description": "\ubcf8 \ud45c\ub294 \ud569\uc131 \ub370\uc774\ud130\uc758 \ud488\uc9c8 \uad00\ub9ac\uac00 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud488\uc9c8 \uad00\ub9ac \uacfc\uc815\uc774 \uc5c6\ub294 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec, \ud488\uc9c8 \uad00\ub9ac \uacfc\uc815\uc774 \ud3ec\ud568\ub41c \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\uc640 F1 \uc810\uc218\uac00 \ud5a5\uc0c1\ub418\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 \uc591\uc131 \ub370\uc774\ud130\uc758 \ud488\uc9c8\uc774 \ub2e8\uc704 \ud14c\uc2a4\ud2b8 \uc0dd\uc131\uae30\uc758 \uc131\ub2a5\uc5d0 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud55c\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3.1 \ub2e8\uc704 \ud14c\uc2a4\ud2b8 \uc0dd\uc131\uae30"}, {"content": "| Hyperparameters | Value |\n|---|---| \n| Temperature | 0.8 |\n| Top P | 0.95 |\n| Frequency Penalty | 0 |\n| Presence Penalty | 0 |", "caption": "Table 4: The hyperparameters of LLMs for solution and unit test generation.", "description": "\uc774 \ud45c\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LLM\uc740 \ucf54\ub4dc \uc194\ub8e8\uc158\uacfc \ub2e8\uc704 \ud14c\uc2a4\ud2b8 \uc0dd\uc131\uc5d0 \uc0ac\uc6a9\ub418\uc5c8\uc73c\uba70, \uad6c\uccb4\uc801\uc73c\ub85c\ub294 temperature, top p, frequency penalty, presence penalty\uc758 \ub124 \uac00\uc9c0 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac12\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.  \uc774\ub294 LLM\uc758 \ucd9c\ub825 \ud655\ub960 \ubd84\ud3ec\ub97c \uc870\uc808\ud558\uc5ec \ub2e4\uc591\ud55c \uc885\ub958\uc758 \uacb0\uacfc\ub97c \uc0dd\uc131\ud558\ub294 \ub370 \ud65c\uc6a9\ub429\ub2c8\ub2e4.  \uac01 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\uc758 \uac12\uc740 \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ubbc0\ub85c, \uc774 \ud45c\ub294 \uc2e4\ud5d8 \ud658\uacbd \uc124\uc815\uc744 \uc774\ud574\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "3.1 Unit Test Generator"}]