[{"figure_path": "https://arxiv.org/html/2412.16153/x2.png", "caption": "Figure 1: Motivation and results of MotiF. (a) Example video frames and the corresponding motion heatmaps calculated from optical flow. In this example, 97%percent9797\\%97 % of the pixels are static while only 3%percent33\\%3 % has meaningful motion. (b) In standard TI2V training pipeline, the model may learn to over-rely on the conditional image to optimize the L2 loss. This issue has been identified in\u00a0[53] and termed as conditional image leakage. We propose MotiF to guide the model\u2019s learning to focus on regions with more motion via motion heatmap re-weighting. (c) Qualitative results comparing MotiF to the baseline on examples from our proposed TI2V Bench.", "description": "\ubcf8 \ub17c\ubb38\uc758 \uadf8\ub9bc 1\uc740 MotiF\uc758 \ub3d9\uc791 \uc6d0\ub9ac\uc640 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. (a)\ub294 \uc608\uc2dc \ube44\ub514\uc624 \ud504\ub808\uc784\uacfc \uad11\ud559 \ud750\ub984(optical flow)\uc744 \uae30\ubc18\uc73c\ub85c \uacc4\uc0b0\ub41c \ubaa8\uc158 \ud788\ud2b8\ub9f5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 97%\uc758 \ud53d\uc140\uc740 \uc815\uc9c0 \uc0c1\ud0dc\uc774\uace0, \ub2e8 3%\ub9cc \uc758\ubbf8\uc788\ub294 \uc6c0\uc9c1\uc784\uc744 \ubcf4\uc785\ub2c8\ub2e4. (b)\ub294 \ud45c\uc900 TI2V \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \ubaa8\ub378\uc774 L2 \uc190\uc2e4\uc744 \ucd5c\uc18c\ud654\ud558\uae30 \uc704\ud574 \uc870\uac74 \uc774\ubbf8\uc9c0(conditional image)\uc5d0 \uacfc\ub3c4\ud558\uac8c \uc758\uc874\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ubb38\uc81c\ub294 \uae30\uc874 \uc5f0\uad6c [53]\uc5d0\uc11c \uc870\uac74 \uc774\ubbf8\uc9c0 \ub204\ucd9c(conditional image leakage)\ub85c \uc815\uc758\ub418\uc5c8\uc2b5\ub2c8\ub2e4. MotiF\ub294 \ubaa8\uc158 \ud788\ud2b8\ub9f5 \uc7ac\uac00\uc911\uce58\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378 \ud559\uc2b5\uc744 \ub354 \ub9ce\uc740 \uc6c0\uc9c1\uc784\uc774 \uc788\ub294 \uc601\uc5ed\uc73c\ub85c \uc720\ub3c4\ud558\uc5ec \uc774 \ubb38\uc81c\ub97c \ud574\uacb0\ud569\ub2c8\ub2e4. (c)\ub294 \uc81c\uc548\ub41c TI2V Bench\uc758 \uc608\uc2dc\ub97c \uc0ac\uc6a9\ud558\uc5ec MotiF\uc640 \uae30\uc900 \ubaa8\ub378(baseline)\uc758 \uc815\uc131\uc801 \uacb0\uacfc\ub97c \ube44\uad50\ud569\ub2c8\ub2e4.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.16153/x3.png", "caption": "Figure 2: High-level comparisons of MotiF vs. prior works. Previous TI2V generation methods mainly focused on deriving additional motion signals (motion score and/or motion mask) as inputs for the model to leverage implicitly. On the contrary, we focus on the learning objective and propose to weight the diffusion loss based on the motion intensity, that is derived from optical flow. Our method is simple, effective, and does not require additional inputs during inference. Moreover, MotiF is complementary to existing techniques.", "description": "\uc774 \uadf8\ub9bc\uc740 MotiF\uc640 \uae30\uc874 TI2V \ubc29\ubc95\ub4e4\uc758 \ucc28\uc774\uc810\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\uc874 \ubc29\ubc95\ub4e4\uc740 \ubaa8\ub378\uc5d0 \ucd94\uac00\uc801\uc778 \ubaa8\uc158 \uc2e0\ud638(\ubaa8\uc158 \uc810\uc218\ub098 \ub9c8\uc2a4\ud06c)\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\uc158 \uc815\ubcf4\ub97c \uac04\uc811\uc801\uc73c\ub85c \ud65c\uc6a9\ud558\ub294 \ub370 \ucd08\uc810\uc744 \ub9de\ucd98 \ubc18\uba74, MotiF\ub294 \ud559\uc2b5 \ubaa9\ud45c\uc5d0 \ucd08\uc810\uc744 \ub9de\ucdb0 \uad11\ud559 \ud750\ub984(optical flow)\uc73c\ub85c\ubd80\ud130 \uc5bb\uc740 \ubaa8\uc158 \uac15\ub3c4\uc5d0 \ub530\ub77c \ud655\uc0b0 \uc190\uc2e4(diffusion loss)\uc758 \uac00\uc911\uce58\ub97c \uc870\uc815\ud569\ub2c8\ub2e4. MotiF\ub294 \uac04\ub2e8\ud558\uace0 \ud6a8\uacfc\uc801\uc774\uba70 \ucd94\ub860 \uc911\uc5d0 \ucd94\uac00\uc801\uc778 \uc785\ub825\uc774 \ud544\uc694\ud558\uc9c0 \uc54a\uace0 \uae30\uc874 \uae30\uc220\ub4e4\uacfc \uc0c1\ud638 \ubcf4\uc644\uc801\uc778 \uad00\uacc4\ub97c \uac00\uc9d1\ub2c8\ub2e4.", "section": "3. Approach"}, {"figure_path": "https://arxiv.org/html/2412.16153/x4.png", "caption": "Figure 3: Example image-text pairs in TI2V Bench. For each scenario (column), we first think of a scene that could be potentially animated to generate different types of motion. We include challenging scenarios when there are multiple objects (yellow/blue/red balloon) in the initial image for fine-grained control or the text prompt describes a new object (frisbee, bubbles) to enter the scene. Then we come up with different prompts and use the publicly available meta.ai tool to generate diverse sets of images. Images of low quality or those not in the appropriate initial state are removed.", "description": "\uadf8\ub9bc 3\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 TI2V Bench \ub370\uc774\ud130\uc14b\uc758 \uc608\uc2dc \uc774\ubbf8\uc9c0-\ud14d\uc2a4\ud2b8 \uc30d\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \uc5f4(scenario)\uc740 \uc560\ub2c8\uba54\uc774\uc158\uc744 \ud1b5\ud574 \ub2e4\uc591\ud55c \ub3d9\uc791\uc744 \uc0dd\uc131\ud560 \uc218 \uc788\ub294 \uc7a0\uc7ac\uc801\uc778 \uc7a5\uba74\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc5ec\ub7ec \uac1c\uc758 \ubb3c\uccb4(\ub178\ub780\uc0c9/\ud30c\ub780\uc0c9/\ube68\uac04\uc0c9 \ud48d\uc120)\uac00 \ucd08\uae30 \uc774\ubbf8\uc9c0\uc5d0 \ud3ec\ud568\ub418\uc5b4 \uc138\ubc00\ud55c \uc81c\uc5b4\uac00 \uac00\ub2a5\ud558\ub3c4\ub85d \ud558\uac70\ub098, \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\uac00 \uc0c8\ub85c\uc6b4 \ubb3c\uccb4(\ud504\ub9ac\uc2a4\ube44, \uac70\ud488)\uac00 \uc7a5\uba74\uc5d0 \ub4f1\uc7a5\ud558\ub294 \uac83\uc744 \uc124\uba85\ud558\ub294 \ub4f1 \uc5b4\ub824\uc6b4 \uc2dc\ub098\ub9ac\uc624\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc591\ud55c \ud504\ub86c\ud504\ud2b8\ub97c \uc0ac\uc6a9\ud558\uc5ec \uacf5\uac1c\uc801\uc73c\ub85c \uc774\uc6a9 \uac00\ub2a5\ud55c Meta AI \ub3c4\uad6c\ub97c \ud1b5\ud574 \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\uc600\uc73c\uba70, \ud488\uc9c8\uc774 \ub0ae\uac70\ub098 \uc801\uc808\ud55c \ucd08\uae30 \uc0c1\ud0dc\uac00 \uc544\ub2cc \uc774\ubbf8\uc9c0\ub294 \uc81c\uac70\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \uc774 \uadf8\ub9bc\uc740 TI2V Bench \ub370\uc774\ud130\uc14b\uc758 \ub2e4\uc591\uc131\uacfc \ub09c\uc774\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ub300\ud45c\uc801\uc778 \uc608\uc2dc\uc785\ub2c8\ub2e4.", "section": "4. TI2V Bench"}, {"figure_path": "https://arxiv.org/html/2412.16153/x5.png", "caption": "Figure 4: Human evaluation results comparing MotiF to nine open-sourced models\u00a0[46, 7, 25, 50, 9, 53, 28, 12, 33] on our proposed TI2V Bench. We achieved considerable improvements across the board with an average preference of 72%percent7272\\%72 %. Through examining the justification choices, we found that our model mostly excel at improving text alignment and object motion, which matches very well with our motivation. Note that all the inference of prior works are done at Brown University.", "description": "\uadf8\ub9bc 4\ub294 \uc81c\uc548\ub41c TI2V Bench\uc5d0\uc11c MotiF\uc640 9\uac00\uc9c0 \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378 [46, 7, 25, 50, 9, 53, 28, 12, 33]\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud55c \uc0ac\uc6a9\uc790 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MotiF\ub294 \ud3c9\uade0 72%\uc758 \uc120\ud638\ub3c4\ub97c \ubcf4\uc774\uba70 \ubaa8\ub4e0 \ubaa8\ub378\uc5d0 \uac78\uccd0 \uc0c1\ub2f9\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. \uc815\ub2f9\ud654 \uc120\ud0dd\uc744 \ubd84\uc11d\ud55c \uacb0\uacfc, MotiF \ubaa8\ub378\uc740 \ud14d\uc2a4\ud2b8 \uc815\ub82c \ubc0f \uac1d\uccb4 \ub3d9\uc791 \uac1c\uc120\uc5d0 \ud0c1\uc6d4\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\uc73c\uba70, \uc774\ub294 MotiF\uc758 \uac1c\ubc1c \ub3d9\uae30\uc640 \uc77c\uce58\ud569\ub2c8\ub2e4. \ubaa8\ub4e0 \ube44\uad50 \ubaa8\ub378\uc758 \ucd94\ub860\uc740 Brown University\uc5d0\uc11c \uc218\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "section": "4.2 Human Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.16153/x6.png", "caption": "Figure 5: Qualitative comparison to prior works on TI2V Bench. Sampled frames are ordered from left to right.", "description": "\ubcf8 \uadf8\ub9bc\uc740 \ub17c\ubb38\uc758 TI2V Bench(Text-Image-to-Video Benchmark) \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec, MotiF\ub97c \ud3ec\ud568\ud55c \uc5ec\ub7ec \uae30\uc874 TI2V \ubaa8\ub378\ub4e4\uc758 \uc131\ub2a5\uc744 \uc815\uc131\uc801\uc73c\ub85c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc5d0\uc11c \uc624\ub978\ucabd\uc73c\ub85c, \uac01 \ubaa8\ub378\uc774 \uc0dd\uc131\ud55c \ube44\ub514\uc624\uc758 \uc77c\ubd80 \ud504\ub808\uc784\ub4e4\uc744 \uc21c\ucc28\uc801\uc73c\ub85c \ub098\uc5f4\ud558\uc5ec, \uac01 \ubaa8\ub378\uc758 \uc7a5\ub2e8\uc810\uacfc \uc2dc\uac01\uc801 \ud488\uc9c8\uc744 \uc9c1\uad00\uc801\uc73c\ub85c \ube44\uad50\ud560 \uc218 \uc788\ub3c4\ub85d \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac01 \ube44\ub514\uc624\ub294 \ub3d9\uc77c\ud55c \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ub418\uc5c8\uc73c\uba70, \ud504\ub808\uc784\ub4e4\uc758 \uc2dc\uac01\uc801 \ube44\uad50\ub97c \ud1b5\ud574 \uac01 \ubaa8\ub378\uc774 \uc81c\uc2dc\ud558\ub294 \uc601\uc0c1\uc758 \ub3d9\uc791, \uc6c0\uc9c1\uc784\uc758 \uc790\uc5f0\uc2a4\ub7ec\uc6c0, \ud14d\uc2a4\ud2b8 \ubc0f \uc774\ubbf8\uc9c0\uc640\uc758 \uc77c\uad00\uc131 \ub4f1\uc744 \ud3c9\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.2. \ube44\uad50 \uc5f0\uad6c"}, {"figure_path": "https://arxiv.org/html/2412.16153/x7.png", "caption": "Figure 6: Loss comparison. We calculate the ratio of the average loss in the high motion region to the average overall loss on a hold-out validation set with different timesteps. MotiF can effectively reduce the relative loss of the high motion regions.", "description": "\uadf8\ub9bc 6\uc740 \ubaa8\uc158 \ud3ec\ucee4\uc2a4 \uc190\uc2e4(MotiF)\uc774 \uace0\uc815\ubc00\ub3c4 \uc601\uc5ed\uc758 \uc190\uc2e4\uc744 \uc5bc\ub9c8\ub098 \ud6a8\uacfc\uc801\uc73c\ub85c \uc904\uc774\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uc190\uc2e4 \ube44\uad50 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc2dc\uac04 \ub2e8\uacc4\uc5d0\uc11c \uac80\uc99d \uc138\ud2b8\uc758 \uc804\uccb4 \ud3c9\uade0 \uc190\uc2e4\uc5d0 \ub300\ud55c \uace0\uc6b4\ub3d9 \uc601\uc5ed\uc758 \ud3c9\uade0 \uc190\uc2e4 \ube44\uc728\uc744 \uacc4\uc0b0\ud558\uc5ec \ube44\uad50\ud569\ub2c8\ub2e4. MotiF\ub294 \uace0\uc6b4\ub3d9 \uc601\uc5ed\uc758 \uc0c1\ub300\uc801 \uc190\uc2e4\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \uc904\uc774\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, MotiF\uac00 \uc815\uc9c0\ub41c \ubc30\uacbd\ubcf4\ub2e4\ub294 \uc2e4\uc81c\ub85c \uc6c0\uc9c1\uc774\ub294 \uc601\uc5ed\uc5d0 \ub354 \uc9d1\uc911\ud558\uc5ec \ud559\uc2b5\ud568\uc73c\ub85c\uc368 \uc815\ud655\ub3c4 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud568\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "5.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.16153/x8.png", "caption": "Figure A1: More qualitative comparison to prior works on TI2V Bench. MotiF can generate videos that align better with the text prompts. More video samples are available in the project website.", "description": "\uadf8\ub9bc A1\uc740 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ub41c TI2V Bench \uae30\uc900\uc73c\ub85c \uae30\uc874 \uc5f0\uad6c\ub4e4\uacfc MotiF\uc758 \ube44\ub514\uc624 \uc0dd\uc131 \uacb0\uacfc\ub97c \uc815\uc131\uc801\uc73c\ub85c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. MotiF\ub294 \ud14d\uc2a4\ud2b8 \ud504\ub86c\ud504\ud2b8\uc640 \ub354 \uc798 \uc77c\uce58\ud558\ub294 \ube44\ub514\uc624\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc6f9\uc0ac\uc774\ud2b8\uc5d0\uc11c \ub354 \ub9ce\uc740 \ube44\ub514\uc624 \uc0d8\ud50c\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "C. \ucd94\uac00 \uc2dc\uac01\ud654"}]