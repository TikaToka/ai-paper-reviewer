[{"content": "| Dataset | Train Size | Validation Size | Test Size |\n|---|---|---|---|\n| User-Product Review | 20,000 | 2,500 | 2,500 |\n| Multilingual Product Review | 20,000 | 2,500 | 2,500 |\n| Stylized Feedback | 20,000 | 2,500 | 2,500 |\n| Hotel Experiences | 9,000 | 2,500 | 2,500 |", "caption": "Table 1: Data Statistics for PGraph Benchmark. The table reports the average input length and average output length in words (done for the test set on GPT-4o-mini on BM25 back on all methods). The average profile size for each task is by user review size.", "description": "\ud45c 1\uc740 \uc81c\uc548\ub41c \uac1c\uc778\ud654\ub41c \uadf8\ub798\ud504 \uae30\ubc18 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ub370\uc774\ud130 \ud1b5\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud3c9\uade0 \uc785\ub825 \uae38\uc774\uc640 \ud3c9\uade0 \ucd9c\ub825 \uae38\uc774\ub294 GPT-4o-mini \ubaa8\ub378\uacfc BM25 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0\uc11c \uce21\uc815\ub418\uc5c8\uc73c\uba70, \ub2e8\uc704\ub294 \ub2e8\uc5b4 \uc218\uc785\ub2c8\ub2e4.  \ub610\ud55c \uac01 \uacfc\uc81c\uc5d0 \ub300\ud55c \ud3c9\uade0 \ud504\ub85c\ud544 \ud06c\uae30(\uc0ac\uc6a9\uc790 \ub9ac\ubdf0 \ud06c\uae30 \uae30\uc900)\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \uac01 \uacfc\uc81c\uc758 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574 \ud3c9\uade0 \uc785\ub825 \ubb38\uc7a5 \uae38\uc774, \ud3c9\uade0 \ucd9c\ub825 \ubb38\uc7a5 \uae38\uc774, \uadf8\ub9ac\uace0 \uc0ac\uc6a9\uc790\ubcc4 \ud3c9\uade0 \ub9ac\ubdf0 \uac1c\uc218\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "2 Personalized Graph-based Benchmark for LLMs"}, {"content": "| Long Text Generation | Metric | PGraphRAG | PGraphRAG\n(Neighbors Only) | PGraphRAG\n(User Only) |\n|---|---|---|---|---|\n| Task 1: User-Product Review Generation | ROUGE-1 | 0.173 | **0.177** | 0.168 |\n|  | ROUGE-L | 0.124 | **0.127** | 0.125 |\n|  | METEOR | 0.150 | **0.154** | 0.134 |\n| Task 2: Hotel Experience Generation | ROUGE-1 | 0.263 | **0.272** | 0.197 |\n|  | ROUGE-L | 0.156 | **0.162** | 0.128 |\n|  | METEOR | 0.191 | **0.195** | 0.121 |\n| Task 3: Stylized Feedback Generation | ROUGE-1 | **0.226** | 0.222 | 0.181 |\n|  | ROUGE-L | **0.171** | 0.165 | 0.134 |\n|  | METEOR | **0.192** | 0.186 | 0.147 |\n| Task 4: Multi-lingual Review Generation | ROUGE-1 | **0.174** | 0.172 | **0.174** |\n|  | ROUGE-L | 0.139 | 0.137 | **0.141** |\n|  | METEOR | **0.133** | 0.126 | 0.125 |", "caption": "Table 2: Graph statistics for the datasets used in the personalized tasks. The table provides the number of users, items, edges (reviews), and the average degree for each dataset: User-Product Graph, Multilingual Product Graph, Stylized Feedback Graph, and Hotel Experiences Graph.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uac1c\uc778\ud654\ub41c \uc791\uc5c5\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130\uc14b\uc758 \uadf8\ub798\ud504 \ud1b5\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud2b9\ud788 \uc0ac\uc6a9\uc790 \uc218, \uc544\uc774\ud15c \uc218, \uac04\uc120(\ub9ac\ubdf0) \uc218, \uadf8\ub9ac\uace0 \uac01 \ub370\uc774\ud130\uc14b(\uc0ac\uc6a9\uc790-\uc81c\ud488 \uadf8\ub798\ud504, \ub2e4\uad6d\uc5b4 \uc81c\ud488 \uadf8\ub798\ud504, \uc2a4\ud0c0\uc77c\ub9ac\uc26c \ud53c\ub4dc\ubc31 \uadf8\ub798\ud504, \ud638\ud154 \uacbd\ud5d8 \uadf8\ub798\ud504)\uc758 \ud3c9\uade0 \ucc28\uc218\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.  \uc774 \uc815\ubcf4\ub294 \uac01 \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\uc640 \uc5f0\uacb0\uc131\uc744 \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "2 Personalized Graph-based Benchmark for LLMs"}, {"content": "| Long Text Generation | Metric | PGraphRAG | PGraphRAG\n(Neighbors Only) | PGraphRAG\n(User Only) |\n|---|---|---|---|---|\n| Task 1: User-Product Review Generation | ROUGE-1 | **0.186** | 0.185 | 0.169 |\n|  | ROUGE-L | **0.126** | 0.125 | 0.114 |\n|  | METEOR | **0.187** | 0.185 | 0.170 |\n| Task 2: Hotel Experience Generation | ROUGE-1 | 0.265 | **0.268** | 0.217 |\n|  | ROUGE-L | 0.152 | **0.153** | 0.132 |\n|  | METEOR | 0.206 | **0.209** | 0.161 |\n| Task 3: Stylized Feedback Generation | ROUGE-1 | **0.205** | 0.204 | 0.178 |\n|  | ROUGE-L | **0.139** | 0.138 | 0.121 |\n|  | METEOR | **0.203** | 0.198 | 0.178 |\n| Task 4: Multilingual Product Review Generation | ROUGE-1 | **0.191** | 0.190 | 0.164 |\n|  | ROUGE-L | **0.142** | 0.140 | 0.123 |\n|  | METEOR | **0.173** | 0.169 | 0.155 |", "caption": "Table 3: Dataset split sizes for training, validation, and testing across four datasets: User-Product Review, Multilingual Product Review, Stylized Feedback, and Hotel Experiences.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc758 \ub370\uc774\ud130\uc14b \ubd84\ud560\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  User-Product Review, Multilingual Product Review, Stylized Feedback, Hotel Experiences \ub124 \uac00\uc9c0 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574 \ud559\uc2b5, \uac80\uc99d, \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc758 \ud06c\uae30\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\uc14b\uc740 \uc0ac\uc6a9\uc790 \ub9ac\ubdf0\uc758 \ud06c\uae30 \ubd84\ud3ec\ub97c \uc720\uc9c0\ud558\uae30 \uc704\ud574 \uacc4\uce35\ud654\ub41c \uc0d8\ud50c\ub9c1\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubd84\ud560\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uac01 \uc138\ud2b8\uc5d0\uc11c \uc0ac\uc6a9\uc790\uc758 \ub9ac\ubdf0 \ud06c\uae30 \ubd84\ud3ec\uac00 \uc6d0\ubcf8 \ub370\uc774\ud130\uc14b\uc758 \ubd84\ud3ec\uc640 \uc77c\uce58\ud558\ub3c4\ub85d \ud569\ub2c8\ub2e4.", "section": "3 Dataset Splits"}, {"content": "| Long Text Generation | Metric | k=1 | k=2 | k=4 |\n|---|---|---|---|---|\n| Task 1: User-Product Review Generation | ROUGE-1 | 0.160 | 0.169 | **0.173** |\n|  | ROUGE-L | 0.121 | **0.125** | 0.124 |\n|  | METEOR | 0.125 | 0.138 | **0.150** |\n| Task 2: Hotel Experiences Generation | ROUGE-1 | 0.230 | 0.251 | **0.263** |\n|  | ROUGE-L | 0.141 | 0.151 | **0.156** |\n|  | METEOR | 0.152 | 0.174 | **0.191** |\n| Task 3: Stylized Feedback Generation | ROUGE-1 | 0.200 | 0.214 | **0.226** |\n|  | ROUGE-L | 0.158 | 0.165 | **0.171** |\n|  | METEOR | 0.154 | 0.171 | **0.192** |\n| Task 4: Multilingual Product Review Generation | ROUGE-1 | 0.163 | 0.169 | **0.174** |\n|  | ROUGE-L | 0.134 | 0.137 | **0.139** |\n|  | METEOR | 0.113 | 0.122 | **0.133** |", "caption": "Table 4: Zero-shot test set results for long text generation using LLaMA-3.1-8B. The choice of retriever and k\ud835\udc58kitalic_k were tuned using the validation set.", "description": "\ud45c 4\ub294 LLaMA-3.1-8B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uae34 \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc81c\ub85c\uc0f7 \ud14c\uc2a4\ud2b8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uc0ac\uc6a9\uc790 \uc81c\ud488 \ub9ac\ubdf0 \uc0dd\uc131, \ud638\ud154 \uacbd\ud5d8 \uc0dd\uc131, \uc2a4\ud0c0\uc77c\ub9ac\uc26c\ud55c \ud53c\ub4dc\ubc31 \uc0dd\uc131, \ub2e4\uad6d\uc5b4 \uc81c\ud488 \ub9ac\ubdf0 \uc0dd\uc131 \ub4f1 \ub124 \uac00\uc9c0 \uc791\uc5c5\uc5d0 \ub300\ud55c ROUGE-1, ROUGE-L, METEOR \ud3c9\uac00 \uc9c0\ud45c \uc810\uc218\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac80\uc0c9\uae30\uc640 k(\uac80\uc0c9 \uacb0\uacfc \uac1c\uc218)\ub294 \uac80\uc99d \uc138\ud2b8\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc870\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub41c \ub2e4\uc591\ud55c \uc9c0\ud45c\uc640 \ud568\uaed8 \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "5.1 \uae30\uc900\uc120 \ube44\uad50"}, {"content": "| Long Text Generation | Metric | k=1 | k=2 | k=4 |\n|---|---|---|---|---|\n| Task 1: User-Product Review Generation | ROUGE-1 | 0.176 | 0.184 | **0.186** |\n|  | ROUGE-L | 0.121 | 0.125 | **0.126** |\n|  | METEOR | 0.168 | 0.180 | **0.187** |\n| Task 2: Hotel Experiences Generation | ROUGE-1 | 0.250 | 0.260 | **0.265** |\n|  | ROUGE-L | 0.146 | 0.150 | **0.152** |\n|  | METEOR | 0.188 | 0.198 | **0.206** |\n| Task 3: Stylized Feedback Generation | ROUGE-1 | 0.196 | 0.200 | **0.205** |\n|  | ROUGE-L | 0.136 | 0.136 | **0.139** |\n|  | METEOR | 0.186 | 0.192 | **0.203** |\n| Task 4: Multilingual Product Review Generation | ROUGE-1 | 0.163 | 0.169 | **0.174** |\n|  | ROUGE-L | 0.134 | 0.137 | **0.139** |\n|  | METEOR | 0.113 | 0.122 | **0.133** |", "caption": "Table 5: Zero-shot test set results for long text generation using GPT-4o-mini. The choice of retriever and k\ud835\udc58kitalic_k were tuned using the validation set.", "description": "\ud45c 5\ub294 GPT-4o-mini \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uae34 \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \uc791\uc5c5\uc5d0 \ub300\ud574 \uc218\ud589\ub41c \uc81c\ub85c\uc0f7 \ud14c\uc2a4\ud2b8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc791\uc5c5\uc5d0 \ub300\ud574, \uc0ac\uc6a9\ub41c \uac80\uc0c9 \ubaa8\ub378(BM25 \ub610\ub294 Contriever)\uacfc \uac80\uc0c9\ub41c \ucee8\ud14d\uc2a4\ud2b8\uc758 \uac1c\uc218(k)\ub294 \uac80\uc99d \uc138\ud2b8\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubbf8\uc138 \uc870\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \uc989,  \ucd5c\uc801\uc758 \uc131\ub2a5\uc744 \ub0b4\ub294 \uac80\uc0c9 \ubaa8\ub378\uacfc \ucee8\ud14d\uc2a4\ud2b8\uc758 \uac1c\uc218\ub97c \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c \uba3c\uc800 \ucc3e\uace0, \uadf8\ub807\uac8c \ucc3e\uc740 \ucd5c\uc801\uc758 \uc124\uc815\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0\uc11c \uc131\ub2a5\uc744 \ud3c9\uac00\ud588\ub2e4\ub294 \uc758\ubbf8\uc785\ub2c8\ub2e4. \ud45c\uc5d0\ub294 ROUGE-1, ROUGE-L, METEOR \uc138 \uac00\uc9c0 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc131\ub2a5\uc774 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. Experiments"}, {"content": "| Short Text Generation | Metric | k=1 | k=2 | k=4 |\n|---|---|---|---|---|\n| Task 5: User Product Review Title Generation | ROUGE-1 | **0.128** | 0.123 | 0.125 |\n|  | ROUGE-L | **0.121** | 0.118 | 0.119 |\n|  | METEOR | **0.123** | 0.118 | 0.117 |\n| Task 6: Hotel Experience Summary Generation | ROUGE-1 | **0.122** | 0.121 | 0.121 |\n|  | ROUGE-L | 0.112 | **0.114** | 0.113 |\n|  | METEOR | **0.104** | 0.102 | 0.099 |\n| Task 7: Stylized Feedback Title Generation | ROUGE-1 | 0.129 | **0.132** | **0.132** |\n|  | ROUGE-L | 0.124 | 0.126 | **0.128** |\n|  | METEOR | 0.129 | **0.130** | 0.129 |\n| Task 8: Multi-lingual Product Review Title Generation | ROUGE-1 | 0.129 | 0.126 | **0.131** |\n|  | ROUGE-L | 0.120 | 0.119 | **0.123** |\n|  | METEOR | 0.117 | 0.116 | **0.118** |", "caption": "Table 6: Zero-shot test set results for short text generation using LLaMA-3.1-8B. The choice of retriever and k\ud835\udc58kitalic_k were tuned using the validation set.", "description": "\ud45c 6\uc740 LLaMA-3.1-8B \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e8\ubb38 \uc0dd\uc131 \uc791\uc5c5\uc5d0 \ub300\ud574 \uc81c\ub85c\uc0f7 \uc124\uc815\uc5d0\uc11c \uc218\ud589\ud55c \ud14c\uc2a4\ud2b8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\uc5d0\ub294 \uc0ac\uc6a9\uc790 \uc81c\ud488 \ub9ac\ubdf0 \uc81c\ubaa9 \uc0dd\uc131, \ud638\ud154 \uacbd\ud5d8 \uc694\uc57d \uc0dd\uc131, \uc2a4\ud0c0\uc77c\ub9ac\uc26c\ud55c \ud53c\ub4dc\ubc31 \uc81c\ubaa9 \uc0dd\uc131, \ub2e4\uad6d\uc5b4 \uc81c\ud488 \ub9ac\ubdf0 \uc81c\ubaa9 \uc0dd\uc131 \ub4f1 \ub124 \uac00\uc9c0 \ub2e8\ubb38 \uc0dd\uc131 \uc791\uc5c5\uc5d0 \ub300\ud55c ROUGE-1, ROUGE-L, METEOR \uc9c0\ud45c\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uac80\uc0c9\uae30\uc640 k (\uac80\uc0c9\ub41c \ucee8\ud14d\uc2a4\ud2b8 \ud56d\ubaa9 \uc218)\ub294 \uac80\uc99d \uc138\ud2b8\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc870\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \uc989, \ubaa8\ub378\uc774 \ud6c8\ub828 \ub370\uc774\ud130\ub97c \ubcf4\uc9c0 \uc54a\uace0 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ud3c9\uac00\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4.", "section": "5.1 \uae30\uc900\uc120 \ube44\uad50"}, {"content": "| Short Text Generation | Metric | k=1 | k=2 | k=4 |\n|---|---|---|---|---|\n| Task 5: User Product Review Title Generation | ROUGE-1 | 0.111 | 0.110 | **0.111** |\n|  | ROUGE-L | **0.106** | 0.105 | **0.106** |\n|  | METEOR | 0.093 | 0.094 | **0.097** |\n| Task 6: Hotel Experience Summary Generation | ROUGE-1 | 0.114 | 0.114 | **0.118** |\n|  | ROUGE-L | 0.109 | 0.109 | **0.112** |\n|  | METEOR | 0.082 | 0.082 | **0.085** |\n| Task 7: Stylized Feedback Title Generation | ROUGE-1 | 0.100 | 0.103 | **0.109** |\n|  | ROUGE-L | 0.098 | 0.101 | **0.107** |\n|  | METEOR | 0.087 | 0.090 | **0.096** |\n| Task 8: Multi-lingual Product Review Title Generation | ROUGE-1 | 0.104 | 0.104 | **0.108** |\n|  | ROUGE-L | 0.098 | 0.098 | **0.104** |\n|  | METEOR | 0.077 | 0.078 | **0.082** |", "caption": "Table 7: Zero-shot test set results for short text generation using GPT-4o-mini. The choice of retriever and k\ud835\udc58kitalic_k were tuned using the validation set.", "description": "\ud45c 7\uc740 GPT-40-mini \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e8\ubb38 \uc0dd\uc131 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc81c\ub85c\uc0f7 \ud14c\uc2a4\ud2b8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \uc0ac\uc6a9\uc790 \uc81c\ud488 \ub9ac\ubdf0 \uc81c\ubaa9 \uc0dd\uc131, \ud638\ud154 \uacbd\ud5d8 \uc694\uc57d \uc0dd\uc131, \uc2a4\ud0c0\uc77c\ub9ac\uc26c\ud55c \ud53c\ub4dc\ubc31 \uc81c\ubaa9 \uc0dd\uc131, \ub2e4\uad6d\uc5b4 \uc81c\ud488 \ub9ac\ubdf0 \uc81c\ubaa9 \uc0dd\uc131 \ub4f1 \ub124 \uac00\uc9c0 \ub2e8\ubb38 \uc0dd\uc131 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc791\uc5c5\ub9c8\ub2e4 ROUGE-1, ROUGE-L, METEOR \uc138 \uac00\uc9c0 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uc600\uc73c\uba70, \uac80\uc99d \uc9d1\ud569\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac80\uc0c9\uae30\uc640 k \uac12\uc744 \uc870\uc815\ud588\uc2b5\ub2c8\ub2e4.  \uc774 \ud45c\ub97c \ud1b5\ud574 \uc81c\ub85c\uc0f7 \uc124\uc815\uc5d0\uc11c GPT-40-mini \ubaa8\ub378\uc758 \ub2e8\ubb38 \uc0dd\uc131 \uc131\ub2a5\uc744 \ub2e4\uc591\ud55c \uc791\uc5c5\uc5d0 \uac78\uccd0 \ube44\uad50 \ubd84\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5 \uc2e4\ud5d8"}, {"content": "| Long Text Generation | Metric | Contriever | BM25 |\n|---|---|---|---| \n| Task 1: User-Product Review Generation | ROUGE-1 | 0.172 | **0.173** |\n|  | ROUGE-L | 0.122 | **0.124** |\n|  | METEOR | **0.153** | 0.150 |\n| Task 2: Hotel Experiences Generation | ROUGE-1 | 0.262 | **0.263** |\n|  | ROUGE-L | 0.155 | **0.156** |\n|  | METEOR | 0.190 | **0.191** |\n| Task 3: Stylized Feedback Generation | ROUGE-1 | 0.195 | **0.226** |\n|  | ROUGE-L | 0.138 | **0.171** |\n|  | METEOR | 0.180 | **0.192** |\n| Task 4: Multilingual Product Review Generation | ROUGE-1 | 0.172 | **0.174** |\n|  | ROUGE-L | 0.134 | **0.139** |\n|  | METEOR | **0.135** | 0.133 |", "caption": "Table 8: Zero-shot test set results on ordinal classification on Tasks 9-12 on BM25 using MAE and RMSE metrics for LLaMA-3.1-8B-Instruct .", "description": "\ud45c 8\uc740 LLaMA-3.1-8B-Instruct \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec BM25 \uae30\ubc18\uc73c\ub85c 9\ubc88\ubd80\ud130 12\ubc88\uae4c\uc9c0\uc758 \uc21c\uc11c\ud615 \ubd84\ub958 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc81c\ub85c\uc0f7 \ud14c\uc2a4\ud2b8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. MAE(\ud3c9\uade0 \uc808\ub300 \uc624\ucc28)\uc640 RMSE(\uc81c\uacf1\uadfc \ud3c9\uade0 \uc81c\uacf1 \uc624\ucc28) \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4. \uac01 \uc791\uc5c5\uc5d0 \ub300\ud55c MAE\uc640 RMSE \uac12\uc740 \ubaa8\ub378\uc758 \uc608\uce21 \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ub0ae\uc740 MAE\uc640 RMSE \uac12\uc740 \ub354 \ub192\uc740 \uc608\uce21 \uc815\ud655\ub3c4\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.", "section": "5.1 Baseline Comparison"}, {"content": "| Long Text Generation | Metric | Contriever | BM25 |\n|---|---|---|---| \n| Task 1: User-Product Review Generation | ROUGE-1 | 0.182 | **0.186** |\n|  | ROUGE-L | 0.122 | **0.126** |\n|  | METEOR | 0.184 | **0.187** |\n| Task 2: Hotel Experiences Generation | ROUGE-1 | 0.264 | **0.265** |\n|  | ROUGE-L | **0.152** | **0.152** |\n|  | METEOR | **0.207** | 0.206 |\n| Task 3: Stylized Feedback Generation | ROUGE-1 | 0.194 | **0.205** |\n|  | ROUGE-L | 0.128 | **0.139** |\n|  | METEOR | 0.201 | **0.203** |\n| Task 4: Multilingual Product Review Generation | ROUGE-1 | 0.190 | **0.191** |\n|  | ROUGE-L | 0.141 | **0.142** |\n|  | METEOR | **0.174** | 0.173 |", "caption": "Table 9: Zero-shot test set results on ordinal classification on Tasks 9-12 on BM25 using MAE and RMSE metrics for GPT-4o-mini .", "description": "\ud45c 9\ub294 GPT-40-mini \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec BM25 \ud3c9\uac00 \uc9c0\ud45c\ub97c \uae30\ubc18\uc73c\ub85c \uc218\ud589\ub41c \uc81c\ub85c\uc0f7 \ubc29\uc2dd\uc758 \uc21c\uc11c\ud615 \ubd84\ub958 \uc791\uc5c5 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uc0ac\uc6a9\uc790 \uc81c\ud488 \ub9ac\ubdf0 \ud3c9\uc810(Task 9), \ud638\ud154 \uacbd\ud5d8 \ud3c9\uc810(Task 10), \uc2a4\ud0c0\uc77c\ub9ac\uc26c\ud55c \ud53c\ub4dc\ubc31 \ud3c9\uc810(Task 11), \ub2e4\uad6d\uc5b4 \uc81c\ud488 \ud3c9\uc810(Task 12) \ub4f1 4\uac00\uc9c0 \uacfc\uc81c\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc808\ub300 \uc624\ucc28(MAE)\uc640 \uc81c\uacf1\uadfc \ud3c9\uade0 \uc81c\uacf1 \uc624\ucc28(RMSE)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uacb0\uacfc\ub4e4\uc740 \ubaa8\ub378\uc758 \uc21c\uc11c\ud615 \ubd84\ub958 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \uac01 \uacfc\uc81c\uc5d0 \ub300\ud55c MAE\uc640 RMSE \uac12\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc885\ud569\uc801\uc73c\ub85c \ud3c9\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5.1 \uae30\uc900\uc120 \ube44\uad50"}, {"content": "| Short Text Generation | Metric | Contriever | BM25 |\n|---|---|---|---| \n| Task 5: User Product Review Title Generation | ROUGE-1 | 0.122 | **0.125** |\n|  | ROUGE-L | 0.116 | **0.119** |\n|  | METEOR | 0.115 | **0.117** |\n| Task 6: Hotel Experience Summary Generation | ROUGE-1 | 0.117 | **0.121** |\n|  | ROUGE-L | 0.110 | **0.113** |\n|  | METEOR | 0.095 | **0.099** |\n| Task 7: Stylized Feedback Title Generation | ROUGE-1 | 0.125 | **0.132** |\n|  | ROUGE-L | 0.121 | **0.128** |\n|  | METEOR | 0.122 | **0.129** |\n| Task 8: Multi-lingual Product Review Title Generation | ROUGE-1 | 0.126 | **0.131** |\n|  | ROUGE-L | 0.118 | **0.123** |\n|  | METEOR | 0.112 | **0.118** |", "caption": "Table 10: Ablation study results using LLaMA-3.1-8B-Instruct on the validation set for the long text generation Tasks 1 - 4.", "description": "\ud45c 10\uc740 \ubcf8 \ub17c\ubb38\uc758 5\uc7a5 \uc2e4\ud5d8\uc5d0\uc11c LLaMA-3.1-8B-Instruct \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uae34 \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \uc791\uc5c5(1~4\ubc88 \uacfc\uc81c)\uc5d0 \ub300\ud574 \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c \uc218\ud589\ub41c ablation study \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.   ablation study\ub294 PGraphRAG\uc758 \ub2e4\uc591\ud55c \ubcc0\ud615(\uc804\uccb4 PGraphRAG, \uc774\uc6c3\ub9cc \uc0ac\uc6a9\ud558\ub294 PGraphRAG, \uc0ac\uc6a9\uc790\ub9cc \uc0ac\uc6a9\ud558\ub294 PGraphRAG)\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uc774\uc6c3 \uc0ac\uc6a9\uc790\uc758 \uc815\ubcf4\ub97c \ud3ec\ud568\ud558\ub294 \uac83\uc774 \ubaa8\ub378 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uc5bc\ub9c8\ub098 \uae30\uc5ec\ud558\ub294\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud574 \uc218\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \uac01 \ubcc0\ud615\ubcc4\ub85c ROUGE-1, ROUGE-L, METEOR \uc9c0\ud45c \uac12\uc744 \uc81c\uc2dc\ud558\uc5ec  \uc5b4\ub5a4 \ubc29\uc2dd\uc774 \uac00\uc7a5 \ud6a8\uacfc\uc801\uc778\uc9c0 \ube44\uad50\ud569\ub2c8\ub2e4.", "section": "5. Experiments"}, {"content": "| Short Text Generation | Metric | Contriever | BM25 |\n|---|---|---|---|\n| Task 5: User Product Review Title Generation | ROUGE-1 | **0.113** | 0.111 |\n|  | ROUGE-L | **0.108** | 0.106 |\n|  | METEOR | **0.097** | **0.097** |\n| Task 6: Hotel Experience Summary Generation | ROUGE-1 | 0.113 | **0.118** |\n|  | ROUGE-L | 0.107 | **0.112** |\n|  | METEOR | 0.080 | **0.085** |\n| Task 7: Stylized Feedback Title Generation | ROUGE-1 | 0.108 | **0.109** |\n|  | ROUGE-L | 0.106 | **0.107** |\n|  | METEOR | 0.094 | **0.096** |\n| Task 8: Multi-lingual Product Review Title Generation | ROUGE-1 | **0.108** | **0.108** |\n|  | ROUGE-L | 0.103 | **0.104** |\n|  | METEOR | **0.082** | **0.082** |", "caption": "Table 11: Ablation study results using GPT-4o-mini on the validation set for long text generation tasks across Tasks 1-4.", "description": "\ud45c 11\uc740 \ubcf8 \ub17c\ubb38\uc758 5\uc7a5 \uc2e4\ud5d8 \uacb0\uacfc\uc5d0\uc11c GPT-4o-mini \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uae34 \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \uc791\uc5c5(\uacfc\uc81c 1-4)\uc5d0 \ub300\ud55c ablation study \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uacfc\uc81c\uc5d0 \ub300\ud574 PGraphRAG, \uc774\uc6c3 \ub178\ub4dc \uc815\ubcf4\ub9cc \uc0ac\uc6a9\ud55c PGraphRAG(Neighbors Only), \uc0ac\uc6a9\uc790 \uc815\ubcf4\ub9cc \uc0ac\uc6a9\ud55c PGraphRAG(User Only) \uc138 \uac00\uc9c0 \ubc29\ubc95\uc758 ROUGE-1, ROUGE-L, METEOR \uc131\ub2a5 \uc9c0\ud45c \uac12\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc5ec, \uadf8\ub798\ud504 \uae30\ubc18 \uac80\uc0c9\uc758 \ud6a8\uacfc\uc640 \uc774\uc6c3 \ub178\ub4dc \uc815\ubcf4 \ud65c\uc6a9\uc758 \uc911\uc694\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Validation set \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "section": "5.2.1 PGraphRAG Ablation Study"}]