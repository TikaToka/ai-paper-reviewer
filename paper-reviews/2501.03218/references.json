{"references": [{"fullname_first_author": "Joya Chen", "paper_title": "VideoLLM-online: Online video large language model for streaming video", "publication_date": "2024", "reason": "This paper is directly compared against in the current work, serving as a key baseline for evaluating active real-time interaction with streaming videos."}, {"fullname_first_author": "Shangzhe Di", "paper_title": "Grounded question-answering in long egocentric videos", "publication_date": "2024", "reason": "This paper provides a dataset used in training the model, which is crucial for evaluating its performance on streaming video QA tasks."}, {"fullname_first_author": "Yanwei Li", "paper_title": "VideoLLAMA 2: Advancing spatial-temporal modeling and audio understanding in video-LLMs", "publication_date": "2024", "reason": "This paper is a key contribution in video LLMs and is used to enhance the streaming video understanding capability of the proposed model."}, {"fullname_first_author": "Junming Lin", "paper_title": "StreamingBench: Assessing the gap for MLLMs to achieve streaming video understanding", "publication_date": "2024", "reason": "This benchmark is used for evaluating the proposed model, providing a comprehensive evaluation of its performance in real-time video stream interactions."}, {"fullname_first_author": "Ye Liu", "paper_title": "E.T.Bench: Towards open-ended event-level video-language understanding", "publication_date": "2024", "reason": "This benchmark provides a dataset used in training and evaluating the model's performance in temporal reasoning and event understanding in streaming videos."}]}