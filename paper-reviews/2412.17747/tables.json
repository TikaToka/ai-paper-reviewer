[{"content": "| Position | 8 Latents | 16 Latents | 32 Latents | 64 Latents |\n|---|---|---|---|---|\n| 1 | -1.53% | -2.48% | -3.28% | -3.94% |\n| 2 | -1.67% | -2.41% | -3.15% | -3.70% |\n| 4 | -1.39% | -1.98% | -2.66% | -3.17% |\n| 8 | -1.22% | -1.56% | -2.11% | -2.61% |\n| 16 | -0.85% | -1.08% | -1.50% | -1.88% |\n| 32 | -0.55% | -0.64% | -0.88% | -1.20% |", "caption": "Table 1: Relative perplexity reduction (in %) achieved by augmented Gemma-2 2B models compared to the baseline, for various numbers of latents and prediction positions following latent augmentation. \"Position\" indicates the token position relative to the augmentation point (e.g., Position 1 is the immediately following token).", "description": "\ud45c 1\uc740 \uae30\ubcf8 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec \uc99d\uac15\ub41c Gemma-2 2B \ubaa8\ub378\uc758 \uc0c1\ub300\uc801 perplexity \uac10\uc18c\uc728(%)\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc5ec\ub7ec \uac1c\uc758 \uc7a0\uc7ac \ubca1\ud130 \uc218\uc640 \uc7a0\uc7ac \ubca1\ud130 \ucd94\uac00 \ud6c4 \uc608\uce21 \uc704\uce58\uc5d0 \ub530\ub978 perplexity \uac10\uc18c\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \"\uc704\uce58\"\ub294 \uc7a0\uc7ac \ubca1\ud130 \ucd94\uac00 \uc9c0\uc810\uc744 \uae30\uc900\uc73c\ub85c \ud55c \ud1a0\ud070 \uc704\uce58\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4(\uc608: \uc704\uce58 1\uc740 \ubc14\ub85c \ub2e4\uc74c \ud1a0\ud070).  \uc989, \uc774 \ud45c\ub294 \uc7a0\uc7ac \ubca1\ud130\ub97c \ucd94\uac00\ud55c \ud6c4, \uba87 \ubc88\uc9f8 \ud1a0\ud070\uc744 \uc608\uce21\ud558\ub294\uc9c0\uc5d0 \ub530\ub77c perplexity\uac00 \uc5bc\ub9c8\ub098 \uac10\uc18c\ud558\ub294\uc9c0\ub97c \uc7a0\uc7ac \ubca1\ud130\uc758 \uac1c\uc218\ubcc4\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ucd94\uac00\ub41c \uc7a0\uc7ac \ubca1\ud130 \uac1c\uc218\uac00 \ub9ce\uc744\uc218\ub85d perplexity \uac10\uc18c \ud6a8\uacfc\uac00 \ud06c\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.1. Perplexity Evaluation"}, {"content": "| Benchmark | Metric | Baseline | 4 Latents | 8 Latents | 16 Latents | 32 Latents | 64 Latents |\n|---|---|---|---|---|---|---|---| \n| MMLU | 5-shot | 52.00 | 52.45 (+0.45) | 52.24 (+0.24) | 52.34 (+0.34) | 54.61 (+2.61) | **56.70 (+4.70)** |\n| GSM8K | 8-shot | 21.38 | 22.67 (+1.29) | 23.12 (+1.74) | 24.72 (+3.34) | 26.76 (+5.38) | **31.43 (+10.05)** |\n| DROP | 3-shot, F1 | 53.69 | 54.64 (+0.95) | 54.91 (+1.23) | 56.23 (+2.55) | 57.37 (+3.68) | **57.77 (+4.08)** |\n| ARC-e | 0-shot | 80.56 | 81.52 (+0.97) | 81.57 (+1.01) | 83.12 (+2.57) | 83.04 (+2.48) | **83.67 (+3.11)** |\n| ARC-c | 0-shot | 50.26 | 51.28 (+1.02) | 52.39 (+2.13) | 53.24 (+2.99) | **54.44 (+4.18)** | **54.44 (+4.18)** |\n| MATH | 4-shot | 16.50 | 16.38 (-0.12) | 16.78 (+0.28) | 17.00 (+0.50) | 17.18 (+0.68) | **18.56 (+2.06)** |\n| Winogrande | 0-shot | 64.01 | 65.35 (+1.34) | 65.35 (+1.34) | 66.30 (+2.29) | 66.30 (+2.29) | **66.61 (+2.60)** |\n| PIQA | 0-shot | 78.18 | 78.62 (+0.44) | 78.67 (+0.49) | 78.94 (+0.76) | 78.94 (+0.76) | **79.00 (+0.82)** |\n| SIQA | 0-shot | 51.79 | 51.59 (-0.20) | 51.64 (-0.15) | 51.74 (-0.05) | **52.30 (+0.51)** | 52.00 (+0.20) |\n| HellaSwag | 0-shot | 73.77 | 74.41 (+0.64) | 74.41 (+0.64) | 74.82 (+1.05) | 75.04 (+1.27) | **75.31 (+1.54)** |\n| Boolq | 0-shot | 75.41 | 75.29 (-0.12) | 77.22 (+1.80) | **78.17 (+2.75)** | 77.03 (+1.62) | 76.91 (+1.50) |\n| MBPP | 3-shot | 30.40 | 29.00 (-1.40) | 31.60 (+1.20) | 31.20 (+0.80) | 31.40 (+1.00) | **31.80 (+1.40)** |\n| AGIEval | 3-5-shot | 31.71 | 32.18 (+0.47) | 30.04 (-1.67) | 31.32 (-0.38) | 32.78 (+1.07) | **33.85 (+2.14)** |\n| TriviaQA | 5-shot | 60.29 | 60.30 (+0.01) | 60.83 (+0.54) | 61.43 (+1.14) | 62.05 (+1.76) | **62.23 (+1.94)** |\n| NQ | 5-shot | 17.14 | 17.35 (+0.21) | 17.89 (+0.75) | 18.16 (+1.02) | 18.91 (+1.77) | **19.20 (+2.06)** |\n| HumanEval | pass@1 | 19.51 | 18.29 (-1.22) | 19.51 (+0.00) | 20.73 (+1.22) | 20.73 (+1.22) | **22.56 (+3.05)** |\n| BBH | 3-shot | 42.22 | 42.36 (+0.14) | 42.37 (+0.15) | 42.53 (+0.31) | 42.48 (+0.26) | **42.64 (+0.41)** |", "caption": "Table 2: Performance of baseline and augmented models across various benchmarks. Results are shown for the baseline (frozen Gemma-2 2B pretrained model) and the model augmented with a learned coprocessor using 4, 8, 16, 32, and 64 latent embeddings, respectively. Results are reported for zero/few-shot settings as indicated in the \u201cMetric\u201d column. Results are accuracy (in %) if not specified in the Metric column. Improvements over the baseline are shown in parentheses. In this setting, the coprocessor is called once, at the end of the prompt.", "description": "\ud45c 2\ub294 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uae30\uc900 \ubaa8\ub378\uacfc \uc99d\uac15 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\uc900 \ubaa8\ub378\uc740 \ub3d9\uacb0\ub41c Gemma-2 2B \uc0ac\uc804 \ud6c8\ub828 \ubaa8\ub378\uc774\uba70, \uc99d\uac15 \ubaa8\ub378\uc740 \ud559\uc2b5\ub41c \ucf54\ud504\ub85c\uc138\uc11c\ub97c \uc0ac\uc6a9\ud558\uc5ec 4, 8, 16, 32, 64\uac1c\uc758 \uc7a0\uc7ac \uc784\ubca0\ub529\uc73c\ub85c \uc99d\uac15\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4. \uacb0\uacfc\ub294 \"Metric\" \uc5f4\uc5d0 \ud45c\uc2dc\ub41c \ub300\ub85c \uc81c\ub85c\uc0f7/\uc18c\uc218\uc0f7 \uc124\uc815\uc5d0 \ub300\ud574 \ubcf4\uace0\ub429\ub2c8\ub2e4. \"Metric\" \uc5f4\uc5d0 \uba85\uc2dc\ub418\uc9c0 \uc54a\uc740 \uacbd\uc6b0 \uacb0\uacfc\ub294 \uc815\ud655\ub3c4(%)\uc785\ub2c8\ub2e4. \uae30\uc900\uc120\uc5d0 \ub300\ud55c \uac1c\uc120 \uc0ac\ud56d\uc740 \uad04\ud638 \uc548\uc5d0 \ud45c\uc2dc\ub429\ub2c8\ub2e4. \uc774 \uc124\uc815\uc5d0\uc11c\ub294 \ud504\ub86c\ud504\ud2b8\uc758 \ub05d\uc5d0\uc11c \ucf54\ud504\ub85c\uc138\uc11c\uac00 \ud55c \ubc88 \ud638\ucd9c\ub429\ub2c8\ub2e4.", "section": "3.2 \uacf5\uac1c \ubca4\uce58\ub9c8\ud06c \ud3c9\uac00"}, {"content": "| Method | Validation set perplexity (\u2193) | GSM8K 8-shot accuracy (\u2191) |\n|---|---|---|\n| Baseline Gemma-2 2B | 10.96 | 21.38 |\n| Pause Token | 11.63 | 22.37 |\n| Latent embeddings (Ours) | **10.60** | **26.76** |", "caption": "Table 3: Comparison between the baseline Gemma-2 2B model, the Pause Token method\u00a0(Goyal et\u00a0al., 2023) (using 32 embeddings), and our approach (also using 32 embeddings). Lower perplexity indicates better next token prediction. Higher accuracy indicates better performance on GSM8K.", "description": "\ud45c 3\uc740 \uc138 \uac00\uc9c0 \ub2e4\ub978 \ubc29\ubc95\uc744 \ube44\uad50\ud558\uc5ec \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uae30\uc900 Gemma-2 2B \ubaa8\ub378, Pause Token \uae30\ubc95(Goyal et al., 2023) (32\uac1c \uc784\ubca0\ub529 \uc0ac\uc6a9), \uadf8\ub9ac\uace0 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \uae30\ubc95(32\uac1c \uc784\ubca0\ub529 \uc0ac\uc6a9)\uc785\ub2c8\ub2e4. \ud37c\ud50c\ub809\uc11c\ud2f0 \uac12\uc774 \ub0ae\uc744\uc218\ub85d \ub2e4\uc74c \ud1a0\ud070 \uc608\uce21 \uc131\ub2a5\uc774 \uc88b\ub2e4\ub294 \uac83\uc744 \ub098\ud0c0\ub0b4\uace0, GSM8K \uc815\ud655\ub3c4\uac00 \ub192\uc744\uc218\ub85d GSM8K \uc131\ub2a5\uc774 \uc88b\ub2e4\ub294 \uac83\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.", "section": "3.3 Pause Token"}, {"content": "| Baseline | 0-shot CoT | 16 Latents | 32 Latents |\n|---|---|---|---| \n| 21.38 | 23.20 | 24.72 | **26.76** |", "caption": "Table 4: Accuracy on GSM8K 8-shot for the baseline Gemma-2 2B model, zero-shot Chain-of-Thought (CoT) prompting, and our approach with 16 and 32 latent embeddings.", "description": "\ud45c 4\ub294 \uc138 \uac00\uc9c0 \ub2e4\ub978 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec GSM8K \ub370\uc774\ud130\uc14b\uc5d0\uc11c 8-shot \uc124\uc815\uc73c\ub85c \uc5bb\uc740 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uccab \ubc88\uc9f8\ub294 \uae30\uc900 Gemma-2 2B \ubaa8\ub378\uc774\uba70, \ub450 \ubc88\uc9f8\ub294 \uc81c\ub85c\uc0f7 \uccb4\uc778 \uc624\ube0c \uc2a4\ub85c\ud2b8(CoT) \ud504\ub86c\ud504\ud305 \uae30\ubc95\uc774\uba70, \uc138 \ubc88\uc9f8\ub294 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud55c \ubc29\ubc95\uc73c\ub85c 16\uac1c\uc640 32\uac1c\uc758 \uc7a0\uc7ac\uc801 \uc784\ubca0\ub529\uc744 \uc0ac\uc6a9\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uc81c\ub85c\uc0f7 CoT \ud504\ub86c\ud504\ud305\uacfc \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud55c \ubc29\ubc95\uc774 \uae30\uc900 \ubaa8\ub378\uc5d0 \ube44\ud574 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc8fc\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c, \uc7a0\uc7ac\uc801 \uc784\ubca0\ub529 \uc218\ub97c \ub298\ub9ac\uba74 \uc815\ud655\ub3c4\uac00 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. Experiments"}, {"content": "| Method | GSM8K Accuracy |\n|---|---| \n| Baseline | 21.38 |\n| LoRA (Rank 64) | 23.35 |\n| LoRA (Rank 128) | 24.03 |\n| From Scratch Training | 25.78 |\n| Full Finetuning | **26.76** |", "caption": "Table 5: GSM8K accuracy comparison of different finetuning methods for the coprocessor, all using 32 latent embeddings. LoRA offers a memory-efficient alternative to full finetuning, achieving reasonable performance gains.", "description": "\ud45c 5\ub294 \ubaa8\ub4e0 \ubc29\ubc95\uc5d0\uc11c 32\uac1c\uc758 \uc7a0\uc7ac \uc784\ubca0\ub529\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucf54\ud504\ub85c\uc138\uc11c\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ubbf8\uc138 \uc870\uc815 \ubc29\ubc95\uc758 GSM8K \uc815\ud655\ub3c4 \ube44\uad50\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. LoRA\ub294 \uc804\uccb4 \ubbf8\uc138 \uc870\uc815\uc5d0 \ube44\ud574 \uba54\ubaa8\ub9ac \ud6a8\uc728\uc801\uc778 \ub300\uc548\uc744 \uc81c\uacf5\ud558\uba70 \uc0c1\ub2f9\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4. \uc774 \ud45c\ub294 \uae30\ubcf8 \ubaa8\ub378, LoRA(Rank 64, Rank 128)\ub97c \uc0ac\uc6a9\ud55c \ubbf8\uc138 \uc870\uc815, \ucc98\uc74c\ubd80\ud130 \ud559\uc2b5, \uadf8\ub9ac\uace0 \uc804\uccb4 \ubbf8\uc138 \uc870\uc815\uc744 \ud3ec\ud568\ud55c \ub124 \uac00\uc9c0\uc758 \ub2e4\ub978 \ucf54\ud504\ub85c\uc138\uc11c \ubbf8\uc138 \uc870\uc815 \ubc29\ubc95\uc5d0 \ub300\ud55c GSM8K \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud569\ub2c8\ub2e4.", "section": "3.3. Comparison with other baselines and variations"}, {"content": "| Baseline | 4 Ahead | 8 Ahead | 16 Ahead | 32 Ahead |\n|---|---|---|---|---|\n| 21.38 | 24.03 (+2.65) | 24.11 (+2.73) | **24.72** (+3.34) | 23.73 (+2.35) |", "caption": "Table 6: GSM8K accuracy for varying numbers of ahead tokens during coprocessor training. 16 ahead tokens achieves the highest accuracy (24.72%, +3.34% over the baseline of 21.38%). 16 latent embeddings are used for all these experiments.", "description": "\ud45c 6\uc740 \ucf54\ud504\ub85c\uc138\uc11c \ud6c8\ub828 \uc911 \uc55e\ucabd \ud1a0\ud070\uc758 \uac1c\uc218\ub97c \ub2e4\ub974\uac8c \ud588\uc744 \ub54c GSM8K \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 16\uac1c\uc758 \uc55e\ucabd \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c \uc815\ud655\ub3c4\uac00 \uac00\uc7a5 \ub192\uc558\uc73c\uba70 (24.72%, \uae30\uc900\uc120 21.38%\ubcf4\ub2e4 3.34% \uc0c1\uc2b9), \ubaa8\ub4e0 \uc2e4\ud5d8\uc5d0\uc11c 16\uac1c\uc758 \uc7a0\uc7ac\uc801 \uc784\ubca0\ub529\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \uc218\uc758 \uc55e\ucabd \ud1a0\ud070(4, 8, 16, 32\uac1c)\uc5d0 \ub300\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uba70, \uc55e\ucabd \ud1a0\ud070 \uc218\uc5d0 \ub530\ub978 \uc131\ub2a5 \ubcc0\ud654\ub97c \ubd84\uc11d\ud558\uc5ec \ucd5c\uc801\uc758 \uc55e\ucabd \ud1a0\ud070 \uac1c\uc218\ub97c \ucc3e\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc774 \ubbf8\ub798 \ud1a0\ud070\uc744 \uc5bc\ub9c8\ub098 \uc798 \uc608\uce21\ud558\ub294\uc9c0 \ud30c\uc545\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc9c0\ud45c\uac00 \ub429\ub2c8\ub2e4.", "section": "3. \uc2e4\ud5d8"}, {"content": "| Benchmark | Metric | Baseline | 4 Latents | 8 Latents | 16 Latents | 32 Latents | 64 Latents |\n|---|---|---|---|---|---|---|---| \n| MMLU | 5-shot | 52.00 | 52.03 (+0.03) | 52.21 (+0.21) | 52.75 (+0.75) | 53.55 (+1.55) | 56.63 (+4.63) |\n| GSM8K | 8-shot | 21.38 | 22.52 (+1.14) | 22.59 (+1.21) | 24.41 (+3.03) | 25.78 (+4.40) | 29.80 (+8.42) |\n| ARC-e | 0-shot | 80.56 | 81.69 (+1.13) | 81.86 (+1.30) | 82.79 (+2.23) | 83.12 (+2.56) | 83.21 (+2.65) |\n| ARC-c | 0-shot | 50.26 | 51.71 (+1.45) | 52.22 (+1.96) | 52.47 (+2.21) | 54.27 (+4.01) | 53.24 (+2.98) |\n| MATH | 4-shot | 16.50 | 16.22 (-0.28) | 16.46 (-0.04) | 16.92 (+0.42) | 17.18 (+0.68) | 18.34 (+1.84) |\n| Winogrande | 0-shot | 64.01 | 65.19 (+1.18) | 65.98 (+1.97) | 66.54 (+2.53) | 66.69 (+2.68) | 67.25 (+3.24) |\n| PIQA | 0-shot | 78.18 | 78.13 (-0.05) | 79.00 (+0.82) | 79.16 (+0.98) | 79.27 (+1.09) | 79.22 (+1.04) |\n| SIQA | 0-shot | 51.79 | 51.94 (+0.15) | 51.64 (-0.15) | 51.84 (+0.05) | 51.94 (+0.15) | 51.89 (+0.10) |\n| HellaSwag | 0-shot | 73.77 | 74.37 (+0.60) | 74.68 (+0.91) | 74.82 (+1.05) | 74.89 (+1.12) | 75.18 (+1.41) |\n| Boolq | 0-shot | 75.41 | 75.66 (+0.25) | 76.94 (+1.53) | 76.97 (+1.56) | 77.80 (+2.39) | 77.46 (+2.05) |\n| MBPP | 3-shot | 30.40 | 30.40 (0.00) | 30.60 (+0.20) | 30.80 (+0.40) | 32.00 (+1.60) | 32.60 (+2.20) |\n| AGIEval | 3-5-shot | 31.71 | 32.52 (+0.81) | 32.22 (+0.51) | 31.92 (+0.21) | 32.78 (+1.07) | 32.35 (+0.64) |\n| TriviaQA | 5-shot | 60.29 | 60.53 (+0.24) | 60.95 (+0.66) | 61.45 (+1.16) | 61.93 (+1.64) | 62.62 (+2.33) |\n| NQ | 5-shot | 17.14 | 17.26 (+0.12) | 17.89 (+0.75) | 18.47 (+1.33) | 18.68 (+1.54) | 19.00 (+1.86) |\n| HumanEval | pass@1 | 19.51 | 18.29 (-1.22) | 18.90 (-0.61) | 20.73 (+1.22) | 19.51 (0.00) | 19.51 (0.00) |\n| BBH | 3-shot | 42.22 | 42.16 (-0.06) | 42.24 (+0.02) | 42.42 (+0.20) | 43.19 (+0.97) | 42.93 (+0.71) |", "caption": "Table 7: Performance of baseline and augmented models across various benchmarks with coprocessor training from scratch. Check Table\u00a02 for more detailed description.", "description": "\ud45c 7\uc740 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \uac00\uc911\uce58\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 \ucc98\uc74c\ubd80\ud130 \ucf54\ud504\ub85c\uc138\uc11c\ub97c \ud6c8\ub828\ud588\uc744 \ub54c \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uae30\uc900 \ubaa8\ub378\uacfc \uc99d\uac15 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uae30\uc900 \ubaa8\ub378\uc740 \ub3d9\uacb0\ub41c Gemma-2 2B \ubaa8\ub378\uc785\ub2c8\ub2e4. \uac01 \ubca4\uce58\ub9c8\ud06c\ub9c8\ub2e4 \uc5ec\ub7ec \uac1c\uc758 \ucf54\ud504\ub85c\uc138\uc11c \uc7a0\uc7ac \uc784\ubca0\ub529 \uc218(4, 8, 16, 32, 64)\uc5d0 \ub530\ub978 \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \uad04\ud638 \uc548\uc758 \uc218\uce58\ub294 \uae30\uc900 \ubaa8\ub378 \ub300\ube44 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubc31\ubd84\uc728\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ud45c 2\ub97c \ucc38\uc870\ud558\uc2ed\uc2dc\uc624.", "section": "3. Experiments"}]