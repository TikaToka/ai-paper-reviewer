[{"figure_path": "https://arxiv.org/html/2412.18450/x1.png", "caption": "Figure 1: Proposed 3DGraphLLM approach leverages 3D semantic scene graph learnable representation supplied as input to an LLM to perform various 3D vision-language tasks.", "description": "\uadf8\ub9bc 1\uc740 \uc81c\uc548\ub41c 3DGraphLLM \ubc29\ubc95\uc774 \ub2e4\uc591\ud55c 3D \ube44\uc804-\uc5b8\uc5b4 \uc791\uc5c5\uc744 \uc218\ud589\ud558\uae30 \uc704\ud574 3D \uc758\ubbf8\ub860\uc801 \uc7a5\uba74 \uadf8\ub798\ud504\uc758 \ud559\uc2b5 \uac00\ub2a5\ud55c \ud45c\ud604\uc744 LLM\uc5d0 \uc785\ub825\uc73c\ub85c \uc81c\uacf5\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  3D \uc7a5\uba74 \uadf8\ub798\ud504\ub294 \uac1d\uccb4\uc640 \uac1d\uccb4 \uac04\uc758 \uc758\ubbf8\ub860\uc801 \uad00\uacc4\ub97c \uc800\uc7a5\ud558\ub294 \uac04\uacb0\ud55c \uc7a5\uba74 \ubaa8\ub378\ub85c, \ub85c\ubd07 \uc791\uc5c5\uc5d0 \uc720\uc6a9\ud569\ub2c8\ub2e4. \uc0ac\uc6a9\uc790\uc640 \uc0c1\ud638 \uc791\uc6a9\ud560 \ub54c, \uad6c\ud604\ub41c \uc9c0\ub2a5\ud615 \uc5d0\uc774\uc804\ud2b8\ub294 \uc790\uc5f0\uc5b4\ub85c \ud45c\ud604\ub41c \uc7a5\uba74\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \uc9c8\ubb38\uc5d0 \uc751\ub2f5\ud560 \uc218 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4. LLM\uc740 \uc790\uc5f0\uc5b4 \uc774\ud574 \ubc0f \ucd94\ub860 \ub2a5\ub825\uc73c\ub85c \uc778\ud574 \uc0ac\uc6a9\uc790-\ub85c\ubd07 \uc0c1\ud638 \uc791\uc6a9\uc5d0 \ud6a8\uacfc\uc801\uc778 \ud574\uacb0\ucc45\uc785\ub2c8\ub2e4.  3DGraphLLM\uc740 3D \uc7a5\uba74 \uadf8\ub798\ud504\uc758 \ud559\uc2b5 \uac00\ub2a5\ud55c \ud45c\ud604\uc744 \uc0dd\uc131\ud558\uace0 \uc774\ub97c LLM\uc5d0 \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec 3D \ube44\uc804-\uc5b8\uc5b4 \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uae30\uc874\uc758 \uc88c\ud45c \uc815\ubcf4\ub9cc \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\ubcf4\ub2e4 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uae30\ub300\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.18450/x2.png", "caption": "Figure 2: \nThe overall architecture of our approach. 3DGraphLLM leverages pre-trained encoders for 3D object point clouds and semantic relationships between objects. We introduce trainable layers to map the extracted graph node and edge features into the token embedding space of a pre-trained LLM. The scene graph is flattened for input into the LLM, with each object represented by a subgraph of its k nearest neighbors. To further adapt the LLM to 3D vision-language tasks, we add new object tokens to the LLM\u2019s vocabulary and fine-tune it using LoRa.", "description": "\uadf8\ub9bc 2\ub294 \uc81c\uc548\ub41c 3DGraphLLM \uc811\uadfc \ubc29\uc2dd\uc758 \uc804\uccb4 \uc544\ud0a4\ud14d\ucc98\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. 3DGraphLLM\uc740 3D \uac1d\uccb4 \uc810 \uad6c\ub984\uacfc \uac1d\uccb4 \uac04\uc758 \uc758\ubbf8 \uad00\uacc4\uc5d0 \ub300\ud574 \uc0ac\uc804 \ud6c8\ub828\ub41c \uc778\ucf54\ub354\ub97c \ud65c\uc6a9\ud569\ub2c8\ub2e4. \ucd94\ucd9c\ub41c \uadf8\ub798\ud504 \ub178\ub4dc\uc640 \uc5d0\uc9c0 \ud2b9\uc9d5\uc744 \uc0ac\uc804 \ud6c8\ub828\ub41c LLM\uc758 \ud1a0\ud070 \uc784\ubca0\ub529 \uacf5\uac04\uc5d0 \ub9e4\ud551\ud558\uae30 \uc704\ud574 \ud6c8\ub828 \uac00\ub2a5\ud55c \uacc4\uce35\uc744 \ub3c4\uc785\ud569\ub2c8\ub2e4. \uc7a5\uba74 \uadf8\ub798\ud504\ub294 LLM\uc5d0 \uc785\ub825\ud558\uae30 \uc704\ud574 \ud3c9\ud3c9\ud558\uac8c \ucc98\ub9ac\ub418\uba70, \uac01 \uac1d\uccb4\ub294 k\uac1c\uc758 \uac00\uc7a5 \uac00\uae4c\uc6b4 \uc774\uc6c3\uc758 \uc11c\ube0c\uadf8\ub798\ud504\ub85c \ud45c\ud604\ub429\ub2c8\ub2e4. 3D \ube44\uc804-\uc5b8\uc5b4 \uc791\uc5c5\uc5d0 LLM\uc744 \ub354 \uc798 \uc801\uc6a9\ud558\uae30 \uc704\ud574 \uc0c8\ub85c\uc6b4 \uac1d\uccb4 \ud1a0\ud070\uc744 LLM\uc758 \uc5b4\ud718\uc5d0 \ucd94\uac00\ud558\uace0 LoRA\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubbf8\uc138 \uc870\uc815\ud569\ub2c8\ub2e4.", "section": "3 \ubc29\ubc95"}, {"figure_path": "https://arxiv.org/html/2412.18450/x3.png", "caption": "Figure 3: \nQualitative examples of 3DGraphLLM performance on the ScanRefer dataset. For each query, we provide an RGB image from the ScanNet dataset showing the selected object, along with a visualization of the RGB point cloud. In the point cloud, green points indicate the points that 3DGraphLLM identified as corresponding to the object from the text query, while the green box highlights the ground truth (GT) box for the query.", "description": "\uadf8\ub9bc 3\uc740 ScanRefer \ub370\uc774\ud130\uc14b\uc5d0\uc11c 3DGraphLLM\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc815\uc131\uc801 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uac01 \uc9c8\uc758\uc5b4\uc5d0 \ub300\ud574 ScanNet \ub370\uc774\ud130\uc14b\uc758 RGB \uc774\ubbf8\uc9c0\uc640 RGB \ud3ec\uc778\ud2b8 \ud074\ub77c\uc6b0\ub4dc \uc2dc\uac01\ud654\ub97c \ud568\uaed8 \uc81c\uacf5\ud569\ub2c8\ub2e4. \ud3ec\uc778\ud2b8 \ud074\ub77c\uc6b0\ub4dc\uc5d0\uc11c \ub179\uc0c9 \uc810\uc740 3DGraphLLM\uc774 \uc9c8\uc758\uc5b4\uc758 \uac1d\uccb4\uc5d0 \ud574\ub2f9\ud55c\ub2e4\uace0 \uc2dd\ubcc4\ud55c \uc810\uc744 \ub098\ud0c0\ub0b4\uace0, \ub179\uc0c9 \uc0c1\uc790\ub294 \uc9c8\uc758\uc5b4\uc5d0 \ub300\ud55c \uc2e4\uc81c \ubc14\uc6b4\ub529 \ubc15\uc2a4(GT \ubc15\uc2a4)\ub97c \uac15\uc870 \ud45c\uc2dc\ud569\ub2c8\ub2e4.", "section": "4.1 \uc2e4\ud5d8 \uacb0\uacfc"}, {"figure_path": "https://arxiv.org/html/2412.18450/x4.png", "caption": "Figure 4: \nComparison of Uni3D object features and VL-SAT semantic edge features for the two nearest neighbors (NNs) based on ground-truth (GT) scene segmentation and Mask3D scene segmentation within the ScanNet training set.\n Left: Uni3D object features are relatively close for GT point clouds and Mask3D point clouds. Center: using the standard approach for selecting NNs to generate VL-SAT features, the features for pairs of Mask3D point clouds differ significantly from those of GT point clouds. Right: after applying a minimum neighbor distance filter for selecting NNs, the VL-SAT features for object pairs from Mask3D instance segmentation align more closely with those from GT instance segmentation.", "description": "\uadf8\ub9bc 4\ub294 ScanNet \ud6c8\ub828 \uc138\ud2b8 \ub0b4\uc5d0\uc11c GT(Ground Truth) \uc7a5\uba74 \ubd84\ud560 \ubc0f Mask3D \uc7a5\uba74 \ubd84\ud560\uc744 \uae30\ubc18\uc73c\ub85c \ub450 \uac1c\uc758 \uac00\uc7a5 \uac00\uae4c\uc6b4 \uc774\uc6c3(NN)\uc5d0 \ub300\ud55c Uni3D \uac1d\uccb4 \ud2b9\uc9d5\uacfc VL-SAT \uc758\ubbf8\uc801 \uac00\uc7a5\uc790\ub9ac \ud2b9\uc9d5\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uc67c\ucabd \ud328\ub110\uc740 GT \ud3ec\uc778\ud2b8 \ud074\ub77c\uc6b0\ub4dc\uc640 Mask3D \ud3ec\uc778\ud2b8 \ud074\ub77c\uc6b0\ub4dc\uc5d0 \ub300\ud574 Uni3D \uac1d\uccb4 \ud2b9\uc9d5\uc774 \uc0c1\ub300\uc801\uc73c\ub85c \uac00\uae5d\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac00\uc6b4\ub370 \ud328\ub110\uc740 VL-SAT \ud2b9\uc9d5\uc744 \uc0dd\uc131\ud558\uae30 \uc704\ud574 NN\uc744 \uc120\ud0dd\ud558\ub294 \ud45c\uc900 \uc811\uadfc \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uba74 Mask3D \ud3ec\uc778\ud2b8 \ud074\ub77c\uc6b0\ub4dc \uc30d\uc758 \ud2b9\uc9d5\uc774 GT \ud3ec\uc778\ud2b8 \ud074\ub77c\uc6b0\ub4dc\uc758 \ud2b9\uc9d5\uacfc \ud06c\uac8c \ub2e4\ub974\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc624\ub978\ucabd \ud328\ub110\uc740 NN\uc744 \uc120\ud0dd\ud558\uae30 \uc704\ud574 \ucd5c\uc18c \uc774\uc6c3 \uac70\ub9ac \ud544\ud130\ub97c \uc801\uc6a9\ud558\uba74 Mask3D \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560\uc5d0\uc11c \uac00\uc838\uc628 \uac1d\uccb4 \uc30d\uc758 VL-SAT \ud2b9\uc9d5\uc774 GT \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560\uc5d0\uc11c \uac00\uc838\uc628 \uac1d\uccb4 \uc30d\uc758 \ud2b9\uc9d5\uacfc \ub354\uc6b1 \uc77c\uce58\ud55c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.3 Ablation Studies. 3D Scene Graph Representation"}, {"figure_path": "https://arxiv.org/html/2412.18450/extracted/6093446/nn_ablation.png", "caption": "Figure 5: \nDependence of inference speed and visual grounding accuracy on the number of nearest neighbors in the object subgraph. This experiment utilizes the RioRefer dataset along with GT instance segmentation.", "description": "\uadf8\ub9bc 5\ub294 \uac1d\uccb4 \ubd80\ubd84 \uadf8\ub798\ud504\uc5d0\uc11c \uac00\uc7a5 \uac00\uae4c\uc6b4 \uc774\uc6c3\uc758 \uc218\uc5d0 \ub530\ub978 \ucd94\ub860 \uc18d\ub3c4\uc640 \uc2dc\uac01\uc801 \uadfc\uac70 \uc815\ud655\ub3c4\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uc2e4\ud5d8\uc5d0\uc11c\ub294 GT \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560\uacfc \ud568\uaed8 RioRefer \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ubcf4\ub2e4 \uc790\uc138\ud788 \uc124\uba85\ud558\uc790\uba74, \uac1d\uccb4\uc758 \ud45c\ud604\uc5d0 \uc0ac\uc6a9\ub418\ub294 \uac00\uc7a5 \uac00\uae4c\uc6b4 \uc774\uc6c3\uc758 \uc218\ub97c \ubcc0\ud654\uc2dc\ud0a4\uba74\uc11c \ucd94\ub860 \uc2dc\uac04\uacfc \uc2dc\uac01\uc801 \uc811\uc9c0(visual grounding) \uc131\ub2a5\uc5d0 \uc5b4\ub5a4 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4.  x\ucd95\uc740 \uac00\uc7a5 \uac00\uae4c\uc6b4 \uc774\uc6c3\uc758 \uc218\uc774\uace0, y\ucd95\uc740 \ucd94\ub860 \uc2dc\uac04(msec)\uacfc \uc815\ud655\ub3c4(Accuracy@0.5)\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. RioRefer \ub370\uc774\ud130\uc14b\uacfc GT \uc778\uc2a4\ud134\uc2a4 \uc138\ubd84\ud654\ub97c \uc0ac\uc6a9\ud55c \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.3 Ablation Studies. 3D Scene Graph Representation"}, {"figure_path": "https://arxiv.org/html/2412.18450/x5.png", "caption": "Figure 6: \nCommon failure cases of 3DGraphLLM related to spatial relationships. Left: In the ScanQA dataset, 3DGraphLLM incorrectly identifies the front/back and left.right directions relative to the observer. Right: In the ScanRefer dataset, 3DGraphLLM confuses left and right. The GT object is highlighted in green, and the 3DGraphLLM prediction is highlighted in red.", "description": "\uadf8\ub9bc 6\uc740 3DGraphLLM\uc774 \uacf5\uac04\uc801 \uad00\uacc4\ub97c \ub2e4\ub8f0 \ub54c \ubc1c\uc0dd\ud558\ub294 \uc77c\ubc18\uc801\uc778 \uc624\ub958 \uc0ac\ub840\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd \uadf8\ub9bc\uc740 ScanQA \ub370\uc774\ud130\uc14b\uc5d0\uc11c 3DGraphLLM\uc774 \uad00\ucc30\uc790\ub97c \uae30\uc900\uc73c\ub85c \uc55e\ub4a4, \uc88c\uc6b0 \ubc29\ud5a5\uc744 \uc798\ubabb \uc2dd\ubcc4\ud55c \uacbd\uc6b0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc624\ub978\ucabd \uadf8\ub9bc\uc740 ScanRefer \ub370\uc774\ud130\uc14b\uc5d0\uc11c 3DGraphLLM\uc774 \uc88c\uc6b0\ub97c \ud63c\ub3d9\ud55c \uacbd\uc6b0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc815\ub2f5 \uac1d\uccb4\ub294 \ub179\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uace0 3DGraphLLM\uc758 \uc608\uce21 \uacb0\uacfc\ub294 \ube68\uac04\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub429\ub2c8\ub2e4.", "section": "A. \uacf5\uac04 \uad00\uacc4 \uc624\ub958 \uc0ac\ub840"}, {"figure_path": "https://arxiv.org/html/2412.18450/x6.png", "caption": "Figure 7: \nFunctional queries about the room and objects to the 3DGraphLLM. Left: 3DGraphLLM is capable of answering questions about functional properties of the room and its room type. Right: 3DGraphLLM is capable of answering questions about the functional properties of objects in a room.", "description": "\uadf8\ub9bc 7\uc740 3DGraphLLM\uc774 \ubc29\uacfc \ubb3c\uccb4\uc5d0 \ub300\ud55c \uae30\ub2a5\uc801 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc67c\ucabd\uc740 3DGraphLLM\uc774 \ubc29\uc758 \uae30\ub2a5\uc801 \uc18d\uc131\uacfc \ubc29 \uc720\ud615\uc5d0 \ub300\ud55c \uc9c8\ubb38\uc5d0 \ub2f5\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc624\ub978\ucabd\uc740 3DGraphLLM\uc774 \ubc29\uc5d0 \uc788\ub294 \ubb3c\uccb4\uc758 \uae30\ub2a5\uc801 \uc18d\uc131\uc5d0 \ub300\ud55c \uc9c8\ubb38\uc5d0 \ub2f5\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc989, \uadf8\ub9bc\uc740 \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc5d0 \uc5c6\ub294 \uc720\ud615\uc758 \uc9c8\ubb38\uc5d0 \ub300\ud574\uc11c\ub3c4 3DGraphLLM\uc774 \uc0c1\uc2dd\uc801\uc778 \uc9c0\uc2dd\uc744 \ud65c\uc6a9\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc8fc\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4.", "section": "4 \uc2e4\ud5d8"}]