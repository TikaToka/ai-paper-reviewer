{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This paper introduced the concept of few-shot learning with language models, a foundation upon which in-context learning (ICL) is built."}, {"fullname_first_author": "Sewon Min", "paper_title": "Rethinking the role of demonstrations: What makes in-context learning work?", "publication_date": "2022-12-07", "reason": "This paper offers a critical analysis of the mechanisms behind ICL, influencing how to select and order examples effectively."}, {"fullname_first_author": "Rishabh Agarwal", "paper_title": "Many-shot in-context learning", "publication_date": "2024-04-11", "reason": "This paper investigates many-shot ICL, directly relevant to the current work's focus on long-context language models and their capacity for handling numerous examples."}, {"fullname_first_author": "Amanda Bertsch", "paper_title": "In-context learning with long-context models: An in-depth exploration", "publication_date": "2024-05-01", "reason": "This paper provides a comprehensive analysis of long-context ICL, which is directly relevant to the current research's exploration of long-context language models."}, {"fullname_first_author": "Johannes von Oswald", "paper_title": "Transformers learn in-context by gradient descent", "publication_date": "2023-07-23", "reason": "This paper offers theoretical insight into the learning mechanisms of transformers, which is essential for understanding how ICL functions with long-context models."}]}