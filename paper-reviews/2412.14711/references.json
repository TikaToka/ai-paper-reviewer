{"references": [{"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-01-06", "reason": "This paper introduced the Mixture-of-Experts (MoE) architecture, which is the foundation of the ReMoE model discussed in the current paper."}, {"fullname_first_author": "Aidan Clark", "paper_title": "Unified scaling laws for routed language models", "publication_date": "2022-07-01", "reason": "This paper presents scaling laws for MoE models, which are important for understanding the performance of ReMoE as the model scales up."}, {"fullname_first_author": "William Fedus", "paper_title": "Switch Transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-01-01", "reason": "This paper introduced Switch Transformers, which is another efficient MoE model that is compared to ReMoE in the current paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper introduced the LLaMA model architecture, which forms the base model of the experiments conducted in the current paper."}, {"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-05-13", "reason": "This paper presents the GQA architecture used in the LLaMA model, which is another key aspect of the experimental setup."}]}