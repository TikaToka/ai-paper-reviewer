<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Parallelized Autoregressive Visual Generation &#183; AI Paper Reviews by AI</title>
<meta name=title content="Parallelized Autoregressive Visual Generation &#183; AI Paper Reviews by AI"><meta name=description content="본 연구는 토큰 의존성을 고려한 병렬화 전략을 통해 자동 회귀 시각적 생성의 속도를 최대 9.5배까지 향상시켰습니다."><meta name=keywords content="Computer Vision,Image Generation,🏢 Peking University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="Parallelized Autoregressive Visual Generation"><meta property="og:description" content="본 연구는 토큰 의존성을 고려한 병렬화 전략을 통해 자동 회귀 시각적 생성의 속도를 최대 9.5배까지 향상시켰습니다."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-19T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-19T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Image Generation"><meta property="article:tag" content="🏢 Peking University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/cover.png"><meta name=twitter:title content="Parallelized Autoregressive Visual Generation"><meta name=twitter:description content="본 연구는 토큰 의존성을 고려한 병렬화 전략을 통해 자동 회귀 시각적 생성의 속도를 최대 9.5배까지 향상시켰습니다."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Parallelized Autoregressive Visual Generation","headline":"Parallelized Autoregressive Visual Generation","abstract":"본 연구는 토큰 의존성을 고려한 병렬화 전략을 통해 자동 회귀 시각적 생성의 속도를 최대 9.5배까지 향상시켰습니다.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.15119\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-12-19T00:00:00\u002b00:00","datePublished":"2024-12-19T00:00:00\u002b00:00","dateModified":"2024-12-19T00:00:00\u002b00:00","keywords":["Computer Vision","Image Generation","🏢 Peking University"],"mainEntityOfPage":"true","wordCount":"3557"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.15119/cover_hu7846429860246751199.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.15119/>Parallelized Autoregressive Visual Generation</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Parallelized Autoregressive Visual Generation</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-19T00:00:00+00:00>19 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>3557 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">17 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.15119/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.15119/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🤗 Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/image-generation/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Image Generation
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-peking-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">🏢 Peking University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#parallel-visual-gen>Parallel Visual Gen</a></li><li><a href=#token-dependency>Token Dependency</a></li><li><a href=#non-local-parallelism>Non-local Parallelism</a></li><li><a href=#ablation-experiments>Ablation Experiments</a></li><li><a href=#future-of-autoregressive>Future of Autoregressive</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#parallel-visual-gen>Parallel Visual Gen</a></li><li><a href=#token-dependency>Token Dependency</a></li><li><a href=#non-local-parallelism>Non-local Parallelism</a></li><li><a href=#ablation-experiments>Ablation Experiments</a></li><li><a href=#future-of-autoregressive>Future of Autoregressive</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.15119</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Yuqing Wang et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>🤗 2024-12-23</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.15119 target=_self role=button>↗ arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.15119 target=_self role=button>↗ Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/parallelized-autoregressive-visual-generation target=_self role=button>↗ Papers with Code</a></p><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>기존의 자동 회귀 시각 생성 모델은 토큰을 순차적으로 생성하기 때문에 속도가 느리다는 문제점이 있습니다. 이는 특히 고해상도 이미지나 비디오 생성에서 더욱 두드러집니다. 본 논문에서는 이러한 문제를 해결하기 위해 새로운 접근 방식을 제안합니다.</p><p>본 논문에서는 <strong>토큰 간의 의존성을 분석</strong>하여 병렬 처리가 가능한 토큰과 순차 처리가 필요한 토큰을 구분하는 알고리즘을 제시합니다. 이를 통해 기존 모델의 구조를 변경하지 않고도 <strong>속도를 최대 9.5배까지 향상</strong>시키면서 <strong>생성 품질은 유지</strong>하거나 <strong>최소한으로 저하</strong>시키는 결과를 얻었습니다. ImageNet 및 UCF-101 데이터셋을 사용한 실험 결과를 통해 제안된 방법의 효과를 검증했습니다.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-d7a8ce6acf4d997304ceefff533b83ed></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-d7a8ce6acf4d997304ceefff533b83ed",{strings:[" 토큰 의존성 분석을 통해 병렬 처리 가능한 토큰과 순차 처리가 필요한 토큰을 구분하는 전략을 제시했습니다. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-b7c12ce415f02062e65a157b2f27ab31></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-b7c12ce415f02062e65a157b2f27ab31",{strings:[" ImageNet과 UCF-101 데이터셋에서 이미지 및 비디오 생성 작업 모두에서 속도 향상과 품질 유지를 확인했습니다. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-363e01dcc53b14532cfe3503da8e1247></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-363e01dcc53b14532cfe3503da8e1247",{strings:[" 기존의 자동 회귀 모델에 손쉽게 통합 가능한 방식으로, 모델의 복잡성을 증가시키지 않습니다. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>이 논문은 <strong>자동 회귀 시각적 생성의 속도를 크게 향상시키는 동시에 품질을 유지하는</strong> 새로운 방법을 제시하여, 실제 응용 프로그램에서 자동 회귀 모델의 실용성을 높입니다. <strong>병렬 처리 전략</strong>과 <strong>토큰 종속성</strong>에 대한 분석은 효율적인 시각적 생성 모델링에 대한 귀중한 통찰력을 제공하며, <strong>미래 연구에 영감을 줄</strong> 것입니다. 특히 <strong>다양한 토큰화 방법 및 시각 영역에 대한 호환성</strong>은 광범위한 연구 분야에 적용 가능성을 시사합니다.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.15119/x1.png alt></figure></p><blockquote><p>🔼 그림 1은 서로 다른 병렬 생성 전략을 비교한 것입니다. 두 전략 모두 초기 토큰 [1, 2, 3, 4]를 순차적으로 생성한 다음, [5a-5d]에서 [6a-6d]로, [7a-7d] 순으로 단계별로 여러 토큰을 병렬로 생성합니다. (a)는 본 연구에서 제안하는 방법으로, 비국소 영역에 걸쳐 약하게 종속적인 토큰들을 병렬로 생성하여 일관된 패턴과 지역적 세부 정보를 유지합니다. (b)는 단순한 방법으로, 국소 영역 내에서 강하게 종속적인 토큰들을 동시에 생성합니다. 강하게 상관관계가 있는 토큰들을 독립적으로 샘플링하면 불일치한 생성과 패턴이 깨지는 현상(예: 왜곡된 호랑이 얼굴과 조각난 얼룩말 줄무늬)이 발생할 수 있습니다.</p><details><summary>read the caption</summary>Figure 1: Comparison of different parallel generation strategies. Both strategies generate initial tokens [1,2,3,4] sequentially then generate multiple tokens in parallel per step, following the order [5a-5d] to [6a-6d] to [7a-7d], etc. (a) Our approach generates weakly dependent tokens across non-local regions in parallel, preserving coherent patterns and local details. (b) The naive method generates strongly dependent tokens within local regions simultaneously, while independent sampling for strongly correlated tokens can cause inconsistent generation and disrupted patterns, such as distorted tiger faces and fragmented zebra stripes.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Params</th><th>Layers</th><th>Hidden</th><th>Heads</th></tr></thead><tbody><tr><td>PAR-L</td><td>343M</td><td>24</td><td>1024</td><td>16</td></tr><tr><td>PAR-XL</td><td>775M</td><td>36</td><td>1280</td><td>20</td></tr><tr><td>PAR-XXL</td><td>1.4B</td><td>48</td><td>1536</td><td>24</td></tr><tr><td>PAR-3B</td><td>3.1B</td><td>24</td><td>3200</td><td>32</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 1은 논문에서 제안하는 병렬화된 자기회귀 시각 생성 모델(PAR)의 크기와 아키텍처 구성을 보여줍니다. PAR 모델은 네 가지 크기(PAR-L, PAR-XL, PAR-XXL, PAR-3B)로 구성되며, 각 크기는 파라미터 수, 레이어 수, 히든 차원, 헤드 수가 다릅니다. 이러한 구성은 이전 연구 [36, 51, 32, 47]의 설정을 따릅니다. 이는 모델의 성능과 효율성에 대한 실험적 비교 분석을 위한 기준을 설정하고, 제안된 모델의 성능을 평가하기 위한 기준 모델을 제시합니다.</p><details><summary>read the caption</summary>Table 1: Model sizes and architecture configurations of PAR. The configurations are following previous works [36, 51, 32, 47].</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Parallel Visual Gen<div id=parallel-visual-gen class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#parallel-visual-gen aria-label=Anchor>#</a></span></h4><p>본 논문의 &ldquo;Parallel Visual Gen&rdquo; 부분은 기존의 순차적인 autoregressive 방식의 시각적 생성 모델의 속도 저하 문제를 해결하기 위한 병렬화 전략을 제시합니다. <strong>핵심 아이디어는 토큰 간의 의존성을 분석하여 약한 의존성을 가진 토큰들은 병렬적으로 생성하고, 강한 의존성을 가진 토큰들은 순차적으로 생성하는 것입니다.</strong> 이를 위해 <strong>공간적으로 멀리 떨어진 토큰들끼리는 의존성이 약하다는 점을 이용</strong>하여, 특정 지역의 초기 토큰들을 순차적으로 생성한 후, 공간적으로 멀리 떨어진 영역들의 같은 위치의 토큰들을 병렬적으로 생성하는 방식을 제안합니다. 이러한 병렬화 전략은 모델의 아키텍처나 토크나이저를 변경하지 않고도 기존 autoregressive 모델에 통합될 수 있으며, <strong>ImageNet 및 UCF-101 데이터셋에서의 실험 결과, 속도 향상과 생성 품질 유지를 동시에 달성</strong>하는 것을 보여줍니다. <strong>특히, 토큰 의존성 분석을 통해 병렬화 전략의 효율성과 안정성을 확보</strong>하는 점이 주목할 만합니다. 이는 시각적 생성 모델의 효율성을 크게 향상시키는 동시에, <strong>다양한 시각적 데이터와 토크나이저에 대한 유연성을 유지</strong>하는 데 기여할 것으로 예상됩니다.</p><h4 class="relative group">Token Dependency<div id=token-dependency class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#token-dependency aria-label=Anchor>#</a></span></h4><p>토큰 의존성은 자연어 처리와 컴퓨터 비전 분야 모두에서 중요한 개념입니다. 특히, <strong>자동 회귀 모델</strong>에서 토큰 간의 순서와 상호 의존성은 모델의 성능과 효율성에 큰 영향을 미칩니다. 이 논문에서 제시된 병렬화된 자동 회귀 시각적 생성 방법은 <strong>토큰 간의 의존성을 분석하여 효율적인 병렬화 전략</strong>을 제시합니다. 강한 의존성을 가진 토큰은 순차적으로 생성하고, 약한 의존성을 가진 토큰은 병렬적으로 생성하여 생성 속도를 높입니다. 이는 <strong>시각적 토큰의 공간적 거리</strong>와 밀접한 관련이 있습니다. 즉, 공간적으로 멀리 떨어진 토큰은 상호 의존성이 약하기 때문에 병렬적으로 처리할 수 있지만, 인접한 토큰은 강한 의존성으로 인해 순차적 생성이 필요합니다. 따라서, <strong>공간적 거리</strong>를 고려한 토큰 그룹화 전략이 중요하며, 초기 토큰은 전역 구조를 확립하기 위해 순차적으로 생성되어야 합니다. 이러한 접근 방식은 모델의 복잡성을 높이지 않으면서도 생성 속도를 개선하는 효율적인 방법입니다. <strong>토큰 의존성 분석</strong>은 병렬화 가능성을 정확하게 판단하고, 모델의 성능 저하를 최소화하는 데 중요한 역할을 합니다.</p><h4 class="relative group">Non-local Parallelism<div id=non-local-parallelism class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#non-local-parallelism aria-label=Anchor>#</a></span></h4><p>비국소적 병렬 처리 개념은 <strong>공간적으로 멀리 떨어진 토큰들 간의 의존성이 약하다는 점</strong>을 이용하여 병렬 처리의 효율성을 높이는 전략입니다. 이러한 전략은 순차적인 처리 방식의 한계를 극복하고, <strong>자동 회귀 모델의 속도를 크게 향상</strong>시키는 데 목표를 두고 있습니다. 단순히 모든 토큰을 동시에 생성하는 것이 아니라, <strong>토큰 간의 의존성을 분석</strong>하여, 상호 의존성이 적은 토큰들을 병렬적으로 처리함으로써, 계산 비용을 절감하고 처리 속도를 개선하는 데 초점이 맞춰져 있습니다. 이는 단순히 처리 속도 개선뿐 아니라, <strong>전체 이미지나 비디오의 일관성과 질적 요소</strong>까지 고려해야 하는 어려운 과제를 해결하기 위한 <strong>핵심적인 전략</strong>이라고 볼 수 있습니다. <strong>국소적 영역 내에서의 강한 상관관계</strong>는 여전히 순차적 처리 방식을 유지해야 함을 시사하며, <strong>글로벌 구조의 일관성 유지를 위한 전략</strong>과 더불어 <strong>모델의 복잡도 증가 없이 효율적인 병렬화</strong>를 달성하고자 하는 시도입니다. 이러한 접근 방식의 효과는 실험 결과를 통해 검증되어야 하지만, <strong>자동 회귀 모델의 한계를 극복</strong>하는 데 있어 중요한 발전으로 평가될 수 있습니다.</p><h4 class="relative group">Ablation Experiments<div id=ablation-experiments class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#ablation-experiments aria-label=Anchor>#</a></span></h4><p>본 논문의 &ldquo;Ablation Experiments&rdquo; 섹션은 모델의 성능에 기여하는 각 구성 요소의 중요성을 밝히는 데 중점을 둡니다. 이를 통해 <strong>모델 설계의 핵심적인 통찰력</strong>을 얻을 수 있습니다. 구체적으로는, 병렬화 전략의 효과, 토큰 의존성 처리, 그리고 어텐션 메커니즘의 선택 등이 실험적으로 검증됩니다. <strong>각 구성 요소의 제거 또는 변형을 통해 성능 변화를 측정</strong>하여, 어떤 요소가 성능 향상에 가장 크게 기여하는지, 그리고 어떤 요소들이 서로 상호 작용하는지를 분석합니다. 이러한 분석은 모델의 강점과 약점을 파악하고, 향후 연구 방향을 제시하는 데 중요한 역할을 합니다. 특히, <strong>병렬화 전략의 효율성과 정확성 간의 절충 관계</strong>를 분석하고, 최적의 병렬화 수준을 결정하는 데 중요한 정보를 제공할 것으로 예상됩니다. 또한, <strong>토큰 간의 의존성을 효과적으로 처리하는 방법</strong>에 대한 실험적 증거를 제시함으로써, 자연어 처리 및 컴퓨터 비전 분야에 시사하는 바가 클 것입니다.</p><h4 class="relative group">Future of Autoregressive<div id=future-of-autoregressive class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-of-autoregressive aria-label=Anchor>#</a></span></h4><p>자동 회귀 모델의 미래는 <strong>매우 밝습니다</strong>. 이 모델들은 이미 자연어 처리와 이미지 생성 분야에서 괄목할 만한 성과를 거두었지만, 여전히 개선의 여지가 많습니다. <strong>병렬화</strong>는 속도를 높이는 중요한 방향이며, 이를 통해 실시간 응용 분야에서의 활용이 더욱 확대될 것입니다. <strong>토큰 의존성</strong>을 효과적으로 관리하는 새로운 기법의 개발 또한 필수적입니다. <strong>다양한 모달리티</strong> (텍스트, 이미지, 비디오 등)를 통합하는 통합 모델의 연구도 중요한 과제입니다. <strong>효율적인 토큰화 기법</strong>은 연산량을 줄이고 모델의 확장성을 높이는 데 기여할 것입니다. 마지막으로, <strong>모델의 설명력을 높이는 연구</strong>는 신뢰도 향상과 안전한 응용을 위해 중요합니다. 이러한 노력들이 결합되어 더욱 강력하고 효율적인 자동 회귀 모델이 탄생할 것으로 예상됩니다.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.15119/x2.png alt></figure></p><blockquote><p>🔼 그림 2는 제안된 병렬 생성 방법(PAR)과 기존의 순차적 자기회귀 생성 방법(LlamaGen [47])의 비교 결과를 보여줍니다. PAR은 LlamaGen에 비해 3.6배에서 최대 9.5배까지 속도 향상을 달성했으며, 생성 품질은 유사하게 유지했습니다. 구체적으로, 이미지 생성 시간이 LlamaGen의 12.41초에서 PAR-4x의 경우 3.46초, PAR-16x의 경우 1.31초로 단축되었습니다. 이러한 측정은 단일 A100 GPU에서 배치 크기 1로 수행되었습니다. 그림에는 각 방법으로 생성된 이미지들의 시각적 비교 결과가 나란히 제시되어 있습니다.</p><details><summary>read the caption</summary>Figure 2: Visualization comparison of our parallel generation and traditional autoregressive generation (LlamaGen [47]). Our approach (PAR) achieves 3.6-9.5×\times× speedup over LlamaGen with comparable quality, reducing the generation time from 12.41s to 3.46s (PAR-4×\times×) and 1.31s (PAR-16×\times×) per image. Time measurements are conducted with a batch size of 1 on a single A100 GPU.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.15119/x3.png alt></figure></p><blockquote><p>🔼 그림 3은 제안된 비국소적 병렬 생성 과정을 보여줍니다. 먼저, 점선으로 구분된 각 영역에 대해 초기 토큰(1-4)을 순차적으로 생성하여 전역 구조를 확립합니다. 그런 다음, 서로 다른 영역에서 정렬된 위치(예: 5a-5d)에 대해 병렬 생성을 수행하고, 다음 정렬된 위치(6a-6d, 7a-7d 등)로 이동하여 병렬 생성을 계속합니다. 같은 숫자는 같은 단계에서 생성된 토큰을 나타내고, 알파벳 접미사(a,b,c,d)는 서로 다른 영역을 나타냅니다.</p><details><summary>read the caption</summary>Figure 3: Illustration of our non-local parallel generation process. Stage 1: sequential generation of initial tokens (1-4) for each region (separated by dotted lines) to establish global structure. Stage 2: parallel generation at aligned positions across different regions (e.g., 5a-5d), then moving to next aligned positions (6a-6d, 7a-7d, etc.) for parallel generation. Same numbers indicate tokens generated in the same step, and letter suffix (a,b,c,d) denotes different regions .</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.15119/x4.png alt></figure></p><blockquote><p>🔼 그림 4는 본 논문에서 제안하는 병렬화된 자기회귀적 시각 생성 프레임워크를 개괄적으로 보여줍니다. (a)는 모델 구현을 보여줍니다. 모델은 처음에 토큰 [1, 2, 3, 4]를 순차적으로 생성한 다음, 학습 가능한 토큰 [M1, M2, M3]을 사용하여 병렬 예측 모드로 전환합니다. (b)는 제안된 병렬 예측 방식(왼쪽)과 기존의 단일 토큰 예측 방식(오른쪽) 간의 가시적 컨텍스트 비교를 보여줍니다. 색상이 있는 셀은 생성 중에 사용 가능한 컨텍스트를 나타냅니다. 기존의 자기회귀적 방식에서는 토큰 6d를 예측할 때, 6a~6c를 포함한 이전 토큰에 모두 접근할 수 있습니다. 하지만 전체 어텐션 없이 병렬 방식을 사용하면 각 토큰(예: 6b)이 이전 그룹의 같은 위치까지의 토큰(예: 5b까지)만 볼 수 있습니다. 따라서 본 논문에서는 그룹 단위 전체 어텐션을 사용하여 이전 그룹 전체에 접근할 수 있도록 합니다.</p><details><summary>read the caption</summary>Figure 4: Overview of our parallel autoregressive generation framework. (a) Model implementation. The model first generates initial tokens sequentially [1,2,3,4], then uses learnable tokens [M1,M2,M3] to help transition into parallel prediction mode. (b) Comparison of visible context between our parallel prediction approach (left) and traditional single-token prediction (right). The colored cells indicate available context during generation. In traditional AR, when predicting token 6⁢d6𝑑6d6 italic_d, the model can access all previous tokens including 6⁢a−6⁢c6𝑎6𝑐6a-6c6 italic_a - 6 italic_c. Without full attention, our parallel approach would limit each token (e.g., 6⁢b6𝑏6b6 italic_b) to only see tokens up to the same position in the previous group (e.g., up to 5⁢b5𝑏5b5 italic_b). We enable group-wise full attention to allow access to the entire previous group.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.15119/x5.png alt></figure></p><blockquote><p>🔼 그림 5는 세 가지 병렬 생성 전략의 비교 결과를 보여줍니다. 상단은 순차적 초기 토큰 생성 후 병렬 원거리 토큰 예측을 결합한 본 논문의 방법을 보여줍니다. 이 방법은 고품질의 일관된 이미지를 생성합니다. 중간은 순차적 초기 토큰 생성 없이 직접 병렬 예측을 수행한 결과를 보여줍니다. 이 경우, 불일치하는 전역 구조가 나타납니다. 하단은 인접 토큰의 병렬 예측 결과를 보여줍니다. 이 경우 국소적인 패턴이 왜곡되고 세부 정보가 손상됩니다.</p><details><summary>read the caption</summary>Figure 5: Qualitative comparison of parallel generation strategies. Top: Our method with sequential initial tokens followed by parallel distant token prediction produces high-quality and coherent images. Middle: Direct parallel prediction without sequential initial tokens leads to inconsistent global structures. Bottom: Parallel prediction of adjacent tokens results in distorted local patterns and broken details.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.15119/x6.png alt></figure></p><blockquote><p>🔼 그림 6은 ImageNet [9] 데이터셋의 다양한 카테고리에 대해 PAR-4x 모델을 사용하여 생성한 이미지 결과를 추가적으로 보여줍니다. PAR-4x는 제안된 병렬화된 자기회귀 방식을 4배의 병렬 처리를 통해 수행한 모델입니다. 이 그림은 다양한 물체와 장면을 묘사하는 이미지들을 보여주어 모델의 시각적 생성 능력을 보다 자세히 보여줍니다. 이미지들의 다양성과 세부 묘사의 정확도는 모델의 성능을 시각적으로 평가하는 데 도움이 됩니다.</p><details><summary>read the caption</summary>Figure 6: Additional image generation results of PAR-4×\times× across different ImageNet [9] categories.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.15119/x7.png alt></figure></p><blockquote><p>🔼 그림 7은 본 논문에서 제안하는 병렬화된 자기회귀 시각 생성 모델(PAR)의 16배속 병렬 처리(PAR-16x)를 사용하여 생성한 이미지 결과를 보여줍니다. 다양한 ImageNet [9] 카테고리에 걸쳐 생성된 이미지들의 다양성과 화질을 보여주는 추가적인 시각적 증거를 제공합니다. 이미지들은 모델의 성능과 다양한 카테고리에 대한 적응력을 시각적으로 보여줍니다.</p><details><summary>read the caption</summary>Figure 7: Additional image generation results of PAR-16×\times× across different ImageNet [9] categories.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.15119/x8.png alt></figure></p><blockquote><p>🔼 그림 8은 제안된 병렬 오토 회귀 비디오 생성 방법의 성능을 UCF-101 데이터셋 [44]을 사용하여 보여줍니다. 각 행은 17프레임 시퀀스에서 샘플링된 프레임들을 보여주며, 해상도는 128x128입니다. PAR-1x는 기본적인 토큰 단위 생성 방법을, PAR-4x는 4개의 토큰을 병렬로 생성하는 방법을, PAR-16x는 16개의 토큰을 병렬로 생성하는 방법을 각각 나타냅니다. 서로 다른 동작 범주에 걸쳐 생성된 비디오의 시각적 결과를 비교하여 모델의 성능을 평가할 수 있습니다.</p><details><summary>read the caption</summary>Figure 8: Video generation results on UCF-101 [44]. Each row shows sampled frames from a 17-frame sequence at 128×128 resolution, generated by PAR-1×\times×, PAR-4×\times×, and PAR-16×\times× respectively across different action categories.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.15119/x9.png alt></figure></p><blockquote><p>🔼 그림 9는 토큰의 조건부 엔트로피 맵을 시각화한 것입니다. 각 맵은 참조 토큰(파란색 정사각형)을 조건으로 했을 때 모든 토큰의 조건부 엔트로피를 보여줍니다. 더 어두운 빨간색은 더 낮은 조건부 엔트로피를 나타내며, 따라서 참조 토큰과의 강한 의존성을 나타냅니다. 시각화는 토큰이 공간적으로 가까운 이웃과는 강한 의존성을, 먼 지역과는 약한 의존성을 보임을 보여줍니다. 즉, 가까운 토큰일수록 서로의 영향을 많이 받고, 먼 토큰일수록 서로의 영향을 적게 받는다는 것을 시각적으로 보여주는 그림입니다.</p><details><summary>read the caption</summary>Figure 9: Visualization of token conditional entropy maps. Each map shows the conditional entropy of all tokens when conditioned on a reference token (blue square). Darker red indicates lower conditional entropy and thus stronger dependency with the reference token. The visualization shows that tokens exhibit strong dependencies with their spatial neighbors and weak dependencies with distant regions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.15119/x10.png alt></figure></p><blockquote><p>🔼 그림 10은 서로 다른 순서(제안된 순서와 래스터 스캔 순서)로 병렬 및 순차적 생성 간의 조건부 엔트로피 차이를 보여줍니다. (a)(d)는 제안된 순서와 래스터 스캔 순서에 대한 병렬(4개 토큰) 생성 전략과 (b)(e)는 순차적 생성 전략을 보여줍니다. 숫자는 각 순서의 생성 단계를 나타냅니다. (c)(f)는 각 순서에 대해 순차적 생성에서 병렬 생성으로 전환할 때 조건부 엔트로피 증가를 시각화합니다. 여기서 어두운 빨간색은 더 큰 엔트로피 증가와 따라서 더 높은 예측 난이도를 나타냅니다. 두 순서 모두 처음 4개의 토큰을 순차적으로 생성합니다(엔트로피 맵의 흰색 영역으로 표시). 서로 다른 공간 블록의 토큰을 병렬로 생성하는 제안된 순서는 연속적인 토큰을 동시에 생성하는 래스터 스캔 순서보다 엔트로피 증가가 더 작아 공간 블록 간의 병렬 생성이 인접 토큰을 동시에 생성하는 것보다 예측 난이도가 더 낮음을 나타냅니다.</p><details><summary>read the caption</summary>Figure 10: Conditional entropy differences between parallel and sequential generation in different orders. (a)(d) show parallel (4 tokens) generation strategies and (b)(e) show sequential generation strategies for our proposed order and raster scan order respectively. Numbers indicate generation step in each order. (c)(f) visualize the conditional entropy increase when switching from sequential to parallel generation for each order, where darker red indicates larger entropy increase and thus higher prediction difficulty. Both orders generate the first four tokens sequentially (shown as white regions in entropy maps). Our proposed order that generates tokens from different spatial blocks in parallel shows smaller entropy increases compared to raster scan order that generates consecutive tokens simultaneously, indicating parallel generation across spatial blocks introduces less prediction difficulty than generating adjacent tokens simultaneously.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Type</th><th>Model</th><th>#Para.</th><th>FID↓</th><th>IS↑</th><th>Precision↑</th><th>Recall↑</th><th>Steps</th><th>Time(s)↓</th></tr></thead><tbody><tr><td>GAN</td><td>BigGAN [3]</td><td>112M</td><td>6.95</td><td>224.5</td><td>0.89</td><td>0.38</td><td>1</td><td>-</td></tr><tr><td></td><td>GigaGAN [19]</td><td>569M</td><td>3.45</td><td>225.5</td><td>0.84</td><td>0.61</td><td>1</td><td>-</td></tr><tr><td></td><td>StyleGan-XL [40]</td><td>166M</td><td>2.30</td><td>265.1</td><td>0.78</td><td>0.53</td><td>1</td><td>0.08</td></tr><tr><td>Diffusion</td><td>ADM [10]</td><td>554M</td><td>10.94</td><td>101.0</td><td>0.69</td><td>0.63</td><td>250</td><td>44.68</td></tr><tr><td></td><td>CDM [16]</td><td>-</td><td>4.88</td><td>158.7</td><td>-</td><td>-</td><td>8100</td><td>-</td></tr><tr><td></td><td>LDM-4 [38]</td><td>400M</td><td>3.60</td><td>247.7</td><td>-</td><td>-</td><td>250</td><td>-</td></tr><tr><td></td><td>DiT-XL/2 [34]</td><td>675M</td><td>2.27</td><td>278.2</td><td>0.83</td><td>0.57</td><td>250</td><td>11.97</td></tr><tr><td>Mask</td><td>MaskGIT [5]</td><td>227M</td><td>6.18</td><td>182.1</td><td>0.80</td><td>0.51</td><td>8</td><td>0.13</td></tr><tr><td>VAR</td><td>VAR-d30 [49]</td><td>2B</td><td>1.97</td><td>334.7</td><td>0.81</td><td>0.61</td><td>10</td><td>0.27</td></tr><tr><td>MAR</td><td>MAR [25]</td><td>943M</td><td>1.55</td><td>303.7</td><td>0.81</td><td>0.62</td><td>64</td><td>28.24</td></tr><tr><td>AR</td><td>VQGAN [11]</td><td>227M</td><td>18.65</td><td>80.4</td><td>0.78</td><td>0.26</td><td>256</td><td>5.05</td></tr><tr><td></td><td>VQGAN [11]</td><td>1.4B</td><td>15.78</td><td>74.3</td><td>-</td><td>-</td><td>256</td><td>5.05</td></tr><tr><td></td><td>VQGAN-re [11]</td><td>1.4B</td><td>5.20</td><td>280.3</td><td>-</td><td>-</td><td>256</td><td>6.38</td></tr><tr><td></td><td>ViT-VQGAN [64]</td><td>1.7B</td><td>4.17</td><td>175.1</td><td>-</td><td>-</td><td>1024</td><td>>6.38</td></tr><tr><td></td><td>ViT-VQGAN-re [64]</td><td>1.7B</td><td>3.04</td><td>227.4</td><td>-</td><td>-</td><td>1024</td><td>>6.38</td></tr><tr><td></td><td>RQTran. [23]</td><td>3.8B</td><td>7.55</td><td>134.0</td><td>-</td><td>-</td><td>256</td><td>5.58</td></tr><tr><td></td><td>RQTran.-re [23]</td><td>3.8B</td><td>3.80</td><td>323.7</td><td>-</td><td>-</td><td>256</td><td>5.58</td></tr><tr><td></td><td>LlamaGen-L [47]</td><td>343M</td><td>3.07</td><td>256.1</td><td>0.83</td><td>0.52</td><td>576</td><td>12.58</td></tr><tr><td></td><td>LlamaGen-XL [47]</td><td>775M</td><td>2.62</td><td>244.1</td><td>0.80</td><td>0.57</td><td>576</td><td>18.66</td></tr><tr><td></td><td>LlamaGen-XXL [47]</td><td>1.4B</td><td>2.34</td><td>253.9</td><td>0.80</td><td>0.59</td><td>576</td><td>24.91</td></tr><tr><td></td><td>LlamaGen-3B [47]</td><td>3.1B</td><td>2.18</td><td>263.3</td><td>0.81</td><td>0.58</td><td>576</td><td>12.41</td></tr><tr><td>AR</td><td>PAR-L-4×</td><td>343M</td><td>3.76</td><td>218.9</td><td>0.84</td><td>0.50</td><td>147</td><td>3.38</td></tr><tr><td></td><td>PAR-XL-4×</td><td>775M</td><td>2.61</td><td>259.2</td><td>0.82</td><td>0.56</td><td>147</td><td>4.94</td></tr><tr><td></td><td>PAR-XXL-4×</td><td>1.4B</td><td>2.35</td><td>263.2</td><td>0.82</td><td>0.57</td><td>147</td><td>6.84</td></tr><tr><td></td><td>PAR-3B-4×</td><td>3.1B</td><td>2.29</td><td>255.5</td><td>0.82</td><td>0.58</td><td>147</td><td>3.46</td></tr><tr><td></td><td>PAR-XXL-16×</td><td>1.4B</td><td>3.02</td><td>270.6</td><td>0.81</td><td>0.56</td><td>51</td><td>2.28</td></tr><tr><td></td><td>PAR-3B-16×</td><td>3.1B</td><td>2.88</td><td>262.5</td><td>0.82</td><td>0.56</td><td>51</td><td>1.31</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 2는 ImageNet 데이터셋을 사용하여 256x256 해상도의 이미지 생성 작업에 대한 다양한 모델들의 성능을 비교 분석한 표입니다. FID(Fréchet Inception Distance), IS(Inception Score), 정밀도, 재현율, 생성에 필요한 단계 수, 그리고 생성 시간을 측정하여 모델들의 이미지 생성 품질과 효율성을 평가합니다. &lsquo;↓&lsquo;는 값이 낮을수록 좋고, &lsquo;↑&lsquo;는 값이 높을수록 좋다는 것을 의미합니다. -re는 rejection sampling 기법을 사용했음을 나타내고, PAR-4x와 PAR-16x는 각각 한 번에 4개와 16개의 토큰을 병렬로 생성하는 PAR 모델의 변형을 의미합니다. 즉, 본 표는 제안된 PAR 모델의 다양한 설정과 기존의 다른 이미지 생성 모델들을 비교하여 성능을 정량적으로 보여주고 있습니다.</p><details><summary>read the caption</summary>Table 2: Class-conditional image generation on ImageNet 256×\times×256 benchmark. “↓↓\downarrow↓” or “↑↑\uparrow↑” indicate lower or higher values are better. “-re” means using rejection sampling. PAR-4×\times× and PAR-16×\times× means generating 4 and 16 tokens per step in parallel, respectively.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>FID↓</th><th>IS↑</th><th>steps↓</th></tr></thead><tbody><tr><td>w/o</td><td>3.67</td><td>221.36</td><td>144</td></tr><tr><td>w</td><td><strong>2.61</strong></td><td>259.17</td><td>147</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 3은 UCF-101 벤치마크에서 클래스 조건부 비디오 생성 방법을 비교한 표입니다. FVD는 비디오 생성 품질을 측정하는 지표로, 값이 낮을수록 성능이 좋음을 의미합니다. PAR-1x는 본 논문에서 제안하는 방법의 토큰 단위 순차적 생성 기준 방식이고, PAR-4x와 PAR-16x는 병렬 생성 방식으로, 생성 속도 향상 비율이 다릅니다. 이 표는 경쟁력 있는 FVD 점수를 달성하면서 생성 단계와 처리 시간을 크게 줄인 병렬 생성 방식의 효과를 보여줍니다.</p><details><summary>read the caption</summary>Table 3: Comparison of class-conditional video generation methods on UCF-101 benchmark. FVD measures generation quality, where lower values (↓↓\downarrow↓) indicate better performance. PAR-1×\times× represents our token-by-token baseline, while PAR-4×\times× and PAR-16×\times× indicate our parallel generation variants with different speedup ratios, achieving competitive FVD scores with significantly reduced generation steps and wall-clock time.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>n</th><th>FID ↓</th><th>IS ↑</th><th>steps ↓</th></tr></thead><tbody><tr><td>1</td><td><strong>2.34</strong></td><td>253.90</td><td>576</td></tr><tr><td>4</td><td>2.35</td><td>263.24</td><td>147</td></tr><tr><td>16</td><td>3.02</td><td>270.57</td><td>51</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 초기 토큰 순차 생성의 중요성을 보여줍니다. 초기 토큰들을 순차적으로 생성하는 것이 FID(Fréchet Inception Distance) 점수를 1.06만큼 향상시키는 반면, 생성 단계 수는 거의 변화가 없음을 보여줍니다. 즉, 이미지 생성의 전반적인 품질을 크게 개선하면서도 효율성을 유지할 수 있음을 시사합니다. 이는 모델이 전역 구조를 잘 파악하고 일관성 있는 이미지를 생성하는 데 초기 토큰의 순차적 생성이 중요함을 나타냅니다.</p><details><summary>read the caption</summary>(a) Importance of initial sequential token generation. Sequential generation of initial tokens improves FID by 1.06 with negligible step increase.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>attn</th><th>FID ↓</th><th>IS ↑</th><th>steps ↓</th></tr></thead><tbody><tr><td>causal</td><td>3.64</td><td>228.08</td><td>147</td></tr><tr><td>full</td><td><strong>2.61</strong></td><td>259.17</td><td>147</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 4(b)는 PAR-XXL 모델에서 병렬로 예측되는 토큰의 수(n)에 따른 성능 변화를 보여줍니다. n=1은 기존의 토큰 단위 순차적 생성 방식(baseline)을 나타냅니다. n=4로 설정하면 생성 단계가 4배 줄어들고 FID 점수는 2.35에서 2.34로 거의 변화가 없음을 보여줍니다. 반면에, n=16으로 설정하면 생성 단계는 11.3배 감소하지만 FID 점수는 0.67 증가하는 것을 확인할 수 있습니다. 이는 병렬 생성의 수준을 높일수록 생성 속도는 빨라지지만, 이미지 품질 저하 가능성이 존재함을 시사합니다.</p><details><summary>read the caption</summary>(b) Number of parallel predicted tokens (PAR-XXL). n=1 is the token-by-token baseline. n=4 reduces steps by 4×\times× with similar FID (2.35 vs. 2.34), while n=16 reduces steps by 11.3×\times× at the cost of 0.67 FID.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>order</th><th>pattern</th><th>FID↓</th><th>IS↑</th><th>steps↓</th></tr></thead><tbody><tr><td>raster</td><td>one</td><td>2.62</td><td>244.08</td><td>576</td></tr><tr><td>distant</td><td>one</td><td>2.64</td><td>262.72</td><td>576</td></tr><tr><td>raster</td><td>multi</td><td>5.64</td><td>265.46</td><td>147</td></tr><tr><td>distant</td><td>multi</td><td><strong>2.61</strong></td><td>259.17</td><td>147</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 병렬 토큰들 간의 어텐션 패턴의 영향을 보여줍니다. &lsquo;전체 어텐션(full attention)&lsquo;은 이전 병렬 그룹들로부터 모든 컨텍스트에 접근할 수 있게 해주는 반면, &lsquo;인과적 어텐션(causal attention)&lsquo;은 제한된 접근만 허용합니다. 전체 어텐션을 사용하면 FID가 1.03만큼 향상되는 것을 보여줍니다. 이는 병렬 처리를 효율적으로 하기 위해서는 이전에 생성된 모든 토큰에 접근하는 것이 중요하다는 것을 시사합니다.</p><details><summary>read the caption</summary>(c) Attention pattern between parallel tokens. Full attention allows complete context access from previous parallel groups (vs. causal attention’s limited access), bringing 1.03 FID improvement.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Params</th><th>FID↓</th><th>IS↑</th><th>steps</th></tr></thead><tbody><tr><td>343M</td><td>3.76</td><td>218.92</td><td>147</td></tr><tr><td>775M</td><td>2.61</td><td>259.17</td><td>147</td></tr><tr><td>1.4B</td><td>2.35</td><td>263.24</td><td>147</td></tr><tr><td>3.1B</td><td><strong>2.29</strong></td><td>255.46</td><td>147</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 단일 토큰 예측과 다중 토큰 예측에서 서로 다른 스캔 순서(래스터 스캔과 본 논문에서 제안하는 지역 기반 거리 순서)의 성능을 비교한 것입니다. 단일 토큰 예측에서는 두 스캔 순서 모두 유사한 성능을 보이지만(FID 2.62 대 2.64), 다중 토큰 예측에서는 지역 기반 거리 순서가 래스터 스캔 순서보다 훨씬 뛰어난 성능을 보입니다(FID 2.61 대 5.64). 이는 다중 토큰을 동시에 예측할 때 인접한 토큰 간의 강한 종속성으로 인해 래스터 스캔 순서에서 문제가 발생하지만, 지역 기반 거리 순서는 종속성이 약한 토큰들을 그룹화하여 이러한 문제를 해결하기 때문입니다.</p><details><summary>read the caption</summary>(d) Comparison of different scan orders under single-token and multi-token prediction. Our region-based distant ordering shows similar performance with raster scan in single-token setting, but significantly outperforms in multi-token prediction (2.61 vs. 5.64 FID).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>config</th><th>value</th></tr></thead><tbody><tr><td><em><strong>training hyper-params</strong></em></td><td></td></tr><tr><td>optimizer</td><td>AdamW [28]</td></tr><tr><td>learning rate</td><td>1e-4(L,XL)/2e-4(XXL,3B)</td></tr><tr><td>weight decay</td><td>5e-2</td></tr><tr><td>optimizer momentum</td><td>(0.9, 0.95)</td></tr><tr><td>batch size</td><td>256(L,XL)/ 512(XXL,3B)</td></tr><tr><td>learning rate schedule</td><td>cosine decay</td></tr><tr><td>ending learning rate</td><td>0</td></tr><tr><td>total epochs</td><td>300</td></tr><tr><td>warmup epochs</td><td>15</td></tr><tr><td>precision</td><td>bfloat16</td></tr><tr><td>max grad norm</td><td>1.0</td></tr><tr><td>dropout rate</td><td>0.1</td></tr><tr><td>attn dropout rate</td><td>0.1</td></tr><tr><td>class label dropout rate</td><td>0.1</td></tr><tr><td><em><strong>sampling hyper-params</strong></em></td><td></td></tr><tr><td>temperature</td><td>1.0</td></tr><tr><td>guidance scale</td><td>1.60 (L) / 1.50 (XL) / 1.435 (XXL) / 1.345 (3B)</td></tr></tbody></table></table></figure><blockquote><p>🔼 이 표는 모델 크기가 증가함에 따라 (4배 병렬 처리) 이미지 생성 품질이 향상되는 것을 보여줍니다. 3억 4300만개의 파라미터를 가진 모델의 FID(Fréchet Inception Distance) 점수는 3.76인 반면, 31억개의 파라미터를 가진 모델의 FID 점수는 2.29로 훨씬 낮아집니다. FID 점수가 낮을수록 생성된 이미지의 품질이 실제 이미지와 더 유사함을 나타냅니다. 즉, 파라미터 수가 증가하면 생성 품질이 지속적으로 향상됨을 알 수 있습니다.</p><details><summary>read the caption</summary>(e) Scaling of model size (4×\times× parallel). Generation quality steadily improves with more parameters, from 343M (FID 3.76) to 3.1B (FID 2.29).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>config</th><th>value</th></tr></thead><tbody><tr><td><em><strong>training hyper-params</strong></em></td><td></td></tr><tr><td>&mdash;</td><td>&mdash;</td></tr><tr><td>optimizer</td><td>AdamW [28]</td></tr><tr><td>learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>5e-2</td></tr><tr><td>optimizer momentum</td><td>(0.9, 0.95)</td></tr><tr><td>batch size</td><td>256</td></tr><tr><td>learning rate schedule</td><td>cosine decay</td></tr><tr><td>ending learning rate</td><td>0</td></tr><tr><td>total epochs</td><td>3000</td></tr><tr><td>warmup epochs</td><td>150</td></tr><tr><td>precision</td><td>bfloat16</td></tr><tr><td>max grad norm</td><td>1.0</td></tr><tr><td>dropout rate</td><td>0.1</td></tr><tr><td>attn dropout rate</td><td>0.1</td></tr><tr><td>class label dropout rate</td><td>0.1</td></tr><tr><td><em><strong>sampling hyper-params</strong></em></td><td></td></tr><tr><td>&mdash;</td><td>&mdash;</td></tr><tr><td>temperature</td><td>1.0</td></tr><tr><td>guidance scale</td><td>1.15</td></tr><tr><td>top-k</td><td>8000</td></tr></tbody></table></table></figure><blockquote><p>🔼 표 4는 이미지 생성 모델 설계에 대한 ablation 연구 결과를 보여줍니다. 각 ablation 연구는 모델의 성능에 미치는 영향을 평가하기 위해, 초기 순차적 토큰 생성 여부, 병렬 예측되는 토큰 수, 어텐션 패턴, 그리고 모델 크기 등의 요소들을 변화시켜 수행되었습니다. 각 실험 조건에 따른 FID(Fréchet Inception Distance), IS(Inception Score), 그리고 생성에 필요한 단계 수를 비교하여, 각 요소가 이미지 생성 품질 및 효율에 미치는 영향을 정량적으로 분석하였습니다.</p><details><summary>read the caption</summary>Table 4: Ablation studies on image generation model designs.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-1fdf4319a039a2f1874c85fa7c9f3fad class=gallery><img src=paper_images/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=paper_images/16.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/&amp;title=Parallelized%20Autoregressive%20Visual%20Generation" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/&amp;text=Parallelized%20Autoregressive%20Visual%20Generation" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/&amp;subject=Parallelized%20Autoregressive%20Visual%20Generation" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.15119/index.md",oid_likes="likes_paper-reviews/2412.15119/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.14835/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Progressive Multimodal Reasoning via Active Retrieval</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-19T00:00:00+00:00>19 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.15118/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Outcome-Refining Process Supervision for Code Generation</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-19T00:00:00+00:00>19 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>