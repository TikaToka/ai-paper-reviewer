[{"content": "| Dataset | Motions | Texts | Scenes | Scene Representation | Scene Type |\n|---|---|---|---|---|---| \n| KIT [42] | 3.9k | 6.2k | No | No | Indoor |\n| HumanML3D [16] | 14.6k | 44.9k | No | No | Indoor |\n| HUMANISE [49] | 19.6k | 19.6k | 643 | RGBD | Indoor |\n| PROX [18] | 28k | No | 12 | RGBD | Indoor |\n| LaserHuman [10] | 3.5k | 12.3k | 11 | RGBD | Indoor/Outdoor |\n| Motion-X [32] | 81.1k | 81.1k | 81.1k | Video | Indoor/Outdoor |\n| HiC-Motion | 300k | 300k | 300k | Video | Indoor/Outdoor |", "caption": "Table 1: \nDataset statistics. HiC-Motion is the largest dataset comprising motions, text, and diverse indoor and outdoor scenes.", "description": "\ud45c 1\uc740 \ub2e4\uc591\ud55c \uc2e4\ub0b4\uc678 \ud658\uacbd\uc5d0\uc11c \ucd2c\uc601\ub41c \ub3d9\uc791, \ud14d\uc2a4\ud2b8 \ubc0f \uc7a5\uba74\uc744 \ud3ec\ud568\ud558\ub294 \ub370\uc774\ud130\uc14b\uc758 \ud1b5\uacc4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. HiC-Motion \ub370\uc774\ud130\uc14b\uc740 \uac00\uc7a5 \ud070 \uaddc\ubaa8\uc758 \ub370\uc774\ud130\uc14b\uc774\uba70, \ub3d9\uc791, \ud14d\uc2a4\ud2b8, \uc7a5\uba74 \uc815\ubcf4\ub97c \ubaa8\ub450 \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uac01 \ub370\uc774\ud130\uc14b\uc758 \ub3d9\uc791, \ud14d\uc2a4\ud2b8 \ubc0f \uc7a5\uba74 \uc218\uc640 \uc7a5\uba74 \ud45c\ud604 \ubc29\uc2dd \ubc0f \uc720\ud615\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. Humans-in-Context Motion Dataset"}, {"content": "| Methods | FID (\u2193) | Accuracy (\u2191) | Diversity (\u2191) | Multimodality (\u2191) |\n|---|---|---|---|---|\n| MDM [45] | 164.595 | 0.325 | 24.758 | 18.924 |\n| MLD [8] | 85.913 | 0.322 | 25.119 | 19.464 |\n| SceneDiff [24] | 543.769 | 0.203 | 4.217 | 3.861 |\n| HUMANISE [49] | 159.935 | 0.225 | 23.287 | 19.956 |\n| MDM+ [45] | 46.035 | 0.620 | 23.002 | 17.627 |\n| Ours-scene | 46.458 | 0.482 | 24.968 | **21.320** |\n| Ours | **44.639** | **0.661** | **26.027** | 20.130 |", "caption": "Table 2: Quantitative results. Our method achieves better quality and diversity scores compared to state-of-the-art text-conditioned, scene-conditioned, and multimodal motion generation models.", "description": "\ud45c 2\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \uc815\ub7c9\uc801 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. FID(Fr\u00e9chet Inception Distance), \uc815\ud655\ub3c4, \ub2e4\uc591\uc131, \ub2e4\uc911 \ubaa8\ub4dc\uc131 \ub4f1\uc758 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4. \uc81c\uc548\ub41c \ubc29\ubc95\uc740 \ucd5c\ucca8\ub2e8\uc758 \ud14d\uc2a4\ud2b8 \uc870\uac74, \uc7a5\uba74 \uc870\uac74, \ub2e4\uc911 \ubaa8\ub4dc \ubaa8\uc158 \uc0dd\uc131 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud558\uc5ec \ub354 \ub098\uc740 \ud488\uc9c8\uacfc \ub2e4\uc591\uc131 \uc810\uc218\ub97c \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.", "section": "5. \uc2e4\ud5d8"}, {"content": "| Methods | Scene-Align (\u2191) | Text-Align (\u2191) | Quality (\u2191) | Total (\u2191) |\n|---|---|---|---|---|\n| MDM [45] | 2.25 | 1.35 | 1.50 | 5.10 |\n| MLD [8] | 2.85 | 1.95 | 1.90 | 6.70 |\n| SceneDiff [24] | 2.05 | 1.20 | 1.20 | 4.45 |\n| HUMANISE [49] | 2.20 | 1.45 | 1.30 | 4.95 |\n| MDM+ [45] | 2.57 | 1.73 | 1.94 | 6.24 |\n| Ours-scene | 2.90 | 2.00 | 1.95 | 6.85 |\n| Ours | **3.55** | **2.70** | **2.85** | **9.10** |", "caption": "Table 3: Automated evaluation. We report average VLM scores (0-5) for generated motions, assessing alignment with scene, text, and pose quality. Our method outperforms all evaluated baselines.", "description": "\ud45c 3\uc740 \uc81c\uc2dc\ub41c \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \uc790\ub3d9\uc73c\ub85c \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud3c9\uac00\ub294 Vision-Language Model(VLM)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0dd\uc131\ub41c \ub3d9\uc791\uc758 \uc2dc\uac01\uc801 \ud488\uc9c8\uacfc, \ubc30\uacbd \uc774\ubbf8\uc9c0 \ubc0f \ud14d\uc2a4\ud2b8 \uc124\uba85\uacfc\uc758 \uc815\ud569\uc131\uc744 0\uc5d0\uc11c 5\uc810 \ucc99\ub3c4\ub85c \ud3c9\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4.  \uac01 \uc9c0\ud45c (\ubc30\uacbd\uacfc\uc758 \uc815\ud569\ub3c4, \ud14d\uc2a4\ud2b8\uc640\uc758 \uc815\ud569\ub3c4, \uc790\uc138\uc758 \ud488\uc9c8)\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc810\uc218\uc640,  \uc804\uccb4 \uc810\uc218\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ubcf8 \ub17c\ubb38\uc758 \ubc29\ubc95\uc774 \ub2e4\ub978 \uae30\uc900 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "5. Experiments"}, {"content": "| Timestep | Text | Scene | FID (\u2193) | Accuracy (\u2191) |\n|---|---|---|---|---|\n| AdaLN | In-Context | In-Context | 44.639 | 0.661 |\n| AdaLN | In-Context | Cross-Attn | 47.656 | 0.567 |\n| In-Context | In-Context | In-Context | 62.927 | 0.554 |\n| In-Context | In-Context | Cross-Attn | 66.827 | 0.519 |", "caption": "Table 4: Ablation study. We study different transformer block designs, and choose AdaLN for timestep conditioning and In-Context for text and scene conditions as our main configuration.", "description": "\ud45c 4\ub294 \ub2e4\uc591\ud55c \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ube14\ub85d \uc124\uacc4\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc2dc\uac04 \ub2e8\uacc4 \uc870\uac74\ud654\uc5d0\ub294 AdaLN\uc744, \ud14d\uc2a4\ud2b8 \ubc0f \uc7a5\uba74 \uc870\uac74\uc5d0\ub294 In-Context \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ubcf8 \ub17c\ubb38\uc758 \uc8fc\uc694 \uad6c\uc131\uc73c\ub85c \uc120\ud0dd\ub418\uc5c8\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \uc870\ud569\uc73c\ub85c \uc2e4\ud5d8\ud558\uc5ec \ucd5c\uc801\uc758 \uc131\ub2a5\uc744 \ub0b4\ub294 \uad6c\uc131\uc744 \ucc3e\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.  FID(Fr\u00e9chet Inception Distance) \uc810\uc218\uc640 \uc815\ud655\ub3c4 \uc810\uc218\ub97c \ud1b5\ud574 \uac01 \uad6c\uc131\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\uc600\uc2b5\ub2c8\ub2e4.", "section": "4.4 Training Strategy"}]