[{"content": "| Model | IFEval | | | | | FollowBench (SSR) | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| | P (L) | I (L) | P (S) | I (S) | Avg. | Lv-1 | Lv-2 | Lv-3 | Lv-4 | Lv-5 | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| **LLaMA3-8B Models** | | | | | | | | | | | |\n| LLaMA3-8B-Instruct | 77.6 | 84.5 | 70.6 | 78.9 | 77.9 | 69.4 | 62.2 | 63.1 | 61.9 | 60.9 | 63.5 |\n| AutoIF-8B\u2020 | 43.1 | 56.0 | 28.8 | 42.2 | 42.5 | 54.6 | 52.1 | 50.0 | 49.0 | 43.7 | 49.9 |\n| SELF | 78.2 | 84.5 | 76.0 | 82.9 | 80.4 | 68.3 | 65.7 | 65.2 | 62.2 | 62.4 | 64.8 |\n| Humpback | 72.5 | 80.2 | 70.1 | 78.1 | 75.2 | 66.8 | 66.1 | 67.2 | 60.2 | 62.6 | 64.6 |\n| Self-Rewarding | 77.3 | 84.2 | 74.1 | 81.7 | 79.3 | 72.8 | 66.6 | 66.8 | **64.9** | 64.1 | 67.0 |\n| Meta-Rewarding | 77.8 | 84.1 | 75.4 | 82.3 | 79.9 | 73.9 | 71.9 | 66.0 | 62.3 | 62.6 | 67.3 |\n| SPaR-8B-SFT | 75.4 | 82.5 | 73.4 | 80.6 | 78.0 | 73.9 | 67.4 | 68.1 | 63.1 | 61.3 | 66.8 |\n| SPaR-8B-DPO-iter1 | 78.0 | 84.7 | 75.8 | 82.6 | 80.3 | **75.3** | 67.7 | 67.6 | 64.7 | 62.3 | 67.5 |\n| SPaR-8B-DPO-iter2 | 78.9 | 85.0 | 77.1 | 83.3 | 81.1 | 73.9 | 71.9 | 69.1 | 64.0 | 62.2 | 68.2 |\n| SPaR-8B-DPO-iter3 | **79.9** | **85.4** | **78.0** | **83.7** | **81.8** | 73.0 | **72.3** | **70.0** | 64.1 | **64.7** | **68.8** |\n| \ncdashline{1-12}\u00a0\u00a0w/ tree search | 82.4 | 87.5 | 79.5 | 85.3 | 83.7 | 73.9 | 71.7 | 70.3 | 66.8 | 64.1 | 69.4 |\n| **GLM-4-9B Models** | | | | | | | | | | | |\n| GLM-4-9B-Chat | 71.5 | 79.9 | 68.0 | 77.2 | 74.2 | 80.8 | 75.1 | 67.4 | 64.3 | **65.4** | 70.6 |\n| SPaR-9B-SFT | 71.5 | 80.5 | 68.8 | 78.1 | 74.7 | 79.4 | 70.9 | 68.2 | 65.1 | 63.7 | 69.5 |\n| SPaR-9B-DPO-iter3 | **77.3** | **84.1** | **73.6** | **81.4** | **79.1** | **82.7** | **76.7** | **67.9** | **68.3** | 64.2 | **72.0** |\n| **LLaMA3-70B Models** | | | | | | | | | | | |\n| LLaMA3-70B-Instruct | 83.7 | 88.9 | 77.1 | 83.8 | 83.4 | 77.1 | 72.5 | 69.4 | 68.7 | 66.3 | 70.8 |\n| AutoIF-70B\u2020 | **85.6** | **90.4** | 80.2 | 86.7 | 85.7 | 71.0 | 67.2 | 66.2 | 64.6 | 63.5 | 66.5 |\n| SPaR-70B-DPO-iter3 | **85.6** | 90.2 | **81.3** | **87.3** | **86.1** | **80.3** | **75.7** | **71.4** | **73.7** | **70.5** | **74.3** |", "caption": "Table 1: Main results of iteratively trained LLMs on instruction-following benchmarks (Cf. Table 6 for full results). P stands for prompt level, and I represents instruction level. L and S denote loose and strict evaluations, respectively. Avg. indicates average results and Lv means level. Results using inference-time tree search are highlighted in green. The highest results for each backbone model is bolded. Scores marked with \u2020 are sourced directly from the original paper.", "description": "\uc774 \ud45c\ub294 \uc5ec\ub7ec \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc744 \uc5ec\ub7ec \ubc88 \ubc18\ubcf5 \ud559\uc2b5\uc2dc\ucf30\uc744 \ub54c \uba85\ub839\uc5b4\ub97c \uc5bc\ub9c8\ub098 \uc798 \ub530\ub974\ub294\uc9c0 \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uac01 LLM\ub9c8\ub2e4 \ucd5c\uace0 \uc131\ub2a5\uc744 \uad75\uac8c \ud45c\uc2dc\ud588\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ub17c\ubb38\uc758 \ud45c 6\uc744 \ucc38\uc870\ud558\uc138\uc694. \ud3c9\uac00 \uc9c0\ud45c\ub294 \ud06c\uac8c \uba85\ub839\uc5b4 \uc218\uc900(I)\uacfc \ud504\ub86c\ud504\ud2b8 \uc218\uc900(P)\uc73c\ub85c \ub098\ub258\uba70, \uac01\uac01 \ub290\uc2a8\ud55c \ud3c9\uac00(L)\uc640 \uc5c4\uaca9\ud55c \ud3c9\uac00(S)\ub85c \uc138\ubd84\ud654\ub429\ub2c8\ub2e4. \ub610\ud55c, \ucd94\ub860 \uacfc\uc815\uc5d0\uc11c \ud2b8\ub9ac \ud0d0\uc0c9 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c\uc758 \uacb0\uacfc\ub294 \ub179\uc0c9\uc73c\ub85c \uac15\uc870 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.3 ACTOR EVALUATION RESULTS"}, {"content": "| Model | Natural | | Adversarial | | | | | | | Average | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | Acc. | F1 | GPTInst | | GPTOut | | Manual | | Neighbor | | Average | | Acc. | F1 |\n| | | | Acc. | F1 | Acc. | F1 | Acc. | F1 | Acc. | F1 | Acc. | F1 | | |\n| GPT-4o-Mini | 74.5 | 70.5 | 69.2 | 61.6 | 60.9 | 51.4 | 59.8 | 51.9 | 72.8 | 66.4 | 65.7 | 57.8 | 67.4 | 60.4 |\n| **_LLaMA3-8B Models_** | | | | | | | | | | | | | | |\n| LLaMA3-8B-Instruct | 60.0 | 51.8 | 55.4 | 46.1 | 47.9 | 39.5 | 51.1 | 36.6 | 54.5 | 45.0 | 52.2 | 41.8 | 53.8 | 43.8 |\n| SELF | 69.5 | 61.6 | 62.0 | 50.7 | 64.9 | 54.8 | 57.6 | 41.8 | 64.6 | 51.3 | 62.2 | 49.6 | 63.7 | 52.0 |\n| Self-Rewarding | **71.0** | **66.3** | 70.1 | **66.7** | 63.8 | 59.5 | 62.0 | 55.7 | 67.5 | 61.7 | 65.9 | 60.9 | 66.9 | 61.9 |\n| Meta-Rewarding | 70.5 | **66.3** | 68.5 | 64.6 | 64.9 | **60.2** | 64.1 | 58.3 | **69.0** | **63.1** | 66.6 | 61.6 | 67.4 | 62.5 |\n| SPaR-8B-SFT | 68.5 | 60.9 | 67.9 | 62.4 | 59.6 | 50.0 | 63.0 | 54.1 | 68.3 | 59.3 | 64.7 | 56.5 | 65.5 | 57.3 |\n| SPaR-8B-RFT-iter1 | 68.5 | 63.2 | 66.8 | 60.6 | 63.8 | 55.3 | 62.0 | 53.3 | 66.8 | 59.0 | 64.9 | 57.1 | 65.6 | 58.3 |\n| SPaR-8B-RFT-iter2 | 70.5 | 64.2 | 66.8 | 61.6 | **66.0** | 60.0 | 65.2 | 57.9 | **69.0** | 62.4 | 66.8 | 60.5 | 67.5 | 61.2 |\n| SPaR-8B-RFT-iter3 | 70.5 | 65.9 | **70.7** | **66.7** | 63.8 | 57.5 | **68.5** | **63.3** | 68.3 | 62.2 | **67.8** | **62.4** | **68.3** | **63.1** |\n| **_GLM-4-9B Models_** | | | | | | | | | | | | | | |\n| GLM-4-9B-Chat | **74.5** | **76.5** | 74.5 | **75.9** | 57.4 | **62.3** | 53.3 | 56.6 | 69.8 | **72.0** | 63.7 | **66.7** | 65.9 | **68.6** |\n| SPaR-9B-SFT | 70.5 | 65.5 | 72.8 | 70.2 | **59.6** | 55.8 | 64.1 | 53.5 | 71.3 | 67.2 | 66.9 | 61.7 | 67.7 | 62.5 |\n| SPaR-9B-RFT-iter3 | 71.0 | 68.8 | **75.5** | 74.6 | 58.5 | 55.2 | **68.5** | **64.2** | **68.7** | 65.9 | **67.8** | 64.9 | **68.4** | 65.7 |\n| **_LLaMA3-70B Models_** | | | | | | | | | | | | | | |\n| LLaMA3-70B-Instruct | 75.0 | 71.9 | 73.4 | 69.6 | **69.1** | **66.7** | 66.3 | **60.8** | 69.0 | 63.4 | 69.5 | 65.1 | 70.6 | 66.5 |\n| SPaR-70B-RFT-iter3 | **78.0** | **74.7** | **78.8** | **76.9** | 64.9 | 61.2 | **67.4** | 59.5 | **72.4** | **68.1** | **70.9** | **66.4** | **72.3** | **68.1** |", "caption": "Table 2: \nEvaluation of judgment capability for iteratively trained LLMs on LLMBar. (Cf. Table 8 for Mistral-7B-Instruct results.) Acc. denotes accuracy. The highest scores for each base model are highlighted in bold.", "description": "\uc774 \ud45c\ub294 LLMBar \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ubc18\ubcf5\uc801\uc73c\ub85c \ud559\uc2b5\ub41c LLM\uc758 \ud310\ub2e8 \ub2a5\ub825 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Mistral-7B-Instruct \uacb0\uacfc\ub294 \ud45c 8\uc744 \ucc38\uc870\ud558\uc138\uc694. \uac01 \uae30\ubcf8 \ubaa8\ub378\uc5d0 \ub300\ud574 \uac00\uc7a5 \ub192\uc740 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uc790\uc5f0\uc5b4\uc640 \uc801\ub300\uc801 \uc9c8\ubb38\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\uc640 F1 \uc810\uc218\uac00 \ud45c\uc2dc\ub418\uba70, \uac01 \uc9c8\ubb38 \uc720\ud615\uc5d0 \ub300\ud574 GPT \uc785\ub825, GPT \ucd9c\ub825, \uc218\ub3d9, \uc774\uc6c3 \ub4f1 \ub2e4\uc591\ud55c \ud3c9\uac00 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud55c \uacb0\uacfc\uac00 \uc81c\uacf5\ub429\ub2c8\ub2e4. \ub610\ud55c, \uac01 \ubaa8\ub378\uc5d0 \ub300\ud574 \uc790\uc5f0\uc5b4\uc640 \uc801\ub300\uc801 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc815\ud655\ub3c4\uc640 F1 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ud45c\ub97c \ud1b5\ud574 SPAR \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c Refiner\uc758 \ud310\ub2e8 \ub2a5\ub825\uc774 \ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. EXPERIMENTS"}, {"content": "| Model | Acc-GPT | Acc-SPaR |\n|---|---|---| \n| GPT-4o-Mini | 79.0 | 71.0 |\n| SPaR-8B-SFT | 73.5 | 71.0 |\n| SPaR-8B-RFT-iter1 | 77.5 | 77.0 |\n| SPaR-8B-RFT-iter2 | 74.5 | 76.0 |\n| SPaR-8B-RFT-iter3 | 79.0 | 90.5 |", "caption": "Table 3: Refinement evaluation results. Acc-GPT uses GPT-4o as judge; -SPaR uses SPaR-8B-RFT-iter3.", "description": "\uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud3c9\uac00 \uba54\ud2b8\ub9ad\uc744 \uc0ac\uc6a9\ud558\uc5ec SPAR \ud504\ub808\uc784\uc6cc\ud06c\uc758 \uac1c\uc120 \uae30\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \"Acc-GPT\" \uc5f4\uc740 GPT-40\uc744 \ud310\uc0ac\ub85c \uc0ac\uc6a9\ud55c \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0b4\uace0 \"Acc-SPAR\" \uc5f4\uc740 SPAR-8B-RFT-iter3\ub97c \ud310\uc0ac\ub85c \uc0ac\uc6a9\ud55c \uc815\ud655\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud45c\uc5d0\uc11c SPAR-8B-RFT-iter3\uac00 \uc790\uccb4 \ud3c9\uac00\uc5d0\uc11c GPT-40\ubcf4\ub2e4 \ub192\uc740 \uc810\uc218\ub97c \ubc1b\uc558\uc9c0\ub9cc GPT-40 \ud3c9\uac00\uc5d0\uc11c\ub294 \uadf8\ub807\uc9c0 \uc54a\ub2e4\ub294 \uc810\uc5d0 \uc720\uc758\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\ub294 \uc790\uccb4 \ud3c9\uac00 \ud3b8\ud5a5\uc758 \uac00\ub2a5\uc131\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3.4 REFINER EVALUATION RESULTS"}, {"content": "| Model | IFEval | | FollowBench (SSR) | \n|---|---|---|---|\n| | Prompt(S) | Instruction(S) | Avg. |\n| SPaR-8B-DPO-iter3 | 78.0 | 83.7 | 68.8 |\n| *w/o* Tree Search | -2.0 | -0.8 | -1.7 |\n| *w/o* Iterative Training | -0.9 | -0.2 | -2.0 |\n| *w/o* Refinement | -2.6 | -1.6 | -3.1 |", "caption": "Table 6: Full results of SPaR-7B, SPaR-9B, and SPaR-70B on instruction-following benchmarks. P stands for prompt level, and I represents instruction level. L and S denote loose and strict evaluations, respectively. Avg. indicates average results and Lv means level. Scores marked with \u2020 are sourced directly from the original paper.", "description": "\uc774 \ud45c\ub294 SPaR-7B, SPaR-9B, SPaR-70B \ubaa8\ub378\uc758 \uba85\ub839\uc5b4 \uc218\ud589 \ubca4\uce58\ub9c8\ud06c \uc810\uc218\ub97c \uc790\uc138\ud788 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. IFEval \ubc0f FollowBench(SSR) \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ud504\ub86c\ud504\ud2b8 \ub808\ubca8(P)\uacfc \uba85\ub839\uc5b4 \ub808\ubca8(I) \ubaa8\ub450\uc5d0 \ub300\ud55c \uc810\uc218, \ub290\uc2a8\ud55c \ud3c9\uac00(L)\uc640 \uc5c4\uaca9\ud55c \ud3c9\uac00(S) \uc810\uc218, \uadf8\ub9ac\uace0 \uac01 \ub808\ubca8(Lv1~Lv5)\ubcc4 \ud3c9\uade0 \uc810\uc218\uac00 \uc81c\uacf5\ub429\ub2c8\ub2e4.  \ub17c\ubb38\uc5d0\uc11c \uc9c1\uc811 \uac00\uc838\uc628 \uc810\uc218\ub294 \u2020\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3. \uc2e4\ud5d8"}, {"content": "| Model | Natural | Adversarial |\n|---|---|---|\n| | Acc. | F1 | Acc. | F1 |\n| SPaR-8B-RFT-iter3 | 70.5 | 65.9 | 67.8 | 62.4 |\n| *w/o* Tree Search | -0.5 | -1.2 | -4.3 | -8.2 |\n| *w/o* Iterative Training | -0.5 | -2.5 | -1.7 | -3.5 |", "caption": "Table 7: Performance on general benchmarks. SPaR maintains the model\u2019s general capabilities.", "description": "\uc774 \ud45c\ub294 SPaR\uc774 \ubaa8\ub378\uc758 \uc77c\ubc18\uc801\uc778 \ub2a5\ub825\uc744 \uc720\uc9c0\ud558\ub294\uc9c0 \uc5ec\ubd80\ub97c \ud3c9\uac00\ud558\uae30 \uc704\ud574 \uc77c\ubc18 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. SPaR\uc744 \ud1b5\ud574 \uad50\uc721\ub41c \ubaa8\ub378\uc740 GSM8k, TriviaQA, MMLU \ubc0f HumanEval\uacfc \uac19\uc740 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc131\ub2a5\uc774 \uc800\ud558\ub418\uc9c0 \uc54a\uace0 \uc624\ud788\ub824 \ud5a5\uc0c1\ub418\ub294 \uacbd\uc6b0\ub3c4 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3. \uc2e4\ud5d8"}, {"content": "| Model | IFEval | | | | | FollowBench (SSR) | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| | **P (L)** | **I (L)** | **P (S)** | **I (S)** | **Avg.** | **Lv-1** | **Lv-2** | **Lv-3** | **Lv-4** | **Lv-5** | **Avg.** |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| *Mistral-7B Models* | | | | | | | | | | | |\n| Mistral-7B-Instruct | 55.1 | 64.9 | 49.9 | 60.2 | 57.5 | 65.1 | 61.6 | 61.6 | 56.8 | 57.2 | 60.4 |\n| SELF | 71.3 | 79.7 | 68.0 | 76.9 | 74.0 | 71.5 | 64.2 | 60.8 | 58.0 | 57.0 | 62.3 |\n| Humpback | 60.4 | 71.0 | 56.6 | 67.6 | 63.9 | 70.7 | 63.9 | 63.8 | 59.8 | 57.9 | 63.2 |\n| Self-Rewarding | 64.3 | 73.5 | 61.0 | 70.7 | 67.4 | 70.8 | 64.8 | 62.3 | 61.9 | **58.3** | 63.6 |\n| Meta-Rewarding | 65.1 | 74.7 | 61.0 | 71.1 | 68.0 | 73.2 | 64.6 | 64.5 | 60.6 | 57.6 | 64.1 |\n| SPaR-7B-SFT | 62.7 | 72.3 | 59.3 | 68.7 | 65.8 | 74.4 | 64.3 | 62.5 | 58.2 | 55.0 | 62.9 |\n| SPaR-7B-DPO-iter1 | 68.2 | 76.6 | 64.7 | 73.6 | 70.8 | 73.2 | 64.6 | 63.1 | 60.3 | 56.6 | 63.6 |\n| SPaR-7B-DPO-iter2 | 70.0 | 78.1 | 65.8 | 74.2 | 72.0 | 72.2 | **65.7** | 61.4 | **62.4** | 57.5 | 63.8 |\n| SPaR-7B-DPO-iter3 | **74.1** | **80.9** | **69.7** | **77.1** | **75.5** | **74.6** | 63.8 | **66.1** | 61.0 | 58.0 | **64.7** |\n| *GLM-4-9B Models* | | | | | | | | | | | |\n| GLM-4-9B-Chat | 71.5 | 79.9 | 68.0 | 77.2 | 74.2 | 80.8 | 75.1 | 67.4 | 64.3 | **65.4** | 70.6 |\n| SPaR-9B-SFT | 71.5 | 80.5 | 68.8 | 78.1 | 74.7 | 79.4 | 70.9 | **68.2** | 65.1 | 63.7 | 69.5 |\n| SPaR-9B-DPO-iter1 | 73.8 | 81.2 | 70.6 | 78.5 | 76.0 | 82.6 | 76.0 | 67.9 | 64.9 | 63.6 | 71.0 |\n| SPaR-9B-DPO-iter2 | 76.7 | 83.3 | 73.2 | 80.9 | 78.5 | 80.4 | 76.6 | 67.4 | **68.7** | 64.1 | 71.4 |\n| SPaR-9B-DPO-iter3 | **77.3** | **84.1** | **73.6** | **81.4** | **79.1** | **82.7** | **76.7** | 67.9 | 68.3 | 64.2 | **72.0** |\n| *LLaMA3-70B Models* | | | | | | | | | | | |\n| LLaMA3-70B-Instruct | 83.7 | 88.9 | 77.1 | 83.8 | 83.4 | 77.1 | 72.5 | 69.4 | 68.7 | 66.3 | 70.8 |\n| AutoIF-70B\u2020 | **85.6** | **90.4** | 80.2 | 86.7 | 85.7 | 71.0 | 67.2 | 66.2 | 64.6 | 63.5 | 66.5 |\n| SPaR-70B-DPO-iter1 | 84.5 | 89.2 | 80.2 | 85.7 | 84.9 | 77.6 | 74.0 | 70.2 | 70.6 | 66.9 | 71.9 |\n| SPaR-70B-DPO-iter2 | 85.0 | 89.4 | 81.5 | 87.2 | 85.8 | **80.4** | **76.4** | 69.9 | **73.7** | 70.2 | 74.1 |\n| SPaR-70B-DPO-iter3 | **85.6** | 90.2 | **81.3** | **87.3** | **86.1** | 80.3 | 75.7 | **71.4** | **73.7** | **70.5** | **74.3** |", "caption": "Table 8: Judgment evalution results on LLMBar for SPaR-7B. Acc. stands for accuracy.", "description": "\uc774 \ud45c\ub294 Mistral-7B-Instruct \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c SPaR \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubc18\ubcf5\uc801\uc73c\ub85c \ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \ud310\ub2e8 \ub2a5\ub825\uc744 LLMBar \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. SPaR\uc740 \uc790\uccb4 \uac1c\uc120\uc744 \uc704\ud55c \uc140\ud504 \ud50c\ub808\uc774 \ud504\ub808\uc784\uc6cc\ud06c\ub85c, \ud14d\uc2a4\ud2b8\uc758 \ubbf8\ubb18\ud55c \ucc28\uc774\ub97c \uac15\uc870\ud558\uc5ec \uba85\ub839\uc5b4\ub97c \ub354 \ud6a8\uacfc\uc801\uc73c\ub85c \ub530\ub974\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud45c\uc5d0\ub294 \uc790\uc5f0\uc5b4 \ubc0f \uc801\ub300\uc801 \uc0d8\ud50c \ubaa8\ub450\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\uc640 F1 \uc810\uc218\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \uc0d8\ud50c \uc720\ud615(GPTInst, GPTOut, Manual, Neighbor)\uc5d0 \ub300\ud574 \uac1c\ubcc4\uc801\uc73c\ub85c \ud3c9\uac00\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uc804\uccb4 \ud3c9\uade0 \uc810\uc218\ub97c \uc81c\uacf5\ud558\uc5ec \ubaa8\ub378\uc758 \uc804\ubc18\uc801\uc778 \ud310\ub2e8 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. SPaR\uc744 \ud1b5\ud574 \ubc18\ubcf5\uc801\uc73c\ub85c \ud6c8\ub828\ub41c \ubaa8\ub378\uc740 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ub2e4\ub978 \uae30\uc900\uc120\ubcf4\ub2e4 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc758 \ud310\ub2e8 \ub2a5\ub825 \ud5a5\uc0c1\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3. EXPERIMENTS"}, {"content": "| Model | GSM8k | TriviaQA | MMLU | HumanEval | Average | \n|---|---|---|---|---|---| \n| **_Mistral-7B Models_** | | | | | | \n| Mistral-7B-Instruct | 42.9 | 72.5 | 57.9 | 32.9 | 51.6 | \n| SPaR-7B-SFT | 56.4 | 72.8 | 56.7 | 44.5 | 57.6 (+6.0) | \n| SPaR-7B-DPO-iter1 | 55.6 | 72.2 | 55.3 | 46.3 | 57.4 (+5.8) | \n| SPaR-7B-DPO-iter2 | 54.4 | 72.1 | 55.8 | 45.1 | 56.9 (+5.3) | \n| SPaR-7B-DPO-iter3 | 58.2 | 71.6 | 55.1 | 46.3 | 57.8 (+6.2) | \n| **_LLaMA3-8B Models_** | | | | | | \n| LLaMA3-8B-Instruct | 75.4 | 75.9 | 63.6 | 55.5 | 67.6 | \n| SPaR-8B-SFT | 75.6 | 76.0 | 64.0 | 61.6 | 69.3 (+1.7) | \n| SPaR-8B-DPO-iter1 | 78.8 | 75.2 | 63.8 | 60.4 | 69.6 (+2.0) | \n| SPaR-8B-DPO-iter2 | 77.0 | 74.9 | 63.1 | 60.4 | 68.9 (+1.3) | \n| SPaR-8B-DPO-iter3 | 77.7 | 75.1 | 63.1 | 60.9 | 69.2 (+1.6) | \n| **_GLM-4-9B Models_** | | | | | | \n| GLM-4-9B-Chat | 80.6 | 69.7 | 71.9 | 74.3 | 74.1 | \n| SPaR-9B-SFT | 82.9 | 69.4 | 71.8 | 73.8 | 74.5 (+0.4) | \n| SPaR-9B-DPO-iter1 | 82.6 | 68.8 | 71.6 | 75.0 | 74.5 (+0.4) | \n| SPaR-9B-DPO-iter2 | 82.8 | 68.9 | 71.8 | 73.8 | 74.3 (+0.2) | \n| SPaR-9B-DPO-iter3 | 83.0 | 69.0 | 72.1 | 73.2 | 74.3 (+0.2) | \n| **_LLaMA3-70B Models_** | | | | | | \n| LLaMA3-70B-Instruct | 92.2 | 87.2 | 80.8 | 79.3 | 84.9 | \n| SPaR-70B-DPO-iter1 | 92.5 | 90.4 | 81.0 | 79.3 | 85.8 (+0.9) | \n| SPaR-70B-DPO-iter2 | 92.9 | 89.5 | 80.4 | 78.7 | 85.4 (+0.5) | \n| SPaR-70B-DPO-iter3 | 93.4 | 86.7 | 80.6 | 79.9 | 85.2 (+0.3) |", "caption": "Table 9: Comparison of decoding strategies on LLMBar.", "description": "\uc774 \ud45c\ub294 LLMBar \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \ub514\ucf54\ub529 \uc804\ub7b5\uc744 \ube44\uad50\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc8fc\uc694 \ub0b4\uc6a9\uc740 \ud0d0\uc695\uc801 \ub514\ucf54\ub529\uacfc \ub2e4\uc218\uacb0 \ud22c\ud45c\ub97c \uc0ac\uc6a9\ud55c \uc5ec\ub7ec \uc0d8\ud50c\ub9c1 \ud69f\uc218\ub97c \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uc0d8\ud50c\ub9c1 \ud69f\uc218\uac00 \uc99d\uac00\ud560\uc218\ub85d \uc790\uc5f0\uc5b4 \ub2f5\ubcc0\uc758 \uc815\ud655\ub3c4\uc640 F1 \uc810\uc218\ub294 \ub2e4\uc18c \ud5a5\uc0c1\ub418\ub294 \ubc18\uba74, \uc801\ub300\uc801 \ub2f5\ubcc0\uc5d0\uc11c\ub294 \uc0d8\ud50c\ub9c1 \ud69f\uc218 5\uc5d0\uc11c \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. \uc774\ub294 \uc0d8\ud50c\ub9c1 \ud69f\uc218 \uc99d\uac00\uac00 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uc911\uc694\ud560 \uc218 \uc788\uc9c0\ub9cc, \uc9c0\ub098\uce58\uac8c \ub298\ub9ac\uba74 \uc624\ud788\ub824 \uc801\ub300\uc801 \ub2f5\ubcc0\uc5d0 \ub300\ud55c \uc131\ub2a5\uc774 \uc800\ud558\ub420 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.", "section": "3 Experiments / D Experiment Results / D.4 Ablation Study on Judgment Capability"}, {"content": "| Model | Natural | | Adversarial | | | | | | Average | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| | Acc. | F1 | GPTInst | | GPTOut | | Manual | | Neighbor | | Average | | | |\n| | | | Acc. | F1 | Acc. | F1 | Acc. | F1 | Acc. | F1 | Acc. | F1 | Acc. | F1 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| Mistral-7B-Instruct | 58.0 | **69.1** | 57.1 | **68.8** | 50.0 | **64.1** | 45.6 | **61.5** | 47.8 | 62.6 | 50.1 | **64.3** | 51.7 | **65.2** |\n| SELF | 68.0 | 65.2 | 71.2 | 68.7 | 56.4 | 56.8 | 62.0 | 52.6 | 67.5 | 62.3 | 64.3 | 60.1 | 65.0 | 61.1 | \n| Self-Rewarding | 68.0 | 64.0 | 69.0 | 63.7 | 59.6 | 53.7 | **63.0** | 57.5 | **69.4** | **64.3** | **65.3** | 59.8 | 65.8 | 60.6 |\n| Meta-Rewarding | 67.5 | 62.4 | 71.7 | 68.7 | 56.4 | 51.8 | **63.0** | 56.4 | 66.8 | 62.1 | 64.5 | 59.7 | 65.1 | 60.3 |\n| SPaR-7B-SFT | 69.5 | 63.9 | 71.7 | 67.5 | 55.3 | 48.8 | 55.4 | 45.3 | **69.4** | 62.3 | 63.0 | 56.1 | 64.3 | 57.6 |\n| SPaR-7B-RFT-iter1 | 67.0 | 62.1 | 66.3 | 62.7 | 56.4 | 52.9 | 60.9 | 52.6 | 64.2 | 60.7 | 61.9 | 57.2 | 63.0 | 58.2 |\n| SPaR-7B-RFT-iter2 | 68.0 | 64.4 | 68.5 | 64.6 | **60.6** | 57.5 | 62.0 | 52.1 | 64.2 | 60.0 | 63.8 | 58.5 | 64.7 | 59.7 |\n| SPaR-7B-RFT-iter3 | **71.0** | 66.7 | **72.3** | 67.5 | 57.4 | 55.6 | 60.9 | 51.4 | 68.3 | 62.6 | 64.7 | 59.2 | **66.0** | 60.7 |", "caption": "Table 10: Comparison of different decoding strategies for refinement task. Acc-GPT stands for the accuracy of using GPT-4o as judge, and Acc-SPaR for the accuracy of using SPaR-8B-RFT-iter3 as judge.", "description": "\uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ub514\ucf54\ub529 \uc804\ub7b5\uc744 \ube44\uad50\ud558\uc5ec SPaR-8B \ubaa8\ub378\uc758 \uac1c\uc120 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Acc-GPT\ub294 GPT-40\uc744 \ud310\uc0ac\ub85c \uc0ac\uc6a9\ud55c \uc815\ud655\ub3c4\uc774\uace0, Acc-SPaR\ub294 SPaR-8B-RFT-iter3\ub97c \ud310\uc0ac\ub85c \uc0ac\uc6a9\ud55c \uc815\ud655\ub3c4\uc785\ub2c8\ub2e4. \ud45c\uc5d0\uc11c BFS\uc640 DFS\uc640 \uac19\uc740 \ud2b8\ub9ac \ud0d0\uc0c9 \uc54c\uace0\ub9ac\uc998\uc774 \ub2e4\ub978 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub6f0\uc5b4\ub098\uba70, \ud2b9\ud788 \ud0d0\uc695\uc801 \ub514\ucf54\ub529\ubcf4\ub2e4 \ub192\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc751\ub2f5 \uac1c\uc120 \uc791\uc5c5\uc5d0\uc11c \ud2b8\ub9ac \ud0d0\uc0c9\uc758 \uc911\uc694\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.5 Ablations and Analysis"}]