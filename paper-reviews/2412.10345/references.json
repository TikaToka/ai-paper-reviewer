{"references": [{"fullname_first_author": "Anthony Brohan", "paper_title": "RT-1: Robotics Transformer for Real-World Control at Scale", "publication_date": "2022-12-06", "reason": "This paper introduced RT-1, a Robotics Transformer model that demonstrated strong performance on real-world robot manipulation tasks, laying the groundwork for future VLA models."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning Transferable Visual Models From Natural Language Supervision", "publication_date": "2021-01-01", "reason": "This work introduced CLIP, a model that learns visual representations from natural language supervision and demonstrates strong zero-shot performance on various tasks, establishing the power of vision-language models."}, {"fullname_first_author": "Moo Jin Kim", "paper_title": "OpenVLA: An Open-Source Vision-Language-Action Model", "publication_date": "2024-06-18", "reason": "This work introduces OpenVLA, a state-of-the-art open-source vision-language-action model which serves as a strong baseline and foundation for our TraceVLA model."}, {"fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "publication_date": "2024-01-01", "reason": "This paper introduces Phi-3-Vision, a lightweight and powerful vision-language model that we utilize as the backbone for our compact TraceVLA-Phi3 model."}, {"fullname_first_author": "Open X-Embodiment Collaboration", "paper_title": "Open X-Embodiment: Robotic learning datasets and RT-X models", "publication_date": "2023-10-13", "reason": "This work provides large scale robotic datasets, which serve as one of the key training sources and benchmark environments for evaluating our TraceVLA models."}]}