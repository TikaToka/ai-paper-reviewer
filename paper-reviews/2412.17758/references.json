{"references": [{"fullname_first_author": "Todor Mihaylov", "paper_title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering", "publication_date": "2018-00-00", "reason": "This paper introduced the OpenBookQA dataset, a benchmark significantly impacted by the evaluation method discussed in the target paper."}, {"fullname_first_author": "Peter Clark", "paper_title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge", "publication_date": "2018-00-00", "reason": "This paper introduced the ARC Challenge and ARC Easy datasets, which are central to the evaluation method comparison in the target paper."}, {"fullname_first_author": "Yonatan Bisk", "paper_title": "PIQA: Reasoning about Physical Commonsense in Natural Language", "publication_date": "2019-00-00", "reason": "This paper introduced the PIQA dataset, another benchmark dataset used to demonstrate the impact of the evaluation method."}, {"fullname_first_author": "Maarten Sap", "paper_title": "SocialiQA: Commonsense Reasoning about Social Interactions", "publication_date": "2019-00-00", "reason": "This paper introduced the SocialIQA dataset, which is used as an example to show how evaluation methods affect conclusions about model capabilities."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring Massive Multitask Language Understanding", "publication_date": "2021-00-00", "reason": "This paper introduced the MMLU benchmark, demonstrating the broad applicability of the discussed evaluation issues across multiple datasets."}]}