[{"content": "| Method | AUROC \u2191 | FPR95 \u2193 |\n|---|---|---|\n| Confidence | 57.5 | 95.1 |\n| Consistency | 73.5 | 75.6 |\n| Object Detector | 61.5 | 95.7 |\n| Isolation | **81.4** | **71.7** |", "caption": "Table 1: Performance comparison of hallucination detection methods for the dataset of Figure 1.", "description": "\ud45c 1\uc740 \ub17c\ubb38\uc758 \uadf8\ub9bc 1\uc5d0 \ud574\ub2f9\ud558\ub294 \ub370\uc774\ud130\uc14b\uc744 \uae30\ubc18\uc73c\ub85c, \ub2e4\uc591\ud55c \ud658\uac01 \uac80\ucd9c \ubc29\ubc95\ub4e4\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c, Confidence, Consistency, Object Detector, Isolation \ub124 \uac00\uc9c0 \ubc29\ubc95\uc758 AUROC (Area Under the Receiver Operating Characteristic curve)\uc640 FPR95 (False Positive Rate at 95% true positive rate) \uac12\uc744 \uc81c\uc2dc\ud558\uc5ec \uac01 \ubc29\ubc95\uc758 \ud658\uac01 \uac80\ucd9c \uc815\ud655\ub3c4\uc640 \uc624\ud0d0\uc728\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.  Isolation \ubc29\ubc95\uc774 \ub2e4\ub978 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \ub354 \ub192\uc740 AUROC \uac12\uacfc \ub0ae\uc740 FPR95 \uac12\uc744 \ubcf4\uc774\uba70 \uac00\uc7a5 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ub098\ud0c0\ub0b4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.1. Motivating Observations"}, {"content": "| Caption | CIDEr | CLIP-S | RefCLIP-S | CLAIR | ALOHa | Ours |\n|---|---|---|---|---|---|---|\n| Clean | 6.4 | 81.3 | 75.5 | **86.9** | 36.2 | **62.8** |\n| Object | 4.8 | 81.0 | 75.3 | 85.2 | 31.5 | 52.3 |\n| Attribution | 6.2 | 80.9 | 75.2 | 80.0 | 34.3 | 60.9 |\n| Relation | **6.7** | **81.4** | **75.6** | 83.5 | **36.9** | 51.9 |", "caption": "Table 2: Meta-evaluation results across various caption evaluation methods. DOCCI and its synthetic hallucinatory captions are used for the meta-evaluation. The highest-rated caption for each method is highlighted in bold. The full table is in Appendix D.", "description": "\ubcf8 \ud45c\ub294 \ub2e4\uc591\ud55c \ucea1\uc158 \ud3c9\uac00 \ubc29\ubc95\uc5d0 \ub300\ud55c \uba54\ud0c0 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. DOCCI \ub370\uc774\ud130\uc14b\uacfc \ud569\uc131\uc801 \ud658\uac01 \ucea1\uc158\uc744 \uc0ac\uc6a9\ud558\uc5ec \uba54\ud0c0 \ud3c9\uac00\ub97c \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \uac01 \ubc29\ubc95\uc5d0 \ub300\ud574 \uac00\uc7a5 \ub192\uc740 \uc810\uc218\ub97c \ubc1b\uc740 \ucea1\uc158\uc740 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uc804\uccb4 \ud45c\ub294 \ubd80\ub85d D\uc5d0 \uc788\uc2b5\ub2c8\ub2e4.  \ud45c\ub294 \uae68\ub057\ud55c \ucea1\uc158, \uac1d\uccb4 \uc624\ub958 \ucea1\uc158, \uc18d\uc131 \uc624\ub958 \ucea1\uc158, \uad00\uacc4 \uc624\ub958 \ucea1\uc158\uc758 \ub124 \uac00\uc9c0 \uc720\ud615\uc5d0 \ub300\ud574 BLEU, ROUGE, METEOR, CIDEr, CLIP-S, RefCLIP-S, CLAIR, ALOHA \ubc0f \uc81c\uc548\ub41c \uc0c8\ub85c\uc6b4 \ubc29\ubc95\uc758 \uc810\uc218\ub97c \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.", "section": "3.3 \ud3c9\uac00 \ubc29\ubc95"}, {"content": "| Metric | FAITHSCORE | FACTSCORE | Ours |\n|---|---|---|---| \n| Spearman\u2019s \u03c1 | 62.5 | 67.9 | **70.2** |", "caption": "Table 3: Comparison of correlations between human preferences and automated metrics in terms of factuality.", "description": "\ubcf8 \ud45c\ub294 \uc0ac\ub78c\uc774 \ud3c9\uac00\ud55c \ub0b4\uc6a9\uacfc \uc790\ub3d9\ud654\ub41c \uc9c0\ud45c \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  \uc790\ub3d9\ud654\ub41c \uc9c0\ud45c\ub294 \uc774\ubbf8\uc9c0 \ucea1\uc158\uc758 \uc0ac\uc2e4\uc131\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \uad6c\uccb4\uc801\uc73c\ub85c, FAITHSCORE, FACTSCORE, \uadf8\ub9ac\uace0 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ub41c \uc0c8\ub85c\uc6b4 \uc9c0\ud45c \uc138 \uac00\uc9c0\ub97c \ube44\uad50\ud558\uc5ec \uc0ac\ub78c\uc758 \ud3c9\uac00\uc640 \uc5bc\ub9c8\ub098 \uc798 \uc77c\uce58\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774\ub294 \uc81c\uc548\ub41c \uc0c8\ub85c\uc6b4 \uc9c0\ud45c\uc758 \uc2e0\ub8b0\uc131\uc744 \ud3c9\uac00\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.", "section": "4.2. Our Metric's Correlation with Human Evaluation"}, {"content": "| Captioner | CapMAS | Metric |  |  |  |  |\n|---|---|---|---|---|---|---|\n|  | LLM | MLLM | CLAIR | Factuality | Coverage | Avg. | \n| LLaVA-NeXT-7B | - | - | 68.8 | 59.9 | **47.9** | 58.9 |\n|  | LLaMA-3-8B | LLaVA-NeXT-7B | 74.1 | 72.2 | 46.9 | **64.4** |\n|  | GPT-4 | LLaVA-NeXT-7B | **74.6** | **73.4** | 46.2 | **64.7** |\n| LLaVA-NeXT-13B | - | - | 70.2 | 62.1 | **48.5** | 60.3 |\n|  | LLaMA-3-8B | LLaVA-NeXT-13B | **75.5** | 77.9 | 45.8 | **66.4** |\n|  | GPT-4 | LLaVA-NeXT-13B | 73.4 | **79.3** | 45.1 | 65.9 |\n| InternVL-Chat-V1.5 | - | - | 74.9 | 65.5 | **48.2** | 62.9 |\n|  | LLaMA-3-8B | InternVL-Chat-V1.5 | **78.2** | **75.9** | 47.3 | **67.1** |\n|  | GPT-4 | InternVL-Chat-V1.5 | 77.8 | 75.7 | 47.3 | 66.9 |\n| GPT-4V | - | - | 82.4 | 77.1 | **53.5** | 71.0 |\n|  | LLaMA-3-8B | LLaVA-NeXT-7B | 83.3 | 83.3 | 50.8 | 72.4 |\n|  | LLaMA-3-8B | LLaVA-NeXT-13B | 81.9 | **85.3** | 48.4 | 71.9 |\n|  | LLaMA-3-8B | InternVL-Chat-V1.5 | **84.6** | 82.1 | **53.5** | **73.4** |", "caption": "Table 4: Effectiveness of our proposed method across various captioning models. In the CapMAS column, the LLM represents the decomposer and corrector, while the MLLM represents the fact-checker. Avg. denotes the average of CLAIR, Factuality, and Coverage.", "description": "\ubcf8 \ud45c\ub294 \uc81c\uc548\ub41c CapMAS \ubc29\ubc95\uc758 \ub2e4\uc591\ud55c \ucea1\uc158 \uc0dd\uc131 \ubaa8\ub378\uc5d0 \ub300\ud55c \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. CapMAS \uc5f4\uc5d0\uc11c LLM\uc740 \ucea1\uc158 \ubd84\ud574 \ubc0f \uc218\uc815\uc744 \ub2f4\ub2f9\ud558\uace0 MLLM\uc740 \uc0ac\uc2e4 \ud655\uc778\uc744 \ub2f4\ub2f9\ud569\ub2c8\ub2e4. \ud3c9\uade0(Avg.)\uc740 CLAIR, \uc0ac\uc2e4\uc131, \uc801\uc6a9 \ubc94\uc704\uc758 \ud3c9\uade0\uac12\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ub2e4\uc591\ud55c LLM\uacfc MLLM \uc870\ud569\uc5d0 \ub530\ub978 CLAIR \uc810\uc218, \uc0ac\uc2e4\uc131 \uc810\uc218, \uc801\uc6a9 \ubc94\uc704 \uc810\uc218 \ubc0f \ud3c9\uade0 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.2. \ucea1\uc158 \uc0ac\uc2e4\uc131 \ud5a5\uc0c1 \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uc2dc\uc2a4\ud15c"}, {"content": "| Method | CLAIR | Factuality | Coverage | Avg. |\n|---|---|---|---|---|\n| Base | 62.1 | 52.8 | 34.3 | 49.7 |\n| VCD (Leng et al., 2024) | 59.7 | 44.6 | 39.3 | 47.9 |\n| OPERA (Huang et al., 2024) | 59.1 | 53.0 | 34.1 | 48.7 |\n| LURE (Zhou et al., 2024) | 57.2 | 51.9 | 27.6 | 45.6 |\n| Volcano (Lee et al., 2024) | 63.9 | 53.7 | 37.7 | 51.7 |\n| LRV (Liu et al., 2023a) | 39.7 | 29.1 | 37.8 | 35.5 |\n| CapMAS (ours) | 66.3 | 63.4 | 33.1 | 54.3 |", "caption": "Table 5: Performance comparison between our proposed method and other methods regarding detailed image captioning. Base refers to the default image captioning of LLaVA-v1.5-7B.", "description": "\ud45c 5\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uacfc \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc744 \ube44\uad50\ud558\uc5ec \uc0c1\uc138\ud55c \uc774\ubbf8\uc9c0 \ucea1\uc158 \uc0dd\uc131 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uae30\uc900(Base)\uc740 LLaVA-v1.5-7B \ubaa8\ub378\uc758 \uae30\ubcf8 \uc774\ubbf8\uc9c0 \ucea1\uc158 \uc0dd\uc131 \uacb0\uacfc\uc785\ub2c8\ub2e4.  \ud45c\uc5d0\ub294 \uc81c\uc548\ub41c \ubc29\ubc95\uacfc \uae30\uc874\uc758 \uc5ec\ub7ec \ucea1\uc158 \uc0dd\uc131 \ubc29\ubc95\ub4e4(VCD, OPERA, LURE, Volcano, LRV)\uc758 \uc131\ub2a5\uc744 CLAIR \uc810\uc218, \uc0ac\uc2e4\uc131 \uc810\uc218, \uc801\uc6a9\ubc94\uc704 \uc810\uc218, \uadf8\ub9ac\uace0 \ud3c9\uade0 \uc810\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uc81c\uc548\ub41c \ubc29\ubc95\uc758 \uc6b0\uc218\uc131\uc744 \ubcf4\uc5ec\uc8fc\uace0\uc790 \ud569\ub2c8\ub2e4.", "section": "4.4. \ub2e4\ub978 \ubc29\ubc95\ub4e4\uacfc\uc758 \ube44\uad50"}, {"content": "| Model | CLAIR |  |  |  | OpenCompass |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n|  | Detailed Image Captioning |  |  |  | Visual Question Answering |  |  |  |\n|  | CLAIR | Factuality | Coverage | Avg. | OpenCompass | MME | POPE | Avg. |\n| InstructBLIP-7B | 57.2 | 44.4 | 30.3 | 43.9 | 31.1 | 1391.4 | 86.1 | 38.4 |\n| LLaVA-v1.5-7B | 61.1 | 56.3 | 30.5 | 49.3 | 36.9 | 1808.4 | 86.1 | 44.6 |\n| LLaVA-NeXT-7B | 63.8 | 58.5 | 42.2 | 54.8 | 44.7 | 1769.1 | 87.5 | 50.8 |\n| LLaVA-NeXT-13B | 64.5 | 62.8 | 43.0 | 56.8 | 47.6 | 1745.6 | 87.8 | 53.1 |\n| Idefics2-8B | 58.1 | 85.2 | 13.4 | 52.2 | 53.0 | 1847.6 | 86.2 | 57.6 |\n| InternVL-Chat-V1.5 | 72.4 | 67.6 | 46.0 | 62.0 | 61.7 | 2189.6 | 87.5 | 65.9 |\n| MiniCPM-V-2.6 | 73.1 | 68.9 | 43.6 | 61.9 | 65.2 | 2268.7 | 83.2 | 68.6 |\n| GPT-4V | 82.4 | 78.6 | 52.6 | 71.2 | 63.5 | 2070.2 | 81.8 | 66.4 |", "caption": "Table 6: Detailed image captioning and VQA performance of various MLLMs. OpenCompass (Duan et\u00a0al., 2024) includes MMBench v1.1 (Liu et\u00a0al., 2023c), MMStar (Chen et\u00a0al., 2024a), MMMU val (Yue et\u00a0al., 2024), MathVista (Lu et\u00a0al., 2024), OCRBench (Liu et\u00a0al., 2024d), AI2D (Kembhavi et\u00a0al., 2016), HallusionBench (Guan et\u00a0al., 2024), and MMVet (Yu et\u00a0al., 2023). For POPE (Li et\u00a0al., 2023b), we report the average F1 score across the three categories: adversarial, popular, and random. We report the sum of the perception and cognition scores for MME (Yin et\u00a0al., 2023b). The best results for each metric are shown in bold.", "description": "\ud45c 6\uc740 \ub2e4\uc591\ud55c \ub2e4\uc911 \ubaa8\ub2ec \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(MLLM)\uc758 \uc0c1\uc138 \uc774\ubbf8\uc9c0 \ucea1\uc158 \uc0dd\uc131 \ubc0f VQA \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. OpenCompass(Duan et al., 2024)\ub294 MMBench v1.1(Liu et al., 2023c), MMStar(Chen et al., 2024a), MMMU val(Yue et al., 2024), MathVista(Lu et al., 2024), OCRBench(Liu et al., 2024d), AI2D(Kembhavi et al., 2016), HallusionBench(Guan et al., 2024), MMVet(Yu et al., 2023)\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. POPE(Li et al., 2023b)\uc758 \uacbd\uc6b0 \uc138 \uac00\uc9c0 \ubc94\uc8fc(\uc801\ub300\uc801, \uc77c\ubc18\uc801, \ubb34\uc791\uc704)\uc5d0 \uac78\uce5c \ud3c9\uade0 F1 \uc810\uc218\ub97c \ubcf4\uace0\ud558\uba70, MME(Yin et al., 2023b)\ub294 \uc778\uc9c0 \ubc0f \uc778\uc2dd \uc810\uc218\uc758 \ud569\uacc4\ub97c \ubcf4\uace0\ud569\ub2c8\ub2e4. \uac01 \uc9c0\ud45c\uc5d0 \ub300\ud55c \ucd5c\uace0 \uc810\uc218\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4.5. MLLM \ucea1\uc158 \uc0dd\uc131\uacfc VQA \ud3c9\uac00 \uacb0\uacfc\uc758 \uc77c\uad00\uc131"}, {"content": "| Task | FACTSCORE | Ours |\n|---|---|---|\n| LLaVA-v1.5-7B vs. InstructBLIP | 67.9 | 70.2 |\n| HUMAN vs. LLaVA-v1.5-7B vs. InstructBLIP | 18.3 | 61.4 |", "caption": "Table 7: Comparison of correlations between human preferences and automated metrics in terms of factuality.", "description": "\ubcf8 \ud45c\ub294 \uc0ac\ub78c\uc758 \ud3c9\uac00\uc640 \uc790\ub3d9\ud654\ub41c \uc9c0\ud45c \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ube44\uad50\ud558\uc5ec \ubb38\uc7a5\uc758 \uc0ac\uc2e4\uc131\uc744 \ud3c9\uac00\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  LLaVA-v1.5-7B\uc640 InstructBLIP \ub450 \ubaa8\ub378\uc758 \ubb38\uc7a5\uc5d0 \ub300\ud55c \uc778\uac04\uc758 \uc0ac\uc2e4\uc131 \ud3c9\uac00\uc640, FAITHSCORE, FACTSCORE \ubc0f \uc81c\uc548\ub41c \uc9c0\ud45c\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uc2a4\ud53c\uc5b4\ub9cc \uc0c1\uad00\uacc4\uc218(Spearman's \u03c1)\ub97c \uc0ac\uc6a9\ud558\uc5ec \uce21\uc815\ud558\uc600\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uc81c\uc548\ub41c \uc9c0\ud45c\uac00 \uae30\uc874 \uc9c0\ud45c\ubcf4\ub2e4 \uc778\uac04\uc758 \ud310\ub2e8\uacfc \ub354 \ub192\uc740 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "4.2 \uc778\uac04 \ud3c9\uac00\uc640\uc758 \uc9c0\ud45c \uc0c1\uad00\uad00\uacc4"}, {"content": "| Captioner | CapMAS |  |  | Metric |  |  |\n|---|---|---|---|---|---|---|\n|  | LLM | MLLM | \u03c0 | CLAIR | Factuality | Coverage |\n| LLaVA-NeXT-7B | - | - | - | 68.8 | 59.9 | 47.9 |\n| LLaMA-3-8B | LLaVA-NeXT-7B | 1.0 | 74.1 | 72.2 | 46.9 |\n| LLaMA-3-8B | LLaVA-NeXT-7B | 0.5 | 73.6 | 76.9 | 43.7 |\n| LLaMA-3-8B | LLaVA-NeXT-7B | 0.3 | 72.2 | 76.8 | 40.0 |\n| LLaVA-NeXT-13B | - | - | - | 70.2 | 62.1 | 48.5 |\n| LLaMA-3-8B | LLaVA-NeXT-13B | 1.0 | 75.5 | 77.9 | 45.8 |\n| LLaMA-3-8B | LLaVA-NeXT-13B | 0.5 | 74.8 | 79.9 | 42.1 |\n| LLaMA-3-8B | LLaVA-NeXT-13B | 0.3 | 72.6 | 80.5 | 39.6 |\n| InternVL-Chat-V1.5 | - | - | - | 74.9 | 65.5 | 48.2 |\n| LLaMA-3-8B | InternVL-Chat-V1.5 | 1.0 | 78.2 | 75.9 | 47.3 |\n| LLaMA-3-8B | InternVL-Chat-V1.5 | 0.5 | 79.0 | 78.8 | 46.0 |\n| LLaMA-3-8B | InternVL-Chat-V1.5 | 0.3 | 77.7 | 81.7 | 42.5 |", "caption": "Table 8: Effectiveness of our proposed method across various captioning models as a function of \u03c0\ud835\udf0b\\piitalic_\u03c0. In the CapMAS column, the LLM represents the decomposer and corrector, while the MLLM represents the fact-checker.", "description": "\ud45c 8\uc740 \uc81c\uc548\ub41c CapMAS \ubc29\ubc95\uc758 \ub2e4\uc591\ud55c \ucea1\uc158 \uc0dd\uc131 \ubaa8\ub378\uc5d0 \ub300\ud55c \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \u03c0 \uac12\uc744 \ubcc0\ud654\uc2dc\ud0a4\uba74\uc11c(\u03c0\ub294  CapMAS\uc5d0\uc11c \ud658\uac01 \uc694\uc18c\ub97c \uad6c\ubd84\ud558\ub294 \uc784\uacc4\uac12 \uc5ed\ud560) LLM(\ubd84\ud574\uae30 \ubc0f \uc218\uc815\uae30)\uacfc MLLM(\uc0ac\uc2e4 \ud655\uc778\uae30)\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.  \ub2e4\uc591\ud55c \ubaa8\ub378(LLaVA-NeXT-7B, LLaVA-NeXT-13B, InternVL-Chat-V1.5)\uacfc  LLM(LLaMA-3-8B, GPT-4) \uc870\ud569\uc5d0 \ub530\ub978  CLAIR \uc810\uc218, \uc0ac\uc2e4\uc131 \uc810\uc218, \uc801\uc6a9\ubc94\uc704 \uc810\uc218\uc758 \ubcc0\ud654\ub97c  \u03c0 \uac12\uc758 \ubcc0\ud654\uc5d0 \ub530\ub77c \ubcf4\uc5ec\uc8fc\ub294 \ud45c\uc785\ub2c8\ub2e4.  \u03c0\uac12\uc774 \uac10\uc18c\ud560\uc218\ub85d \uc0ac\uc2e4\uc131\uc740 \uc99d\uac00\ud558\uc9c0\ub9cc \uc801\uc6a9\ubc94\uc704\ub294 \uac10\uc18c\ud558\ub294 \uacbd\ud5a5\uc744 \ubcf4\uc5ec\uc8fc\ub294  \uc0ac\uc2e4\uc131\uacfc \uc801\uc6a9\ubc94\uc704 \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubd84\uc11d\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.", "section": "3.2 \ucea1\uc158 \uc0ac\uc2e4\uc131 \ud5a5\uc0c1 \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uc2dc\uc2a4\ud15c"}, {"content": "| Caption | BLEU | ROUGE | METEOR | CIDEr | CLIP-S | RefCLIP-S | CLAIR | ALOHa | Ours |\n|---|---|---|---|---|---|---|---|---|---| \n| Clean | 4.2 | 22.0 | 13.7 | 6.4 | 81.3 | 75.5 | **86.9** | 36.2 | **62.8** |\n| Object | **4.9** | **22.3** | **14.5** | 4.8 | 81.0 | 75.3 | 85.2 | 31.5 | 52.3 |\n| Attribution | 4.1 | 21.8 | 13.6 | 6.2 | 80.9 | 75.2 | 80.0 | 34.3 | 60.9 |\n| Relation | 4.1 | 21.8 | 13.7 | **6.7** | **81.4** | **75.6** | 83.5 | **36.9** | 51.9 |", "caption": "Table 9: Meta-evaluation results across various caption evaluation methods. DOCCI and its synthetic hallucinatory captions are used for the meta-evaluation. The highest-rated caption for each method is highlighted in bold.", "description": "\ud45c 9\ub294 \ub2e4\uc591\ud55c \ucea1\uc158 \ud3c9\uac00 \ubc29\uc2dd\uc5d0 \ub300\ud55c \uba54\ud0c0 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. DOCCI \ub370\uc774\ud130\uc14b\uacfc \ud569\uc131 \ud658\uac01 \ucea1\uc158\uc744 \uc0ac\uc6a9\ud558\uc5ec \uba54\ud0c0 \ud3c9\uac00\ub97c \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \uac01 \ubc29\uc2dd\uc5d0 \ub300\ud574 \uac00\uc7a5 \ub192\uc740 \uc810\uc218\ub97c \ubc1b\uc740 \ucea1\uc158\uc740 \uad75\uac8c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  BLEU, ROUGE, METEOR, CIDEr, CLIP-S, RefCLIP-S, CLAIR, ALOHA \ubc0f \uc81c\uc548\ub41c \ubc29\uc2dd\uc744 \ud3ec\ud568\ud55c \uc5ec\ub7ec \ucea1\uc158 \ud3c9\uac00 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uae68\ub057\ud55c \ucea1\uc158, \uac1c\uccb4(Object), \uc18d\uc131(Attribution), \uad00\uacc4(Relation) \ub4f1 \ub124 \uac00\uc9c0 \uc720\ud615\uc758 \ucea1\uc158\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4.", "section": "3.3 \ud3c9\uac00 \ubc29\ubc95"}]