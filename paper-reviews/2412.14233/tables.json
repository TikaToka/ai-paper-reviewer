[{"content": "| Attributes | Visual Specialists | Detailed Process |\n|---|---|---|\n| Object |  |  |\n| Size | Detection model | Using **the area of the bounding box** to measure the size of the instance. |\n| Depth | Depth & Detection model | **Average the depth map values within the bounding box region** to obtain the depth information. |\n| Emotion | Emotion model | If the detected region is labeled as **\u201dperson\u201d**, an emotion model is used **to extract an emotion label**. |\n| OCR | OCR Model | Using an OCR model to **extract the text content and bounding box** from the region. |\n| Animal | Fine Grained model | A fine-grained recognition model to identify **specific species of the animal**. |\n| Plants | Fine Grained model | A fine-grained recognition model to identify **specific species of the plants**. |\n| Aircrafts | Fine Grained model | A fine-grained recognition model to identify **specific model of the aircraft**. |\n| Logo | Fine Grained model | A fine-grained recognition model to **identify logos** in the region. |\n| Landmark | Fine Grained model | A fine-grained recognition model to **identify landmarks** within the region. |\n| Food | Fine Grained model | A fine-grained recognition model to identify **specific species of the food**. |\n| Celebrity | Fine Grained model | Using a fine-grained recognition model to **identify celebrity** within the region. |\n| Relation |  |  |\n| P2O relation | HOI Model | Using an HOI model to **determine the relationship between the person and the object**, while the bounding boxes of both the person and the object define their respective regions. |\n| Count | Detection model | **Counting the number of all objects** in the image based on the detection results. |\n| 2D Absolute Location | Detection model | Using the bounding box to **determine the instance\u2019s position within the image**, including regions such as **left**, **right**, **top**, **bottom**, **center**, **top-left**, **bottom-left**, **top-right**, and **bottom-right**. |\n| 2D Relative Location | Detection model | Using the bounding box to **determine the relative position among multiple objects within** the image, including regions such as **left**, **right**, **near**, **next to**, **close by**, and so on. |\n| 3D Relative Location | Detection & Depth model | Using the depth attributes of different instances to **capture the 3D spatial relationships of objects** **relative to the camera**, such as \u201dInstance_A is **in front of** Instance_B\u201d or \u201dInstance_A is **behind of** Instance_B\u201d relative to the camera.", "caption": "Table 1: Summary of attributes our approach extracts through visual specialists. It includes the specific attribute names, the models used, and the extraction process for each.", "description": "\uc774 \ud45c\ub294 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 \uc811\uadfc \ubc29\uc2dd\uc778 DCE(Descriptive Caption Enhancement)\uac00 \uc2dc\uac01\uc801 \uc804\ubb38\uac00 \ubaa8\ub378\uc744 \ud1b5\ud574 \ucd94\ucd9c\ud558\ub294 \uc18d\uc131\ub4e4\uc758 \uc694\uc57d \uc815\ubcf4\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc18d\uc131\uc758 \uc774\ub984, \uc0ac\uc6a9\ub41c \ubaa8\ub378, \uadf8\ub9ac\uace0 \ucd94\ucd9c \uacfc\uc815\uc744 \uc0c1\uc138\ud788 \uc124\uba85\ud569\ub2c8\ub2e4.  \uac1d\uccb4\uc758 \ud06c\uae30, \uae4a\uc774, \uac10\uc815, \ud14d\uc2a4\ud2b8 \uc815\ubcf4(OCR), \ub3d9\ubb3c, \uc2dd\ubb3c, \ud56d\uacf5\uae30, \ub85c\uace0, \ub79c\ub4dc\ub9c8\ud06c, \uc74c\uc2dd, \uc720\uba85\uc778 \ub4f1 \ub2e4\uc591\ud55c \uc18d\uc131\uacfc \uac1d\uccb4 \uac04\uc758 \uad00\uacc4(P2O \uad00\uacc4, HOI, 2D/3D \uc0c1\ub300 \uc704\uce58)\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uc5b4, \uc774\ubbf8\uc9c0 \ucea1\uc158\uc758 \ud48d\ubd80\ud55c \uc138\ubd80 \uc815\ubcf4\ub97c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.", "section": "3. Approach"}, {"content": "| Using an HOI model to <ins>determine the relationship between the person and the object</ins>, while |\n| the bounding boxes of both the person and the object define their respective regions. |", "caption": "Table 2: Human evaluation of attribute richness, conducted on 100 validation samples with 10 volunteers.", "description": "\uc774 \ud45c\ub294 10\uba85\uc758 \uc790\uc6d0\ubd09\uc0ac\uc790\ub4e4\uc774 100\uac1c\uc758 \uac80\uc99d \uc0d8\ud50c\uc5d0 \ub300\ud574 \uc218\ud589\ud55c \uc18d\uc131 \ud48d\ubd80\uc131\uc5d0 \ub300\ud55c \uc778\uac04 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc0d8\ud50c\uc5d0 \ub300\ud574 InternVL2, LLaVA-NeXT, \uadf8\ub9ac\uace0 DCE \uc138 \uac00\uc9c0 \ubc29\ubc95\uc73c\ub85c \uc0dd\uc131\ub41c \ucea1\uc158\uc758 \uacf5\uac04\uc801 \uad00\uacc4, \uc778\uac04-\uac1d\uccb4 \uc0c1\ud638 \uc791\uc6a9, \uc138\ubd84\ud654\ub41c \uc18d\uc131, OCR, \uac10\uc815 \ub4f1 \ub2e4\uc591\ud55c \uc18d\uc131\uc758 \ud48d\ubd80\uc131\uc744 \ud3c9\uac00\ud588\uc2b5\ub2c8\ub2e4.  \uc218\uce58\ub294 \uac01 \uc18d\uc131\uc5d0 \ub300\ud55c \ud3c9\uade0 \uc810\uc218\ub97c \ub098\ud0c0\ub0b4\uba70, DCE\uac00 \ub2e4\ub978 \ub450 \ubc29\ubc95\ubcf4\ub2e4 \ub354 \ud48d\ubd80\ud55c \uc18d\uc131\uc744 \uac00\uc9c4 \ucea1\uc158\uc744 \uc0dd\uc131\ud588\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "section": "3.2 Object Relation"}, {"content": "| Using the bounding box to **determine the instance\u2019s position within the image**, including regions|\n| such as **left**, **right**, **top**, **bottom**, **center**, **top-left**, **bottom-left**, **top-right**, and **bottom-right**|", "caption": "Table 3: Performance on seven General Visual Question Answering benchmarks. The red and blue colors respectively represent the optimal and suboptimal results on each benchmark. \u2217*\u2217 indicates the use of LLaVA-NeXT\u2019s open-source SFT data, with certain private data excluded.", "description": "\ud45c 3\uc740 7\uac00\uc9c0 \uc77c\ubc18\uc801\uc778 \uc2dc\uac01\uc801 \uc9c8\ubb38 \ub2f5\ubcc0 \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube68\uac04\uc0c9\uacfc \ud30c\ub780\uc0c9\uc740 \uac01 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ucd5c\uace0 \ubc0f \ucd5c\uc800 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc740 \ud2b9\uc815 \uc2dc\uac01\uc801 \uc9c8\ubb38 \ub2f5\ubcc0 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \ubc31\ubd84\uc728\ub85c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  LLaVA-NeXT\uc758 \uc624\ud508\uc18c\uc2a4 SFT \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud55c \uacb0\uacfc\ub294 \ubcc4\ud45c(*)\ub85c \ud45c\uc2dc\ub418\uc5b4 \uc788\uc73c\uba70, \uc77c\ubd80 \uac1c\uc778 \uc815\ubcf4\uac00 \uc81c\uc678\ub418\uc5c8\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \ud45c\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\uc640 \ud559\uc2b5 \ub370\uc774\ud130\uc758 \ud6a8\uacfc\ub97c \ube44\uad50\ud558\uc5ec \uc2dc\uac01\uc801 \uc9c8\ubb38 \ub2f5\ubcc0 \uc131\ub2a5\uc5d0 \ub300\ud55c \ud1b5\ucc30\ub825\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "section": "4.2. \uc8fc\uc694 \uacb0\uacfc"}, {"content": "| Using the bounding box to **determine the relative position among multiple objects within**\n| **the image**, including regions such as **left**, **right**, **near**, **next to**, **close by**, and so on.", "caption": "Table 4: Performance on seven Large Multi-Modal benchmarks. The red and blue colors respectively represent the optimal and suboptimal results on each benchmark. \u2217*\u2217 indicates the use of LLaVA-NeXT\u2019s open-source SFT data, with certain private data excluded.", "description": "\ud45c 4\ub294 7\uac00\uc9c0 \ub300\uaddc\ubaa8 \ub2e4\uc911 \ubaa8\ub2ec \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube68\uac04\uc0c9\uacfc \ud30c\ub780\uc0c9\uc740 \uac01 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ucd5c\uc801 \ubc0f \ucd5c\uc545\uc758 \uacb0\uacfc\ub97c \uac01\uac01 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  * \ud45c\uc2dc\ub294 \ud2b9\uc815 \uac1c\uc778 \uc815\ubcf4\uac00 \uc81c\uc678\ub41c LLaVA-NeXT\uc758 \uc624\ud508 \uc18c\uc2a4 SFT \ub370\uc774\ud130 \uc0ac\uc6a9\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\uc758 \ub2e4\uc911 \ubaa8\ub2ec \ub2a5\ub825\uc744 \ube44\uad50 \ud3c9\uac00\ud558\uc5ec \uac01 \ubaa8\ub378\uc758 \uac15\uc810\uacfc \uc57d\uc810\uc744 \ud30c\uc545\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.  \uac01 \ubaa8\ub378\uc758 \ube44\uc804 \uc778\ucf54\ub354, \uc5b8\uc5b4 \ubaa8\ub378, \uadf8\ub9ac\uace0 \ubca4\uce58\ub9c8\ud06c\ubcc4 \uc131\ub2a5 \uc810\uc218\uac00 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \ud2b9\ud788,  LLaVA-NeXT \ubaa8\ub378\uc758 \uacbd\uc6b0 \uc624\ud508 \uc18c\uc2a4 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud55c \uacbd\uc6b0\uc640 \uadf8\ub807\uc9c0 \uc54a\uc740 \uacbd\uc6b0\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}, {"content": "| Using the depth attributes of different instances to **capture the 3D spatial relationships of objects**\n| **relative to the camera**, such as \u201dInstance_A is **in front of** Instance_B\u201d or \u201dInstance_A is **behind of** Instance_B\u201d relative to the camera.\n", "caption": "Table 5: Comparison of Different Image Captioning Annotation Methods.", "description": "\ud45c 5\ub294 \uc11c\ub85c \ub2e4\ub978 \uc774\ubbf8\uc9c0 \ucea1\uc158 \uc8fc\uc11d \ubc29\ubc95\ub4e4\uc744 \ube44\uad50 \ubd84\uc11d\ud55c \ud45c\uc785\ub2c8\ub2e4.  \uc778\uac04\uc774 \uc9c1\uc811 \uc791\uc131\ud55c \ucea1\uc158, InternVL2-26B\uc640 LLaVA-NeXT-34B \ubaa8\ub378\uc774 \uc0dd\uc131\ud55c \ucea1\uc158, \uadf8\ub9ac\uace0 \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud558\ub294 DCE \uae30\ubc95\uc744 \uc774\uc6a9\ud574 \uc0dd\uc131\ud55c \ucea1\uc158\uc744 \ube44\uad50\ud558\uc5ec, OKVQA, GQA, ScienceQA, TextVQA, MMBench, MM-Vet, SEED-Bench \ub4f1 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 DCE\uac00 \ub2e4\ub978 \ubc29\ubc95\ub4e4\uc5d0 \ube44\ud574 \ub354\uc6b1 \uc815\ud655\ud558\uace0 \uc0c1\uc138\ud55c \ucea1\uc158\uc744 \uc0dd\uc131\ud558\uba70, \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc5d0\uc11c\ub3c4 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "4. Experiments"}]