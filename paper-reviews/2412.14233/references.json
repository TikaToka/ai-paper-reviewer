{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-08", "reason": "This paper is a foundational technical report describing the capabilities and limitations of GPT-4, a large language model crucial for understanding the context of the current research."}, {"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: A Visual Language Model for Few-Shot Learning", "publication_date": "2022-12-01", "reason": "Flamingo is a significant contribution to vision-language models, providing a strong baseline for the current research's improvements to multimodal perception."}, {"fullname_first_author": "Anas Awadalla", "paper_title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models", "publication_date": "2023-08-01", "reason": "OpenFlamingo provides an open-source framework that enables reproducibility and wider access to vision-language model development, making it highly relevant to the current study."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond", "publication_date": "2023-12-01", "reason": "Qwen-VL is a strong multimodal model, serving as a direct comparison point to showcase the enhanced capabilities achieved by the current research."}, {"fullname_first_author": "Zhe Chen", "paper_title": "InternVL: Scaling Up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks", "publication_date": "2024-01-01", "reason": "InternVL is another crucial reference due to its significant impact on vision-language models, providing a benchmark against which to measure the advancements made in this work."}]}