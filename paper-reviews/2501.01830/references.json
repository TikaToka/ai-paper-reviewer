{"references": [{"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-02", "reason": "This paper is foundational to the field of instruction following in LLMs, a key concept for the red-teaming discussed in this paper."}, {"fullname_first_author": "Liu, X.", "paper_title": "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models", "publication_date": "2024-10-05", "reason": "This paper directly addresses the problem of automatically generating adversarial prompts to uncover vulnerabilities in LLMs, a key area this paper seeks to improve."}, {"fullname_first_author": "Mazeika, M.", "paper_title": "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal", "publication_date": "2024-02-04", "reason": "This paper provides the HarmBench dataset used in this paper's experiments, establishing a standardized benchmark for evaluating safety and robustness in LLMs."}, {"fullname_first_author": "Hong, Z.-W.", "paper_title": "Curiosity-driven red-teaming for large language models", "publication_date": "2024-00-00", "reason": "This paper introduces a reinforcement learning approach for red-teaming LLMs, a method directly compared to the approach in this paper."}, {"fullname_first_author": "Zou, A.", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-15", "reason": "This paper examines the transferability of adversarial attacks across different LLMs, an important consideration when designing robust red-teaming techniques."}]}