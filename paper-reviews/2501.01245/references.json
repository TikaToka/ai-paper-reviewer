{"references": [{"fullname_first_author": "Bertasius", "paper_title": "Is space-time attention all you need for video understanding?", "publication_date": "2021-00-00", "reason": "This paper proposes the TimeSformer architecture, a backbone model used in the SeFAR framework, demonstrating its importance to the current work."}, {"fullname_first_author": "Carreira", "paper_title": "Quo vadis, action recognition? a new model and the kinetics dataset", "publication_date": "2017-00-00", "reason": "This paper introduced the Kinetics dataset and a novel action recognition model which is a foundational work in the field and provides context for the current paper's advancements."}, {"fullname_first_author": "Shao", "paper_title": "FineGym: A hierarchical video dataset for fine-grained action understanding", "publication_date": "2020-00-00", "reason": "This paper introduced the FineGym dataset, one of the primary datasets used for evaluating the SeFAR model, making it crucial to the paper's experimental validation."}, {"fullname_first_author": "Sohn", "paper_title": "FixMatch: Simplifying semi-supervised learning with consistency and confidence", "publication_date": "2020-00-00", "reason": "This paper introduced the FixMatch framework, which SeFAR builds upon, representing a core methodological foundation of the research."}, {"fullname_first_author": "Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-00-00", "reason": "This paper introduced the Vision Transformer (ViT) architecture, which is the foundational visual encoder used within the SeFAR framework, highlighting its significance to the proposed model."}]}