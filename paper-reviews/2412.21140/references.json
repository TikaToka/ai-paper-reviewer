{"references": [{"fullname_first_author": "Rohan Taori", "paper_title": "Stanford alpaca: An instruction-following llama model", "publication_date": "2023-00-00", "reason": "This paper introduced Alpaca, an early and influential method for knowledge distillation of state-of-the-art LLMs, inspiring further advancements in open-source multilingual chatbots."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduced LLaMA, a foundational large language model that has substantially accelerated the development of open-source alternatives to leading closed-source models like GPT-4."}, {"fullname_first_author": "Ilya Gusev", "paper_title": "rulm: A toolkit for training neural language models", "publication_date": "2023-00-00", "reason": "This paper provided a toolkit for training neural language models, facilitating the development of open-source multilingual chatbots and offering a cost-effective pipeline for language adaptation."}, {"fullname_first_author": "Yiming Cui", "paper_title": "Efficient and effective text encoding for Chinese llama and alpaca", "publication_date": "2023-04-08", "reason": "This paper proposed a language-specific continued pre-training pipeline for full LLM language adaptation, enabling the creation of high-performing open-source models with improved computational efficiency."}, {"fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7B", "publication_date": "2023-10-06", "reason": "This paper introduced Mistral-7B, one of the most performant open-source instruction-tuned LLMs that is directly compared in the paper, aiding in the evaluation of the proposed language adaptation method."}]}