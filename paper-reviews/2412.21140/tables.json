[{"content": "| Model | Micro-Avg | DaruMMLU | DaruMERA | DaruSum | DaruCopy (EN) | DaruCopy (RU) |\n|---|---|---|---|---|---|---|\n| Openchat 3.5 (Mistral-7B) | 0,607 | 0,543 | 0,526 | 0,322 | 0,999 | 0,917 |\n| LLaMa-3-8B (Instruct) | 0,610 | 0,571 | 0,510 | 0,322 | 1,000 | 0,972 |\n| Saiga (LLaMa-3-8B) | 0,608 | 0,574 | 0,514 | 0,320 | 0,995 | 0,939 |\n| Vikhr-5.2 (Mistral-7B) | 0,587 | 0,494 | 0,573 | 0,308 | 0,959 | 0,693 |\n| Qwen-2 7B | 0,613 | 0,624 | 0,548 | 0,300 | 0,938 | 0,842 |\n| Mistral Nemo (12B) | 0,639 | 0,592 | 0,576 | 0,320 | 0,998 | 0,924 |\n| Ours |  |  |  |  |  |  |\n| Openchat 3.5 + LEP-Extended + calibration (best) | 0,632<sup>\u2191</sup> | 0,541 | 0,563<sup>\u2191</sup> | 0,321 | 1,000 | 0,989<sup>\u2191</sup> |\n| LLaMa-3-8B (Instruct) + LEP-Extended + calibration (best) | 0,618<sup>\u2191</sup> | 0,565<sup>\u2193</sup> | 0,521<sup>\u2191</sup> | 0,339<sup>\u2191</sup> | 1,000 | 0,984<sup>\u2191</sup> |", "caption": "Table 1: Darumeru zero-shot evaluation results for popular open-source instruct-tuned models.", "description": "\ud45c 1\uc740 \ub2e4\uc591\ud55c \uc624\ud508\uc18c\uc2a4 \uc9c0\uc2dc \uc870\uc815\ud615 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc5d0 \ub300\ud55c Darumeru \uc81c\ub85c\uc0f7 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Darumeru\ub294 \ub7ec\uc2dc\uc544\uc5b4 \uc801\uc751\uc744 \uc704\ud574 \ud2b9\ubcc4\ud788 \uace0\uc548\ub41c \uc0c8\ub85c\uc6b4 \ubca4\uce58\ub9c8\ud06c\uc774\uba70, \ub2e4\uc591\ud55c \ub7ec\uc2dc\uc544\uc5b4 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \uac15\uac74\uc131\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \ud45c\uc5d0\ub294 Openchat 3.5, LLaMa-3-8B(Instruct), Saiga, Vikhr-5.2, Qwen-2 7B, Mistral Nemo \ub4f1 \uc5ec\ub7ec \ubaa8\ub378\uc758 Micro-Avg(\ud3c9\uade0 \uc815\ud655\ub3c4), DaruMMLU, DaruMERA, DaruSum, DaruCopy(EN), DaruCopy(RU) \uc810\uc218\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378\uc758 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \uc774\ud574 \ubc0f \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub2e4\uc591\ud55c \uc9c0\ud45c\uc785\ub2c8\ub2e4.", "section": "3.1 Open-source LLM Benchmark"}, {"content": "| Model | Vocab | Symbols per token | Micro-Avg | DaruMMLU | DaruMERA | DaruSum | DaruCopy (EN) | DaruCopy (RU) |\n|---|---|---|---|---|---|---|---|---|\n| Mistral-7B | Original | 2,44 | 0,604 | **0,545** | 0,504 | 0,307 | **1,000** | **1,000** |\n|  | BPE | **3,76** | 0,616 | 0,528 | 0,537 | **0,316** | 0,995 | 0,984 |\n|  | Unigram | **3,78** | 0,614 | **0,544** | 0,311 | 0,995 | 0,960 |\n|  | Extended | **3,77** | **0,617** | 0,532 | **0,314** | **1,000** | 0,995 |\n| LLaMa-3-8B | Original | 2,89 | **0,629** | **0,582** | **0,547** | **0,326** | 0,980 | 0,982 |\n|  | BPE | **4,40** | 0,618 | 0,532 | 0,321 | **1,000** | 0,963 |\n|  | Unigram | **4,35** | 0,609 | 0,517 | 0,316 | **1,000** | 0,951 |\n|  | Extended | 3,78 | **0,627** | **0,550** | **0,325** | 0,980 | 0,983 |\n|  | Optimized | 3,40 | 0,620 | 0,552 | 0,536 | 0,323 | 0,981 | **0,989** |", "caption": "Table 2: Darumeru few-shot evaluation results for best language-adaptation checkpoints.", "description": "\ubcf8 \ud45c\ub294 \ub17c\ubb38\uc758 2.4.2\uc808(Case Study: Self-Calibration)\uc5d0\uc11c \uc5b8\uae09\ub41c \ucd5c\uc801\uc758 \uc5b8\uc5b4 \uc801\uc751 \uac80\uc0ac\uc810\uc5d0 \ub300\ud55c Darumeru \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \ubaa8\ub378(Mistral-7B, LLaMa-3-8B)\uacfc \ud1a0\ud070\ud654 \ubc29\ubc95(BPE, Unigram, Extended, Optimized)\uc5d0 \ub530\ub978 \ud3c9\uade0 \uc810\uc218(Micro-Avg), \uadf8\ub9ac\uace0 \uac01 \ud558\uc704 \ubca4\uce58\ub9c8\ud06c(DaruMMLU, DaruMERA, DaruSum, DaruCopy (EN), DaruCopy (RU))\uc5d0 \ub300\ud55c \uc810\uc218\ub97c \ubcf4\uc5ec\uc8fc\uc5b4, \ub2e4\uc591\ud55c \uc5b8\uc5b4 \uc801\uc751 \uc804\ub7b5\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.  'Symbols per token' \uc5f4\uc740 \ud1a0\ud070\ub2f9 \ud3c9\uade0 \uc2ec\ubcfc \uc218\ub97c \ub098\ud0c0\ub0b4\uc5b4, \ud1a0\ud070\ud654 \ud6a8\uc728\uc131\uc744 \ud568\uaed8 \uace0\ub824\ud55c \ubd84\uc11d\uc774 \uac00\ub2a5\ud558\ub3c4\ub85d \ud569\ub2c8\ub2e4.", "section": "2.4 Continued Pre-training"}, {"content": "| Vocab | LEP method | Micro-Avg | DaruMMLU | DaruMERA | DaruSum | DaruCopy (En) | DaruCopy (Ru) |\n|---|---|---|---|---|---|---|---| \n| OpenChat-3.5 |  |  |  |  |  |  |  |\n| BPE | Swap | **0,587** | **0,528** | **0,526** | 0,277 | 0,988 | **0,829** |\n| BPE | Overlap | 0,584 | 0,525 | 0,523 | 0,281 | 0,986 | 0,818 |\n| BPE | Conversion | 0,583 | 0,526 | 0,524 | **0,284** | **0,993** | 0,791 |\n| Unigram | Swap | 0,556 | **0,517** | 0,517 | 0,282 | 0,985 | 0,614 |\n| Unigram | Overlap | **0,572** | 0,514 | **0,534** | 0,297 | 0,981 | **0,680** |\n| Unigram | Conversion | 0,565 | 0,515 | 0,519 | **0,301** | **0,999** | 0,651 |\n| Extended | Swap | **0,608** | **0,535** | **0,540** | 0,298 | **0,999** | **0,907** |\n| Extended | Overlap | **0,607** | **0,535** | **0,539** | **0,307** | **0,999** | 0,898 |\n| Extended | Conversion | **0,609** | **0,535** | **0,541** | **0,306** | **0,999** | **0,909** |\n| LLaMa-3-8B (instruct) |  |  |  |  |  |  |  |\n| BPE | Swap | 0,565 | **0,544** | 0,486 | **0,317** | **0,999** | 0,729 |\n| BPE | Overlap | **0,569** | **0,546** | **0,489** | 0,314 | **0,999** | **0,753** |\n| BPE | Conversion | **0,570** | **0,546** | **0,490** | **0,318** | **0,999** | **0,754** |\n| Unigram | Swap | **0,582** | **0,545** | **0,488** | **0,313** | **0,999** | 0,865 |\n| Unigram | Overlap | 0,580 | **0,545** | 0,482 | **0,314** | **0,999** | 0,876 |\n| Unigram | Conversion | **0,584** | **0,545** | **0,488** | **0,315** | 0,994 | **0,889** |\n| Extended | Swap | 0,592 | **0,557** | 0,498 | **0,319** | 0,969 | 0,921 |\n| Extended | Overlap | **0,597** | **0,556** | **0,504** | **0,321** | 0,964 | **0,936** |\n| Extended | Conversion | **0,597** | **0,556** | 0,501 | 0,318 | **0,994** | 0,921 |\n| Optimized | Swap | 0,594 | **0,554** | **0,499** | **0,327** | 0,970 | **0,928** |\n| Optimized | Overlap | 0,586 | **0,553** | 0,495 | 0,323 | 0,925 | 0,925 |\n| Optimized | Conversion | **0,598** | **0,555** | **0,500** | 0,324 | **0,995** | **0,928** |", "caption": "Table 3: Darumeru zero-shot evaluation results for Learned Embedding Propagation methods.", "description": "\ud45c 3\uc740 \uc81c\uc548\ub41c \ud559\uc2b5 \uc784\ubca0\ub529 \uc804\ud30c(LEP) \ubc29\ubc95\uc758 \uc131\ub2a5\uc744 \ub2e4\ub8e8\uba54\ub8e8 \ubca4\uce58\ub9c8\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud3c9\uac00\ud55c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uac01 \uc5b4\ud718 \uc801\uc751 \ubc29\ubc95(BPE, Unigram, Extended, Optimized)\uc5d0 \ub300\ud574 \uc138 \uac00\uc9c0 LEP \ubc29\ubc95(\uc9c1\uc811 \uc784\ubca0\ub529 \uad50\ud658, \uacb9\uce58\ub294 \ud1a0\ud070 \uc218\uc815, \uc5b4\ud718 \ubcc0\ud658)\uc758 \ub2e4\ub8e8\uba54\ub8e8 \uc810\uc218\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  Mistral-7B\uc640 LLaMa-3-8B \ubaa8\ub378 \ubaa8\ub450\uc5d0 \ub300\ud55c \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.", "section": "3.3 \ud559\uc2b5 \uc784\ubca0\ub529 \uc804\ud30c"}, {"content": "| Model | Fine-tuning data | Micro-Avg | DaruMMLU | DaruMERA | DaruSum | DaruCopy (EN) | DaruCopy (RU) |\n|---|---|---|---|---|---|---|---| \n| OpenChat-3.5 |  |  |  |  |  |  |  |\n| Original model | - | 0,607 | **0,543** | 0,526 | 0,322 | **0,999** | 0,917 |\n|  | saiga d7 | 0,611 | 0,540 | **0,528** | **0,325** | **0,999** | 0,945 |\n|  | +copy task | **0,615** | 0,541 | 0,524 | 0,324 | **1,000** | **0,995** |\n| Unigram | - | 0,565 | 0,515 | 0,519 | 0,301 | **0,999** | 0,651 |\n|  | saiga d7 | 0,599 | 0,556 | 0,316 | **0,999** | 0,754 |\n|  | +copy task | **0,630** | **0,559** | **0,321** | **1,000** | **0,999** |\n| Extended | - | 0,609 | 0,535 | 0,541 | 0,306 | **0,999** | 0,909 |\n|  | saiga d7 | 0,616 | **0,543** | **0,566** | 0,319 | **0,999** | 0,845 |\n|  | +copy task | **0,632** | 0,563 | **0,321** | **1,000** | **0,989** |\n| LLaMa-3-8B (instruct) |  |  |  |  |  |  |  |\n| Original model | - | 0,610 | 0,571 | 0,510 | 0,322 | **1,000** | 0,972 |\n|  | saiga d7 | 0,615 | 0,512 | 0,329 | **1,000** | 0,983 |\n|  | +copy task | **0,616** | **0,513** | **0,332** | **1,000** | **0,995** |\n| Extended | - | 0,597 | 0,556 | 0,501 | 0,318 | 0,994 | 0,921 |\n|  | self-calibration | 0,606 | 0,552 | 0,512 | 0,321 | **1,000** | 0,958 |\n|  | saiga d7 | 0,614 | 0,519 | 0,338 | 0,995 | 0,961 |\n|  | +copy task | **0,618** | 0,565 | **0,521** | **0,339** | **1,000** | **0,984** |\n| Optimized | - | 0,598 | **0,555** | 0,500 | 0,324 | 0,995 | 0,928 |\n|  | self-calibration | 0,601 | 0,550 | 0,501 | 0,325 | **1,000** | 0,950 |\n|  | saiga d7 | 0,611 | 0,515 | 0,336 | **1,000** | 0,971 |\n|  | +copy task | **0,617** | **0,555** | **0,522** | **0,339** | **1,000** | **0,989** |", "caption": "Table 4: Benchmark results for model calibration schemes of Conversion LEP models", "description": "\ud45c 4\ub294 \ubcc0\ud658 LEP \ubaa8\ub378\uc758 \ubaa8\ub378 \ubcf4\uc815 \ubc29\uc2dd\uc5d0 \ub300\ud55c \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  \uc774 \ud45c\ub294 \ub2e4\uc591\ud55c \ud1a0\ud070\ud654 \ubc29\uc2dd(BPE, Unigram, Extended, Optimized)\uacfc \ubcf4\uc815 \uae30\ubc95(\uc6d0\ubcf8 \ubaa8\ub378, Saiga \ub370\uc774\ud130\uc14b\uc73c\ub85c \ucd94\uac00 \ubbf8\uc138 \uc870\uc815, \uc790\uccb4 \ubcf4\uc815)\uc744 \uc0ac\uc6a9\ud55c  OpenChat-3.5\uc640 LLaMa-3-8B \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 DaruMMLU, DaruMERA, DaruSum, DaruCopy (EN), DaruCopy (RU) \ub4f1 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \ube44\uad50 \ubd84\uc11d\ud55c \uacb0\uacfc\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uac01 \ubca4\uce58\ub9c8\ud06c\ub294 \ud2b9\uc815 \uc5b8\uc5b4 \uc774\ud574 \ubc0f \uc0dd\uc131 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\uba70, \ubcf4\uc815 \ubc29\ubc95\uc5d0 \ub530\ub978 \ubaa8\ub378 \uc131\ub2a5 \ud5a5\uc0c1 \ubc0f \uc800\ud558 \uc5ec\ubd80\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "section": "2.4 Case Study: Continued Instruction-Tuning Calibration"}]