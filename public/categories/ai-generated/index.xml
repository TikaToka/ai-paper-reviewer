<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Generated on AI Paper Reviews by AI</title>
    <link>http://localhost:1313/ai-paper-reviewer/categories/ai-generated/</link>
    <description>Recent content in AI Generated on AI Paper Reviews by AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Â© 2024 AI Paper Reviews by AI</copyright>
    <lastBuildDate>Thu, 17 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/ai-paper-reviewer/categories/ai-generated/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13218/</link>
      <pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13218/</guid>
      <description>CBT-BENCH: A new benchmark systematically evaluates LLMs&amp;rsquo; potential for assisting Cognitive Behavioral Therapy (CBT), revealing strengths and weaknesses in various CBT tasks.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13218/cover.png" />
    </item>
    
    <item>
      <title>Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13394/</link>
      <pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13394/</guid>
      <description>The CIA Suite, a novel extensible framework, enables cross-lingual evaluation of multilingual LLMs using evaluator LLMs and a new multilingual benchmark dataset.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13394/cover.png" />
    </item>
    
    <item>
      <title>DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13726/</link>
      <pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13726/</guid>
      <description>DAWN: a novel non-autoregressive diffusion framework for all-at-once generation of dynamic talking head videos, achieving higher quality and speed than autoregressive methods.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13726/cover.png" />
    </item>
    
    <item>
      <title>Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13674/</link>
      <pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13674/</guid>
      <description>Diffusion Curriculum Learning (DisCL) generates high-quality synthetic data via image-guided diffusion, significantly boosting accuracy in long-tail and low-quality data classification.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13674/cover.png" />
    </item>
    
    <item>
      <title>MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13370/</link>
      <pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13370/</guid>
      <description>MagicTailor empowers text-to-image models with component-level control, enabling precise customization of generated images by modifying specific visual elements.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13370/cover.png" />
    </item>
    
    <item>
      <title>MedINST: Meta Dataset of Biomedical Instructions</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13458/</link>
      <pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13458/</guid>
      <description>MEDINST: a massive biomedical instruction dataset (133 tasks, 7M samples) improves LLM cross-task generalization in medical analysis.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13458/cover.png" />
    </item>
    
    <item>
      <title>Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13184/</link>
      <pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13184/</guid>
      <description>Router-Tuning and MindSkip boost Transformer efficiency by dynamically skipping less crucial layers, achieving a 21% speedup with minimal performance loss.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13184/cover.png" />
    </item>
    
    <item>
      <title>SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13276/</link>
      <pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13276/</guid>
      <description>SeerAttention learns to automatically identify and leverage inherent attention sparsity in LLMs, drastically boosting inference speed and scalability while maintaining accuracy.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13276/cover.png" />
    </item>
    
    <item>
      <title>Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13232/</link>
      <pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13232/</guid>
      <description>Boosting LLM-based web agents&amp;rsquo; performance, this study introduces World-Model-Augmented agents that simulate action outcomes for improved decision-making in complex web navigation tasks.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.13232/cover.png" />
    </item>
    
    <item>
      <title>Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.12791/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.12791/</guid>
      <description>KeyNMF, a novel topic modeling method, reveals how Chinese diaspora media&amp;rsquo;s information dynamics correlate with major European political events, highlighting the PRC&amp;rsquo;s potential influence.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.12791/cover.png" />
    </item>
    
    <item>
      <title>Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.12788/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.12788/</guid>
      <description>Meta-Chunking: A novel text segmentation method using LLMs improves RAG efficiency by 1.32 on 2WikiMultihopQA, consuming only 45.8% of the time compared to similarity chunking.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.12788/cover.png" />
    </item>
    
    <item>
      <title>Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.11190/</link>
      <pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.11190/</guid>
      <description>Mini-Omni2: An open-source, multi-modal AI model closely replicating GPT-40&amp;rsquo;s vision, speech, and text capabilities, offering valuable insights for future research.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.11190/cover.png" />
    </item>
    
    <item>
      <title>SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.11331/</link>
      <pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.11331/</guid>
      <description>Shakti: a 2.5B parameter LLM optimized for edge AI, boasts high performance and efficiency on resource-constrained devices via novel VGQA, SwiGLU, and RoPE.</description>
      
    </item>
    
    <item>
      <title>HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.10812/</link>
      <pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.10812/</guid>
      <description>HART, a novel hybrid autoregressive transformer, generates high-quality 1024x1024 images efficiently, rivaling diffusion models while being significantly faster.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.10812/cover.png" />
    </item>
    
  </channel>
</rss>
