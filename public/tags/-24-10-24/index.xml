<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ðŸ”– 24-10-24 on AI Paper Reviews by AI</title>
    <link>http://localhost:1313/ai-paper-reviewer/tags/-24-10-24/</link>
    <description>Recent content in ðŸ”– 24-10-24 on AI Paper Reviews by AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Â© 2024 AI Paper Reviews by AI</copyright>
    <lastBuildDate>Thu, 24 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/ai-paper-reviewer/tags/-24-10-24/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18976/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18976/</guid>
      <description>CAMEL-Bench: a new Arabic LMM benchmark enabling comprehensive evaluation of large multimodal models across eight diverse domains, revealing significant room for improvement even in state-of-the-art m&amp;hellip;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18976/cover.png" />
    </item>
    
    <item>
      <title>CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18505/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18505/</guid>
      <description>CCI3.0-HQ: A new 500GB high-quality Chinese dataset boosts Chinese LLM performance, outperforming existing datasets on key benchmarks.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18505/cover.png" />
    </item>
    
    <item>
      <title>Data Scaling Laws in Imitation Learning for Robotic Manipulation</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18647/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18647/</guid>
      <description>Robotic manipulation policies achieve near 90% success in novel environments and with unseen objects using a data-efficient imitation learning approach guided by discovered power-law scaling laws.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18647/cover.png" />
    </item>
    
    <item>
      <title>DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18860/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18860/</guid>
      <description>DeCoRe, a training-free decoding strategy, significantly reduces LLM hallucinations by contrasting outputs from masked and unmasked retrieval heads, improving contextual faithfulness.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18860/cover.png" />
    </item>
    
    <item>
      <title>Distill Visual Chart Reasoning Ability from LLMs to MLLMs</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18798/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18798/</guid>
      <description>Researchers created REACHQA, a dataset improving visual reasoning in LLMs by using code as an intermediary to translate chart representations into text, enabling efficient and scalable data synthesis.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18798/cover.png" />
    </item>
    
    <item>
      <title>Framer: Interactive Frame Interpolation</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18978/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18978/</guid>
      <description>Framer: a novel interactive frame interpolation method allows users to customize video transitions by intuitively adjusting keypoints, resulting in seamless and creative video generation.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18978/cover.png" />
    </item>
    
    <item>
      <title>LOGO -- Long cOntext aliGnment via efficient preference Optimization</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18533/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18533/</guid>
      <description>LOGO, a novel training strategy, significantly enhances long-context language model generation by efficiently optimizing preferences, achieving performance comparable to GPT-4 on real-world tasks with&amp;hellip;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18533/cover.png" />
    </item>
    
    <item>
      <title>MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18977/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18977/</guid>
      <description>MotionCLR: Training-free human motion editing via attention mechanism manipulation.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18977/cover.png" />
    </item>
    
    <item>
      <title>Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18775/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18775/</guid>
      <description>VINE: A novel watermarking method significantly enhances robustness against advanced image editing techniques while maintaining high image quality, outperforming existing methods.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18775/cover.png" />
    </item>
    
    <item>
      <title>Should We Really Edit Language Models? On the Evaluation of Edited Language Models</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18785/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18785/</guid>
      <description>Language model editing, while efficient for small updates, causes inevitable performance drops and safety issues when scaled, urging a reassessment of its practical applications.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18785/cover.png" />
    </item>
    
    <item>
      <title>Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18451/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18451/</guid>
      <description>Skywork-Reward achieves state-of-the-art results on RewardBench using a smaller, high-quality preference dataset and refined training techniques, highlighting the importance of data curation in LLM re&amp;hellip;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18451/cover.png" />
    </item>
    
    <item>
      <title>SMITE: Segment Me In TimE</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18538/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18538/</guid>
      <description>SMITE: a novel method for flexible-granularity video segmentation using only a few reference images, achieving temporally consistent results with superior accuracy.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18538/cover.png" />
    </item>
    
    <item>
      <title>Stable Consistency Tuning: Understanding and Improving Consistency Models</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18958/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18958/</guid>
      <description>Stable Consistency Tuning (SCT) boosts consistency model speed and quality by reducing training variance and discretization errors, achieving new state-of-the-art results on ImageNet-64.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18958/cover.png" />
    </item>
    
    <item>
      <title>Taipan: Efficient and Expressive State Space Language Models with Selective Attention</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18572/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18572/</guid>
      <description>Taipan, a hybrid language model, efficiently handles long contexts (up to 1 million tokens) by selectively applying attention, outperforming existing models in both speed and accuracy.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18572/cover.png" />
    </item>
    
    <item>
      <title>The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18441/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18441/</guid>
      <description>This paper enhances Transformer models by applying probabilistic optimization, yielding efficient subword encoding, hyperparameter optimization, and novel attention mechanisms for improved generative &amp;hellip;</description>
      
    </item>
    
    <item>
      <title>Unbounded: A Generative Infinite Game of Character Life Simulation</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18975/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18975/</guid>
      <description>UNBOUNDED, a generative infinite game, uses AI to create an open-ended character life simulation where players interact using natural language, transcending traditional game design.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18975/cover.png" />
    </item>
    
    <item>
      <title>Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18693/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18693/</guid>
      <description>ScaleQuest synthesizes a million high-quality mathematical reasoning problems using efficient open-source methods, substantially boosting LLM reasoning performance.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18693/cover.png" />
    </item>
    
    <item>
      <title>WAFFLE: Multi-Modal Model for Automated Front-End Development</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18362/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18362/</guid>
      <description>WAFFLE, a novel multi-modal model, revolutionizes front-end development by accurately translating UI designs into HTML code using structure-aware attention and contrastive learning, significantly outp&amp;hellip;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18362/cover.png" />
    </item>
    
    <item>
      <title>Why Does the Effective Context Length of LLMs Fall Short?</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18745/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18745/</guid>
      <description>Researchers unveil why LLMs underperform with long contexts, attributing it to skewed position frequency distribution, and introduce STRING, a training-free method that dramatically enhances long-cont&amp;hellip;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18745/cover.png" />
    </item>
    
  </channel>
</rss>
