[{"content":"","date":"12 December 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-shanghai-artificial-intelligence-laboratory/","section":"Tags","summary":"","title":"ğŸ¢ Shanghai Artificial Intelligence Laboratory","type":"tags"},{"content":"","date":"12 December 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/ai-generated/","section":"Categories","summary":"","title":"AI Generated","type":"categories"},{"content":"","date":"12 December 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/","section":"AI Paper Reviews by AI","summary":"","title":"AI Paper Reviews by AI","type":"page"},{"content":"","date":"12 December 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"12 December 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/human-ai-interaction/","section":"Tags","summary":"","title":"Human-AI Interaction","type":"tags"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2412.09596 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rPan Zhang et el. ğŸ¤— 2024-12-13 â†— arXiv â†— Hugging Face â†— Papers with Code TL;DR # í˜„ì¡´í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë“œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(MLLM)ì€ ìˆœì°¨ì  êµ¬ì¡°ë¡œ ì¸í•´ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬ì™€ ì¥ê¸°ê°„ ìƒí˜¸ì‘ìš©ì— ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. íŠ¹íˆ, ëª¨ë“  ì •ë³´ë¥¼ ì¥ê¸°ê°„ ìœ ì§€í•˜ëŠ” ê²ƒì€ ë¹„ìš©ê³¼ íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œ ë¹„ì‹¤ìš©ì ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ì—°êµ¬ëŠ” Specialized Generalist AIì˜ ê°œë…ì—ì„œ ì˜ê°ì„ ì–»ì–´, ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ë° ì˜¤ë””ì˜¤ ë°ì´í„°ì— ëŒ€í•œ ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ìƒˆë¡œìš´ ì‹œìŠ¤í…œì¸ InternLM-XComposer2.5-OmniLive(IXC2.5-OL)ì„ ì œì‹œí•©ë‹ˆë‹¤.\nIXC2.5-OLì€ ìŠ¤íŠ¸ë¦¬ë° ì§€ê°, ë‹¤ì¤‘ ëª¨ë“œ ì¥ê¸° ê¸°ì–µ, ì¶”ë¡  ëª¨ë“ˆì˜ ì„¸ ê°€ì§€ ì£¼ìš” ëª¨ë“ˆë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¬ë° ì§€ê° ëª¨ë“ˆì€ ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹¤ì¤‘ ëª¨ë“œ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ê³  ì£¼ìš” ì •ë³´ë¥¼ ê¸°ì–µì— ì €ì¥í•˜ë©°, ì‚¬ìš©ì ì§ˆë¬¸ì— ë”°ë¼ ì¶”ë¡ ì„ ì´‰ë°œí•©ë‹ˆë‹¤. ë‹¤ì¤‘ ëª¨ë“œ ì¥ê¸° ê¸°ì–µ ëª¨ë“ˆì€ ë‹¨ê¸° ê¸°ì–µì„ ì¥ê¸° ê¸°ì–µìœ¼ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ì••ì¶•í•˜ì—¬ ê²€ìƒ‰ íš¨ìœ¨ì„±ê³¼ ì •í™•ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì¶”ë¡  ëª¨ë“ˆì€ ì§ˆë¬¸ì— ì‘ë‹µí•˜ê³  ì¶”ë¡  ì‘ì—…ì„ ì‹¤í–‰í•˜ë©°, ì§€ê° ë° ê¸°ì–µ ëª¨ë“ˆê³¼ í˜‘ë ¥í•©ë‹ˆë‹¤. IXC2.5-OLì€ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œë˜ì–´ ë‹¤ë¥¸ ì—°êµ¬ìë“¤ì˜ ì—°êµ¬ì— ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # ë³¸ ë…¼ë¬¸ì€ ì¥ê¸°ê°„ì— ê±¸ì¹œ ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ë° ì˜¤ë””ì˜¤ ìƒí˜¸ì‘ìš©ì„ ìœ„í•œ í¬ê´„ì ì¸ ë‹¤ì¤‘ ëª¨ë“œ ì‹œìŠ¤í…œì„ ì œì‹œí•¨ìœ¼ë¡œì¨ AI ì—°êµ¬ìë“¤ì—ê²Œ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì‹¤ì‹œê°„ ì§€ê°, ê¸°ì–µ, ì¶”ë¡  ë©”ì»¤ë‹ˆì¦˜ì„ ë¶„ë¦¬í•˜ì—¬ ì¸ê°„ì˜ ì¸ì§€ ëŠ¥ë ¥ì„ ëª¨ë°©í•˜ê³ , ì§€ì†ì ì¸ ì ì‘í˜• ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ì‹œìŠ¤í…œ ì„¤ê³„ëŠ” AI ë¶„ì•¼ì˜ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì—´ì–´ì¤ë‹ˆë‹¤. ë˜í•œ, ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œëœ ì½”ë“œ ë° ëª¨ë¸ì€ ë‹¤ë¥¸ ì—°êµ¬ìë“¤ì´ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë”ìš± ë°œì „ëœ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° í¬ê²Œ ê¸°ì—¬í•  ê²ƒì…ë‹ˆë‹¤. íŠ¹íˆ, ì¥ê¸°ê°„ ìƒí˜¸ì‘ìš©ì— ëŒ€í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ë ¤ëŠ” ì‹œë„ëŠ”, ì§€ì†ì ì´ê³  ì ì‘ë ¥ ìˆëŠ” AI ì‹œìŠ¤í…œ ê°œë°œì— ëŒ€í•œ ì¤‘ìš”í•œ ë°œì „ ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.\nVisual Insights # ğŸ”¼ ê·¸ë¦¼ 1ì€ ì‚¬ëŒì˜ ì¸ì§€ ëŠ¥ë ¥ê³¼ ì „ë¬¸í™”ëœ ì¼ë°˜í™” AIì—ì„œ ì˜ê°ì„ ì–»ì–´ ê°œë°œëœ InternLM-XComposer2.5-OmniLive (IXC2.5-OL) ì‹œìŠ¤í…œì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ì‹¤ì‹œê°„ ìƒí˜¸ ì‘ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ëª¨ë“ˆë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì²«ì§¸, ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ë° ì˜¤ë””ì˜¤ ì…ë ¥ì„ ì§€ì›í•˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì¸ì‹ ëª¨ë“ˆì…ë‹ˆë‹¤. ë‘˜ì§¸, ë‹¨ê¸° ë©”ëª¨ë¦¬ë¥¼ ì¥ê¸° ë©”ëª¨ë¦¬ë¡œ ì••ì¶•í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë“œ ì¥ê¸° ë©”ëª¨ë¦¬ ëª¨ë“ˆì…ë‹ˆë‹¤. ì…‹ì§¸, ê²€ìƒ‰ëœ ë©”ëª¨ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì¶”ë¡  ëª¨ë“ˆì…ë‹ˆë‹¤. ê° ëª¨ë“ˆì€ ì‹œìŠ¤í…œì˜ ì—°ì†ì ì´ê³  ì ì‘ì ì¸ ì„œë¹„ìŠ¤ ì œê³µì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\nread the caption Figure 1: Inspired by human-like cognition and Specialized Generalist AI, we introduce InternLM-XComposer2.5-OmniLive (IXC2.5-OL), a system that facilitates real-time interaction with: (1) a streaming perception module supports streaming video and audio inputs; (2) a multi-modal long memory module that compresses short-term memory into long-term memory; and (3) a reasoning module that answers queries based on retrieved memories. Stage Task Dataset Data Num Pretrain ASR GigaSpeech [11] 8,282,987 SFT ASR WenetSpeech [140] 17,821,017 LibriSpeech [87] 281,241 VCTK [111] 44,070 AISHELL-1 [8] 120,098 AISHELL-4 [39] 102,254 MD-RAMC [129] 219,325 ASCEND [76] 12,314 KeSpeech [106] 888,428 DASR [27] 190,732 CommonVoice [2] 2,813,852 CLS FSD50K [35] 40,966 AudioSet [53] 18,683 Silence 475 ğŸ”¼ í‘œ 1ì€ ë…¼ë¬¸ì˜ ì˜¤ë””ì˜¤ ë²ˆì—­ ëª¨ë“ˆì— ëŒ€í•œ ì‚¬ì „ í›ˆë ¨ ë° ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì •ì— ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì— ëŒ€í•œ ê°œìš”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‚¬ì „ í›ˆë ¨ ë‹¨ê³„ëŠ” GigaSpeech ë° WenetSpeech ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ìë™ ìŒì„± ì¸ì‹(ASR) ì‘ì—…ì—ë§Œ ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ì§€ë„ í•™ìŠµ ë¯¸ì„¸ ì¡°ì • ë‹¨ê³„ëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ASR ë° ì˜¤ë””ì˜¤ ë¶„ë¥˜(CLS) ì‘ì—…ì„ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤. Common Voiceì˜ ê²½ìš° ì˜ì–´ ë° ì¤‘êµ­ì–´ ë¶„í• ë§Œ ì‚¬ìš©í•˜ë©°, CLS ì‘ì—…ì—ëŠ” 475ê°œì˜ ìì²´ ì œì‘ëœ \u0026lsquo;ì¹¨ë¬µ\u0026rsquo; ìƒ˜í”Œì´ ì¶”ê°€ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ë³¸ í‘œëŠ” ë°ì´í„°ì…‹ì˜ ì´ë¦„, ì‘ì—… ì¢…ë¥˜, ë°ì´í„°ì…‹ í¬ê¸° ë“±ì˜ ì •ë³´ë¥¼ ì œê³µí•˜ì—¬ ì˜¤ë””ì˜¤ ë²ˆì—­ ëª¨ë“ˆì˜ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•œ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì˜ ì—­í• ì„ ëª…í™•íˆ ë³´ì—¬ì¤ë‹ˆë‹¤.\nread the caption Table 1: Overview of datasets used in pretraining and supervised fine-tuning (SFT) for the Audio Translation Module. The pretraining stage focuses solely on the automatic speech recognition (ASR) task, utilizing the GigaSpeech and WenetSpeech datasets. The SFT stage includes both ASR and audio classification (CLS) tasks, leveraging diverse datasets. For CommonVoice, we only use the English and Chinese splits. Additionally, 475 self-constructed â€œSilenceâ€ samples are used for CLS tasks. In-depth insights # Multimodal Streaming # ë³¸ ë…¼ë¬¸ì€ ë©€í‹°ëª¨ë‹¬ ìŠ¤íŠ¸ë¦¬ë°ì— ëŒ€í•œ ì‹¬ë„ìˆëŠ” ë…¼ì˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ë° ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ì™€ ê´€ë ¨ëœ ì–´ë ¤ì›€ì„ ê°•ì¡°í•˜ë©°, ê¸°ì¡´ì˜ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ì•„í‚¤í…ì²˜ ê¸°ë°˜ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë¶„ë¦¬ëœ ìŠ¤íŠ¸ë¦¬ë° ì¸ì‹, ì¶”ë¡  ë° ë©”ëª¨ë¦¬ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì§€ì†ì ì´ê³  ì ì‘ì ì¸ ì„œë¹„ìŠ¤ ì œê³µì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ì¥ê¸°ê°„ì˜ ìƒí˜¸ ì‘ìš©ì—ì„œë„ íš¨ìœ¨ì„±ì„ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨ê¸° ë° ì¥ê¸° ë©”ëª¨ë¦¬ í†µí•©ì€ íš¨ê³¼ì ì¸ ì •ë³´ ê²€ìƒ‰ê³¼ ì •í™•ë„ í–¥ìƒì— ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, ì¸ê°„ì˜ ì¸ì§€ ëŠ¥ë ¥ ëª¨ë°©ì„ ì‹œë„í•˜ëŠ” í•µì‹¬ ì „ëµì…ë‹ˆë‹¤. íŠ¹íˆ, ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ ë°ì´í„°ì˜ ë™ì‹œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì „ëµì´ ì¤‘ìš”í•˜ê²Œ ë‹¤ë¤„ì§€ë©°, ì´ëŠ” ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ìˆ˜ì¤€ì„ ë„˜ì–´ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒí™©ì„ ì´í•´í•˜ê³  ë°˜ì‘í•˜ëŠ” ì§€ëŠ¥í˜• ì‹œìŠ¤í…œ êµ¬ì¶•ìœ¼ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ëŠ” ì œì•ˆëœ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì…ì¦í•˜ë©°, ì‹¤ì œ ì„œë¹„ìŠ¤ ì ìš© ê°€ëŠ¥ì„±ì„ ë†’ì…ë‹ˆë‹¤.\nLong-Term Memory # ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ ì¥ê¸° ê¸°ì–µ ë©”ì»¤ë‹ˆì¦˜ì€ ë‹¨ìˆœíˆ ê³¼ê±° ì •ë³´ë¥¼ ë¬´í•œì • ì €ì¥í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, íš¨ìœ¨ì ì¸ ì •ë³´ ì••ì¶• ë° ê²€ìƒ‰ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¸ê°„ì˜ ë‡Œê°€ ë‹¨ê¸° ê¸°ì–µì„ ì¥ê¸° ê¸°ì–µìœ¼ë¡œ ì••ì¶•í•˜ì—¬ ì €ì¥í•˜ëŠ” ë°©ì‹ì—ì„œ ì˜ê°ì„ ì–»ì€ ê²ƒìœ¼ë¡œ, ì œí•œëœ ìš©ëŸ‰ ë‚´ì—ì„œ ì¥ê¸°ê°„ì— ê±¸ì¹œ ìƒí˜¸ì‘ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë‹¨ê¸° ê¸°ì–µì€ ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë§Œì„ ì¶”ì¶œí•˜ì—¬ ì••ì¶•í•˜ê³ , ì´ë¥¼ ì¥ê¸° ê¸°ì–µìœ¼ë¡œ í†µí•©í•˜ëŠ” ê³¼ì •ì„ í†µí•´ íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ì˜ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì €ì¥í•¨ìœ¼ë¡œì¨, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ë‹¤ì–‘í•œ ì •ë³´ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œ ì¶”ë¡ ì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ ê¸°ì¡´ì˜ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì°½ì— ëª¨ë“  ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë°©ì‹ì˜ ë¹„íš¨ìœ¨ì„±ì„ ê·¹ë³µí•˜ê³ , ì‹¤ì‹œê°„ ìƒí˜¸ ì‘ìš©ì´ í•„ìš”í•œ ì‹œìŠ¤í…œì— ì í•©í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì˜ ì¥ê¸° ê¸°ì–µ ë©”ì»¤ë‹ˆì¦˜ì€ ì¸ê°„ì˜ ì¸ì§€ ê³¼ì •ì„ ëª¨ë°©í•˜ì—¬ ì§€ì†ì ì´ê³  ì ì‘ì ì¸ AI ì„œë¹„ìŠ¤ ì œê³µì— ê¸°ì—¬í•˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤.\nSpecialized Generalist # ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ \u0026ldquo;ì „ë¬¸ê°€ ì¼ë°˜í™”\u0026rdquo; ê°œë…ì€ ë‹¨ì¼ ê±°ëŒ€ ëª¨ë¸ì´ ëª¨ë“  ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ëŒ€ì‹ , íŠ¹ì • ê¸°ëŠ¥ì— íŠ¹í™”ëœ ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ í†µí•©í•˜ì—¬ ìƒí˜¸ ì‘ìš©í•˜ëŠ” ì‹œìŠ¤í…œì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ì¸ê°„ì˜ ë‘ë‡Œê°€ íŠ¹ì • ì˜ì—­(ì‹œê°, ì²­ê°, ê¸°ì–µ, ì¶”ë¡ )ì„ ë‹´ë‹¹í•˜ëŠ” ì „ë¬¸í™”ëœ ì˜ì—­ìœ¼ë¡œ ë‚˜ë‰˜ì–´ ìˆìœ¼ë©´ì„œë„, ì´ë“¤ ì˜ì—­ì´ í†µí•©ì ìœ¼ë¡œ ì‘ìš©í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹ì—ì„œ ì˜ê°ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ë° ì˜¤ë””ì˜¤ ì²˜ë¦¬ì— ìˆì–´, ê° ëª¨ë“ˆ(ì§€ê°, ê¸°ì–µ, ì¶”ë¡ )ì˜ ì „ë¬¸í™”ëŠ” ì‹¤ì‹œê°„ ìƒí˜¸ ì‘ìš©ì˜ íš¨ìœ¨ì„±ê³¼ ì •í™•ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‹¤ì‹œê°„ ì§€ê° ëª¨ë“ˆì€ ì˜ìƒê³¼ ìŒì„± ë°ì´í„°ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ì—¬ ì£¼ìš” ì •ë³´ë§Œì„ ì¶”ì¶œí•˜ê³ , ì¥ê¸° ê¸°ì–µ ëª¨ë“ˆì€ ë‹¨ê¸° ê¸°ì–µì„ íš¨ìœ¨ì ìœ¼ë¡œ ì••ì¶•í•˜ì—¬ ì¥ê¸° ê¸°ì–µìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì „ë¬¸í™”ëœ ëª¨ë“ˆ ê°„ì˜ íš¨ìœ¨ì ì¸ ì •ë³´ êµë¥˜ë¥¼ í†µí•´ ì§€ì†ì ì´ê³  ì ì‘ì ì¸ ì„œë¹„ìŠ¤ê°€ ê°€ëŠ¥í•´ì§€ë©°, ì¸ê°„ì˜ ì¸ì§€ ëŠ¥ë ¥ì— ê°€ê¹Œìš´ AI ì‹œìŠ¤í…œ êµ¬í˜„ì— í•œê±¸ìŒ ë” ë‹¤ê°€ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¨ìˆœí•œ ê±°ëŒ€ ëª¨ë¸ì˜ í™•ì¥ì´ ì•„ë‹Œ, ì‹œìŠ¤í…œ ì„¤ê³„ ë° ê¸°ëŠ¥ ë¶„í• ì„ í†µí•œ ê·¼ë³¸ì ì¸ ì ‘ê·¼ ë°©ì‹ì˜ ë³€í™”ë¥¼ ì˜ë¯¸í•˜ë©°, ì•ìœ¼ë¡œì˜ AI ì‹œìŠ¤í…œ ê°œë°œ ë°©í–¥ì— ì¤‘ìš”í•œ ì‹œì‚¬ì ì„ ì œê³µí•©ë‹ˆë‹¤.\nBenchmark Results # ë³¸ ë…¼ë¬¸ì˜ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ëŠ” ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ SOTA ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤ëŠ” ì ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ, ì˜¤ë””ì˜¤ ì¸ì‹ê³¼ ë¹„ë””ì˜¤ ì´í•´ ì‘ì—… ëª¨ë‘ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ê²°ê³¼ë¥¼ ì œì‹œí•˜ë©°, íŠ¹íˆ ì œí•œëœ ë§¤ê°œë³€ìˆ˜ ê·œëª¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•œ ì ì´ ì¸ìƒì ì…ë‹ˆë‹¤. ì´ëŠ” ì œì•ˆëœ ëª¨ë¸ì˜ íš¨ìœ¨ì„±ê³¼ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¬ë° ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ ìƒë‹¹í•œ ê²½ìŸë ¥ì„ ë³´ì—¬ì£¼ì–´ ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©ì— ëŒ€í•œ ì í•©ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, ë¹„êµ ëŒ€ìƒ ëª¨ë¸ì˜ ì¢…ë¥˜ ë° ë²„ì „ì— ëŒ€í•œ ëª…í™•í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ê²°ê³¼ í•´ì„ì— ì£¼ì˜ê°€ í•„ìš”í•˜ë©°, ì¶”ê°€ì ì¸ ë²¤ì¹˜ë§ˆí¬ ë° ë¶„ì„ì„ í†µí•´ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ê³¼ í•œê³„ë¥¼ ë³´ë‹¤ ë©´ë°€í•˜ê²Œ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì œì‹œëœ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ëŠ” ê³ ë¬´ì ì´ì§€ë§Œ, ë³´ë‹¤ í­ë„“ì€ í‰ê°€ì™€ ì‹¬ì¸µì ì¸ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.\nFuture Directions # ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ InternLM-XComposer2.5-OmniLive (IXC2.5-OL) ì‹œìŠ¤í…œì€ ì¥ê¸°ê°„ì— ê±¸ì¹œ ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ë° ì˜¤ë””ì˜¤ ìƒí˜¸ì‘ìš©ì„ ìœ„í•œ íšê¸°ì ì¸ ì‹œë„ì´ë‚˜, ì—¬ì „íˆ ê°œì„ ì˜ ì—¬ì§€ê°€ ë§ì€ ë¶„ì•¼ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë¯¸ë˜ ì—°êµ¬ ë°©í–¥ìœ¼ë¡œëŠ” ì²«ì§¸, ëª¨ë“ˆ ê°„ì˜ ë”ìš± íš¨ìœ¨ì ì¸ í†µí•©ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì€ ëª¨ë“ˆë“¤ì´ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ë°, ì´ëŠ” ì²˜ë¦¬ ì†ë„ ë° íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œ ê°œì„ ë  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ë‘˜ì§¸, ëª¨ë¸ì˜ í™•ì¥ì„± ë° ì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒì— ì£¼ë ¥í•´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸ì€ íŠ¹ì • ë°ì´í„°ì…‹ì— ëŒ€í•´ í›ˆë ¨ë˜ì—ˆìœ¼ë¯€ë¡œ, ë‹¤ì–‘í•œ í™˜ê²½ ë° ë°ì´í„°ì— ëŒ€í•œ ì ì‘ë ¥ì„ ë†’ì´ëŠ” ì—°êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì…‹ì§¸, ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œì¼œì•¼ í•©ë‹ˆë‹¤. ì‹¤ì œ ì‘ìš© ë¶„ì•¼ì—ì„œëŠ” ë§¤ìš° ë¹ ë¥¸ ì‘ë‹µ ì†ë„ê°€ ìš”êµ¬ë˜ë¯€ë¡œ, ì—°ì‚° íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ì•Œê³ ë¦¬ì¦˜ ë° í•˜ë“œì›¨ì–´ ê°€ì†í™” ê¸°ìˆ  ê°œë°œì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ë„·ì§¸, ë‹¤ì–‘í•œ ì–¸ì–´ ë° ë¬¸í™”ì  ë°°ê²½ì— ëŒ€í•œ ì§€ì›ì„ í™•ëŒ€í•´ì•¼ í•©ë‹ˆë‹¤. ì „ ì„¸ê³„ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ë¬¸í™”ë¥¼ í¬ê´„í•˜ëŠ” ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ìœ¤ë¦¬ì  ë° ì‚¬íšŒì  ì±…ì„ì„ ê³ ë ¤í•œ ì—°êµ¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤. AI ì‹œìŠ¤í…œì˜ í¸í–¥ì„±, í”„ë¼ì´ë²„ì‹œ, ì•ˆì „ì„± ë“±ì— ëŒ€í•œ ì² ì €í•œ ê²€í†  ë° ëŒ€ë¹„ì±… ë§ˆë ¨ì´ í•„ìš”í•©ë‹ˆë‹¤.\nMore visual insights # More on figures ğŸ”¼ ê·¸ë¦¼ 2ëŠ” InternLM-XComposer2.5-OmniLive (IXC2.5-OL) ì‹œìŠ¤í…œì˜ íŒŒì´í”„ë¼ì¸ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. IXC2.5-OLì€ ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš© ì‹œìŠ¤í…œìœ¼ë¡œ, ë™ì‹œì— ì‘ë™í•˜ëŠ” ì„¸ ê°€ì§€ ëª¨ë“ˆë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. 1) ìŠ¤íŠ¸ë¦¬ë° ì¸ì‹ ëª¨ë“ˆ: ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹œì²­ê° ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ê³  ì£¼ìš” ì„¸ë¶€ ì •ë³´ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ë©° ì‚¬ìš©ì ì¿¼ë¦¬ì— ë”°ë¼ ì¶”ë¡ ì„ í™œì„±í™”í•©ë‹ˆë‹¤. 2) ë‹¤ì¤‘ ëª¨ë“œ ì¥ê¸° ë©”ëª¨ë¦¬ ëª¨ë“ˆ: ë‹¨ê¸° ë° ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ í†µí•©í•˜ì—¬ íš¨ìœ¨ì ì¸ ê²€ìƒ‰ê³¼ ì •í™•ë„ í–¥ìƒì„ ìœ„í•´ ë‹¨ê¸° ë©”ëª¨ë¦¬ë¥¼ ì¥ê¸° ë©”ëª¨ë¦¬ë¡œ ì••ì¶•í•©ë‹ˆë‹¤. 3) ì¶”ë¡  ëª¨ë“ˆ: ì¿¼ë¦¬ì— ì‘ë‹µí•˜ê³  ì¶”ë¡  ì‘ì—…ì„ ì‹¤í–‰í•˜ë©° ì¸ì‹ ë° ë©”ëª¨ë¦¬ ëª¨ë“ˆê³¼ ì¡°ì •í•©ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì‹œìŠ¤í…œì˜ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë¨¼ì € ìŒì„± ì¸ì‹ì„ í†µí•´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ë˜ê³ , ì´í›„ ìŠ¤íŠ¸ë¦¬ë° ì¸ì‹ ëª¨ë“ˆê³¼ ë‹¤ì¤‘ ëª¨ë“œ ì¥ê¸° ë©”ëª¨ë¦¬ ëª¨ë“ˆì„ í†µí•´ ì²˜ë¦¬ë˜ì–´ ì¶”ë¡  ëª¨ë“ˆì— ì „ë‹¬ë©ë‹ˆë‹¤. ì¶”ë¡  ëª¨ë“ˆì€ ê´€ë ¨ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ í…ìŠ¤íŠ¸ ì‘ë‹µì€ TTS (í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜) ëª¨ë“ˆì„ í†µí•´ ìŒì„±ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ë©ë‹ˆë‹¤.\nread the caption Figure 2: Pipeline of the InternLM-XComposer2.5-OmniLive. (IXC2.5-OL). The IXC2.5-OL is a real-time interacting system that is constructed by three simultaneous modules: 1) the Streaming Perception Module, 2) the Multi-modal Long Memory Module, and 3) the Reasoning Module. ğŸ”¼ ê·¸ë¦¼ 3ì€ IXC2.5-OL ì‹œìŠ¤í…œì˜ íŒŒì´í”„ë¼ì¸ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ í”„ëŸ°íŠ¸ì—”ë“œ, SRS ì„œë²„, ë°±ì—”ë“œ ì„œë²„ì˜ ì„¸ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. í”„ëŸ°íŠ¸ì—”ë“œëŠ” ë¹„ë””ì˜¤ ë° ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ìº¡ì²˜í•˜ê³  ë°±ì—”ë“œ ì„œë²„ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì¬ìƒí•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. SRS ì„œë²„ëŠ” ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ë©°, ë°±ì—”ë“œ ì„œë²„ëŠ” ì˜¤ë””ì˜¤ì™€ ë¹„ë””ì˜¤ë¥¼ ì½ê³ , ë©”ëª¨ë¦¬ë¥¼ ì¶”ì¶œí•˜ê³ , ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ê·¸ë¦¼ì—ì„œ ë…¹ìƒ‰ ìƒìëŠ” ìŠ¤ë ˆë“œ ë˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nread the caption Figure 3: System pipeline of the IXC2.5-OL. The system comprises the Frontend, SRS Server, and Backend Server. The Frontend is utilized for capturing video and audio streams and for playing audio from the Backend Server. The SRS Server is employed for managing live streams. The Backend Server is responsible for reading audio and video, extracting memory, and answering questions. The green boxes in the figure represent a thread or a process. More on tables Model Dataset Memory Module ShareGPT4Video [15], Ego4D [41], ActivityNet [32], Semantics Implicit QA, Reference Implicit QA IXC2.5 ShareGPT4Video [15], ActivityNet [32], FunQA [122], TrafficQA [125], VideoChat2-IT [61], LLaVA-Video [152] ğŸ”¼ í‘œ 2ëŠ” ë…¼ë¬¸ì˜ IXC2.5-OL ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©ëœ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ ëª©ë¡ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ë°ì´í„°ì…‹ì€ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ë¹„ë””ì˜¤ì™€ ì§ˆì˜ì‘ë‹µ ìŒ(question-answer pairs)ì„ í¬í•¨í•˜ë©°, ì‹œìŠ¤í…œì˜ ë‹¤ì–‘í•œ ëª¨ë“ˆ(Streaming Perception Module, Multi-modal Long Memory Module, Reasoning Module) í•™ìŠµì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, \u0026lsquo;Semantics Implicit Question\u0026rsquo; ë° \u0026lsquo;Reference Implicit Question\u0026rsquo;ê³¼ ê°™ì´ ê°„ì ‘ì ì¸ ì§ˆë¬¸ì„ í¬í•¨í•˜ëŠ” ë°ì´í„°ì…‹ë„ í¬í•¨ë˜ì–´ ì‹œìŠ¤í…œì˜ ê²¬ê³ ì„± ë° ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.\nread the caption Table 2: Video Datasets used in IXC2.5-OL. Method LLM Wenetspeech (CN) Librispeech (ENG) Test_Net â†“ Test_Meeting â†“ \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; Qwen2-Audio [26] Qwen2-7B [128] 7.8 8.4 Mini-Omni [123] Qwen2-0.5B [128] - - VITA [38] Mixtral-8x7B [47] 12.2 16.5 IXC2.5-OL Qwen2-1.5B [128] 9.0 9.2 ğŸ”¼ í‘œ 3ì€ ìë™ ìŒì„± ì¸ì‹(ASR) ì‘ì—…ì— ëŒ€í•œ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. \u0026lsquo;CN\u0026rsquo;ì€ ì¤‘êµ­ì–´ ìŒì„±ì„, \u0026lsquo;ENG\u0026rsquo;ëŠ” ì˜ì–´ ìŒì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì„±ëŠ¥ì€ WER(ë‹¨ì–´ ì˜¤ë¥˜ìœ¨)ì„ ì‚¬ìš©í•˜ì—¬ ì¸¡ì •ë©ë‹ˆë‹¤. ì´ í‘œëŠ” ë‹¤ì–‘í•œ ëª¨ë¸(Qwen2-Audio, Qwen2-7B, Mini-Omni, VITA, IXC2.5-OL)ì˜ ì¤‘êµ­ì–´ ë° ì˜ì–´ ìŒì„± ì¸ì‹ ì„±ëŠ¥ì„ WER ìˆ˜ì¹˜ë¥¼ í†µí•´ ë¹„êµ ë¶„ì„í•˜ì—¬ ê° ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. WER ê°’ì´ ë‚®ì„ìˆ˜ë¡ ë” ë†’ì€ ì •í™•ë„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\nread the caption Table 3: Evaluation results on ASR tasks: â€CNâ€ refers to Chinese speech, while â€ENGâ€ refers to English speech. The performance is measured using WER â†“â†“\\downarrowâ†“ (Word Error Rate). Method Params Topic Rea. Anomaly Recog. Needle QA Ego Rea. Plot QA Action Or. Action Co. M-Avg Closed-source APIs. Claude-3-Opus - 67.2 43.5 21.6 40.2 47.8 18.2 16.7 36.5 Qwen-VL-Max - 67.4 63.5 40.3 40.9 43.3 25.0 14.8 42.2 GPT-4 Turbo - 79.5 68.0 45.9 47.4 60.6 26.5 16.1 49.2 GPT-4o - 87.4 74.5 64.8 57.1 65.1 56.7 46.3 64.6 Open-source models. MovieChat [99] 7B 29.5 25.0 24.2 24.7 25.8 28.6 22.8 25.8 LLaMA-VID [65] 7B 50.8 34.5 30.1 32.7 32.5 23.9 27.8 33.2 LLaVA-1.6 [71] 7B 60.6 41.0 43.1 38.4 41.0 25.5 25.7 39.3 ShareGPT4Video [15] 7B 75.8 51.5 47.6 43.2 48.4 34.0 23.3 46.4 VideoLlaMA2 [23] 7B 74.6 64.5 49.9 43.8 45.1 34.0 27.4 48.5 LongVA [149] 7B 83.3 58.5 69.3 50.0 67.2 38.6 27.2 56.3 IXC2.5 [148] 7B - - - - - - - 58.8 InternVL2 [22] 8B - - - - - - - 64.0 LLaVA-OneVision [57] 7B - - - - - - - 64.7 Video-XL [97] 7B - - - - - - - 64.9 IXC2.5-OL 7B 84.1 68.5 76.6 60.8 75.1 57.1 41.3 66.2 ğŸ”¼ í‘œ 4ëŠ” MLVU ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. IXC2.5-OL ëª¨ë¸ì€ 70ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ëª¨ë¸ ì¤‘ì—ì„œ ìµœê³  ì„±ëŠ¥(SOTA)ì„ ë‹¬ì„±í–ˆìœ¼ë©°, ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ê³¼ ìƒìš© APIë¥¼ ëª¨ë‘ ëŠ¥ê°€í•˜ëŠ” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ì´í•´ ì‘ì—…(ì£¼ì œ ì¶”ë¡ , ì´ìƒ ê°ì§€, ì§ˆë¬¸ ì‘ë‹µ ë“±)ì— ëŒ€í•œ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•˜ì—¬ IXC2.5-OLì˜ ìš°ìˆ˜ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤. í‘œì—ëŠ” ê° ì‘ì—…ì— ëŒ€í•œ ì •í™•ë„ì™€ ì „ì²´ í‰ê·  ì„±ëŠ¥ì´ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.\nread the caption Table 4: Evaluation results on MLVU benchmark. IXC2.5-OL has demonstrated excellent performance, surpassing both open-source models and closed-source APIs, achieving SOTA at the 7B model scale. Method Params Short Medium Long Overall Closed-source APIs. GPT-4V - 70.5 55.8 53.5 59.9 Claude 3.5 Sonnet - 71.0 57.4 51.2 60.0 GPT-4o mini - 72.5 63.1 58.6 64.8 GPT-4o - 80.0 70.3 65.3 71.9 Gemini 1.5 Pro - 81.7 74.3 67.4 75.0 Open-source models. ShareGPT4Video [15] 7B 48.3 36.3 35.0 39.9 VideoLlaMA2 [23] 7B - - - 47.9 LongVA [149] 7B 61.1 50.4 46.2 52.6 Video-XL [97] 7B 64.0 53.2 49.2 55.5 VITA [38] 8x7B 65.9 52.9 48.6 55.8 IXC2.5 [148] 7B - - - 55.8 InternVL2 [22] 8B - - - 56.3 LLaVA-OneVision [57] 7B - - - 58.2 mPLUG-Owl3 [131] 7B 70.0 57.7 50.1 59.3 MiniCPM-V 2.6 [130] 8B - - - 60.9 IXC2.5-OL 7B 72.7 58.2 50.8 60.6 ğŸ”¼ í‘œ 5ëŠ” Video-MME ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. Video-MMEëŠ” ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ì´í•´ ì‘ì—…ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ ì¢…í•©ì ì¸ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤. ì´ í‘œëŠ” IXC2.5-OL ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì˜¤í”ˆì†ŒìŠ¤ ìµœì²¨ë‹¨(SOTA) ëª¨ë¸ë“¤ê³¼ ë¹„êµí•˜ì—¬ ë³´ì—¬ì£¼ë©°, IXC2.5-OLì´ ì˜¤í”ˆì†ŒìŠ¤ SOTA ëª¨ë¸ë“¤ê³¼ ê±°ì˜ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” ì—¬ëŸ¬ ë¹„ë””ì˜¤ ì´í•´ ê³¼ì œì— ëŒ€í•œ ì •í™•ë„(ì˜ˆ: ì£¼ì œ ì¶”ë¡ , ì´ìƒ ê°ì§€, ì§ˆë¬¸ ì‘ë‹µ ë“±)ë¥¼ ìˆ˜ì¹˜ë¡œ ì œì‹œí•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìì„¸íˆ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.\nread the caption Table 5: Evaluation results on Video-MME benchmark. IXC2.5-OL demonstrates performance close to that of the open-source SOTA. Method Params OP CR CS ATP EU TR PR SU ACP CT Overall Human - 89.47 92.00 93.60 91.47 95.65 92.52 88.00 88.75 89.74 91.30 91.46 Closed-source APIs. Claude 3.5 Sonnet - 80.49 77.34 82.02 81.73 72.33 75.39 61.11 61.79 69.32 43.09 72.44 GPT-4o - 77.11 80.47 83.91 76.47 70.19 83.80 66.67 62.19 69.12 49.22 73.28 Gemini 1.5 Pro - 79.02 80.47 83.54 79.67 80.00 84.74 77.78 64.23 71.95 48.70 75.69 Open-source models. VideoLLM-online [12] 8B 39.07 40.06 34.49 31.05 45.96 32.40 31.48 34.16 42.49 27.89 35.99 VideoLLaMA2 [23] 7B 55.86 55.47 57.41 58.17 52.80 43.61 39.21 42.68 45.61 35.23 49.52 VILA-1.5 [68] 8B 53.68 49.22 70.98 56.86 53.42 53.89 54.63 48.78 50.14 17.62 52.32 LongVA [149] 7B 70.03 63.28 61.20 70.92 62.73 59.50 61.11 53.66 54.67 34.72 59.96 InternVL2 [22] 8B 68.12 60.94 69.40 77.12 67.70 62.93 59.26 53.25 54.96 56.48 63.72 Kangaroo [72] 7B 71.12 84.38 70.66 73.20 67.08 61.68 56.48 55.69 62.04 38.86 64.60 MiniCPM-V 2.6 [130] 8B 71.93 71.09 77.92 75.82 64.60 65.73 70.37 56.10 62.32 53.37 67.44 Qwen2-VL [113] 7B 75.20 82.81 73.19 77.45 68.32 71.03 72.22 61.19 69.04 46.11 69.04 LLaVA-OneVision [57] 7B 80.38 74.22 76.03 80.72 72.67 71.65 67.59 65.45 65.72 45.08 71.12 IXC2.5-OL 7B 82.83 73.77 78.66 82.95 72.50 76.01 61.11 60.67 71.59 58.85 73.79 ğŸ”¼ í‘œ 6ì€ ì‹¤ì‹œê°„ ì‹œê°ì  ì´í•´ë¥¼ ìœ„í•œ StreamingBenchì— ëŒ€í•œ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¸¡ì • ì§€í‘œëŠ” ê°ì²´ ì¸ì‹(OP), ì¸ê³¼ ì¶”ë¡ (CR), í´ë¦½ ìš”ì•½(CS), ì†ì„± ì¸ì‹(ATP), ì‚¬ê±´ ì´í•´(EU), í’ë¶€í•œ í…ìŠ¤íŠ¸ ì´í•´(TR), ì „ë§ì  ì¶”ë¡ (PR), ê³µê°„ì  ì´í•´(SU), ë™ì‘ ì¸ì‹(ACP), ê³„ì‚°(CT)ì„ í¬í•¨í•©ë‹ˆë‹¤. IXC2.5-OLì€ ëª¨ë“  ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì¤‘ì—ì„œ ê°€ì¥ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, í´ë¡œì¦ˆë“œ ì†ŒìŠ¤ APIì¸ Gemini 1.5 Proì— ê·¼ì ‘í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. í‘œëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì˜ ì‹¤ì‹œê°„ ì‹œê°ì  ì´í•´ ëŠ¥ë ¥ì„ ë¹„êµ ë¶„ì„í•˜ì—¬ ê° ëª¨ë¸ì˜ ê°•ì ê³¼ ì•½ì ì„ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. íŠ¹íˆ, ê°ì²´ ì¸ì‹, ì‚¬ê±´ ì´í•´, ë™ì‘ ì¸ì‹ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì‹œê°ì  ì´í•´ ê³¼ì œì—ì„œ ê° ëª¨ë¸ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ëª…í™•í•˜ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.\nread the caption Table 6: Evaluation results on StreamingBench for Real-Time Visual Understanding. Metrics include Object Perception (OP), Causal Reasoning (CR), Clips Summarization (CS), Attribute Perception (ATP), Event Understanding (EU), Text-Rich Understanding (TR), Prospective Reasoning (PR), Spatial Understanding (SU), Action Perception (ACP), and Counting (CT). IXC2.5-OL excels among all open-source models, and falling just short of the closed-source API, Gemini 1.5 Pro. Method Params CP FP-S FP-C HL Mean LR AR RR CSR TP Mean Overall Closed-source APIs. Claude 3.5 Sonnet - 1.57 1.39 1.07 1.40 1.38 1.13 1.70 1.48 1.54 1.04 1.35 1.38 Gemini 1.0 Pro - 1.61 1.56 1.30 0.65 1.50 1.15 1.57 1.55 1.36 1.33 1.39 1.48 Gemini 1.5 Pro - 1.99 2.04 1.70 1.90 1.98 1.98 2.02 1.92 1.78 1.63 1.86 1.94 GPT-4V - 1.83 1.65 1.40 1.76 1.66 1.45 1.91 1.86 1.83 1.53 1.69 1.68 GPT-4o - 2.23 2.24 2.01 1.90 2.19 2.11 2.12 2.17 1.94 1.97 2.08 2.15 Open-source models. MovieLLM [101] 7B 0.95 0.82 0.70 0.15 0.81 0.52 1.12 1.22 0.54 1.05 0.97 0.87 LLaVA-OneVision [57] 72B 1.22 1.07 0.90 0.21 1.03 0.76 0.96 0.55 0.81 0.48 0.70 0.94 PLLaVA [126] 7B 1.08 1.06 0.86 0.52 1.02 0.64 1.25 1.17 0.98 1.01 1.03 1.03 ShareGPT4Video [15] 7B 1.20 1.05 1.00 0.32 1.04 0.89 1.06 1.19 1.01 0.99 1.03 1.05 VideoStreaming [89] 7B 1.38 1.13 0.8 0.32 1.13 0.77 1.27 1.11 1.01 1.10 1.09 1.12 LLaVA-NeXT-Video [151] 7B 1.35 1.15 0.97 0.58 1.14 0.64 1.38 1.30 1.27 1.03 1.13 1.14 VILA1.5 [68] 13B 1.51 1.45 1.26 0.24 1.39 0.80 1.52 1.30 1.40 1.28 1.28 1.36 InternVL2 [22] 8B 1.41 1.37 1.15 0.19 1.30 0.90 1.34 1.38 1.14 1.00 1.16 1.26 Qwen2-VL [113] 7B 1.63 1.51 1.19 0.55 1.46 1.16 1.56 1.49 1.37 1.21 1.35 1.44 IXC2.5-OL 7B 1.53 1.61 1.20 0.15 1.49 0.93 1.44 1.57 1.30 1.08 1.25 1.42 ğŸ”¼ í‘œ 7ì€ MMBench-Video ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. MMBench-VideoëŠ” ë¹„ë””ì˜¤ ì´í•´ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ê³¼ì œë¥¼ í¬í•¨í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤. ì´ í‘œì—ëŠ” ì´ 9ê°€ì§€ ê³¼ì œì— ëŒ€í•œ ì„±ëŠ¥ì´ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê° ê³¼ì œëŠ” ë¹„ë””ì˜¤ ì´í•´ì˜ íŠ¹ì • ì¸¡ë©´ì„ í‰ê°€í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Coarse Perception(CP)ì€ ë¹„ë””ì˜¤ì˜ ì „ë°˜ì ì¸ ë‚´ìš© ì´í•´ë¥¼ ì¸¡ì •í•˜ê³ , Single-Instance Finegrained Perception(FP-S)ê³¼ Cross-Instance Finegrained Perception(FP-C)ëŠ” ì„¸ë¶€ì ì¸ ê°ì²´ ì¸ì‹ ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. Hallucination(HL)ì€ ì˜ëª»ëœ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” ê²½í–¥ì„ í‰ê°€í•˜ê³ , Logic Reasoning(LR), Attribute Reasoning(AR), Relation Reasoning(RR), Commonsense Reasoning(CSR), Temporal Reasoning(TP)ì€ ê°ê° ë…¼ë¦¬ì  ì¶”ë¡ , ì†ì„± ì¶”ë¡ , ê´€ê³„ ì¶”ë¡ , ìƒì‹ì  ì¶”ë¡ , ì‹œê°„ì  ì¶”ë¡  ëŠ¥ë ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤. í‘œì—ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì œì‹œë˜ì–´ ìˆìœ¼ë©°, ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ì™€ ê° ê³¼ì œì— ëŒ€í•œ ì„±ëŠ¥ ì ìˆ˜ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì—¬ëŸ¬ ëª¨ë¸ì˜ ë¹„ë””ì˜¤ ì´í•´ ëŠ¥ë ¥ì„ ë¹„êµ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nread the caption Table 7: Evaluation results on MMBench-Video. Tasks include Coarse Perception (CP), Single-Instance Finegrained Perception (FP-S), Cross-Instance Finegrained Perception (FP-C), Hallucination (HL), Logic Reasoning (LR), Attribute Reasoning (AR), Relation Reasoning (RR), Commonsense Reasoning (CSR), and Temporal Reasoning (TP). Method Params AS AP AA FA UA OE OI OS MD AL ST AC MC MA SC FP CO EN ER CI Avg GPT-4V - 55.5 63.5 72.0 46.5 73.5 18.5 59.0 29.5 12.0 40.5 83.5 39.0 12.0 22.5 45.0 47.5 52.0 31.0 59.0 11.0 43.5 GPT-4o - 61.5 56.5 72.0 54.0 82.0 62.5 66.5 44.0 36.5 33.5 93.0 54.5 33.5 54.5 53.5 74.5 71.5 32.5 71.0 42.5 57.5 Closed-source APIs. VideoLLaMA [144] 7B 27.5 25.5 51.0 29.0 39.0 48.0 40.5 38.0 22.5 22.5 43.0 34.0 22.5 32.5 45.5 32.5 40.0 30.0 21.0 37.0 34.1 VideoChat [60] 7B 33.5 26.5 56.0 33.5 40.5 53.0 40.5 30.0 25.5 27.0 48.5 35.0 20.5 42.5 46.0 26.5 41.0 23.5 23.5 36.0 35.5 MiniCPM-V 2.6 [130] 7B 38.0 43.0 63.0 35.5 67.5 55.5 46.0 35.5 25.5 33.0 77.5 48.0 37.0 54.0 42.5 40.0 31.0 38.0 43.0 40.5 44.7 VideoChat2 [62] 7B 66.0 47.5 83.5 49.5 60.0 58.0 71.5 42.5 23.0 23.0 88.5 39.0 42.0 58.5 44.0 49.0 36.5 35.0 40.5 65.5 51.1 Qwen2-VL [113] 7B 51.0 58.0 77.5 47.0 64.0 63.0 65.5 40.0 25.5 35.5 77.0 43.5 47.0 62.0 42.0 61.5 49.5 41.5 47.5 41.5 52.0 PLLaVA [126] 34B 65.0 53.0 83.5 45.0 77.5 70.0 64.5 38.5 37.5 49.0 89.5 41.5 43.5 70.0 53.0 52.5 65.0 39.5 60.5 58.0 57.8 LLaVA-OneVision [57] 72B 63.0 58.0 84.5 46.5 85.5 64.0 73.5 41.5 37.0 69.0 95.0 47.5 47.5 75.5 53.5 52.0 70.5 34.0 64.0 54.5 60.8 InternVL2 [22] 8B 75.0 62.0 83.5 40.5 69.5 96.0 72.0 29.5 58.0 53.0 88.5 39.5 83.0 97.0 51.0 78.5 65.0 33.0 48.0 67.0 64.5 Open-source models. IXC2.5-OL 7B 84.5 81.0 75.0 46.0 81.0 92.0 79.5 36.5 83.0 47.0 90.0 60.5 75.0 93.0 58.0 60.5 74.0 42.0 53.0 62.0 68.7 ğŸ”¼ í‘œ 8ì€ MVBenchë¼ëŠ” ë¹„ë””ì˜¤ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. MVBenchëŠ” ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ì´í•´ ì‘ì—…ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ì•¡ì…˜ ìˆœì„œ(AS), ì•¡ì…˜ ì˜ˆì¸¡(AP), ì•¡ì…˜ ë°˜ì˜ì–´(AA), ì„¸ë¶„í™”ëœ ì•¡ì…˜(FA), ì˜ˆìƒì¹˜ ëª»í•œ ì•¡ì…˜(UA), ê°ì²´ ì¡´ì¬(OE), ê°ì²´ ìƒí˜¸ì‘ìš©(OI), ê°ì²´ ì„ê¸°(OS), ì´ë™ ë°©í–¥(MD), ì•¡ì…˜ ì§€ì—­í™”(AL), ì¥ë©´ ì „í™˜(ST), ì•¡ì…˜ ê°œìˆ˜(AC), ì´ë™ ê°œìˆ˜(MC), ì´ë™ ì†ì„±(MA), ìƒíƒœ ë³€í™”(SC), ì„¸ë¶„í™”ëœ í¬ì¦ˆ(FP), ìºë¦­í„° ìˆœì„œ(CO), ì‹œì  íƒìƒ‰(EN), ì—í”¼ì†Œë“œ ì¶”ë¡ (ER), ë°˜ì‚¬ì‹¤ì  ì¶”ë¡ (CI) ë“± 20ê°€ì§€ ì‘ì—…ì— ëŒ€í•œ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\nread the caption Table 8: Evaluatation results on MVBench. Tasks include Action Sequence (AS), Action Prediction (AP), Action Antonym (AA), Fine-grained Action (FA), Unexpected Action (UA), Object Existence (OE), Object Interaction (OI), Object Shuffle (OS), Moving Direction (MD), Action Localization (AL), Scene Transition (ST), Action Count (AC), Moving Count (MC), Moving Attribute (MA), State Change (SC), Fine-grained Pose (FP), Character Order (CO), Egocentric Navigation (EN), Episodic Reasoning (ER), and Counterfactual Inference (CI). Full paper # ","date":"12 December 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2412.09596/","section":"Paper Reviews by AI","summary":"InternLM-XComposer2.5-OmniLive: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ë° ì˜¤ë””ì˜¤ ìƒí˜¸ì‘ìš©ì„ ìœ„í•œ ì¸ê°„ì˜ ì¸ì§€ëŠ¥ë ¥ì„ ëª¨ë°©í•œ í˜ì‹ ì  ë‹¤ì¤‘ ëª¨ë“œ AI ì‹œìŠ¤í…œ","title":"InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions","type":"paper-reviews"},{"content":"","date":"12 December 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/multimodal-learning/","section":"Tags","summary":"","title":"Multimodal Learning","type":"tags"},{"content":"","date":"12 December 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"12 December 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/categories/-daily-papers/","section":"Categories","summary":"","title":"ğŸ¤— Daily Papers","type":"categories"},{"content":" Welcome to AI Paper Reviewer! AI Paper Reviewer is a unique platform dedicated to providing insightful reviews and summaries of artificial intelligence research papers. The content is entirely generated by advanced AI systems, offering a novel approach to understanding and disseminating complex scientific literature.\nMission The mission is to make cutting-edge AI research more accessible to a wider audience. By leveraging the power of AI, we aim to:\nSummarize complex research papers in clear, concise language Highlight key findings and their potential implications Provide context and connections to related work in the field Foster a deeper understanding of AI advancements among researchers, students, and enthusiasts How It Works All the pipeline is implemented in this repo, but briefly:\nScanning the latest AI research papers collected from Hugging Face Daily Papers. Extracting visual information (figures, charts, tables) from the papers. Generating descriptive text for the visual information. Generating summaries and reviews of the papers. This project leverages the following tech stack:\nUpstage\u0026rsquo;s Document Parse: Extracting visual information from the papers. Google\u0026rsquo;s Gemini 1.5 Pro: Extracting visual information from the papers if Document Parse is not available. Google\u0026rsquo;s Gemini 1.5 Flash: Generating summaries and reviews of the papers. Google\u0026rsquo;s Gemini 1.5 Flash 8B: Double checking if visual information is correctly extracted. Hugo: Static site generator. Blowfish: Theme for Hugo. Disclaimer While we strive for accuracy and clarity, please note that all content on this site is AI-generated. We encourage readers to refer to the original papers for the most authoritative information.\nWe hope you find AI Paper Reviewer a valuable resource in your AI learning journey!\n","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/about/","section":"AI Paper Reviews by AI","summary":"\u003ch1 class=\"relative group\"\u003eWelcome to AI Paper Reviewer! \n    \u003cdiv id=\"welcome-to-ai-paper-reviewer\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n\u003c/h1\u003e\n\u003cp\u003eAI Paper Reviewer is a unique platform dedicated to providing insightful reviews and summaries of artificial intelligence research papers. The content is entirely generated by advanced AI systems, offering a novel approach to understanding and disseminating complex scientific literature.\u003c/p\u003e","title":"About This Project","type":"page"},{"content":"","date":"29 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/","section":"Paper Reviews by AI","summary":"","title":"Paper Reviews by AI","type":"paper-reviews"},{"content":"","date":"28 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-university-of-alberta/","section":"Tags","summary":"","title":"ğŸ¢ University of Alberta","type":"tags"},{"content":"","date":"28 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/large-language-models/","section":"Tags","summary":"","title":"Large Language Models","type":"tags"},{"content":"","date":"28 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/natural-language-processing/","section":"Tags","summary":"","title":"Natural Language Processing","type":"tags"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2410.20650 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rYongchang Hao et el. 2024-11-01 â†— arXiv â†— Hugging Face â†— Papers with Code TL;DR # Training and deploying large neural networks is hampered by limited on-device memory. While techniques like quantization exist, they often compromise model performance. This paper introduces a novel solution to this problem.\nThe proposed method, NeuZip, uses a lossless compression algorithm for training, focusing on the low-entropy nature of the exponent bits in floating-point numbers. For inference, a lossy variant offers further memory reduction by controlling the relative change of each parameter. Experiments on various models showed that NeuZip significantly reduces memory usage (e.g., Llama-3 8B model training memory reduced from 31GB to under 16GB) while maintaining, or even improving, performance, surpassing existing techniques like quantization.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # This paper is important because it presents NeuZip, a novel and effective method for memory-efficient training and inference of large neural networks. This addresses a critical limitation in deep learning, enabling researchers to train and deploy larger, more powerful models with limited resources. The proposed technique offers a significant improvement over existing methods, opening up new avenues for research in memory optimization and large model deployment.\nVisual Insights # ğŸ”¼ Figure 1 presents histograms visualizing the distribution of sign bits, exponent bits, and mantissa bits within the parameters of the LLama-3 8B model. Each histogram shows the frequency of occurrence for each possible binary value (represented on the x-axis), providing insights into the entropy of each component. This analysis is crucial to understanding NeuZip\u0026rsquo;s compression strategy, which focuses on the low-entropy nature of specific bits to achieve memory efficiency.\nread the caption Figure 1: The histograms of different components of the parameters of LLama-3 8B modelÂ (Dubey etÂ al., 2024). The xğ‘¥xitalic_x-axis is all possible binary values and the yğ‘¦yitalic_y-axis represent the frequency of each value. Name GPT-Neo-XL 2.7B Loss GPT-Neo-XL 2.7B Mem GPT-Neo-XL 2.7B Speed Llama-3 8B Loss Llama-3 8B Mem Llama-3 8B Speed LLama-2 13B Loss LLama-2 13B Mem LLama-2 13B Speed Vanilla 8.81 11.22 0.96 8.61 30.97 0.77 - OOM - LOMO 8.81 6.97 0.94 8.61 19.47 0.78 9.10 26.26 0.49 +NeuZip Lossless 8.81 5.54 0.70 8.61 15.25 0.45 9.10 18.58 0.28 ğŸ”¼ This table presents the results of pre-training three different decoder-only language models (GPT-Neo-XL 2.7B, Llama-3 8B, and Llama-2 13B) on a language modeling task. The models were trained using four different methods: a standard training approach (Vanilla), an approach using the Layer-wise Optimization with Memory Optimization (LOMO) technique, LOMO combined with lossless NeuZip compression, and LOMO combined with lossless NeuZip. The table shows for each model and training method the cross-entropy loss calculated on a validation set, the peak memory usage during training (measured in gibibytes, GiB), and the training speed (number of iterations per second). The best performing method for each model is highlighted in bold.\nread the caption Table 1: Pre-training decoder-only models on the language modeling task. The loss numbers are calculated on the validation set with the cross-entropy loss. Memory is reported in GiB (10243superscript102431024^{3}1024 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT B). Speed represents the number of iterations per second. The bold numbers represent the top results. In-depth insights # Low-Entropy Weights # The research paper section on \u0026ldquo;Low-Entropy Nature of Neural Network Parameters\u0026rdquo; posits that neural network weights exhibit low entropy. This is primarily attributed to weight initialization strategies, which often center weights around zero (e.g., Gaussian initialization), and the effects of regularization techniques (e.g., weight decay) that consistently reduce weight magnitudes during training. This central tendency, alongside the implicit regularization effect of stochastic gradient descent, contributes to the low-entropy characteristic. The paper highlights that this property, specifically the low entropy of exponent bits in the floating-point representation of weights, makes these weights highly compressible, offering a pathway for efficient memory management in training and inference without significantly sacrificing model performance. The low entropy is key to the success of NeuZip\u0026rsquo;s compression algorithm, as it forms the fundamental basis for achieving significant memory savings.\nANS Compression # The research paper introduces Asymmetric Numeral Systems (ANS) as a lossless compression algorithm for the exponent bits of floating-point numbers in neural network weights. This is motivated by the observation that the exponent bits exhibit low entropy due to the concentration of weights around zero, a common characteristic resulting from initialization and training dynamics. ANS is chosen due to its high throughput on parallel computing devices like GPUs, essential for efficient training. Lossless compression ensures that no precision is lost during training, maintaining the full capability of the network while simultaneously reducing memory usage. The algorithm efficiently handles the dynamic range of exponents by treating each bit as a base-n number with a variable base determined by symbol frequency. The authors leverage the multi-layer structure of neural networks to compress and decompress on a per-layer basis, minimizing the overall memory footprint, and fully preserving backpropagation capabilities. The choice of ANS allows NeuZip to successfully reduce memory consumption without sacrificing model performance during training.\nLossy Inference # The research paper introduces NeuZip, a memory-efficient technique for neural network training and inference. Its lossy inference component focuses on reducing memory usage during inference by selectively compressing the mantissa bits of floating-point numbers. This is motivated by the observation that inference is less sensitive to precision loss than training. By controlling the relative change in each parameter through controlled rounding and truncation of mantissa bits, NeuZip achieves significant memory reduction. The effectiveness is empirically demonstrated on various models and tasks, showing a favorable trade-off between memory usage and performance. Lossy NeuZip is presented as a practical approach to enable deployment of large models on resource-constrained devices, maintaining high accuracy despite the lossy compression scheme.\nMemory Benchmarks # The provided text does not contain a heading explicitly titled \u0026lsquo;Memory Benchmarks\u0026rsquo;. Therefore, a summary cannot be generated. To create the summary, please provide the relevant text from the PDF\u0026rsquo;s section on memory benchmarks. The summary would then analyze the memory usage results reported for various models and techniques (e.g., vanilla training, LOMO, NeuZip variants, and quantization methods) to determine their memory efficiency. It would likely highlight the significant memory savings achieved by the proposed NeuZip method, especially compared to the baseline and quantization approaches, while maintaining or even improving model performance. The summary might also touch upon the impact of hyperparameter choices such as block size on memory usage and performance trade-offs, focusing on NeuZip\u0026rsquo;s position on the Pareto frontier, which indicates a superior memory-performance balance. In addition, the summary might discuss the memory efficiency achieved during both training and inference phases and emphasize the achievability of training large language models (LLMs) on consumer-grade GPUs due to NeuZip\u0026rsquo;s efficiency.\nFuture Directions # The research paper does not include a section specifically titled \u0026lsquo;Future Directions\u0026rsquo;. Therefore, it is not possible to provide a summary about a heading that does not exist in the provided document. To generate the requested summary, please provide a PDF containing a \u0026lsquo;Future Directions\u0026rsquo; section.\nMore visual insights # More on figures ğŸ”¼ This figure illustrates the reverse-mode automatic differentiation (backpropagation) process for a linear layer in a neural network, comparing different memory-saving techniques. (a) Vanilla shows the standard approach, where both weights and activations/gradients are stored in memory throughout the entire process. This contrasts with methods like (b) activation checkpointing (AC), (c) AC combined with Low-Memory Optimization (LOMO), and (d) NeuZip. These optimized techniques utilize various strategies to reduce memory usage during backpropagation, either by recomputing certain values or leveraging compressed representations, as seen in NeuZip\u0026rsquo;s compressed weight storage.\nread the caption (a) Vanilla ğŸ”¼ This figure shows the memory usage pattern of the activation checkpointing (AC) method for a linear layer in a neural network during reverse-mode automatic differentiation (backpropagation). Blue blocks represent data temporarily loaded into memory for calculations, while red blocks denote data constantly residing in memory. Activation checkpointing saves memory by recomputing activations during backpropagation, but still needs to store weights and other intermediate variables. The image visually compares vanilla, AC, AC with Layer-wise Optimization using Memory Optimization (LOMO), and NeuZip, which is the proposed method in the paper.\nread the caption (b) AC ğŸ”¼ This figure shows the reverse-mode automatic differentiation (backpropagation) process for a linear layer in a neural network using the AC+LOMO memory-saving technique. Blue blocks represent data temporarily loaded into memory during computation for the current layer, while red blocks show data that remains in memory throughout the entire training process. AC+LOMO combines activation checkpointing (AC) and Layer-wise Ordering of Memory Optimization (LOMO) to reduce memory usage. Activation checkpointing recomputes activations instead of storing them, while LOMO optimizes memory usage by efficiently managing memory allocation and deallocation across layers. This visualization contrasts AC+LOMO with other memory-saving approaches, highlighting its efficiency in reducing peak memory usage during training.\nread the caption (c) AC+LOMO ğŸ”¼ This figure shows a diagram illustrating the reverse-mode automatic differentiation (backpropagation) process in a linear layer of a neural network using NeuZip. It compares NeuZip\u0026rsquo;s memory-saving approach with other methods like vanilla, activation checkpointing (AC), and AC combined with LOMO. Blue blocks represent data temporarily loaded into memory, while red blocks represent data persistently stored in memory. NeuZip significantly reduces memory usage by compressing weight matrices and utilizing the multi-layer structure of neural networks to avoid storing large buffers.\nread the caption (d) NeuZip ğŸ”¼ This figure illustrates the memory usage of different training methods for a single linear layer during backpropagation. It compares vanilla training, activation checkpointing (AC), AC with Layer-wise Optimization using Memory Optimization (AC+LOMO), and the proposed NeuZip method. Blue blocks represent data loaded into memory temporarily for a single layer\u0026rsquo;s computation, while red blocks denote data persistently stored throughout training. NeuZip is shown to reduce memory usage by strategically compressing parameters.\nread the caption Figure 2: Reverse-mode automatic differentiation (e.g., back-propagation) with different memory-saving techniques for a linear layer. Blocks colored blue are loaded in memory temporarily for the calculation of this layer, whereas the blocks colored red are always in memory throughout training. ğŸ”¼ This figure illustrates the Pareto frontier for different model compression techniques, showing the trade-off between memory usage and model performance. The x-axis represents memory consumption (in GiB), and the y-axis represents model performance (e.g., perplexity). Three different model sizes (Llama-3 8B, Llama-2 13B, Yi-1.5 34B) are shown, each with results for a vanilla (uncompressed) model, a quantization method, and several NeuZip variants. Points closer to the bottom-left corner indicate better memory efficiency and higher performance. The results demonstrate that NeuZip variants generally lie closer to or on the Pareto frontier compared to quantization methods, indicating a better balance between memory efficiency and performance.\nread the caption Figure 3: The trade-off between memory and performance for different methods. ğŸ”¼ This figure compares the throughput (in GiB/s) of various methods for compressing and decompressing matrices of varying sizes in neural network training. Panel (a) shows the compression throughput of CPU offloading, quantization, lossy NeuZip, and lossless NeuZip. Panel (b) displays the decompression throughput of GPU reloading, de-quantization, lossy NeuZip decompression, and lossless NeuZip decompression. The results illustrate the relative efficiency of each method in terms of data transfer rate and memory usage.\nread the caption Figure 4: The throughput experiment. (a) Comparison of CPU-offloading, quantization, lossy NeuZip compression, and lossless NeuZip compression. (b) Comparison of GPU-reloading, de-quantization, lossy NeuZip decompression, and lossless NeuZip decompression. ğŸ”¼ This figure displays histograms illustrating the distribution of sign bits, exponent bits, and mantissa bits within the floating-point numbers representing the parameters of a randomly initialized Llama-3 8B model. The x-axis of each histogram represents the possible values for each component (bits), while the y-axis represents the frequency of occurrence for each value in the model\u0026rsquo;s parameters. The histograms visually demonstrate the low entropy nature of the exponent bits, a key observation supporting the NeuZip compression method described in the paper.\nread the caption Figure 5: The histograms of different floating-point components of the parameters of a randomly initialized Llama-3 8B model. More on tables Name T5 1B BLEU T5 1B Mem T5 1B Speed T5 3B BLEU T5 3B Mem T5 3B Speed T5 11B BLEU T5 11B Mem T5 11B Speed Vanilla 79.9 3.82 3.69 85.1 11.32 2.43 - OOM - LOMO 79.9 2.75 3.68 85.1 7.07 2.47 82.3 25.95 0.69 + NeuZip Lossless 79.9 2.39 2.02 85.1 5.21 1.33 82.3 20.68 0.46 QLoRA INT8 70.4 5.84 1.11 72.1 11.54 1.12 63.5 33.36 0.37 QLoRA FP4 70.1 3.63 1.70 72.1 7.35 1.74 63.3 22.73 0.58 QLoRA FP42 70.6 3.61 1.63 72.0 7.27 1.61 60.6 22.38 0.57 QLoRA NF4 70.4 3.63 1.83 71.2 7.35 1.65 59.4 22.73 0.57 QLoRA NF42 70.5 3.61 1.64 71.2 7.07 1.57 57.9 22.38 0.57 ğŸ”¼ This table presents the results of fine-tuning various encoder-decoder models on an SQL generation task. It compares different model compression techniques (including the proposed NeuZip method) in terms of their impact on model performance (measured by BLEU score using SacreBLEU), memory usage (reported in GiB), and training speed (iterations per second). The top-performing model for each metric in each model size is highlighted in bold.\nread the caption Table 2: Fine-tuning encoderâ€“decoder models on the SQL generation task. The BLEU scores are calculated with SacreBLEU. Memory is reported in GiB (10243superscript102431024^{3}1024 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT B). Speed represents the number of iterations per second. The bold numbers represent the top results. Name Llama-3 8B PPL Llama-3 8B Mem Llama-3 8B Speed Llama-2 13B PPL Llama-2 13B Mem Llama-2 13B Speed Yi-1.5 34B PPL Yi-1.5 34B Mem Yi-1.5 34B Speed Vanilla 9.89 15.08 5.07 10.87 24.36 3.59 - OOM - Quant INT8 10.07 8.63 3.54 10.97 12.74 2.27 10.87 33.41 1.13 Quant FP4 11.51 5.77 3.45 11.38 7.37 1.87 11.57 19.54 1.75 Quant NF4 10.75 5.77 3.38 11.15 7.37 1.83 11.06 19.54 1.67 Quant FP42 11.50 5.44 3.41 11.38 6.87 1.86 11.57 18.11 1.61 Quant NF42 10.75 5.44 3.34 11.15 6.87 1.81 11.06 18.11 1.54 NeuZip 0-bit 13.64 5.24 3.44 12.46 6.30 1.87 12.06 16.20 0.94 NeuZip 1-bit 10.77 6.05 3.38 11.17 7.77 1.86 11.04 20.14 0.93 NeuZip 3-bit 9.93 7.70 3.38 10.90 10.73 1.84 10.76 27.92 0.93 NeuZip 7-bit (lossless) 9.89 10.95 3.39 10.87 16.66 1.84 10.72 43.40 0.94 ğŸ”¼ Table 3 presents a comprehensive evaluation of the lossy NeuZip compression technique on various neural network models and tasks. It compares the performance (perplexity), memory usage (in GiB), and training speed (iterations per second) of lossy NeuZip against several baseline methods, including standard models and various quantization techniques (INT8, FP4, NF4). The table shows the perplexity scores, memory requirements, and iteration speeds achieved by each method, enabling a detailed comparison of the trade-off between memory efficiency and model accuracy. The bold values indicate the best results for each model and task, while the underlined numbers highlight the second-best performing methods.\nread the caption Table 3: Evaluating lossy NeuZip on different models and tasks. â€˜PPLâ€ represents the perplexity values. Memory is reported in GiB. Speed represents the number of iterations per second. The bold numbers represent the top results, whereas the underlined numbers are the second-best ones. Name T5 1B PPL T5 1B Mem T5 1B Speed T5 3B PPL T5 3B Mem T5 3B Speed T5 11B PPL T5 11B Mem T5 11B Speed Vanilla 2.614 1.37 23.73 2.571 5.31 19.86 2.568 21.06 6.20 Quant INT8 2.615 1.28 4.24 2.573 4.94 4.28 2.569 19.59 2.58 Quant NF4 2.632 1.08 11.64 2.588 4.12 11.82 2.579 16.28 4.48 Quant FP4 2.646 1.08 11.92 2.594 4.12 11.99 2.585 16.28 4.59 Quant FP42 2.646 1.05 10.39 2.594 4.03 9.72 2.585 15.93 4.52 Quant NF42 2.632 1.05 10.39 2.587 4.03 9.96 2.579 15.93 4.39 NeuZip 0-bit 2.731 0.40 11.82 2.668 1.41 8.70 2.651 5.35 3.24 NeuZip 1-bit 2.641 0.48 11.68 2.591 1.78 8.61 2.581 6.65 3.21 NeuZip 3-bit 2.614 0.66 11.99 2.574 2.42 8.60 2.569 9.27 3.19 NeuZip 7-bit (lossless) 2.614 0.99 11.55 2.571 3.73 8.77 2.568 14.46 3.23 ğŸ”¼ This table presents the results of evaluating decoder-only language models on a language modeling task. The models are compared across three metrics: perplexity (PPL), memory usage (in GiB), and training speed (iterations per second). Perplexity scores are adjusted to the word level to allow for fair comparison across models with different tokenization schemes. The table includes results for a vanilla (uncompressed) model, several quantization methods (INT8, FP4, NF4, FP42, NF42), and different variations of the NeuZip algorithm (0-bit, 1-bit, 3-bit, and 7-bit (lossless)). This allows for a comprehensive comparison of model performance, efficiency, and memory footprint.\nread the caption (a) Evaluating decoder-only models on the language modeling task. Here, the perplexities are adjusted to word level to compare across different tokenizations. Name Block 32 Block 32 Block 64 Block 64 Block 128 Block 128 Block 256 Block 256 Block 512 Block 512 PPL Mem PPL Mem PPL Mem PPL Mem PPL Mem NeuZip 0-bit 6.341 35.7 6.694 34.6 6.853 34.2 7.639 33.8 7.104 33.5 NeuZip 1-bit - OOM 4.611 42.7 4.662 42.2 4.640 41.8 4.649 41.4 ğŸ”¼ This table presents the results of evaluating encoder-decoder models (T5 models of various sizes) on a language modeling task. Because all models used the same tokenizer, perplexity is reported at the token level for simpler comparison and easier interpretation of the results. The table likely shows metrics like perplexity (PPL), memory usage (Mem), and training speed (Speed) for different model sizes and/or compression techniques. The focus is on comparing the impact of different methods on efficiency and accuracy.\nread the caption (b) Evaluating encoderâ€“decoder models on the language modeling task. Since all models use the same tokenizer, we reported perplexities at the token level for simplicity. Full paper # ","date":"28 October 2024","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2410.20650/","section":"Paper Reviews by AI","summary":"NeuZip dynamically compresses neural network weights, achieving memory-efficient training and inference without performance loss, significantly reducing the memory footprint of large language models.","title":"NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks","type":"paper-reviews"},{"content":"","date":"5 June 2023","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/-seoul-national-university/","section":"Tags","summary":"","title":"ğŸ¢ Seoul National University","type":"tags"},{"content":"\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e 2306.02728 \u003c?xml version=\"1.0\" encoding=\"iso-8859-1\"?\u003e\rMinjoon Jung et el. â†— arXiv â†— Hugging Face â†— Papers with Code TL;DR # ë¹„ë””ì˜¤ ìˆœê°„ ê²€ìƒ‰(VMR)ì€ ìì—°ì–´ ì§ˆì˜ì— ë§ëŠ” ë¹„ë””ì˜¤ ë‚´ íŠ¹ì • ìˆœê°„ì„ ì°¾ëŠ” ê³¼ì œì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ê¸°ì¡´ VMR ëª¨ë¸ë“¤ì€ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ì˜ ê³ ìœ í•œ ëª¨í˜¸ì„±ìœ¼ë¡œ ì¸í•´ ì•½í•œ ì •ë ¬ ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤. ì¦‰, ì§ˆì˜ì–´ê°€ í•´ë‹¹ ìˆœê°„ì˜ ì„¸ë¶€ ì •ë³´ë¥¼ ì™„ì „íˆ í¬ê´„í•˜ì§€ ëª»í•˜ê±°ë‚˜, ìˆœê°„ì— ê´€ë ¨ ì—†ëŠ” í”„ë ˆì„ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ì„±ëŠ¥ í–¥ìƒì— ì œì•½ì´ ë°œìƒí•©ë‹ˆë‹¤.\në³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë°°ê²½ ì •ë³´ë¥¼ í™œìš©í•˜ëŠ” ìƒˆë¡œìš´ ëª¨ë¸ì¸ BM-DETRì„ ì œì•ˆí•©ë‹ˆë‹¤. BM-DETRì€ ëŒ€ì¡° í•™ìŠµ ë°©ì‹ì„ ì±„íƒí•˜ì—¬, ë¹„ë””ì˜¤ ë‚´ ë‹¤ë¥¸ ìˆœê°„ì— ë§¤ì¹­ë˜ëŠ” ë¶€ì •ì ì¸ ì§ˆì˜ì–´ë¥¼ í™œìš©í•©ë‹ˆë‹¤. ëª¨ë¸ì€ ê° í”„ë ˆì„ê³¼ ì–‘ì„± ì§ˆì˜ì–´ ë° ë¶€ì •ì  ì§ˆì˜ì–´ì˜ ë³´ì™„ì ì¸ ê´€ê³„ë¥¼ í•™ìŠµí•˜ì—¬, íƒ€ê²Ÿ ìˆœê°„ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì£¼ë³€ ë°°ê²½ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ìˆœê°„ ê°ì§€ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ë¹„ë””ì˜¤ ë‚´ ì „ë°˜ì ì¸ ì •ë ¬ì„ ê°œì„ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, BM-DETRì€ ê¸°ì¡´ ëª¨ë¸ë“¤ì— ë¹„í•´ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ê³  íš¨ìœ¨ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\nKey Takeaways # \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\r\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\rWhy does it matter? # ë³¸ ë…¼ë¬¸ì€ ì•½í•œ ì •ë ¬ ë¬¸ì œë¡œ ì–´ë ¤ì›€ì„ ê²ªëŠ” ë¹„ë””ì˜¤ ìˆœê°„ ê²€ìƒ‰(VMR) ë¶„ì•¼ì— ì¤‘ìš”í•œ ê¸°ì—¬ë¥¼ í•©ë‹ˆë‹¤. ì œì•ˆëœ BM-DETR ëª¨ë¸ì€ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë©°, íš¨ìœ¨ì„±ì´ ë†’ì•„ ì—°êµ¬ìë“¤ì´ ë¹„ë””ì˜¤ ì´í•´ì™€ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì ‘ê·¼ì„±ì´ ë¶€ì¡±í•œ ìƒí™©ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•˜ì—¬, í–¥í›„ ì—°êµ¬ì˜ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤. ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ì •ë ¬ ë¬¸ì œì— ëŒ€í•œ ìƒˆë¡œìš´ í•´ê²°ì±…ì„ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ì´í•´ ì‘ì—…ì— ì ìš©ë  ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì´ ìˆìŠµë‹ˆë‹¤.\nVisual Insights # ğŸ”¼ ê·¸ë¦¼ 1ì€ ë…¼ë¬¸ì˜ ì•½í•œ ì •ë ¬ ë¬¸ì œì™€ ì œì•ˆëœ ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìƒë‹¨ì€ ì§ˆì˜ì–´ì™€ ë¹„ë””ì˜¤ êµ¬ê°„ì˜ ê²½ê³„ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ì•½í•œ ì •ë ¬ ë¬¸ì œì˜ ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. í•˜ë‹¨ì€ ê¸°ì¡´ ë°©ë²•(ì™¼ìª½)ê³¼ ì œì•ˆëœ ë°©ë²•(ì˜¤ë¥¸ìª½)ì˜ ë¹„êµë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ë°°ê²½ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë”ìš± ì •í™•í•˜ê²Œ ë¹„ë””ì˜¤ êµ¬ê°„ì„ ê²€ì¶œí•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\nread the caption Figure 1: Top: An example of the weak alignment problem. Bottom: Comparison between traditional (left) and proposed (right) methods. Dataset Domain #Videos #Queries Avg (sec) Moment/Video Avg (sec) Query CharadesSTA Activity 6.7K 16.1K 8.1 / 30.6 7.2 Anet-Cap Activity 15K 72K 36.2 / 117.6 14.8 TACoS Cooking 127 18K 5.4 / 287.1 10 QVHighlights Vlog / News 10.2K 10.3K 24.6 / 150 11.3 ğŸ”¼ í‘œ 1ì€ VMR(Video Moment Retrieval) ë°ì´í„°ì…‹ì˜ í†µê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ë°ì´í„°ì…‹(Charades-STA, ActivityNet-Captions, TACOS, QVHighlights)ì— ëŒ€í•´ ë¹„ë””ì˜¤ ìˆ˜, ì§ˆì˜ ìˆ˜, ë¹„ë””ì˜¤ ë‹¹ í‰ê·  ëª¨ë¨¼íŠ¸ ê¸¸ì´(ì´ˆ), ì§ˆì˜ ë‹¹ í‰ê·  ë‹¨ì–´ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. \u0026lsquo;Avg Moment/Video\u0026rsquo;ëŠ” ê° ë¹„ë””ì˜¤ì—ì„œ ëª¨ë¨¼íŠ¸ì˜ í‰ê·  ê¸¸ì´(ì´ˆ)ë¥¼, \u0026lsquo;Avg Query\u0026rsquo;ëŠ” ì§ˆì˜ ë¬¸ì¥ì˜ í‰ê·  ë‹¨ì–´ ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ í‘œëŠ” ë‹¤ì–‘í•œ VMR ë°ì´í„°ì…‹ì˜ í¬ê¸°ì™€ íŠ¹ì§•ì„ ë¹„êµí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ì˜ íŠ¹ì„±ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\nread the caption Table 1: Statistics of VMR datasets. Avg Moment/Video denotes an average length of moment/video in seconds. Avg Query means an average number of words in query sentences. In-depth insights # Weak Alignment Issue # ì—°êµ¬ ë…¼ë¬¸ì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ëŠ” \u0026ldquo;Weak Alignment Issue\u0026quot;ëŠ” ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ì—ì„œ í”íˆ ë°œìƒí•˜ëŠ” ë¬¸ì œì ì„ ì§€ì¹­í•©ë‹ˆë‹¤. ìì—°ì–´ ì§ˆì˜ì–´ì™€ ë¹„ë””ì˜¤ì˜ ì‹œë§¨í‹± ì •ë³´ ê°„ ì •í™•í•œ ë§¤ì¹­ì´ ì–´ë µë‹¤ëŠ” ì ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì§ˆì˜ì–´ê°€ ë¹„ë””ì˜¤ì˜ íŠ¹ì • ìˆœê°„ì„ ì™„ë²½íˆ í¬ê´„í•˜ì§€ ëª»í•˜ê±°ë‚˜, í•´ë‹¹ ìˆœê°„ì— ê´€ë ¨ ì—†ëŠ” í”„ë ˆì„ë“¤ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ì €í•˜ì‹œí‚¤ëŠ” ì£¼ìš” ì›ì¸ì´ ë©ë‹ˆë‹¤. ë°ì´í„°ì…‹ì˜ ì• ë§¤ëª¨í˜¸í•œ ì£¼ì„(annotation) ë˜í•œ ì´ ë¬¸ì œë¥¼ ì‹¬í™”ì‹œí‚¤ëŠ” ìš”ì†Œì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, \u0026ldquo;ì‚¬ëŒì´ ìœ ë¦¬ì— ë¬¼ì„ ë”°ë¥¸ë‹¤\u0026quot;ë¼ëŠ” ì§ˆì˜ì–´ëŠ” \u0026ldquo;ë¬¼ì„ ë§ˆì‹ ë‹¤\u0026quot;ë¼ëŠ” ë™ì‘ì„ í¬í•¨í•˜ëŠ” ì–´ë…¸í…Œì´ì…˜ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ Weak Alignment ë¬¸ì œëŠ” ëª¨ë¸ì´ íŠ¹ì • ë¹„ë””ì˜¤ ìˆœê°„ì„ ì •í™•íˆ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ì–´ë µê²Œ ë§Œë“¤ê³ , ê²°ê³¼ì ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒì— ì œì•½ì„ ì´ˆë˜í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ê¸°ìˆ ë“¤ì„ í†µí•´ Weak Alignment ë¬¸ì œë¥¼ ì™„í™”í•˜ê³  ì„±ëŠ¥ í–¥ìƒì„ ë„ëª¨í•˜ëŠ”ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.\nBM-DETR Model # BM-DETR ëª¨ë¸ì€ ê¸°ì¡´ VMR(Video Moment Retrieval) ëª¨ë¸ì˜ ì•½í•œ ì •ë ¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ë°°ê²½ ì¸ì‹ ê¸°ë°˜ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì…ë‹ˆë‹¤. ê¸ì •ì  ì§ˆì˜ì™€ ë¶€ì •ì  ì§ˆì˜ë¥¼ ëª¨ë‘ í™œìš©í•˜ì—¬ íƒ€ê²Ÿ ëª¨ë©˜íŠ¸ì™€ ê´€ë ¨ ì—†ëŠ” ë°°ê²½ ì •ë³´ê¹Œì§€ ê³ ë ¤í•¨ìœ¼ë¡œì¨ ëª¨ë©˜íŠ¸ ê°ì§€ì˜ ì •í™•ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤. **í”„ë ˆì„-ì§ˆì˜ í™•ë¥  ë§¤ì²˜(PFM)**ë¥¼ í†µí•´ ê° í”„ë ˆì„ê³¼ ì§ˆì˜ ê°„ì˜ ê´€ê³„ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ê³ , ì‹œê°„ì  ì´ë™(temporal shifting) ê¸°ë²•ì„ í†µí•´ ëª¨ë¸ì˜ ì‹œê°„ ë¶ˆë³€ì„±ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ë˜í•œ, **ë¯¸ì„¸í•œ ì˜ë¯¸ì  ì •ë ¬(fine-grained semantic alignment)**ì„ í†µí•´ ì˜ìƒê³¼ ì§ˆì˜ ê°„ì˜ ì˜ë¯¸ì  ì¼ì¹˜ì„±ì„ ê°œì„ í•˜ì—¬ ì „ë°˜ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì–´ëƒˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ VMR ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, íŠ¹íˆ ì•½í•œ ì •ë ¬ ë¬¸ì œê°€ ì‹¬ê°í•œ ë°ì´í„°ì…‹ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ê³„ì‚° íš¨ìœ¨ì„±ë„ ë†’ì•„ ê¸°ì¡´ì˜ contrastive learning ê¸°ë°˜ ë°©ë²•ë“¤ë³´ë‹¤ íš¨ìœ¨ì ì…ë‹ˆë‹¤.\nContrastive Approach # ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” **ëŒ€ì¡°ì  ì ‘ê·¼ë²•(Contrastive Approach)**ì€ ë¹„ë””ì˜¤ ìˆœê°„ ê²€ìƒ‰(VMR) ê³¼ì œì—ì„œì˜ ì•½í•œ ì •ë ¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í•µì‹¬ ì „ëµì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë‹¨ì¼ ì¿¼ë¦¬ ê¸°ë°˜ ë°©ë²•ë¡ ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ì, ê¸ì •ì  ì¿¼ë¦¬ì™€ ë¶€ì •ì  ì¿¼ë¦¬ë¥¼ ë™ì‹œì— í™œìš©, ëª©í‘œ ìˆœê°„ê³¼ ì£¼ë³€ ë°°ê²½ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. ë¶€ì •ì  ì¿¼ë¦¬ì˜ í™œìš©ì€ ëª¨ë¸ì´ ëª©í‘œ ìˆœê°„ì— ëŒ€í•œ ì´í•´ë„ë¥¼ ë†’ì´ê³ , ê´€ë ¨ ì—†ëŠ” í”„ë ˆì„ì„ ê±¸ëŸ¬ë‚´ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¨ìˆœíˆ ê¸ì •ì  ì¿¼ë¦¬ë§Œ ì‚¬ìš©í•˜ëŠ” ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ì •í™•ë„ì™€ ë¯¼ê°ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤. í”„ë ˆì„-ì¿¼ë¦¬ ë§¤ì¹­ í™•ë¥ ì„ ê³„ì‚°í•˜ì—¬ í”„ë ˆì„ ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ì€ ëª¨ë¸ì˜ ìˆœê°„ ê°ì§€ ì„±ëŠ¥ì„ ë†’ì´ê³  ì •í™•í•œ ì •ë ¬ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ê²°ë¡ ì ìœ¼ë¡œ, ì œì•ˆëœ ëŒ€ì¡°ì  ì ‘ê·¼ë²•ì€ VMR ì„±ëŠ¥ í–¥ìƒì— í¬ê²Œ ê¸°ì—¬í•˜ë©°, íŠ¹íˆ ì•½í•œ ì •ë ¬ ë¬¸ì œê°€ ì‹¬ê°í•œ ë°ì´í„°ì…‹ì—ì„œ íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\nAblation Experiments # ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ ë°°ê²½ ì¸ì‹ ëª¨ë©˜íŠ¸ ê²€ì¶œ íŠ¸ëœìŠ¤í¬ë¨¸(BM-DETR)ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ê¸° ìœ„í•´ **ì—ì´ë¸”ë ˆì´ì…˜ ì‹¤í—˜(Ablation Experiments)**ì´ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ê° êµ¬ì„± ìš”ì†Œ ë° ì†ì‹¤ í•¨ìˆ˜ì˜ ì¤‘ìš”ì„±ì„ ê·œëª…í•˜ê³ , ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•˜ëŠ” ìš”ì¸ì„ ë¶„ì„í•˜ê¸° ìœ„í•œ í•„ìˆ˜ì ì¸ ê³¼ì •ì…ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ë°°ê²½ ì •ë³´ í™œìš©, ì •êµí•œ ì˜ë¯¸ì  ì •ë ¬, í•™ìŠµ ê°€ëŠ¥í•œ êµ¬ê°„, ê·¸ë¦¬ê³  ì‹œê°„ì  ì´ë™ ê¸°ë²• ë“±ì˜ ì˜í–¥ì„ ê°œë³„ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬ BM-DETRì˜ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶„ì„ì„ í†µí•´ ê° ëª¨ë“ˆì˜ ê¸°ì—¬ë„ë¥¼ ëª…í™•í•˜ê²Œ íŒŒì•…í•˜ê³ , ëª¨ë¸ ì„¤ê³„ì˜ í•©ë¦¬ì„±ê³¼ íš¨ìœ¨ì„±ì„ ê²€ì¦í•˜ëŠ”ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ë˜í•œ, ë‹¤ì–‘í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ ì¡°í•©ì„ ì‹¤í—˜í•˜ì—¬ ìµœì ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ì¡°í•©ì„ ì°¾ê³ , ê° ì†ì‹¤ í•¨ìˆ˜ì˜ ì¤‘ìš”ì„±ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ëª¨ë“  êµ¬ì„± ìš”ì†Œì™€ ì†ì‹¤ í•¨ìˆ˜ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ìƒë‹¹í•œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìœ¼ë©°, ì´ë¥¼ í†µí•´ ì œì‹œëœ BM-DETRì˜ íš¨ê³¼ì ì¸ ì„¤ê³„ë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.\nOOD Robustness # ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ BM-DETR ëª¨ë¸ì˜ OOD(Out-of-Distribution) ê°•ê±´ì„±ì€ ê¸°ì¡´ VMR(Video Moment Retrieval) ëª¨ë¸ë“¤ì´ í›ˆë ¨ ë°ì´í„°ì…‹ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ëŠ” ê²½í–¥ì„ ê·¹ë³µí•˜ê³ ì í•˜ëŠ” ì‹œë„ì—ì„œ ë¹„ë¡¯ë©ë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ë“¤ì€ í›ˆë ¨ ë°ì´í„°ì˜ í†µê³„ì  íŠ¹ì„±ì— ì§€ë‚˜ì¹˜ê²Œ ìµœì í™”ë˜ì–´, í›ˆë ¨ ë°ì´í„°ì™€ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§„ ë°ì´í„°(OOD ë°ì´í„°)ì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ì €í•˜ë˜ëŠ” í˜„ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. BM-DETRì€ ë°°ê²½ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ íŠ¹ì • ëª¨ë©˜íŠ¸ì— ëŒ€í•œ ì´í•´ë„ë¥¼ ë†’ì´ê³ , ë‹¤ì–‘í•œ ì¿¼ë¦¬ì™€ì˜ ìƒê´€ê´€ê³„ë¥¼ í•™ìŠµí•¨ìœ¼ë¡œì¨ OOD ë°ì´í„°ì— ëŒ€í•œ ê°•ê±´ì„±ì„ í™•ë³´í•˜ê³ ì í•©ë‹ˆë‹¤. íŠ¹íˆ, Charades-CD ë°ì´í„°ì…‹ì˜ test-ood splitì„ ì‚¬ìš©í•œ ì‹¤í—˜ ê²°ê³¼ëŠ” BM-DETRì´ ê¸°ì¡´ ëª¨ë¸ë“¤ë³´ë‹¤ OOD ë°ì´í„°ì— ëŒ€í•´ í›¨ì”¬ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” BM-DETRì˜ ëª¨ë¸ ì„¤ê³„ê°€ ë°ì´í„°ì…‹ì˜ í¸í–¥ì„±ì— ëœ ì˜ì¡´ì ì´ë©°, ë³´ë‹¤ ì¼ë°˜í™”ëœ íŠ¹ì§•ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, BM-DETRì˜ OOD ê°•ê±´ì„±ì€ ë‹¨ìˆœíˆ ì„±ëŠ¥ í–¥ìƒì„ ë„˜ì–´, ì‹¤ì œ í™˜ê²½ì—ì„œì˜ VMR ëª¨ë¸ì˜ ì‹ ë¢°ì„±ê³¼ ì•ˆì •ì„±ì„ ë†’ì´ëŠ” ë° í¬ê²Œ ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ë°ì´í„°ì…‹ ìì²´ì˜ í’ˆì§ˆ ê°œì„  ì—†ì´ ëª¨ë¸ì˜ ê°•ê±´ì„±ë§Œìœ¼ë¡œ ì™„ë²½í•œ í•´ê²°ì±…ì„ ì œì‹œí•˜ê¸°ëŠ” ì–´ë µë‹¤ëŠ” ì ì„ ìœ ë…í•´ì•¼ í•©ë‹ˆë‹¤.\nMore visual insights # More on tables Method Video Feat Charades-STA (IoU=0.5) Charades-STA (IoU=0.7) 2D-TAN [56] C3D 39.70 27.10 DRN [51] C3D 45.40 26.40 VSLNet [54] C3D 47.31 30.19 CBLN [24] C3D 47.94 28.22 IVG-DCL [30] C3D 50.24 32.88 MomentDiff [22] C3D 53.79 30.18 BM-DETR (ours) C3D 54.42 33.84 2D-TAN [56] VGG 41.34 23.91 DRN [51] VGG 42.90 23.68 CBLN [24] VGG 43.67 24.44 FVMR [11] VGG 42.36 24.14 SSCS [7] VGG 43.15 25.54 MMN [44] VGG 47.31 27.28 QD-DETR [28] VGG 52.77 31.13 G2L [21] VGG 47.91 28.42 MomentDiff [22] VGG 51.94 28.25 BM-DETR (ours) VGG 54.22 35.54 MDETR [19] SF+C 53.63 31.37 QD-DETR [28] SF+C 57.31 32.55 UniVTG [23] SF+C 58.01 35.65 MomentDiff [22] SF+C 55.57 32.42 BM-DETR (ours) SF+C 59.48 38.33 ğŸ”¼ í‘œ 2ëŠ” Charades-STA ë°ì´í„°ì…‹ì— ëŒ€í•œ ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ìˆœê°„ ê²€ìƒ‰(VMR) ë°©ë²•ë“¤ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ë°©ë²•ì€ ë¹„ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œì— ì‚¬ìš©ëœ ë°©ë²• (Video Feat) ê³¼ í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œì— ì‚¬ìš©ëœ ë°©ë²• (Text Feat) ì— ë”°ë¼ ì„±ëŠ¥ì´ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. í‘œì—ëŠ” IoU(Intersection over Union) ê°’ì´ 0.5 ë° 0.7ì¼ ë•Œì˜ R@1 (Top-1 ì •í™•ë„) ë° í‰ê·  í‰ê·  ì •ë°€ë„(mAP)ê°€ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê° ëª¨ë¸ì˜ ìˆœê°„ ê²€ì¶œ ì •í™•ë„ë¥¼ ë¹„êµ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nread the caption Table 2: Performance results on Charades-STA. Method Text Feat ActivityNet-Captions (Video Feat: C3D) TACoS (Video Feat: C3D) IoU=0.5 IoU=0.7 IoU=0.3 IoU=0.5 2D-TAN [56] Glove 44.51 26.54 37.29 25.32 VSLNet [54] Glove 43.22 26.16 29.61 24.27 DRN [51] Glove 45.45 24.39 - 23.17 CBLN [24] Glove 48.12 27.60 38.98 27.65 DeNet [59] Glove 43.79 - - - IVG-DCL [30] Glove 43.84 27.10 38.84 29.07 SSCS [7] Glove 46.67 27.56 41.33 29.56 GTR [2] Glove 50.57 29.11 40.39 30.22 BM-DETR (ours) Glove 49.62 30.61 49.87 33.67 MMN [44] DistilBERT 48.59 29.26 39.24 26.17 G2L [21] BERT 51.68 33.35 42.74 30.95 BM-DETR (ours) BERT 49.98 30.88 50.46 35.87 ğŸ”¼ í‘œ 3ì€ ActivityNet-Captionsì™€ TACoS ë‘ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ì— ëŒ€í•œ ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ IoU(Intersection over Union) ì„ê³„ê°’ 0.3ê³¼ 0.5ë¥¼ ì‚¬ìš©í•˜ì—¬ R@1 (top-1 ì •í™•ë„)ê³¼ IoU ì„ê³„ê°’ 0.5ì™€ 0.75ë¥¼ ì‚¬ìš©í•œ mAP (í‰ê·  ì •ë°€ë„)ë¥¼ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ íŠ¹ì§•(C3D, Glove)ê³¼ í…ìŠ¤íŠ¸ íŠ¹ì§•(Glove, BERT, DistilBERT) ì¡°í•©ì— ë”°ë¥¸ BM-DETRì„ í¬í•¨í•œ ì—¬ëŸ¬ ìµœì²¨ë‹¨ VMR(Video Moment Retrieval) ë°©ë²•ì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•˜ì—¬ ì œì•ˆëœ BM-DETR ëª¨ë¸ì˜ ìš°ìˆ˜ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\nread the caption Table 3: Performance results on ActivityNet-Captions and TACoS. Method Text Feat IoU=0.5 IoU=0.7 mAP@0.5 mAP@0.75 Avg. MCN [1] CLIP 11.41 2.72 24.94 8.22 10.67 CAL [8] CLIP 25.49 11.54 23.40 7.65 9.89 XML [20] CLIP 41.83 30.35 44.63 31.73 32.14 XML+ [19] CLIP 46.69 33.46 47.89 34.67 34.90 MDETR [19] CLIP 52.89 33.02 54.82 29.40 30.73 UMT [26] CLIP 56.23 41.18 53.83 37.01 36.12 QD-DETR [28] CLIP 62.40 44.98 62.52 39.88 39.86 UniVTG [23] CLIP 58.86 40.86 57.60 35.59 35.47 MomentDiff [22] CLIP 57.42 39.66 54.02 35.73 35.95 BM-DETR (ours) CLIP 60.12 43.05 63.08 40.18 40.08 ğŸ”¼ í‘œ 4ëŠ” QVHighlights ë°ì´í„°ì…‹ì— ëŒ€í•œ ì œì•ˆëœ BM-DETR ëª¨ë¸ì˜ ì„±ëŠ¥ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œ(IoU=0.5, IoU=0.7, mAP@0.5, mAP@0.75, í‰ê·  mAP)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ì˜ ìµœì²¨ë‹¨ VMR ë°©ë²•ë“¤ê³¼ ë¹„êµ ë¶„ì„í•˜ì—¬ BM-DETRì˜ ìš°ìˆ˜ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë¹„ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ ë°©ë²•(SF+C) ë° í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ ë°©ë²•(CLIP)ë„ í•¨ê»˜ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.\nread the caption Table 4: Performance results on QVHighlights. Method Text Feat Charades-CD (Video Feat: I3D) Charades-CD (Video Feat: I3D) IoU=0.5 IoU=0.7 2D-TAN [56] Glove 35.88 13.91 LG [29] Glove 42.90 19.29 DRN [51] Glove 31.11 15.17 VSLNet [54] Glove 34.10 17.87 DCM [47] Glove 45.47 22.70 Shuffling [13] Glove 46.67 27.08 BM-DETR (ours) Glove 53.37 30.12 ğŸ”¼ í‘œ 5ëŠ” Charades-CD ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. Charades-CDëŠ” ê¸°ì¡´ Charades-STA ë°ì´í„°ì…‹ê³¼ ë‹¬ë¦¬, í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì‹œê°„ì  ë¶„í¬ê°€ ë‹¤ë¥´ê²Œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ í‘œëŠ” ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ê³¼ ì‹œê°„ì  í¸í–¥ì— ëŒ€í•œ ê°•ê±´ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤. í‘œì—ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì˜ IoU(Intersection over Union) 0.5ì™€ 0.7 ê¸°ì¤€ì˜ R@1 ë° í‰ê·  mAP(Mean Average Precision) ì„±ëŠ¥ì´ ì œì‹œë˜ì–´ ìˆìœ¼ë©°, BM-DETR ëª¨ë¸ì˜ ìš°ìˆ˜ì„±ì„ ë³´ì—¬ì£¼ëŠ” ê²°ê³¼ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\nread the caption Table 5: Performance results on Charades-CD. Method Charades-STA GT â†‘ Charades-STA Non-GT â†“ Charades-STA â–³ â†‘ TACoS GT â†‘ TACoS Non-GT â†“ TACoS â–³ â†‘ ActivityNet-Captions GT â†‘ ActivityNet-Captions Non-GT â†“ ActivityNet-Captions â–³ â†‘ QVHighlights GT â†‘ QVHighlights Non-GT â†“ QVHighlights â–³ â†‘ Baseline 0.42 0.20 0.22 0.56 0.18 0.38 0.52 0.24 0.28 0.67 0.35 0.32 BM-DETR (ours) 0.56 0.13 0.43 0.60 0.11 0.49 0.56 0.21 0.35 0.73 0.28 0.45 ğŸ”¼ í‘œ 6ì€ ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ì •ë ¬ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì •í™•í•œ ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ì •ë ¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´, ì‹ (6)ì—ì„œ ê³„ì‚°ëœ ê° í”„ë ˆì„ì˜ ê²°í•© í™•ë¥  í‰ê· ê°’ì„ ì§€ìƒ ì§„ì‹¤ ëª¨ë©˜íŠ¸ ë‚´ë¶€(GT)ì™€ ì™¸ë¶€(Non-GT)ë¡œ ë‚˜ëˆ„ì–´ ë¹„êµ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. GTëŠ” ì§€ìƒ ì§„ì‹¤ ëª¨ë©˜íŠ¸ì— ì†í•œ í”„ë ˆì„ë“¤ì˜ ê²°í•© í™•ë¥  í‰ê· ì´ê³ , Non-GTëŠ” ì§€ìƒ ì§„ì‹¤ ëª¨ë©˜íŠ¸ì— ì†í•˜ì§€ ì•Šì€ í”„ë ˆì„ë“¤ì˜ ê²°í•© í™•ë¥  í‰ê· ì…ë‹ˆë‹¤. ë‘ ê°’ì˜ ì°¨ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ì§€ìƒ ì§„ì‹¤ ëª¨ë©˜íŠ¸ë¥¼ ì‹ë³„í•˜ëŠ”ì§€ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°’ì´ í´ìˆ˜ë¡ ëª¨ë¸ì˜ ì •ë ¬ ì„±ëŠ¥ì´ ì¢‹ìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nread the caption Table 6: Evaluation of video-text alignment. The average of the joint probabilities of frames p (in EquationÂ 6) inside and outside the ground-truth moment, denoted as GT and Non-GT, respectively. BMD FS LS TS Charades-STA (IoU=0.5) Charades-STA (IoU=0.7) 51.43 28.87 âœ“ 54.73 33.28 âœ“ 53.76 32.13 âœ“ 54.39 32.23 âœ“ 53.47 31.12 âœ“ âœ“ 55.02 33.64 âœ“ âœ“ 53.98 33.53 âœ“ âœ“ âœ“ 58.79 35.04 âœ“ âœ“ âœ“ âœ“ 59.48 38.33 ğŸ”¼ í‘œ 7ì€ BM-DETR ëª¨ë¸ì˜ ì„±ëŠ¥ì— ê° êµ¬ì„± ìš”ì†Œê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. BMD(Background-aware Moment Detection), FS(Fine-grained Semantic Alignment), LS(Learnable Spans), TS(Temporal Shifting) ë“± ë„¤ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¥¼ ì œê±°í–ˆì„ ë•Œì˜ ì„±ëŠ¥ ë³€í™”ë¥¼ IoU 0.5ì™€ 0.7 ê¸°ì¤€ìœ¼ë¡œ ì •ëŸ‰ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•˜ì—¬ ê° êµ¬ì„± ìš”ì†Œì˜ ì¤‘ìš”ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° êµ¬ì„±ìš”ì†Œì˜ ê¸°ì—¬ë„ë¥¼ ëª…í™•íˆ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” í‘œì…ë‹ˆë‹¤.\nread the caption Table 7: Ablations on model components. BMD: background-aware moment detection, FS: fine-grained semantic alignment, LS: learnable spans, and TS: temporal shifting. \\mathcal{L} \\mathcal{L}_m \\mathcal{L}_s \\mathcal{L}_p Charades-STA Charades-STA IoU=0.5 IoU=0.7 IoU=0.5 IoU=0.7 \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; âœ“ âœ“ âœ“ 18.36 5.31 âœ“ 29.02 14.63 âœ“ âœ“ 56.49 36.11 âœ“ âœ“ 57.42 36.01 âœ“ âœ“ 56.32 35.45 âœ“ âœ“ âœ“ 58.10 36.23 âœ“ âœ“ âœ“ 57.84 36.70 âœ“ âœ“ âœ“ 58.68 37.59 âœ“ âœ“ âœ“ âœ“ 59.48 38.33 ğŸ”¼ í‘œ 8ì€ ì†ì‹¤ í•¨ìˆ˜ë“¤ì— ëŒ€í•œ ablation study ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ì—¬ë„ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•´, ëª¨ë©˜íŠ¸ ìœ„ì¹˜ ì°¾ê¸° ì†ì‹¤ê³¼ í´ë˜ìŠ¤ ì†ì‹¤ì„ ê²°í•©í•œ â„’ (â„’caligraphic_L), í”„ë ˆì„ ë§ˆì§„ ì†ì‹¤ â„’m (â„’msubscriptâ„’m caligraphic_Lstart_POSTSUBSCRIPT roman_m end_POSTSUBSCRIPT), ì˜ë¯¸ì  ì •ë ¬ ì†ì‹¤ â„’s (â„’ssubscriptâ„’s caligraphic_Lstart_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT), ê·¸ë¦¬ê³  í”„ë ˆì„ í™•ë¥  ì†ì‹¤ â„’p (â„’psubscriptâ„’p caligraphic_Lstart_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT) ë“± ë„¤ ê°€ì§€ ì†ì‹¤ í•¨ìˆ˜ì˜ ì¡°í•©ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”ë¥¼ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤. ê° ì†ì‹¤ í•¨ìˆ˜ì˜ ìœ ë¬´ì— ë”°ë¥¸ ì„±ëŠ¥ ì°¨ì´ë¥¼ í†µí•´ ê° ì†ì‹¤ í•¨ìˆ˜ì˜ ëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•œ ì¤‘ìš”ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nread the caption Table 8: Ablations on losses. We denote each loss as â„’â„’\\mathcal{L}caligraphic_L: combination of moment localization loss and class loss, â„’msubscriptâ„’m\\mathcal{L}_{\\rm m}caligraphic_L start_POSTSUBSCRIPT roman_m end_POSTSUBSCRIPT: frame margin loss, â„’ssubscriptâ„’s\\mathcal{L}_{\\rm s}caligraphic_L start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT: semantic align loss, and â„’psubscriptâ„’p\\mathcal{L}_{\\rm p}caligraphic_L start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT: frame probability loss. Method Iteration Total Inference Total Training #GPU MMN [44] 0.32s 37s 10h 6 G2L [21] 0.84s 43s - 8 BM-DETR (ours) 0.19s 21s 3h 1 ğŸ”¼ í‘œ 9ëŠ” ActivityNet-Captions ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ë¹„ë””ì˜¤ ìˆœê°„ ê²€ìƒ‰(VMR) ì‘ì—…ì—ì„œ ì œì•ˆëœ BM-DETR ëª¨ë¸ê³¼ ê¸°ì¡´ ë°©ë²•ë“¤ì˜ íš¨ìœ¨ì„±ì„ ë¹„êµí•œ í‘œì…ë‹ˆë‹¤. BM-DETRì˜ ì¶”ë¡  ë° í•™ìŠµ ì‹œê°„, ì´ í•™ìŠµ ì‹œê°„, ì‚¬ìš©ëœ GPU ìˆ˜ë¥¼ ê¸°ì¡´ ë°©ë²•ë“¤(MMN, G2L)ê³¼ ë¹„êµí•˜ì—¬ BM-DETRì˜ íš¨ìœ¨ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì˜ ê²°ê³¼ëŠ” í•´ë‹¹ ë…¼ë¬¸ì˜ ì›ë˜ ê²°ê³¼ë¥¼ ë”°ë¦…ë‹ˆë‹¤.\nread the caption Table 9: Efficiency comparison on Anet-Cap. The results of the other studies follow the original papers. Full paper # ","date":"5 June 2023","externalUrl":null,"permalink":"/ai-paper-reviewer/paper-reviews/2306.02728/","section":"Paper Reviews by AI","summary":"BM-DETR: ë°°ê²½ ì •ë³´ í™œìš©ìœ¼ë¡œ ë¹„ë””ì˜¤ ìˆœê°„ ê²€ìƒ‰ì˜ ì•½í•œ ì •ë ¬ ë¬¸ì œ í•´ê²°!","title":"Background-aware Moment Detection for Video Moment Retrieval","type":"paper-reviews"},{"content":"","date":"5 June 2023","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision","type":"tags"},{"content":"","date":"5 June 2023","externalUrl":null,"permalink":"/ai-paper-reviewer/tags/video-understanding/","section":"Tags","summary":"","title":"Video Understanding","type":"tags"},{"content":"","externalUrl":null,"permalink":"/ai-paper-reviewer/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/ai-paper-reviewer/series/","section":"Series","summary":"","title":"Series","type":"series"}]