<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Large Language Models on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/large-language-models/</link><description>Recent content in Large Language Models on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Mon, 23 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/large-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/</guid><description>B-STAR: 자기 학습 추론자에서 탐색과 활용의 균형을 모니터링하고 조정하여 성능을 향상시키는 새로운 프레임워크</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/cover.png"/></item><item><title>Deliberation in Latent Space via Differentiable Cache Augmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/</guid><description>대규모 언어 모델의 추론 성능을 향상시키는 새로운 방법인 ‘차별 가능한 캐시 증강’ 기법 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/cover.png"/></item><item><title>Diving into Self-Evolving Training for Multimodal Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/</guid><description>M-STAR: 다모달 추론을 위한 자기 진화 훈련의 새로운 프레임워크를 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/cover.png"/></item><item><title>ResearchTown: Simulator of Human Research Community</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17767/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17767/</guid><description>RESEARCHTOWN: LLM 기반 인간 연구 공동체 시뮬레이터로, 다양한 연구 활동을 현실적으로 모방하며 학제 간 연구 아이디어 생성 가능</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17767/cover.png"/></item><item><title>OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/</guid><description>OpenRFT는 제한된 도메인 특정 데이터를 사용하여 일반적인 추론 모델을 미세 조정하는 새로운 방법을 제시합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/cover.png"/></item><item><title>Revisiting In-Context Learning with Long Context Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/</guid><description>장문 컨텍스트 언어 모델에서 정교한 샘플 선택 전략보다 &lt;strong>무작위 샘플링&lt;/strong>이 ICL 성능 향상에 더 효과적이며, &lt;strong>데이터 증강&lt;/strong>을 통해 저자원 작업 성능을 5% 향상시켰다는 놀라운 연구 결과를 발표!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/cover.png"/></item><item><title>NILE: Internal Consistency Alignment in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/</link><pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/</guid><description>NILE 프레임워크는 LLM의 내부 지식과 IFT 데이터셋의 세계 지식 간 일관성을 높여 LLM 성능을 최대 68.5%까지 향상시킵니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/cover.png"/></item><item><title>AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/</guid><description>AceMath는 사전 훈련 및 보상 모델링을 통해 최첨단 수학 추론 능력을 달성한 프런티어급 모델 시리즈입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/cover.png"/></item><item><title>Fietje: An open, efficient LLM for Dutch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/</guid><description>Fietje: 오픈소스 소형 네덜란드어 LLM 공개!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/cover.png"/></item><item><title>How to Synthesize Text Data without Model Collapse?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/</guid><description>합성 데이터 기반 언어 모델 학습의 붕괴 문제 해결: 토큰 편집 기법 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/cover.png"/></item><item><title>LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/</guid><description>M-ALERT는 다국어 LLM의 안전성을 평가하기 위한 새로운 벤치마크입니다. 영어, 프랑스어, 독일어, 이탈리아어, 스페인어 5개 언어의 75,000개 프롬프트를 포함하며, 다양한 언어 및 범주에서 LLM의 안전성 불일치를 밝혀냈습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/cover.png"/></item><item><title>MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/</guid><description>MixLLM: 출력 특징 간의 전역 혼합 정밀도 양자화와 고효율 시스템 설계를 통해 LLM의 정확도와 효율성을 동시에 향상시키는 획기적인 양자화 방법</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/cover.png"/></item><item><title>Outcome-Refining Process Supervision for Code Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/</guid><description>복잡한 알고리즘 추론이 필요한 코드 생성 과제에서 기존의 한계를 극복하는 새로운 방법론, Outcome-Refining Process Supervision (ORPS) 제시</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/cover.png"/></item><item><title>RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/</guid><description>ROBUSTFT는 잡음이 포함된 응답 아래에서 대규모 언어 모델의 강건한 지도 학습 미세 조정을 위한 프레임워크로, 잡음 감지 및 재라벨링을 통해 하류 작업 성능을 향상시킵니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/cover.png"/></item><item><title>TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14642/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14642/</guid><description>TOMG-Bench: LLM 기반 오픈 분자 생성 벤치마크 제시! 25개 LLM 평가 및 새로운 instruction tuning 데이터셋 OpenMolIns 공개로, 오픈소스 LLM의 성능 향상 및 분자 발견의 새로운 가능성 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14642/cover.png"/></item><item><title>AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/</guid><description>AntiLeak-Bench: 자동화된 벤치마킹으로 LLM 데이터 오염 방지</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/cover.png"/></item><item><title>RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/</guid><description>RAG-RewardBench: RAG 환경에서 보상 모델 평가를 위한 최초의 벤치마크 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/cover.png"/></item><item><title>Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13663/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13663/</guid><description>ModernBERT: 빠르고 메모리 효율적인 장문 컨텍스트 미세 조정 및 추론을 위한 최첨단 양방향 인코더!</description></item><item><title>TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/</guid><description>TheAgentCompany 벤치마크는 실제 소프트웨어 회사 환경을 모방하여 LLM 에이전트의 실제 업무 수행 능력을 평가하며, AI 에이전트의 현실 세계 적용 가능성과 한계를 보여줍니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/cover.png"/></item><item><title>DateLogicQA: Benchmarking Temporal Biases in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13377/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13377/</guid><description>DateLogicQA: LLM의 시간적 추론 편향 벤치마크 제시! 토큰화, 표상 및 논리 수준 편향 분석으로 시간적 데이터 처리 개선 방안 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13377/cover.png"/></item><item><title>SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/</guid><description>SepLLM은 특수 토큰의 중요성을 활용하여 LLM 추론을 가속화하고 긴 시퀀스를 효율적으로 처리합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/cover.png"/></item><item><title>SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/</guid><description>Self-play with refinement boosts instruction-following in LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/cover.png"/></item><item><title>The Open Source Advantage in Large Language Models (LLMs)</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12004/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12004/</guid><description>오픈소스 LLM, 폐쇄형 LLM 대비 투명성과 접근성은 높지만, 성능은 낮음. 하이브리드 전략이 미래.</description></item><item><title>Whisper-GPT: A Hybrid Representation Audio Large Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11449/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11449/</guid><description>Whisper-GPT: 하이브리드 음성 및 음악 LLM으로, 연속 오디오와 이산 토큰을 결합하여 향상된 성능을 제공합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11449/cover.png"/></item><item><title>Reliable, Reproducible, and Really Fast Leaderboards with Evalica</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11314/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11314/</guid><description>Evalica: 벤치마킹을 쉽고 빠르고 신뢰할 수 있게 만드는 툴킷</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11314/cover.png"/></item><item><title>Smaller Language Models Are Better Instruction Evolvers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/</guid><description>소형 언어 모델이 더 나은 명령 생성자!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/cover.png"/></item><item><title>Byte Latent Transformer: Patches Scale Better Than Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/</guid><description>BLT: 바이트 기반 LLM, 토큰보다 패치 우선.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/cover.png"/></item><item><title>Large Action Models: From Inception to Implementation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/</guid><description>LLM에서 LAM으로: 실제 작업을 수행하는 AI 에이전트 구축.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/cover.png"/></item><item><title>SCBench: A KV Cache-Centric Analysis of Long-Context Methods</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/</guid><description>SCBench는 멀티턴 및 멀티리퀘스트 시나리오에서 장문 맥락 메서드를 평가하는 새로운 벤치마크입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/cover.png"/></item><item><title>GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong Prompt Optimizers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09722/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09722/</guid><description>GREATER는 추론에 대한 그레이디언트를 활용하여 소규모 언어 모델의 프롬프트를 최적화하여 대규모 LLM 없이도 성능을 향상시킵니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09722/cover.png"/></item><item><title>Phi-4 Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/</guid><description>Phi-4: 140억 매개변수 언어 모델은 &lt;strong>데이터 품질에 중점을 둔 훈련 레시피&lt;/strong>로 개발되어 추론 능력을 대폭 향상시켰습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/cover.png"/></item><item><title>SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/</guid><description>Smaller language models reason better with fine-tuned training recipes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/cover.png"/></item><item><title>NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/</guid><description>NeuZip dynamically compresses neural network weights, achieving memory-efficient training and inference without performance loss, significantly reducing the memory footprint of large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/cover.png"/></item><item><title>Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.14703/</link><pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.14703/</guid><description>LLM의 개성을 정량적으로 평가하는 새로운 벤치마크 TRAIT 제시: 신뢰성 및 타당성 높은 8,000개의 질문으로 구성, LLM 개성의 독특성과 일관성 규명, 모델 정렬 과정의 영향 분석 및 제한점 제시.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.14703/cover.png"/></item></channel></rss>