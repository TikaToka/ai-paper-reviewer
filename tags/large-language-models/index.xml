<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Large Language Models on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/large-language-models/</link><description>Recent content in Large Language Models on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2025 AI Paper Reviews by AI</copyright><lastBuildDate>Thu, 02 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/large-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01540/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01540/</guid><description>BoxingGym: LLM 기반 과학적 에이전트의 실험 설계 및 모델 발견 능력 종합 평가 벤치마크</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01540/cover.png"/></item><item><title>CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01257/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01257/</guid><description>CODEELO 벤치마크: 인간 수준의 Elo 등급으로 LLM의 경쟁적 코드 생성 능력 평가</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01257/cover.png"/></item><item><title>Dynamic Scaling of Unit Tests for Code Reward Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01054/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01054/</guid><description>단위 테스트의 수를 늘려 코드 보상 모델의 정확성을 높이는 방법을 제시하는 연구!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01054/cover.png"/></item><item><title>Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00712/</link><pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00712/</guid><description>TAPE(conTextualized equivAriant Position Embedding) 프레임워크를 통해 문맥 정보를 활용한 동적 위치 인코딩으로 트랜스포머의 위치 기반 주소 지정 성능을 향상시켰습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00712/cover.png"/></item><item><title>Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00658/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00658/</guid><description>심층 신경망의 장기 의존성을 모델링하는 구조적 상태 공간 모델(SSM)의 한계를 극복! 최신 연구에서 SSM의 &lt;strong>최근 편향(recency bias)&lt;/strong> 및 &lt;strong>과도한 평활화(over-smoothing)&lt;/strong> 문제를 규명하고, 이를 해결하는 **극성화 기법(polarization)**을 제시하여 장기 토큰 상관관계 정확도를 높였습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00658/cover.png"/></item><item><title>Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21187/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21187/</guid><description>대규모 언어 모델의 과도한 연산 문제 해결: 효율적인 추론을 위한 새로운 지표 및 자기 학습 전략 제시</description></item><item><title>Efficiently Serving LLM Reasoning Programs with Certaindex</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20993/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20993/</guid><description>Dynasor은 LLM 추론 프로그램의 자원 사용을 최적화하는 시스템으로, &lt;strong>certaindex&lt;/strong>라는 새로운 지표를 활용하여 어려운 질의에는 더 많은 연산을, 간단한 질의에는 적은 연산을 할당하고, 전망이 없는 질의는 조기에 종료함으로써 정확도, 지연 시간 및 비용을 균형 있게 맞춥니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20993/cover.png"/></item><item><title>Facilitating large language model Russian adaptation with Learned Embedding Propagation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21140/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21140/</guid><description>LEP(Learned Embedding Propagation)는 적은 양의 학습 데이터만으로도 다국어 대규모 언어 모델을 효율적으로 적응시키는 새로운 기법입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21140/cover.png"/></item><item><title>HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21199/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21199/</guid><description>LLM의 점진적 추론 및 문제 해결 능력을 평가하기 위한 새로운 벤치마크 HumanEval Pro, MBPP Pro, BigCodeBench-Lite Pro 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21199/cover.png"/></item><item><title>HUNYUANPROVER: A Scalable Data Synthesis Framework and Guided Tree Search for Automated Theorem Proving</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20735/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20735/</guid><description>HunyuanProver: 대규모 언어 모델 기반의 확장 가능한 데이터 합성 프레임워크와 안내 트리 탐색을 통해 최첨단 자동 정리 증명 성능 달성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20735/cover.png"/></item><item><title>Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19512/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19512/</guid><description>미세 조정으로 안전성이 저하된 LLM의 성능을 향상시키는 동시에 안전성을 유지하는 간편하고 효과적인 모델 결합 방법 제시!</description></item><item><title>Xmodel-2 Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19638/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19638/</guid><description>Xmodel-2: 12억 매개변수의 추론 전문 대규모 언어 모델로, 효율적인 설계와 훈련 전략을 통해 최첨단 성능 달성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19638/cover.png"/></item><item><title>Token-Budget-Aware LLM Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18547/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18547/</guid><description>토큰 예산 인식 LLM 추론 프레임워크(TALE)를 통해 LLM 추론의 토큰 비용을 크게 줄이면서 성능 저하를 최소화했습니다!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18547/cover.png"/></item><item><title>B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/</guid><description>B-STAR: 자기 학습 추론자에서 탐색과 활용의 균형을 모니터링하고 조정하여 성능을 향상시키는 새로운 프레임워크</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/cover.png"/></item><item><title>Deliberation in Latent Space via Differentiable Cache Augmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/</guid><description>대규모 언어 모델의 추론 성능을 향상시키는 새로운 방법인 ‘차별 가능한 캐시 증강’ 기법 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/cover.png"/></item><item><title>Diving into Self-Evolving Training for Multimodal Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/</guid><description>M-STAR: 다모달 추론을 위한 자기 진화 훈련의 새로운 프레임워크를 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/cover.png"/></item><item><title>Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17739/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17739/</guid><description>FoPE: 주파수 영역 특징 개선으로 긴 문맥 길이 일반화 달성!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17739/cover.png"/></item><item><title>In Case You Missed It: ARC 'Challenge' Is Not That Challenging</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17758/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17758/</guid><description>기존 다중 선택 문제 평가 방식의 오류를 지적하고, 모든 옵션을 함께 고려하는 새로운 평가 방식을 제안하여 모델 성능 평가의 정확성을 높였습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17758/cover.png"/></item><item><title>ResearchTown: Simulator of Human Research Community</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17767/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17767/</guid><description>RESEARCHTOWN: LLM 기반 인간 연구 공동체 시뮬레이터로, 다양한 연구 활동을 현실적으로 모방하며 학제 간 연구 아이디어 생성 가능</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17767/cover.png"/></item><item><title>YuLan-Mini: An Open Data-efficient Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17743/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17743/</guid><description>YuLan-Mini: 24억 개 매개변수를 가진 데이터 효율적인 개방형 LLM</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17743/cover.png"/></item><item><title>OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/</guid><description>OpenRFT는 제한된 도메인 특정 데이터를 사용하여 일반적인 추론 모델을 미세 조정하는 새로운 방법을 제시합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/cover.png"/></item><item><title>Revisiting In-Context Learning with Long Context Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/</guid><description>장문 컨텍스트 언어 모델에서 정교한 샘플 선택 전략보다 &lt;strong>무작위 샘플링&lt;/strong>이 ICL 성능 향상에 더 효과적이며, &lt;strong>데이터 증강&lt;/strong>을 통해 저자원 작업 성능을 5% 향상시켰다는 놀라운 연구 결과를 발표!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/cover.png"/></item><item><title>NILE: Internal Consistency Alignment in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/</link><pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/</guid><description>NILE 프레임워크는 LLM의 내부 지식과 IFT 데이터셋의 세계 지식 간 일관성을 높여 LLM 성능을 최대 68.5%까지 향상시킵니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/cover.png"/></item><item><title>Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15797/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15797/</guid><description>대규모 언어 모델들의 앙상블을 통해 복잡한 추론 문제를 더욱 효과적으로 해결하는 새로운 프레임워크, LE-MCTS를 제안합니다!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15797/cover.png"/></item><item><title>AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/</guid><description>AceMath는 사전 훈련 및 보상 모델링을 통해 최첨단 수학 추론 능력을 달성한 프런티어급 모델 시리즈입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/cover.png"/></item><item><title>Fietje: An open, efficient LLM for Dutch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/</guid><description>Fietje: 오픈소스 소형 네덜란드어 LLM 공개!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/cover.png"/></item><item><title>How to Synthesize Text Data without Model Collapse?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/</guid><description>합성 데이터 기반 언어 모델 학습의 붕괴 문제 해결: 토큰 편집 기법 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/cover.png"/></item><item><title>LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/</guid><description>M-ALERT는 다국어 LLM의 안전성을 평가하기 위한 새로운 벤치마크입니다. 영어, 프랑스어, 독일어, 이탈리아어, 스페인어 5개 언어의 75,000개 프롬프트를 포함하며, 다양한 언어 및 범주에서 LLM의 안전성 불일치를 밝혀냈습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/cover.png"/></item><item><title>MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/</guid><description>MixLLM: 출력 특징 간의 전역 혼합 정밀도 양자화와 고효율 시스템 설계를 통해 LLM의 정확도와 효율성을 동시에 향상시키는 획기적인 양자화 방법</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/cover.png"/></item><item><title>Outcome-Refining Process Supervision for Code Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/</guid><description>복잡한 알고리즘 추론이 필요한 코드 생성 과제에서 기존의 한계를 극복하는 새로운 방법론, Outcome-Refining Process Supervision (ORPS) 제시</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/cover.png"/></item><item><title>ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14711/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14711/</guid><description>ReLU 라우팅을 사용하는 완전 미분 가능한 MoE 아키텍처 ReMoE를 통해 대규모 언어 모델의 확장성과 효율성을 획기적으로 개선했습니다!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14711/cover.png"/></item><item><title>RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/</guid><description>ROBUSTFT는 잡음이 포함된 응답 아래에서 대규모 언어 모델의 강건한 지도 학습 미세 조정을 위한 프레임워크로, 잡음 감지 및 재라벨링을 통해 하류 작업 성능을 향상시킵니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/cover.png"/></item><item><title>TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14642/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14642/</guid><description>TOMG-Bench: LLM 기반 오픈 분자 생성 벤치마크 제시! 25개 LLM 평가 및 새로운 instruction tuning 데이터셋 OpenMolIns 공개로, 오픈소스 LLM의 성능 향상 및 분자 발견의 새로운 가능성 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14642/cover.png"/></item><item><title>AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/</guid><description>AntiLeak-Bench: 자동화된 벤치마킹으로 LLM 데이터 오염 방지</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/cover.png"/></item><item><title>RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/</guid><description>RAG-RewardBench: RAG 환경에서 보상 모델 평가를 위한 최초의 벤치마크 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/cover.png"/></item><item><title>Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13663/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13663/</guid><description>ModernBERT: 빠르고 메모리 효율적인 장문 컨텍스트 미세 조정 및 추론을 위한 최첨단 양방향 인코더!</description></item><item><title>TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/</guid><description>TheAgentCompany 벤치마크는 실제 소프트웨어 회사 환경을 모방하여 LLM 에이전트의 실제 업무 수행 능력을 평가하며, AI 에이전트의 현실 세계 적용 가능성과 한계를 보여줍니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/cover.png"/></item><item><title>DateLogicQA: Benchmarking Temporal Biases in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13377/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13377/</guid><description>DateLogicQA: LLM의 시간적 추론 편향 벤치마크 제시! 토큰화, 표상 및 논리 수준 편향 분석으로 시간적 데이터 처리 개선 방안 제시!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13377/cover.png"/></item><item><title>SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/</guid><description>SepLLM은 특수 토큰의 중요성을 활용하여 LLM 추론을 가속화하고 긴 시퀀스를 효율적으로 처리합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/cover.png"/></item><item><title>SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/</guid><description>Self-play with refinement boosts instruction-following in LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/cover.png"/></item><item><title>The Open Source Advantage in Large Language Models (LLMs)</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12004/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12004/</guid><description>오픈소스 LLM, 폐쇄형 LLM 대비 투명성과 접근성은 높지만, 성능은 낮음. 하이브리드 전략이 미래.</description></item><item><title>Whisper-GPT: A Hybrid Representation Audio Large Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11449/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11449/</guid><description>Whisper-GPT: 하이브리드 음성 및 음악 LLM으로, 연속 오디오와 이산 토큰을 결합하여 향상된 성능을 제공합니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11449/cover.png"/></item><item><title>Reliable, Reproducible, and Really Fast Leaderboards with Evalica</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11314/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11314/</guid><description>Evalica: 벤치마킹을 쉽고 빠르고 신뢰할 수 있게 만드는 툴킷</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11314/cover.png"/></item><item><title>Smaller Language Models Are Better Instruction Evolvers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/</guid><description>소형 언어 모델이 더 나은 명령 생성자!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/cover.png"/></item><item><title>Byte Latent Transformer: Patches Scale Better Than Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/</guid><description>BLT: 바이트 기반 LLM, 토큰보다 패치 우선.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/cover.png"/></item><item><title>Large Action Models: From Inception to Implementation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/</guid><description>LLM에서 LAM으로: 실제 작업을 수행하는 AI 에이전트 구축.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/cover.png"/></item><item><title>SCBench: A KV Cache-Centric Analysis of Long-Context Methods</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/</guid><description>SCBench는 멀티턴 및 멀티리퀘스트 시나리오에서 장문 맥락 메서드를 평가하는 새로운 벤치마크입니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/cover.png"/></item><item><title>GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong Prompt Optimizers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09722/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09722/</guid><description>GREATER는 추론에 대한 그레이디언트를 활용하여 소규모 언어 모델의 프롬프트를 최적화하여 대규모 LLM 없이도 성능을 향상시킵니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09722/cover.png"/></item><item><title>Phi-4 Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/</guid><description>Phi-4: 140억 매개변수 언어 모델은 &lt;strong>데이터 품질에 중점을 둔 훈련 레시피&lt;/strong>로 개발되어 추론 능력을 대폭 향상시켰습니다.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/cover.png"/></item><item><title>SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/</guid><description>Smaller language models reason better with fine-tuned training recipes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/cover.png"/></item><item><title>NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/</guid><description>NeuZip dynamically compresses neural network weights, achieving memory-efficient training and inference without performance loss, significantly reducing the memory footprint of large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/cover.png"/></item><item><title>Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.14703/</link><pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.14703/</guid><description>LLM의 개성을 정량적으로 평가하는 새로운 벤치마크 TRAIT 제시: 신뢰성 및 타당성 높은 8,000개의 질문으로 구성, LLM 개성의 독특성과 일관성 규명, 모델 정렬 과정의 영향 분석 및 제한점 제시.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.14703/cover.png"/></item></channel></rss>