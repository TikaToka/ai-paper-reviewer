<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ğŸ¢ Zhejiang University on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-zhejiang-university/</link><description>Recent content in ğŸ¢ Zhejiang University on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 AI Paper Reviews by AI</copyright><lastBuildDate>Tue, 31 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-zhejiang-university/index.xml" rel="self" type="application/rss+xml"/><item><title>VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00599/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00599/</guid><description>VideoRefer SuiteëŠ” &lt;strong>ì •êµí•œ ê³µê°„-ì‹œê°„ì  ê°œì²´ ì´í•´ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ë¹„ë””ì˜¤ LLM(VideoRefer)ê³¼ ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹(VideoRefer-700K), ì¢…í•©ì ì¸ ë²¤ì¹˜ë§ˆí¬(VideoRefer-Bench)ë¥¼ ì œì‹œ&lt;/strong>í•©ë‹ˆë‹¤.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00599/cover.png"/></item><item><title>OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/</link><pubDate>Sat, 28 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/</guid><description>OneKE: ë„ì»¤ ê¸°ë°˜, ë‹¤ì¤‘ ì—ì´ì „íŠ¸ LLM ì§€ì‹ ì¶”ì¶œ ì‹œìŠ¤í…œìœ¼ë¡œ ì›¹, PDFì—ì„œ ë‹¤ì–‘í•œ ë„ë©”ì¸ ì§€ì‹ ì¶”ì¶œ ê°€ëŠ¥</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/cover.png"/></item><item><title>Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/</guid><description>ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ê°ì²´ ë°©í–¥ ì¶”ì •ì˜ ì •í™•ë„ë¥¼ í¬ê²Œ ë†’ì´ëŠ” &amp;lsquo;Orient Anything&amp;rsquo; ëª¨ë¸ ì œì‹œ!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/cover.png"/></item><item><title>Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/</guid><description>ì €ë ´í•œ ë¼ì´ë‹¤ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ 4K ê³ í•´ìƒë„ ì •í™•í•œ ê³„ëŸ‰ì  ê¹Šì´ ì¶”ì •ì„ ìœ„í•œ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„, Prompt Depth Anything ì œì‹œ!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/cover.png"/></item></channel></rss>